[
    {
        "title": "RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought"
    },
    {
        "review": {
            "id": "EyMZdNlpiZ",
            "forum": "cG8Q4FE0Hi",
            "replyto": "cG8Q4FE0Hi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4043/Reviewer_vr2D"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4043/Reviewer_vr2D"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to improve LLM\u2019s reasoning abilities and address the challenges of overlooking, question misinterpretation and condition hallucination in LLMs\u2019 generated solutions. It proposes RCoT to detect and rectify such factual inconsistency through four steps, including reconstruction, decomposition, comparison, and revision. The experiments are conducted on randomly sampled sub-sets of seven arithmetic datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The motivation is clear and the analysis of challenges is reasonable.\n- The performance of the proposed RCoT is demonstrated to be superior to the standard baselines."
                },
                "weaknesses": {
                    "value": "- The experiments are only conducted on randomly sampled sub-sets of the test sets, which may raise concerns about the convincingness of the results. The experiment results do not allow for a direct apple-to-apple comparison with the reported results in other papers, such as those related to Self-consistency.\n- The experiments on other reasoning tasks, such as commonsense reasoning and symbolic reasoning, are absent.\n- The performance improvement is not significant compared to the self-consistency (84.5 v.s 83.5).  In addition, did the paper's testing of the Self-consistency algorithm use 30 paths? In Self-consistency, the typical number of paths used is (1, 5, 10, 20, 40). Why were 30 paths chosen? If the reason is to make comparable comparisons based on average tokens, it would be appropriate to report the performance and average tokens under different numbers of paths.\n- There is a lack of in-depth analysis and evaluation beyond overall performance, such as the absence of assessment regarding improvements (Quantitative or user-study-based evaluations) in the three areas of overlooking, question misinterpretation, and condition hallucination. Table 5 only evaluates on 45 cases.\n- The method is somewhat incremental. The decomposition, comparison, and revision components are not new in the context of CoT. While reconstruction is used in many fields, its application within CoT is new and appears to be the main technical contribution. However, the overall framework of RCoT is incremental and complex.\n- Minor suggestions about the presentation:\n    - On page 1, this paper introduces the condition of  \"2 days away\" in Figure 1 is mistakenly overlooked. However, there are no \"2 days away\" in Figure 1.\n    - Can the three different examples in the Introduction be unified?\n    - The font size in Table 1 is too small.\n    - The order of citing figures and charts is mixed up. For example, Table 4 is cited before Tables 2 and 3, but it is located below them in the paper."
                },
                "questions": {
                    "value": "See Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4043/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697975781220,
            "cdate": 1697975781220,
            "tmdate": 1699636367429,
            "mdate": 1699636367429,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tVq17tSC3y",
                "forum": "cG8Q4FE0Hi",
                "replyto": "EyMZdNlpiZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4043/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4043/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer vr2D (1/2)"
                    },
                    "comment": {
                        "value": "**W1: only conducted on randomly sampled sub-sets of the test sets**\n\nThanks for pointing out this problem! According to our observation, in asdiv, svamp, Date, addsub, singleeq datasets, 256 samples can well cover various types of problems. For the more diverse datasets aqua and gsm8k, we include the entire aqua dataset, and for gsm8k we expand the number of samples to 550 (see Figure 1). Considering that 3 groups of 256 can already cover all datasets as various types of problems and reduce budget overhead, so we conduct experiments on sub-datasets.\n\nWe would also want to address that apple-to-apple comparison to results in other papers is hard, or sometimes, impossible. For example, Self-Consistency does not use ChatGPT, Active-Prompting uses CodeX which is not supported anymore, and Self-Refine uses gpt-3.5-turbo which changes over time. Therefore, we believe implementing them ourselves and guaranteeing them in a fair environment is a better choice.\n\n\n**W2: The experiments on other reasoning tasks, such as commonsense reasoning and symbolic reasoning, are absent**\n\nThis a very interesting question! Currently, we only focus on math reasoning tasks due to the limited time. In the future, we will explore RCoT on other reasoning tasks. Thank you for your suggestions.\n\n**W3: Report the performance and average tokens under different numbers of paths**\n\nWe apologize for the confusion. And we report more different numbers of paths(10 and 20 for self-consistency). See the result:\n\n| Method                                                                      |  GSM8K |  AQuA  | AddSub |  Date  | SingleEq |  ASDiv |  SVAMP |   Avg acc  | Avg tokens |\n|--------------------------------------------------------------------------|:------:|:------:|:------:|:------:|:--------:|:------:|:------:|:----------:|:----------:|\n| SC (10 trials per problem)                                      | 80.46% | 68.11% | 86.32% | 78.90% |  92.57%  | 88.67% |  78.1% |   81.88%   |   1800.2   |\n| SC (20 trials per problem)                                      | 81.64% | 69.29% | 88.28% | 79.68% |  92.57%  | 89.84% |  79.6% |   82.99%   |   3784.1   |\n| SC (30 trials per problem)                                      | 81.64% | 70.86% | 88.67% | 80.07% | 92.96%   | 90.23% | 80.47% | 83.56%     | 5615.0     |\n| RCoT (1 trial per problem)                                      | 82.03% | 56.29% | 87.20% | 71.87% | 92.4%    | 86.33% | 79.69% | 79.40%     | 1831.0     |\n| RCoT (3 trials per problem)                                     | 83.20% | 72.83% | 89.84% | 78.91% | 93.75%   | 91.80% | 81.25% | **84.51%** | 5453.3     |\n| Self-Refine attempt 0 (i.e., Standard CoT)                      | 79.12% | 45.28% | 90.62% | 51.38% | 97.65%   | 83.59% | 75.29% | 74.70%     | 190.2      |\n| Self-Refine attempt 1 (1 trials per problem * 2 call per trial) | 80.72% | 49.21% | 91.41% | 52.75% | 98.04%   | 84.37% | 76.86% | 76.19%     | 3108.4     |\n| Self-Refine attempt 2 (2 trials per problem * 2 call per trial) | 80.72% | 49.21% | 91.41% | 52.75% | 98.04%   | 84.37% | 76.86% | 76.19%     | 3324.9     |\n| Self-Refine attempt 3 (3 trials per problem * 2 call per trial) | 80.72% | 49.21% | 91.41% | 52.75% | 98.04%   | 84.37% | 76.86% | 76.19%     | 3359.6     |\n| Self-Refine attempt 4 (4 trials per problem * 2 call per trial) | 80.72% | 49.21% | 91.41% | 52.75% | 98.04%   | 84.37% | 76.86% | 76.19%     | 3367.7     |\n| Self-Refine attempt 5 (5 trials per problem * 2 call per trial) | 80.72% | 49.21% | 91.41% | 52.75% | 98.04%   | 84.37% | 76.86% | 76.19%     | 3367.7     |\n\nAlthough RCoT cannot be compared with self-consistency with similar costs, we highlight that combining RCoT with other methods can achieve better performance. RCoT mainly focuses on checking and revising LLMs\u2019 first answer, which is different from other traditional methods.  Moreover, combining RCoT with existing methods can improve upper-bound performance. For example, we can observe from Table 1 and Table 4 that combining Self-Consistency and Active-Prompting methods with RCoT can further improve average accuracy by 5.1% and 2.1% across seven datasets, respectively."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4043/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700366063193,
                "cdate": 1700366063193,
                "tmdate": 1700366063193,
                "mdate": 1700366063193,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NKPS8OQ4vS",
            "forum": "cG8Q4FE0Hi",
            "replyto": "cG8Q4FE0Hi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4043/Reviewer_rMGU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4043/Reviewer_rMGU"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed a novel approach RCoT to identify and rectify factual inconsistencies in the outputs of large language models (LLMs). The approach works by first prompting the LLM to reconstruct the question based on its answers, and then prompting the LLM to determine whether the reconstructed question is identical to the original question in terms of the conditions derived. If there are discrepancies between the conditions, the finer differences are used to rectify the LLM to provide a more accurate and consistent answer. Experiments show that the proposed RCoT outperform baselines in seven arithmetic datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The general thoughts of the problem are interesting and novel -- proof by contradiction -- use LLM to prove things by contradiction. From my understanding, LLMs are used in the following different scenarios: (1) reconstructing the problem; (2) Listing the conditions of the original and reconstructed problems; (3) determining whether there are hallucinated and overlooked conditions; (4) Determining whether the reconstructed problem is identical to the original one. (5) rectifying the results based on the finer feedback summarized from (3). The above uses of the LLM are interesting and each worth individual study about the effects."
                },
                "weaknesses": {
                    "value": "Although the general thoughts of the problem are interesting and novel, the paper itself has many obvious flaws.\n\nFirst, the prompting method is overused. The paper does not establish causal connections between the different prompting stages. For example, to determine whether the reconstructed problem is identical to the original one (Figure 20, \u201cquestion comparison\u201d), the method does not consider the prompted results from \u201cproblem decomposition\u201d and \u201ccondition comparison\u201d. Additionally, to rectify the solution, the model does not take the results from \u201cquestion comparison\u201d into account.\n\nSecond, upon reviewing the examples  (Figure 20 \"I apologize for my mistake...\". ), I believe that the authors are exploiting the \"dialog system\" nature of the LLM interfaces. The dialog system introduces an extra conditioning on the chat history, which means that the actual prompts listed in the paper are all conditioned on previous prompts that have been used. As a result, I believe that the experiments are flawed. In contrast, the methods such as CoT (Wei et al., 2023), Active Prompt (Diao et al., 2023), and Self-Consistency (Wang et al., 2023) are stateless, meaning that they only involve a single interaction between a human and the LLM interface. I recommend that the authors learn from CoT which models P(answer|question, reasoning chain of other examples) to better formulate their condition dependences.\n\nThird, the gain of providing the reason seems to be moderate. From table 2, \"judgement\" (Figure 20 \"question comparison\") should be the key factor while \"reason\" (Figure 20, \u201cproblem decomposition\u201d and \u201ccondition comparison\u201d) seems to be less important. From Table 4, the proposed complicated RCoT seems to be worse than Self-Consistency (Wang et al., 2023)."
                },
                "questions": {
                    "value": "I expect the authors to justify their choice of interactively prompting the large language models. Currently, I felt they only proved it kind of works but did not explain the reason. However, I felt the comparison are not fair and the results are not easy to reproduce."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4043/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698565527647,
            "cdate": 1698565527647,
            "tmdate": 1699636367341,
            "mdate": 1699636367341,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CQYlAL8oSo",
                "forum": "cG8Q4FE0Hi",
                "replyto": "NKPS8OQ4vS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4043/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4043/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer rMGU"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the detailed and helpful feedback! We are happy that you find our method novel and effective! We address your concerns in the following texts:\n\n**W1:  the prompting method is overused**\n\nThanks for raising this important point. We would like to clarify that \u201cproblem\u201d and \u201cquestion\u201d are different in our paper. Specifically, \u201cproblem\u201d = \u201cconditions\u201d + \u201cquestion\u201d. For example, a problem \u201cI have two apples, and he has three apples. How many apples do we have in total?\u201d = conditions [\u201cI have two apples\u201d, \u201che has three apples\u201d] + the question \u201cHow many apples do we have in total?\u201d. Therefore, to determine whether the reconstructed problem is identical to the original one, we first decompose problems into \u201cconditions\u201d (multiple conditions per problem) and \u201cquestions\u201d (one question per problem), and then conduct \u201ccondition comparison\u201d and \u201cquestion comparison\u201d. In short, \u201cproblem comparison\u201d = \u201ccondition comparison\u201d + \u201cquestion comparison\u201d. The final feedback is formulated based on the results of \u201cproblem comparison\u201d.\nWe believe there are some misunderstandings between caused by Fig 20. We apologize for the confusion and make a clearer Figure 20.  To rectify solutions, we do take into account \u201cquestion comparison\u201d results. However, in Fig 20, the \u201cquestion comparison\u201d results tell us that the question is correctly understood by models. Therefore, the final feedback in Fig 20 does not contain the result \u201cquestion comparison\u201d as the model did not make mistakes here. The final feedback will include the results of \u201cquestion comparison\u201d if mistakes are found in this process.\nWe apologize for the confusion and add clarifications accordingly in the paper.\n\n**W2: the flaw of the \"dialog system\"**\n\nOur revision step only uses the history of original wrong answer, which is the same as Self-Refine[1]. The core of RCoT is to let LLMs check the answer and rectify it by themselves, so it is necessary to provide the previous answer. It\u2019s worth noting that reconstruction, decomposition, comparison and revision are independent. Each stage only utilizes previous results but not conditions on previous prompts. We argue that \u201cstateless\u201d or not should not be the \u201cflaw\u201d of the method or experiment design, since all methods (RCoT, CoT, Self-Consistency, Self-Refine) only utilize LLM itself without external intervention. RCoT and Self-Refine enables LLM to act as humans that can check and revise answers by themselves to achieve the correct answers. \n\n[1] Madaan, Aman, et al. \"Self-refine: Iterative refinement with self-feedback.\" arXiv preprint arXiv:2303.17651 (2023).\n\n**W3: the gain of providing the reason seems to be moderate**\n\nWe hope our clarification in W1 could help address the understanding of Table 2. The \u201cw/o reason\u201d means we do not tell the LLM what exact mistakes it made, but only tell the LLM that it made mistakes. That is to say, as long as one error is found during \u201cproblem comparison\u201d, we will tell the model it makes mistakes and needs to rectify its answers. Otherwise, we tell the model it does not make mistakes and the current answer is the final answer. The \u201cw/o judgment + reason\u201d has nothing to do with RCoT; it just tells models to double-check answers every time. Therefore, \u201cw/o judgment + reason\u201d denotes the necessity of giving coarse feedback (i.e., Wrong or Correct), whereas \u201cw/o reason\u201d further denotes that fine-grained feedback formulated from the results of \u201cproblem decomposition\u201d can enhance the LLM\u2019s capability of rectifying responses. \n\nWe would also like to point out that RCoT performs better than Self-Consistency in 6 datasets and only underperforms Self-Consistency in one dataset in Table 4."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4043/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700364705239,
                "cdate": 1700364705239,
                "tmdate": 1700364705239,
                "mdate": 1700364705239,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QxUw9F0CSC",
            "forum": "cG8Q4FE0Hi",
            "replyto": "cG8Q4FE0Hi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4043/Reviewer_QEoj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4043/Reviewer_QEoj"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to tackle challenges like condition overlooking, question misinterpretation, and condition hallucination that LLMs meet in arithmetic reasoning benchmarks. On top of chain-of-thought (CoT) prompting, this paper proposes RCoT, which asks the LLM to rewrite the problem, compare it to the original one, and identify fine-grained differences in conditions and questions, thus finding mistakes and revising the original answer. Experiments show consistent improvements across benchmarks and LLMs, verifying effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "*Originality*: The core idea of this paper is novel and original.\n\n*Quality*: The method is well-motivated and extensively evaluated.\n\n*Clarity*: The delivery is very clear and easy to understand, I did not find issues in understanding.\n\n*Reproducibility*: Code is provided to encourage reproducibility.\n\n*Significance*: This work touches a major issue in LLMs, which is of much research significance."
                },
                "weaknesses": {
                    "value": "- One drawback of this work could be its complexity, as illustrated in the diagram and verified by token counts. I understand it is comparable to some previous works, but optimizing its complexity is still an important aspect.\n- (minor) The comparisons in tab. 1 is not very clear, eg, the results marked in green are not straightforward to understand except by reading the captions.\n- (minor) Venues are missing from multiple references."
                },
                "questions": {
                    "value": "Suggestion: it might be better to call \"reconstruction\" as \"rewriting\" or \"paraphrasing\".\nDisclaimer: since I am not very familiar with related literature, my current rating is relatively conservative, and I'll reconsider it after reading opinions from other reviewers."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4043/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4043/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4043/Reviewer_QEoj"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4043/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698691246164,
            "cdate": 1698691246164,
            "tmdate": 1699636367238,
            "mdate": 1699636367238,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vV0hGf9Z90",
                "forum": "cG8Q4FE0Hi",
                "replyto": "QxUw9F0CSC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4043/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4043/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer QEoj"
                    },
                    "comment": {
                        "value": "We thank Reviewer QEoj for the constructive comments. We are glad you find the RCOT method novel and effective. We will address your question in the following paragraphs.\n\n**W1: One drawback of this work could be its complexity**\n\nWe understand the concern about the complexity of RCoT. However, as our experiments show, only fine-grained comparison could lead to fine-grained revision. See more detail in our general response W2.\n\n**W2 and W3:  The comparisons in tab. 1 is not very clear; Venues are missing from multiple references**\n\nWe apologize for not clarifying some results and venues in the paper which led to difficulty in understanding. Specifically, we have already clarified the comparisons in Tab 1 and revised missed venues to help readers better understand RCoT.\n\n**Q1: it might be better to call \"reconstruction\" as \"rewriting\" or \"paraphrasing\"**\n\nThank you for the helpful suggestion! Our idea is to get the original problem by reversing the answer, and find the factual inconsistency between the reversed problem and the original problem. Therefore, we think that \"rewriting\" or \"paraphrasing cannot express our reversing ideas very well. We apologize for the confusion and re-clarify the concept in our paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4043/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700364701632,
                "cdate": 1700364701632,
                "tmdate": 1700364701632,
                "mdate": 1700364701632,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Fm51g1Ch5f",
                "forum": "cG8Q4FE0Hi",
                "replyto": "BUGwxct4DX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4043/Reviewer_QEoj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4043/Reviewer_QEoj"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' response. Currently I have no more unresolved major concerns, and prefer to keep my positive recommendation."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4043/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644321308,
                "cdate": 1700644321308,
                "tmdate": 1700644321308,
                "mdate": 1700644321308,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]