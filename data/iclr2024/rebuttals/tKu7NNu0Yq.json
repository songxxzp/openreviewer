[
    {
        "title": "DeepEMD: A Transformer-based Fast Estimation of the Earth Mover\u2019s Distance"
    },
    {
        "review": {
            "id": "3NiluxK9Ro",
            "forum": "tKu7NNu0Yq",
            "replyto": "tKu7NNu0Yq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7308/Reviewer_yNgN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7308/Reviewer_yNgN"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an attention-based model designed to approximate the Earth Mover's Distance (EMD) for point clouds. Traditional EMD calculations are computationally expensive, so surrogates like the Chamfer distance are often used. To overcome this, the authors focus on computing the point cloud matchings rather than the EMD itself. Experiments show that this model can be used to estimate the EMD faster than using the Hungarian method or Sinkhorn algorithm. The paper demonstrates how the approach can be used to train a point cloud Variational Autoencoder (VAE) using the approximated EMD."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1. Framing of the problem: The framing of the point-cloud matching problem as learning a cost function is quite intersting and makes me think of a number of applications for which this can be used. The empirical results provided, particularly the comparison against the Chamfer distance, the Hungarian method, and the Sinkhorn algorithm (as seen in Figure 8), is nice. This comparison gives a clear view of where the proposed model stands in terms of computational efficiency and accuracy.\n\n\nS2. Clarity: The paper's structure and presentation seem to be well-organized, with the problem clearly articulated. The choice of casting the problem as an estimation of an attention matrix and its relevance is also clear. However, for a reader unfamiliar with point cloud analysis or attention mechanisms, some sections might be dense. A more thorough introduction or background on these concepts could enhance clarity.\n\nS3. Significance: Notwithstanding missing comparisons (see Weaknesses below) I think the proposed model for approximating EMD could be quite useful in a number of scenarios (basically anywhere the Hungarian algorithm or the Chamfer distance is used at the moment). The model's performance, particularly in the out-of-distribution tests, is quite promising. The model also seems to generalize well to larger point-clouds than on which it was trained."
                },
                "weaknesses": {
                    "value": "W1. Lack of Novelty: The problem of point-cloud registration is very well-studied, with numerous learning-based methods proposed over the years. Some of these also include attention mechanisms to do the matching. See the methods listed here: https://github.com/XuyangBai/awesome-point-cloud-registration\n\nW2. Comparative Evaluation: The paper does not compare its method to state-of-the-art registration approaches. Given that point-cloud registration is a long-studied problem, there are likely several strong baseline methods against which the proposed model should be evaluated.\n\nW3. Generalization may be an issue: While the paper mentions that the model generalizes well to larger point clouds during inference than those seen during training, most of the experiments are carried out on datasets consistent of synthetic 3D models. What happens when this is applied to real LiDAR scans or depth images?\n\nW4. Scalability Issues: Attention mechanisms, particularly in the context of a large number of outputs, can be memory-intensive. Given that point clouds can be large (millions of points), it might be difficult to scale the proposed approach to such cases."
                },
                "questions": {
                    "value": "I would suggest the following specific changes to address the weaknesses listed above:\n\nW1. It would be helpful if the paper's contribution can be positioned more clearly within the existing landscape of point-cloud registration methods. Providing a thorough review of existing methods, especially those that leverage learning-based approaches, will help highlight the contributions of this work.\n \nW2. The authors should compare their approach against leading methods in point-cloud registration, possibly ones from the above list. By doing so, they can better demonstrate the advantages of their method, be it in terms of accuracy, speed, or other criteria.\n\nW4. The memory overhead of the attention-based method should be addressed, particularly in the context of large point clouds. Providing benchmarks for total memory usage would be beneficial."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7308/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697937613542,
            "cdate": 1697937613542,
            "tmdate": 1699636873192,
            "mdate": 1699636873192,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7DfrDryceJ",
                "forum": "tKu7NNu0Yq",
                "replyto": "3NiluxK9Ro",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7308/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7308/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your time reviewing our paper. We will try to address your concerns regarding related works, the weaknesses you highlighted, and your questions. \n\n\nW1,W2 : We are not solving a point cloud registration task. While we evaluate on 3D point clouds,\nDeepEMD does not assume anything about the input data and does not process 3D point clouds in\nany special way, in particular does not try to \u201cmatch\u201d according to a local appearance similarity.\nThe main goal is to approximate EMD between two distributions which is \u2019similar\u2019 to point cloud\nmatching when working with point clouds as uniform distributions. In principle, DeepEMD can be\nutilized as a fast surrogate for EMD wherever EMD is applicable, including tasks such as single-view\npoint cloud generation or point cloud completion.\n\nW3. As discussed above, there is no inherent limitation in the DeepEMD architecture and we\nonly choose to evaluate on point clouds as they represent uniform probability distribution and\nshape/object based point clouds datasets are easily available (number of samples and reasonable\ncardinality). We believe that the model could generalize well to these other datasets (LiDAR, etc.)\nbut we would need it to scale to larger point clouds (or support sizes/ number of histogram bins).\nFurther, test time generalization for a model trained on shapes might not be accurate enough\nfor very different datasets but we can always train on those new datasets. The novelty in our\nmethod is to regress for the matching/coupling rather than the distance. We believe that this\nground truth supervision with the matching allows the excellent generalisation capabilities to the\nmodel as compared to methods which regress for the distance itself. In particular, in the SetVAE\napplication, the distribution of the generative model changes over iterations, hence, the population\nEMD also changes since one marginal (decoder output) is changing. We observe that the behaviour\nof DeepEMD is excellent during the whole process. This may be due in part to the noise we inject\nto the data samples during training of DeepEMD (explained in Sec. 4.1 and Sec. B.1). Note that\nwe designed this data augmentation scheme before running any SetVAE related application, and\ndid not revisit it, which makes us confident it is actually extremely robust.\n\nW4. The same limitations also arise with other methods like Hungarian, Sinkhorn, EMD, etc.\nWe agree that investigating this line of work will be very useful. Concurrent research on attention\nmodels for increasing context length, reducing attention complexity, etc. can be applied to our\nmethod and bring in the gains seamlessly.\n\nConsidering the strengths you recognized, the \u2019reject\u2019 rating seems disproportionately severe. We\nare committed to enhancing the paper based on your feedback, so if you have any other concerns, we look\nforward to possibly addressing them during the discussion period. If this is not the case, we hope you\nwill be open to changing your final rating, especially considering your acknowledgment of our paper\u2019s\ncontribution and potential usefulness."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7308/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700174412576,
                "cdate": 1700174412576,
                "tmdate": 1700174412576,
                "mdate": 1700174412576,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4RI2ah4HZC",
            "forum": "tKu7NNu0Yq",
            "replyto": "tKu7NNu0Yq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7308/Reviewer_sp92"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7308/Reviewer_sp92"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces DeepEMD, a method for approximating the Earth Mover's Distance (EMD) with a significant speed improvement over traditional EMD calculation methods. The DeepEMD model is based on a multi-head multi-layer transformer, followed by a single-head full attention layer to predict the matching matrix. DeepEMD is evaluated on synthetic and real-world datasets, demonstrating its effectiveness in EMD approximation and gradient estimation. The model shows good generalization capabilities and can be used as a loss function in point cloud autoencoders. The conclusion also discusses potential future research directions, including faster transformer variants, architectural improvements, and extensions to other optimal transport problems."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-\tThe primary strength of this work is its focus on addressing the computational complexity of calculating the Earth Mover's Distance. By introducing DeepEMD, the authors significantly speed up the computation.\n-\tThe results demonstrate that DeepEMD effectively approximates the true EMD, providing strong correlations between predicted and true distances. It also successfully estimates gradients, which is essential for various applications. This makes it a valuable tool for point cloud data analysis and potentially for other domains where EMD is utilized."
                },
                "weaknesses": {
                    "value": "-\tWhile the paper compares DeepEMD with other algorithms, it fails to compare against other state of the art works that compute EMD in terms of speed and accuracy. \n-\tThe authors fail to make a tradeoff between speed and accuracy. \n-\tThe authors talk about several works like CD, DPDist etc. in related works section which are faster than the proposed work. However, authors fail to compare their work with these works."
                },
                "questions": {
                    "value": "-\tHow does this work compare against other recent works in terms of speed and accuracy?\n-\tFigure 8 shows DeepEMD to be performing 100x faster than Hungarian algorithm but has almost half the accuracy against the same algorithm. How much does DeepEMD sacrifice accuracy to achieve speed? Can you provide a more detailed speed vs accuracy tradeoff?\n-\tIs there a specific reason to use only 100 iterations for comparison? Can a better graph be provided that compares the methods over iterations?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7308/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7308/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7308/Reviewer_sp92"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7308/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698739528227,
            "cdate": 1698739528227,
            "tmdate": 1699636873041,
            "mdate": 1699636873041,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dxORiflnDA",
                "forum": "tKu7NNu0Yq",
                "replyto": "4RI2ah4HZC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7308/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7308/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your time reviewing our paper. We will try to address your concerns regarding\nrelated works, the weaknesses you highlighted, and your questions. We hope that it will lead you to reconsider your recommendation, and/or provide further insights for more improvements.\n\nW1. We experimented with several variants of Sinkhorn, including sinkhorn log, sinkhorn stabilized,\ngreenkhorn, etc. However, we discovered that they either exhibited significantly slower performance\ncompared to the vanilla variant, frequently failed to converge to a satisfactory optimal transport\nmatrix within a finite timeframe, or sometimes exhibited both issues.\n\nW2. It is not clear what failing to make a tradeoff between speed and accuracy means here. Further, accuracy is a crude metric - when the model makes a slight assignment mistake for a point assignment, accuracy penalizes it while cosine similarity can still reveal the approximation error. The result on training auto-encoder with DeepEMD as a surrogate metric also emphasises the robustness of its approximation.\n\nW3. We do provide comparisons with CD and for the other related works we provide qualitative\nreasoning behind why DeepEMD is superior.\n\nQ1. We would be more than happy to provide this comparison, but there were no recent works on\nthis task in our knowledge (other than the ones discussed above W1).\n\nQ2. Plese refer to the answer to W2.\n\nQ3. We can observe that Sinkhorn with 5 iterations is more expensive than DeepEMD and\nperforms worse than DeepEMD. While further increasing the number of iteration might lead to a\nbetter convergence but the cost-performance trade-off in this regime is not very relaevant for our\ncomparisons."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7308/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700174305424,
                "cdate": 1700174305424,
                "tmdate": 1700174305424,
                "mdate": 1700174305424,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4fcg3JrIaN",
            "forum": "tKu7NNu0Yq",
            "replyto": "tKu7NNu0Yq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7308/Reviewer_ijdr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7308/Reviewer_ijdr"
            ],
            "content": {
                "summary": {
                    "value": "The paper deals with the problem of computing (distance-based) similarity between 3D point clouds of shapes. The distance metric considered here is the EMD distance (earth mover distance) between 3D point clouds (considered as discrete distributions). \nThe authors have proposed a new approximate approach to compute EMD, which they refer to as DeepEMD. DeepEMD is significantly faster than classical methods (Hungarian algorithm) or the Sinkhorn method that allows an approximate solution to be computed. \nThe authors argue that their DeepEMD computation is efficiently enough that it can be used to compute EMD-based losses during training of other deep architectures. In particular they demonstrate promising results in training a point cloud VAE, by using DeepEMD instead of existing approaches to compute EMD."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper is well written and the general approach of using transformers to compare 3D point clouds and directly predict a matching matrix or the Earth movers distance (EMD) is a sound one. The architecture used by DeepEMD (the transformer-based model) produces accurate results at a fraction of the running time of some of the well-known existing methods."
                },
                "weaknesses": {
                    "value": "The core idea of using transformers to compare pairs of 3D point clouds representing similar or overlapping shapes is not new. See [A, B]. This has been explored in the literature in papers such as these two papers. The transformer architectures in those papers also predict a matching matrix/scores that is then used to compute correspondences to solve a registration task. The proposed architecture is different from the ones in these papers but from what I can tell, the architectures/models in [A, B] appear to be superior as they allow the ability to deal with partial matching, outliers where the work proposed here cannot deal with such scenarios.\n\n[A] RPM-Net: Robust Point Matching using Learned Features, Yew and Lee 2020.\n\n[B] REGTR: End-to-end Point Cloud Correspondences with Transformers, Yew and Lee 2022.\n\nThe authors claim that their proposed approach to compute EMD approximately is significantly faster than existing techniques. They primarily consider two baselines, the well-known Hungarian method which computes EMD exactly and the well-known method that used entropic regularization to compute approximate EMD (Cuturi et al 2013) using matrix scaling / Sinkhorn Knopp algorithm. It is true that the proposed method is significantly faster than the Hungarian method and the Sinkhorn implementation tested here. However, there are a number of works in the literature which are either variants of the Sinkhorn method that have a better convergence than the original Sinkhorn method [C] or have investigated faster algorithms for computing EMD [D, E] that do not require any learning and therefore do not have the issue of overfitting/failure to generalize. A comparison with methods such as [C, D, E] is needed (perhaps there are a few other works in the literature) to make a stronger case for the proposed method.\n\n[C] Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration, Altschuler et al. 2017\n\n[D] A Fast Proximal Point Method for Computing Exact Wasserstein Distance, Xie et al 2019\n\n[E] Fast Sinkhorn I: An O(N) algorithm for the Wasserstein-1 metric, Liao et al 2022.\n\nThe title/abstract is not particularly appropriate. The proposed approach is aimed at computing EMD-based distance between 3D point clouds of geometric shapes. However, the title of the paper makes it sound like a generic method to compute EMD between arbitrary distributions has been proposed. Morever, the approximation factor in the proposed method is somewhat adhoc and not well analyzed, as some of the existing methods in the literature."
                },
                "questions": {
                    "value": "Q1. Some of the implementation details should be clarified. How are the samples from the real datasets (ShapeNet, ModelNet40) used? What does \"In order to improve and assess generalization, we augment the train and test splits with synthetic perturbations.\" (page 6) exactly refer to?\n\nQ2. The sentences \"These quite remarkable behavior .. as shown in Table 2.\" should be toned down a bit. What is the reason for this behavior? Further technical insights into what is being reported would make the claim stronger.\n\nQ3. Why can't the linear-time EMD method of Shirdhonkar and Jacobs (2008) be used in practice. The sentence \"However, their approach is limited to low dimensional histograms.\" needs to be explained further. Couldn't the 3D point clouds have been represented as coarser histograms and compared using their method?\n\nQ4 (comment) the discussion of the method proposed by Amos et al. 2022 is not easy to follow. The distinction / similarity with their work needs to be explained more clearly."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7308/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698800757942,
            "cdate": 1698800757942,
            "tmdate": 1699636872923,
            "mdate": 1699636872923,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GcjLDTnCpG",
                "forum": "tKu7NNu0Yq",
                "replyto": "4fcg3JrIaN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7308/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7308/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Many thanks for your thorough review of our paper. We address the raised concerns, weaknesses and questions below and hope that it will lead you to reconsider your recommendation, and/or provide further insights for more improvements.\n\nW1. RPM-Net [A] solves the point cloud registration task and uses an alignment algorithm\n(Sinkhorn iterations) to soft-align point features from feature extraction network (shared MLP).\nDeepEMD doesn\u2019t solve the same task but instead can be utilized within RPM-Net for soft-\nalignment as an alternative to Sinkhorn iterations. Moreover, it does not have a transformer\narchitecture.\n[B] also solves the point cloud registration task but does not use point matching/alignment.\nThe input point clouds are converted into a smaller set of downsampled keypoints, and then\ncontextualizes the key-point features through cross attention, and finally predicts the rigid trans-\nformation. \n\nAlso DeepEMD is not necessarily limited to 3D point clouds as discussed below in more\ndetail. The concern about outliers is also not applicable here as our goal is to approximate the\noptimal transport map and not point cloud registration. It is not clear what \u2019partial matching\u2019\nimplies here. If it is in regard with handling different cardinalities, DeepEMD can be easily adapted\nto this situation.\n\nW2. We experimented with several variants of Sinkhorn, including sinkhorn log, sinkhorn stabilized,\ngreenkhorn (ref [C], POT provides an implmentation), etc. However, we discovered that they ei-\nther exhibited significantly slower performance compared to the vanilla variant, frequently failed\nto converge to a satisfactory optimal transport matrix within a finite timeframe, or sometimes\nexhibited both issues. Further [D] has a similar complexity as Sinkhorn. Also note that [D] demonstrates that \u201dregularized\nvariations with large regularization parameter will degrade the performance in several important\nmachine learning applications, and small regularization parameter will fail due to numerical sta-\nbility issues with existing algorithms.\u201d which applies to our case for using Sinkhorn and variants\nfor benchmarking. The methods in [E] is limited to the Wasserstein-1 metric and doesn\u2019t generalize for Wasserstein-p.\n\nW3. Although we choose to evaluate on 3D point clouds, note that DeepEMD does not assume\nanything about the input data and does not process 3D point clouds in any special way but works\ndirectly with the raw features. Thus the limitation to 3D objects is not significant.The main goal\nis to approximate EMD between two distributions which is \u2019similar\u2019 to point cloud matching when\nworking withpoint clouds as uniform distributions. In principle, DeepEMD can be utilized as a\nfast surrogatefor EMD wherever EMD is applicable.\n\nQ1. We have provided more details on augmentation in Appendix Section B.1\n\nQ2. An intuitive explanation for such behaviour could be : The prediction of matching requires\nto have reasoning able to match pieces of structures that may be distant, and taking into account\nwhat has already been matched. It is unclear how we could even shape the data to use convnets,\nfor example. Transformers are extremely relevant here because we are dealing with sets without\nany ordering prior information, and we do not know in advance the structural complexity of the\ndistributions (e.g. number of \u201dpieces\u201d, geometric degrees of freedom) and an attention model does\nnot summarize the said structure into a fixed dimension embedding.\n\nQ3. The paper mentions in the abstract \u201dWe present a novel linear time algorithm for approx-\nimating the EMD for low dimensional histograms...\u201d. They provide a discussion in Sec 4.1.4 and\nSec 4.2.\n\nQ4. Amos et al. 2022 presents an amortization based method and the overall approach works in\ntwo stages : 1.) A Meta-OT model (neural network) is trained to predict approximate OT solution.\n2.) The approximate solution is then used to initialize Sinkhorn\u2019s algorithm to further refine and\noutput the final solution. As such it is orthogonal to our work as DeepEMD can be used in stage\n1 for predicting the approximate OT solution. Note that they consider the entropic regularization\nproblem and do not approximate EMD itself. We will be happy to include this in the paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7308/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700174902801,
                "cdate": 1700174902801,
                "tmdate": 1700174902801,
                "mdate": 1700174902801,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XJFiS2KW5I",
                "forum": "tKu7NNu0Yq",
                "replyto": "GcjLDTnCpG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7308/Reviewer_ijdr"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission7308/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission7308/Reviewers",
                    "ICLR.cc/2024/Conference/Submission7308/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7308/Reviewer_ijdr"
                ],
                "content": {
                    "title": {
                        "value": "Thanks, some follow-up questions"
                    },
                    "comment": {
                        "value": "I appreciate your detailed response and clarifications. However, could you clarify your responses to Q3 and Q4 and clearly explain what benefit your proposed method has over those two existing approaches. Please clarify why these methods could not be included in the experimental evaluation? Or is there evidence elsewhere (i.e. in another paper) that shows that such baselines would be inferior to the baselines included in the paper. In that case, please share a reference we can check.\n\nOne more follow-up question. Regarding the argument you presented in paragraph (W2), are the observations regarding the Sinkhorn variants and (D,E) reported somewhere in the paper? For the task at hand, does it matter that (E) is limited to Wasserstein-1? Is the generalization to Wasserstein-p important?\n\nRegarding my earlier comment about 'partial matching', it has to do with the fact that in point-cloud registration it is common situation that 3D points in one point cloud scan are absent (or not visible) in the other scan and vice versa. See the RPM-Net paper figures and description for more details.  Can your method be used to compute the EMD for such data (with partial overlap)?"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7308/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700455216306,
                "cdate": 1700455216306,
                "tmdate": 1700455216306,
                "mdate": 1700455216306,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "I0ttqzfbrD",
            "forum": "tKu7NNu0Yq",
            "replyto": "tKu7NNu0Yq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7308/Reviewer_Fn5r"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7308/Reviewer_Fn5r"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a deep learning based approach DeepEMD for estimating EMD from two point clouds. DeepEMD is much faster then the O(N^3) EMD, and also achieves better performances compared with EMD, CD or Sinkhorn."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed EMD is much more efficient than EMD or Sinkhorn. \n2. Fig. 5 shows that the estimated distance from DeepEMD do not have large errors compared with ground truth EMD."
                },
                "weaknesses": {
                    "value": "1. The evaluation can be improved. All the experiments are conducted at object level. However, the authors do not evaluate DeepEMD under scenes or other types of point clouds. I thinks DeepEMD is limited to the 3D objects since it is only trained with 1024 points under only object-level datasets. A discussion of generality of DeepEMD is needed.\n2. More 3D visualization can improve the representation. The paper do not contain any 3D visualization for reconstruction, generation, etc. This makes the paper lack intuitive qualitative comparison.\n3. It will be much more convincing if the authors adopt DeepEMD as a loss to previous generative models (e.g. single-view point cloud generation, point cloud completion) and report the performance compared to the baseline models."
                },
                "questions": {
                    "value": "1. I think that the argmax in Eq.(10) is not differentiable, how did you solve that?\n2. Will DeepEMD still be efficient with a large point number? \n3. How long dose it take to train DeepEMD, and how much GPU do you use?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7308/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698808736520,
            "cdate": 1698808736520,
            "tmdate": 1699636872810,
            "mdate": 1699636872810,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hqhwyBZXby",
                "forum": "tKu7NNu0Yq",
                "replyto": "I0ttqzfbrD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7308/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7308/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the their valuable feedback and time. We address the main concerns raised below. We hope that it will lead you to reconsider your recommendation, and/or provide further insights for more improvements.\n\nW1. Although we choose to evaluate on 3D point clouds, note that DeepEMD does not assume anything about the input data and does not process 3D point clouds in any special way but works directly with the raw features. Hence, there is no limitation per se of our method to 3D objects.\n\nW2. This is indeed a very good point, such representation will be added to the final version.\n\nW3. We do show that DeepEMD (see Sec 4.4) as a loss works well for training an  auto-encoder as well as a variational auto-encoder. In principle, DeepEMD can be utilized as a fast surrogate for EMD wherever EMD is applicable, including tasks such as single-view point cloud generation or point cloud completion. In addition to good approximation to the EMD and its gradient, we show the most straight-forward evaluation by training  auto-encoders which are completely based on the distance metric and thus appropriate in assessing the quality of DeepEMD as a surrogate metric.\n\nQ1. We only use eq. 10 during inference and thus it doesn't need to be differentiable.\n\nQ2. We do show in Sec 4.3 the test time performance with point clouds as large as 8196 and observe that it performs excellently. With transformer architectures which are either faster or scale up to larger context sizes, we should be able to scale up DeepEMD.\n\nQ3. In general, we typically need to train a single DeepEMD model for a given dataset (note that this step may not be needed given the generalization capabilities of the model). Further, training DeepEMD takes about 4 mins for an epoch of training on a single NVidia RTX 3090 and 16 GB memory with a batch size of 16 (we train for 75 epochs). During inference, it needs 0.8 GB GPU memory. Also note that the inference and training times can be improved by 5-8x with efficient attention implementations, eg. flash attention. When used as a loss, the parameters of DeepEMD are fixed and gradients for the same are not needed, thus its memory footprint is very limited.\n\nWe hope this response has clarified some of your questions, particularly concerning why mesh-based approaches are not suitable candidates for baselines in this case. Please let us know if anything remains unclear, as we would be happy to provide further information if necessary."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7308/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700173910137,
                "cdate": 1700173910137,
                "tmdate": 1700173910137,
                "mdate": 1700173910137,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]