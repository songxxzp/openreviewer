[
    {
        "title": "MagicDrive: Street View Generation with Diverse 3D Geometry Control"
    },
    {
        "review": {
            "id": "BfWtEzzmL7",
            "forum": "sBQwvucduK",
            "replyto": "sBQwvucduK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission153/Reviewer_TY6p"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission153/Reviewer_TY6p"
            ],
            "content": {
                "summary": {
                    "value": "This work focus on street view generation conditioned on BEV layout.\nThe authors harness the power of pretrained stable diffusion model and ControlNet to generate realistic images.\nCross view attention is applied to achieve better multi-view consistency.\nExperiments are conducted on nuscenes to demonstrate superior realism over the baselines.\nThe generative results can serve as data augmentation for downstream task to boost the performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Decent and realistic results. Some images are hard to distinguish unless zoom in.\n2. The overall pipeline is sound to me, and compare to the baselines, it shows improved realism and better multi-camera consistency.\n3. It can boost downstream perception performance.\n4. Clear writing, easy to understand"
                },
                "weaknesses": {
                    "value": "1. The paper\u2019s claim of \u201cgeometry control\u201d appears to be somewhat overstated. Geometry encompasses more than just pose and Box3D; it also includes shape, topology, and the ability to modify any object within the scene.\n2. The consistency aspect of the results is not fully realized. While I acknowledge that the multi-camera consistency is superior to that of the baselines, the broader aspect of consistency, such as consistency from novel viewpoints (e.g., moving cameras away from the original view, rotating it 360 degrees, modifying the focal length to zoom into details in distant regions), seems to be lacking. Based on my observations from the website and deductions from the approach, achieving such consistency with the current representation seems highly unlikely.\n3. The novelty of this work is unclear to me, as I am not very familiar with this topic. Upon a quick review of BEVGen and BEVControl, it appears that the main difference lies in the new modeling of 3D bounding boxes (in this work, the authors decouple the 3D boxes and road maps and model them separately), the use of Stable Diffusion, and cross-view attention. However, none of these elements seem to be significantly innovative."
                },
                "questions": {
                    "value": "Equation 9 is not closed in $||$\n\nWhat if apply the data augmentation to the SOTA camera 3D detection models, can you achieve the new SOTA on nuscenes?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission153/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission153/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission153/Reviewer_TY6p"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission153/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698725029573,
            "cdate": 1698725029573,
            "tmdate": 1699635940683,
            "mdate": 1699635940683,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1Sjlo9RPiz",
                "forum": "sBQwvucduK",
                "replyto": "BfWtEzzmL7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission153/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission153/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer TY6p"
                    },
                    "comment": {
                        "value": "Dear Reviewer TY6p,\n\nWe greatly appreciate your time and insightful feedback. Below is our response to the identified weaknesses and questions.\n\n---\n\n### Weakness 1: the claim of \u201cgeometry control\u201d appears to be somewhat overstated.\n\nThank you for highlighting this concern. To clarify, we have added a footnote on page 2 that specifically defines the scope of \"geometry control\" as considered in this study. We hope this addition provides a clearer understanding of the term.\n\n---\n\n### Weakness 2: the consistency aspect of the results is not fully realized.\n\nMagicDrive, like the MLP in NeRF, uses raw camera pose as an input, enabling potential generalization with a diverse range of camera poses. For instance, by swapping back and front cameras, we have observed generalizations across different FOVs and extrinsic parameters (detailed in Appendix G). However, due to the fixed camera configuration in the nuScenes dataset, which limits variation, our model currently does not generalize to any camera pose. With a more diverse dataset of camera poses, we anticipate enhanced generalization capabilities.\n\n---\n\n### Weakness 3: novelty about the paper compared with BEVGen and BEVControl.\n\nStreet view generation presents unique challenges, distinct from other text/pixel-level conditional generations. This complexity arises not only from the increased number of conditions but also from the unique format and perspective requirements, including the crucial aspect of multi-view consistency. \n\n1. **Difference in handling conditions**: Compared to BEVGen, MagicDrive employs a distinct framework and produces significantly higher-quality data. BEVControl, on the other hand, projects 3D annotations to image view for ControlNet guidance, which loses crucial occlusion relationships and multi-view connections. In contrast, MagicDrive leverages raw 3D bounding boxes and BEV maps for encoding, leading to enhanced controllability, as demonstrated in Figure 5. Our innovative encoding method maintains its novelty and effectiveness even when compared to later developments like DrivingDiffusion [1].\n2. **Enhanced Multi-View Consistency**: Street views have a limited number of cameras and different camera configurations (e.g., exposure). Therefore, the novelty lies in proposing a simple module that effectively solves the problem. Despite BEVGen and BEVControl employing more complex methods, their results are relatively modest compared to MagicDrive (as shown in Figure 5).\n\nGiven that BEVControl was released in August, our work should be viewed as concurrent. Additionally, the effectiveness of our proposed methods is further substantiated by the ablations in Section 6 and Appendix C.\n\n---\n\n### Question 1: typos.\n\nWe thank the reviewer for pointing out the issue. We have fixed the typo.\n\n---\n\n### Question 2: data augmentation for SOTA camera 3D detection models.\n\nOur paper primarily focuses on the controllable generation of high-quality street views. The training on downstream tasks is presented as a proof of concept, demonstrating the potential of synthetic data to enhance 3D perception models. For this purpose, we selected a strong and influential baseline model, BEVFusion, which remains a formidable contender in camera 3D detection when considering single-frame input.\n\nWe acknowledge that achieving new SOTA nuScenes often involves extensive engineering efforts, such as utilizing unannotated data [2-3] and applying test-time augmentation. Therefore, our approach is to compare with a reliable baseline, rather than striving for new SOTA results. This comparison is intended to underscore the effectiveness of our method in a realistic and pragmatic context.\n\n---\n\n[1] X. Li, Y. Zhang and X. Ye. DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model. arXiv preprint arXiv:2310.07771, 2023.\n\n[2] X. Lin, T. Lin, Z. Pei, L. Huang and Z. Su. Sparse4D v2: Recurrent Temporal Fusion with Sparse Model, 2023.\n\n[3] Z. Zong, D. Jiang, G. Song, Z. Xue, J. Su, H. Li and Y. Liu. Temporal Enhanced Training of Multi-view 3D Object Detector via Historical Object Prediction, 2023."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700107862152,
                "cdate": 1700107862152,
                "tmdate": 1700107862152,
                "mdate": 1700107862152,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "go0meXlWEb",
            "forum": "sBQwvucduK",
            "replyto": "sBQwvucduK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission153/Reviewer_UU4E"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission153/Reviewer_UU4E"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel framework for generating street view imagery with diverse 3D geometry controls such as camera poses, road maps, and 3D bounding boxes, using tailored encoding strategies. Existing methods primarily focus on 2D control which limits their utility in 3D perception tasks essential for autonomous driving. This paper consider a BEV view input, and input these control through encoding each of the information and insert these in the cross attention inside the diffusion UNet. In order to ensure the consistency between different views. It also introduce the cross-view attention for the training."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Adopting diffusion for driving view synthese which trying to solve the data limitation in corner cases for self-driving is important. \n\n- The overall strategy is sound and the paper proposed reasonable ways to encoding different information. Including Scene-level Encoding,  3D Bounding Box Encoding,  Road Map Encoding, these encoding are well organized and normalized in inserting to the cross attention module.  It also enables the final multi-level control of the generation. \n\n- Other modules such as cross-view module help in image synthesis consistency. \n\n- The experiments, show that it outperforms the other baselines such as BEVGen and BEV-Control, for synthesizing multi-camera views."
                },
                "weaknesses": {
                    "value": "- the synthesized views are impressive,  the experiments are conducted in 700 street-view scenes for training and 150 for validation, which is a much smaller scale than the real-world senario. Wonder how to possiblly make it generalizable for real world domain. Does this be helpful to improve the detection & other understanding tasks when the data is large.\n\n- In addition, not only for the dark scene, many generated instances such as human can be distorted with diffusion models. Wonder how that affects the detection accuracy for each subclass. The author provides overall accuracy in 3D object detection, may also analysis the details how to mix the synthesized images and real-images for training the model."
                },
                "questions": {
                    "value": "Diffusion models are costly, Could the framework be extended or modified to handle real-time or near real-time massive generation requirements, which are crucial for applications in autonomous driving systems? This also related to handle dynamic entities in the scene such as moving vehicles or pedestrians, especially when synthesizing views over a period of time?\n\nHow closely does the synthetic data correlate with real-world data and what measures were taken to ensure the accuracy and reliability of this synthetic data?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission153/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698773339714,
            "cdate": 1698773339714,
            "tmdate": 1699635940615,
            "mdate": 1699635940615,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8BNo6ZMbQO",
                "forum": "sBQwvucduK",
                "replyto": "go0meXlWEb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission153/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission153/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer UU4E (1/2)"
                    },
                    "comment": {
                        "value": "Dear Reviewer UU4E,\n\nWe appreciate your time and valuable insights. Below is our response to the weaknesses and questions.\n\n---\n### Weakness 1: limited scale of the dataset used in experiments and the generalizability of MagicDive to real-world scenarios. \nNuScenes, a well-established real-world dataset for autonomous driving, forms the basis of our training, allowing MagicDrive to demonstrate strong generalization across various scenarios, including unseen objects' positions and maps. This adaptability is evident from our tests involving manual modifications of object boxes (rotation, deletion, and moving), as detailed in Figure 6. While the current dataset scale is smaller than real-world scenarios, our results already show promising generalization capabilities. Scaling up the dataset size would likely present a broader spectrum of real-world scenarios, enhancing the model's performance even further. It is expected that such an expansion would significantly enhance MagicDrive's utility in downstream perception tasks.\n\n---\n### Weakness 2: effects of distortion in generated images on the detection accuracy for each subclass and details on how synthetic images are integrated with real images.\n\nThank you for your insightful comments. In response, we have expanded Appendix E to include a subclass-specific analysis of detection accuracy. This analysis demonstrates that the integration of synthetic data, even with instances of distortion, positively impacts the detection performance of each subclass. For instance, pedestrian detection shows a notable improvement of +2.36 mAP. This suggests that despite some distortions, the high-quality rendering of objects, particularly those close to the ego car, substantially aids in enhancing perception model accuracy.\n\nIn our experiments, we generate an equal number of samples as the training set for data augmentation in BEV segmentation. For 3D object detection, we randomly delete 50% of objects in each scene for generation, as detailed in Sec. 5.2. Exploring various strategies for integrating real and synthetic data is indeed a worthy endeavor. While our current focus is on advancing high-quality street view generation, we acknowledge the importance of this area and leave its detailed exploration to future research.\n\n---\n### Question 1: feasibility of adapting MagicDrive for real-time generation and how it manages dynamic entities over time.\n\n1. **Regarding Real-time Generation.** Our current model is optimized more for offline applications, such as simulation and data augmentation, with a focus on realism and controllability rather than low latency. However, we are aware of the importance of real-time processing in autonomous driving. The generation time in our current setup is approximately 3 seconds for 6 views using a single V100 card. Although this is not yet real-time, advancements in inference acceleration (e.g., advanced sampling techniques, model distillation methods like the Latent Consistency Model [1], or engineering solutions like low-bit inference [2] and flash attention [3]) are promising areas for reducing this latency. These potential improvements are areas we plan to explore in future research to make our model more applicable to real-time scenarios.\n2. **Handling Dynamic Entities.** To address the dynamic nature of autonomous driving scenes, we are exploring the integration of video generation techniques, such as tune-a-video [4], into MagicDrive. This would allow the model to account for moving entities over time. Preliminary results from these explorations show promise, and we have included a demonstration video in Appendix F and on our [anonymous website](https://magic-drive.github.io/). This indicates that adapting MagicDrive to dynamically changing environments is feasible."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700107550844,
                "cdate": 1700107550844,
                "tmdate": 1700108003891,
                "mdate": 1700108003891,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IINcdFVyM7",
            "forum": "sBQwvucduK",
            "replyto": "sBQwvucduK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission153/Reviewer_jXFZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission153/Reviewer_jXFZ"
            ],
            "content": {
                "summary": {
                    "value": "The goal of this paper is to have multi-modality control on street scene generation process. Overall idea of this paper is to use ControlNet framework on top of pre-trained stable diffusion model to support conditional generation over street view dataset. The complexity comes in terms of how to design multi-modality conditions for the ControlNet condition. For this purpose, the authors introduce various cross attention over their conditions to fuse their conditions onto scene representation (they also feed non-scene related condition directly to stable diffusion). The training follows ControlNet paradigm with classifier-free guidance to encourage output more aligned with conditions. Result-wise, they compare with BEVGen and BEVControl on nuScenes. The experiment aims to reveal they produce more realistic images and have better control over output space for street view generation task."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is very nicely organized and written. \n2. The quality of the generated street view is realistic\n3. We can see MagicDrive has more precise control on street generation than baselines"
                },
                "weaknesses": {
                    "value": "1. The main concern is for their marginal technical contribution. The proposed method is ControlNet applied into street view generation setting with multi-modality condition. The novelty probably lies in how to organize the condition into controlNet setting, which might not sufficient for acceptance. \n2. MagicDrive does not ensure consistency across adjacent frames after checking their website demo."
                },
                "questions": {
                    "value": "1. Do you have different CFG weights for different conditions? If so, I am curious on how you make that work."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission153/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission153/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission153/Reviewer_jXFZ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission153/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698799165919,
            "cdate": 1698799165919,
            "tmdate": 1699635940516,
            "mdate": 1699635940516,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WpUVaRFCSY",
                "forum": "sBQwvucduK",
                "replyto": "IINcdFVyM7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission153/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission153/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jXFZ"
                    },
                    "comment": {
                        "value": "Dear Reviewer jXFZ,\n\nWe thank you for pointing out that \"*The quality of the generated street view is realistic*\" and \"*We can see MagicDrive has more precise control on street generation than baselines*\" as strengths. Here we provide our response to weaknesses and questions.\n\n---\n### Weaknesses 1: technical contributions compared to ControlNet\n\nStreet view generation presents unique challenges, distinct from other conditional methods such as ControlNet. This complexity arises not only from the increased number of conditions but also from the unique format and perspective requirements, including the crucial aspect of multi-view consistency. Our technical contributions, detailed in Sec. 1, are summarized as follows:\n\n1. **Utilization of Raw 3D Data in MagicDrive**: Unlike other works, MagicDrive is the first to employ raw BEV maps and objects' bounding boxes & camera pose with 3D coordinates for street view generation. This approach differs significantly from ControlNet for two main reasons:\n    - **Interactive Conditions**: The conditions are interdependent. For instance, the relevance of 3D bounding boxes is contingent on the camera pose, and the terrain height in BEV maps is inferred from the objects' positional heights.\n    - **Varying Coordinates/Perspectives**: We handle conditions presented in different coordinates and perspectives, such as BEV maps in BEV perspective and 3D objects and camera pose in LiDAR coordinates. All of them are distinct from the pixel-level conditions in ControlNet.\n    \n    Actually, we only adapt ControlNet's method for map condition injection, and propose our own encoders for other conditions. Note that MagicDrive is the first to show that ControlNet is capable of encoding conditions from different perspectives. Besides, our tailored encoding of the box and camera, as evidenced in Sec. 6 and Table 4, is crucial for performance and multi-view consistency.\n\n2. **Innovative Cross-View Attention Module**: To address the limited overlap and varying characteristics of each camera view (e.g., intrinsic differences, and exposure variation as shown in Figure 5), we developed a simple yet effective cross-view attention module. This module ensures consistency across views, a feature not addressed by ControlNet. As demonstrated in Figure 5, MagicDrive outperforms BEVGen and BEVControl in achieving this consistency, offering a more effective solution.\n\n---\n### Weaknesses 2: consistency across adjacent frames.\nIt could be relatively easy to implement temporal consistency for video generation with MagicDrive. For instance, we applied the concept from 'tune-a-video' [1] to modify a low-resolution version of our MagicDrive, and the results are presented in both Appendix F and on our [anonymous website](https://magic-drive.github.io/). Since this extension is straightforward, it is not highlighted as a major contribution in our paper.\n\n---\n### Question 1: do you have different CFG weights for different conditions?\nNo. We consider whether CFG for map condition exists with others or not, and different weights (scales) for CFG, as shown in Figure 7. If exist, they share the same CFG weight.\n\n---\n\n[1] J. Z. Wu, Y. Ge, X. Wang, S. W. Lei, Y. Gu, Y. Shi, W. Hsu, Y. Shan, X. Qie, and M. Z. Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. ICCV, 2023."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700107375864,
                "cdate": 1700107375864,
                "tmdate": 1700107979301,
                "mdate": 1700107979301,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xGwpFICqc1",
                "forum": "sBQwvucduK",
                "replyto": "WpUVaRFCSY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission153/Reviewer_jXFZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission153/Reviewer_jXFZ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the feedback! I will remain my rating due to novelty concern."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707720715,
                "cdate": 1700707720715,
                "tmdate": 1700707720715,
                "mdate": 1700707720715,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8JIYNZFcIL",
                "forum": "sBQwvucduK",
                "replyto": "IINcdFVyM7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission153/Reviewer_jXFZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission153/Reviewer_jXFZ"
                ],
                "content": {
                    "comment": {
                        "value": "Sorry, please do not ignore my comments. I don't agree with the argument that MagicDrive clearly demonstrates superior innovation compared to ControlNet. On one hand, I appreciate the delivered quality of this paper. But I still question the novelty since it is rather minor in my opinion on top of ControlNet framework with additional adaptation of conditions for one specific task.\n\nAs for cross-view consistency, correct me if I misunderstood the result, isn't that clear that it is not consistent from the website demo (image sequence for continuous scenes)?"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713141076,
                "cdate": 1700713141076,
                "tmdate": 1700713536089,
                "mdate": 1700713536089,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ay12tbyXuD",
                "forum": "sBQwvucduK",
                "replyto": "IINcdFVyM7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission153/Reviewer_jXFZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission153/Reviewer_jXFZ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for follow-up with additional clarification. With regards to \"cross-view consistency\", in [UPPER] Generated video 1 from MagicDrive. [LOWER] Ground truth for video 1, it is not consistent, isn't it? The street window changes across frames, right? I am not sure it makes sense to evaluate on \"temporal consistency\" for this task -- street view generation.\n\nThanks again for author's detailed and clear clarification! My rating is final. Don't get me wrong, I do believe this is a great paper, but I kinda expect more on the claimed contribution."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719925826,
                "cdate": 1700719925826,
                "tmdate": 1700720602226,
                "mdate": 1700720602226,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "anGYCnTWqM",
            "forum": "sBQwvucduK",
            "replyto": "sBQwvucduK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission153/Reviewer_dbzg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission153/Reviewer_dbzg"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes MagicDrive - a Bird's-eye-view(BEV)-to-street-view image generation method. Given a BEV road map, 3D bounding boxes for objects, the camera pose, and an input prompt it generates a consistent, multi-view image set for autonomous driving purposes. It is capable of scene, background and foreground control by prompting - lighting conditions, weather, object orientation, object deletion are available. \n\nThe main paper contributions are the a view-consistent image generation and 3D bounding box encoding for objects, as opposed to previous approaches that used only the BEV map.\n\nThe algorithm yields favorable visual results compared to similar methods (15-16 FID vs 20+) and the augmented data it generates improves upon the BEVFormer 3D object detection (\\~+2mAP, depending on input modality) and Cross View Transformer vehicle and road mIoU (\\~4-5%) on the nuScenes validation dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- consistent cross-camera image generation\n    - a cross-view attention model with neighboring views \n- better problem modelling compared to older methods\n    - 3D object bounding box and camera pose allows a wider array of edits and more accurate terrain representation\n    - prompting the diffusion model allows for more diverse output images"
                },
                "weaknesses": {
                    "value": "## Summary ##\nA view-consistent UNet generation method and bounding box inputs for a controlNet BEV-to-RGB are the main contributions. Apart from the benefit of encoding bounding boxes, unclear whether the chosen consistency method is ideal.\n## Details ##\n\n- engineering work / limited contributions\n    - ControlNet stable diffusion pipeline coupled with a multi-view conditional UNet  \n        - there are other consistency methods - inpainting, panorama input, feature volumes - why is this cross-view attention module the best choice?\n- limited comparisons, different baseline numbers\n    - the authors use BEVFormer for some comparisons and CVT for others \n        -  for BEVFormer the reported numbers are significantly lower compared to the original paper and I don't believe it's only the resolution; no numbers match\n- method not mature enough\n    - to the best of my knowledge, neither of the two baselines (BEVGen/BEVControl) have been accepted at a major conference; furthermore, MagicDrive disregards other practical considerations such as temporally-consistent frames [1*]\n___\n[1*] DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model. arXiv preprint arXiv:2310.07771."
                },
                "questions": {
                    "value": "1. Why are the BEVFusion numbers much lower? Why not use BEVFusion for the BEV segmentation as well?\n2. If the aim is just to generate novel views, why not add additional elements to the bounding box images and use controlNet image encoding? See [1*] for inspiration.\n3. If the data augmentation strategy works so well, why not start with a state-of-the art method such as [2*] and see what it can be improved from there?\n4. Why not present other methods for consistent view generation? Arguably [4*] deals with the same problem; the scope is different, but they also have reasonable depth maps.\n5. The method is heavily reliant on nuScenes; how would you consider improving generalization?\n\n___\n[1*]Li, X., Zhang, Y., & Ye, X. (2023). DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model. arXiv preprint arXiv:2310.07771.\n[2*] Hu, H., Wang, F., Su, J., Hu, L., Feng, T., Zhang, Z., & Zhang, W. EA-BEV: Edge-aware Bird\u2019s-Eye-View Projector for 3D Object Detection.\n[3*]H\u00f6llein, L., Cao, A., Owens, A., Johnson, J., & Nie\u00dfner, M. (2023). Text2room: Extracting textured 3d meshes from 2d text-to-image models. arXiv preprint arXiv:2303.11989. https://github.com/lukasHoel/text2room\n[4*]Bahmani, S., Park, J. J., Paschalidou, D., Yan, X., Wetzstein, G., Guibas, L., & Tagliasacchi, A. (2023). Cc3d: Layout-conditioned generation of compositional 3d scenes. arXiv preprint arXiv:2303.12074. https://github.com/sherwinbahmani/cc3d"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission153/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698858081399,
            "cdate": 1698858081399,
            "tmdate": 1699635940423,
            "mdate": 1699635940423,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4wPoFon9ke",
                "forum": "sBQwvucduK",
                "replyto": "anGYCnTWqM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission153/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission153/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dbzg (1/3)"
                    },
                    "comment": {
                        "value": "Dear Reviewer dbzg,\n\nWe thank you for highlighting our work's strength in \"*better problem modeling compared to older methods.*\" Indeed, this aspect is also one of the major novelties of our paper. Here, we present a point-by-point response to the weaknesses proposed by the reviewer.\n\n---\n### Weakness 1: novelty of the proposed method, especially the difference between ControlNet, and the design of multi-view consistency module.\n\n1. **Differences between ControlNet and MagicDrive.**\n\n   **Firstly, our task diverges significantly from ControlNet's scope**. The complexity of street view generation mandates considering multiple, interdependent conditions (e.g., BEV map, 3D object parameters, and camera pose). For example, given bounding boxes with 3D coordinates, only if the camera pose is presented can determine which box to appear in the view; given maps in BEV, the height of terrain should be reasoning from the height of objects' positions. This multi-faceted approach transcends ControlNet's simpler image-space controls by necessitating intricate interactions and perspective-aware condition handling. \n\n   **Secondly, our approach distinctively extends ControlNet by integrating a diverse set of conditions.** Our conditions differ substantially from standard images. We demonstrate in Sec. 6 and Table 4 (see w/o $E_{box}$ example) that our tailored encoding mechanisms are crucial for achieving high-quality results, especially in maintaining multi-view consistency.\n\n2. **Challenges and non-trivial design in achieving multi-view consistency.** Implementing cross-view consistency in street view generation presents unique challenges. Unlike indoor settings, each street view has limited overlap and varies significantly in camera characteristics (e.g., intrinsic properties, exposure), as exemplified in Figure 5. Our MagicDrive notably outperforms existing methods like BEVGen and BEVControl in achieving more effective consistency, as demonstrated in the comparative analysis in Figure 5.\n\n3. **Reasons for choosing cross-view attention over alternatives.** Other consistency methods, such as inpainting or panorama inputs, typically assume significant overlaps and uniform view characteristics, which are not feasible in street view scenarios. Our cross-view attention module, being straightforward yet highly effective, addresses these limitations by accommodating the disparate nature of street views, thereby surpassing the performance of other baseline methods.\n\n---\n### Weakness 2: limited comparisons with baselines and different baseline numbers.\n\n1. **Reasons for using BEVFusion\\* and CVT as baselines.** In the evaluation, we consider CVT for BEV segmentation because it is used in both BEVGen and BEVControl. We consider BEVFusion because it is one of the most influential works in 3D BEV object detection, which is also referenced as one of the baselines in DrivingDiffusion [1].\n\n\\* We think BEVFormer in the comments is a typo of BEVFusion.\n\n2. **Why the reported numbers of BEVFusion are significantly lower compared to the original paper?** We have updated the number in Table 2 and Table 6 by enabling `test_mode` as described in [issue \\#411](https://github.com/mit-han-lab/bevfusion/issues/411), which we did not notice when submitting our paper. Taking Table 6 \"CAM-Only 2x\" as an example, BEVFusion reports 35.56 mAP, while our reproduced mAP is 35.49. The mAP with data from MagicDrive is 35.74, improving the mAP by 0.25."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700107007782,
                "cdate": 1700107007782,
                "tmdate": 1700107920939,
                "mdate": 1700107920939,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PQ5hEZMKgQ",
                "forum": "sBQwvucduK",
                "replyto": "anGYCnTWqM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission153/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission153/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dbzg (2/3)"
                    },
                    "comment": {
                        "value": "### Weakness 3: the practicality of MagicDrive.\n\n1. **Problem settings and practical applications.** Street-view data generation is a promising way to improve the perception capabilities of autonomous vehicles. The key challenges of this task include achieving controllability and ensuring multi-view consistency. Our approach addresses these problems effectively. Additionally, we utilize nuScenes, a widely recognized real-world dataset for autonomous driving, to demonstrate the realism of our generated street views. The practicality of our method is further substantiated by evaluating its performance on two different perception tasks, both in training and testing environments. This underscores the relevance and applicability of our settings to real-world scenarios.\n\n2. **Reasons for choosing BEVGen and BEVControl as baselines.** At the time of our submission, these baselines represented the forefront of using controllable generation for street view synthesis, addressing the challenge of multi-view consistency. Our comparison demonstrates that MagicDrive significantly surpasses these baselines in generation quality. Moreover, our experiments using the CVT show substantial improvements over BEVGen, highlighting the advancement our approach brings to the field. We do not apply DrivingDiffusion as a baseline since it was released on arXiv after the ICLR submission deadline.\n\n3. **Extending MagicDrive to achieve temporal consistency.** Ensuring temporal consistency in the generation of MagicDrive is a relatively straightforward process. To illustrate, we have implemented a technique inspired by the method from tune-a-video[2], applying it to fine-tune a low-resolution version of MagicDrive. The preliminary results of this enhancement are detailed in Appendix F, and a demonstrative video is available on our [anonymous website](https://magic-drive.github.io/). These demonstrate that MagicDrive is not only scalable but also serves as a robust foundational model for controllable street-view generation.\n\n---\n### Question 1: experiments of BEVFusion.\n\n1. **Why are the BEVFusion numbers much lower?** Sorry for the confusion caused. We have updated the results in Table 2 and Table 6 by enabling `test_mode` as described in [issue \\#411](https://github.com/mit-han-lab/bevfusion/issues/411) in the GitHub of BEVFunsion, which we did not notice when submitting our paper. The updated results are close to those reported in the paper of BEVFusion. Still, MagicDrive improves the performance of 3D object detection.\n\n2. **Why not use BEVFusion for BEV segmentation?** In fact, we did employ BEVFusion for BEV segmentation. However, due to the absence of an established baseline for comparison, we initially refrained from presenting these results in our submission. To address this, we have now included the BEVFusion-tested results for BEV segmentation on the nuScenes validation set in Appendix F\uff0e\n\n---\n### Question 2: advantages of our methods compared with projecting 3D bounding boxes onto the image views like DrivingDiffusion.\n**First, for 3D object encoding**, as illustrated in Figure 2 of Sec. 1, it is crucial for accurate 3D control. After projection from 3D to 2D, the occlusion relationship and connection between views, for example, is unclear. Please see the visualization in Figure 13 for further understanding. It will be much clearer if 3D positions are available. **Second, for raw BEV maps**, as demonstrated in the last paragraph of Sec. 4.1, projecting a map from BEV to FPV without height information is an ill-posed problem. Therefore, there is no guarantee that this projection will not affect fidelity.\n\n---\n### Question 3: why not start with a SOTA perception method for data augmentation? \nOur choice of BEVFusion as the baseline is motivated by its significant influence and robust performance in the field. The primary focus of our method is to explore the impact of generating high-quality street-view data, rather than to establish new benchmarks in downstream perception tasks. The improvements we have demonstrated in these tasks serve as a proof of concept for the effectiveness of our approach. In terms of generation capabilities, our method surpasses existing baselines. Furthermore, while EA-BEV utilizes 1600x900 resolution images, replicating this in our training and generation processes is challenging due to limitations in resources and time. \n\n---\n### Question 4: why not present other methods for consistent view generation?\n\nPlease refer to the response to Weakness 1, where we stated several key differences between street view generation and other settings that request view consistency. Additionally, we note that mentioned CC3D [3] offers street view generation on their website, but the quality is inferior compared to ours."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700107061944,
                "cdate": 1700107061944,
                "tmdate": 1700107949475,
                "mdate": 1700107949475,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FM508O2snp",
                "forum": "sBQwvucduK",
                "replyto": "anGYCnTWqM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission153/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission153/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dbzg (3/3)"
                    },
                    "comment": {
                        "value": "### Question 5: reasons for choosing nuScenes and the generalization ability of our method. \nNote that nuScenes is a typical autonomous driving dataset featuring multiple diverse driving views. Therefore, we chose it as our testbed. Our method is based on stable diffusion, pre-trained with billions of images, demonstrating strong generalization ability in variant applications. Given a larger and more diverse dataset, we anticipate that MagicDrive would exhibit enhanced generalization in generating novel street views.\n\n---\n\n[1] X. Li, Y. Zhang and X. Ye. DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model. arXiv preprint arXiv:2310.07771, 2023.\n\n[2] J. Z. Wu, Y. Ge, X. Wang, S. W. Lei, Y. Gu, Y. Shi, W. Hsu, Y. Shan, X. Qie, and M. Z. Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. ICCV. 2023.\n\n[3] S. Bahmani, J. J. Park, D. Paschalidou, X. Yan, G. Wetzstein, L. Guibas and A. Tagliasacchi. CC3D: Layout-conditioned generation of compositional 3d scenes. arXiv preprint arXiv:2303.12074. 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission153/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700107102794,
                "cdate": 1700107102794,
                "tmdate": 1700107621816,
                "mdate": 1700107621816,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]