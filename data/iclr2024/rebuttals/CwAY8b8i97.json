[
    {
        "title": "Spike Accumulation Forwarding for Effective Training of Spiking Neural Networks"
    },
    {
        "review": {
            "id": "cbHCqV9Qei",
            "forum": "CwAY8b8i97",
            "replyto": "CwAY8b8i97",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5588/Reviewer_q39D"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5588/Reviewer_q39D"
            ],
            "content": {
                "summary": {
                    "value": "This article introduces a new method for training Spiking Neural Networks (SNNs) known as Spike Accumulation Forwarding (SAF). The authors conduct theoretical analysis and experimental comparisons to establish the equivalence of SAF with other methods, such as OTTT, while evaluating its performance and efficiency. Experimental data demonstrates that SAF can significantly reduce training time and memory usage with little to no loss in accuracy. Additionally, it is shown that SAF-trained parameters can be used for inference in SNNs composed of LIF neurons."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The article introduces an innovative method, SAF, for training SNNs, and its effectiveness is demonstrated through both theoretical analysis and experiments."
                },
                "weaknesses": {
                    "value": "1. The relatively simple dataset used in the article (CIFAR-10) may limit the understanding of the method's applicability to more complex datasets.\n2. The Figure 2 in the experimental section is relatively blurry.\n3. The memory compression in Table 1 is also somewhat limited. Are there better results available to demonstrate the effectiveness of the method?\n4. The experimental section lacks specific details about the SNN model, and it would be beneficial to test the method on different models."
                },
                "questions": {
                    "value": "Do the authors have plans to apply the SAF method to other types of neural networks or larger datasets? Are there plans for more experiments to validate SAF's performance in different scenarios?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5588/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698132775463,
            "cdate": 1698132775463,
            "tmdate": 1699636575865,
            "mdate": 1699636575865,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PH7HKO4OGE",
                "forum": "CwAY8b8i97",
                "replyto": "cbHCqV9Qei",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5588/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5588/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer q39D"
                    },
                    "comment": {
                        "value": "Thank you for your review comments. We reply to the weak points and answer your questions. For clarity, we have addressed each of your comments individually in the following responses:\n\n**---1.The relatively simple dataset used in the article (CIFAR-10) may limit the understanding of the method's applicability to more complex datasets.  \n---4.The experimental section would be beneficial to test the method on different models.  \n---Do the authors have plans to apply the SAF method to other types of neural networks or larger datasets? Are there plans for more experiments to validate SAF's performance in different scenarios?**  \nIn Section 5, our main objective is to ensure that there are no inconsistencies between our theory and experiment. Our theory suggests that SAF-E is equivalent to OTTT$_ {\\\\rm O}$, and SAF-F is equivalent to spike representation. These equivalences can be confirmed through the current experiments. Additionally, it is worth noting that OTTT and Spike Representation, as mentioned in Xiao et al. [2022], can be applied to more complex datasets. Given that our methods are in full theoretical agreement with these approaches, this fact clearly supports the scalability of our method. Therefore, it is evident that scalability can be demonstrated without the large and complex experiments. We are willing to add it to our paper if necessary.\n\n**---2.The Figure 2 in the experimental section is relatively blurry.**  \nThe blurriness in Figure 2 is indeed a result of the overlap of lines, which is expected because SAF-E and OTTT$_ {\\\\rm O}$ are theoretically equivalent. Conversely, in Figure 4, the non-overlapping lines representing SAF-F and OTTT$_ {\\\\rm A}$ indicate that they are not equivalent. This result also aligns with our theoretical predictions.\n\n\n**---3.The memory compression in Table 1 is also somewhat limited. Are there better results available to demonstrate the effectiveness of the method?**  \nCompared to OTTT, SAF achieved a notable reduction of 30\\% in memory usage. Similarly, the training time was also reduced by the same percentage, at least. These results are not only significant but also demonstrating substantial differences between the two methods.\n\n**---4.The experimental section lacks specific details about the SNN model,**    \nThe details of the model used and the training setup can be found in Appendix C, and the source code is available in the Supplementary material. It is important to note that all setups were consistent with the experiments conducted by Xiao et al. [2022]."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5588/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203899050,
                "cdate": 1700203899050,
                "tmdate": 1700204196530,
                "mdate": 1700204196530,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aJxpwvhWdA",
                "forum": "CwAY8b8i97",
                "replyto": "PH7HKO4OGE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5588/Reviewer_q39D"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5588/Reviewer_q39D"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the clarification"
                    },
                    "comment": {
                        "value": "I still have concerns about large-scale datasets. I am wondering if the proposed method applies to more complex datasets."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5588/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710007614,
                "cdate": 1700710007614,
                "tmdate": 1700710007614,
                "mdate": 1700710007614,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "b8Hpp0H0Uc",
            "forum": "CwAY8b8i97",
            "replyto": "CwAY8b8i97",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5588/Reviewer_owpx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5588/Reviewer_owpx"
            ],
            "content": {
                "summary": {
                    "value": "Spike accumulation forwarding (SAF) is a method that reduces memory requirements and training time for SNNs by accumulating spikes over multiple time steps. This approach is consistent with Spike Representation and online training through time methods. The authors tested SAF's effectiveness on the CIFAR-10 dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors tried to solve an important challenge with training SNNs, namely the need for large amounts of memory and long training times. And the only small improvement I see is in memory and training times."
                },
                "weaknesses": {
                    "value": "The authors didn\u2019t compare SAF to BPTT and other successful training methods, requiring a comprehensive understanding.\n\nThey conducted experiments on a single set of hypermeters, which lacks further exploration of the robustness of the methods. Understanding the effects of various hyperparameters on SAF's performance and how to tune them for various applications would be improved by a more thorough analysis.\n\nThe authors have not provided a detailed analysis of the computational complexity of SAF. The authors have empirically demonstrated that SAF reduces memory needs and training time; however, they haven't done a thorough investigation of the computational complexity of the approach. The experiment is currently based on GPU architecture. It would be easier to comprehend the computational resources needed to execute SAF and its potential for real-world applications with a more thorough analysis.\n\nThe theoretical results only show that the proposed method has the same capabilities as OTTT and SpikeRepresentation, and no new guarantee of training is presented there. Hence, the contribution is reduced to find an efficient forward method that fits the training models of OTTT and SpikeRepresentation."
                },
                "questions": {
                    "value": "Can you provide detailed explanations of how much computational overhead is needed when training? What kind of computation is included (float-point computation or spike computation)? I am interested in this because the training, unlike other online training, is based on float-point representation.\n\nRegarding the robustness of the training algorithm, can you provide some results that have different settings of $\\lambda$ and $T$.\n\nAlso, the current dataset setting is lacking. Only CIFAR-10 experiments are conducted. What about CIFAR-100 and ImageNet?\n\nOverall, I am more inclined to reject this paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5588/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698222625278,
            "cdate": 1698222625278,
            "tmdate": 1699636575751,
            "mdate": 1699636575751,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZRz9rw7qWQ",
                "forum": "CwAY8b8i97",
                "replyto": "b8Hpp0H0Uc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5588/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5588/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer owpx 1/2"
                    },
                    "comment": {
                        "value": "Thank you for your review comments. We reply to the weak points and answer your questions. For clarity, we have addressed each of your comments individually in the following responses:\n\n**---The authors didn\u2019t compare SAF to BPTT and other successful training methods, requiring a comprehensive understanding.**  \nThe primary objective of this paper is indeed to establish the theoretical consistency between SAF-E and OTTT$_{\\rm O}$, as well as SAF-F and spike representation. Therefore, comparisons with other methods are beyond the scope of the paper's claims. It is worth noting that previous studies have already demonstrated the superior performance of OTTT over BPTT and other methods.\n\n**---They conducted experiments on a single set of hypermeters, which lacks further exploration of the robustness of the methods. Understanding the effects of various hyperparameters on SAF's performance and how to tune them for various applications would be improved by a more thorough analysis.  \n---Regarding the robustness of the training algorithm, can you provide some results that have different settings of $\\mathbf{\\lambda}$ and $\\mathbf{T}$.**  \nAs mentioned earlier, the paper indeed provides a theoretical guarantee of consistency between SAF-E and OTTT$_{\\rm O}$, as well as SAF-F and spike representations. Additionally, it is worth noting that the hyperparameters of SAF are exactly the same as the hyperparameters of OTTT. This implies that conducting a hyperparameter search will yield identical accuracies to OTTT.  However, it is important to note that time and memory usage are not explicitly addressed in the theoretical analysis. To provide a more comprehensive understanding of the practical implications, Figures 3 (A, B) and 5 (A, B) present the training time and memory usage when varying $T$, as it has the most significant impact on these indicators. This allows for a more complete evaluation of the performance of the methods beyond accuracy alone.\n\n**---Also, the current dataset setting is lacking. Only CIFAR-10 experiments are conducted. What about CIFAR-100 and ImageNet?**  \nIn Section 5, our main objective is to ensure that there are no inconsistencies between our theory and experiment. Our theory suggests that SAF-E is equivalent to OTTT$_{\\rm O}$, and SAF-F is equivalent to spike representation. These equivalences can be confirmed through the current experiments. Additionally, it is worth noting that OTTT and Spike Representation, as mentioned in Xiao et al. [2022], can be applied to more complex datasets. Given that our methods are in full theoretical agreement with these approaches, this fact clearly supports the scalability of our method. Therefore, it is evident that scalability can be demonstrated without the large and complex experiments. We are willing to add it to our paper if necessary."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5588/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700204459573,
                "cdate": 1700204459573,
                "tmdate": 1700204459573,
                "mdate": 1700204459573,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "D0Skh6bPgU",
                "forum": "CwAY8b8i97",
                "replyto": "b8Hpp0H0Uc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5588/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5588/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Respose to Reviewer owpx 2/2"
                    },
                    "comment": {
                        "value": "**---The authors have not provided a detailed analysis of the computational complexity of SAF. The authors have empirically demonstrated that SAF reduces memory needs and training time; however, they haven't done a thorough investigation of the computational complexity of the approach. The experiment is currently based on GPU architecture. It would be easier to comprehend the computational resources needed to execute SAF and its potential for real-world applications with a more thorough analysis.  \n---Can you provide detailed explanations of how much computational overhead is needed when training? What kind of computation is included (float-point computation or spike computation)? I am interested in this because the training, unlike other online training, is based on float-point representation.**\n\nWe compare between the computational complexity (FLOPs) of OTTT and SAF during the table below. $c$, $k$, $w$, and $h$ represent the channel size, kernel size, and the width and height of the feature map. Also, $d_{\\rm in}$ and $d_{\\rm out}$ represent input and output sizes when fully connected. By using SAF, we can halve the computation of weights.Moreover, SAF only requires the retention of spike accumulation (float), while OTTT needs to retain membrane potentials (float), spike trains (spike), and spike accumulation (float). In other words, SAF only uses floating-point computation during training and spike computation during inference.   \nWe will add this table to Appendix if necessary.\n|                        | **OTTT$_ {\\\\rm O}$/OTTT$_ {\\\\rm A}$** | **SAF-F/SAF-E (ours)** |\n|------------------------|----------------------------------|------------------------|\n| FLOPs (Conv2D)         | $2 c^2 k^2 wh$                   | $c^2 k^2 wh$           |\n| FLOPs (FC)             | $2 d_ {\\\\rm in} d_ {\\\\rm out}$       | $d_{\\\\rm in} d_ {\\\\rm out}$|\n|------------------------|----------------------------------|------------------------|\n| spike accumulation $\\\\widehat{\\\\boldsymbol {a}} ^ l[t]$ | $\\\\checkmark$ | $\\\\checkmark $|\n| spike $\\\\boldsymbol {s} ^ l[t]$    | $\\\\checkmark$ | -                      |\n| Membrane potential $\\widehat{\\\\boldsymbol {U}}^{l+1}[t]$ |$ \\\\checkmark$ | -   |\n\n**---The theoretical results only show that the proposed method has the same capabilities as OTTT and SpikeRepresentation, and no new guarantee of training is presented there. Hence, the contribution is reduced to find an efficient forward method that fits the training models of OTTT and SpikeRepresentation.**  \nSAF offers memory reduction by eliminating the need to calculate membrane potentials during training, as well as halving the time required for weight calculation during forward propagation. In our experiments, SAF achieved a significant reduction of 30\\% in memory usage compared to OTTT. Similarly, the training time was also reduced by the same percentage, at least. These results demonstrate substantial differences between the two methods and highlight the significance of SAF's contributions. Moreover, we have established the theoretical consistency of SAF with OTTT and spike representation for both of forward and backward. Based on these contributions, we believe that our paper is suitable for acceptance by ICLR."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5588/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700205423429,
                "cdate": 1700205423429,
                "tmdate": 1700205423429,
                "mdate": 1700205423429,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aUZVqsLwbj",
            "forum": "CwAY8b8i97",
            "replyto": "CwAY8b8i97",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5588/Reviewer_J1Az"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5588/Reviewer_J1Az"
            ],
            "content": {
                "summary": {
                    "value": "This paper improves the SNN representation of OTTT method. It simplifies the formulas used in SNN representation forward propagation, thereby reducing the memory and time required in the training phase. The proposed approach in this paper offers insights into SNN training. However, some weaknesses and limitations still remain within the content of this paper."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This article presents improvements to OTTT by further simplifying the forward propagation equations. Additionally, the authors demonstrate the consistency between the gradients of their SAF method and OTTT. They provide experimental evidence that their method outperforms OTTT in certain scenarios."
                },
                "weaknesses": {
                    "value": "Weakness\n1. There is numerous equations in this paper, and many parts lack detailed reasoning, making it challenging for readers to follow along. For instance, in the section Spike Representation, the intermediate steps of most equations are omitted. And $\\mathbf{x}$ lacks explanation.\n2. What is the purpose of section 2.4.3. The network structure that included in this paper and OTTT do not have feedback connection.\n3. Merely validating the consistency between theoretical reasoning and the experimental results on CIFAR10 is insufficient for the author's purpose. It is evident that the author's approach, despite claiming consistency in the computation of reverse gradients with OTTT, yields different training outcomes (accuracy and fire rate). Moreover, based on the results presented by the author, it is clear that the SNN trained using the SAF method outperforms the OTTT method. Since the OTTT method involves more complex datasets, the author needs to demonstrate the superiority of SAF on those datasets as well.\n4. The author mentions that Equation 2 holds true strictly when the network time step T tends to infinity. However, in practice, the author uses relatively small values of T, which introduces errors between the actual spike accumulation of the SNN and the weighted spike accumulation. These errors arise due to the uneven distribution of spikes[1]. Could the author provide insights on the differences between the training results (using representation) and the actual LIF forward results to address this issue?\n\n[1] Bu T, Fang W, Ding J, et al. Optimal ANN-SNN Conversion for High-accuracy and Ultra-low-latency Spiking Neural Networks[C]//International Conference on Learning Representations. 2021."
                },
                "questions": {
                    "value": "See the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5588/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698725472753,
            "cdate": 1698725472753,
            "tmdate": 1699636575594,
            "mdate": 1699636575594,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ICP6XJpnSX",
                "forum": "CwAY8b8i97",
                "replyto": "aUZVqsLwbj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5588/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5588/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer J1Az"
                    },
                    "comment": {
                        "value": "Thank you for your review comments. We reply to the weak points and answer your questions. For clarity, we have addressed each of your comments individually in the following responses:\n\n**---1.There is numerous equations in this paper, and many parts lack detailed reasoning, making it challenging for readers to follow along. For instance, in the section Spike Representation, the intermediate steps of most equations are omitted. And $x$ lacks explanation.**  \nDue to page limitations, it is only possible to include some reasoning in the main text. The detailed reasoning of Section 4 is provided in Appendix A. Also, $x$ is the value of the input data. In the case of CIFAR-10, it is the value of each pixel in the image. This explanation has been added to the main text.\n\n**---2.What is the purpose of section 2.4.3. The network structure that included in this paper and OTTT do not have feedback connection.**  \nThe case with feedback is also mentioned in the paper of OTTT. Therefore, we considered SAF with feedback in Section 4.3, and experimental results were stated in Appendix E.\n\n**---3.Merely validating the consistency between theoretical reasoning and the experimental results on CIFAR10 is insufficient for the author's purpose. It is evident that the author's approach, despite claiming consistency in the computation of reverse gradients with OTTT, yields different training outcomes (accuracy and fire rate). Moreover, based on the results presented by the author, it is clear that the SNN trained using the SAF method outperforms the OTTT method. Since the OTTT method involves more complex datasets, the author needs to demonstrate the superiority of SAF on those datasets as well.**  \nIt is indeed consistent with SAF-F for spike representation, but not with OTTT${_ \\\\rm A}$. Section 5.2 confirms this distinction, highlighting the differences between the two methods. Consequently, the firing rates are expected to differ, which is correct. On the other hand, Section 5.1 describes the results of an experiment that confirms the equivalence between SAF-E and OTTT${_ \\\\rm O}$. The figure presented in this section demonstrates the same firing rates. Thanks to your review, we have added explanations to the figure captions in Section 5, improving readability.\n\n**---4.The author mentions that Equation 2 holds true strictly when the network time step T tends to infinity. However, in practice, the author uses relatively small values of T, which introduces errors between the actual spike accumulation of the SNN and the weighted spike accumulation. These errors arise due to the uneven distribution of spikes[1]. Could the author provide insights on the differences between the training results (using representation) and the actual LIF forward results to address this issue?**  \nThere are two reasons why we set $T=6$. First, OTTT's original paper (Xiao et al. [2022]) sets $T=6$. Second, in the case of CIFAR-10, the original paper on spike representation (Meng et al. [2022]) compares the four cases of $T=20,15,10,5$. The results show that the accuracy drop was less than 1\\%, and SOTA was achieved at $T=5$. Since SAF-F and OTTT$_{\\rm A}$ are different at $T=6$, it is unnecessary to increase $T$ to obtain similar results."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5588/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700203552355,
                "cdate": 1700203552355,
                "tmdate": 1700204177170,
                "mdate": 1700204177170,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ueAB6V8Cet",
                "forum": "CwAY8b8i97",
                "replyto": "ICP6XJpnSX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5588/Reviewer_J1Az"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5588/Reviewer_J1Az"
                ],
                "content": {
                    "title": {
                        "value": "Response to the rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for the further explanation. After re-check the content, it seemed that the current method is more or less a simplification rather than an improvement over OTTT. Since OTTT itself is not a widely adopted training framework for SNNs, the impact of the current work is questionable.  \n\nAlso, the reason for asking for results on more complex datasets is that we cannot assume scalability for sure. \n\nThe explanation for choosing T = 6 is not satisfactory. I understand why they choose T = 6 but it lacks approximation reasoning why choosing T = 6 is a reasonable setup as such a small T may introduce very significant bias."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5588/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700446513014,
                "cdate": 1700446513014,
                "tmdate": 1700446513014,
                "mdate": 1700446513014,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MY5WPcvNDt",
            "forum": "CwAY8b8i97",
            "replyto": "CwAY8b8i97",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5588/Reviewer_WuBX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5588/Reviewer_WuBX"
            ],
            "content": {
                "summary": {
                    "value": "The authors present \u201cSpike Accumulation Forwarding (SAF)\u201d for training spiking neural networks, an approach build on Online Training Through Time (OTTT) to utilize spike representations in form of accumulations through both the forward and the backward pass. This approach helps reduce the memory footprint of SNNs, since the membrane potential of the previous time step does not need to be tracked. In an extensive theoretical analysis, the authors prove the feasibility of their approach as well as its equivalence with the LIF neuron and the consistency with the existing approach OTTT. The theoretical findings are backed by a few pracitical examples."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The approach is novel and useful, as memory footprint in SNN training is a major issue not only in the backward but also in the forward paths.\n- The rigorous theoretical analysis proves the authors claims, and the brief experimental evaluation seems to confirm it."
                },
                "weaknesses": {
                    "value": "- The paper is in parts poorly written and very hard to follow. It would benefit from substantial language editing. Also, there are some Sentences and sections, e.g. 3rd paragraph of the introduction, whose meaning I do not understand at all.\n- The experimental evaluation is kept rather short. While it does seem to confirm the theoretical findings, there is no clear description of the experiments (e.g. the utilized model architecture and training setup), and no source code, which prohibits reproduction of the results. \n- The prior work and related concepts upon which the contribution of the authors build is not explained very well. Half of the assumptions and derivations are to be found in other papers, making it almost impossible to fully grasp the paper as a stand alone. It appears to be follow-up work by the authors of previous work, hence the authors don\u2019t seem to find it necessary to fully explain the background concepts"
                },
                "questions": {
                    "value": "- It is not clear to me why the approach works for small time steps. As stated in section 3.2 Spike representation assumes a rather large latency (T -> inf) to work. Does that not also apply to SAF? In Fig. 5 you show that firing rates for OTTT and SAF are NOT Identical. Then why do your experiments assume T=6? Where does the averaged spike representation come from in practice? \n- In Section 3, Spike Representation, what is x for the weighted average input? Not mentioned before\n- What model architecture and training setup was used for the experiments. As it, they are not reproducible by others."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5588/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698836926510,
            "cdate": 1698836926510,
            "tmdate": 1699636575467,
            "mdate": 1699636575467,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3lIhnBoE8c",
                "forum": "CwAY8b8i97",
                "replyto": "MY5WPcvNDt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5588/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5588/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WuBX"
                    },
                    "comment": {
                        "value": "Thank you for your review comments. We reply to the weak points and answer your questions. For clarity, we have addressed each of your comments individually in the following responses:\n\n**---The paper is in parts poorly written and very hard to follow. It would benefit from substantial language editing. Also, there are some Sentences and sections, e.g. 3rd paragraph of the introduction, whose meaning I do not understand at all.**  \nWe apologize if there was any confusion. However, we have had native English speakers proofread and confirm that there are no problems.\n\n**---The experimental evaluation is kept rather short. While it does seem to confirm the theoretical findings, there is no clear description of the experiments (e.g. the utilized model architecture and training setup), and no source code, which prohibits reproduction of the results.  \n---What model architecture and training setup was used for the experiments. As it, they are not reproducible by others.**  \nThe details of the model used and the training setup can be found in Appendix C, and the source code is available in the Supplementary material. It is important to note that all setups were consistent with the experiments conducted by Xiao et al. [2022].\n\n**---The prior work and related concepts upon which the contribution of the authors build is not explained very well. Half of the assumptions and derivations are to be found in other papers, making it almost impossible to fully grasp the paper as a stand alone. It appears to be follow-up work by the authors of previous work, hence the authors don\u2019t seem to find it necessary to fully explain the background concepts**  \nThe concept of this study is fully explained in Section 4.1. On the other hand, due to page limitations, it is only possible to include some reasoning in the main text. The detailed is provided in Appendix A.   \nSAF offers memory reduction by eliminating the need to calculate membrane potentials during training, as well as halving the time required for weight calculation during forward propagation. In our experiments, SAF achieved a significant reduction of 30\\% in memory usage compared to OTTT. Similarly, the training time was also reduced by the same percentage, at least. These results demonstrate substantial differences between the two methods and highlight the significance of SAF's contributions. Moreover, we have established the theoretical consistency of SAF with OTTT and spike representation for both of forward and backward. Based on these contributions, our paper goes beyond being a mere follow-up, and we believe that it is suitable for acceptance by ICLR.\n\n**---It is not clear to me why the approach works for small time steps. As stated in section 3.2 Spike representation assumes a rather large latency (T $\\to \\inf$) to work. Does that not also apply to SAF? In Fig. 5 you show that firing rates for OTTT and SAF are NOT Identical. Then why do your experiments assume T=6? Where does the averaged spike representation come from in practice?**  \nThere are two reasons why we set $T=6$. First, OTTT's original paper (Xiao et al. [2022]) sets $T=6$. Second, in the case of CIFAR-10, the original paper on spike representation (Meng et al. [2022]) compares the four cases of $T=20,15,10,5$. The results show that the accuracy drop was less than 1\\%, and SOTA was achieved at $T=5$. Since SAF-F and OTTT$ _ \\rm{A}$ are different at $T=6$, it is unnecessary to increase $T$ to obtain similar results.    \nIt is indeed consistent with SAF-F for spike representation, but not with OTTT$_ \\rm{A}$ . Section 5.2 confirms this distinction, highlighting the differences between the two methods. Consequently, the firing rates are expected to differ, which is correct. On the other hand, Section 5.1 describes the results of an experiment that confirms the equivalence between SAF-E and OTTT$ _ \\rm{O}$. The figure presented in this section demonstrates the same firing rates. Thanks to your review, we have added explanations to the figure captions in Section 5, improving readability.\n\n**---In Section 3, Spike Representation, what is x for the weighted average input? Not mentioned before**  \n$x$ is the value of the input data. In the case of CIFAR-10, it is the value of each pixel in the image. This explanation has been added to the main text."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5588/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700202966999,
                "cdate": 1700202966999,
                "tmdate": 1700204155219,
                "mdate": 1700204155219,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]