[
    {
        "title": "Toward a Mechanistic Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model"
    },
    {
        "review": {
            "id": "M3ZaNASXvz",
            "forum": "VJEcAnFPqC",
            "replyto": "VJEcAnFPqC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7555/Reviewer_V1TB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7555/Reviewer_V1TB"
            ],
            "content": {
                "summary": {
                    "value": "This paper sheds light on step-wise inference in LLMs by casting the problem as a graph navigation problem. A dataset of random DAGs is generated with start and goal nodes and an autoregressive transformer is trained to navigate the graph from start to goal by next step prediction. The problem can be mapped to step-wise inference in LLMs and the paper investigates several settings in which this is manifested."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I have found the problem setting of the paper interesting - these are important questions and some of the more challenging aspects of LLMs are investigated here.\n\nThe casting to DAG navigation is a good choice - it covers a lot of potentially related inference problems with a simple model which is easy to generate data for and train."
                },
                "weaknesses": {
                    "value": "I think my main issue with the paper is that LLMs are trained on very very different data (though in a roughly similar setup).\nI am not sure the mapping between DAG navigation and what LLMs actually learn is as simple as claimed in the paper as LLMs need to do many other things when trained."
                },
                "questions": {
                    "value": "Would the authors shed more light on the limitations and dissimilarities between the proposed model and actual LLM training?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7555/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698788515170,
            "cdate": 1698788515170,
            "tmdate": 1699636914438,
            "mdate": 1699636914438,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZqL7m0DiVO",
                "forum": "VJEcAnFPqC",
                "replyto": "M3ZaNASXvz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7555/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7555/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response part 1"
                    },
                    "comment": {
                        "value": "We thank you for your positive feedback and we are happy that you think casting stepwise inference as graph navigation is a good choice and that our setting was interesting.\nWe also realized that we might not have motivated the connection between graph navigation and stepwise inference. We have now made it more explicit and also added examples from LLMs (Appendix Figure 8)\n\n>**Comment:** I think my main issue with the paper is that LLMs are trained on very very different data (though in a roughly similar setup). I am not sure the mapping between DAG navigation and what LLMs actually learn is as simple as claimed in the paper as LLMs need to do many other things when trained.\n\n**Response:** This is a valid concern and we have significantly improved the writeup to address it further. To emphasize, we note that our use of graph navigation was meant as an abstraction of reasoning tasks where stepwise inference protocols have been shown to substantially improve performance. While we did not intend to suggest that graph navigation is a universal model for all such tasks, we do believe that for several tasks currently studied in literature [1,2,3,4], it serves as a faithful abstraction. For example, a LLM is often used to solve problems like the following (taken from [1]): \u201cImagine a building with 6 rooms. From the lobby you have two choices, you can go to room 1 and room 2. You enter room 1 and at the end of room 1 there is a door that leads to room 3, and room 3 leads to room 5. There\u2019s a chest in room 5. You open it and there is 10 dollars but you do not take the money. Then you exit and start over. This time in the lobby you choose room 2, which has a door to room 4, and room 4 has a door that leads to room 6. You find a chest with 60 dollars in room 6 but you do not take the money. You return to the lobby. You will be able to choose 1 path that leads to the most money. Which room from the lobby will lead to the path where one can make the most money.\u201d *Implicitly*, the computation required to solve this task involves graph navigation, though the graph is latent \u2013 the model has to map natural language onto a graph. This is similar to our setup, where a path sampled from a graph serves the same purpose: it represents a reasoning task that involves achieving a goal (navigation to a node), but the underlying graph is *implicit* and never fully observed. One can similarly consider the example of math word problems (from GSM8K [5]): \u201cA carnival snack booth made 50 dollars selling popcorn each day. It made three times as much selling cotton candy. For a 5-day activity, the booth has to pay 30 dollars rent and 75 dollars for the cost of the ingredients. How much did the booth earn for 5 days after paying the rent and the cost of ingredients?\u201d Implicitly, the model has to parse these natural language sentences into a computational graph and navigate on it.\n\nGiven the similarities highlighted above between our setup and practical tasks, we do believe the claims discussed in our work will transfer to real world settings. However, to perform validation of our generated hypotheses, such as the diversity-accuracy tradeoff and short path bias in stepwise inference, we believe a dedicated study is needed and hence leave it for future work."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7555/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700461711103,
                "cdate": 1700461711103,
                "tmdate": 1700461831475,
                "mdate": 1700461831475,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "C6RgfMwjfr",
            "forum": "VJEcAnFPqC",
            "replyto": "VJEcAnFPqC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7555/Reviewer_JQZB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7555/Reviewer_JQZB"
            ],
            "content": {
                "summary": {
                    "value": "To analyze the stepwise inference in large language models, the authors examine a synthetic graph navigation problem and replicate several phenomena originally observed in LLMs. This work provides a simplified platform for researchers to study properties of LLM."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. A comprehensive framework of DAG is provided as substitute for stepwise inference in LLM\n2. Reasoning gap, the diversity-accuracy tradeoff, in-context control are replicated in well-designed experiments, showing the effectiveness of the proposed enviroment.\n3. Well-written, easy to follow."
                },
                "weaknesses": {
                    "value": "1.While several phenomena originally observed in LLMs were replicated in this DAG couterpart. It will be more convincing if some extraoplated property in DAG system can be found in LLMs which was not discovered before.\n\n2. Emergent abilities of large language lodels were thought to be a consequence of unpredictable scaling, therefore, I doubt if a simpilified DAG substitute can replicate this behaviour."
                },
                "questions": {
                    "value": "I will increase my score if my concern is well addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7555/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698810628848,
            "cdate": 1698810628848,
            "tmdate": 1699636914331,
            "mdate": 1699636914331,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YyXX5HGW7v",
                "forum": "VJEcAnFPqC",
                "replyto": "C6RgfMwjfr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7555/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7555/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response part 1"
                    },
                    "comment": {
                        "value": "We thank you for your review and enthusiasm about our work! We are happy to hear that you think our framework is comprehensive, our experiments were well-designed and that our paper was well-written. We respond to specific comments below. \n\n>**Comment:** While several phenomena originally observed in LLMs were replicated in this DAG couterpart. It will be more convincing if some extrapplated property in DAG system can be found in LLMs which was not discovered before. \n\n**Response:** We appreciate the reviewer\u2019s concern that we might have only replicated phenomena at scale. We would like to take the opportunity to clarify that our study does more than mere replication of phenomena already observed in LLMs. \n\nFirstly, we ground and validate our synthetic setup by replicating phenomena observed at large scale, namely the step-wise inference gap. \n\nNext, we go beyond by:\n(i) identifying new phenomena leveraging the fact that we have an interpretable ground truth in the data-generating process. Specifically, we have found and characterized the following phenomena:\n\n- The dependence of the stepwise inference gap on the structure of the underlying graph\n\n- A short-path bias when there exist several paths between a pair of nodes\n\n- The diversity-accuracy tradeoff\n\n- The emergence of global planning capability *after* learning to take correct steps over training\n\n- The dependence on the length of the context during training to generalization during evaluation\n\n- The model\u2019s bias towards specific examples when there exist conflicting examples in context\n\n (ii) We have been quantitative and precise while characterizing these phenomena. Leveraging the interpretability, steerability that our controlled synthetic setup provides, we have access to ground truth and thus we can put exact numbers on the model\u2019s behavior. Evaluation of LLMs is a looming open problem [1], with no existing solution due to the scale, complexity, and lack of accessibility of their huge training corpus. Data leakage is a confounding issue which makes evaluation inherently challenging due to the ambiguity of defining test-train split and similar ambiguity for defining appropriate metrics [2]. Our synthetic setup allows us to bypass both these issues.\n\nIn principle, a further study could be performed by training models at scale and confirming at scale, which is what the reviewer is suggesting. We have acknowledged this limitation of our current work in a new section at the end of our introduction. This is a fundamental trade-off when one decides to use synthetic data: we gain interpretability and steerability by having a synthetic DGP where everything is controlled and this allows us to make precise quantitative statements and formulate hypotheses for the origin of phenomena.\n\nFurthermore, we\u2019d like to point the reviewer to a few studies where graph navigation was tested in-context in language models at large-scale:\n\nMommenejad et al. Evaluating Cognitive Maps in Large Language Models with CogEval: No Emergent Planning (NeurIPS 2023)\n\nSaparov et al. Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought (ICLR 2023)\n\nDziri et al. Faith and Fate: Limits of Transformers on Compositionality (NeurIPS 2023)\n\nWe have included a figure in our appendix containing example prompts and model outputs from each of these papers.\n\nMoreover, a recent mechanistic study even designs probes to extract the underlying logical graph during stepwise inference in LLMs!:\n\nTowards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models Hou et al (2023)  https://arxiv.org/pdf/2310.14491.pdf"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7555/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700461371886,
                "cdate": 1700461371886,
                "tmdate": 1700461371886,
                "mdate": 1700461371886,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2B8NquARz6",
                "forum": "VJEcAnFPqC",
                "replyto": "C6RgfMwjfr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7555/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7555/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response part 3"
                    },
                    "comment": {
                        "value": ">**Comment:** Emergent abilities of large language models were thought to be a consequence of unpredictable scaling, therefore, I doubt if a simpilified DAG substitute can replicate this behaviour.\n\n\n**Response:** We *respectfully* disagree with the claim that emergent abilities only occur in large scale models. Abilities like in-context learning (ICL) and stepwise inference protocols like chain-of-thought (CoT) have been shown and extensively studied in small toy models by several recent works by designing well defined synthetic tasks [1,2,3,4,5,6,7,8,9,10]. The goal of such studies (and ours well) is to study interesting phenomenology of LLMs and design abstract setups that enable development of precise mechanistic hypotheses for how the phenomenon works. For example [4,5] use a synthetically designed setup to develop hypotheses for how ``knowledge\u201d about an entity is stored in a pretrained model, showing that such knowledge can be manipulated via relatively simple linear transformations. This development of mechanistic hypotheses is precisely what we have done with our experiments. \n\nWe also note that to ensure the point about relation to LLMs is addressed in the paper, we have substantially revised our manuscript to further motivate the explicit connection between graph navigation and stepwise inference in more realistic settings. Specifically, we have rewritten parts of the introduction (we have elaborated our exposition on the relation between stepwise inference and graph navigation, added a discussion of the limitations of a synthetic data approach), rewritten our abstract, contextualized our results more thoroughly with existing literature (added Appendix figure 8 which shows examples of graph navigation in a LLM), and also edited the formatting and arrangement of figures and results in paper (including promoting the training dynamics figure to the main text since it shows that global planning capability arises *after* models learn to take correct steps). As an example, our edits now highlight several recent studies that either use or model stepwise inference with LLMs to study tasks that are specific instantiations of our proposed graph navigation problem (e.g., see [11,12, 13, 14]). We have included a figure in Appendix (Fig. 8) that describes example prompts and model outputs from each of these papers to exemplify the relation of our proposed synthetic benchmark to realistic setups and tasks considered by prior work in showing the value of stepwise inference. Given that these studies exist in the (very recent) literature, we argue the connection between stepwise inference, planning, and graph navigation is explicit and our updated draft ensures this connection is emphasized upon. \n\nWith our detailed responses and changes to the paper, we hope that we have addressed the reviewer's concerns and the reviewer increases their score.\n\n[1] Bai et al. Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection (NeurIPS 2023)\n\n[2] Akyurek et al. What learning algorithm is in-context learning? Investigations with linear models (2022) https://arxiv.org/abs/2211.15661\n\n[3] Ahn et al. Transformers learn to implement preconditioned gradient descent for in-context learning (NeurIPS 2023)\n\n[4] Zeyuan Allen-Zhu and Yuanzhi Li, (2023) Physics of Language Models: Part 3.1, Knowledge Storage (2023)\n\n[5] Zeyuan Allen-Zhu and Yuanzhi Li, Physics of Language Models: Part 3.2, Knowledge Manipulation (2023)\n\n[6] Maya Okawa et al. Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task (NeurIPS 2023)\n\n[7] Lindner et al (2023) Tracr: Compiled Transformers as a Laboratory for Interpretability https://arxiv.org/abs/2301.05062\n\n[8] Feng et al, Towards demystifying the mystery behind chain of thought: A theoretical perspective (2023)\n\n[9] Li et al. Dissecting Chain-of-Thought: Compositionality through In-Context Filtering and Learning (NeurIPS 2023)\n\n[10] Prystawski et al. Why think step by step? Reasoning emerges from the locality of experience (NeurIPS 2023)\n\n[11] Mommenejad et al. Evaluating Cognitive Maps in Large Language Models with CogEval: No Emergent Planning (NeurIPS 2023)\n\n[12] Saparov et al. Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought (ICLR 2023)\n\n[13] Dziri et al. Faith and Fate: Limits of Transformers on Compositionality (NeurIPS 2023)\n\n[14]  Hou et al (2023) Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models  https://arxiv.org/pdf/2310.14491.pdf"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7555/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700461640056,
                "cdate": 1700461640056,
                "tmdate": 1700461908226,
                "mdate": 1700461908226,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bZgb5ksSUe",
                "forum": "VJEcAnFPqC",
                "replyto": "2B8NquARz6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7555/Reviewer_JQZB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7555/Reviewer_JQZB"
                ],
                "content": {
                    "title": {
                        "value": "Reply to rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for the detailed rebuttal. Most of my concerns are well addressed. i recommend this paper for publication."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7555/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684867018,
                "cdate": 1700684867018,
                "tmdate": 1700684867018,
                "mdate": 1700684867018,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zA5QeBC62B",
            "forum": "VJEcAnFPqC",
            "replyto": "VJEcAnFPqC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7555/Reviewer_3PuG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7555/Reviewer_3PuG"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to study the step-wise inference mechanism by exploring the graph navigation problem."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ Originality:\n\n    The idea of modeling step-wise inference as graph navigation is interesting."
                },
                "weaknesses": {
                    "value": "- Quality:\n\n    i) Although the authors claim to reveal the connection between graph navigation and the step-wise inference mechanism in transformer, the experiments, to my understanding, are solely about training transformers on the synthesized graph dataset. It remains unclear to me how this set-up can be translated into the study of step-wise inference mechanism, despite the conceptual similarity between DAG and the step-wise inference mechanism. It is unclear whether this finding on the synthesized graph dataset can be safely transferred to real-world large-scale dataset.\n\n    ii) The definition of hierarchical graph and random graph is confusing to me. It seems possible to convert the random graph in fig. 10 into a hierarchical graph by simply re-grouping the nodes.\n\n- Significance:\n\n    The highlighted finding, i.e., the step-wise inference gap is influenced by (i) the underlying DAG structure, and (ii) the length of the training samples is only studied in a very shallow level. The DAG structure in the context seems to be not well-defined and well-categorized. The discussion about the length of the training samples mainly focuses on how it affects the length of the model output."
                },
                "questions": {
                    "value": "Please see the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7555/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7555/Reviewer_3PuG",
                        "ICLR.cc/2024/Conference/Submission7555/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7555/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827653242,
            "cdate": 1698827653242,
            "tmdate": 1700684079256,
            "mdate": 1700684079256,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wfBQ24rrZY",
                "forum": "VJEcAnFPqC",
                "replyto": "zA5QeBC62B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7555/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7555/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response Part 1"
                    },
                    "comment": {
                        "value": "We thank you for your review and glad to hear that you find our idea of modeling step-wise inference as graph navigation interesting. We address your specific concerns below.\n\n>**Comment:** Although the authors claim to reveal the connection between graph navigation and the step-wise inference mechanism in transformer, the experiments, to my understanding, are solely about training transformers on the synthesized graph dataset. It remains unclear to me how this set-up can be translated into the study of step-wise inference mechanism, despite the conceptual similarity between DAG and the step-wise inference mechanism. It is unclear whether this finding on the synthesized graph dataset can be safely transferred to real-world large-scale dataset.\n\n**Response:** Thank you for this question, which touches on very important aspects of the motivation, objectives, and limitations of our work. Your question has prompted us to add the new paragraph at the end of the Introduction on \"The motivation, contributions, and limitations of our model-experimental systems approach.\" (Please also see our global response for related discussion as well.) To provide further evidence for the correctness of the analogy between our synthetic graph navigation task and stepwise inference tasks that real LLMs are evaluated on, we have also created a new Appendix Figure 8, where we showcase several recent works [1,2,3,4] that have studied the impact of stepwise inference on tasks that can easily be modeled in our graph navigation framework. We elaborate key arguments below. \n\nFirst, our contributions can be divided into three main parts of (i) formulation of synthetic graph navigation model motivated by the analogy between DAG and the step-wise inference; (ii) empirical validation of the model by precise reproduction of phenomena observed in LLMs; and (iii) discovery of new mechanistic hypotheses such as the impact of the underlying graph structure on stepwise inference, path stitching mechanisms, diversity-accuracy tradeoff, shorter-path bias in inference, and more.\n\nIn response to your specific concerns, beyond the \"conceptual similarity between DAG and the stepwise inference mechanism\", we have experimentally validated our model by reproducing an array of phenomena observed in LLMs. Furthermore, while all experimental results are obtained by \"training transformers on the synthesized graph dataset\", as you correctly point out, we believe that our novel mechanistic hypotheses are our important contribution. Finally, we agree with the necessary trade-offs and limitations of our study that our proposed hypotheses are not conclusive about the mechanisms underlying LLMs. We have clarified this in our new paragraph as, \"To draw an analogy to the study of biological neural networks, where neural mechanisms identified in small-scale model organisms such as fruit flies or mice may not be directly applicable to medical applications involving the human brain, our observations should not be taken as definitive conclusions that are readily applicable to modern large-scale generative models.\u201d Nevertheless, we are hopeful that our novel hypotheses can translate to larger-scale settings, based on the fact that our model system, despite its simplicity, successfully reproduced an array of known phenomena in LLMs. Testing them would require training and evaluating models on larger-scale setup, but would be an exciting future direction."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7555/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700461144484,
                "cdate": 1700461144484,
                "tmdate": 1700461144484,
                "mdate": 1700461144484,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "a5Lnq3vIiw",
                "forum": "VJEcAnFPqC",
                "replyto": "zA5QeBC62B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7555/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7555/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response part 2"
                    },
                    "comment": {
                        "value": "To summarize, the goal of our paper was to use graph navigation as an abstraction of tasks where stepwise inference is known to play a critical role and correspondingly develop and validate precise hypotheses that can help understand stepwise inference. This is similar to several recent works studying synthetic problems for developing a better understanding of LLMs [4,5, 6, 7, 8,9,10]. As we note above, we believe our abstraction is faithful and hence we expect our results will transfer to realistic scenarios. Such an investigation is best left to future work, however. We also emphasize that to further clarify the comments above, we have rewritten parts of the introduction where we have elaborated our exposition on the relation between stepwise inference and graph navigation, rewritten our abstract and also edited the formatting and arrangement of figures and results in paper (including promoting the training dynamics figure the main text since it shows that global planning capability arises *after* models learn to take correct steps). \nWe hope this has clarified the contributions and limitations of our work, but we are happy to discuss further if you have any remaining concerns.\n\n[1] Mommenejad et al. Evaluating Cognitive Maps in Large Language Models with CogEval: No Emergent Planning (NeurIPS 2023)\n\n[2] Saparov et al. Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought (ICLR 2023)\n\n[3] Dziri et al. Faith and Fate: Limits of Transformers on Compositionality (NeurIPS 2023)\n\n[4] Hou et al. Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models. (arXiv, 2023)\n\n[5] Zeyuan Allen-Zhu and Yuanzhi Li, (2023) Physics of Language Models: Part 3.1, Knowledge Storage (2023)\n\n[6] Zeyuan Allen-Zhu and Yuanzhi Li, Physics of Language Models: Part 3.2, Knowledge Manipulation (2023)\n\n[7] Maya Okawa et al. Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task (NeurIPS 2023)\n\n[8] Zhou et al. What Algorithms can Transformers Learn? A Study in Length Generalization. (arXiv, 2023; https://arxiv.org/abs/2310.16028)\n\n[9] Feng et al, Towards demystifying the mystery behind chain of thought: A theoretical perspective (arXiv, 2023)\n\n[10] Li et al. Dissecting Chain-of-Thought: Compositionality through In-Context Filtering and Learning (NeurIPS 2023)"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7555/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700461159693,
                "cdate": 1700461159693,
                "tmdate": 1700461787059,
                "mdate": 1700461787059,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FvBWdlujj7",
                "forum": "VJEcAnFPqC",
                "replyto": "zA5QeBC62B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7555/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7555/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response part 3"
                    },
                    "comment": {
                        "value": "> **Comment**:  The definition of hierarchical graph and random graph is confusing to me. It seems possible to convert the random graph in fig. 10 into a hierarchical graph by simply re-grouping the nodes.\n\n**Response**: \nIn a hierarchical graph, nodes are pre-partitioned into layers. The only connections allowed are between successive layers and these are chosen randomly. Whereas in a random graph, all connections are chosen at random. It is not possible to simply regroup nodes in a random graph and convert it to a hierarchical graph. To provide evidence for this, we now refer the reviewer to Appendix Figure 9 from our paper, where we highlight and discuss the differences between a hierarchical and random graph. In that figure, the *bottom panels* show the distribution of path lengths in the two families of graphs: hierarchical graphs have an exponential distribution owing to their structure and random graphs have a gaussian distribution. For example, given any pair of nodes, in a hierarchical graph every path between them is of the same length (determined by the difference of their layer numbers), whereas in a random graph there is a diversity of path lengths (for instance, there might be an edge between the nodes as well as a path of length more than 10, as is the case with our synthetic random graphs). We wish to emphasize that our specific approach to using a hierarchical graph is designed to enable more precise control over the data distribution.\n\nTo further ensure this point is clear, we have added pseudocode for the algorithms for generating both the random and hierarchical graphs\u2014**Please refer to Algorithms 1 and 2 in the Appendix A.3**. We hope that this,along  with the path characterization provided in Figure 9, addresses concerns about possible similarities between the two setups.\n\n> **Comment:**\nThe highlighted finding, i.e., the step-wise inference gap is influenced by (i) the underlying DAG structure, and (ii) the length of the training samples is only studied in a very shallow level. The DAG structure in the context seems to be not well-defined and well-categorized. The discussion about the length of the training samples mainly focuses on how it affects the length of the model output.\n\n**Response:**\nOur highlighted findings in a single graph scenario include: (i) the inference gap consistently persists across diverse data distributions in the training data (Figure 3); (ii) a diversity versus accuracy trade-off exists in finite temperature stepwise inference for transformers (Figure 4); and (iii) demonstrating that there is a bias towards shorter paths and the observation that over the course of training the model first learns to take correct steps and then acquires the capability of global planning (Figure 5). We recognize that our approach serves as a first step towards formulating a set of hypotheses for further validation in large-scale, realistic settings, and there is considerable scope for extending our research to more realistic settings such as different data distributions, noisy or incomplete information, and different graph structures. As part of this endeavor, We have now repeated our analysis on a setup where a ground-truth graph is \u201cnoisily observed\u201d. Specifically, every time a path is sampled between two nodes, we intentionally corrupt a random fraction of the tokens on the path by 5-20%. The model is now trained on these corrupted paths\u2014this randomization further ensures that overfitting is infeasible, since a given path is never seen twice (due to corruptions). *In this setup, we again see that our claim on the performance gap induced by stepwise inference continues to persist.* We have also defined our training protocol and description of the graphs used in training in more detail (in Appendix). \n\n\nSummary: We appreciate your constructive feedback, which helped us to clarify the motivation, objectives, and limitations of our synthetic data approach. We hope that the contributions of our work are now clearer and that the reviewer will consider raising their score to support the acceptance of our work."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7555/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700461199088,
                "cdate": 1700461199088,
                "tmdate": 1700461227736,
                "mdate": 1700461227736,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ES0rBAW9K2",
                "forum": "VJEcAnFPqC",
                "replyto": "FvBWdlujj7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7555/Reviewer_3PuG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7555/Reviewer_3PuG"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' point-to-point and detailed responses. I appreciate the efforts they made to address my concerns. However, my major concerns remain unchanged. Solely being able to \"reproduce an array of phenomena observed in LLMs\" on the **synthetic graph data** does not serve as strong evidence that the current findings are useful for LLMs in real-world tasks. An interesting aspect of the step-wise inference process of LLMs is that it naturally extracts a DAG-like structure from the **raw-data** (featuring noisy, not-that-structured often distributed observations). However, directly feeding the already highly-structured clean graph data into the model completely skip this abstraction step. In addition, the random graph and hierarchical graph definition are still not well-established given the explanations. As long as there is no loop in the graph, it is trivial to convert the \"random graph\" into the \"hierarchical graph\" defined in this paper. To be more specific, it is clear that a hierarchical DAG generated using algo 2. in the appendix can also be generated by algo 1, by simply re-ordering the nodes in the `adjMatrix` in algo 1.\n\nTherefore, as the authors also acknowledge, the current \"proposed hypotheses are not conclusive about the mechanisms underlying LLMs\" given the insufficient experiment evaluation and/or modeling of the step-wise inference process. My suggestion is that the authors can consider reproducing their findings using at least synthetic language datasets (as the referred works [1,2,3,4] did) and reformulate the model to make a stronger, clearer and more serious connection with the step-wise inference process of LLMs. Considering the author responses and comments from other reviewers, I will keep my rating as it is and stick to rejecting this work."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7555/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683721495,
                "cdate": 1700683721495,
                "tmdate": 1700683721495,
                "mdate": 1700683721495,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FxzU0X8Df7",
                "forum": "VJEcAnFPqC",
                "replyto": "xE53EV1Efp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7555/Reviewer_3PuG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7555/Reviewer_3PuG"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your follow-up discussion and pointers to these related works, which are very helpful indeed. First of all, let's be very clear that I'm not against papers that focus on scientific modeling. But can the authors be more explicit and specific about what is the theoretical contribution in the current modeling of step-wise inference process of LLMs (other than the direct application of the definition of DAGs)? The referred work [1] and [2] did study the related processes using simplied models and datasets, but also provided enough insights using well-established theoretical frameworks and convincing derivations. This, to my opinion, is a key part for scientific modeling, i.e., not only do we make guesses, but the insights and discoveries derived from the hypotheses are also important. Sometimes, these are more important because we can check whether the modeling is right or wrong from the deduction. \n\nSecond, as stated in my response, I'm not against synthetic data. I suggested using at least synthetic **language** datasets as used in the referred works, as directly feeding the already highly-structured clean graph data into the model creates a non-negligible gap between the inputs of the current framework and those actually used for LLMs, which are, as stated, \"noisy, not-that-structured with the information distributed among different observations\". The emergence of the DAG structure itself in the step-wise inference process is non-trivial and should be more carefully considered."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7555/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691422344,
                "cdate": 1700691422344,
                "tmdate": 1700691422344,
                "mdate": 1700691422344,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BGxST2ne99",
                "forum": "VJEcAnFPqC",
                "replyto": "zA5QeBC62B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7555/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7555/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for continued engagement.\n\n> I suggested using at least synthetic language datasets as used in the referred works, \n\nFirstly, we'd like to point out that [1] and [2] did not use any language datasets in their proofs or experiments. [1] uses a similar setup to ours where tokens correspond to nodes on a graph and [2] uses random 4-64 dimensional euclidean vectors.\n\nSecondly, while [1] and [2] have a fraction of theory results, a majority of synthetic data work does not. As examples from diverse areas, consider: [3] which studies how transformers navigate small tree-structured graphs from CFGs, [4] which studies a synthetic image dataset in diffusion models and empirically shows the delayed emergence of compositionality and [5] which studies RL agents interacting with simple causal DAGs with 5 nodes, with purely empirical results and no theorems with proofs. These are most similar in style to our work, which studies \"empirical models\" , first validates their approach by reproducing phenomena and then produces hypotheses to test at scale.\n\nOur explicit list of hypotheses and contributions is given at the end of the introduction, just before discussion of our limitations and each hypothesis is stated in the corresponding results section. To summarize everything together:\n\n- We find that the Step-by-Step Inference gap depends on two key factors, firstly the structure of the underlying DAG: the Step-by-Step Inference gap is larger for graphs that are hierarchical as opposed to random and secondly, the length of the training samples: the Step-by-Step Inference gap is larger if the model has been trained on a set of shorter paths that have to be ``stitched\" together to build the path during evaluation.\n\n- We find that models have a shorter path bias in the case of high path diversity.\n\n- We systematically and quantitatively characterize the diversity-accuracy tradeoff, showing curves with loss of accuracy and increase of diversity as a function of sampling temperature.\n\n- Over the course of training, we find that global planning capabilities arise after the model has learnt to take correct steps\n\n- We hypothesize that for chain-of-thought and related methods at scale: the model will fail to generalize to reasoning chains longer than those present in its training data.\n\n- When presented with 2 conflicting chains in-context, we find that the model has a strong bias toward choosing the first chain over the second.\n\n[3] Allen-Zhu et al. Physics of Language Models: Part 1, Context-Free Grammar https://arxiv.org/abs/2305.13673\n\n[4] Maya Okawa et al. Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task. NeurIPS 2023 (https://arxiv.org/pdf/2310.09336.pdf)\n\n[5] Ishita Dasgupta et al. Causal Reasoning from Meta-reinforcement Learning ICLR 2019 (https://arxiv.org/abs/1901.08162)"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7555/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700347503,
                "cdate": 1700700347503,
                "tmdate": 1700701234627,
                "mdate": 1700701234627,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qqLwsRvLZZ",
                "forum": "VJEcAnFPqC",
                "replyto": "zA5QeBC62B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7555/Reviewer_3PuG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7555/Reviewer_3PuG"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for their efforts to continue this discussion. \n\nI would like to again point out that first, my suggestion about using synthetic language datasets originates from the following two concerns: **i)** the lack of significant theoretical contribution other than the direct application of the definition of DAGs (if we were to verify the hypotheses on synthetic graph data as in [1, 2]), and **ii)** in addition to i), the non-negligible gap between the inputs of the current framework and those actually used for LLMs, i.e., insufficient empirical evidence for safely transferring the current findings to real-world tasks. These two concerns together undermine my confidence of the current submission being acceptable to a top conference like ICLR.\n\nI thank the authors for further pointing me to [3], [4] and [5]. While [3] is surely an interesting piece of work using only synthetic data with few or no theoretical deduction, the hypotheses are examined with extensive, comprehensive and well-designed experiments which I highly appreciate. These experiments are scaled up and deliberated designed to closely approximate the richness of real language (e.g., see Sec. 2.2 Why Such CFGs in [3]) unlike those in the current submission. [4] and [5], however, are somehow unrelated to the current topic of our discussion, in my opinion. Although [4] did use synthetic data to probe the compositional generalization ability of conditional diffusion models as the core motivating examples and results, there is a section, i.e., `Sec. 4.2 Additional Experimental Results with Real Data' to showcase that the observations are truly consistent (at least partially) with real data. I am admittedly not very familiar with the area of causal reasoning, which is the case in [5] and the case is seemingly not very related to the current problem.\n\nAgain, I appreciate the efforts the authors made to try to address my concerns and think the idea of modeling step-wise inference as graph navigation is interesting. It is not likely, however, that I will change my opinion with more listed related/unrelated works if the main concerns, which I believe are clearly stated, are not addressed. I respect the endeavors and opinions from other reviewers, and honor the final decision made by the committee no matter what that will be."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7555/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705856623,
                "cdate": 1700705856623,
                "tmdate": 1700712737128,
                "mdate": 1700712737128,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4dgSAK7DDt",
            "forum": "VJEcAnFPqC",
            "replyto": "VJEcAnFPqC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7555/Reviewer_7XTH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7555/Reviewer_7XTH"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces and explores a novel paradigm that casts stepwise inference, a crucial element in logical reasoning, as a graph navigation challenge. Using directed acyclic graphs (DAGs) inspired by computational graphs and execution traces, the paper proposes a synthetic autoregressive language model setup for solving navigation tasks. The primary aim is to simplify, control, and interpret the mechanisms behind stepwise inference in Large Language Models (LLMs). This work serves as a foundational step towards creating a controllable and interpretable data generation process, offering insights into the stepwise inference in autoregressive transformers, which can inspire future research in logical reasoning and stepwise inference."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper presents an innovative framework that leverages stepwise inference within transformer models to navigate complex graph structures, showcasing a significant advance in understanding logical reasoning paths. The method enhances the interpretability of models but with a deeper mechanistic insight.\n2. The paper integrates theoretical concepts with empirical validation, showcasing a comprehensive study on synthetic graph navigation tasks. Experimental design and results, particularly the diversity vs. accuracy trade-off, provide a compelling case for the model's efficacy and reliability.\n3. The paper also introduces a data generating process that augments the richness of training datasets, allowing for more robust model training."
                },
                "weaknesses": {
                    "value": "1. While the paper claims to address stepwise inference in transformers and introduces a graph navigation model, the experiments seem to focus narrowly on synthetic tasks without sufficient evidence of the model's generalizability to a more realistic or applicable datasets.\n2. The approach primarily involves modeling the decision-making process using directed acyclic graphs (DAGs). However, there are concerns that the model may overfit these synthetic graph structures. The paper does not adequately address how the model handles noisy real-world graphs, which may exhibit cycles, incomplete information, or random behavior that are common in real-world applications. In this context, a deeper analysis of the robustness of the proposed method is crucial."
                },
                "questions": {
                    "value": "Illustrated in the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7555/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699163550965,
            "cdate": 1699163550965,
            "tmdate": 1699636914073,
            "mdate": 1699636914073,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HUMJEXEkAR",
                "forum": "VJEcAnFPqC",
                "replyto": "4dgSAK7DDt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7555/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7555/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response Part 1"
                    },
                    "comment": {
                        "value": "Thank you for your detailed review and positive feedback! We are glad that you think our paper presents \u201can innovative framework\u201d, showcases a \u201csignificant advance in understanding logical reasoning paths\u201d and that our study was \u201ccomprehensive\u201d. We were also excited to study the diversity-accuracy tradeoff, and believe ours is the first work to systematically and quantitatively study this phenomena. Our goal was exactly this: to devise and explore an appropriate systematic synthetic framework to study the phenomenology of stepwise inference. We address your specific questions below.\n\n\n> **Comment:** While the paper claims to address stepwise inference \u2026, the experiments seem to focus narrowly on synthetic tasks without sufficient evidence of the model's generalizability to a more realistic or applicable datasets.\n\n**Response:** Thank you for raising this comment. We have substantially revised our manuscript to address your concern by further motivating the explicit connection between graph navigation and stepwise inference in more realistic settings. Specifically, we have rewritten parts of the introduction (we have elaborated our exposition on the relation between stepwise inference and graph navigation, added a discussion of the limitations of a synthetic data approach), contextualized our results more thoroughly with existing literature, and also edited the formatting and arrangement of figures and results in paper (including promoting the training dynamics figure to the main text since it shows that global planning capability arises *after* models learn to take correct steps). As an example, our edits now highlight several recent studies that either use or model stepwise inference with LLMs to study tasks that are specific instantiations of our proposed graph navigation problem (e.g., see [1, 2, 3, 4]). We have also included a figure in Appendix (Fig. 9) that describes example prompts and model outputs from each of these papers to exemplify the relation of our proposed synthetic benchmark to realistic setups and tasks considered by prior work in showing the value of stepwise inference. Given these (very recent) studies, we argue the connection between stepwise inference, planning, and graph navigation is explicit and our updated draft ensures this connection is emphasized upon. \n\nWe\u2019d also like to contextualize how our methodology of proposing and studying a synthetic problem fits into the literature. Specifically, the goal of our work was to analyze an interesting phenomenon witnessed in LLMs, i.e., stepwise inference, by designing an abstraction of the problem. This enables development of precise mechanistic hypotheses. In LLM and generative model literature, this is a fairly common approach. For example, [5,6] use a synthetically designed setup to develop hypotheses for how ``knowledge\u201d about an entity is stored in pretrained model, show that such knowledge can be manipulated via relatively simple linear transformations. [7] used a procedurally defined synthetic dataset with a relatively small model to demonstrate that emergent abilities seen in neural networks are partially driven by the compositional nature of real world data. In other work [8], use Tracr-compiled transformers to show that if primitive operations involved in a formal algorithm can be implemented in a model, stepwise inference is sufficient for length generalization. Similarly, [9] used context-free grammars to demonstrate that stepwise inference is sufficient to solve problems that require dynamic programming. [10] use CoT to show transformers can learn arbitrary compositions of MLPs in-context. Our work lies in the same regime as these studies and has the goal of defining a well-designed task for studying stepwise inference protocols. \n\n\n[1] Mommenejad et al. Evaluating Cognitive Maps in Large Language Models with CogEval: No Emergent Planning (NeurIPS 2023)\n\n[2] Saparov et al. Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought (ICLR 2023)\n\n[3] Dziri et al. Faith and Fate: Limits of Transformers on Compositionality (NeurIPS 2023)\n\n[4] Hou et al. Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models. (arXiv, 2023)\n\n[5] Zeyuan Allen-Zhu and Yuanzhi Li, (2023) Physics of Language Models: Part 3.1, Knowledge Storage (2023)\n\n[6] Zeyuan Allen-Zhu and Yuanzhi Li, Physics of Language Models: Part 3.2, Knowledge Manipulation (2023)\n\n[7] Maya Okawa et al. Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task (NeurIPS 2023)\n\n[8] Zhou et al. What Algorithms can Transformers Learn? A Study in Length Generalization. (arXiv, 2023; https://arxiv.org/abs/2310.16028)\n\n[9] Feng et al, Towards demystifying the mystery behind chain of thought: A theoretical perspective (arXiv, 2023)\n\n[10] Li et al. Dissecting Chain-of-Thought: Compositionality through In-Context Filtering and Learning (NeurIPS 2023)"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7555/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700461023558,
                "cdate": 1700461023558,
                "tmdate": 1700461023558,
                "mdate": 1700461023558,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]