[
    {
        "title": "BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences"
    },
    {
        "review": {
            "id": "1hJNIij1gh",
            "forum": "qDKTMjoFbC",
            "replyto": "qDKTMjoFbC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1934/Reviewer_y1yi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1934/Reviewer_y1yi"
            ],
            "content": {
                "summary": {
                    "value": "> **TL;DR:** The proposed BurstAttention algorithm reduces 40% of communication overheads and thus achieves 2x\n speedup during training 32K sequence length. However, the experiments due not include comparisons to some popular distributed algorithms (Data and Pipeline Parallelism). Addressing my concerns (especially W.1. and W.2.) and questions would improve my score.\n\nThe paper introduces BurstAttention, an efficient distributed attention framework designed to address the challenges associated with processing long sequences in Transformer-based large language models (LLMs). While attention modules have been essential for the success of LLMs, their quadratic time and memory complexities pose obstacles for long sequences. BurstAttention divides long sequences into partitions across distributed clusters and employs global and local attention optimization strategies to optimize memory access and communication operations. FlashAttention can be viewed as a specialization of BurstAttention on a single device. Experimental results reveal that BurstAttention reduces communication overheads by 40% and achieves a 2\u00d7 speedup in training on sequences of 32K length with 8\u00d7A100 GPUs, outperforming existing competitive distributed attention solutions. BurstAttention is also shown to adapt well to various optimization strategies and offers greater scalability in comparison to other solutions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* **S.1.** The proposed BurstAttention algorithm tackles an important problem in the computational costs of training and inference of LLMs. \n* **S.2.** The paper is well written, the illustrations are informative, and the algorithms are easy to follow.\n* **S.3.** The experimental results show that the  BurstAttention algorithm outperforms existing algorithms, especially for long context windows.\n* **S.4.** The experiments are conducted on several model sizes and include ablations for the speedup gains."
                },
                "weaknesses": {
                    "value": "* **W.1.** The paper provides comparison to several algorithm, but does not include the common DataParallel and Pipeline Parallelism due to sequence length limitations. As these algorithms are the most popular approaches it would help to show even a single experiments using them as a baseline.\n* **W.2.** The experiments are conducted on a single hardware setup, providing experiments on different configurations would help.\n* **W.3.** The paper states that BurstAttention can be easily extended to other cross-attention modules, however, it does not provide any details. Providing additional details would help."
                },
                "questions": {
                    "value": "* **Q.1.** Which PCIe is used in the experiments and what is the bandwidth?\n* **Q.2.** How would the experiments look if the communication bandwidth would be higher/lower?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1934/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698151419330,
            "cdate": 1698151419330,
            "tmdate": 1699636124565,
            "mdate": 1699636124565,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pNGwJSwRem",
                "forum": "qDKTMjoFbC",
                "replyto": "1hJNIij1gh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1934/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1934/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to your questions"
                    },
                    "comment": {
                        "value": "Thank you for your constructive feedback on our submission. We appreciate your recognition of the strengths of our work and the areas where improvements can be made. We have carefully considered your comments and would like to address your concerns and questions as follows.\n\n### 1. **For Data/Pipeline Parallelism Baselines**:\n   \nWe acknowledge the absence of comparisons with common parallelism approaches like data parallelism and pipeline parallelism in our current studies. Our decision to omit these from the initial set of experiments is due to inherent limitations with sequence lengths. From our pilot experiments, directly adopting data parallelism and pipeline parallelism can only handle those sequences shorter than 8192.\n\nWe understand the importance of these popular approaches in the field and are planning to conduct further experiments. We aim to explore the applicability of sequence parallelism in various scenarios and compare the differences between different parallelism strategies. This will be a significant aspect of our ongoing research. In our additional experiments for reviewer Gb5f & J3Ya, we have explored combining ZeRO (one optimization for data parallelism) and BurstAttention, and this combination has shown some promising results to trade off runtime and memory efficiency.\n\n### 2. **Experiments on More Hardware Configurations**:\n\nFrankly, the current experiments are conducted on a specific hardware setup due to the limited resources we can use for this paper. Inspired by your suggestions, we have recognized the value of evaluating our method across different configurations. \n\nHowever, because of some force majeure, we may need time to prepare other hardware configurations (e.g., 4090 and H100). If these hardware configurations are met, our future research plans include conducting experiments on clusters and devices with more extreme lower/higher communication constraints. We believe this will provide us with a deeper understanding of how BurstAttention performs under varied conditions and will help us in tailoring our attention algorithms and parallelism strategies to more specialized scenarios.\n\n### 3. **Questions Regarding PCIe and Communication Bandwidth**:\n   \nRegarding your first question, our experiments utilize PCIe 4.0, and we do not enable CUDA direct access between GPUs for experiments. The reason for this setting can refer to our additional experiments for reviewer Gb5f & J3Ya. \n\nConcerning your second question, our current experimental results indicate a clear advantage of our approach in scenarios with limited communication capabilities. Since one great advantage of BurstAttention is its low communication overhead and highly overlapping communication and computation operations, the benefits of BurstAttention are less pronounced in environments with high communication bandwidth (e.g., using NVLink and InfiniBand). \n\nAlthough enabling NVLink and InfiniBand is expensive and such device conditions are uncommon for LLM inference, we have recognized the potential for further optimization when communication bandwidth is higher. We are exploring various communication scenarios to better understand and optimize the performance of BurstAttention under different communication conditions. Thanks again for your insightful suggestion.\n\nWe hope these responses address your concerns and questions. We are committed to continuously improving our work and value your input as it guides our future research directions."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1934/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709542761,
                "cdate": 1700709542761,
                "tmdate": 1700709542761,
                "mdate": 1700709542761,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vD8J01fdQH",
            "forum": "qDKTMjoFbC",
            "replyto": "qDKTMjoFbC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1934/Reviewer_J3Ya"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1934/Reviewer_J3Ya"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents BurstAttention, an efficient distributed attention for long context LLMs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. the method is well motivated, well presented and easy to understand.\n2. the comparison is throughout and complete."
                },
                "weaknesses": {
                    "value": "1. The comparison with Megatron is not up-to-date. For instance, Figure 2b does not use the most memory-efficient implementation. The baseline should be set up Megatron-v3 (a combination of tensor model parallelism and sequence parallelism). \n2. The latency comparison against Megatron is not well analyzed, see questions below.\n3. The argument with sparse attention is vague. There are no experiments."
                },
                "questions": {
                    "value": "Why table 3 shows such a big improvement against Megatron, are they all resulted from the communication volume? In particular, can the author provides the ablation on the communication wall clock time? (1) the communication volume is less indicative, because of different distributed prototype has different throughput themself. For instance, Megatron uses all-gather, which is highly optimized in NCCL. If the system uses P2P (and is the system using P2P? Can the author provide more details?), even the communication volume is less, the wall clock time can be higher. (2) In the experiment, the objective is causal language modeling, where the system seems to have unbalanced workload, e.g. the machine hosting first query will be idle most of the time. However, in Megatron, the workload is balanced because they do not partition sequences. Does the system balance the workload? If not, the system should be 2x slower than Megatron due to the unbalanced workload?\n\nThe reviewer is happy to raise score, if these questions are addressed clearly."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1934/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1934/Reviewer_J3Ya",
                        "ICLR.cc/2024/Conference/Submission1934/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1934/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698540019731,
            "cdate": 1698540019731,
            "tmdate": 1700695010865,
            "mdate": 1700695010865,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "imgYmX6tYL",
                "forum": "qDKTMjoFbC",
                "replyto": "vD8J01fdQH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1934/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1934/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to your questions(Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful and detailed review of our submission. We appreciate the time you took to analyze our work and the insightful questions you raised. Here are our responses to your questions.\n\n### 1. **For Comparison with Megatron V3**\n\nThanks for your insightful suggestion, we have conducted new experiments for comparison with Megatron V3. For the new experiments, we replace the original tensor parallelism with the implementation of Megatron V3, an implementation of tensor parallelism that considers the features of sequence parallelism. \n\nFurthermore, we have expanded our training experiments on 32 A100 GPUs. All these experiments are conducted on a cluster interconnected by a RoCE (RDMA over Converged Ethernet) network with an approximate bandwidth of 200 Gb/s, and each machine of the cluster has eight A100 GPUs connected via PCIe 4.0 with peer-to-peer (P2P) communication disabled. The results of the new experiments are shown in the tables below. \n\n### 1.1 The Results of Training LLMs\n\nThe results below indicate that Megatron V3's tensor parallelism shows better memory efficiency compared to BurstAttention ( since BurstAttention does not shard model parameters ), while BurstAttention has shorter training runtime and lower latency. Moreover, the longer the sequence length is, the more time-efficient BurstAttention is. \n\nFurthermore, combining BurstAttention with ZeRO optimization (ZeRO has very limited benefits over TP since TP itself shards model parameters) brings significant improvements in memory efficiency. Although BurstAttention+ZeRO brings little additional communication overheads, BurstAttention+ZeRO still achieves memory efficiency comparable to Megatron V3 and demonstrates superior speed in both multi-node setups than Megatron V3. This suggests that BurstAttention, with its current optimizations, offers a more efficient solution in terms of speed, even when faced with a memory-efficient competitor like Megatron V3.\n\n#### LLaMA-7b Training Runtime (seconds) on 8xA100 GPUs\n\n| Sequence Length                            | 1024      | 2048      | 4096      | 8192      | 16384     | 32768      |\n|:----------------------------------|:----------|:----------|:----------|:----------|:----------|:-----------|\n| Ring Self-Attention                     | 1.16\u00b10.07 | 0.93\u00b10.03 | 1.49\u00b10.02 | 2.92\u00b10.01 | OOM       | OOM        |\n| TP (Megatron V3) w/ FlashAttention | 1.63\u00b10.11 | 1.37\u00b10.03 | 2.15\u00b10.15 | 3.38\u00b10.08 | 6.93\u00b10.13 | 14.76\u00b10.06 |\n| BurstAttention                    | 1.05\u00b10.09 | 0.77\u00b10.03 | 1.62\u00b10.05 | 3.08\u00b10.06 | 5.18\u00b10.15 | 9.93\u00b10.02  |\n\n#### LLaMA-7b Training Memory Usage (GB) on 8xA100 GPUs\n\n| Sequence Length                            |   1024 |   2048 |   4096 |   8192 | 16384   | 32768   |\n|:----------------------------------|-------:|-------:|-------:|-------:|:--------|:--------|\n| Ring Self-Attention                     |  26.94 |  28.54 |  33.2  |  48.97 | OOM     | OOM     |\n| TP (Megatron V3) w/ FlashAttention |   5.81 |   6.66 |   8.35 |  11.75 | 18.51   | 32.11   |\n| BurstAttention                    |  26.69 |  27.55 |  29.23 |  32.93 | 40.26   | 54.69   |\n\n#### LLaMA-7b Training Runtime (seconds) on 32xA100 GPUs\n\n| Sequence Length                            | 32768      | 65536      | 98304      | 131072     |\n|:----------------------------------|:-----------|:-----------|:-----------|:-----------|\n| TP (Megatron V3) w/ FlashAttention | 17.14\u00b10.10 | 34.95\u00b10.18 | 55.35\u00b10.46 | 73.99\u00b10.23 |\n| BurstAttention                    | 8.93\u00b10.03  | 18.12\u00b10.13 | 28.91\u00b10.05 | 40.61\u00b10.13 |\n| BurstAttention+ZeRO               | 13.81\u00b10.07 | 23.12\u00b10.09 | 33.56\u00b10.07 | 44.22\u00b10.06 |\n\n\n#### LLaMA-7b Training Memory Usage (GB) on 32xA100 GPUs\n\n| Sequence Length                            |   32768 |   65536 |   98304 |   131072 |\n|:----------------------------------|--------:|--------:|--------:|---------:|\n| TP (Megatron V3) w/ FlashAttention |    9.89\u00b10.01 |   18.17\u00b10.01 |   26.43\u00b10.01 |    34.70\u00b10.01  |\n| BurstAttention                    |   35.87\u00b10.01 |   46.15\u00b10.01 |   56.24\u00b10.01 |    66.45\u00b10.01 |\n| BurstAttention+ZeRO               |   11.55\u00b10.01 |   21.83\u00b10.01 |   31.92\u00b10.01 |    42.14\u00b10.01 |\n\n### 1.2 The Results of Inferring LLMs\n\nBesides the results of training LLMs, we also show the results of inferring LLMs. In terms of inferring LLMs, BurstAttention still achieves a speed improvement than Megatron V3. Note that although TP (Megatron V3) is more memory efficient than TP (Megatron V1), the all-reduce operation used by TP (Megatron V1) is better optimized than the reduce-scatter and all-gather operations used by TP (Megatron V3). In the actual inference process, TP (Megatron V1) is even slightly faster than TP (Megatron V3).\n\n\n[To be continued]"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1934/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642739396,
                "cdate": 1700642739396,
                "tmdate": 1700642739396,
                "mdate": 1700642739396,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "f21H9rnYzX",
                "forum": "qDKTMjoFbC",
                "replyto": "vD8J01fdQH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1934/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1934/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to your questions(Part 2)"
                    },
                    "comment": {
                        "value": "#### LLaMA-7b Inference First Token Latency (seconds) on 8xA100 GPUs\n| Sequence Length                 | 4096      | 8192      | 16384     | 32768     | 65536      | 131072     | 262144     |\n|:-----------------------|:----------|:----------|:----------|:----------|:-----------|:-----------|:-----------|\n| Ring Self-Attention    | 0.42\u00b10.01 | 0.87\u00b10.01 | 2.00\u00b10.01 | 5.13\u00b10.05 | OOM        | OOM        | OOM        |\n| TP (Megatron V1) w/ Flash   | 0.67\u00b10.01 | 1.29\u00b10.01 | 2.58\u00b10.01 | 5.27\u00b10.01 | 11.63\u00b10.02 | 27.54\u00b10.01 | 71.52\u00b10.06 |\n| TP (Megatron V3) w/ Flash   | 0.73\u00b10.02 | 1.36\u00b10.01 | 2.68\u00b10.01 | 5.67\u00b10.01 | 12.25\u00b10.01 | 28.73\u00b10.03 | 75.52\u00b10.05 |\n| BurstAttention w/o LAO | 0.46\u00b10.01 | 0.88\u00b10.01 | 1.79\u00b10.01 | 3.88\u00b10.01 | 10.78\u00b10.01 | OOM        | OOM        |\n| BurstAttention         | 0.44\u00b10.01 | 0.84\u00b10.01 | 1.68\u00b10.01 | 3.27\u00b10.01 | 6.49\u00b10.01  | 16.01\u00b10.01 | 49.32\u00b10.11 |\n\n#### LLama-13b Inference First Token Latency (seconds) on 8xA100 GPUs\n| Sequence Length                 | 4096      | 8192      | 16384     | 32768     | 65536      | 131072     | 262144      |\n|:-----------------------|:----------|:----------|:----------|:----------|:-----------|:-----------|:------------|\n| Ring Self-Attention    | 0.66\u00b10.01 | 1.36\u00b10.01 | 3.08\u00b10.01 | 7.98\u00b10.02 | OOM        | OOM        | OOM         |\n| TP (Megatron V1) w/ Flash   | 1.05\u00b10.01 | 2.01\u00b10.01 | 4.03\u00b10.01 | 8.41\u00b10.01 | 18.56\u00b10.02 | 44.39\u00b10.04 | OOM         |\n| TP (Megatron V3) w/ Flash   | 1.07\u00b10.01 | 2.09\u00b10.01 | 4.20\u00b10.01 | 8.76\u00b10.01 | 19.06\u00b10.06 | 45.46\u00b10.03 | 119.03\u00b10.04 |\n| BurstAttention w/o LAO | 0.72\u00b10.01 | 1.39\u00b10.01 | 2.77\u00b10.05 | 5.99\u00b10.01 | 16.95\u00b10.01 | OOM        | OOM         |\n| BurstAttention         | 0.69\u00b10.01 | 1.40\u00b10.05 | 2.57\u00b10.03 | 5.08\u00b10.02 | 9.92\u00b10.01  | 25.91\u00b10.01 | 78.80\u00b10.07  |\n\n\n### 1.3 The Results of Scaling Experiments\n\nSome reviewers suggest that we should provide some experimental results of scaling GPU number, sequence length, and batch size. We list the results of scaling experiments in the tables below.\n\n#### LLama-13b Inference First Token Latency (seconds) with Scaling the GPU Number and Sequence Length.\n\n\n**Part 1: Performance on 1x and 2x GPU Configurations**\n\n| Setting                   | 1xGPUs SeqLen 16384   | 1xGPUs SeqLen 32768   | 1xGPUs SeqLen 65536   | 2xGPUs SeqLen 16384   | 2xGPUs SeqLen 32768   | 2xGPUs SeqLen 65536   |\n|:-------------------------|:----------------------|:----------------------|:----------------------|:----------------------|:----------------------|:----------------------|\n| TP (Megatron V3) w/ Flash | 3.70\u00b10.01             | 10.90\u00b10.01            | 35.52\u00b10.01            | 4.01\u00b10.01             | 9.85\u00b10.01             | 26.45\u00b10.01            |\n| BurstAttention           | 3.86\u00b10.01             | 11.23\u00b10.01            | 36.35\u00b10.02            | 3.52\u00b10.01             | 8.93\u00b10.01             | 24.83\u00b10.01            |\n\n**Part 2: Performance on 4x and 8x GPU Configurations**\n\n| Setting                   | 4xGPUs SeqLen 16384   | 4xGPUs SeqLen 32768   | 4xGPUs SeqLen 65536   | 8xGPUs SeqLen 16384   | 8xGPUs SeqLen 32768   | 8xGPUs SeqLen 65536   |\n|:-------------------------|:----------------------|:----------------------|:----------------------|:----------------------|:----------------------|:----------------------|\n| TP (Megatron V3) w/ Flash | 3.37\u00b10.01             | 7.52\u00b10.01             | 18.36\u00b10.01            | 4.13\u00b10.01             | 8.88\u00b10.01             | 19.08\u00b10.01            |\n| BurstAttention           | 2.66\u00b10.01             | 6.06\u00b10.01             | 15.77\u00b10.01            | 2.73\u00b10.01             | 5.77\u00b10.01             | 13.05\u00b10.01            |\n\n\n#### LLaMA-7b Training Throughput (Token/s) with Scaling the Batch Size\n\n| Setting                 | Batch 1 Seqlen 4096   | Batch 2 Seqlen 4096   | Batch 4 Seqlen 4096   | Batch 8 Seqlen 4096   |\n|:-----------------------|:----------------------|:----------------------|:----------------------|:----------------------|\n| TP (Megatron V3) w/ FlashAttention | 2180.98\u00b123.42 | 2489.12\u00b17.60  | 2592.22\u00b18.21  | 2616.87\u00b112.61 |\n| BurstAttention         | 2551.82\u00b132.24 | 2815.29\u00b19.73  | 3395.43\u00b121.25 | 3955.27\u00b14.80  |\n| RingAttention          | 1800.74\u00b115.99 | 2106.35\u00b116.37 | 2887.44\u00b15.11  | OOM       |\n\nThe scaling experiment results proved that BurstAttention can be more efficient in both training and inferencing.\n\nFor inferencing, BurstAttention improves its performance as the number of GPUs and sequence length increase. This suggests that BurstAttention scales effectively across different hardware configurations and can handle larger sequences efficiently. Also, it demonstrates lower first token latency across various sequence lengths compared to TP (Megatron V1 and V3) with FlashAttention and Ring Self-Attention.\n\n[To be continued]"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1934/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642998227,
                "cdate": 1700642998227,
                "tmdate": 1700642998227,
                "mdate": 1700642998227,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IA7MgfJQMk",
                "forum": "qDKTMjoFbC",
                "replyto": "vD8J01fdQH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1934/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1934/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to your questions(Part 3)"
                    },
                    "comment": {
                        "value": "For training, the results of the LLaMA-7b model training experiments indicate that BurstAttention has higher training throughput compared to the tensor parallelism of Megatron V3 with FlashAttention, especially as the batch size increases. This points to BurstAttention's capability to process larger batches more efficiently, which is crucial for training LLMs effectively.\n\nWhile BurstAttention exhibits impressive performance in various aspects, it's important to acknowledge certain limitations, as highlighted by experimental results. Specifically, it has been observed that in certain scenarios, Megatron V3's tensor parallelism demonstrates a more efficient memory footprint compared to BurstAttention. It's noteworthy that when integrated with the ZeRO (Zero Redundancy Optimizer) technique, BurstAttention not only achieves comparable efficiency but can also surpass Megatron in terms of throughput. This integration effectively leverages the strengths of both technologies, optimizing memory usage while enhancing overall processing speed.\n\n\n### 2. **For Peer-to-Peer (P2P) Communication settings**\n\nFirstly, we don't know that the reviewer is referring to the P2P communication operation such as send-recv or the P2P transport in the PCIe network, which means CUDA direct access between GPUs. We will presume that the reviewer is referring to CUDA direct access between GPUs before we continue the following discussion. However, if the reviewer's focus is indeed on the communication operations such as send-recv, then it should be noted the ring-collective operations are implemented using send-recv operation.\n\nWe acknowledge your observation regarding our experiments' absence of peer-to-peer (P2P) communication. Since we focus on proposing a distributed attention framework that solves extremely long sequences, it is hard for communication across machines to utilize P2P features. Moreover, it is also difficult for the environments of inferring LLMs (often using NVIDIA 4090 or L40 for inference) to meet P2P conditions. Hence, in our experiments, we turn off P2P communication as it is not a common setting in many communication-restricted scenarios (e.g., NVIDIA 3090/4090, NVIDIA A100/V100 PCIe, other non-NVIDIA devices, and the scenarios mentioned above). \n\nIn our latest experiments, we have optimized the original BurstAttention framework by overlapping the computation and communication phases. As a result, we can observe an acceleration in performance, evident in both multi-node and single-node experiments. Intriguingly, this optimization appears to be particularly efficacious in the variant of BurstAttention without FlashAttention, which is inherently more computation-intensive. This observation suggests a significant interplay between computational density and the effectiveness of our communication-computation overlapping strategy, opening avenues for further exploration in the optimization of distributed attention mechanisms.\n\n   \n### 3. **For the Workload Balancing of Causal Language Modeling**:\n   \nRegarding your query about workload balancing, we have not optimized the causal language modeling process in BurstAttention or other experiments. In our current implementation, we adhere to the conventional attention-masking approach without skipping computations. Hence, for both BurstAttention and Megatron V1/V3, the workload is balanced, and the experimental comparison is fair.\n\nIn fact, adopting the conventional attention-masking approach for parallel computing brings unnecessary computational overheads. For future work, we aim to address the workload balancing issue of causal language modeling by exploring more complex communication topologies beyond simple ring communication. A potential strategy we are considering involves repartitioning the sequence dimension across those dimensions unrelated to computation to ensure an even distribution of the attention mask across devices. \n\n### 4. **For the Argument with Sparse Attention**:\n\n\nWe acknowledge your observation that the sparse attention aspect of our work may appear vague. Since implementing sparse attention based on BurstAttention just requires changing the attention masking matrix, we thus only briefly introduce the compatibility of BurstAttention with sparse attention.\n\nThis question is very insightful, especially as it highly relates to the workload imbalance issue. We are still working on addressing the workload imbalance issue associated with causal language modeling, and the imbalance issue is more serious for sparse attention. We are developing more refined strategies to effectively manage this challenge, providing greater clarity and depth in our future iterations. \n\nThanks again for your suggestion, and we will add more discussions about the workload imbalance issue and sparse attention in our revision, which will make our work more solid."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1934/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643036065,
                "cdate": 1700643036065,
                "tmdate": 1700643036065,
                "mdate": 1700643036065,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JyJ8Bfcjpm",
                "forum": "qDKTMjoFbC",
                "replyto": "IA7MgfJQMk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1934/Reviewer_J3Ya"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1934/Reviewer_J3Ya"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for getting back to the important questions, with such a detailed analysis.\n\nI should be more clear on P2P, I actually meant the send/recv in the communication operation. I am bringing up this because these operations are not optimized by themselves. Concretely, if you are using implementing BurstAttention by send/recv, you will likely miss a lot of optimizations in all-gather-like kernels (e.g. topology-aware). This is also pointed out in the RSA paper. This may be less of an issue if you implement overlapping in a good way.\n\nFor the second problem, I don't think my concerns are addressed. Megatron-LM can use flash-attention with causal implementation in the kernel, and it is not fair for them to use an \"attention-masking\" approach. In BurstAttention, you are manipulating in the sequence dimension, and are responsible to recover the causal implementation yourself. This is a key design, vs tensor-parallelism and sequence parallelism over attention.\n\nThe review can not raise score mainly due to the second reason. However, I will reduce my confidence score, and let ACs to judge the significance of the issue."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1934/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694974302,
                "cdate": 1700694974302,
                "tmdate": 1700694974302,
                "mdate": 1700694974302,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hOpv46BFzE",
            "forum": "qDKTMjoFbC",
            "replyto": "qDKTMjoFbC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1934/Reviewer_Jngs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1934/Reviewer_Jngs"
            ],
            "content": {
                "summary": {
                    "value": "The paper aims at processing extremely long sequences in attention under distributed settings. The proposed solution is to partition the computation of attention modules across multiple devices at the sequence dimension, which seems to be an improved version of the prior RingAttention method. Through the proposed global attention optimization using online softmax, the BurstAttention method can reduce the memory usage of intermediate results. In addition, BurstAttention can be integrated with FlashAttention, as the local attention optimization, to further reduce latency and support longer sequences."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well-motivated at addressing the memory and communication challenges of processing extremely long sequences in multi-device settings.\n- The proposed global attention optimization seems to be an effective extension to RingAttention in reducing memory usage."
                },
                "weaknesses": {
                    "value": "While integrating two prior methods (RingAttention and FlashAttention) to be more efficient by itself could be a contribution, the paper could elaborate more on the new challenges, if any, and different method designs to establish its new contributions. In particular, the local attention optimization does not seem to bring any new insights than using FlashAttention in a distributed system."
                },
                "questions": {
                    "value": "- Can you comment on the significance of the reduced latency of the first token? How is the implication on overall latency?\n- What is the new contribution over FlashAttention? Or simply integrating it into the BurstAttention framework?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1934/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1934/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1934/Reviewer_Jngs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1934/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698808105527,
            "cdate": 1698808105527,
            "tmdate": 1699636124409,
            "mdate": 1699636124409,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SKW5DMTvbr",
                "forum": "qDKTMjoFbC",
                "replyto": "hOpv46BFzE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1934/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1934/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to regarding the two questions"
                    },
                    "comment": {
                        "value": "Thank you for your insightful review and valuable questions regarding our paper. We appreciate your recognition of our work's motivation and effectiveness in addressing the challenges of processing extremely long sequences in distributed settings. We would like to address your questions and concerns as follows.\n\n### 1. **The Significance of Reducing the First Token Latency for Inference**:\n\nAlthough the first token latency might represent a small fraction of the overall inference process, it is critically important for real-time AI services such as ChatGPT. In such applications, the system's responsiveness significantly impacts the user experience, and these applications usually output results in a streaming manner to improve responsiveness. Since the first token's latency is the longest, reducing the first token's latency is thus a crucial performance metric, as it directly influences the perceived responsiveness and efficiency of the model in these user-facing streaming scenarios. [1] and [2] also explain why the first token latency is an important metric for LLM serving, which can provide more details.\n   \n[1] https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices\n[2] https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/transformers-neuronx/generative-llm-inference-with-neuron.html\n\n### 2. **The Contributions of BurstAttention over RingAttention and FlashAttention**:\n\nOur main contribution is to build a unified distributed framework for extremely-long sequence parallelism. Under the unified framework, by comprehensively considering the characteristics of clusters and devices, we propose local attention optimization to reduce memory overhead, global attention optimization to reduce communication overhead, and further overlap of global operations and local operations to minimize latency. Rather than an integration of global attention operations and local attention operations, BurstAttention is a significant reimagining and enhancement of how devices interact and function together in a distributed system to perform attention for long sequences. \n\nAlthough we mentioned in the paper that RingAttention and FlashAttention can be seen as the degradation of BurstAttention in specific scenarios, this cannot simply be seen as BurstAttention being a combination of global RingAttention and local FlashAttention. In fact, since RingAttention will first compute all local attention scores and then conduct softmax operations across machines to obtain global results, this attention mode makes it difficult to cooperate with the online softmax operations of FlashAttention. This means that combining RingAttention and FlashAttention is non-trivial. Even if RingAttention and FlashAttention are combined, this simple combination cannot fully utilize the cluster and device characteristics for performance optimization.\n\nIn summary, the unified framework and the derived optimization strategies are central to our contribution, enabling us to specifically optimize the ring-collective operations, fully overlap computation and communication, and process much longer sequences more efficiently than existing baselines. We will add more discussion of this in our revision.\n\n\nThanks again for your suggestion. We hope these responses will clarify the aspects highlighted in your review. We value your feedback and think these suggestions can significantly help us improve our work and its presentation."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1934/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640652029,
                "cdate": 1700640652029,
                "tmdate": 1700640652029,
                "mdate": 1700640652029,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ro3SRzyEhd",
            "forum": "qDKTMjoFbC",
            "replyto": "qDKTMjoFbC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1934/Reviewer_s62K"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1934/Reviewer_s62K"
            ],
            "content": {
                "summary": {
                    "value": "The paper contributes a new sequence parallelism scheme for attending over long sequences that uses GPU memory hierarchy and distributed ring communication to improve inference and training throughput. The authors show the effectiveness of this method on Llama2-7B for training and inference and Llama2-13B for inference, and show improvements over both TP+FlashAttention, which has lower memory footprint but lower throughput, and RingAttention, which has higher memory footprint and higher throughput. The authors also show that their formulation, which uses local and global attention optimization to improve memory footprint and global communication, maintains exact attention correctness while still bringing these performance improvements."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper analysis for training and inference time tradeoff with respect to memory and throughput is solid for causal decoder-only transformer models. The algorithm and communication and memory overhead analysis is also very detailed and thorough for both GAO and LAO."
                },
                "weaknesses": {
                    "value": "The paper only evaluates two decoder-only models at inference time, and a single small model at training time. The scaling law along the sequence dimension is present but a scaling law along the model capacity dimension is also important given that the dimensionality of the Q/K/V will also increase with model size and therefore the communication and memory overhead as well."
                },
                "questions": {
                    "value": "For the tensor parallelism/TP+Flash Attention baselines, is the implementation being used the Megatron-LM implementation which parallelizes each head across devices (and therefore reduces the memory footprint/communication)? \n\nFor the time-to-first-token experiment is the sequence length equivalent to the prompt input length e.g. the model generates the first token conditioned on 4096/8192/.../262144 tokens?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1934/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1934/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1934/Reviewer_s62K"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1934/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698813298163,
            "cdate": 1698813298163,
            "tmdate": 1699636124338,
            "mdate": 1699636124338,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zvQ2YS2cTs",
                "forum": "qDKTMjoFbC",
                "replyto": "Ro3SRzyEhd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1934/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1934/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to your question"
                    },
                    "comment": {
                        "value": "Thank you for your insightful feedback on our submission. We greatly appreciate your recognition of the thorough analysis and detailed algorithmic contributions in our paper. Below, we address each of your concerns and questions to clarify our approach and its implications.\n\n### 1. **For the Details of Tensor Parallelism Implementation**:\n\nYes, we use the tensor parallelism implementation of Megatron for our experiments, which parallelizes attention heads across devices to reduce the memory footprint. However, even with head-wise parallelization, Megatron V1's tensor parallelism still exhibits substantial memory usage. Notably, the communication volume in Megatron V1's attention mechanism remains significant, particularly the all-reduce operation to obtain attention outputs. Hence, the communication and memory overheads are not notably reduced by parallelizing along the attention head dimension. More importantly, the increase of memory overhead brought by parameter growth is linear, while the rise brought by sequence length growth is quadratic, i.e., the core memory overhead is to store attention intermediate results rather than storing parameters in processing extremely long sequences, which cannot be solved by only parallelizing attention heads.\n\nIn supplementary experiments, we compare our method with the tensor parallelism approach in Megatron V3, which employs all-gather and reduce-scatter operations instead of all-reduce operations in Megatron V1. For Megatron V3's approach, tensor parallelism's communication volume remains akin to that of Megatron V1, but the memory footprint is significantly reduced. These supplementary results can refer to the responses to Reviewer Gb5f and Reviewer J3Ya.\n\n### 2. **For the First Token Latency**:\n\nRegarding your question on the first token latency, you are correct. This metric indeed refers to the time taken for the first inference step, specifically the encoding time. It is an essential measure since the computation for long sequence attention primarily occurs during the encoding phase in inference. In real-time AI services such as ChatGPT, the system's responsiveness significantly impacts the user experience, and these applications usually output results in a streaming manner to improve responsiveness. Since the first token's latency is the longest, reducing the first token's latency is thus a crucial performance metric, as it directly influences the perceived responsiveness and efficiency of the model in these streaming scenarios. [1] and [2] also explain why the first token latency is an important metric for LLM serving, which can provide more details.\n   \n[1] https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices\n[2] https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/appnotes/transformers-neuronx/generative-llm-inference-with-neuron.html\n\n### 3. **For the Scaling Law Along the Model Capacity Dimension**: \n\nWe acknowledge the importance of evaluating the scaling law along the model capacity dimension. In this paper, our analysis primarily focuses on the O(N^2) attention complexity and its optimization through BurstAttention. To clearly evaluate the effectiveness and efficiency of different methods to handle extremely long attention, we choose a fixed model size for our experiments, which can bring an optimization perspective that is independent of the trade-offs introduced by parallel methods and optimizations.\n\nWe understand that experiments on larger models would require additional optimization strategies, and the performance bottlenecks may vary based on different hardware configurations. Therefore, simply comparing the performance on larger models and clusters without these considerations may not yield clear insights. We agree this suggestion is meaningful and plan to extend our analysis to larger models in future work, taking into account these complex optimization and device factors."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1934/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642081618,
                "cdate": 1700642081618,
                "tmdate": 1700642081618,
                "mdate": 1700642081618,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wLWHFiR5QB",
                "forum": "qDKTMjoFbC",
                "replyto": "zvQ2YS2cTs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1934/Reviewer_s62K"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1934/Reviewer_s62K"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response. The clarification on these points is helpful. Without analysis on larger models, I still have concerns that this method may not scale as suggested in the paper. I will maintain my score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1934/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690984023,
                "cdate": 1700690984023,
                "tmdate": 1700690984023,
                "mdate": 1700690984023,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nipdrI5D1d",
            "forum": "qDKTMjoFbC",
            "replyto": "qDKTMjoFbC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1934/Reviewer_Gb5f"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1934/Reviewer_Gb5f"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents BurstAttention, a method for partitioning the attention operation across multiple GPUs in order to process very long sequences. The method combines a global step, which computes attention using a ring-based method and online softmax, with a blocked local attention implementation to take advantage of on-chip cache. The memory, local I/O, and communication costs of BurstAttention are analyzed. Finally, BurstAttention is evaluated experimentally on LLaMA-2 7- and 13-billion parameter models for inference and training performance, where BurstAttention outperforms other methods such as RingAttention."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Long sequence lengths are a critical problem for LLMs, and sparse attention mechanisms may not always be sufficient to tackle them. This paper is helping to address this, both in inference and training, by providing a method to decompose the attention computation among multiple GPUs.\n2. The combination of the global and local attention optimizations, plus online softmax, enable improved performance in the paper's benchmark results.\n3. The benchmark results compare with several existing methods for high-performance attention computation, including FlashAttention and RingAttention."
                },
                "weaknesses": {
                    "value": "1. The novelty of the paper is unclear. It seems to me that it is essentially a straightforward combination of three existing ideas: RingAttention (Li et al., 2022); online softmax (Milakov & Gimelshein, 2018); and FlashAttention (Dao et al., 2022). Further, the ring-based approach to attention seems essentially to be a variant on classic 1D distributed matrix-matrix multiplication algorithms or Cannon's algorithm with one dimension of the mesh trivial (see, e.g., Golub's \"Matrix Computations\" book, or the Cannon or Fox algorithm). One could imagine pipelining the distributed attention computation (it is unclear to me whether this paper does this), but then it still seems very similar to SUMMA (Van De Geijn & Watts, 1997).\n2. The analysis section seems quite perfunctory. It adequately describes the characteristics of the particular implementation, but does not address optimality. E.g., there are lower bounds for distributed matrix-matrix multiplication (via pebbling and in communication-avoiding algorithms) that could be used. Similarly, FlashAttention includes a lower-bound analysis that might be leveraged here.\n3. The experimental evaluation does not include any error bars or measures of variance. Especially since this includes cross-device communication, and BurstAttention introduces additional communication on top of existing methods (e.g., tensor or data parallelism) which may interfere, these are important to include.\n4. The experimental section lacks any detailed analysis of the algorithm and includes only end-to-end results. For example, what percent of peak device flop was sustained? How much communication is performed? What is the breakdown in runtime of communication and computation? How well does the local attention optimization perform (e.g., reduction in cache misses)? These sorts of analyses would help inform how BatchAttention performs in different scenarios, what its bottlenecks are, and why, precisely, it is faster than existing methods. (Further, the paper states in Section 1 that BurstAttention \"can fully optimize the input-output (I/O) and communication procedures in distributed clusters\", yet does not back this claim up.)\n5. The evaluation is conducted only up to 8 GPUs which are interconnected via PCIe. This is not especially large scale for training LLMs (although could be reasonable for fine-tuning), and it is not clear how the method will scale."
                },
                "questions": {
                    "value": "1. Please clarify the novelty of the paper (see above).\n2. Can you contextualize the analysis in Section 4 with lower bounds or otherwise provide an indication of how far BurstAttention is from optimal?\n3. Please add error bars (or similar) to all experimental results.\n4. Please expand the experimental analysis; see above for suggestions.\n5. How does BurstAttention scale to larger numbers of GPUs or on different networks?\n6. A related point, how does BurstAttention perform when combined with other standard parallelism techniques for training, e.g., data-parallelism/FSDP, pipelining, etc.? Does its additional communication cause interference?\n7. In Section 5.3, why fix the batch size to 1 for training? This seems quite small.\n8. In Section 5.4, why is a different sequence length used for scaling than is presented in the earlier results? This makes it hard to contextualize the scaling.\n\n-----\n\nIn light of the authors' response addressing some of my concerns, I have raised my score. However, I still remain concerned about the novelty and depth of the experiments."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1934/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1934/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1934/Reviewer_Gb5f"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1934/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698887391710,
            "cdate": 1698887391710,
            "tmdate": 1700761799599,
            "mdate": 1700761799599,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WTWcaIXGeR",
                "forum": "qDKTMjoFbC",
                "replyto": "nipdrI5D1d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1934/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1934/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to your questions(Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for your insightful comments and questions regarding our manuscript. Your feedback is invaluable in enhancing the quality and comprehensiveness of our research. We have carefully considered each point you raised and provide the following responses.\n\n### 1. **For the Novelty**\n\nOur main contribution is to build a unified distributed framework for extremely-long sequence. Under the unified framework, by comprehensively considering the characteristics of clusters and devices, we propose local attention optimization to reduce memory overhead, global attention optimization to reduce communication overhead, and further overlap of global operations and local operations to minimize latency. Rather than an integration of global attention operations and local attention operations, BurstAttention is a significant reimagining and enhancement of how devices interact and function together in a distributed system to perform attention for long sequences. \n\nAlthough we mentioned in the paper that RingAttention and FlashAttention can be seen as the degradation of BurstAttention in specific scenarios, this cannot simply be seen as BurstAttention being a combination of global RingAttention and local FlashAttention. In fact, since RingAttention will first compute all local attention scores and then conduct softmax operations across machines to obtain global results, this attention mode makes it difficult to cooperate with the online softmax operations of FlashAttention. This means that combining RingAttention and FlashAttention is non-trivial. Even if RingAttention and FlashAttention are combined, this simple combination cannot fully utilize the cluster and device characteristics for performance optimization.\n\nIn summary, the unified framework and the derived optimization strategies are central to our contribution, enabling us to specifically optimize the ring-collective operations, fully overlap computation and communication, and process much longer sequences more efficiently than existing baselines. We will add more discussion of this in our revision.\n\n\n### 2. **For More Experiments and Analysis**\n\nThanks for your insightful suggestion, we have added some new experiments to make our claim more clear. And we also take the suggestion of Reviewer J3Ya and conduct new experiments for comparison with Megatron V3. For the new experiments, we replace the original tensor parallelism with the implementation of Megatron V3, an implementation of tensor parallelism that considers the features of sequence parallelism. These results can refer to 2.1. \n \nFurthermore, we have expand our training experiments on 32 A100 GPUs. All these experiments are conducted on a cluster interconnected by a RoCE (RDMA over Converged Ethernet) network with an approximate bandwidth of 200 Gb/s, and each machine of the cluster has eight A100 GPUs connected via PCIe 4.0 with peer-to-peer (P2P) communication disabled. The results of the new experiments are shown in the tables below. \n\n#### For Error Bars\n\nThank you for your insightful suggestion. Our experiments in the paper are the average of multiple experiments, yet we neglect to give error bars as the results of our multiple experiments are very close. We have followed your suggestion to add error bars for both existing experiments and new experiments. Thanks again for your suggestion.\n\n#### For Cluster Scale\n\nWe acknowledge your observation regarding the limited scale of the cluster used in our experiments. We agree that evaluating our BurstAttention framework on a larger-scale cluster would be beneficial for further validating its scalability and efficiency with longer sequences. The initial experiments were conducted on a modestly sized cluster to demonstrate the feasibility and efficacy of our approach.\n\nOur expanded experiments on this 32-GPU setup, with the training length extended to 128k, have demonstrated notable improvements in speed over the tensor parallel approach, especially under these specific network and hardware configurations. Even models with hundreds of billions of parameters like GPT-3 rarely use more than 8 GPUs for inference, so this 32-GPU experiments are only conducted on training. These results underscore the robustness and scalability of our method, affirming its effectiveness in a real-world, high-performance computing environment. \n\n[To be continued]"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1934/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717893886,
                "cdate": 1700717893886,
                "tmdate": 1700717893886,
                "mdate": 1700717893886,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "i1MuleeerQ",
                "forum": "qDKTMjoFbC",
                "replyto": "nipdrI5D1d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1934/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1934/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "#### The Detailed Analysis of Experiments Results\nWe sincerely appreciate your valuable suggestion regarding the need for more detailed experimental analysis. We understand that fine-grained analysis can offer deeper insights into the practical bottlenecks of BurstAttention.\n\nHowever, in the context of multi-device distributed training, we have overlapped communication and computation. This overlapping is especially pertinent in communication-constrained scenarios, which are our primary focus. In such scenarios, computation are often subsumed within communication times, rendering traditional metrics like sustained flop percentage less applicable. Furthermore, our algorithm includes non-matmul FLOPs, which further complicates the use of flop-based metrics for comprehensive performance assessment.\n\nGiven these considerations, we opted for end-to-end results as our primary performance indicator. This metric directly reflects the extent of optimization achieved by BurstAttention.\n\nNevertheless, we understand that fine-grained would help to explain where the bottleneck is and how BurstAttention sloved it. Moving forward, we plan to conduct additional experiments on a variety of cluster configurations and attempt to pinpoint the optimal scenarios for BurstAttention's deployment. This will involve investigating how different input shapes, GPU performances, and communication bandwidths impact the effectiveness of BurstAttention compared to other algorithms. In the course of these experiments, we will endeavor to include more detailed profiling of BurstAttention to provide a clearer understanding of its performance characteristics.\n\n#### For the Combination of BurstAttention and Data Parallelism/FSDP\n\nSince our framework is for both training and inference, and data/pipeline parallelism methods are seldomly adopted for inference, we mainly focus on comparing our framework with existing sequence and tensor parallelism methods. In our new experiments, we combine BurstAttention and ZeRO together to show the effect of combing BurstAttention and Data Parallelism/FSDP, and these results can refer to 2.1. \n\nNote that, combing data/pipeline methods and tensor/sequence parallelism methods only introduces additional communication costs, and has no side effects on our analysis on attention with extremely long sequences.\n\n\n\n#### For the Choice of Batch Size and Sequence Length\n\nFor training LLMs, the total tokens of a batch are required to be between 2M and 4M, otherwise, the model performance may be degraded. This means that the longer the training sequence length is, the smaller the batch size is. Due to this, several GPUs may need to compute one example. For example, using 2048 GPUs to train 128-layer GPT-3, the sequence length is 4096, the batch size is 1024, data parallelism is 16, pipeline parallelism is 32, and tensor parallelism is 4. In this scenario, the optimal setup is to divide a batch into 64 micro-batches with a micro-batch size of 1. In this case, four GPUs under the tensor parallelism group are inevitably required to calculate one piece of data together. In view of this, in our paper, we use batch size = 1 for our experiments. \n\nTo make our experimental results more sufficient, we follow the reviewers' suggestions and add more experiments with scaling batch and sequence sizes. These results are shown in 2.3.\n\n### 2.1 The Results of Training LLMs\n\nThe results below indicate that while Megatron V3's tensor parallelism shows better memory efficiency compared to BurstAttention, since BurstAttention does not do model parameters' sharding, BurstAttention has shorter training runtime and better time efficiency. Moreover, the longer the sequence length, the more time-efficient BurstAttention is.\n\nFurthermore, combining BurstAttention with ZeRO optimization (ZeRO has very limited benefits over TP since TP itself shards model parameters) brings significant improvements in memory efficiency. Although BurstAttention+ZeRO brings little additional communication overheads, BurstAttention+ZeRO still achieves memory efficiency comparable to Megatron V3 and demonstrates superior speed in both multi-node and single-node setups than Megatron V3. This suggests that BurstAttention, with its current optimizations, offers a more efficient solution in terms of speed, even when faced with a memory-efficient competitor like Megatron V3.\n\n\n[To be continued]"
                    },
                    "title": {
                        "value": "Response to your questions(Part 2)"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1934/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717983903,
                "cdate": 1700717983903,
                "tmdate": 1700718000835,
                "mdate": 1700718000835,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "32i6wNQOFD",
                "forum": "qDKTMjoFbC",
                "replyto": "nipdrI5D1d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1934/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1934/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to your questions(Part 3)"
                    },
                    "comment": {
                        "value": "#### LLama-7b Training Runtime (seconds) on 8xA100 GPUs\n\n| Sequence Length                            | 1024      | 2048      | 4096      | 8192      | 16384     | 32768      |\n|:----------------------------------|:----------|:----------|:----------|:----------|:----------|:-----------|\n| Ring Self-Attention                     | 1.16\u00b10.07 | 0.93\u00b10.03 | 1.49\u00b10.02 | 2.92\u00b10.01 | OOM       | OOM        |\n| TP(Megatron V3) w/ FlashAttention | 1.63\u00b10.11 | 1.37\u00b10.03 | 2.15\u00b10.15 | 3.38\u00b10.08 | 6.93\u00b10.13 | 14.76\u00b10.06 |\n| BurstAttention                    | 1.05\u00b10.09 | 0.77\u00b10.03 | 1.62\u00b10.05 | 3.08\u00b10.06 | 5.18\u00b10.15 | 9.93\u00b10.02  |\n\n#### LLama-7b Training Memory Usage (GB) on 8xA100 GPUs\n\n| Sequence Length                            |   1024 |   2048 |   4096 |   8192 | 16384   | 32768   |\n|:----------------------------------|-------:|-------:|-------:|-------:|:--------|:--------|\n| Ring Self-Attention                     |  26.94 |  28.54 |  33.2  |  48.97 | OOM     | OOM     |\n| TP(Megatron V3) w/ FlashAttention |   5.81 |   6.66 |   8.35 |  11.75 | 18.51   | 32.11   |\n| BurstAttention                    |  26.69 |  27.55 |  29.23 |  32.93 | 40.26   | 54.69   |\n\n#### LLama-7b Training Runtime (seconds) on 32xA100 GPUs\n\n| Sequence Length                            | 32768      | 65536      | 98304      | 131072     |\n|:----------------------------------|:-----------|:-----------|:-----------|:-----------|\n| TP(Megatron V3) w/ FlashAttention | 17.14\u00b10.10 | 34.95\u00b10.18 | 55.35\u00b10.46 | 73.99\u00b10.23 |\n| BurstAttention                    | 8.93\u00b10.03  | 18.12\u00b10.13 | 28.91\u00b10.05 | 40.61\u00b10.13 |\n| BurstAttention+ZeRO               | 13.81\u00b10.07 | 23.12\u00b10.09 | 33.56\u00b10.07 | 44.22\u00b10.06 |\n\n\n#### LLama-7b Training Memory Usage (GB) on 32xA100 GPUs\n\n| Sequence Length                            |   32768 |   65536 |   98304 |   131072 |\n|:----------------------------------|--------:|--------:|--------:|---------:|\n| TP(Megatron V3) w/ FlashAttention |    9.89\u00b10.01 |   18.17\u00b10.01 |   26.43\u00b10.01 |    34.70\u00b10.01  |\n| BurstAttention                    |   35.87\u00b10.01 |   46.15\u00b10.01 |   56.24\u00b10.01 |    66.45\u00b10.01 |\n| BurstAttention+ZeRO               |   11.55\u00b10.01 |   21.83\u00b10.01 |   31.92\u00b10.01 |    42.14\u00b10.01 |\n\n\n### 2.2 The Results of Inferring LLMs\n\nBesides the results of training LLMs, we also show the results of inferring LLMs. In terms of inferring LLMs, BurstAttention still achieves a speed improvement than Megatron V3. Note that, although TP(Megatron V3) is more memory efficient than TP(Megatron V1), the all-reduce operation used by TP(Megatron V1) is better optimized than the reduce-scatter and all-gather operations used by TP(Megatron V3). In the actual inference process, TP(Megatron V1) is even slightly faster than TP(Megatron V3).\n\n#### LLama-7b Inference First Token Latency (seconds) on 8xA100 GPUs\n| Sequence Length                 | 4096      | 8192      | 16384     | 32768     | 65536      | 131072     | 262144     |\n|:-----------------------|:----------|:----------|:----------|:----------|:-----------|:-----------|:-----------|\n| Ring Self-Attention    | 0.42\u00b10.01 | 0.87\u00b10.01 | 2.00\u00b10.01 | 5.13\u00b10.05 | OOM        | OOM        | OOM        |\n| TP(Megatron V1) w/ Flash   | 0.67\u00b10.01 | 1.29\u00b10.01 | 2.58\u00b10.01 | 5.27\u00b10.01 | 11.63\u00b10.02 | 27.54\u00b10.01 | 71.52\u00b10.06 |\n| TP(Megatron V3) w/ Flash   | 0.73\u00b10.02 | 1.36\u00b10.01 | 2.68\u00b10.01 | 5.67\u00b10.01 | 12.25\u00b10.01 | 28.73\u00b10.03 | 75.52\u00b10.05 |\n| BurstAttention w/o LAO | 0.46\u00b10.01 | 0.88\u00b10.01 | 1.79\u00b10.01 | 3.88\u00b10.01 | 10.78\u00b10.01 | OOM        | OOM        |\n| BurstAttention         | 0.44\u00b10.01 | 0.84\u00b10.01 | 1.68\u00b10.01 | 3.27\u00b10.01 | 6.49\u00b10.01  | 16.01\u00b10.01 | 49.32\u00b10.11 |\n\n#### LLama-13b Inference First Token Latency (seconds) on 8xA100 GPUs\n| Sequence Length                 | 4096      | 8192      | 16384     | 32768     | 65536      | 131072     | 262144      |\n|:-----------------------|:----------|:----------|:----------|:----------|:-----------|:-----------|:------------|\n| Ring Self-Attention    | 0.66\u00b10.01 | 1.36\u00b10.01 | 3.08\u00b10.01 | 7.98\u00b10.02 | OOM        | OOM        | OOM         |\n| TP(Megatron V1) w/ Flash   | 1.05\u00b10.01 | 2.01\u00b10.01 | 4.03\u00b10.01 | 8.41\u00b10.01 | 18.56\u00b10.02 | 44.39\u00b10.04 | OOM         |\n| TP(Megatron V3) w/ Flash   | 1.07\u00b10.01 | 2.09\u00b10.01 | 4.20\u00b10.01 | 8.76\u00b10.01 | 19.06\u00b10.06 | 45.46\u00b10.03 | 119.03\u00b10.04 |\n| BurstAttention w/o LAO | 0.72\u00b10.01 | 1.39\u00b10.01 | 2.77\u00b10.05 | 5.99\u00b10.01 | 16.95\u00b10.01 | OOM        | OOM         |\n| BurstAttention         | 0.69\u00b10.01 | 1.40\u00b10.05 | 2.57\u00b10.03 | 5.08\u00b10.02 | 9.92\u00b10.01  | 25.91\u00b10.01 | 78.80\u00b10.07  |\n\n[To be continued]"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1934/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718051920,
                "cdate": 1700718051920,
                "tmdate": 1700718187129,
                "mdate": 1700718187129,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wgr6OnbcL9",
                "forum": "qDKTMjoFbC",
                "replyto": "nipdrI5D1d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1934/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1934/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to your questions(Part 4)"
                    },
                    "comment": {
                        "value": "### 2.3 The Results of Scaling Experiments\n\nSome reviewers suggest us to provide some experimental results of scaling GPU number, sequence length, and batch size. We list the results of scaling experiments in the tables below.\n\n#### LLama-13b Inference First Token Latency (seconds) with Scaling the GPU Number and Sequence Length.\n\n| Setting                   | 1xGPUs SeqLen 16384   | 1xGPUs SeqLen 32768   | 1xGPUs SeqLen 65536   | 2xGPUs SeqLen 16384   | 2xGPUs SeqLen 32768   | 2xGPUs SeqLen 65536   | 4xGPUs SeqLen 16384   | 4xGPUs SeqLen 32768   | 4xGPUs SeqLen 65536   | 8xGPUs SeqLen 16384   | 8xGPUs SeqLen 32768   | 8xGPUs SeqLen 65536   |\n|:-------------------------|:----------------------|:----------------------|:----------------------|:----------------------|:----------------------|:----------------------|:----------------------|:----------------------|:----------------------|:----------------------|:----------------------|:----------------------|\n| TP(Megatron V3) w/ Flash | 3.70\u00b10.01             | 10.90\u00b10.01            | 35.52\u00b10.01            | 4.01\u00b10.01             | 9.85\u00b10.01             | 26.45\u00b10.01            | 3.37\u00b10.01             | 7.52\u00b10.01             | 18.36\u00b10.01            | 4.13\u00b10.01             | 8.88\u00b10.01             | 19.08\u00b10.01            |\n| BurstAttention           | 3.86\u00b10.01             | 11.23\u00b10.01            | 36.35\u00b10.02            | 3.52\u00b10.01             | 8.93\u00b10.01             | 24.83\u00b10.01            | 2.66\u00b10.01             | 6.06\u00b10.01             | 15.77\u00b10.01            | 2.73\u00b10.01             | 5.77\u00b10.01             | 13.05\u00b10.01            |\n\n#### LLama-7b Training Throughput (Token/s) with Scaling the Batch Size\n\n| Setting                 | Batch 1 Seqlen 4096   | Batch 2 Seqlen 4096   | Batch 4 Seqlen 4096   | Batch 8 Seqlen 4096   |\n|:-----------------------|:----------------------|:----------------------|:----------------------|:----------------------|\n| TP(Megatron V3) w/ FlashAttention | 2187.12\u00b125.86         | 2229.40\u00b1159.50        | 2203.90\u00b15.73          | 2452.88\u00b14.58          |\n| BurstAttention         | 2521.53\u00b130.34         | 2660.22\u00b170.99         | 2966.01\u00b1270.85        | 3525.83\u00b147.52         |\n\n\n\n### 3. **For the Analysis Section**\n\nThank you for your insightful suggestion. In our paper, since we mainly focus on the practical performance of BurstAttention, we thus briefly give the communication and memory complexities of different distributed methods and then compare the end-to-end results of these methods. We highly agree that contextualizing our analysis with lower bounds or comparative benchmarks would provide a clearer understanding of BurstAttention's efficiency relative to theoretical optima.\n\nIn recent days, we have added more detailed experiments based on reviewer suggestions. In the future, we will further enrich our analysis to include a more detailed examination of the efficiency of BurstAttention, providing insights from a theoretical perspective as well as comparisons with existing benchmarks. These more detailed analyses will be included in our further revisions.\n\n\nWe are grateful for the opportunity to address these points and hope that our responses clarify the choices made in our research. We remain open to further suggestions and are committed to continual improvement of our work."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1934/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718116107,
                "cdate": 1700718116107,
                "tmdate": 1700718116107,
                "mdate": 1700718116107,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]