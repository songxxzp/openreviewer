[
    {
        "title": "Visual Grounding with attention-driven constraint balancing"
    },
    {
        "review": {
            "id": "5mohXHBvNM",
            "forum": "uikf2Ue0XQ",
            "replyto": "uikf2Ue0XQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1978/Reviewer_qRXz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1978/Reviewer_qRXz"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on improving Visual Grounding using Transformer-based models. They first analyze the correlation between the model fusion/decoder layer's attention score within the ground truth bounding box and the corresponding IoU value for multiple benchmarks. Based on their observations, they propose two objectives: (1) Rho-modulated Attention Constraint (RAC); and (2) \nMomentum Rectification Constraint (MRC) for attention regularization. RAC constraints the model to generate attention maps focusing on the text included in the bounding box. MRC is a momentum distillation module for rectifying RAC in cases in which background information is needed. They also introduce Difficulty Adaptive Training to make the model pay more attention to hard samples. Finally, they conduct experiments using multiple transformer-based models on several benchmarks including RefCOCO, RefCOCO+, and RefCOCO-g. Their experimental results show better performances than the baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) They design the objectives from the analysis and observations, which makes the design easy to follow and intuitive. \n(2) The proposed RAC, MRC loss, and the DAT training strategy can be adapted to different transformer-based visual grounding models, therefore the methods are general to use in the future. \n(3) They conduct comprehensive ablation experiments to show each proposed element is needed to get the best performances on multiple benchmarks. Also, their main results show consistent improvements over these benchmarks using different backbone models."
                },
                "weaknesses": {
                    "value": "(1) The benchmarks used in the paper are RefCOCO, RefCOCO+ and RefCOCO-g. But I see in the supplementary material the authors also try ReferItGame and Flicker30K. Could you also provide additional experimental results on these benchmarks to compare with the baselines?\n(2) By adding the RAC and MRC, how long did you train the model? Can you compare it with the baseline training time as well?"
                },
                "questions": {
                    "value": "Please answer the questions in Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1978/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698685753388,
            "cdate": 1698685753388,
            "tmdate": 1699636129540,
            "mdate": 1699636129540,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NoljPQ70Mq",
                "forum": "uikf2Ue0XQ",
                "replyto": "5mohXHBvNM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1978/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1978/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "# Response 4.1: Response to Weakness\u00a01 about additional experimental results.\nThank you for your inquiry regarding additional experimental results. First, we would like to clarify the reason we did not conduct experiments on ReferItGame and Flicker30K in our paper. However, we are pleased to provide the additional experiment results you requested on ReferItGame.\u00a0Due to the enormity of the Flicker30K training set and the limited time and training resources during the rebuttal period, we cannot guarantee the completion of all experiments during the rebuttal phase. However, we will include all experiments in the final version's appendix.\n\n**Reasons\u00a0for omitting experiments on ReferItGame and Flicker30K:** Flicker30K is primarily a phrase grounding benchmark, and it may not fully represent the challenges of Visual Grounding characterized by complex and free-form language expression. This distinction is evident in MDETR\u00a0[1], where Table 2 in their paper outlines benchmarks for Visual Grounding (also known as referring expression comprehension) using RefCOCO, RefCOCO+, and RefCOCOg, akin to our approach. However, Table 3 in their paper is dedicated to the phrase grounding task using Flicker30K. Similar configurations are observed in SiRi, where validation is conducted solely on RefCOCO, RefCOCO+, and RefCOCOg. Considering the scope of our experiment, which involves evaluating five models on four benchmarks, resulting in 45 quantitative results and numerous ablation studies, additional validation on ReferItGame and Flicker30K would extend beyond the intended experiment scope. Specifically, Flicker30K comprises over 427k training samples.\n\n\n**Additional experiments on ReferItGame:** Nevertheless, to address your concerns to a certain extent, we performed an additional experiment on ReferItGame. Here, we reproduced the TransVG model and conducted an experiment with TransVG(+AttBalance). The results, presented in the table below, demonstrate that our AttBalance on ReferItGame indeed results in a significant improvement.\n|                | TransVG | TransVG(+AttBalance) |\n|----------------|---------|-----------------------|\n| ReferItGame val | 71.94   | 73.84                 |\n| ReferItGame test| 69.69   | 70.57                 |\n\n[1] MDETR -- Modulated Detection for End-to-End Multi-Modal Understanding. ICCV 2021.\n\n# Response 4.2: Response to Weakness\u00a02\u00a0about training time.\nThank you for your question regarding the training time for our model with the inclusion of RAC and MRC compared to the baseline. We appreciate your interest in this aspect of our work. To address your question, we investigate the training time consumption of both the baseline (TransVG) and our method (TransVG(+AttBalance)) using the experiment results from the table provided earlier. It is essential to note that all experiments were conducted on 8 Quadro RTX 6000 GPUs with the same CUDA and PyTorch environment. As demonstrated in the table below, the additional training time introduced by our AttBalance falls within a reasonable range.\n\n|               | TransVG          | TransVG(+AttBalance) |\n|---------------|------------------|-----------------------|\n| Training time | 11 hours 05 mins | 13 hours 53 mins      |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1978/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700278907194,
                "cdate": 1700278907194,
                "tmdate": 1700278907194,
                "mdate": 1700278907194,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lJt8MlALLV",
            "forum": "uikf2Ue0XQ",
            "replyto": "uikf2Ue0XQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1978/Reviewer_zMiJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1978/Reviewer_zMiJ"
            ],
            "content": {
                "summary": {
                    "value": "This study first pinpoints a general problem that the loss function adopted in visual grounding tasks does not differ from that in object detection tasks, failing to consider the alignment between the language expressions and the visual features. To that end, the authors propose an attention-driven constraint balancing (AttBalance) module to explicitly constrain attention to focus more in the range of the bounding box. Experimental results are presented on various datasets using different models to validate the effectiveness of AttBalance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors point out an important problem that loss functions in visual grounding is not specially designed to consider vision-language interactions.\n    \n- An attention balance method is proposed based on the found positive correlation between the attention value inside a bounding box and the model\u2019s performance.\n    \n- The proposed module is able to achieve performance gain across different methods on major visual grounding tasks."
                },
                "weaknesses": {
                    "value": "- My major concern is about the intuitive motivation of this study. The authors argue that \u201chigher attention values within the ground truth bounding box (bbox) generally indicate a better overall performance\u201d through two individual experiments. Specifically, a Spearman\u2019s rank correlation between the attention values and the models\u2019s predicted IoU is shown to indicate the positive correlation between the model\u2019s prediction and the attention value. Even though the results using TransVG-R101 generally comply with the assumption, the results using VLTVG-R101 barely do so. The trend on unc testA even contradicts the argument.\n    \n- Moreover, one could list a lot of cases where the attention values inside a bounding box do not necessarily correlate with the models' performance. Take small objects for example, the most influential factors would lie in the background for determining the semantics of the object instead of the attention inside the bounding box. Therefore, the motivation of this study fails to persuade the reviewer of the effectiveness of the proposed module in various scenarios.\n    \n- The introduced regulation on the attention value is to guide the attention to focus more on the area inside a bounding box. Even though the authors assert that the loss could apply to different architectures, more hyperparameters are also introduced which would also hurt its transferability on other visual grounding models. For example, in the sentence \u201cwe partition the attention values within the ground truth region of VLTVG\u2019s last layer into 8 equal number parts, omitting the extreme intervals, i.e. those less than 0.1 or greater than 0.9\u201d, one would identify at least 3 hyperparameters whose ablation is also missing in the manuscript.\n    \n- Some major writing issues. Each figure annotation should explain itself whereas the annotations in this study e.g., Figure 1 and Figure 3 to too short to comprehend without referring to the main manuscript.\n    \n- Minor writing issues, e.g., \u201canalyzing\u201d -> \u201canalyze\u201d in 4.3. The authors are encouraged to further proofread the manuscript."
                },
                "questions": {
                    "value": "The major concerns would be how to prove its motivation in general. I would like to also raise some other question regarding the experimental details.\n\n- In regards to the ablation study, it is weird that only applying MRC even causes degradation of the performance. In contrast, it is able to largely benefit the model when combined with RAC. This is an interesting yet strange phenomenon, especially given that the feature map distillation from a moving-averaged teacher has been effective on various self-supervised/semi-supervised task. This is not well addressed in the manuscript with only one sentence \u201c \u201csuggesting that merely smoothing the attention behavior by MomModal does not bring benefits\u201d"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1978/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698754701346,
            "cdate": 1698754701346,
            "tmdate": 1699636129457,
            "mdate": 1699636129457,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9LYTgsB2LU",
                "forum": "uikf2Ue0XQ",
                "replyto": "lJt8MlALLV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1978/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1978/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "# Response 3.1: Response to Weakness\u00a01\u00a0about the Spearman\u2019s rank correlation.\nCertainly, we appreciate your attention to Spearman's rank correlation results, particularly in the case of VLTVG-R101 on unc testA. We would like to provide clarification on a few things:\n\n* Spearman's rank correlation (rho) measures the statistical dependence between the rankings of two variables. A positive value of rho indicates the positive association between the ranks of the two variables. In the case of VLTVG-R101 on unc testA, even though the results show a minimum correlation of 0.35, this still indicates a positive correlation. Therefore, it aligns with the conclusions drawn in our ANALYSIS section, specifically Conclusions 1 and 2, stating that higher attention values inside the bounding box generally imply better performance. It is important to note that this is a general trend and not an absolute rule for every data sample.\n\n* Furthermore, the observed variations in correlation degrees across layers, models, and datasets, as indicated by the different trends in each line, support our Conclusion 3. This conclusion highlights that the correlation degree is not uniform and follows no predefined pattern. Given these observations, we propose adapting the scaling of the RAC using Spearman's rank correlation (rho) factor. This adaptive scaling approach aims to account for the variations observed in correlation degrees across different scenarios, eliminating the need for manually designed hyperparameters.\n\n# Response 3.2: Response to Weakness\u00a02\u00a0about the motive regarding Attention variability across different scenarios.\nI appreciate your thoughtful consideration and concern regarding the correlation between attention values inside a bounding box and model performance, especially in scenarios involving small objects. Your observation raises a valid point, and we would like to provide further clarification:\n\n* Detecting a target object intuitively involves the model attending to the region of the object. However, as you rightly pointed out, background factors can also significantly influence the final prediction, especially in cases involving small objects. Determining whether the foreground or background matters more is challenging, particularly in visual grounding scenarios where the text is complex and contains many background clues. This nuanced understanding aligns with the findings from our ANALYSIS, where all Spearman's rank correlation (rho) values are positive, but none reach 1.\n\n* This is exactly why we should propose MRC to rectify RAC, where RAC serves as the intuitive motivation, while MRC is introduced to rectify this approach and mitigate excessive bias assumptions. Our motivation comes from the ANALYSIS and is verified in the ablation study. As shown in Table 2, RAC can bring impressive improvement, and MRC further enhances these results.\n\n# Response 3.3: Response to Weakness\u00a03\u00a0about the hyperparameters.\nThank you for your detailed examination of our implementation, particularly concerning hyperparameters. We would like to address the raised concern and maybe some misunderstanding in Figure 3:\n\n**hyperparameters:** As emphasized in Response 2.3\u00a0to Reviewer L37Q, we have thoroughly explained the rationale behind these hyperparameters in our answer to that question, and I would encourage you to refer to it for more clarity. Here, for a brief recap, it is crucial to recognize the substantial differences in the models' structure, training settings, and dataset features. The distinct fusion/decoding modules of various models naturally result in slight adaptations of our AttBalance. Furthermore, variations in training settings and dataset characteristics necessitate minor adjustments to the training epoch in our AttBalance.\n\n**Clarify misunderstandings in Figure 3:** In reference to the configuration in Figure 3, it appears there might be a misunderstanding. This setup is used exclusively for analyzing the last layer attention distribution of VLTVG across the dataset. It serves to provide insight into our motivation for incorporating DAT. It is important to note that these are not hyperparameters in our DAT, which is computed adaptively based on BoxRatio and attention regulation for each data sample, without manual-craft design.\n\nWe are grateful for the acknowledgment of our generalization and effectiveness, particularly highlighted in the reviews: Strength 2 from Reviewer 6bMP, Strength 1 from Reviewer L37Q, Strength 3 from you, and Strengths 2 & 3 from Reviewer qRXz. It is rewarding to see recognition for our consistent improvement across all five models on four benchmarks."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1978/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700278636930,
                "cdate": 1700278636930,
                "tmdate": 1700278636930,
                "mdate": 1700278636930,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bpiqPWS5so",
                "forum": "uikf2Ue0XQ",
                "replyto": "lJt8MlALLV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1978/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1978/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder: Seeking Your Feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer zMiJ,\n\nWe deeply appreciate the time and expertise you have dedicated to reviewing our submission. Your insights are invaluable, and we are thankful for the thorough consideration you have given to our work.\n\nHaving diligently responded to and addressed your concerns regarding rho, attention, writing, and the MRC-related experiments, we believe your feedback is pivotal in elevating the quality of our submission.\n\nAs we approach the impending deadline, we respectfully urge you to provide your timely response. Your expert perspective is essential to our revision, and your prompt feedback would be immensely beneficial.\n\nLooking forward to hearing from you promptly.\n\nBest regards,"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1978/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700500024834,
                "cdate": 1700500024834,
                "tmdate": 1700500024834,
                "mdate": 1700500024834,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nZTY0FQFr8",
            "forum": "uikf2Ue0XQ",
            "replyto": "uikf2Ue0XQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1978/Reviewer_L37Q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1978/Reviewer_L37Q"
            ],
            "content": {
                "summary": {
                    "value": "The paper tackles the problem of visual grounding, aiming to produce relevant object bounding boxes in the image based on free-form text input. The proposed method imposes explicit constraints to ensure that self-attention within the transformer layers are focused on the ground truth bounding box areas during training. Specifically, the authors introduce (1) Rho-modulated Attention Constraint (RAC), a BCE loss that promotes the sum of the attention values within the ground-truth bounding box to be 1 and 0 elsewhere, (2) Momentum Rectification Constraint (MRC) to help the model converge smoothly with RAC, and (3) Difficulty Adaptive Training (DAT) to dynamically change the weight of losses. When combined with existing models. The boost is large for QRNet and TransVG, but marginal for VLTVG."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1. The proposed approach consistently enhances the performance of existing models when integrated. Notably, when paired with QRNet, it outperforms all other methods that are compared in the paper. However, the paper lacks comparison with more recent SOTA methods such as VG-LAW [a].\n\n[a] Language Adaptive Weight Generation for Multi-task Visual Grounding, Su et al., CVPR 2023\n\nS2. The paper includes an ablation study to assess the impact of the various components proposed in the paper. \n\nS3. The paper provides an analysis of correlation between the grounding performance (IoU of the predicted bounding boxes against the ground truth bounding boxes) and the summation of the attention value within the ground truth bounding boxes that motivates the proposed method. \n\nS4. The paper provides qualitative results, showing the attention maps both with and without the proposed method, which I found to be insightful."
                },
                "weaknesses": {
                    "value": "W1. The paper assumes that the lack of explicit attention guidance results in suboptimal performance. However, it is difficult for me to buy the assumption, given that the model is trained in an end-to-end manner. Factors like the size and diversity of the training dataset could also be responsible if the attention appears dispersed.\n\nW2. The constraints proposed in the paper involve numerous hyperparameters and heuristics, including adding constraints and subsequently introducing other ones to temper the initial constraints. \n\nW3. The discussion of the prior work that uses object detection losses only, which is one of the motivations of the paper, references papers published over two years ago. It might be worth considering more recent work such as [a] that leverages focal loss and a segmentation loss (DICE loss) similar to Segment Anything Model (SAM) and [b] which uses both object-text and patch-text alignment losses. It might be interesting to compare the attention maps with [a] as well, to validate whether there's a genuine need for explicit guidance on attentions.\n\n[a] Language Adaptive Weight Generation for Multi-task Visual Grounding, Su et al., CVPR 2023\n[b] YORO - Lightweight End to End Visual Grounding, Ho et al., ECCV 2022\n\nW4. The third contribution highlighted in the paper, namely,  \u201c(iii) Our framework can be seamlessly integrated into different transformer-based methods.\u201d is not a valid contribution, but a feature of the proposed framework mentioned in (ii). I suggest merging it with the contribution (ii). \n\nW5. Presentation: Both the main text and the captions omit explanations for legends and notations in the figures and tables (Figure 1, Table 2). The quality of writing could also be further improved. \n\n[Minor comments] \nI suggest revising the abstract for enhanced clarity. \n\nIn \u201cSpecifically, we achieve uniform gains across five different models evaluated on four different benchmarks.\u201d, I suggest \u201cconstantly improves over\u201d rather than \u201cuniform gains\u201d, because the gains are not equal. \n\nAlso, references can be adjusted. Instead of \u201cTransVG Deng et al. (2021)\u201d, consider using \u201cTransVG proposed by Deng et al. (2021)\u201d or  \u201cTransVG (Deng et al., 2021)\u201d."
                },
                "questions": {
                    "value": "Q1. The performance improvement is significant for QRNet and TransVG, yet only slight for VLTVG. I'm curious if this variation is mirrored in the attention map. When examining the attention map, similar to what's shown in Figure 4, is the shift in attention for VLTVG more subtle compared to that of QRNet?\n\nQ2. In Section 2\u2019s analysis, I want to double check if the \u201cIOU value\u201d in \u201cThen we record the IoU value of corresponding data points\u201d is the IoU between the object bounding box predictions against the GT bounding boxes. \n\nQ3. In Section 2\u2019s analysis, were the attention values normalized in any way?\n\nQ4. The paper delves into directing visual features towards specific regions (bbox regions) in an image. Instead of their proposed method, why not introduce a learnable 2D regional weighting layer, akin to the approach in [a], to modulate the attention? This would be done as Attention_i = weighted_2D_mask * Attention_i, where the weighted_2D_mask is produced by a shallow convnet that takes a CxWxH feature map and is trained end-to-end without extra supervision. This could serve as another baseline to validate whether there's a need for explicit guidance/supervision on attentions. \n\n[c] Large-Scale Image Retrieval with Attentive Deep Local Features, ICCV 2017"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1978/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1978/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1978/Reviewer_L37Q"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1978/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822278314,
            "cdate": 1698822278314,
            "tmdate": 1699636129345,
            "mdate": 1699636129345,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wLLZcObRU2",
                "forum": "uikf2Ue0XQ",
                "replyto": "nZTY0FQFr8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1978/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1978/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "# Response 2.1: Response to slight inaccuracy in the summary.\nThanks for your\u00a0clear summary of our method. There is a slight inaccuracy in the summary; it should be the cross-attention from the object query to vision tokens, instead of self-attention.\n\n# Response 2.2: Response to Weakness\u00a01\u00a0about our assumption, considering attention dispersion.\nWe appreciate your insightful question about the potential impact causing attention dispersion. We will address this issue both theoretically and based on practical experimental results below.\n\n**Assumption on the behavior of attention:**\n\n* The dispersion of attention depends on different situations; it is challenging to definitively state whether it should be entirely focused inside the box or what percentage can be outside the box. Sometimes, it behaves incorrectly and appears dispersed, attending to the wrong background. However, there are instances where it behaves correctly, especially when the background provides important clues or other factors like the size and diversity of the data, as you mentioned. However, intuitively, we should at least pay close attention to the target object if we want to detect it.\n\n* These situations have already been considered in our ANALYSIS section. Specifically, a higher level of attention focused inside the box generally tends to lead to more accurate performance (Conclusion 1). However, there is no clear pattern for us to confirm the extent of attention concentration (Conclusions 2 & 3). Here, we express our gratitude for your recognition of this motivation in your Strength 3.\n\n**Experimental results to verify our assumption:** Regarding the overall assumption concerning the suboptimal supervision problem, we have verified it both quantitatively and qualitatively. As demonstrated in Table 1, and with the support from your Strength 1, there are consistently impressive enhancements after applying AttBalance. In the qualitative results, there is a noticeable shift in the layers of TransVG after applying AttBalance, as illustrated in Figure 4 and supported by your Strength 4.\n\n# Response 2.3: Response to Weakness\u00a02\u00a0about the hyperparameters and the motive for each proposed module.\nThanks for your discussion about hyperparameters in our method and the motivation for proposing RAC with the introduction of MRC to rectify it.\n\n**Hyperparameter Concerns:** Addressing concerns about the hyperparameters of our method, it is crucial to note the significant differences in the models themselves, their corresponding training settings, and the features of datasets.\n* From the perspective of the model\u2019s structure, QRNet differs from TransVG with its early language modulation before the final encoder. This necessitates applying AttBalance in all layers of QRNet but fewer layers in TransVG, given the constraint's relation to language-related regions. VLTVG does not involve word-pixel cross-attention in the latter decoding stage but features multi-layer constraints. This difference from TransVG & QRNet in terms of constraint requires a slight tuning of AttBalance with the multi-layer constraint of VLTVG.\n* From the perspective of training settings, TransVG trains for 90 epochs, VLTVG additionally freezes the backbones in the first 10 epochs, and QRNet trains for 160 epochs. These varying training settings require slight adaptations to AttBalance's training epochs.\n* From the perspective of datasets, due to the differing difficulty and features among RefCOCO, RefCOCO+, and RefCOCOg, there are variations in the harshness of our AttBalance. For instance, RefCOCO+ features exclude location words, while RefCOCOg features longer and more elaborate language expressions.\n\nIn summary, we made slight changes to the hyperparameters to achieve optimal results. However, these adjustments do not affect our method's effectiveness, given the consistent enhancements observed across all five models on four benchmarks. At this point, we also appreciate the recognition of our generalization and effectiveness from Strength 2 of Review 6bMP, Strength 1 from you, Strength 3 of Reviewer zMiJ, and Strength 2 & 3 from Reviewer qRXz.\n\n**Heuristics Concerns:** Regarding concerns about the heuristics of our method, we believe we have provided an intuitive motivation, including the ANALYSIS section for RAC & MRC and the analysis from Figure 3 for DAT. Our method is not proposed based on performance tuning but is derived from a clear motivation with a thorough analysis. At this point, we appreciate the support from Strength 2 of peer Reviewer zMiJ and Strength 1 of Reviewer qRXz."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1978/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700277337333,
                "cdate": 1700277337333,
                "tmdate": 1700277337333,
                "mdate": 1700277337333,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "flzyLf4HvX",
                "forum": "uikf2Ue0XQ",
                "replyto": "nZTY0FQFr8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1978/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1978/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "# Response 2.4: Response to Weakness\u00a03\u00a0about the comparison with VG-LAW & YORO.\nWe sincerely appreciate the thoughtful suggestions from the reviewer regarding the consideration of recent works, such as VG-LAW and YORO, in our discussion. Here, we clarify a few things about the loss in VG-LAW, the loss in YORO, and the comparison with VG-LAW.\n\n**VG-LAW's Focal Loss and DICE Loss:** Before delving into the focal loss and DICE loss of VG-LAW, it is crucial to highlight the task distinctions between our paper (Visual Grounding) and VG-LAW (Multi-task Visual Grounding). In Visual Grounding, only one box is provided for each text-image pair as the ground truth annotation. Conversely, in Multi-task Visual Grounding, both the box and the segmentation are provided as the ground truth annotation. This distinction enables VG-LAW to employ focal loss and DICE loss in its segmentation branch, as shown in equations 5 & 6 in the VG-LAW paper. These segmentation losses are not applicable in Visual Grounding, where only the box is provided. So, comparing their loss based on segmentation annotations with our loss based on bounding box annotations would be unfair.\n\n**YORO's Object-Text and Patch-Text Loss:** For the object-text and patch-text loss in YORO, it should be noted that they also rely on additional annotations to supervise these two losses, i.e., the ground truth word labels in the text. For example, the text input 'Fuzzy bench closest to you,' as shown in YORO\u2019s paper, requires additional labels to designate the words 'Fuzzy' and 'bench' as positive with a value of 1, and the other words 'closest,' 'to,' and 'you' as negative with a value of 0. This indicates the inferred target at the word level. Such additional annotation is expensive and exhaustive and is not consistent with the current VG task, which only provides one ground truth box for each text-image pair. VG requires the model to learn to identify which object is the inferred one and its location in the end-to-end learning, without providing the ground truth of the referred object in the text annotations. Support for this can be found in recent works on standard visual grounding, e.g., TransVG\u00a0[1], QRNet [2], and VLTVG [3]. Furthermore, upon examining the ablation study of YORO, i.e., Table 2 in their paper, it is evident that the improvements brought about by their object-text or patch-text loss are quite limited, i.e., +0.22% on CopRef-test and +0.34% on ReferItGame-val for object-text loss, and +0.3% on CopRef-test for patch-text loss. In contrast, our AttBalance brings an average improvement of 4.82% on QRNet.\n\n[1] TransVG: End-to-End Visual Grounding with Transformers. ICCV 2021.\n\n[2] Shifting More Attention to Visual Backbone: Query-modulated Refinement Networks for End-to-End Visual Grounding. CVPR 2022.\n\n[3] Improving Visual Grounding with Visual-Linguistic Verification and Iterative Reasoning. CVPR 2022.\n\n**Comparison with VG-LAW:** Regarding the concern raised in Strength 1 and the comparison with VG-LAW, we would like to clarify the meaning of \"lacks comparison with VG-LAW\".\n* If your question refers to the fact that the final enhanced performances of TransVG, VLTVG, and QRNet have not been compared with VG-LAW, we want to clarify that AttBalance serves as a framework of constraints designed to enhance models by guiding their attention behavior. It does not introduce a new model structure. This is why Table 1 compares the performance of the original model with the one enhanced by AttBalance to verify the effectiveness of AttBalance. We intentionally avoid comparing models with different structures, such as QRNet(+AttBalance) vs. VG-LAW. However, it is worth noting that even if we focus solely on state-of-the-art performance in Visual Grounding, QRNet(+AttBalance) still outperforms VG-LAW, as illustrated in the tables below:\n| Model          | Label               | RefCOCO | RefCOCO+ | RefCOCOg-g | RefCOCOg-umd |\n|----------------|---------------------|---------|----------|------------|--------------|\n| VG-LAW         | box                 | 86.06   | 88.56    | 82.87      | 75.74        |\n| VG-LAW*        | box, segmentation   | 86.62   | 89.32    | 83.16      | 76.37        |\n| QRNet(+AttBalance) | box              | 87.32   | 89.64    | 83.87      | 77.51        |\n\nHere, we outperform +1.11% on RefCOCO, +1.81% on RefCOCO+, and +4.11% on RefCOCOg-umd, indicating a clear superiority in our performance. It is important to note that VG-LAW lacks evaluation on RefCOCOg-g. Even when competing with the multitask version of VG-LAW, VG-LAW*, which incorporates a considerable number of additional segmentation labels in supervised learning, our QRNet(+AttBalance) still outperforms VG-LAW*, specifically by +2.82% on RefCOCOg-umd. The outperformance is particularly noteworthy considering the higher cost and exhaustive nature of segmentation labels from the Referring Image Segmentation task compared to the bounding box labels from Visual Grounding."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1978/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700277903318,
                "cdate": 1700277903318,
                "tmdate": 1700291477885,
                "mdate": 1700291477885,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QA1NA07PmO",
                "forum": "uikf2Ue0XQ",
                "replyto": "nZTY0FQFr8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1978/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1978/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder: Seeking Your Feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer L37Q,\n\nWe extend our sincere appreciation for the time and expertise you've dedicated to reviewing our submission. Your insights are invaluable, and we are grateful for the thoughtful consideration you've given to our work.\n\nHaving thoroughly addressed and resolved your concerns regarding attention, hyperparameters, writing, and various comparative experiments, we believe your feedback is instrumental in enhancing the quality of our submission.\n\nAs we approach the impending deadline, we respectfully urge you to provide your timely response. Your expert perspective is crucial to our revision, and your swift feedback would be immensely beneficial.\n\nLooking forward to hearing from you promptly.\n\nBest regards,"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1978/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499854441,
                "cdate": 1700499854441,
                "tmdate": 1700499854441,
                "mdate": 1700499854441,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8tORFRqy6X",
            "forum": "uikf2Ue0XQ",
            "replyto": "uikf2Ue0XQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1978/Reviewer_6bMP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1978/Reviewer_6bMP"
            ],
            "content": {
                "summary": {
                    "value": "This paper propose a novel framework named Attention-Driven Constraint Balancing (AttBalance) in Visual Grounding task which analyze the attention mechanisms of transformer-based models, and optimize the behavior of visual features within language-relevant regions. This approach propose a framework to incorporate language-related region guidance for fully optimized training."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) The article explores balancing the regulation of the attention behavior during training and mitigate the data imbalance problem, The idea of AttBalance is interesting.\n\n(2) Compared with benchmark methods, with the guidance of AttBalance, all transformer-based models consistently obtain an impressive improvement."
                },
                "weaknesses": {
                    "value": "1.\tthis paper\u2019s main contribution is attention mechanisms of transformer-based models. While I believe in 2021, there was already a work which put attention transformer in visual grounding, which called Word2Pix, what is the strength of this paper compare to Word2Pix? Some innovative points are needed to demonstrate the superiority of this method in visual grounding.\n2.\tThe related work part can be put in section 2 rather in section 5.\n3.\tAs shown in Table 2, could you explain why the incorporation of only the MRC module results in a decline on average, and when the MRC is used to rectify the RAC can it lead to an increase. Can you provide a more detailed explanation of how MRC affects RAC and lead to an increase?"
                },
                "questions": {
                    "value": "Please see the above weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1978/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1978/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1978/Reviewer_6bMP"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1978/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698899180137,
            "cdate": 1698899180137,
            "tmdate": 1700726528072,
            "mdate": 1700726528072,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ao4qx6HuPX",
                "forum": "uikf2Ue0XQ",
                "replyto": "8tORFRqy6X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1978/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1978/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "# Response 1.1: Response to Weakness\u00a01 about comparison with Word2Pix.\nWe appreciate the reviewer's insightful comment on the need for innovative aspects in our work compared to Word2Pix.\u00a0It\u00a0is\u00a0important to clarify that there is no direct relationship between Word2Pix and our AttBalance. Here\u00a0are\u00a0the details:\n* Word2Pix primarily introduces a one-stage model incorporating word-pixel attention, a module that is commonplace in current Transformer-based models like TransVG[1], VLTVG\u00a0[2], and QRNet\u00a0[3]\u00a0(e.g., the Visual-Linguistic Transformer in TransVG and QRNet, and the verification module in VLTVG).\n* In contrast, AttBalance is designed to address the issue of insufficient supervision in these models. It guides the attention layer to focus on language-related regions while also considering the background.\n\nTherefore, the concept of \"putting an attention transformer in visual grounding\" is not novel today and is not directly related to our motivation or method. The novelty of our approach lies in the perspective of supervision learning.\u00a0Additionally, it\u00a0is\u00a0worth noting that the classification loss and attribute loss in Word2Pix rely on additional classification and attribute labels, which deviate from the current visual grounding (VG) setting. The VG task typically involves a text-image pair with one ground truth bounding box, a widely-used setting supported by top conference papers like TransVG [1], VLTVG [2], QRNet [3], SeqTR [4], and SiRi [5].\n\n[1] TransVG: End-to-End Visual Grounding with Transformers. ICCV 2021.\n\n[2] Improving Visual Grounding with Visual-Linguistic Verification and Iterative Reasoning. CVPR 2022.\n\n[3] Shifting More Attention to Visual Backbone: Query-modulated Refinement Networks for End-to-End Visual Grounding. CVPR 2022.\n\n[4] SeqTR: A Simple yet Universal Network for Visual Grounding. ECCV 2022 Oral.\n\n[5] SiRi: A Simple Selective Retraining Mechanism for Transformer-based Visual Grounding. ECCV 2022.\n\n# Response 1.2: Response to Weakness\u00a02 about the manuscript of the related work.\nI appreciate the reviewer's suggestion to relocate the related work section to section 2. Initially, the reason we put related work in the latter section is that we have an additional Analysis section to induce our motivation. We were concerned that introducing related work earlier might make the paper appear miscellaneous before delving into the detailed method, which typically would be around the fourth page.\nHowever, considering that readers might question the\u00a0necessity\u00a0to dig into the transformer-based model supervision problem without thinking about other CNN-based models, for example, the Word2pix model you proposed in Weakness\u00a01. We will follow your suggestion\u00a0to move the related work section to section 2.\n\n# Response 1.3: Response to Weakness\u00a03 about the insight of why solely using\u00a0MRC induces\u00a0decline and its effect on RAC.\nI appreciate the insightful query regarding the observed decline with only the MRC module and its subsequent improvement when used to rectify the RAC. I\u00a0would like to clarify a few things:\n\n**Decline when only using\u00a0MRC:** The Momentum model is a continuously-ensembled version that incorporates exponentially moving averaged versions from previous training steps. Given that each of the models from previous training steps is more prone to underfitting than the current one, it is intuitive that their ensemble may not consistently provide a reliable attention map to guide the current model. This is why solely relying on MRC results in a slight decline.\n\n**Improvement when using MRC to rectify\u00a0RAC:** Nevertheless, the momentum model proves beneficial when employed as a complementary constraint to smooth the original constraint. In many cases, the original constraint may not be inherently reliable. For instance, the Image-Text Contrastive Learning constraint in ALBEF [1] may lack reliability when confronted with noisy unpaired data. Thus, the momentum model brings\u00a0the current model's learning target closer to its previous ensemble behavior, thereby smoothing the current learning process. In the context of our visual grounding, the RAC that focuses the model's attention on language-related regions generally leads to improvements, as discussed in the conclusion\u00a0(1) of our ANALYSIS section and supported by Table 2 in our ablation study. However, the RAC is not universally reliable, considering that in visual grounding, we might also consider background objects as potential clues, as they may be referenced in the language. Consequently, employing the MRC to rectify the RAC consistently brings about improvement, as detailed in the conclusions (2, 3) of our ANALYSIS section and validated by Table 2 in our ablation study.\u00a0We\u00a0will\u00a0concisely incorporate this discussion into the revised version of the paper.\n\n[1] Align before Fuse: Vision and Language Representation Learning with Momentum Distillation. NeurIPS 2021 Spotlight."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1978/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700276764714,
                "cdate": 1700276764714,
                "tmdate": 1700276764714,
                "mdate": 1700276764714,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Y1v9XEIMGv",
                "forum": "uikf2Ue0XQ",
                "replyto": "8tORFRqy6X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1978/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1978/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder: Seeking Your Feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer 6bMP,\n\nThank you for dedicating time to review our paper. Your feedback is immensely valuable to us, and we would be grateful for any insights you can provide.\nWe have taken great care to address the concerns raised, particularly those regarding the comparison with Word2Pix, writing considerations, and the MRC-related experiments. Your feedback has been instrumental in refining our paper.\n\nAs we approach the looming deadline, we respectfully urge you to consider providing your prompt response. Your timely insights are pivotal to the finalization of our submission, and your contribution is immensely valued.\n\nLooking forward to hearing from you promptly.\n\nBest regards,"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1978/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700499634475,
                "cdate": 1700499634475,
                "tmdate": 1700499634475,
                "mdate": 1700499634475,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JZTZ6OUEv6",
                "forum": "uikf2Ue0XQ",
                "replyto": "Ao4qx6HuPX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1978/Reviewer_6bMP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1978/Reviewer_6bMP"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. The clarification of theoretical contribution have been helpful in addressing my concerns. Hence, I raised my rating to 6."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1978/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726656728,
                "cdate": 1700726656728,
                "tmdate": 1700726656728,
                "mdate": 1700726656728,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]