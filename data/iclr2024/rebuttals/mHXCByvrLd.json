[
    {
        "title": "Rethinking Optimal Transport in Offline Reinforcement Learning"
    },
    {
        "review": {
            "id": "bmy03DUcAh",
            "forum": "mHXCByvrLd",
            "replyto": "mHXCByvrLd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6000/Reviewer_sJfc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6000/Reviewer_sJfc"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel offline RL algorithm that comes from the popular optimal transport methods. Experiment results on Meta-World show that it achieves SOTA performance on various tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Offline RL is recently a heated topic, and the authors propose a novel offline RL algorithm that has strong theoretical support. The idea of using optimal transport as policy regularization is novel and interesting. Experiment results are quite strong."
                },
                "weaknesses": {
                    "value": "I am not quite familiar with optimal transport, so actually I did not fully check the methods. One possible weakness is that XMRL introduces an additional component $f$, which introduces additional learning complexity, especially in complex tasks."
                },
                "questions": {
                    "value": "1. $f$ seems to be learned in an adversarial training manner. Is there any connection between XMRL and ATAC [1], an offline algorithm that also uses adversarial training?\n2. How much additional computation complexity does XMRL introduce? The authors discuss about absolute training time, but how much more time does XMRL take compared to methods like IQL or CQL?\n3. It seems that the larger $w$ is, the higher the performance. Can the authors further discuss the impact of $w$ on performance?\n\n\n[1] Cheng, Ching-An, et al. \"Adversarially trained actor critic for offline reinforcement learning.\" International Conference on Machine Learning. PMLR, 2022."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6000/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6000/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6000/Reviewer_sJfc"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6000/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697872845269,
            "cdate": 1697872845269,
            "tmdate": 1699636643106,
            "mdate": 1699636643106,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cwyr8iyrE2",
                "forum": "mHXCByvrLd",
                "replyto": "bmy03DUcAh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6000/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6000/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "Thank you for spending time reviewing our paper and providing valuable feedback that will help us improve the manuscript. Please find below the answers to your questions.\n\n>**Q1: $f$ seems to be learned in an adversarial training manner. Is there any connection between XMRL and ATAC [1], an offline algorithm that also uses adversarial training?.**\n\nOur method XMRL differs from ATAC by using the function $f$ for a different purpose. In XMRL, $f$ is introduced as a function to enforce constraints during the learning process, as described in Section 2, whereas in ATAC they use adversarial training to optimize the critic function $Q$ (denoted as $f$ in their paper).\n\nMoreover, although ATAC is more closely related to the CQL approach, which we compare to in our experiments, our method can use a cost/critic function trained using ATAC's method. This synergy between XMRL and ATAC could be an interesting direction for future research, showing how adversarially trained critic functions could improve constraint-based methods like ours. In the revised version of our paper, we have updated the *related works section* to include ATAC [1] as a critic penatly method. \n\n---\n\n>**Q2: How much additional computation complexity does XMRL introduce? The authors discuss about absolute training time, but how much more time does XMRL take compared to methods like IQL or CQL?.**\n\nFor the MuJoCo tasks pre-trained models were used, we trained our method for 100K steps using the pre-trained critic, the training time in the paper is reported for these settings. Comparing our method on MuJoCo Halfcheetah with training from scratch, IQL takes 10 hours, XMRL: 11 hours, CQL: 18 hours.\n\nRegarding the AntMaze tasks, we used the JAX framework, which is known for its efficiency, and the timing was as follows: for example, for the AntMaze (large-play) ReBrac takes ~60 minutes and our takes XMRL: 75.1 minutes.\n\nWhile introducing an additional discriminator into the training process could potentially increase the computational complexity, in reality XMRL introduces minimal overhead compared to other methods.\n\n---\n\n>**Q3: It seems that the larger w is, the higher the performance. Can the authors further discuss the impact of w on performance??**\n\nPlease, see the general response to all reviewers and revised Appendix A.2 for a detailed discussion of this topic.\n\n---\n\n>**Concluding remarks** We truly value your reviews. Please respond to our post to let us know if the clarifications above suitably address your concerns about our work. If the responses above are sufficient, we kindly ask that you consider raising your score."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6000/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725918876,
                "cdate": 1700725918876,
                "tmdate": 1700725918876,
                "mdate": 1700725918876,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DPHMtB6vOv",
            "forum": "mHXCByvrLd",
            "replyto": "mHXCByvrLd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6000/Reviewer_Z7BD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6000/Reviewer_Z7BD"
            ],
            "content": {
                "summary": {
                    "value": "The key idea of the paper is to compute the optimal transport between states and actions with an action-value cost function. This provides a new way to balance the two tasks in offline RL: policy improvement and distribution shift avoidance. Based on this idea, the authors propose a new algorithm called Extremal Monge Reinforcement Learning and have shown that the new algorithm outperforms BC and previous offline RLs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea of thinking of an offline RL problem as an Optimal Transport (OT) problem is interesting. Especially considering that the Extremal Optimal Transport (ET) can be used for policy improvement.\n2. The experimental results are sufficient and promising."
                },
                "weaknesses": {
                    "value": "1. This contribution of the paper is insignificant, as it is barely a direct application of  Extremal Optimal Transport [1], without providing a new understanding of offline RL or solving/alleviating existing problems in offline RL.\n2. The idea is not convincing. It is hard to see why we should consider an offline RL task an OT problem. Why do we want to preserve the distribution of $\\mathcal{A}$ instead of considering the distribution $\\mathcal{A}(s)$ or support $Supp(\\mathcal{A}(s))$ independently for each state? According to my understanding, offline RL considers the distribution $\\mathcal{A}(s)$ instead of the distribution of $\\mathcal{A}$, as shown in W-BRAC (eq. 8).\n3. XMLR does not seem better than W-BRAC. As stated in 2, the theoretical foundation of XMLR is not convincing. Furthermore, in practice, XMLR faces the same problems as W-BRAC. 1) both of them train an additional discriminator $f$. 2) both have a hyper-parameter to control the extreme of the policy, which is difficult to choose. \n4. The experimental results are not surprising. According to Table 2, the results of XMRL are close to the results of ReBRAC.\n5. The Writing should be improved. Typos, e.g., in the first line in section 2.4, should be fixed.\n\n[1] Gazdieva, Milena, et al. \"Extremal Domain Translation with Neural Optimal Transport.\" arXiv preprint arXiv:2301.12874 (2023)."
                },
                "questions": {
                    "value": "1. What is the benefit of formulating an offline RL problem as an OT problem?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6000/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6000/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6000/Reviewer_Z7BD"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6000/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698503780239,
            "cdate": 1698503780239,
            "tmdate": 1699636643000,
            "mdate": 1699636643000,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RhQlDDzsQr",
                "forum": "mHXCByvrLd",
                "replyto": "DPHMtB6vOv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6000/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6000/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to review our paper and provide useful feedback. Your questions will help us improve the manuscript. Below are the answers to your questions.\n\n> **Q1: This contribution of the paper is insignificant, as it is barely a direct application of Extremal Optimal Transport [1], without providing a new understanding of offline RL or solving/alleviating existing problems in offline RL.**\n\nOur paper addresses the well-known \"stitching problem\" in offline RL, as detailed in **Section 3.2**. This problem has been recognized in the offline RL literature (Levine et al., 2020) as a significant obstacle to achieving strong performance in offline reinforcement learning. \n\nWe believe that our contribution goes beyond a direct application of Extremal OT. We argue that without our reformulation in **Section 2**, it is unclear that Extremal OT can applicable to RL. We establish a unique connection between OT and RL domains, offering a novel perspective on the problem - **this connection has not been considered before.**\n\nThen we demonstrates the practical applicability of this novel connection by integrating recent findings in OT, specifically Extremal OT, to effectively mitigate the snitching problem. The superior performance demonstrated in our evaluation results underscores the effectiveness of our approach compared to existing methods.\n\n---\n\n> **Q2: The idea is not convincing. It is hard to see why we should consider an offline RL task an OT problem. Why do we want to preserve the distribution of A instead of considering the distribution or support A(s) independently for each state? According to my understanding, offline RL considers the distribution A(s) instead of the distribution of A, as shown in W-BRAC (eq. 8)?** \n\nThe main reason for consider an offline RL task an OT problem is because this formulation allows to solve stitching problem, and outperforms a previously proposed approaches.  \n\nRegarding the choice between using $ \\mathcal{A} $  and $  \\mathcal{A}(s) $ , we'd like to clarify that our method is indeed flexible and can consider both $  \\mathcal{A}$ and $  \\mathcal{A}(s)$  distributions. In the conclusion section of our initial submission, we provided a rationale for our choice to prioritize $ A$  over $  \\mathcal{A}(s) $ . This choice, was guided by our experimental observations. We found that focusing on $ f(a)$  rather than $ f(s,a)$  did not compromise performance, and we preferred this representation for its methodological simplicity. \n\nIn practice we can incorporate a state-conditional information through conditional optimal transport. This simply requires adding an additional input variable to the $ f$  function to produce $f(s,a)$ .\n\nWe added a separate discussion section on this topic into the revised paper. Also we have included $ f(s,a)$  results in the revised Appendix A.3. All results with $ f(s,a)$ can be reproduced with the original code provided in the Supplementary Material. Please see the networks.py file for the state-conditional version of the potential $ f(s,a)$ .\n\n---\n\n> **Q3.1: XMLR does not seem better than W-BRAC. As stated in 2, the theoretical foundation of XMLR is not convincing. Furthermore, in practice, XMLR faces the same problems as W-BRAC. 1) both of them train an additional discriminator.**\n\nWe initially did not include W-BRAC in the direct comparisons because our experiments showed that XMLR outperformed methods that in turn showed significant improvements over W-BRAC, suggesting a clear performance hierarchy. \n\nRegarding the training of an additional discriminator, we highlight an important difference in **Section 2**: W-BRAC requires *Lipschitz* constraints on the discriminator. *Lipschitz* constraints often complicate W-BRAC's discriminator training, while weight clipping, gradient penalty, and other regularizations are required. In contrast, XMLR's approach does not require Lipschitz constraints on the discriminator.\n\n---\n\n> **Q3.2 both methods have a hyper-parameter to control the extreme of the policy, which is difficult to choose.**\n\nThe role of the parameter in W-BRAC is completely different. Please see the general response to all reviewers for a detailed discussion of this topic."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6000/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722436998,
                "cdate": 1700722436998,
                "tmdate": 1700722436998,
                "mdate": 1700722436998,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "97WVE3be0B",
            "forum": "mHXCByvrLd",
            "replyto": "mHXCByvrLd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6000/Reviewer_wDEF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6000/Reviewer_wDEF"
            ],
            "content": {
                "summary": {
                    "value": "In offline reinforcement learning, optimal transport can be used to assess the distance between a certain policy and an expert policy. When using the Wasserstein distance, this leads to the W-BRAC algorithm from [Wu et al, 2022]. This paper overcomes some drawbacks of BRAC and related methods (choice of regularizing hyperparameter and Lipschitz constraints) by looking at offline RL as a saddle-point problem that arises from the dual form of the Kantorovich problem, and leveraging standard duality machinery. To avoid inefficient actions (stichting between \u201cgood\u201d trajectories in the dataset), set inclusion constraints are replaced by inequality constraints, providing extremal transport in some cases. The proposed method, which appears to be sensitive to a regularizing hyperparameter, provides improvements over relevant baselines in the MuJoCo and Antmaze benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Novel formulation of offline RL using standard duality results and OT machinery.\n- The empirical evaluation contains 9 Mujoco and 6 Antmaze environments against relevant benchmarks. Although performance improvements are not significant in some environments, the proposed method is almost consistently among the top 3. By comparing XMRL with BC and CQL alone, the impact of the proposed approach is disentangled from that of the techniques used to avoid overestimation bias.\n- The paper is clearly written and, to the best of my knowledge, the related work is adequately described."
                },
                "weaknesses": {
                    "value": "- I do not see the relevance of proposition 3.1 (policy improvement), since it does not appear to give any insight about the method (the proposition is not cited/used anywhere in the paper) and the proof is basically the same as the one for classic policy iteration improvement.\n- Evaluating the performance of the method in more challenging environments or tasks would enrich the experimental section. The impact of entropy regularization on the performance of the method is not assessed. \n- The method seems considerably sensitive to the parameter w (as seen in appendix 2), so one of the drawbacks of W-BRAC is still present in this method.\n- Typos: \u201cFol all experiments\u201d section 4.2."
                },
                "questions": {
                    "value": "-"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6000/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811601504,
            "cdate": 1698811601504,
            "tmdate": 1699636642897,
            "mdate": 1699636642897,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7MCdvlogOq",
                "forum": "mHXCByvrLd",
                "replyto": "97WVE3be0B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6000/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6000/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response"
                    },
                    "comment": {
                        "value": "Thank you for your detailed analysis of our paper and your thoughtful suggestions for improvement. All typos have been corrected. Below is our response to your comments.\n\n> **Q1:I do not see the relevance of proposition 3.1 (policy improvement), since it does not appear to give any insight about the method (the proposition is not cited/used anywhere in the paper) and the proof is basically the same as the one for classic policy iteration improvement.**\n\nThank you for raising your concern that **Proposition 3.1** is not cited in the paper. The primary purpose of **Proposition 3.1** is to emphasize the focus of our method on policy improvement rather than behavior cloning, as stated in the Introduction and **Section 3**. We mentioned our proposition now in **Sections 1, and 3**.\n\n---\n\n> **Q2:Evaluating the performance of the method in more challenging environments or tasks would enrich the experimental section.**\n\nIn response to your suggestion, we conducted experiments on the Pen and Door environments of the D4RL. Our XMRL-BC method showed performance similar to the ReBRAC method, specifically: **pen-expert: 154.9\u00b14.2, pen-cloned: 100.1\u00b120, pen-human: 97.5\u00b111, door-expert: 104.9\u00b14.5**. For the environments like door-cloned and door-human, our method achieved zero scores similar to ReBRAC. We have included these results in *Appendix A.4* and additionally provided an ablation study on parameter $w$ for these environments in **A.2**.\n\n---\n\n> **Q3:The impact of entropy regularization on the performance of the method is not assessed?**\n\nWe did not perform an ablation study on entropy regularization as *it is not part of our contribution*. Entropy regularization was only applied in the context of CQL-based experiments performed on the Antmaze environments, while the CQL-backbone includes entropy regularization by design. Conversely, for the *simpler* MuJoCo environments, we opted for a version of CQL Eq.(14) without any regularization.\n\n---\n\n> **Q4: The method seems considerably sensitive to the parameter w (as seen in appendix 2), so one of the drawbacks of W-BRAC is still present in this method.**\n\nThis is not true. The drawbacks of W-BRAC are not present in our method, while the role of our parameter is completely different. The fact that a hyperparameter (even a different) is required is noted as a *limitation* in the **(Section 4.3)** of the initial submission. Please see the general response to all reviewers for a detailed discussion of this topic. \n\n---\n\n>**Concluding remarks**: Please respond to our post to let us know if the clarifications above suitably address your concerns about our work. If you finds the responses above are sufficient, we kindly ask that you consider raising your score."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6000/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700724700179,
                "cdate": 1700724700179,
                "tmdate": 1700724700179,
                "mdate": 1700724700179,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]