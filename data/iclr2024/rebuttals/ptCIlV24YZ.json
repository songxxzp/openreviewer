[
    {
        "title": "Image Clustering via the Principle of Rate Reduction in the Age of Pretrained Models"
    },
    {
        "review": {
            "id": "FIuO7JwFu2",
            "forum": "ptCIlV24YZ",
            "replyto": "ptCIlV24YZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5846/Reviewer_DGyy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5846/Reviewer_DGyy"
            ],
            "content": {
                "summary": {
                    "value": "This paper propose a new image clustering workflow that leverages the powerful feature representation capabilities of large pretrained models (like CLIP) to effectively and efficiently perform image clustering. It first develop a new algorithm to estimate the number of clusters in a given dataset. Through extensive experiments, the paper demonstrate that the workflow performs well on standard datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Novel image clustering pipeline: The paper proposes a novel image clustering pipeline that leverages the powerful feature representation of large pre-trained models such as CLIP and cluster images effectively and efficiently at scale. The paper also develops a new algorithm to estimate the number of clusters in a given dataset, and a simple yet effective self-labeling algorithm that generates meaningful text labels for clusters.\n2. State-of-the-art performance: The paper demonstrates that the proposed pipeline achieves state-of-the-art clustering performance on standard datasets such as CIFAR-10, CIFAR-100, and ImageNet-1k. The paper also shows that the pipeline works well on datasets without predefined labels, such as WikiArt"
                },
                "weaknesses": {
                    "value": "1. Dependence on pre-trained models: The paper heavily relies on the pre-trained models such as CLIP to provide the initial feature representation and the text candidates for labeling. The paper does not explore how the choice of pre-trained models affects the clustering performance or the quality of labels. The paper also does not consider the potential biases or limitations of the pre-trained models, such as their data sources, or domains.\n2. Limited evaluation and comparison: The paper also does not report enough ablation studies or sensitivity analysis to show the impact of different components or hyperparameters of the pipeline."
                },
                "questions": {
                    "value": "How does the choice of pre-trained models affect the clustering performance or the quality of labels?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5846/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5846/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5846/Reviewer_DGyy"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5846/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698557957682,
            "cdate": 1698557957682,
            "tmdate": 1699636618545,
            "mdate": 1699636618545,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IjzHXsxHdl",
                "forum": "ptCIlV24YZ",
                "replyto": "FIuO7JwFu2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5846/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5846/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer DGyy"
                    },
                    "comment": {
                        "value": "Dear Reviewer`DGyy`:\n\nThank you for the helpful comments and interesting questions! We appreciate that you find our work \u201cnovel\u201d and attains \u201cstate-of-the-art\u201d.  We respond to your questions below.\n\n>Dependence on pre-trained models: The paper heavily relies on the pre-trained models such as CLIP to provide the initial feature representation and the text candidates for labeling. The paper does not explore how the choice of pre-trained models affects the clustering performance or the quality of labels. The paper also does not consider the potential biases or limitations of the pre-trained models, such as their data sources, or domains.\n\nThanks for pointing it out! We are with you that a more comprehensive discussion on the choice of pre-training solidifies our paper. Notably, we have conducted experiments using two additional popular pretrain models, MAE and DINO; please kindly see Table 3 in the Appendix, where we presented the observation in Appendix A.1. We also conducted experiments using OpenCLIP Table 9 in rebuttal supplementary material, which is pre-trained on open source data. Empirically, we find  that CLIP, OpenCLIP and DINO all lead to much higher clustering accuracy when used as pre-training for CPP. \n\n>Limited evaluation and comparison: The paper also does not report enough ablation studies or sensitivity analysis to show the impact of different components or hyperparameters of the pipeline. \n\nTo address your concerns, We summarize the ablation studies we have conducted and added in this rebuttal period:\n\n1. Our method improves as the pretrain model improves: we explore different pretraining: CLIP, MAE, DINO, and OpenCLIP. We have presented the results in Appendix A.1. and Table 9 in rebuttal supplementary material. \n\n2. Diversified Initialization is important: we presented the ablation study of optimizing equation (4) in Appendix B.\n\n3. Our method is  quite robust to hyperparameters: We additionally reported the effect of a different choice of $\\gamma$ for sinkhorn distance projection. Please kindly refer to Appendix C.1. and Table 7(c) for definition, Table 10 in rebuttal supplementary material for additional results.\n\n**References**\n\n[1] Ding, Tianjiao, Shengbang Tong, Kwan Ho Ryan Chan, Xili Dai, Yi Ma, and Benjamin D. Haeffele. Unsupervised manifold linearizing and clustering. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 5450\u20135461, October 2023."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700119799218,
                "cdate": 1700119799218,
                "tmdate": 1700119799218,
                "mdate": 1700119799218,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "B6owMa8Qkl",
                "forum": "ptCIlV24YZ",
                "replyto": "FIuO7JwFu2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5846/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5846/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "Dear Reviewer DGyy,\n\nWe are grateful for your efforts in the review, and we hope our response together with extra results covered your concerns. We are more than happy to further clarify or address additional questions. Please let us know if you still have any unclear parts of our work.\n\nSincerely, Authors"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700555120589,
                "cdate": 1700555120589,
                "tmdate": 1700555120589,
                "mdate": 1700555120589,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7JFBblGEqH",
                "forum": "ptCIlV24YZ",
                "replyto": "B6owMa8Qkl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5846/Reviewer_DGyy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5846/Reviewer_DGyy"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response which has answered some of my concerns. I appreciate the considerable amount of experimental work that has been undertaken. However, I still have concerns about the fairness of the comparison with other clustering methods, since the big pretrained backbone (ViT-L) is used while most of other methods use much smaller backbones, like ResNet-18/34. Therefore, I will maintain my original score."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664016625,
                "cdate": 1700664016625,
                "tmdate": 1700664016625,
                "mdate": 1700664016625,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5zZJzVxJwM",
            "forum": "ptCIlV24YZ",
            "replyto": "ptCIlV24YZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5846/Reviewer_7qGL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5846/Reviewer_7qGL"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a unified approach for integrating feature learning with clustering processes. Its novel method for selecting the optimal number of clusters enhances the practicality of KNN and similar clustering methods for large-scale data, even when using computationally intensive models like CLIP. Additionally, it achieves state-of-the-art results on various datasets."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1- SOTA results\n2- I find that enhancing KNN methods \u2013 whether by improving their scalability, representation, or explainability \u2013 is valuable. \n3- Good visual analysis Fig 3 and 4."
                },
                "weaknesses": {
                    "value": "1- The paper's flow is somewhat challenging to follow.\n2- The acronym MLC is initially mentioned in the contributions section without prior definition, leading to initial confusion, though its relation to previous works becomes clearer later in the paper.\n3- While the method appears to be a practical extension of MLC, its level of novelty and contribution to the field is not distinctly evident."
                },
                "questions": {
                    "value": "1- The complexity added by Equation 4 to the optimization process isn't clear. Is it possible for there to be multiple optimal values for K?\n2- Could a simpler method, like the elbow method, be used to determine K?\n3- A brief explanation of what \"more-structured representation\" means in the context of related works would be helpful."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5846/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5846/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5846/Reviewer_7qGL"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5846/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698806404132,
            "cdate": 1698806404132,
            "tmdate": 1701059150086,
            "mdate": 1701059150086,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "X08XNLI6hP",
                "forum": "ptCIlV24YZ",
                "replyto": "5zZJzVxJwM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5846/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5846/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 7qGL"
                    },
                    "comment": {
                        "value": "Dear reviewer `7qGL`:\n\nThank you for the helpful comments and interesting questions! We appreciate that you find  our work valuable and contains good analysis.  We respond to your questions below.\n\n>The acronym MLC is initially mentioned in the contributions section without prior definition, leading to initial confusion, though its relation to previous works becomes clearer later in the paper.\n\nThanks for the suggestion! We will add a citation where the acronym shows up for the first time in the future revisions.\n\n>The complexity added by Equation 4 to the optimization process isn't clear. Is it possible for there to be multiple optimal values for K?\n\n**Complexity:** Overall, our pipeline has two parts: 1) learning the representation Z and membership Pi via optimizing equation (MLC), and 2) given learned Z and Pi, select the number of clusters, which is where Equation 4 appears. Part 1) is the computationally heavier one, as it involves training/finetuning a neural network with a large number of parameters (such as the transformers used in the paper). Part 2) is efficient, as it does not update the parameters of the neural network at all, and only involves spectral clustering on Pi (for which fast algorithms exist [5]) and evaluating Equation 6 (which takes O($d^2b$ ) to compute with b being the batch size). \n\n**Multiple Optimal Values for k:** You are right, in principle it is possible to have multiple optimal values of k.  For example, this could happen when the cost reduced by splitting a cluster into two is the same as that incurred by encoding one more label. However, empirically, as a result of complexity of real-world datasets, this did not happen in our experiments, as seen in Figure 4. \n\n>Could a simpler method, like the elbow method, be used to determine K?\n\nGood question! In the elbow method, one needs to compute the sum of squared distances from each point to the centroid of its assigned cluster. This is a good measure of cluster complexity only when the data from each cluster are close to a point. However, in our case, the data of each cluster are expected to lie close to a low-dimensional linear subspace, so the elbow method does not apply.\n\n>A brief explanation of what \"more-structured representation\" means in the context of related works would be helpful.\n\nThanks for the suggestion! We originally wants to convey that \u201cmore-structured\u201d means:\n- features within the same cluster tend to span a low-dimensional subspace (i.e., within-cluster diverse)\n- subspaces from different clusters tend to be orthogonal (between-cluster discriminative).\n\nConceptually, learning such a representation is the goal of the rate reduction family[2, 3, 4], which has been mentioned in the introduction section. Empirically, we validate the claim in Figure 2, where we observed better image-to-image search and block-diagonal structures.\n\nNevertheless, we are willing to refine the representation and explain it more clearly in the related works section in future revisions.\n\n>While the method appears to be a practical extension of MLC, its level of novelty and contribution to the field is not distinctly evident.\n\nWe clarified the novelty and significance of the paper in the General Response above. Hopefully that alleviates your concern. \n\n**References**\n\n[1] Ma, Yi, Harm Derksen, Wei Hong, and John Wright. \"Segmentation of multivariate mixed data via lossy data coding and compression.\" IEEE transactions on pattern analysis and machine intelligence 29, no. 9 (2007): 1546-1562.\n\n[2] Yu, Yaodong, Kwan Ho Ryan Chan, Chong You, Chaobing Song, and Yi Ma. \"Learning diverse and discriminative representations via the principle of maximal coding rate reduction.\" Advances in Neural Information Processing Systems 33 (2020): 9422-9434.\n\n[3] Baek, Christina, Ziyang Wu, Kwan Ho Ryan Chan, Tianjiao Ding, Yi Ma, and Benjamin D. Haeffele. \"Efficient maximal coding rate reduction by variational forms.\" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 500-508. 2022.\n\n[4] Ding, Tianjiao, Shengbang Tong, Kwan Ho Ryan Chan, Xili Dai, Yi Ma, and Benjamin D. Haeffele. Unsupervised manifold linearizing and clustering. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 5450\u20135461, October 2023. \n\n[5] Yan, Donghui, Ling Huang, and Michael I. Jordan. \"Fast approximate spectral clustering.\" Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining. 2009."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700119517914,
                "cdate": 1700119517914,
                "tmdate": 1700119603707,
                "mdate": 1700119603707,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wW5CaoJxjx",
                "forum": "ptCIlV24YZ",
                "replyto": "5zZJzVxJwM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5846/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5846/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "Dear Reviewer 7qGL\n\nWe are grateful for your efforts in the review, and we hope our response together with extra results covered your concerns. We are more than happy to further clarify or address additional questions. Please let us know if you still have any unclear parts of our work.\n\nSincerely, Authors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700555087184,
                "cdate": 1700555087184,
                "tmdate": 1700555087184,
                "mdate": 1700555087184,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Tl59uLr7IL",
            "forum": "ptCIlV24YZ",
            "replyto": "ptCIlV24YZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5846/Reviewer_5jgz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5846/Reviewer_5jgz"
            ],
            "content": {
                "summary": {
                    "value": "The submission introduces a new image clustering pipeline named CPP, which leverages large pre-trained models like CLIP to efficiently and effectively cluster images, particularly on large-scale datasets. The authors propose to estimate the optimal number of clusters in a dataset and optimize the rate reduction objective using pre-trained features, resulting in a notable improvement in clustering accuracy (e.g., from 57% to 66% on ImageNet-1k). Furthermore, by utilizing CLIP's multimodal capabilities, a simple yet effective self-labeling algorithm is developed to generate meaningful text labels for the clusters. The pipeline demonstrates state-of-the-art performance across various standard datasets including CIFAR-10, CIFAR-100, and ImageNet-1k, and extends its applicability to datasets without predefined labels like LAION-Aesthetics and WikiArts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. **Leveraging Large Pre-trained Models**: The integration of the powerful image encoder from CLIP into the clustering framework MLC significantly enhances the pipeline\u2019s ability to process and analyze images, leading to state-of-the-art clustering performance on various datasets.\n\n2. **Improvement in Clustering Accuracy**: Through the optimization of the rate reduction objective using pre-trained features, the pipeline achieves a noticeable improvement in clustering accuracy, as demonstrated on ImageNet-1k.\n\n3. **Self-Labeling Algorithm**: The pipeline includes a simple yet effective self-labeling algorithm that leverages CLIP\u2019s vision-text capabilities, resulting in semantically meaningful clusters that are comprehensible to humans."
                },
                "weaknesses": {
                    "value": "1. **Limited Innovation in Methodology**: The main innovation of the proposed method seems to be centered around utilizing features extracted by CLIP for initialization, but there appears to be a lack of novelty in the algorithmic aspect of the approach.\n2. **Concerns about Stability and Sensitivity**: As depicted in Fig.4, the vicinity of the extreme points in model selection appears quite flat, raising concerns about the algorithm's stability and its sensitivity to perturbations, such as the choice of hyperparameters and network architecture.\n3. **Potential Information Leakage**: Given that CLIP has been trained on vast amounts of data, there is a suspicion of cluster/label information leakage in almost all of the experimental data (CIFAR and ImageNet) presented, which could potentially bias the quantitative results.\n4. **Lack of Adequate Metrics for Text Labeling**: While the automated text annotation aspect of the pipeline is interesting, it seems to be lacking appropriate metrics to adequately evaluate and validate its performance."
                },
                "questions": {
                    "value": "Please refer to the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5846/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826475293,
            "cdate": 1698826475293,
            "tmdate": 1699636618331,
            "mdate": 1699636618331,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yG4sXFtS7y",
                "forum": "ptCIlV24YZ",
                "replyto": "Tl59uLr7IL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5846/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5846/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 5jgz"
                    },
                    "comment": {
                        "value": "Dear Reviewer 5jgz:\n\nThank you for the helpful comments and interesting questions! We appreciate that you recognize the contribution of our work (leverages pretrain models, improves cluster accuracy and proposes self-label algorithm).  We respond to your questions below.\n\n>Lack of Adequate Metrics for Text Labeling: While the automated text annotation aspect of the pipeline is interesting, it seems to be lacking appropriate metrics to adequately evaluate and validate its performance.\n\nCPP is one of the first methods to estimate text labels for **learned clusters** on datasets without ground-truth clusters or labels, thus we could not find existing metrics to evaluate. Still, we agree you have raised an interesting point.  To give some insights, we compute a simple metric for text labeling. We take the embeddings of images and the text label of each learned cluster using image and text encoders of CLIP ViT L-14, and compute the mean cosine similarity between image-text pairs. The mean similarity is around 0.22 to 0.3 for WikiArt and CIFAR-10. In contrast, irrelevant labels yield scores lower than 0.15. Our observation of the score frequency distribution aligns with Gadre et al[1]. For more quantitative and qualitative results, please kindly refer to Figure 15 in rebuttal supplementary material. We will summarize the results in the revised versions.\n\n>Concerns about Stability and Sensitivity: As depicted in Fig.4, the vicinity of the extreme points in model selection appears quite flat, raising concerns about the algorithm's stability and its sensitivity to perturbations, such as the choice of hyperparameters and network architecture.\n\nWe\u2019d like to clarify that:\n\n- Algorithm 1 is a post-hoc procedure after training. Hence, it will not impact our method\u2019s stability.\n\n- Nevertheless, we think it crucial to examine the algorithm's stability in general. We summarize the ablation studies we have conducted and added in this rebuttal period below:\n\n1. Our method improves as the pretrain model improves: we explore different pretraining: CLIP, MAE, DINO, and OpenCLIP. We have presented the results in Appendix A.1. and Table 9 in rebuttal supplementary material. \n\n2. Diversified Initialization is important: we presented the ablation study of optimizing equation (4) in Appendix B.\n\n3. Our method is  quite robust to hyperparameters: We additionally reported the effect of a different choice of $\\gamma$ for sinkhorn distance projection. Please kindly refer to Appendix C.1. and Table 7(c) for definition, Table 10 in rebuttal supplementary material for additional results.\n\n>Potential Information Leakage: Given that CLIP has been trained on vast amounts of data, there is a suspicion of cluster/label information leakage in almost all of the experimental data (CIFAR and ImageNet) presented, which could potentially bias the quantitative results.\n\nWe are with you on this point. Indeed, in the age of large pre-trained models, it is unlikely, if not impossible, that an image dataset publicly available for download has not been seen by pre-trained models. The only way to alleviate potential information leakage is to collect a private dataset that is available only for testing but not training, which we believe would be an interesting future contribution to the community.\n\nNevertheless, we hope the following experiments reinforce the robustness of our evaluation. \n\nWe presented additional quantitative evaluations of the clustering step in CPP leveraging: \n\n1. DINO[2] ViT(Appendix A.1) pre-trained on pure visual data.\n\n2. OpenCLIP[3](rebuttal supplementary material Table 9) pre-trained on LAION[4]. \n\nThere is less risk of \"Information Leakage\" for these models, since they were pre-trained in transparent settings of training strategy and data source. Notably, leveraging both of these models, CPP achieves competitive results on ImageNet-1k (DINO 61.9%, OpenCLIP 66.8%), with explicit improvement over KMeans. We hope these results prove the robustness of our evaluation and alleviate your concern.\n\n>Limited Innovation in Methodology: The main innovation of the proposed method seems to be centered around utilizing features extracted by CLIP for initialization, but there appears to be a lack of novelty in the algorithmic aspect of the approach.\n\nWe clarified the novelty and significance of the paper in the General Response above. Hopefully that alleviates your concern."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700119224738,
                "cdate": 1700119224738,
                "tmdate": 1700119224738,
                "mdate": 1700119224738,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GtfhRuREO3",
                "forum": "ptCIlV24YZ",
                "replyto": "Tl59uLr7IL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5846/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5846/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "Dear Reviewer 5jgz,\n\nWe are grateful for your efforts in the review, and we hope our response together with extra results covered your concerns. We are more than happy to further clarify or address additional questions. Please let us know if you still have any unclear parts of our work.\n\nSincerely, Authors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700554992437,
                "cdate": 1700554992437,
                "tmdate": 1700554992437,
                "mdate": 1700554992437,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0njcXHDVQk",
            "forum": "ptCIlV24YZ",
            "replyto": "ptCIlV24YZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5846/Reviewer_hovR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5846/Reviewer_hovR"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a novel method that leverages the rate reduction principle to learn to do image clustering using pretrained models. A technique for automatically select the optimal number of clusters is also proposed based on the same principle. Finally, a self-labeling mechism is proposed to label the clusters with semantic labels.\nExperiments show that the proposed method achieves a good performance, as well as give a good estimation of the optimal number of clusters."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. This paper provides an alternative way of performing image clustering, which seems to be performing well and could be of interesting for the community.\n2. The method enables automatic estimation of the optimal number of clusters in a dataset, from the result the method seems to perform pretty well."
                },
                "weaknesses": {
                    "value": "1. The main experiments are done on somewhat small datasets like CIFAR, or coarse grained dataset like COCO, the paper would be stronger if it could include finer-grained dataset for clustering like iNaturalist."
                },
                "questions": {
                    "value": "1. I would be interesting in how the method perform on fine-grained datasets.\n2. It would be better if the paper could include results of using other variant of CLIP models, such OpenCLIP."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5846/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698926104810,
            "cdate": 1698926104810,
            "tmdate": 1699636618230,
            "mdate": 1699636618230,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5FDFKAbRSU",
                "forum": "ptCIlV24YZ",
                "replyto": "0njcXHDVQk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5846/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5846/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer hovR"
                    },
                    "comment": {
                        "value": "Dear reviewer `hovR`:\n\nThank you for the helpful comments and interesting questions! We appreciate that you recognize that our work enables automatic estimation of optimal number of clusters and could be interesting for the community. We respond to your questions below.\n\n>It would be better if the paper could include results of using other variant of CLIP models, such OpenCLIP.\n\nThank you for your valuable suggestion! We have conducted new experiments and reported the results in Table 9, rebuttal supplementary material. From our quantitative evaluation, we observed that OpenCLIP-ViT-L/14 achieved even better clustering accuracy on CIFAR-20, CIFAR-100, and ImageNet-1k compared with the original CLIP from OpenAI. It demonstrates great potential of our work, because it  improves as the pretrain model continues to evolve. \n\n>The main experiments are done on somewhat small datasets like CIFAR, or coarse grained dataset like COCO, the paper would be stronger if it could include finer-grained dataset for clustering like iNaturalist.\n\nThank you for your suggestion! To clarify, we\u2019ve already conducted experiments on large datasets such as LION-Aesthetic, which contains over 2.7 million images. Please kindly refer to our results in Figure 4, 5, and 14, where we observed that CPP performed well qualitatively, i.e. conceptually meaningful clusters with labels. Nevertheless, we appreciate your advice. We believe that a quantitative validation on finer-grained datasets will further solidify our results. Due to limited time in the rebuttal period, we will conduct experiments on iNaturalist and upload the results in future revisions."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700117514362,
                "cdate": 1700117514362,
                "tmdate": 1700117596768,
                "mdate": 1700117596768,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9MXofgtfqS",
                "forum": "ptCIlV24YZ",
                "replyto": "5FDFKAbRSU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5846/Reviewer_hovR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5846/Reviewer_hovR"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for the response, it clarifies my concern. I would keep my positive score for this paper, and I would forward to the open source of the code of the paper"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483400181,
                "cdate": 1700483400181,
                "tmdate": 1700483400181,
                "mdate": 1700483400181,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "funIzBLHkq",
            "forum": "ptCIlV24YZ",
            "replyto": "ptCIlV24YZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5846/Reviewer_2xo4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5846/Reviewer_2xo4"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the image clustering problem in the age of large pre-trained models. Specifically, this paper develops a method to determine the cluster number in a given dataset. Then, this paper validates that the features from large pretrained models, such as CLIP, help achive better custering accuracy than the traditional feature pre-training. Moreover, this paper also develops a self-labeling method to produce text labels for the clusters. Experiments on many image datasets, including ImageNet-1k, demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper achieves state-of-the-art results on many image datasets, including ImageNet-1k."
                },
                "weaknesses": {
                    "value": "Three main technique contributions are developed in this paper, including a method to determine the cluster number, a validation that the features from CLIP push the limits of image clustering, and a self-labeling method to annotate the text-labels for the clusters. I have three concerns about these three technique contributions:\n\n- This paper seems did not discuss the existing methods to determine the cluster number and the difference among them. Do all the existing clustering methods not discuss how to determine the cluster number?\n- The proposed clustering method is a simple combination between CLIP features and MLC optimization method. I realize it is meaningful to validate the superiority of CLIP features in image clustering, but the technique contribution itself is kind of subtle.\n- A self-labeling method to annotate the text-labels for the clusters in *Algorithm 2* simply uses a cosine similarity metric to determine which texts are the closest ones given text candidates, which is a very simple solution. It does not meet my expectations that the proposed self-labeling method strongly relies on the pre-define text candidates. What if the text candidates are not given?"
                },
                "questions": {
                    "value": "See the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5846/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5846/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5846/Reviewer_2xo4"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5846/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699350557804,
            "cdate": 1699350557804,
            "tmdate": 1700653103315,
            "mdate": 1700653103315,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ng8VHEUb81",
                "forum": "ptCIlV24YZ",
                "replyto": "funIzBLHkq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5846/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5846/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 2xo4"
                    },
                    "comment": {
                        "value": "Dear reviewer `2xo4`:\n\nThank you for the helpful comments and interesting questions! We appreciate that you recognize that our work achieved state-of-the-art on many datasets including ImageNet-1k. We respond to your questions below.\n\n>This paper seems did not discuss the existing methods to determine the cluster number and the difference among them. Do all the existing clustering methods not discuss how to determine the cluster number?\n\nGood question! The short answer to this question is No, existing deep clustering methods do not discuss how to determine the cluster number. We would like to clarify that 1) determining the number of clusters is a classical topic, 2) yet it is missing in modern deep clustering works.\n\nWe go into further details below\n\n**Determining the number of clusters is a classical topic**\n\nThe general paradigm to select the number of clusters, one first runs the clustering algorithm to estimate k clusters, then computes some score that measures how well the data fit the cluster model and how complex the cluster model is. Computing such scores is a well-studied topic: it can be done based on various assumptions on the geometric/statistical structures of the cluster models. Examples include the elbow method (as 7qGL also pointed out), Akaike/Bayesian Information Criterion.  We thank the reviewer for raising this question and we will add the discussion here to our revised version of the text. \n\n**Yet it is often missing in modern deep clustering works**\n\nNote, however, that one needs to estimate many cluster models, each corresponding to one choice of k. For most deep clustering works, this means retraining a deep network (or fine-tuning a cluster head) for each k, which is costly especially when data reaches ImageNet scale. Therefore, these works typically assume knowledge of the true number of clusters or an upper bound of it. Our method is different, because it learns the representation and membership (with deep networks) that do not rely on the number of clusters. This makes CPP suitable for real-world scale data. \n\n>A self-labeling method to annotate the text-labels for the clusters in Algorithm 2 simply uses a cosine similarity metric to determine which texts are the closest ones given text candidates, which is a very simple solution. It does not meet my expectations that the proposed self-labeling method strongly relies on the pre-defined text candidates. What if the text candidates are not given?\n\nVery interesting question! To clarify, we design Algorithm 2 in a time-efficient manner, as cosine similarity requires minimum computational cost. This design choice is partly inspired by the success of dual encoder-style VLMs such as CLIP[5]. These models have demonstrated their effectiveness in Image-Text retrieval due to their ability to pre-compute and store feature vectors. Leveraging this property, we can preprocess a large-scale open vocabulary of text candidates, thus alleviating the bias of candidate selection. \n\n>The proposed clustering method is a simple combination between CLIP features and MLC optimization method. I realize it is meaningful to validate the superiority of CLIP features in image clustering, but the technique contribution itself is kind of subtle.\n\nWe clarified the novelty and significance of the paper in the General Response above. Hopefully that alleviates your concern."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700117358048,
                "cdate": 1700117358048,
                "tmdate": 1700501367633,
                "mdate": 1700501367633,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9ljoT0xUYX",
                "forum": "ptCIlV24YZ",
                "replyto": "funIzBLHkq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5846/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5846/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "Dear Reviewer 2xo4,\n\nWe are grateful for your efforts in the review, and we hope our response together with extra results covered your concerns. We are more than happy to further clarify or address additional questions. Please let us know if you still have any unclear parts of our work.\n\nSincerely, Authors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700554803189,
                "cdate": 1700554803189,
                "tmdate": 1700554803189,
                "mdate": 1700554803189,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VP7DoAgs5Z",
                "forum": "ptCIlV24YZ",
                "replyto": "9ljoT0xUYX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5846/Reviewer_2xo4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5846/Reviewer_2xo4"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for your rebuttal. I acknowledge the state-of-the-art clustering performance on large-scale datasets, and very appreciate it. Despire the proposed method pushes the limits of clustering, I am still concerned about the technique novelty, as mentioned by the authors that CPP indeed combines CLIP and MLC, and the experimental performance is just one aspect to evaluate a paper. For example, someone can exploit a more advanced CLIP-like pretrained model to combine with another existing state-of-the-art clustering method to further push the limits of image clustering. I would like to raise the rating from reject to borderline reject, and let AC to make the final decision."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653075031,
                "cdate": 1700653075031,
                "tmdate": 1700653075031,
                "mdate": 1700653075031,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]