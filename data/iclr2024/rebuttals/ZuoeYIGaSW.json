[
    {
        "title": "$\\texttt{PREMIER-TACO}$ is a Few-Shot Policy Learner: Pretraining Multitask Representation via Temporal Action-Driven Contrastive Loss"
    },
    {
        "review": {
            "id": "j28DSc0BGP",
            "forum": "ZuoeYIGaSW",
            "replyto": "ZuoeYIGaSW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5198/Reviewer_YiPn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5198/Reviewer_YiPn"
            ],
            "content": {
                "summary": {
                    "value": "The paper aims to learn representations for sequential decision-making tasks. Based on temporal action contrastive learning (TACO), the authors adopt a negative sampling strategy to improve the representation especially in multitask contexts. Employing a shallow ConvNet, the authors benchmark their method on Deepmind Control Suite and MetaWorld."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-motivated with careful discussions on the challenges and criteria for learning decision-making representations, as well as the shortage of baseline TACO.\n2. The introduced method, Premier-TACO, seems easy to implement.\n3. The results indicate the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1. The introduced Premier-TACO shows incremental contribution over the baseline TACO. Specifically, the only difference is the contrastive loss adopted by TACO and the triplet loss adopted by Premier-TACO. The technique of negative sampling is quite common in the area of metric learning and widely adopted in applications other than robotics, e.g., face recognition. Moreover, the empirical comparison with the baseline TACO is very limited in this paper. The effectiveness of simply adding negative sampling is questionable.\n\n2. The authors repeated several times the infeasibility of adopting visual foundation models (such as those trained on ImageNet or Ego4D) in sequential decision-making tasks. However, it is ungrounded. The authors should evaluate these models (e.g., CLIP, DINOv2, EgoVLP, etc.) on the same benchmark for comparison.\n\n3. The model adopted in the paper is quite small. The pretraining is only performed on synthetic data and also small-scale. The practicality of the method needs further study."
                },
                "questions": {
                    "value": "Why TACO in Fig.9 achieves the same results with different batch sizes?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5198/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698661480779,
            "cdate": 1698661480779,
            "tmdate": 1699636516492,
            "mdate": 1699636516492,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ydRfkdRDe2",
                "forum": "ZuoeYIGaSW",
                "replyto": "j28DSc0BGP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5198/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5198/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YiPn"
                    },
                    "comment": {
                        "value": "Thanks Reviewer YiPn for the valuable review of our paper. We appreciate the questions you raised and are committed to delivering a comprehensive response to address the issues.\n\n---\n> 1. The introduced Premier-TACO shows incremental contribution over the baseline TACO. Specifically, the only difference is the contrastive loss adopted by TACO and the triplet loss adopted by Premier-TACO. The technique of negative sampling is quite common in the area of metric learning and widely adopted in applications other than robotics, e.g., face recognition. Moreover, the empirical comparison with the baseline TACO is very limited in this paper. The effectiveness of simply adding negative sampling is questionable. \n\nIn response to your suggestion, we have also included additional discussions on existing negative sampling strategies for contrastive learning related to Premier-TACO in Appendix A. We have also updated Table 1 and 2 with TACO as a separate column and updated Figure 9 with a comparison of Premier-TACO vs. TACO in more detail. \n\nFirst, Premier-TACO is tailored toward a fundamentally different setting than TACO. While TACO is designed for task-specific representation learning in single-task offline and online reinforcement learning, Premier-TACO aims for a broader representation pretraining objective, which is to learn a universal/foundational visual representation from multitask offline datasets. To scale up to multitask offline pretraining, we propose the simple yet highly effective negative sampling strategy in Premier-TACO, which is the key to both scalability and great empirical performance. Without this modification, TACO is not competitive, as shown in the updated Table 1 and 2 as well as Figure 9.\n\nNext, we believe that the general question of multitask representation learning as a foundational model for efficient downstream policy learning itself holds substantial significance for both the machine learning and robotics communities. We acknowledge that our approach bears some resemblance to existing hard negative sampling strategies in contrastive learning and metric learning literature. However, few of them have been successfully applied to the sequential decision making setting, which has to deal with unique challenges such as data distribution shifts between data collection policy and learned policy, task heterogeneity, and pretraining data qualities, as discussed in the introduction.\n\nFurthermore, in our paper, we also discuss the additional aspects of generalization to unseen embodiments and camera views, pretraining dataset quality, and, importantly, how to finetune/adapt existing pre-trained visual encoders for control. These aspects are unique and also crucial to pretraining for sequential decision-making tasks and are under-explored in the existing literature. Many existing works pretrain only on vision datasets and then apply the pretrained vision foundational models directly for downstream policy learning. Thus they often overlook control-relevant considerations and thus suffer from a domain gap between pretraining datasets and downstream visuomotor tasks. Addressing this gap, Premier-TACO proposes a nice solution that could effectively adapt large pre-trained visual encoders for control purposes, as illustrated in Figure 8. Therefore, we firmly believe that this adds significant value to our contribution.\n\n> 2. The authors repeated several times the infeasibility of adopting visual foundation models (such as those trained on ImageNet or Ego4D) in sequential decision-making tasks. However, it is ungrounded. The authors should evaluate these models (e.g., CLIP, DINOv2, EgoVLP, etc.) on the same benchmark for comparison.\n\nWe would like to kindly point out that in the main experimental results of Table 1 and Table 2, we have already compared with the **best** pretrained visual encoders (PVRs) / vision foundational models across 4 existing works on applying PVRs for control tasks. (We present the best PVR results in Table 1, and the results of all four PVRs are in Table 3 of Appendix B.2.) \nWith a significant performance gain across 18 unseen tasks and 2 different domains, we show that existing works on adapting visual foundational models directly to downstream control tasks with large domain gaps without domain-specific finetuning are suboptimal."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699976823194,
                "cdate": 1699976823194,
                "tmdate": 1700539493833,
                "mdate": 1700539493833,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "c1aIh4lyfh",
                "forum": "ZuoeYIGaSW",
                "replyto": "j28DSc0BGP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5198/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5198/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer YiPn,\n\nSince today is the last day of the author-reviewer discussion period, we kindly ask you to review our previous response and revised manuscript and let us know if there are any other questions. We are happy to answer any additional questions you may have. Thank you."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689720384,
                "cdate": 1700689720384,
                "tmdate": 1700689815745,
                "mdate": 1700689815745,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "B0qNl6mHBY",
            "forum": "ZuoeYIGaSW",
            "replyto": "ZuoeYIGaSW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5198/Reviewer_7fxb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5198/Reviewer_7fxb"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Premier-TACO, a few-shot policy learner for sequential decision-making tasks. This method is build upon the existing work, i.e., temporal action contrastive learning (TACO) objective, and employs a negative example sampling strategy, which is beneficial for large-scale multitask offline pretraining. Experiments on Deepmind Control Suite and MetaWorld show superior performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The performance on both seen and unseen tasks are superior than other methods.\n2. The proposed one negative sample selection is reasonable since it is harder when selecting the negative sample among a window slot than selecting from a batch as in TACO."
                },
                "weaknesses": {
                    "value": "1. It would be better to add an ablation of using the negative sample strategy in TACO on sequential decision-making tasks. It is important to show the effectiveness of the proposed negative sample selection strategy.\n2. Premier-TACO uses additional negative samples selected from a temporal window. Compared with TACO, is the batch size doubled? If it is true, what is the result when decreasing the batch size of Premier-TACO to 1/2$N$ compared with TACO with $N$.\n3. Does the selection number influence the performance? How about select more than one samples as negatives? \n4. Similarly, the negative sample is selected randomly from $W$ window. How about select the hardest one or easiest one from $W$ window?\n5. Is Premier-TACO model-free? If yes, can it be applied on other model structures used in the previous methods, e.g., SPR?"
                },
                "questions": {
                    "value": "Please refer to the Weakness Section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5198/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698724153098,
            "cdate": 1698724153098,
            "tmdate": 1699636516393,
            "mdate": 1699636516393,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VGx9aUHjpX",
                "forum": "ZuoeYIGaSW",
                "replyto": "B0qNl6mHBY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5198/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5198/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Reviewer 7fxb"
                    },
                    "comment": {
                        "value": "Thank you for the valuable review of our paper. We appreciate the questions you raised and are committed to delivering a comprehensive response to address the issues.\n\n---\n> 1. It would be better to add an ablation of using the negative sample strategy in TACO on sequential decision-making tasks. It is important to show the effectiveness of the proposed negative sample selection strategy.\n\nThanks for your constructive feedback. we have made updates to Figure 9, presenting a comparison of the averaged performance across 10 unseen tasks from Deepmind Control Suite between Premier-TACO (with different batch sizes) and TACO. Additionally, we have revised Table 1 and Table 2 by incorporating an additional column for TACO in the baselines, facilitating a direct comparison with Premier-TACO, as per your suggestion. These modifications to Table 1 and 2 as well as the updated Figure 9 effectively illustrate the efficacy of the novel negative sample selection strategy in Premier-TACO, where Premier-TACO outperforms TACO with at least 40\\% across all batch sizes. We believe that these additions should address your concern on the empirical comparison against TACO baseline and we welcome any further discussion or suggestions from the reviewer.\n\n\n\n---\n> 2. Premier-TACO uses additional negative samples selected from a temporal window. Compared with TACO, is the batch size doubled? If it is true, what is the result when decreasing the batch size of Premier-TACO to 1/2N compared with TACO with N?\n\nThank you for your question regarding the batch size used in Premier-TACO. To clarify, the batch size in Premier-TACO is not doubled compared to TACO. As outlined on page 9, line 2 of our manuscript, and demonstrated in Figure 9, we compare Premier-TACO and TACO using an fixed batch size of 4096. This batch size is consistent with that used in our main experimental results (Tables 1 and 2). \n\nTo address your question further, we have updated Figure 9 to include comparisons of Premier-TACO and TACO both at different batch sizes. This additional analysis should provide a clearer understanding of how batch size variations impact the performance of both methods. \n\nFinally, in the update to Table 1, we have included a column for TACO where it is pretrained with the same batch size of 4096, aligning it with other contrastive learning baselines. This ensures that the comparisons made are fair and consistent across different methods.\n\n\n> 3. Does the selection number influence the performance? How about select more than one samples as negatives?\n\nThank you for this insightful question. Indeed, we have considered this in the original design of our learning objective. In the table below, we compare the performance when selecting a single negative sample randomly from a size W window against when using all samples within this window across 10 unseen Deepmind Control Suite tasks. \n\n|                | Sampling 1     | Sampling All|\n| -----------    | ----     | ----------- |\n| Finger Spin    | 75.2 \u00b1 0.6 | 70.2 \u00b1 8.4 |\n| Hopper Hop     | 75.3 \u00b1 4.6 | 76.1 \u00b1 3.0 |\n| Walker Walk    | 88.0 \u00b1 0.8 | 88.5 \u00b1 0.4 |\n| Humanoid Walk  | 51.4 \u00b1 4.9 | 56.4 \u00b1 8.9 |\n| Dog Trot       | 93.9 \u00b1 5.4 | 92.1 \u00b1 4.0 |\n| Cup Catch      | 98.9 \u00b1 0.1 | 98.3 \u00b1 1.6 |\n| Reacher Hard   | 81.3 \u00b1 1.8 | 80.1 \u00b1 5.8 |\n| Cheetah Run    | 65.7 \u00b1 1.1 | 69.3 \u00b1 2.3 |\n| Quadruped Walk | 83.2 \u00b1 5.7 | 85.4 \u00b1 4.  |\n| Quadruped Run  | 76.8 \u00b1 7.5 | 82.1 \u00b1 9.1 |\n| Overall        |    79.0    |  79.8 |\n\n\nOur findings indicate that using all samples from the size W window does not significantly enhance performance compared to Premier-TACO. Moreover, this approach considerably increases the computational overhead. Given these results, we chose a more computationally efficient strategy of sampling a single negative example from the size W window. We appreciate this question that you raised, and we have incorporated it into Appendix D of the updated manuscript."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699976591335,
                "cdate": 1699976591335,
                "tmdate": 1700539449725,
                "mdate": 1700539449725,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BuC7XkS9kn",
                "forum": "ZuoeYIGaSW",
                "replyto": "B0qNl6mHBY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5198/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5198/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 7fxb,\n\nSince today is the last day of the author-reviewer discussion period, we kindly ask you to review our previous response and revised manuscript and let us know if there are any other questions. We are happy to answer any additional questions you may have. Thank you."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689749206,
                "cdate": 1700689749206,
                "tmdate": 1700689829152,
                "mdate": 1700689829152,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Y4Qi3FdeYH",
            "forum": "ZuoeYIGaSW",
            "replyto": "ZuoeYIGaSW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5198/Reviewer_AXkc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5198/Reviewer_AXkc"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Premier-TACO, a multitask feature representation learning method, aiming to enhance the efficiency of few-shot policy learning in sequential decision-making tasks. Premier-TACO pretrains a general feature representation using s small subset of multitask offline datasets and then fine-tunes the network to specific tasks with a few experts. Additionally, Premier-TACO employs a negative example sampling strategy on contrastive learning objectives. Experimental results show that Premier-TACO can outperform the state-of-the-art on DeepMind Control Suite and MetaWorld."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe paper is well-written and easy to follow.\n2.\tThe proposed method Premier-TACO can simultaneously achieve versatility, efficiency, robustness, and compatibility. \n3.\tEmpirical results demonstrate that Premier-TACO can achieve SOTA results on several benchmarks."
                },
                "weaknesses": {
                    "value": "1.\tThe novelty is somewhat limited. The proposed method is built on the temporal action constrastive learning (TACO) objective [1]. The overall framework is similar to [1]. The authors additionally employ a negative example sampling strategy. But the negative sampling has been widely used in constrastive learning [2][3]. Considering the above factors, I think that the innovation of the method is limited\n2.\tThe detailed experimental comparisons and discussions with TACO are missed. \n3.\tIn the ablation study, in order to show the effectiveness of the proposed negative example sampling strategy, the authors should compare Premier-TACO with a baseline without using a negative example sampling strategy. The related experimental results should be added.\n\n[1] TACO: Temporal latent action-driven contrastive loss for visual reinforcement learning, NeurIPS 2023.\n\n[2] Robust Contrastive Learning Using Negative Samples with Diminished Semantics, NeurIPS 2021.\n\n[3] Hard Negative Sampling Strategies for Contrastive Representation Learning, arxiv 2022."
                },
                "questions": {
                    "value": "See Weakness for detail."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5198/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5198/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5198/Reviewer_AXkc"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5198/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699274523276,
            "cdate": 1699274523276,
            "tmdate": 1699636516302,
            "mdate": 1699636516302,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YCJdWAhFCI",
                "forum": "ZuoeYIGaSW",
                "replyto": "Y4Qi3FdeYH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5198/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5198/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer AXkc"
                    },
                    "comment": {
                        "value": "Thanks Reviewer AXKc for the valuable review of our paper. We appreciate the questions you raised and are committed to delivering a comprehensive response to address the issues.\n\n---\n> 1. The novelty is somewhat limited. The proposed method is built on the temporal action constrastive learning (TACO) objective [1]. The overall framework is similar to [1]. The authors additionally employ a negative example sampling strategy. But the negative sampling has been widely used in constrastive learning [2][3]. Considering the above factors, I think that the innovation of the method is limited.\n\nIn response to your suggestion, we have also included additional discussions on existing negative sampling strategies for contrastive learning related to Premier-TACO in the Appendix A. Here, we would like to highlight several key contributions of our work that distinguish our method from TACO [1] and the existing works in hard negative mining  [2,3]. \n\nFirst, Premier-TACO is tailored toward a fundamentally different setting than TACO. While TACO is designed for task-specific representation learning in single-task offline and online reinforcement learning, Premier-TACO aims for a broader representation pretraining objective, which is to learn a universal/foundational visual representation from multitask offline datasets. To scale up to multitask offline pretraining, we propose the simple yet highly effective negative sampling strategy in Premier-TACO, which is the key to both scalability and great empirical performance.\n\nNext, we believe that the general question of multitask representation learning as a foundational model for efficient downstream policy learning itself is holds substantial significance for both the machine learning and robotics communities. In this paper, we show that with the negative example strategy employed by Premier-TACO, it achieves an impressive performance gain compared with a comprehensive set of baseline pretraining methods across two domains. We acknowledge that our approach bears some resemblance to existing hard negative sampling strategies in contrastive learning literature. However, few of them have been successfully applied to the sequential decision making setting, which has to deal with unique challenges such as data distribution shifts between data collection policy and learned policy, task heterogeneity, and pretraining data qualities, as discussed in the introduction.\n\nFurthermore, in our paper, we also discuss the additional aspects of generalization to unseen embodiments and camera views, pretraining dataset quality, and, importantly, how to finetune/adapt existing pre-trained visual encoders for control. These aspects are unique and also crucial to pretraining for sequential decision-making tasks and are under-explored in the existing literature. Many existing works pretrain only on vision datasets and then apply the pretrained vision foundational models directly for downstream policy learning. Thus they often overlook control-relevant considerations and thus suffer from a domain gap between pretraining datasets and downstream visuomotor tasks. Addressing this gap, Premier-TACO proposes a nice solution that could effectively adapt large pre-trained visual encoders for control purposes, as illustrated in Figure 8. Therefore, we firmly believe that this adds significant value to our contribution.\n\n> 2. The detailed experimental comparisons and discussions with TACO are missed.\n\nThank you for your feedback regarding the comparison with the baseline TACO. In Figure 9 of the original manuscript, we have already presented a comparison of the averaged performance across 10 unseen tasks from the Deepmind Control Suite between Premier-TACO (with different batch sizes) and TACO. We appreciate your suggestion to highlight this comparison in the paper, and so we have updated Table 1 accordingly. This revision includes an additional column for TACO in the baselines, enhancing the clarity of our empirical comparison. For the results of MetaWorld in Table 2, the experiments are still running. We will incorporate the results into our manuscript as soon as they are available.\n\n====== Update: results of TACO for MetaWorld (Table 2) are now updated ==========="
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699976379007,
                "cdate": 1699976379007,
                "tmdate": 1700539384865,
                "mdate": 1700539384865,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XTyA5ce6Fv",
                "forum": "ZuoeYIGaSW",
                "replyto": "Y4Qi3FdeYH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5198/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5198/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> 3. In the ablation study, in order to show the effectiveness of the proposed negative example sampling strategy, the authors should compare Premier-TACO with a baseline without using a negative example sampling strategy. The related experimental results should be added.\n\nThank you for your suggestion on the additional empirical comparison. In our evaluation now, we have already compared with SPR, which applies a BYOL-based objective that does not leverage negative examples. Additionally, we have also compared with TACO, which uses a naive negative example sampling strategy and treats every other data point in the batch as negative examples. Furthermore, Figure 9 of our ablation study is now updated to include a more fine-grained comparison between Premier-TACO and TACO with different batch sizes. With a significant performance gain shown by Premier-TACO, the results should be sufficient to demonstrate the effectiveness of the negative sampling strategy proposed in Premier-TACO. If there are specific additional baselines that don't use negative sampling that you believe are important to compare against, please let us know."
                    },
                    "title": {
                        "value": "Response to Reviewer AXkc"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699976400842,
                "cdate": 1699976400842,
                "tmdate": 1699977202131,
                "mdate": 1699977202131,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uhee0cB0zV",
                "forum": "ZuoeYIGaSW",
                "replyto": "Y4Qi3FdeYH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5198/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5198/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Any additional questions?"
                    },
                    "comment": {
                        "value": "Dear Reviewer AXkc,\n\nIn our earlier response and revised manuscript, we have conducted additional experiments and provided detailed clarifications based on your questions and concerns. As we are ending the stage of the author-reviewer discussion soon, we kindly ask you to review our revised paper and our response and consider adjusting the scores if our response has addressed all your concerns. Otherwise, please let us know if there are any other questions. We would be more than happy to answer any further questions."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700539593310,
                "cdate": 1700539593310,
                "tmdate": 1700539604219,
                "mdate": 1700539604219,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "68StfdyO8A",
                "forum": "ZuoeYIGaSW",
                "replyto": "Y4Qi3FdeYH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5198/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5198/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer AXkc,\n\nSince today is the last day of the author-reviewer discussion period, we kindly ask you to review our previous response and revised manuscript and let us know if there are any other questions. We are happy to answer any additional questions you may have. Thank you."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5198/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689781216,
                "cdate": 1700689781216,
                "tmdate": 1700689840688,
                "mdate": 1700689840688,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]