[
    {
        "title": "LIPEx -- Locally Interpretable Probabilistic Explanations -- To Look Beyond The True Class"
    },
    {
        "review": {
            "id": "aZnRheEwwS",
            "forum": "R7946uagL2",
            "replyto": "R7946uagL2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4219/Reviewer_BvXE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4219/Reviewer_BvXE"
            ],
            "content": {
                "summary": {
                    "value": "Summary:\n\nThe paper proposes a new perturbation-based multi-class explanation framework, LIPEx. While LIME learns a regression model to identify the important features to the prediction of one class, LIPEx aims to learn an explanation matrix that highlights the features crucial to the predictions of all classes. Experimental results demonstrate that LIPEx provides explanations that are more faithful than those produced by LIME and some other popular XAI methods. Additionally, it has been shown that LIPEx operates with greater efficiency than LIME."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The paper is well-structured and well-written, making it easy to understand.\n\n2. The research question, which revolves around determining the importance of different features for all possible class likelihoods estimated by a classifier, is indeed crucial and deserves further research."
                },
                "weaknesses": {
                    "value": "1. Missing many related works.\n\nIn the statement of paper, \"The full spectrum of feature influence on each class at a particular data point can help to understand how well the model has been trained to discriminate a particular class from the rest. However, existing explanation frameworks do not provide any clue on the aforementioned issue,\" the authors may have overstated the absence of existing frameworks that address this problem.\n\nWhile it may be true that a comprehensive solution providing a full spectrum of feature influence on each class may not exist yet, several works have made attempts to explain not just the top-1 class, but also other classes. These include various counterfactual and contrastive explanation frameworks, which provide insights into \"why A but not B\" and \"why B but not A\" scenarios. For instance, consider the following papers:\n\n- \"Contrastive Explanations for Model Interpretability\" (Jacovi et al., 2021)\n\n- \"Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR\" (Wachter et al., 2018)\n\n- \"Counterfactual Visual Explanations\" (Goyal et al., 2019)\n\n- \"SCOUT: Self-aware Discriminant Counterfactual Explanations\" (Wang & Vascon, 2020)\n\n- \"Two-Stage Holistic and Contrastive Explanation of Image Classification\" (Xie et al., 2023)\n\n- \"CoCoX: Generating Conceptual and Counterfactual Explanations via Fault-Lines\" (Akula  et al., 2020)\n\n- \"Why Not Other Classes?: Towards Class-Contrastive Back-Propagation Explanations\" (Wang et al., 2022)\n\nThese studies are not referenced or evaluated in the paper under review.\n\n2. \n\nIn the paper's faithfulness evaluation using removal-based methods (which involves the elimination of top features identified by explanations), two concerns arise:\n\n2a. The stronger baseline of perturbation-based explanation methods, such as RISE (as discussed in this [paper](https://arxiv.org/pdf/1806.07421.pdf)), is not taken into account. The current experiment only considers LIME and Occlusion, which limits the comprehensiveness of the findings.\n\n2b. The current evaluation is solely focused on the top-1 prediction class. However, the authors claim that their paper's goal is to provide an explanation framework for all classes. As such, evaluations should also be extended to other classes \u2014 at least for the class with the second highest probability, as done in this [study](https://openreview.net/forum?id=X5eFS09r9hm). When evaluating the class with the second highest probability, a comparison with contrastive explanation methods should be included to ensure a more inclusive and in-depth evaluation.\n\n3.\n\nConcerning interpretability for human users, it's challenging to distinguish a substantial improvement of LIPEx over LIME, particularly in the sole visual example provided - Figure 8. Moreover, since the authors claim that their proposed method can effectively explain other classes, it would be beneficial to see more examples where LIPEx offers users a better overall understanding of model behaviors by checking the important features for different predicted classes. \n\nOverall, I do not see a significant improvement of LIPEx over LIME and other current XAI methods at this moment."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Not applicable."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4219/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4219/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4219/Reviewer_BvXE"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4219/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698035047200,
            "cdate": 1698035047200,
            "tmdate": 1699636389012,
            "mdate": 1699636389012,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "wKq59I1oOg",
            "forum": "R7946uagL2",
            "replyto": "R7946uagL2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4219/Reviewer_pXhu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4219/Reviewer_pXhu"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new explainability framework building upon LIME, a method that uses simple, interpretable models to approximate and explain complex \"black box\" algorithms locally. The introduced framework, named LIPEx, specifically enhances this approach for multi-class classifiers by suggesting that valuable insights can be gained not just from the true class but also from examining the features associated with other classes.\n\nFor instance, in sentiment analysis, while understanding the features that contribute to a 'joyful' classification is important, analyzing features that are inversely related to a 'sad' classification can also provide meaningful explanations.\n\nLIPEx achieves this by performing a linear approximation of the classifier's network up to the logits level; before the softmax operation is applied. It constructs a linear surrogate model based on perturbations of the input, which allows for the examination of feature associations for every class of interest through the model's weights. By extending the focus beyond the true class label, LIPEx offers more comprehensive explainability insights than its predecessor, LIME, which only considers the true class.\n\nThe idea is natural and the contribution is rather straightforward. The framework seems useful; however, there is room for improvement; especially regarding the presentation of the paper. I would be willing to increase my score if some of my questions are answered."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The idea is very intuitive and natural and the experimental results are thorough and convincing.\n2) The intuition behind linear approximation being accurate for small perturbations in ReLU networks is very interesting."
                },
                "weaknesses": {
                    "value": "1) Some issues with the figures and formatting: (i) The font sizes for Figures 2, 4, and 6 are not readable. (ii) Figures 10 onward do not have any captions and some of them are out of the page margin.\n2) More convincing image examples are required. One where the improvement of LIPEx over LIME is evident.\n3) LIPEx works only with neural networks that define logits and softmax layers; however, in general, the surrogate model framework aims to approximate the black box model without any knowledge about its inner workings. Therefore, LIPEx is somewhere between being a black box explainability method and an explainability method that knows the inductive biases of the architecture."
                },
                "questions": {
                    "value": "1) Is there any ablation for using the Hellinger distance measure over other discrepancy methods such as KL divergence or total variation? Remember that just because the loss function is less for one does not mean it is better in explanations.\n2) Why is $\\pi(\\mathbf{1}_s, x)$ defined this way? Is this standard? This means that for further away perturbation the Hellinger distance association should be more pronounced. But this goes against the fact that perturbations should be close to the original input.\n3) Can you provide a similar table to Table 3 for the LIME baseline as well? \n4) Why are the models in Algorithm 1 (in Appendix A) only considered to have linear least squares loss with $\\ell_2$-norm?\n5) Why do the TVLoss and HDLoss oscillate so much in Figure 5 (Appendix D.2)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4219/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811924053,
            "cdate": 1698811924053,
            "tmdate": 1699636388936,
            "mdate": 1699636388936,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "noh1N21gBX",
            "forum": "R7946uagL2",
            "replyto": "R7946uagL2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4219/Reviewer_KfmR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4219/Reviewer_KfmR"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes another method, named 'LIPEx', to explain individual predictions of black-box models. Its operations are similar to LIME: First, LIPEx creates a set of perturbed samples and obtains the corresponding predictions from the black-box model. Then, it uses the perturbed data and the predictions to train a surrogate model.\n\nThe proposed surrogate model remaps the black-box model's predictions into a matrix where one of the dimension represents the output classes and the other represents the features. This matrix is defined to be the \"explanation matrix\". The loss function used to learn the surrogate model has been modified in two ways: First, the weighing function has been changed to a custom one. Second, the distance metric has been changed to use the Hellinger distance.\n\nThe paper claims that LIPEx \"provides insight into how every feature deemed to be important affects the prediction probability for each of the possible classes\", is more efficient than LIME, and \"causes more change in predictions for the underlying model than (other models)\"."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The use of an \"explainable matrix\" allows one to quickly visualize the importance of the features on the output classes, which is quite useful. It is also commendable that the paper has performed many test to compare its methods against other explainable A.I. methods and is transparent with the results. While the performance of LIPEx is only marginally better than the next best model, the good performance of LIPEx when a low number of perturbed data points is used can be an useful characteristic."
                },
                "weaknesses": {
                    "value": "The presentation of the paper requires major revisions. This version is poorly organized, full of mistakes and confusing. However, I think that it can be a decent paper if the authors can write well.\n1. The introduction uses many materials from the latter sections and can be shortened significantly.\n2. Abbreviations should be defined at first mention (e.g., 'XAI' in the abstract').\n3. The authors should describe the experiments in detail in the main text instead of explaining it in the captions (e.g., Table 3).\n4. The authors can consider writing simpler and shorter sentences instead of long ones. Please pay attention to the grammar and your usage of commas.\n5. Some equations can be defined in a clearer way and explained better in the text.\n\nThe authors can consider describing the framework laid out by LIME before clearly explaining how LIPEx modifies each component (e.g., weighing function, distance metric). The paper reads as though LIPEx is fundamentally different from LIME but I see many similarities.\n\nThe tests reveal that the performance of LIPEx is not too different from LIME on many datasets. The paper can benefit from a clearer emphasis of the advantages of LIPEx."
                },
                "questions": {
                    "value": "Are you referring to perturbed data points when you use the term 'perturbations'? Please clarify your terminology.\n\nHow does the computational time of deploying LIPEx compare with LIME? Estimating a linear model is is quick, which is probably why LIME is so commonly used despite other proposed improvements.\n\nCan the input data, defined as 'x' in your equations, take on negative values? If not, then aren't you just using a static weight in the loss function?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4219/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4219/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4219/Reviewer_KfmR"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4219/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699003975833,
            "cdate": 1699003975833,
            "tmdate": 1699636388870,
            "mdate": 1699636388870,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "sf6Uu59IXg",
            "forum": "R7946uagL2",
            "replyto": "R7946uagL2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4219/Reviewer_LRFa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4219/Reviewer_LRFa"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces LIPEx, an XAI method which replicates the class-wise prediction probabilities of a complex model, and provides locally linear explanations via linear regression over data perturbations, with respect to the squared Hellinger distance. The authors study the sensitivity, faithfulness and stability of LIPEx, via distortions, perturbations and noise applied to the model and input data. They further compare LIPEx to LIME and existing gradient and path based saliency attribution methods, and present results for BERT on 20NewsGroups & Emotion; for VGG-16 and InceptionNetV3 on Imagenette."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The literature review is thorough and demonstrates a comprehensive understanding of the motivations, principles and existing methods in XAI.   \n2. The authors analyse the faithfulness (to the original model) and trustworthiness (robustness of explanations) via different experiments, centred around perturbing either the input data or the original model.  \n3. XAI for model debugging is an interesting and important task to work on in both image and text modalities."
                },
                "weaknesses": {
                    "value": "1. **Unreliable Results.**  I have serious concerns about the reliability of numerical results in this paper:   \n- Top-K results are inconsistent between Tables 2 and 6 (despite representing the same set of image experiments). To elaborate, Table 2 presents numbers for Top-K, K=2, 3, 4; whereas Table 6 presents the exact same numbers but for K=3, 4, 5. For instance, Table 2 reports LIPEx Top-4 performance for VGG-16 as $0.867$ whereas this becomes $0.82$ in Table 7. Similar shifts are observed for all other rows and columns.  \n- Numbers are presented to arbitrary precision in Tables 1 & 2 (main text) and 6 & 7 (supplement): some values are given accurate to 1 decimal point (d.p.), some to 2 d.p., others to 3 d.p.  \n- Other more minor presentation issues include mislinking of Tables 6 and 7 (LaTeX references are swapped), misspelling of \"Imagenette\" as \"Imagenetee\".\n- The numerical results are wildly inconsistent and should be thoroughly verified before any resubmission.    \n  \n2. **Poor presentation.** The mathematical notation in this submission is unnecessarily convoluted; the manuscript contains both redundant definitions and missing definitions. It is confusing to follow and hard to understand. For instance, it takes one several rereadings to comprehend what LIPEx actually takes as an input; what the term \"feature\" refers to at different points of the paper.    \n  \n3. **Limited evaluation.** LIPEx is evaluated on relatively simple datasets (20NewsGroups, Emotion and Imagenette) and architectures (BERT, VGG-16, InceptionNetV3). It is unclear how such local interpretability results translate to larger complex datasets WikiText-103 or ImageNet-1K, and various recent text/vision models.\n\n4. **Non-standard image comparisons.** In Table 2, \"top features detected by LIPEx\" refer to \"image segments which were obtained from Segment Anything\", as clarified in the caption. LIME and LIPEx can directly leverage off-the-shelf SAM segmentation maps (only needing to \"get a weight for each segment\") whereas saliency-map based methods do not have direct access to these ground-truth segmentations (they need to first compute pixel-wise saliencies, which are then summed for each segmentation \"feature\" mask to constitute the feature \"weight\"). This is not a fair and standard comparison in vision XAI, since LIME and LIPEx only assign weights to precomputed segments whereas other methods are required to perform pixel-wise localisation.  \n  \n4. **Limited novelty.** The LIPEx method tries to replicate the complex model's predictions with respect to the set of chosen features, with a locally linear model. It makes use of the squared Hellinger distance and $l2$ regularization, which are well-known and standard functions in existing literature."
                },
                "questions": {
                    "value": "1. Could the authors kindly explain the inconsistent presentation of numerical results?   \n2. Could the authors address my concerns (as detailed in W3 & 4) regarding various limitations of image and text modality evaluations?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4219/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699117371937,
            "cdate": 1699117371937,
            "tmdate": 1699636388795,
            "mdate": 1699636388795,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]