[
    {
        "title": "DOMINO: A Dual-System for Multi-step Visual Language Reasoning"
    },
    {
        "review": {
            "id": "uu2JAO0ea7",
            "forum": "BWSTBrmRqD",
            "replyto": "BWSTBrmRqD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3926/Reviewer_j7sH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3926/Reviewer_j7sH"
            ],
            "content": {
                "summary": {
                    "value": "The paper focus on visual language reasoning problems which requires extraction of text or numbers from information-dense images like charts or plots. The proposed method includes a dual-system for multi-step multimodal reasoning, which consists of a \u201cSystem-1\u201d step for visual information extraction and a \u201cSystem-2\u201d step for deliberate reasoning. By fine-tuning LLaMA-2 70B on only a small amount of data on multi-step reasoning, the accuracy of the model surpasses the best fully-supervised end-to-end approach by 5.7% and a pipeline approach with FlanPaLM (540B) by 7.5% on ChartQA."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "\u2022 The paper is well written and easy to understand. Figure 1 provides a good overview of the complete system.\n\u2022 The paper presents promising results on ChartQA and outperforms prior supervised baselines.\n\u2022 The paper includes ablation studies in Figure 3."
                },
                "weaknesses": {
                    "value": "\u2022 Novelty: The core idea of the paper is very similar to prior work, including \u201cVisual Programming: Compositional Visual Reasoning Without Training, CVPR 2023\u201d which also uses a large LLM for reasoning and perception modules to extract information from images. Additionally, \u201cSocratic Models: Composing Zero-Shot Multimodal Reasoning with Language, arXiv 2022\u201d also performs zero-shot multi-modal reasoning in a similar fashion. \u201cLook, Remember and Reason: Visual Reasoning with Grounded Rationales, ICML workshop 2023\u201d combines System-1 and System-2 inference in a single model using rationales. \n\n\u2022 It is unclear why the performance on PlotQA much worse compared to ChartQA. The paper mentions that PlotQA \u201cis a synthetic dataset with template based and restricted types of questions\u201d. But this should be easier to solve compared to ChartQA, as the proposed approach also follows templated reasoning steps. The paper should make it clear with ample qualitative examples why performance on PlotQA is lacking. \n\n\u2022 Fairness of the comparison to Few-Shot DePlot versions of GPT-4 and LLaMA: The proposed  DOMINO version of LLaMA has more information about the chart in question. Therefore, it is unclear if the evaluation is fair. \n\n\u2022 Qualitive examples: The paper is lacking qualitive examples from PlotQA in the main paper. The main paper only includes a single qualitative example from ChartQA in the main paper. Examples of failure cases in Table 7 are hard to follow as the associated charts are not available. The paper should include more quantitative examples  which are easier to follow. The format of \u201cGPT-4 Technical Report, arXiv 2023\u201d can serve as a guiding example.\n\n\u2022 Additional datasets: The paper evaluates performance only on two datasets. There are also more challenging datasets available: SciCap (http://scicap.ai/). The SciCap uses real-world data and requires high-level reasoning along with low-level understanding of scientific figures. It would be an ideal testbed to evaluate the performance of the proposed approach."
                },
                "questions": {
                    "value": "\u2022 The paper should discuss prior work such as \u201cVisual Programming: Compositional Visual Reasoning Without Training, CVPR 2023\u201d, \u201cSocratic Models: Composing Zero-Shot Multimodal Reasoning with Language, arXiv 2022\u201d, \u201cLook, Remember and Reason: Visual Reasoning with Grounded Rationales, ICML workshop 2023\u201d in more detail.\n\n\u2022 The should discuss the challenges associated with PlotQA in more detail, ideally with qualitative examples.\n\n\u2022 The fairness of the comparison to Few-Shot DePlot versions of GPT-4 and LLaMA should be discussed in more detail."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3926/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698730246921,
            "cdate": 1698730246921,
            "tmdate": 1699636353095,
            "mdate": 1699636353095,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oEe4OZrkaf",
                "forum": "BWSTBrmRqD",
                "replyto": "uu2JAO0ea7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3926/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3926/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer j7sH"
                    },
                    "comment": {
                        "value": "Thank you for the comments and suggestions. Answers to your questions as well as clarifications to some of your comments below: \n\n> Novelty: The core idea of the paper is very similar to prior work, including \u201cVisual Programming: Compositional Visual Reasoning Without Training, CVPR 2023\u201d, \u201cSocratic Models: Composing Zero-Shot Multimodal Reasoning with Language, arXiv 2022\u201d and \u201cLook, Remember and Reason: Visual Reasoning with Grounded Rationales, ICML workshop 2023\u201d.\n\nDOMINO differs from these works in how the interactions between the language and vision modules are decided. Both \u201cSocratic Models\u201d and \u201cLook, Remember and Reason\u201d pre-define the interactions with templates, which do not generalize to questions with new reasoning processes. \u201cVisual Programming\u201d decides the whole interaction process by generating a program, but the LM does not get to react differently based on the intermediate results returned from the vision module, which limits the question types that could be solved. By contrast, DOMINO learns to compose the atomic operations on the fly based on both the question and the intermediate results from the vision module, which is more flexible.\n\n\n> It is unclear why the performance on PlotQA much worse compared to ChartQA. The paper mentions that PlotQA \u201cis a synthetic dataset with template based and restricted types of questions\u201d. But this should be easier to solve compared to ChartQA, as the proposed approach also follows templated reasoning steps. The paper should make it clear with ample qualitative examples why performance on PlotQA is lacking.\n\nThe charts in PlotQA involve extremely large numbers ranging from 0 to 3:50e+15, which pose a challenge to reasoning for both the vision and language models. The fully-supervised method leverages the sufficiently large training set (with over 20M examples) to learn the data bias, which is also pointed out in the DePlot paper (Liu et al., 2023).\n\n> Fairness of the comparison to Few-Shot DePlot versions of GPT-4 and LLaMA: The proposed DOMINO version of LLaMA has more information about the chart in question. Therefore, it is unclear if the evaluation is fair.\n\nThe few-shot DePlot method includes the entire table associated with the chart, whereas the DOMINO version of LLaMA does not include this information (as illustrated in Table 4) and as such does not necessarily include more information about the chart. We agree that a table is not an authentic representation of the chart, and this is one of the motivations for our work. DOMINO may include other types of information (e.g., color) and can also ask for additional information by interacting with the vision module.\n\n> Qualitative examples: The paper is lacking qualitative examples from PlotQA in the main paper. The main paper only includes a single qualitative example from ChartQA in the main paper. Examples of failure cases in Table 7 are hard to follow as the associated charts are not available. The paper should include more quantitative examples which are easier to follow. The format of \u201cGPT-4 Technical Report, arXiv 2023\u201d can serve as a guiding example.\n\nDue to the page limit, we only include two examples in Table 4 in the main paper and attach the associated charts in Figure 4 in the Appendix. We have included the associated charts for the examples in Table 7 in the updated draft for your reference and we will move one PlotQA example to the main paper.\n\n> Additional datasets: The paper evaluates performance only on two datasets. There are also more challenging datasets available: SciCap (http://scicap.ai/). The SciCap uses real-world data and requires high-level reasoning along with low-level understanding of scientific figures. It would be an ideal testbed to evaluate the performance of the proposed approach.\n\nBesides ChartQA and PlotQA, we also conducted experiments on two out-of-distribution datasets including DVQA and FigureQA as shown in Table 2 in the main paper. We focused on visual language reasoning in this work since it requires the LM to really rely on the visual information to answer a question about a chart and conduct multi-step reasoning while in other VQA tasks, the LM either can rely on language prior (but not necessarily the image) or does not need to conduct multi-step reasoning to answer a question. Thus, SciCap, which is actually a caption generation task, might not be an ideal testbed for our problem. Based on the examples shown in SciCap, some information mentioned in the target caption is not even grounded by the images. But we view SciCap as a potentially good resource for training our vision module to conduct the Describe operation. Thank you for your suggestion."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3926/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259520457,
                "cdate": 1700259520457,
                "tmdate": 1700259520457,
                "mdate": 1700259520457,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wQfk9cp7iP",
                "forum": "BWSTBrmRqD",
                "replyto": "oEe4OZrkaf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3926/Reviewer_j7sH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3926/Reviewer_j7sH"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response. However, my major concerns remain:\n\n1) Novelty: The paper is nice application of prior works especially: \u201cSocratic Models: Composing Zero-Shot Multimodal Reasoning with Language, arXiv 2022\u201d. However, the paper provides limited new insights.\n\n2) Efficiency: Related to the point above, I do not believe that the presented system will gain much traction in the future, due to the multiple calls to different LLMs leading to poor efficiency (also pointed out by Reviewer CNjW). Current SOTA LLM such as GPT-4V already shows promising performance on similar tasks without the need of multiple function calls (https://arxiv.org/pdf/2309.17421.pdf). \n\n3) Results on PlotQA: The current results show that the method is brittle even after fine-tuning.\n\nI will keep my score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3926/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700524311303,
                "cdate": 1700524311303,
                "tmdate": 1700524311303,
                "mdate": 1700524311303,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MLDR1kTD18",
            "forum": "BWSTBrmRqD",
            "replyto": "BWSTBrmRqD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3926/Reviewer_CNjW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3926/Reviewer_CNjW"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces DOMINO, a dual-system designed for charts/plots reasoning. DOMINO consists of two models: The first model, called system-1, uses vision and language to extract specific information from images. The second model, system-2, is a large language model that decomposes tasks and generates answers. Experimental results indicate that DOMINO surpasses traditional pipeline approaches in handling both in- and out-of-distribution data. With limited training samples, DOMINO also achieves SOTA results on ChartQA."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This method is intuitive, and I am happy to see the introduction of dual-system into vision-language reasoning.\n2. The proposed method achieves SOTA results on ChartQA.\n3. Analysis shows that DOMINO is more robust in handling complex charts."
                },
                "weaknesses": {
                    "value": "1. The author didn't discuss about the efficiency. How does the inference efficiency of DOMINO compare to the baseline method?\n2. The template seems relatively limited, more non-chartQA tasks are needed to confirm the potential of this method."
                },
                "questions": {
                    "value": "1. What types of charts are included in ChartQA and PlotQA? I think adding relevant descriptions can help people have a more intuitive understanding of the capabilities of this method.\n2. Does the author consider the dual-system approach to be universally applicable? Can it replace other MLLM methods (such as BLIP2 [1], LLAVA [2]) and become a common solution for solving visual QA problems? For example, besides tasks like chartQA, can DOMINO also generalize to other tasks (such as VQA)?\n\n[1] Li, J., Li, D., Savarese, S., & Hoi, S.C. (2023). BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. ArXiv, abs/2301.12597.\n[2] Liu, H., Li, C., Wu, Q., & Lee, Y.J. (2023). Visual Instruction Tuning. ArXiv, abs/2304.08485."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3926/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3926/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3926/Reviewer_CNjW"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3926/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698738645817,
            "cdate": 1698738645817,
            "tmdate": 1699636353013,
            "mdate": 1699636353013,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CfdbOMVULp",
                "forum": "BWSTBrmRqD",
                "replyto": "MLDR1kTD18",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3926/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3926/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer CNjW"
                    },
                    "comment": {
                        "value": "Thank you for your comments and feedback. Please find below the answers to your questions:\n\n**W1: The author didn't discuss about the efficiency. How does the inference efficiency of DOMINO compare to the baseline method?**\n\nAlthough there may be multiple calls to the vision module in DOMINO,  DOMINO is more efficient compared to the few-shot DePlot model due to two reasons: (1) The vision module in DOMINO only needs to generate the required information based on the image while the vision module in DePlot needs to generate the whole table, which can be arbitrarily long. (2) As a result of (1), DOMINO does not need to take as input the whole table sequence which would consume a large part of the context window in the LM of DePlot.\nThe supervised end-to-end method is more efficient in inference than both DePlot and DOMINO. However, the method generates the answers directly which is prone to error and lacks interpretability for complex questions that require multi-step reasoning. We have included this discussion in Section 6 of the paper.\n\n**W2: The template seems relatively limited, more non-chartQA tasks are needed to confirm the potential of this method.**\n\nWe agree that DOMINO can be applied to other VQA tasks. In this work, we focused on visual language reasoning as it requires accurate information extraction from an image in addition to strong numerical/logical reasoning skills to answer a question based on the extracted information, as also discussed in previous works such as (Liu et al., 2023). As such, we compiled the minimum number of templates that could generally be used across different chart/plot types.\n\n**Q1: What types of charts are included in ChartQA and PlotQA? I think adding relevant descriptions can help people have a more intuitive understanding of the capabilities of this method.**\n\nThe chart types in ChartQA include bar, line and pie charts. PlotQA has bar, line and dot-line charts. Thanks for your suggestion. We have expanded the descriptions of the experimented datasets in Appendix A.1.1 to include this information.\n\n**Q2: Does the author consider the dual-system approach to be universally applicable? Can it replace other MLLM methods (such as BLIP2 [1], LLAVA [2]) and become a common solution for solving visual QA problems? For example, besides tasks like chartQA, can DOMINO also generalize to other tasks (such as VQA)?**\n\nYes, DOMINO can be applied to other VQA tasks since the LM can always use language as the vehicle for reasoning and guide the vision module to obtain the required information. We focused on visual language reasoning in this work since it requires the LM to really rely on the visual information to answer a question about a chart and conduct multi-step reasoning while in other VQA tasks, the LM either can rely on language prior (but not necessarily the image) or does not need to conduct multi-step reasoning to answer a question."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3926/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700258954373,
                "cdate": 1700258954373,
                "tmdate": 1700258954373,
                "mdate": 1700258954373,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jvkVoIq7Pg",
                "forum": "BWSTBrmRqD",
                "replyto": "CfdbOMVULp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3926/Reviewer_CNjW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3926/Reviewer_CNjW"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal"
                    },
                    "comment": {
                        "value": "I do appreciate the authors' time in responding to my comments. But after reading the response, some of my concerns have not been resolved:\n\n* Efficiency: Although the end-to-end baselines are weaker in performance compared to DOMINO, they have smaller parameter sizes and better inference efficiency than DOMINO. So when the parameter size of the end-to-end baselines is expanded to the same scale as DOMIMNO, can DOMINO still outperform these baselines? I hope the author can further improve the inference efficiency of DOMINO and add detailed efficiency comparisons.\n\n* Generalizability: I agree the dual-system approach can be generalized to other tasks, but the experiments in this paper have not yet supported this. I hope the author can expand this method to more multimodal tasks in the future.\n\nThus, I will keep my original score."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3926/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631737542,
                "cdate": 1700631737542,
                "tmdate": 1700631737542,
                "mdate": 1700631737542,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0U9IyG7djF",
            "forum": "BWSTBrmRqD",
            "replyto": "BWSTBrmRqD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3926/Reviewer_AQ48"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3926/Reviewer_AQ48"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a two component system for chart/plot reasoning. The system is composed of a DePlot backbone and a LLaMa-2 model, the former used to extract information from the chart, while the later for decomposing the question and give final answer based on reasoning. After fine-tuning DePlot on instruction level tasks and LLM on a small number of hand written solutions, the system surpasses prompt-based baselines and some supervised methods. The performance gain is attributed to the improvement in both decomposition and answering."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is clearly written.\n2. The results are great, compared to few-shot baselines, and the performance gain is analyzed carefully.\n3. The paper proposed a demonstration of two stage reasoning using LLMs for task decomposition using the feedback from perception results, which is novel compared to similar LLM-guided systems without feedback, e.g., [1]. The efficiency of fine-tuning of LLM also supports the decomposition of System-1/2.\n4. The authors thoroughly discussed the functionality of each component in the reasoning process through ablation studies and analyzed the error made by the models.\n\n[1] https://arxiv.org/abs/2211.11559"
                },
                "weaknesses": {
                    "value": "A few unclear points are raised in Questions."
                },
                "questions": {
                    "value": "1. Why some of the results are not shown in Table 1?\n2. Why is the correct answer for arithmetic in Table 7 is -50752953286.0?\n3. What is the evaluation prompt used when there is no `Describe` step?\n4. It would be great if the authors could discuss the applicability of the proposed method on other VQA tasks, e.g. CLEVR.\n5. Is there any examples of the model failed at decomposing the problem?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3926/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3926/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3926/Reviewer_AQ48"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3926/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698796621973,
            "cdate": 1698796621973,
            "tmdate": 1699636352931,
            "mdate": 1699636352931,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pJ6tOOV3tp",
                "forum": "BWSTBrmRqD",
                "replyto": "0U9IyG7djF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3926/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3926/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer AQ48"
                    },
                    "comment": {
                        "value": "Thank you for your comments and feedback. Please find below the answers to your questions:\n\n**Q1. Why some of the results are not shown in Table 1?**\n\nThe results for the baseline models are from their respective papers which are cited in the table. These works have not provided the results for all the datasets and/or dataset splits. We will clarify this in the paper. For GPT-4, we did not run the model on the much larger PlotQA evaluation sets due to high cost and usage limits as pointed out in the footnote on page 7.\n\n**Q2: Why is the correct answer for arithmetic in Table 7 is -50752953286.0?**\n\nWe apologize for not including the original charts for each example in the paper and will include them in the updated version. The correct answer is from the PlotQA dataset. The question-answer pairs in PlotQA are generated by templates automatically and PlotQA considers the \u201cdifference\u201d between A and B just as $A-B$ instead of the absolute difference. That is why the answer is negative in Table 7.\n\n**Q3: What is the evaluation prompt used when there is no Describe step?**\n\nIn this case, the reasoning process starts directly with task decomposition and not the Describe step. For example, the first demonstration in the prompt would be:\n```\nQ: In which year the private health expenditure per person in Oman is 210.69?\nA: Let\u2019s extract the data of Oman.\nThe data is 183.88 in 2008, 233.80 in 2009, 210.69 in 2010, 195.26 in 2011, 196.32 in 2012,\n154.21 in 2013, 153.22 in 2014.\nThe value 210.69 is in year 2010. So the answer is 2010.\n```\nWe have added the prompt used for this case to Appendix A-2.\n\n**Q4: It would be great if the authors could discuss the applicability of the proposed method on other VQA tasks, e.g. CLEVR.**\n\nOur proposed method can be applied to other VQA tasks given an appropriate vision module. We focused on visual language reasoning to necessitate the multiple interactions between language and vision modules since other VQA tasks either do not require multi-step reasoning or can be accomplished by language prior alone. We will highlight the flexibility of our method in the final draft.\n\n**Q5: Is there any examples of the model failed at decomposing the problem?**\n\nThere are examples of incorrect decomposition due to ambiguity in the question. In some cases, the model fails to request the only information required for answering although the model still answers correctly finally:\n```\nQ: How many people use daily?\nA: Let's describe the figure.\nThe figure shows the data of: Value (blue). The x-axis shows: Weekly | Daily | Stopped using.\nLet's extract the data of Value.\nThe data is 41.2 in Monthly, 37.8 in Weekly, 18.2 in Daily, 2.8 in Stopped using.\nAmong [41.2, 37.8, 18.2, 2.8], the value that is 18.2 is the number of people who use daily. So the answer is 18.2.\n```\nIn other cases, the incorrect decomposition leads to invalid queries for the vision module. The vision module then gives the wrong intermediate result which finally leads to an incorrect answer.\n```\nQ: What's the percentage of people who don't believe it can impact us and don't know much about covid?\nA: Let's describe the figure.\nThe figure shows the data of: Value (dark blue). The x-axis shows: Staying alert and to the ground | Don't believe it can impact us or | Don't know much about.\nLet's extract the data of Value.\nThe data is 1.0 in Staying alert and the 2019.\nThe percentage of people who don't believe it can impact us and don't know much about covid is 1.0. So the answer is 1.0.\n```\nWe will extend our error analysis in the revised draft to include such examples."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3926/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700258642986,
                "cdate": 1700258642986,
                "tmdate": 1700258642986,
                "mdate": 1700258642986,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mfsSlhiShh",
                "forum": "BWSTBrmRqD",
                "replyto": "pJ6tOOV3tp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3926/Reviewer_AQ48"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3926/Reviewer_AQ48"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response. Though the other two reviewers raised concerns about efficiency, I do believe that retrieval is a critical component when the scale of the data is large and hope the authors could provide some examples. I will keep my score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3926/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528223636,
                "cdate": 1700528223636,
                "tmdate": 1700528223636,
                "mdate": 1700528223636,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]