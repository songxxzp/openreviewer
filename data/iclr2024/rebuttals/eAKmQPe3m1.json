[
    {
        "title": "PixArt-$\\alpha$: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis"
    },
    {
        "review": {
            "id": "UgS2GjIVHo",
            "forum": "eAKmQPe3m1",
            "replyto": "eAKmQPe3m1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1660/Reviewer_7QAx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1660/Reviewer_7QAx"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a text-to-image model that achieves good quality results while using much smaller networks and datasets than current SOTA models. The key to the improvement seems to be some combination of (1) transformer-based diffusion (2) multi-stage training (5 stages for a single model) (3) careful dataset curation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Being able to train a high-quality generative model with an ~order of magnitude reduction in resources is a very important contribution, as the cost of training large models limits access to large, well-resourced organizations.\n\n- The results are good, and the evaluations are sufficient. The FID score is a bit high compared to newer models, but I agree with the authors that this is (at best) an imprecise metric for quality.\n\n- The paper is overall well written and easy to read."
                },
                "weaknesses": {
                    "value": "I have found some inconsistencies in reported results:\n\n- The GPU days reported in A.5 contradict the main table; Summing the last column and using their 2.2 scale factor for converting from v100 to a100 days yields 753 A100 days vs. the 675 reported in the main table. It's a relatively small difference (11%) but concerning that the reported numbers don't align.\n\n- Some of the baseline numbers are inconsistent / misleading. For example, the authors report that Imagen consumed 15B images. The Imagen authors report a dataset size of 800M. It seems that 15B, at least roughly, corresponds to the total number of non-unique images seen during training, e.g. (batch size) x (training steps). If we applied this metric to PixArt-alpha (using the table in A.5), we would get a number of 31B - 2x the Imagen results. I did not dig into the other baseline numbers, but these should be checked carefully because they are central to the paper's contributions.\n\n- There are 3 primary improvements (dataset curation, multi-stage training, and unet->transformers), however there are no ablations quantifying how much of an effect each one has on model performance. While the results and final model are good, a lack of ablations limits the research contribution of the paper. \n\n- The authors do not discuss the training of the VAE; since this is an important part of \"learning the pixel distribution of natural images\", the training time and data quantity of the VAE should be included in the paper."
                },
                "questions": {
                    "value": "- My intuition says that training cost and CO2 emissions should be directly proportional to GPU hours, however these proportions are inconsistent. For example, Fig 2b shows a 14x increase in cost of Imagen over PixArt, while the ratio of reported GPU hours is 10. For GigaGAN, Fig2b shows a 9.4x increase, where as Table 2 shows a 7x increase in GPU hours."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1660/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1660/Reviewer_7QAx",
                        "ICLR.cc/2024/Conference/Submission1660/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1660/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698083852014,
            "cdate": 1698083852014,
            "tmdate": 1700716013794,
            "mdate": 1700716013794,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "voioqZGDN1",
                "forum": "eAKmQPe3m1",
                "replyto": "UgS2GjIVHo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1660/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1660/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7QAx (1/3)"
                    },
                    "comment": {
                        "value": "Dear Reviewer 7QAx,\n\nWe sincerely appreciate your meticulousness and patience in helping us identify the statistical errors in our paper. With your generous help, we believe we can make the article more complete. We will address all issues in the revision, and we hope to receive your approval and forgiveness. We also provide additional clarifications to your comments below.\n\n---\n\n### Weakness\n\n**W1: Inconsistency of GPU Days and Data Usage**\n\nWe sincerely apologize for the statistical errors in our paper due to the large amount of data information when we wrote this paper. We have fixed all these errors, including the misinterpretation of data referenced from GigaGAN [1] in the revised version. The updates involve GPU days and the training data required by other methods like Imagen [2]. The updated data shows that Pixart uses 5% data volume compared to SDv1.5 and costs less than 2% training time compared to RAPHAEL. Compared to RAPHAEL, our training costs are only 1%, saving approximately \\\\$3,000,000 (PixArt-\u03b1\u2019s  \\\\$28,400 v.s. RAPHAEL\u2019s \\\\$3,080,000). All the conclusion **consistently demonstrates the efficiency of PixArt**.\n\nMoreover, Nvidia's tech report [8] illustrates the acceleration ratio between A100 and V100 spec in the first Table below, where the V100->A100 speedup for the Transformer BERT is **5x** v.s. U-Net **2.2x**. We have conservatively estimated the lowest speedup for PixArt in the original paper (we adopted the U-Net speedup ratio). If we consider BERT speedup since PixArt also uses pure Transformer architecture, same as BERT, the training time of PixArt would significantly reduce to **1656 V100 GPU days / 5** = **332 A100 GPU days**.\n\n|         | ResNet-50 v1.5 | U-Net Medical | BERT    |\n| ------- |:--------------:|:-------------:|:-------:|\n| A100    | 4.1 hours      | 3 minutes     | 5 Days  |\n| V100    | 9.7 hours      | 7 minutes     | 24 Days |\n| Speedup | 2.4x           | **2.2x**      | **5x**  |\n\n| Method   | #Images |     GPU days      |\n| -------- |:-------:|:-----------------:|\n| DALL\u00b7E   |  250M   |         -         |\n| GLIDE    |  250M   |         -         |\n| LDM      |  400M   |         -         |\n| DALL\u00b7E 2 |  650M   |    41,667 A100    |\n| SDv1.5   |  5000M  |    6,250 A100     |\n| GigaGAN  |  2700M  |    4,783 A100     |\n| Imagen   |  860M   |    7,132 A100     |\n| RAPHAEL  | 5000M+  |    60,000 A100    |\n| PIXART-\u03b1 |   25M   |  753 A100 (2.2x)  |\n| PIXART-\u03b1 |   25M   | **332 A100** (5x) |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1660/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700124219034,
                "cdate": 1700124219034,
                "tmdate": 1700124494629,
                "mdate": 1700124494629,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gFw7sDCqZT",
                "forum": "eAKmQPe3m1",
                "replyto": "UgS2GjIVHo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1660/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1660/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7QAx (2/3)"
                    },
                    "comment": {
                        "value": "**W2: Design of Data, Multi-Stage Training, and UNet->Transformer**\n\nThanks for this valuable suggestion. Systematically ablation studies of Text-to-Image (T2I) models from model architecture, data curation, and training are computationally expensive and would demand extensive resources, which exceed our current capabilities. However, we still try our best to give some insights and analysis of the above aspects:  \n\n\n1. Data Design: \n* Regarding the selection and re-annotation of LLaVA data, we discussed this in Section 2.4 via the total number of concepts. Additionally, we found that over 90% of the nouns in the LAION concept statistics appear less than 10 times throughout the dataset. Consequently, we believe that the large number of infrequently occurring nouns may be the main reason for prolonging the training time. This is where we attribute the primary cause for rapid training; merely replacing UNet with Transformer wouldn't be enough to accelerate training speed by an order of magnitude.\n* We are continuing to explore the lower limit for the data required in stage 3. \nIn stage 3 training, we found that increasing the data volume only marginally improves the model's capabilities, whereas decreasing the data volume has a progressively larger impact. \nWe increased the data scale from 14M to 30M but did not observe significant improvements. We also decreased data to 1.4M, 700K, and 140K. The model's performance was relatively stable at 1.4M and 700K, but there was noticeable quality degradation at 140K. Due to time constraints, our current conclusion is that training with 1.4M data points in stage 3 achieves results comparable to 14M. However, volumes below 700K may lead to overfitting, as shown in the below table. The properties of the data should match the VN/DN analysis mentioned in Section 2.4. \n\n| Data Volumn | Iterations | Image size |  FID  | CLIP score |\n|:----------- |:----------:|:----------:|:-----:|:----------:|\n| 30M         |    120K    |    512     | 8.62  |   0.276    |\n| 14M         |    100K    |    512     | 8.61  |   0.275    |\n| 14M         |    15K     |    256     | 16.61 |   0.268    |\n| 1.4M        |    15K     |    256     | 16.96 |   0.266    |\n| 700K        |    15K     |    256     | 17.14 |   0.265    |\n| 140K        |    15K     |    256     | 21.58 |   0.255    |\n\n2. Multi-stage Training: \n* We tried to skip the first stage and directly train the text-to-image task on SAM. However, due to the significant difference in scale between image and text in cross-attention inputs, and the lack of a corresponding relationship, this usually led to training crashes often represented by NAN. Therefore, we believe that pixel alignment in the first stage is key. \n* The decoupling of stages 2 and 3 is partial because the pseudo-label quality of SAM cannot be guaranteed by current open-source VLMs, leading to illusions, bias, and other issues. Therefore, we only use this data for high-concept-density text-image alignment learning, corresponding to stage 2 pre-training. Fine-tuning with high-aesthetics data can improve the model's aesthetic quality. We have conducted experiments to skip stage 2, and directly train stage 3 based on ImageNet pretrain. Under the same training iterations, we observed that both the visualization results and the text prompt instruction following ability were noticeably inferior compared to using stage 2's SAM pretraining.\n3. UNet->Transformer:\n* In the original DiT paper, they already thoroughly compared Transformer and UNet architectures on class-condition image generation, validating transformers are easier to scale up and have higher performance upper bound. This is the core reason we chose DiT as the base diffusion model and extended it to the text-to-image model.\n* Moreover, we chose Transformer architecture because it also has several future potentials, including MaskDiT [3], and Window/Sparse/Linear Attention [4~6]. We leave these improvements in the future work.\n\n**W3: VAE Training and Its Data Usage**\n\nOur attempt to train a VAE resulted in an approximate training duration of 25 hours, utilizing 64 V100 GPUs on the OpenImage dataset. Training VAE seems does not consume too much time. We treat the pre-trained VAE as a ready-made component of a model zoo, the same as the pre-trained CLIP/T5-XXL text model, and our total training process did not include the training of VAE and we are also not sure whether other methods e.g. SDXL considered the VAE training time. To ensure a fair comparison, we have temporarily excluded the VAE training time and data quantity. We clarify this point in Appendix A.5 in the revised version of our paper."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1660/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700124325444,
                "cdate": 1700124325444,
                "tmdate": 1700124470249,
                "mdate": 1700124470249,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LmvNqM8wDT",
                "forum": "eAKmQPe3m1",
                "replyto": "UgS2GjIVHo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1660/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1660/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7QAx"
                    },
                    "comment": {
                        "value": "Dear Reviewer 7QAx,\n\nThank you for your detailed review and the valuable feedback. We have carefully addressed each of your concerns and provided clarifications and experiment in our previous response. We would like to kindly request your response to the provided explanations and revisions.\n\nWe appreciate your thorough evaluation of our work and look forward to hearing from you and addressing any further questions or concerns you may have.\n\nThank you for your continued engagement and support."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1660/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536163241,
                "cdate": 1700536163241,
                "tmdate": 1700536163241,
                "mdate": 1700536163241,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UfRXDLHSZ3",
                "forum": "eAKmQPe3m1",
                "replyto": "LmvNqM8wDT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1660/Reviewer_7QAx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1660/Reviewer_7QAx"
                ],
                "content": {
                    "comment": {
                        "value": "The authors have satisfied many of my concerns. I've raised my score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1660/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716153721,
                "cdate": 1700716153721,
                "tmdate": 1700716153721,
                "mdate": 1700716153721,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UACSwsOaZp",
            "forum": "eAKmQPe3m1",
            "replyto": "eAKmQPe3m1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1660/Reviewer_Bg3n"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1660/Reviewer_Bg3n"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new text-to-image synthesis model, called PixArt-$\\alpha$. The proposed approach introduces several improvements to conventional models on the side of the training process and architecture. In particular, the paper proposes a training strategy decomposition, in which 1) the learning of dependencies between pixels; 2) text-image alignment; 3) high-resolution synthesis, are all modelled in different subsequent phases. On the architecture side, the paper builds on top of Diffusion Transformer pre-trained on class-conditional ImageNet, proposing several tricks to adopt it to text-to-image synthesis with minimal overhead. In addition, the authors re-visit the training datasets used to train text-to-image models. By applying LLaVA to label existing datasets, much more dense labelling of images in terms of nouns/image is achieved.\n\nIn effect, the proposed approach achieves state-of-the-art quality of synthesis but significantly reduces the training time and the needed amount of training data. The method is shown to be on par or better than models like Imagen, SDXL, while using only 10% of the training time and 10% of the training samples."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Overall, this is a very actual paper with very strong experimental result, big significance and potential impact.\n\n- $\\underline{\\text{Contribution}}$. The proposed method achieves state-of-the-art results with resources that are an order of magnitude lower than of current mainstream methods. Given that biggest T2I are very expensive to train, this is a big step forward for the community to democratize T2I models for broader audiences, to decrease costs and the carbon footprint required for training. \n- $\\underline{\\text{Insights}}$. The paper introduces valuable insights for the community, particularly on the side of dataset design. The paper demonstrates that the design of previous datasets like LAION suffers from deficient descriptions and infrequent diverse vocabulary usage, which led previous models for many additional epochs needed until convergence. The provided VN/DN analysis can influence the next generation of image-text datasets.\n- $\\underline{\\text{Results}}$. The paper demonstrates strong experimental results. In particular, visual results are impressive, beautiful, and have good alignment to conditioning text. The model is shown to outperform big models like DALLE-2 or SDXL by human perception. Overall, the quality of results clearly matches the bar or ICLR.\n- $\\underline{\\text{Applications}}$. It is demonstrated that the proposed method supports applications that are generally expected from big T2I models, like personalization (DreamBooth, ControlNet etc).\n- $\\underline{\\text{Presentation}}$. The paper is very well written, the ideas are very easy to follow. Explanations are high-level and clearly deliver lessons for the community. More technical discussions are presented in supplementary."
                },
                "weaknesses": {
                    "value": "I do not see major issues that would preclude publication, but I would still ask a couple of questions:\n\n- How expensive was the dataset collection? I expect running a big image-text model like LLaVA on millions of images required a lot of time, GPU ressources, and memory. Should this non-trivial step be in some way included to the analysis of costs and co2 emissions?\n- Although using a model like LLava clearly improves VN/DN, it probably introduces new biases to the distribution of text prompts? Does this affect the scope of text prompts for which PixArt-$\\alpha$ works well or poorly?"
                },
                "questions": {
                    "value": "What are the plans of the authors regarding open-sourcing? \nIs it expected to be released 1) complete training code; 2) collected dataset; 3) \"internal\" dataset?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1660/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698673043493,
            "cdate": 1698673043493,
            "tmdate": 1699636094029,
            "mdate": 1699636094029,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oPSN92KHEH",
                "forum": "eAKmQPe3m1",
                "replyto": "UACSwsOaZp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1660/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1660/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Bg3n"
                    },
                    "comment": {
                        "value": "Dear Reviewer Bg3n,\n\nThank you very much for your appreciation and high praise of our article. We will take your suggestions into account to make the article even better. We address your comments below.\n\n---\n\n### Questions\n\n**Q1: Cost and Resource Requirements for LLaVA auto-captioning.**\n\nWe use LLAVA-7B to generate captions. LLaVA's annotation time on the SAM dataset is approximately 24 hours with 64 V100; it is negligible compared to training time. Following your suggestion, we clarify this point in Appendix A.5 in the revised version of our paper.\n\n**Q2: Potential Bias Introduced by LLaVA**\n\nLLaVA may introduce certain illusions and biases, but it does not harm the learning of concepts and text-image alignment. Therefore, we only use LLaVA-caption for text-image alignment pre-training. In stage 3, we finetune our internal data with GT text prompts (users given) to quickly adapt to user preference. Our main contribution is a proof of concept that using VLM models like LLaVA for captioning is reasonable and practical. As VLM rapidly evolves, some new VLMs, e.g. GPT-4V, can produce better auto-labeling captions and significantly contribute to T2I pre-training.\n\n**Q3: Open Source Planning**\n\nWe have released training/inference code, checkpoints, and demos, which have had a certain impact on the AIGC community."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1660/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700123874593,
                "cdate": 1700123874593,
                "tmdate": 1700124556125,
                "mdate": 1700124556125,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "40mjt8mD1t",
                "forum": "eAKmQPe3m1",
                "replyto": "oPSN92KHEH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1660/Reviewer_Bg3n"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1660/Reviewer_Bg3n"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for getting back"
                    },
                    "comment": {
                        "value": "I thank the authors for their answers. I remain positive in my assessment and have no further questions to the authors."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1660/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700562110564,
                "cdate": 1700562110564,
                "tmdate": 1700562110564,
                "mdate": 1700562110564,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TWcgNokh9n",
            "forum": "eAKmQPe3m1",
            "replyto": "eAKmQPe3m1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1660/Reviewer_GFA3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1660/Reviewer_GFA3"
            ],
            "content": {
                "summary": {
                    "value": "PIXART-\u03b1 is a novel Text-to-Image (T2I) model capable of generating photorealistic images from text descriptions while maintaining low training costs. The model's approach involves decomposing the T2I task into three distinct stages, which include pixel dependency learning, text-image alignment learning, and high-resolution and aesthetic image generation. This is achieved through the modification of the diffusion transformer architecture, incorporating cross-attention layers, simplifying adaptive normalization layers, and utilizing re-parameterization techniques for an efficient T2I Transformer. Additionally, the model leverages high-informative data from a vision-language model (LLaVA) for generating quality image captions, drawing from the SAM dataset and fine-tuning using JourneyDB and an internal dataset."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The PIXART-\u03b1 model is particularly cost-effective to train, making it an attractive option for researchers and organizations with limited computational resources, as it minimizes the financial and hardware requirements associated with training a state-of-the-art T2I model.\n\nIt employs a straightforward and simplified approach to achieve its text-to-image synthesis, ensuring that the model's architecture and training process are accessible and comprehensible to a broader range of researchers and practitioners.\n\nNotably, PIXART-\u03b1 boasts fewer tunable parameters, which not only contributes to its cost-effectiveness but also makes it more manageable and less prone to overfitting or complex hyperparameter tuning, streamlining the implementation and optimization process."
                },
                "weaknesses": {
                    "value": "Given its smaller dataset, there are concerns about the generalizability and compositional capabilities of the model for a broader range of concepts.\n\nA more extensive test for out-of-domain generalizability would add valuable insights into the model's adaptability and effectiveness in diverse scenarios."
                },
                "questions": {
                    "value": "What led to the decision of utilizing ImageNet data as the pretraining source instead of a pretrained VQGAN, and how did this choice impact the model's performance and capabilities?\n\nThe selection of the Diffusion Transformer as the basis for the model's architecture over the more conventional approach of the Latent Diffusion Model (Unet) is mentioned in Appendix A10, but can a more comprehensive analysis, including ablation studies, be provided to thoroughly compare the strengths and weaknesses of these architectural choices?\n\nThe model's performance raises the important question of determining the minimum number of images required for training while still ensuring generalizability and maintaining the compositional properties of Text-to-Image models. Can the authors shed some light on what is the minimum set of images required to train such a model from scratch. What should be the properties of such a dataset?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1660/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698797501033,
            "cdate": 1698797501033,
            "tmdate": 1699636093917,
            "mdate": 1699636093917,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4SzXFuKaSt",
                "forum": "eAKmQPe3m1",
                "replyto": "TWcgNokh9n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1660/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1660/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GFA3 (1/2)"
                    },
                    "comment": {
                        "value": "Dear Reviewer GFA3,\n\nThank you for appreciating our approach. We address your comments below.\n\n---\n\n### Weakness\n\n**W1: Concerns About OoD Generalization with Smaller Dataset.**\n\nWe believe there are no generalizability issues in PixArt. As detailed in Section 2.4, although our total training samples are fewer than LAION-5B, each sample contains much more information, leading to better overall information volume and density. Specifically, as shown in **Table 1**, the total number of valid distinct nouns (VN) in SAM and Internal data is 152K + 23K = **175K**, which is comparable with LAION's **210K**. Moreover, we empirically found that for some OoD prompts, such as surreal image generation, like the prompt in Figure 1 where *\"a cactus with a happy face\"*, SDXL fails to produce satisfactory images while PixArt's generated image is good. \n\nIn summary, we can conclude that our dataset scale is big enough, and PixArt does not suffer from generalizability issues. \n\n---\n### Questions\n\n**Q1: Using ImageNet-Pretrained DiT Over Pretrained VQGAN**\n\nWe apologize for the confusion regarding the use of ImageNet. We explain our choice in two aspects:\n\n1. Image encoder/decoder choice: We chose VAE over VQGAN [1], following current open-source popular methods like SDXL [2], SD [3], and DiT [4]. The choice between VAE and VQGAN is not the primary focus of our paper, and using VQGAN as an alternative is also feasible.\n2. Genrative model choice: We chose the Diffusion Transformer instead of the Auto-regressive Transformer in VQGAN, mainly because Denoising Diffusion Probabilistic Models (DDPMs) are the most mainstream and mature data modeling method for image generation, currently most of the advanced image generative models are based on DDPMs. Meanwhile, auto-regressive Transformer for image generation is also promising since some recent papers, e.g. CM3Leon (meta), also adopted this solution.\n\n**Q2: Comprehensive Ablation of UNet vs Transformer Architectures**\n\nIn the original DiT paper, they already thoroughly compared Transformer and UNet architectures on class-condition image generation, validating transformers are easier to scale up and have higher performance upper bound. This is the initial reason we chose DiT as the base diffusion model.\n\nHowever, systematically assessing the impact of different model architectures in the Text-to-Image (T2I) domain requires considering several factors, such as the model size, data volume, and computational requirements. Therefore, exploring the effectiveness of various architectures in the T2I field would demand extensive resources, which exceed our current capabilities. Our primary motivation for this project is to accelerate training T2I model under limited resources. The core contributions of our paper are how to use data more efficiently and how to smartly decouple the training process to speed up training T2I models ultimately. \n\n\n**Q3: Insights about Minimum Scale and Properties of the Dataset**\n\nThat's an excellent question. We are continuing to explore the lower limit for the data required in stage 3. Below is our observation. \n\nIn stage 3 training, we found that increasing the data volume only marginally improves the model's capabilities, whereas decreasing the data volume has a progressively larger impact.\n\nSpecifically, we increased the data scale from 14M to 30M but did not observe significant improvements. We also decreased data to 1.4M, 700K, and 140K. The model's performance was relatively stable at 1.4M and 700K, but there was noticeable quality degradation at 140K. Due to time constraints, our current conclusion is that training with 1.4M data points in stage 3 achieves results comparable to 14M. However, volumes below 700K may lead to overfitting, as shown in the below table. The properties of the data should match the VN/DN analysis mentioned in Section 2.4. \n\n| Data Volumn | Iterations | Image size | FID   | CLIP score |\n|:-----------:|:----------:|:----------:|:-----:|:----------:|\n| 30M         | 120K       | 512        | 8.62  | 0.276      |\n| 14M         | 100K       | 512        | 8.61  | 0.275      |\n| 14M         | 15K        | 256        | 16.61 | 0.268      |\n| 1.4M        | 15K        | 256        | 16.96 | 0.266      |\n| 700K        | 15K        | 256        | 17.14 | 0.265      |\n| 140K        | 15K        | 256        | 21.58 | 0.255      |\n\n---"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1660/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700123281553,
                "cdate": 1700123281553,
                "tmdate": 1700124600167,
                "mdate": 1700124600167,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nXVuBzg3je",
                "forum": "eAKmQPe3m1",
                "replyto": "TWcgNokh9n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1660/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1660/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GFA3"
                    },
                    "comment": {
                        "value": "Dear Reviewer GFA3,\n\nThank you for your detailed review and the valuable feedback. We have carefully addressed each of your concerns and provided clarifications and experiment in our previous response. We would like to kindly request your response to the provided explanations and revisions.\n\nWe appreciate your thorough evaluation of our work and look forward to hearing from you and addressing any further questions or concerns you may have.\n\nThank you for your continued engagement and support."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1660/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536253494,
                "cdate": 1700536253494,
                "tmdate": 1700536253494,
                "mdate": 1700536253494,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UT55rKdFW2",
            "forum": "eAKmQPe3m1",
            "replyto": "eAKmQPe3m1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1660/Reviewer_VQLf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1660/Reviewer_VQLf"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces several recipes to accelerate the training of text-to-image foundation models. These include\n- Use pretrained DiT\n- Use Flan-T5 XXL\n- Use LLaVA captions.\n- AdaLN / AdaLN single architectures to reduce model size.\nOverall, these methods allow training of a reasonable quality model in 10% of the resources used than Stable Diffusion, which makes it more possible to democratize the training recipes in text-to-image foundation models."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Originality: the empirical evaluation of synthetic captions in text to image generation is not systematically studied until DALL-E 3, and the new AdaLN architecture might be useful.\nClarify: the paper is quite clear about most details about the training, which makes reproducibly much more likely.\nSignificance: the paper mostly explores valid heuristics for training text-to-image foundation models quickly, some conclusions can be helpful in the community: 1) DiT architecture instead of UNet, 2) the use of synthetic captions, 3) the use of SAM dataset."
                },
                "weaknesses": {
                    "value": "The paper mostly is a combination of multiple ideas that exist in the literature, so \"novelty\" in the traditional sense is somewhat limited."
                },
                "questions": {
                    "value": "1. The SAM dataset blurs human faces in their training, won't this cause problem in generation cases where generating a face (not closed up) is needed?\n2. How does the model generate images with more extreme aspect ratios?\n3. The DiT architecture has a fixed patch size. As resolution becomes higher, so will the number of tokens be higher. Will this cause a bottleneck in training and inference (such as 1k resolution)?\n4. DALL-E 3 technical report mentions the pitfall of \"overfitting\" to automated captions, is this the case in PixArt model? If not, how is this mitigated?\n5. Since the dataset size is smaller, does it have trouble producing named entities, such as celebrities? \n6. How critical is training DiT on ImageNet needed? While being able to start with an existing model is good it also limits the possibilities to explore different architectures. \n7. The CLIP score of the CLIP-FID curve of Pixart seems worse than SD 1.5. Is there any reason for that?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1660/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819513880,
            "cdate": 1698819513880,
            "tmdate": 1699636093821,
            "mdate": 1699636093821,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KosPMo2rhk",
                "forum": "eAKmQPe3m1",
                "replyto": "UT55rKdFW2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1660/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1660/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VQLf (1/2)"
                    },
                    "comment": {
                        "value": "Dear Review VQLf,\n\nThank you for appreciating our approach. We will address your comments below.\n\n---\n\n### Weakness\n\n**W1: Combination of Multiple Ideas.**\n\nWe understand the 'traditional novelty' mentioned in the review. We will discuss more about the novelty here. \n1. **Data**. Our paper's primary contribution to the community lies in the auto-labeling approach we have developed, a topic that has yet to see much discussion thus far. Coupled with the analysis of concept density, as detailed in Section 2.4, we provide a fresh perspective on assessing the suitability of a dataset for diffusion training.\n2. **Training process**. Decoupling the entire training process into multiple stages is another innovative finding in our work. This approach significantly accelerates the training process, unlike previous methods [1,2], which decoupled the training process into different resolutions. While these earlier methods enhance the model's forward pass speed, we contribute to faster learning.\n3. **Model structure**. we design an adaptation technique to validate the feasibility of transfer from class-condition to Text-to-Image (T2I) field and make it outperform existing methods.\n\nIn summary, our paper presents novel findings, insights, and designs concerning data, the training process, and model structure. We believe our approach could inspire the community in future research endeavors.\n\n---\n\n### Questions\n**Q1: Concerns about Blurry Faces in SAM Dataset.**\n\nThe blurry faces in the SAM dataset indeed pose challenges in pretraining models to generate clear faces. However, we primarily utilize SAM for Text-Image Alignment learning, where high image quality is not crucial. In later high-quality fine-tuning phases, we have ample data with clear facial images, allowing for rapid rectification of such issues.\n\n**Q2: How to Generate Images with Extreme Aspect Ratios?**\n\nDrawing inspiration from SDXL [1], we pre-define various aspect ratios ranging from 1:4 to 4:1 and train batches of images with the same aspect ratio. We also add aspect ratios (AR) as one additional condition to the model, allowing the model to be aware of different ARs. During inference, users can specify a specific AR and a corresponding noise map with the same AR will be initiated and input into the model. Technically, PixArt can easily generate images with arbitrary aspect ratios. However, most of the training data's aspect ratio ranges from 1:4 to 4:1, so we cannot guarantee the quality of the generated images with an extreme aspect ratio.\n\n\n**Q3: Efficiency of Training/Inference When Image Resolution Becomes Higher.**\n\nIncreasing image sizes in transformers will negatively impact training/inference efficiency. Therefore, in our work, most training (stages 1,2, and part of stage 3) was conducted at 256px size and only finetuning several steps at 512px/1024px size. Training on 32G V100 GPUs, we do not observe significant bottleneck within 1K resolution. However, larger resolutions, such as 2K, might pose challenges due to memory constraints. Several potential solutions can be used to improve the Transformer's efficiency, including MaskDiT [3], and Window/Sparse/Linear Attention [4~6]. We leave it for future work."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1660/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700122506146,
                "cdate": 1700122506146,
                "tmdate": 1700124636302,
                "mdate": 1700124636302,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Fluwfg2JKC",
                "forum": "eAKmQPe3m1",
                "replyto": "UT55rKdFW2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1660/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1660/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VQLf (2/2)"
                    },
                    "comment": {
                        "value": "**Q4: How PixArt Mitigate the Oitfall of \"Overfitting\" to Automated Captions Compared with DALL-E 3?**\n\nDALL-E 3 and PixArt follow different training processes. DALL-E 3 uses a mixture of ground truth (GT) and pseudo captions during training. In contrast, PixArt initially uses  LLaVA-generated pseudo captions for pretraining (stage 2), then finetuning with GT text-image pairs (stage 3). Considering the captioning quality of LLaVA and similar VLM models are still under enhancement, the automated captions are mainly used for image-text alignment learning. Moreover, in stage 3, the model also learns text-image alignment; the only difference is that the captions of stage 3 are provided by real users (Section 2.4). After stage 3 finetuning, the model effectively aligns with users' preferences/habits, enabling PixArt to avoid overfitting from pseudo captions.\n\n**Q5: Generation Named Entities (e.g. celebrities), Given Smaller Dataset Size**\n\nPixArt has a certain ability to generate images of well-known people such as Donald Trump and Elon Musk, but it can't guarantee accuracy for individuals not present in the dataset. We test and observe that even when the Stable Diffusion is trained on a 5B volume dataset, its ability to generate celebrity images remains limited. To ease this problem, we can add related datasets for training. e.g. CelebA [7], which encompasses an array of celebrity faces.\n\n**Q6: Benefits of Pretraining DiT on ImageNet and Restriction on Model Exploration**\n\nPretraining on ImageNet does not limit model exploration. Instead, it enhances training stability. \n1. Directly training a random-initialized DiT on the SAM dataset led to frequent NaN issues because the models need to learn image denoising and text-image alignment simultaneously. \n2. We have validated the feasibility and efficiency of extending the class-conditioned model to the text-conditioned model; this part is orthogonal to the network architecture.\n\nSo it is conceptually simple to explore different architectures to pretrain on ImageNet and finetune on the SAM dataset without restrictions.\n\n**Q7: Reasons for Lower CLIP Scores Compared to SD1.5**\n\nPixArt and SD1.5 exhibit comparable CLIP scores. \n1. Considering both FID and CLIP scores, the comparison between PixArt and SD1.5 is evenly matched, with PixArt demonstrating a higher CLIP score at the lowest FID point, as shown in Fig. 20.\n2. The datasets used by SD and PixArt differ: the SD utilizes the LAION-5B dataset and filters the images with CLIP scores < 0.28 [8]. Training with the data already having high CLIP scores might present an advantage. \n3. We tested the ground truth image-text pair CLIP score on MSCOCO-3W, which is only **0.257**, and we found that both SD1.5 and PixArt exceed this score. Therefore, we believe this metric can not effectively reflect the true performance of the advanced T2I models.\n\n---\n\n### Reference\n[1] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M \u0308 uller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. In arXiv, 2023.\n\n[2] DeepFloyd. Deepfloyd, 2023. URL https://www.deepfloyd.ai/.\n\n[3] Zheng H, Nie W, Vahdat A, et al. Fast Training of Diffusion Models with Masked Transformers[J]. arXiv preprint arXiv:2306.09305, 2023.\n\n[4] Beltagy I, Peters M E, Cohan A. Longformer: The long-document transformer[J]. arXiv preprint arXiv:2004.05150, 2020.\n\n[5] Child R, Gray S, Radford A, et al. Generating long sequences with sparse transformers[J]. arXiv preprint arXiv:1904.10509, 2019.\n\n[6] Choromanski K, Likhosherstov V, Dohan D, et al. Rethinking attention with performers[J]. arXiv preprint arXiv:2009.14794, 2020.\n\n[7] Liu Z, Luo P, Wang X, et al. Large-scale celebfaces attributes (celeba) dataset[J]. Retrieved August, 2018, 15(2018): 11.\n\n[8] Schuhmann C, Beaumont R, Vencu R, et al. Laion-5b: An open large-scale dataset for training next generation image-text models[J]. Advances in Neural Information Processing Systems, 2022, 35: 25278-25294."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1660/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700122562052,
                "cdate": 1700122562052,
                "tmdate": 1700123501060,
                "mdate": 1700123501060,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]