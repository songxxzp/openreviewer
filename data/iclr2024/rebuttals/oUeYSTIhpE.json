[
    {
        "title": "DisCo-DSO: Coupling Discrete and Continuous Optimization for Efficient Generative Design in Hybrid Spaces"
    },
    {
        "review": {
            "id": "3oQXdDEImD",
            "forum": "oUeYSTIhpE",
            "replyto": "oUeYSTIhpE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5807/Reviewer_gpUA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5807/Reviewer_gpUA"
            ],
            "content": {
                "summary": {
                    "value": "DisCo-DSO is a novel approach for optimizing in hybrid discrete-continuous spaces. It uses a generative model to jointly optimize discrete and continuous variables, leading to improved performance and efficiency, especially in complex optimization tasks like interpretable reinforcement learning with decision trees."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I believe that research focusing on effectively exploring solutions while considering the impact of discrete and continuous variables on the objective function has its significance. This study proposes research that optimizes both discrete and continuous variables simultaneously, extending the conventional method of constructing solutions used in AI-based combinatorial optimization research to a continuous approach."
                },
                "weaknesses": {
                    "value": "I believe there are various approaches to solving optimization problems that involve a mix of continuous and discrete variables. Instead of merely extending the existing modeling structure, I don't consider optimizing both discrete and continuous variables simultaneously as a significant contribution in itself. Research that effectively explores solutions while considering the impact of discrete and continuous variables on the objective function holds its own merit. This study, by proposing optimization of both discrete and continuous variables simultaneously and expanding the traditional approach used in AI-based combinatorial optimization research to a continuous one, may need to offer more than just an extension of the existing modeling structure to make a substantial contribution."
                },
                "questions": {
                    "value": "1. I'm having difficulty understanding why an autoregressive policy structure is necessary for generating solutions to optimization problems involving a mix of continuous and discrete variables. While sequential structures are commonly used in reinforcement learning for combinatorial optimization problems to effectively learn policies for arbitrary problems, this study appears to be focusing on optimizing a specific given problem. In such a case, wouldn't it be more efficient to explore the entire solution space rather than constructing solutions sequentially?\n\n2. How is the order for selecting optimization variables determined, and does this order have the potential to affect the performance?\n\n3. Mixed Integer Programming (MIP) is a well-known class of optimization problems that involve optimizing both discrete and continuous variables, and many studies attempt to optimize MIPs using deep learning. How does this research differ from those studies involving MIPs?\n\n4. The experimental content seems limited. While three types of problems are presented, the first problem only compares the number of function evaluations, and the second problem presents curves indicating the improvement in solutions for a specific problem. It would be beneficial to solve a more diverse set of problems and provide statistical evidence to validate the performance."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5807/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5807/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5807/Reviewer_gpUA"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5807/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698551439998,
            "cdate": 1698551439998,
            "tmdate": 1700720435828,
            "mdate": 1700720435828,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fEp1nnKiY9",
                "forum": "oUeYSTIhpE",
                "replyto": "3oQXdDEImD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5807/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5807/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors to Reviewer gpUA"
                    },
                    "comment": {
                        "value": "We thank the reviewer for her/his comments and suggestions. We will address the reviewer's comments in the following.\n\n**1. \"Why is an autoregressive policy structure is necessary? ... this study appears to be focusing on optimizing a specific given problem. In such a case, wouldn't it be more efficient to explore the entire solution space rather than constructing solutions sequentially?\"**\nWe would like to emphasize that DisCo-DSO is explicitly tailored for generative optimization scenarios where the solution is constructed sequentially. \nIn this context, the sequential construction of the solution arises due to the following factors: \n1. The length of the sequence is variable.\n2. The order of the sequence is not predetermined but is determined by the model.\n3. There exist prefix-dependent positional constraints (or priors) dictated by the specific problem at hand. For instance, in symbolic regression, the model should not be allowed to add a trigonometric operator if it has already done so in the past to avoid nested trigonometric operators.\n\nThe reviewer is right in that we are solving specific problem instances, but even then, the above factors are present. An autoregressive policy is well-suited to address these factors. We will clarify this in the camera-ready version of the paper.\n\n**2. \"How is the order for selecting optimization variables determined, and does this order have the potential to affect the performance?\"**\nIn this work, we are doing *de novo* design: we start from an empty set and let the model decide which and how many tokens\n(or building blocks) to add to the solution. The length of the solution is variable and the order in which the tokens are added is determined by the model. The ordering is a decision variable of the optimization problem (we can not fix it a priori). The reviewer is right that, in the tasks of symbolic regression and decision tree search, the order in which the tokens are added will affect the final design and thus the performance. The model should learn to add the tokens in the order that maximizes the reward. We will clarify this in the camera-ready version of the paper.\n\n**3. \"How does this research differ from those studies involving MIPs?\"**\nThe main differences between DisCo-DSO and approaches that use deep learning to solve MIP problems are the following:\n1. The search space in DisCo-DSO is less structured than in MIP problems. The solutions in DisCo-DSO are variable length and the order in which the tokens are added is determined by the model. In MIP problems, the solutions are fixed length and the order is fixed.\n2. In DisCo-DSO we have positional constraints that depend on the prefix of the solution. For instance, in symbolic regression, the model should not be allowed to add a trigonometric operator if it has already done so in the past to avoid nested trigonometric operators. In general MIP problems, the constraints are fixed and do not depend on the prefix of the solution.\n3. In DisCo-DSO, discrete and continuous variables are linked. For instance, in decision tree search, we have the discrete variable $x_i < \\beta$ and the continuous variable $\\beta =5.3$, that jointly form the decision tree node $x_i < 5.3$. The search mechanism is designed to optimize both variables jointly. It is unclear if the approaches that use deep learning to solve MIP problems can handle this type of coupling. We will clarify this in the camera-ready version of the paper.\n\nWe will add a discussion about the differences between DisCo-DSO and approaches that use deep learning to solve MIP problems in the camera-ready version of the paper.\n\n**4. \"It would be beneficial to solve a more diverse set of problems and provide statistical evidence to validate the performance\".**\nIn this paper, we wanted to investigate the benefits of DisCo-DSO over the standard decoupled approach in generative optimization scenarios where the solution is constructed sequentially. \nFor generality, we wanted to cover deterministic and stochastic optimization problems and also, different search spaces and semantics. We chose \nsymbolic regression (deterministic optimization over the space of mathematical expressions) and decision tree policies in RL (stochastic optimization over the space of decision trees) as representative examples of these scenarios.\nThe implementation of each of these tasks in non-trivial and required a significant amount of work. We believe that the results are conclusive and statistically significant, and show that DisCo-DSO outperforms the standard decoupled approach in terms of generalization over the test data and quality of solution per number of function evaluations."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699920393355,
                "cdate": 1699920393355,
                "tmdate": 1699920760420,
                "mdate": 1699920760420,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EgUE4hZKNK",
                "forum": "oUeYSTIhpE",
                "replyto": "fEp1nnKiY9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5807/Reviewer_gpUA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5807/Reviewer_gpUA"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response."
                    },
                    "comment": {
                        "value": "Thank you for providing clarification through the rebuttal; I have raised my score to 5. However, I still believe that the novelty and scalability of this research are lacking."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720422836,
                "cdate": 1700720422836,
                "tmdate": 1700720422836,
                "mdate": 1700720422836,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7iuZbiY6PD",
            "forum": "oUeYSTIhpE",
            "replyto": "oUeYSTIhpE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5807/Reviewer_6T4T"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5807/Reviewer_6T4T"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a method for using reinforcement learning to train a model that outputs a sequence of both discrete and corresponding continuous values. The goal here is to tackle problems that involve predicting an object that has both discrete and continuous values such as a decision tree with selection of branching feature as well as the corresponding threshold, or producing a symbolic regression expression which is a combination of selected functions and constants. Their approach jointly produces both discrete symbol and a corresponding continuous value as opposed to previous work which focused on generating discrete backbones and then found the continuous values for the fixed skeleton. They evaluate their approach empirically by comparing against baselines that handle discrete and continuous decisions independently, as well as baselines from the literature for symbolic regression and interpretable RL (identifying a decision tree for solving simple RL problems). They demonstrate improved performance over previous approaches both in terms of solution quality, as well as in the efficiency as they require fewer calls to the evaluation metric to train their approach since the continuous variables don\u2019t need to be optimized separately."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The main strength of the proposed approach is the empirical performance seems to be much better than previous approaches for solving optimization over hybrid discrete and continuous spaces. The results seem to indicate substantial improvement from the given model, and the evaluation is done on both symbolic regression and interpretable RL which are quite diverse domains. Furthermore, the approach itself may have broader impact for other settings in hybrid design spaces, and in identifying interpretable machine learning models."
                },
                "weaknesses": {
                    "value": "Some of the small weaknesses here are in the motivation of the approach as well as tackling problems which require solutions that are more heavily constrained. \nThe approach is partially motivated by the idea that jointly generating the discrete and continuous objects makes the reward more aligned with the solution itself rather than approaches which generate the discrete backbone and then optimize the continuous variables after the fact. However, it seems that this approach also may have misleading rewards for the skeleton or continuous solution if neither are optimal. It may shy away from high quality discrete solutions even though there may be one setting of continuous variables which are highly performant.\nIt is also unclear how this approach may work on more complicated discrete-continuous settings where the feasible region may be more complex such as in solving mixed integer linear programming, or in other cases where the continuous space may be more decoupled from the discrete space such as in cases where there are many continuous decisions that need to be made but not all of them have a corresponding discrete decision.\nAnother small limitation is that it would be helpful to see the applicability of this approach on more tasks that have hybrid domains such as real world use cases of symbolic regression."
                },
                "questions": {
                    "value": "How does this method avoid the issue of poor or misleading rewards for the right discrete skeleton? It might be the case that the continuous predictions are incorrect while the skeleton is correct.\n\nIs it possible to generate continuous decisions that are unrelated to discrete decisions and thus uncoupled?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5807/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698784838747,
            "cdate": 1698784838747,
            "tmdate": 1699636611889,
            "mdate": 1699636611889,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uiBf2UNBz1",
                "forum": "oUeYSTIhpE",
                "replyto": "7iuZbiY6PD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5807/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5807/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors to Reviewer 6T4T"
                    },
                    "comment": {
                        "value": "We highly appreciate the reviewer's comments and suggestions. Her or his comments show that the reviewer has read the paper carefully and understood the main contributions of the paper. We will address the reviewer's comments in the following.\n\n**\"However, it seems that this approach also may have misleading rewards for the skeleton or continuous solution if neither are optimal. It may shy away from high quality discrete solutions even though there may be one setting of continuous variables which are highly performant\"**\nOur results in symbolic regression and decision tree search do not suggest that the model is shying away from high quality discrete solutions. On the contrary, the results show that a coupled approach outperforms a decoupled approach in terms of generalization over the test data and quality of solution per number of function evaluations.\n\nThe reviewer is right that there could be a situation where DisCo-DSO produce the optimal discrete skeleton and fail to find the optimal continuous solution (this could also happen in a decoupled approach). However, the opposite is often true in decoupled approaches: we find a poor discrete skeleton and we make it work by overfitting the continuous parameters to the data. This can have the effect of confusing the model and lead to a suboptimal solution. As we show in the experiments in the paper, this effect is catastrophic in terms of generalization over the test data (parsimony of the produced solutions) and wasted function evaluations.\n\n**\"It is also unclear how this approach may work on more complicated discrete-continuous settings where the feasible region may be more complex such as in solving mixed integer linear programming\"**\nWe would like to emphasize that DisCo-DSO is explicitly tailored for generative modeling scenarios where :\n1. The length of the solution is variable.\n2. The order of the solution is not predetermined but is determined by the model.\n3. There exist prefix-dependent positional constraints (or priors) dictated by the specific problem at hand.\nStandard mixed integer linear programming problems problems do not satisfy these conditions.\n\nThat said, we believe that DisCo-DSO can be extended to mixed integer linear programming problems using the $\\text{const}(\\beta)$ operator (similar to symbolic regression). We could then use rejection sampling to sample from the feasible region. Although interesting, we believe that this is beyond the scope of this paper. We will clarify this in the camera-ready version of the paper.\n\n**\"...or in other cases where the continuous space may be more decoupled from the discrete space such as in cases where there are many continuous decisions that need to be made but not all of them have a corresponding discrete decision\".**\nDisCo-DSO would support this case by using the $\\text{const}(\\beta)$ operator (similar to symbolic regression). Note that $\\beta$ does not need to be related to a discrete decision. The relation is only established by the ${\\tt eval}$ operator. For instance, consider the fully continuous problem of finding the maximum of a function $f(\\boldsymbol{x})$ with $\\boldsymbol{x} \\in \\mathbb{R}^n$. In this case, we will have \n$\\mathcal{L}= \\hat{\\mathcal{L}}=\\{\\text{const}(\\beta)\\}$. DisCo-DSO will produce skeletons $\\langle \\text{const}(\\beta_1), \\dots, \\text{const}(\\beta_n) \\rangle$ and the $\\beta_i$'s will be stochastically optimized by standard expectation\u2013maximization. We will clarify this in the camera-ready version of the paper.\n\n**Application to real world use cases of symbolic regression.**\nThe objective of the experiments in the symbolic regression task was to investigate the sample efficiency of DisCo-DSO compared to traditional decoupled approaches. We believe that those conclusions will be valid regardless of the origin of the data.\n\n**\"How does this method avoid the issue of poor or misleading rewards for the right discrete skeleton?\"**\nOur empirical evaluation in symbolic regression and decision tree search suggests that DisCo-DSO does not suffer from this issue, but rather outperforms decoupled approaches in terms of generalization over the test data and quality of solution per number of function evaluations. See above for more details.\n\n**\"Is it possible to generate continuous decisions that are unrelated to discrete decisions and thus uncoupled?\"**\nWe addressed this question above. The short answer is yes. We will clarify this in the camera-ready version of the paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699920577878,
                "cdate": 1699920577878,
                "tmdate": 1699981994426,
                "mdate": 1699981994426,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "A3mP4KpXVQ",
                "forum": "oUeYSTIhpE",
                "replyto": "uiBf2UNBz1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5807/Reviewer_6T4T"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5807/Reviewer_6T4T"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for providing responses to my original comments. Additionally, the authors have provided more results demonstrating their performance improvements against baselines in their experimental settings. \n\nHowever, given that the main support for the claims in this paper is empirical, it would be good to give stronger empirical evidence that this method works in some real world setting. One way of doing this might be to provide results for symbolic regression in some symbolic regression dataset such as those proposed in [1], or the SRBench competition [2] that was used in the referenced previous work the authors build off of [3]. Given the demonstrated improvement in interpretable RL domains, it seems that this approach has potential to be generally applicable and I wouldn\u2019t be against having it accepted. However, I believe that more could be done to strengthen the claims of empirical performance over previous approaches for the task of symbolic regression.\n\n\n[1] La Cava, William, et al. \"Contemporary Symbolic Regression Methods and their Relative Performance.\" Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1). 2021.\n\n[2] https://cavalab.org/srbench/competition-2022/#real-world-track-rankings\n\n[3] Brenden K. Petersen, Mikel Landajuela, T. Nathan Mundhenk, Cl\u00b4audio Prata Santiago, Sookyung Kim, and Joanne Taery Kim. Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients. In 9th International Conference on Learning Rep- resentations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700610129847,
                "cdate": 1700610129847,
                "tmdate": 1700610129847,
                "mdate": 1700610129847,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ingIl6GYoh",
            "forum": "oUeYSTIhpE",
            "replyto": "oUeYSTIhpE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5807/Reviewer_CEut"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5807/Reviewer_CEut"
            ],
            "content": {
                "summary": {
                    "value": "As opposed to de-coupling the discrete and continuous representations, the authors concatenate the two of these are use autoregressive methods for optimization. The approach is applied to a toy problem in symbolic regression and reinforcement learning for decision trees."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The combination of discrete and continuous representations is an important direction for research, with the neuro-symbolic community making a lot of progress and several application domains of interest, including symbolic regression, combinatorial optimization, and symbolic distillation. The pedagogical example is an intuitive way to demonstrate the utility of the approach."
                },
                "weaknesses": {
                    "value": "The concatenation of discrete and continuous variables into a single vector and the higher-level approach are very straightforward and their novelty seems limited."
                },
                "questions": {
                    "value": "Why are you comparing your method against the baselines you define as opposed to using baselines from the literature in the case of symbolic regression (Figure 3)? (For decision trees, baselines from the literature are used). Given recent work on neuro-symbolic regression, the authors should compare against methods in the literature."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5807/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698813207896,
            "cdate": 1698813207896,
            "tmdate": 1699636611793,
            "mdate": 1699636611793,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5rBpZnL4ec",
                "forum": "oUeYSTIhpE",
                "replyto": "ingIl6GYoh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5807/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5807/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors to Reviewer CEut"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the comments and suggestions. We will address the reviewer's comments in the following.\n\n**\"The concatenation of discrete and continuous variables into a single vector and the higher-level approach are very straightforward and their novelty seems limited.\"**\nWe agree with the observation that the concatenation of discrete and continuous variables into a single vector, along with the higher-level approach, may seem straightforward. However, we would like to highlight that the novelty of our approach lies in its application to the specific problem of optimization with generative models for hybrid spaces. \nThe integration involves several challenges that we address in the paper, such as the need for prefix-dependent positional constraints, and the extension of the gradient update to handle the new search space. To the best of our knowledge, this is the first instance where such a concept has been systematically employed in this context. We believe this application of the idea adds a significant contribution to the field and opens up new avenues for research.\n\n**\"Why are you comparing your method against the baselines you define as opposed to using baselines from the literature in the case of symbolic regression (Figure 3)?\"**\nThe paper already contains well-established baselines for symbolic regression. In particular:\n\n1. \"Decoupled-GP-BFGS\": This baseline corresponds to a standard implementation of Genetic Programming for symbolic regression \u00e0 la Koza, 1994. This represents the current most common approach to symbolic regression in the literature.\n2. \"Decoupled-RL-BFGS\": This baseline corresponds exactly to the method \"Deep Symbolic Regression\" from Petersen et al., 2021. We use the same codebase and the same hyperparameters. We will clarify this in the camera-ready version of the paper.\n\nWe want to emphasize that the main contribution of the paper is not the application of DisCo-DSO to symbolic regression, but rather the investigation of the coupled discrete-continuous optimization approach in generative optimization scenarios where the solution is constructed sequentially. Nevertheless, following the reviewer's suggestion, we have computed results for two recent deep learning approaches to symbolic regression: Kamienny et al., 2022 and Biggio et al., 2021. These are the results that we obtained for a similar number of function evaluations (note that their code does not natively report the number of function evaluations) for Kamienny et al., 2022:\n\n| Model | Mean R test | Std R test |\n| --- | --- | --- |\n| DisCo-DSO | 0.7045 | 0.3007 |\n| Decoupled-RL-BFGS | 0.6400 | 0.3684 |\n| Decoupled-RL-evo | 0.0969 | 0.2223 |\n| Decoupled-RL-anneal | 0.1436 | 0.3015 |\n| Decoupled-GP-BFGS | 0.4953 | 0.4344 |\n| Decoupled-GP-evo | 0.0747 | 0.1763 |\n| Decoupled-GP-anneal | 0.1364 | 0.2608 |\n| Kamienny et al., 2022 | 0.5699 | 0.1065 |\n\nand for Biggio et al., 2021 (note that this system only supports $\\leq 3$ dimensions):\n\n| Model | Mean R test | Std R test |\n| --- | --- | --- |\n| DisCo-DSO | 0.6632 | 0.3194 |\n| Decoupled-RL-BFGS | 0.6020 | 0.4169 |\n| Decoupled-RL-evo | 0.0324 | 0.1095 |\n| Decoupled-RL-anneal | 0.1173 | 0.2745 |\n| Decoupled-GP-BFGS | 0.5372 | 0.4386 |\n| Decoupled-GP-evo | 0.0988 | 0.1975 |\n| Decoupled-GP-anneal | 0.1615 | 0.2765 |\n| Biggio et al., 2021 | 0.6858 | 0.1995 |\n\n\nWe can see that the coupled approach of DisCo-DSO still outperforms or gives comparable results to these baselines. We will add these results together with a discussion about the differences between DisCo-DSO and these baselines in the camera-ready version of the paper.\n\n\nReferences:\n\n- John R Koza. Genetic programming as a means for programming computers by natural selection. Statistics and computing, 4:87\u2013112, 1994.\\\\\n- Brenden K. Petersen et al. Deep symbolic regression: Recovering mathematical expressions from data via risk-seeking policy gradients.  ICLR 2021, Virtual Event, Austria, May 3-7, 2021.\\\\\n- Pierre-Alexandre Kamienny, St\u00b4ephane d\u2019Ascoli, Guillaume Lample, and Franc\u00b8ois Charton. End-to- end symbolic regression with transformers. arXiv preprint arXiv:2204.10532, 2022.\\\\\n- Luca Biggio, Tommaso Bendinelli, Alexander Neitz, Aurelien Lucchi, and Giambattista Parascan- dolo. Neural symbolic regression that scales. In International Conference on Machine Learning, pp. 936\u2013945. PMLR, 2021."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700275849976,
                "cdate": 1700275849976,
                "tmdate": 1700275849976,
                "mdate": 1700275849976,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fo0nbJTNRf",
            "forum": "oUeYSTIhpE",
            "replyto": "oUeYSTIhpE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5807/Reviewer_AbyM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5807/Reviewer_AbyM"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes DisCO-DSO, a generative modeling approach to learn a joint distribution over continuous and discrete variables. Prior works follow a decoupled approach, where the discrete and continuous variables are modeled separately, leading to inefficiency in the sampling and optimization procedure. DisCO-DSO uses an autoregressive model and produces two latent variables, one is used to generate the discrete distribution, and the other is used to generate the continuous distribution. This method requires one evaluation step per sample, whereas prior approaches use black-box optimization methods to sample the continuous variable for each discrete token. Experiments are performed on a newly proposed parameterized bitstring task, symbolic regression for equations, and learning decision tree policies for RL. The experiments demonstrate competitive performance with existing methods while improving efficiency."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This work proposes a fairly simple approach to model hybrid spaces, which improves the efficiency compared to existing work. The idea of modeling the discrete and continuous distributions using different latent vectors in this context is novel, to the best of my understanding. The parameterized bitstring task is simple yet effective for benchmarking the performance of hybrid space generative models. In my opinion, this work has a moderate impact on a specific sub-area of generative modeling. \n\nThe presentation and quality of writing are mostly clear. The paper provides relevant context and then describes the proposed method along with model diagrams to illustrate the difference to prior work clearly."
                },
                "weaknesses": {
                    "value": "The main weaknesses of the paper are poor baselines in the experiments and some organizational changes for clarity. The experiments consider baselines that decouple the discrete and continuous space optimization, but many of the recent works mentioned in the related works are not considered as baselines. Without this comparison, it is difficult to ascertain the empirical performance of DisCO-DSO. Another issue is that while the writing is clear, there are some minor organizational changes that can improve the readability of the paper.  See the questions below for more details."
                },
                "questions": {
                    "value": "**********************Comparison with prior work:********************** The related works section describes prior work in the area with different approaches to the problem of modeling joint discrete-continuous spaces, such as Petersen et al., 2021;  Kamienny et al., 2022; Sahoo et al., 2018 and specifically for symbolic regression such as Biggio et al., 2021; Landajuela et al., 2021. The comparison with Petersen et al., 2021 is especially relevant since DisCO-DSO uses the same risk-seeking policy gradient approach to optimize the reward-based objective. Without comparison with relevant prior work, it is difficult to accurately gauge the significance of the empirical contribution.\n\n****************************************************************Choice of autoregressive model:**************************************************************** DisCO-DSO uses LSTMs for autoregressive sequence generation. It would be interesting to see the effect on performance if the backbone model was changed, possible options include GRUs and Transformers.\n\n********************************************************Organization and structure:******************************************************** The readability of the paper can be improved by using paragraph titles to better organize large bodies of text, particularly Section 2, Section 3.2, Section 4.2 and Section 4.3. \n\n**********************References:**********************\n\n- Brenden K. Petersen, Mikel Landajuela, T. Nathan Mundhenk, Cl\u00b4audio Prata Santiago, Sookyung\nKim, and Joanne Taery Kim. Deep symbolic regression: Recovering mathematical expressions\nfrom data via risk-seeking policy gradients. In 9th International Conference on Learning Rep-\nresentations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. [OpenReview.net](http://openreview.net/), 2021a. URL\nhttps://openreview.net/forum?id=m5Qsh0kBQG.\n- Pierre-Alexandre Kamienny, St\u00b4ephane d\u2019Ascoli, Guillaume Lample, and Franc\u00b8ois Charton. End-to-\nend symbolic regression with transformers. arXiv preprint arXiv:2204.10532, 2022.\n- Subham Sahoo, Christoph Lampert, and Georg Martius. Learning equations for extrapolation and\ncontrol. In International Conference on Machine Learning, pp. 4442\u20134450. PMLR, 2018. URL\nhttp://proceedings.mlr.press/v80/sahoo18a.html.\n- Luca Biggio, Tommaso Bendinelli, Alexander Neitz, Aurelien Lucchi, and Giambattista Parascan-\ndolo. Neural symbolic regression that scales. In International Conference on Machine Learning,\npp. 936\u2013945. PMLR, 2021.\n- Mikel Landajuela, Brenden K Petersen, Sookyung Kim, Claudio P Santiago, Ruben Glatt, Nathan\nMundhenk, Jacob F Pettit, and Daniel Faissol. Discovering symbolic policies with deep rein-\nforcement learning. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International\nConference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp.\n5979\u20135989. PMLR, 18\u201324 Jul 2021c. URL https://proceedings.mlr.press/v139/\nlandajuela21a.html."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5807/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699008535623,
            "cdate": 1699008535623,
            "tmdate": 1699636611679,
            "mdate": 1699636611679,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7CdyStjvZ4",
                "forum": "oUeYSTIhpE",
                "replyto": "fo0nbJTNRf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5807/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5807/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors to Reviewer AbyM"
                    },
                    "comment": {
                        "value": "We thank the reviewer for her/his comments. The \"summary\" and \"strengths\" sections that the reviewer compiled show that the reviewer has read the paper carefully and understood the main contributions of the paper. We will address the reviewer's comments in the following.\n\n**\"In my opinion, this work has a moderate impact on a specific sub-area of generative modeling\".**\nWe believe that the field of optimization with generative models is a very important area of research with many real-world applications, whose importance is only going to increase in the future. We believe that our method can have a significant impact in this field by providing a rather simple and effective way to optimize generative models in hybrid spaces.\n\n**Comparison with prior work**.\nWe want to clarify that the baseline Petersen et al., 2021 is present in each experiment under the alias \"Decoupled-RL-BFGS\". Our work builds upon the openly available codebase of Petersen et al., 2021, accessible at https://github.com/dso-org/deep-symbolic-optimization, establishing a direct one-to-one correspondence between Petersen et al., 2021, and \"Decoupled-RL-BFGS\". We regret any lack of clarity in our initial presentation and commit to explicitly addressing this correspondence in the forthcoming camera-ready version.\n\nThe reviewer cites baseline studies by Kamienny et al., 2022; Sahoo et al., 2018; Biggio et al., 2021; and Landajuela et al., 2021. Please note that the method in Landajuela et al., 2021 corresponds to Petersen et al., 2021 (but applied to the symbolic policy discovery for RL). Thus, this method is already present in our paper under the alias \"Decoupled-RL-BFGS\".\n\nWe compile results on our validation benchmark for Kamienny et al., 2022 using the codebase provided by the authors. These are the results that we obtained for a similar number of function evaluations (note that their code does not natively report the number of function evaluations, so we had to estimate it):\n\n| Model | Mean R test | Std R test |\n| --- | --- | --- |\n| DisCo-DSO | 0.7045 | 0.3007 |\n| Decoupled-RL-BFGS | 0.6400 | 0.3684 |\n| Decoupled-RL-evo | 0.0969 | 0.2223 |\n| Decoupled-RL-anneal | 0.1436 | 0.3015 |\n| Decoupled-GP-BFGS | 0.4953 | 0.4344 |\n| Decoupled-GP-evo | 0.0747 | 0.1763 |\n| Decoupled-GP-anneal | 0.1364 | 0.2608 |\n| Kamienny et al., 2022 | 0.5699 | 0.1065 |\n\nFollowing the reviewer's suggestion, we also consider Biggio et al., 2021 as a baseline. However, we were not able to directly apply their method to our validation benchmark, as their system only supports $\\leq 3$ dimensions. We then just report the subsets of the validation benchmark that are supported by their system. These are the results that we obtained for a similar number of function evaluations (note that their code does not natively report the number of function evaluations and we had to estimate it):\n\n| Model | Mean R test | Std R test |\n| --- | --- | --- |\n| DisCo-DSO | 0.6632 | 0.3194 |\n| Decoupled-RL-BFGS | 0.6020 | 0.4169 |\n| Decoupled-RL-evo | 0.0324 | 0.1095 |\n| Decoupled-RL-anneal | 0.1173 | 0.2745 |\n| Decoupled-GP-BFGS | 0.5372 | 0.4386 |\n| Decoupled-GP-evo | 0.0988 | 0.1975 |\n| Decoupled-GP-anneal | 0.1615 | 0.2765 |\n| Biggio et al., 2021 | 0.6858 | 0.1995 |\n\n\nWe can see that the coupled approach of DisCo-DSO still outperforms or gives comparable results to these baselines (note that the results of Biggio et al., 2021 are only comparable for $\\leq 3$ dimensions). We will add these results together with a discussion about the differences between DisCo-DSO and these baselines in the camera-ready version of the paper.\n\n**Choice of autoregressive model**.\nThe reviewer raises an interesting point. To address it, we have repeated the experiments in Symbolic Regression (Task 4.2) using different autoregressive models of different sizes. Specifically, we consider a GRU and a LSTM recurrent cell with 16, 32, and 64 hidden units. The results are shown in the following table:\n\n| Model | Mean R test | Std R test |\n| --- | --- | --- |\n| DisCo-DSO-GRU16 | 0.7377 | 0.3161 |\n| DisCo-DSO-GRU32 | 0.7092 | 0.3442 |\n| DisCo-DSO-GRU64 | 0.7236 | 0.3261 |\n| DisCo-DSO-LSTM16 | 0.7385 | 0.3177 |\n| DisCo-DSO-LSTM32 | 0.7241 | 0.3134 |\n| DisCo-DSO-LSTM64 | 0.7302 | 0.3182 |\n\nWe observe that the results show little difference between the different models architectures and sizes. This trend is consistent with results in the RL literature, where often the choice size of the policy network has little effect on the performance of the algorithm, as long as the network is large enough to capture the complexity of the problem. We will add this discussion in the camera-ready version of the paper.\n\n**Organization and structure**.\nWe will add paragraph titles to the sections mentioned by the reviewer in the camera-ready version of the paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700275770924,
                "cdate": 1700275770924,
                "tmdate": 1700275770924,
                "mdate": 1700275770924,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]