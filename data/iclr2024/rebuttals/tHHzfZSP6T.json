[
    {
        "title": "How Capable Can a Transformer Become? A Study on Synthetic, Interpretable Tasks"
    },
    {
        "review": {
            "id": "XxIGk7Ozpo",
            "forum": "tHHzfZSP6T",
            "replyto": "tHHzfZSP6T",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8467/Reviewer_xno8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8467/Reviewer_xno8"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates the capabilities of transformers to learn individual functions and compose them in a sequential manner. A major contribution of the study is the introduction of a synthetic dataset that is both interpretable and straightforward to implement.\nWhile the paper is mostly well-written, it falls short in clearly describing the experimental setup. \n\n\n\nOverall, the paper is commendable for its innovative synthetic dataset, which provides valuable insights into the capabilities of transformers for function composition. However, the paper would benefit from greater clarity in its experimental descriptions and more robust support for some of its conclusions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The dataset introduced is both simple and valuable for future research.\n- The paper presents perceptive experimental analyses, notably the discussion on the differences between in-order and out-of-order generalizations."
                },
                "weaknesses": {
                    "value": "- The experimental setup lacks sufficient clarity, making it challenging to understand the specific procedures.\n- The paper's conclusion regarding the significance of later attention layers is not adequately substantiated by the experimental data; it lacks insight."
                },
                "questions": {
                    "value": "- Section 3.2 lacks a clear definition and differentiation between permutations and bijections. It should be explicitly stated that the results in Section 4.1 pertain only to bijections.\n- Figure 3 needs more explanation about the notations. For instance, what $f_{i-j}$ means?\n- The explanation between \"random\" and \"21 base\" in Section 4.1 is unclear. \n- The bottom-left subplot in Figure 7 exhibits an anomalous trend compared to other subplots; could you please explain this? In particular, there is no clean trend that transformer first learn composition of more functions. \n- The experimental setup for Figure 6(b) in Section 4.3 is not elucidated. What is the rationale for having 25 bijections and 25 permutations?\n - The paper claims that adding permutations aids direct composition; this is a counterintuitive finding that warrants further elaboration.\n- Figure 6(b) requires more detailed explanation, and several typos need rectification.\n\n\nTypos:\n - In page 2, remove one of the \"considered\" and the quotation masks are not correct. \n - in Figure 11: \"direction composition\" should be corrected to \"direct composition.\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8467/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698142873489,
            "cdate": 1698142873489,
            "tmdate": 1699637057099,
            "mdate": 1699637057099,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bU6kD9GVtj",
                "forum": "tHHzfZSP6T",
                "replyto": "XxIGk7Ozpo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8467/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8467/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to Reviewer xno8 (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their positive feedback. We are glad that the reviewer finds our synthetic setup to be interpretable and useful for future research, the insights on Transformer capabilities to be valuable, and our experimental analyses to be perceptive. We have addressed the questions and suggestions below. We hope the reviewer champions the acceptance of this paper.\n\n**>>> The paper's conclusion regarding the significance of later attention layers is not adequately substantiated by the experimental data; it lacks insight.**\n\nWe have added two new experiments to further substantiate this claim. \n\nIn the first experiment, we visualize the attention maps of a Transformer on the step-by-step prompt. In Figure 7(right), we observe that the attention map selects two tokens, 1 task token (which picks the function) and 1 input token which is the intermediate result of a composition. Hence, attention seems to play an important role in selecting the function and the input that the function should be applied to.\n\nIn the second experiment, presented in Appendix B.4 (figure 14), we study the attention layers of transformers of different depths (1 to 60 layers). Across most Transformers, we find that compositions occur in the later layers of a Transformer. We also observe that a sharp upward inflection in accuracy in all the plots occurs after an MLP layer. The attention layers seem to select the input and the function to apply and the computation seems to occur in the MLP. \n\nDespite the strong evidence, these claims are still speculative and we have made sure to convey the same in the manuscript.\n\n**>>> The bottom-left subplot in Figure 7 exhibits an anomalous trend compared to other subplots; could you please explain this? In particular, there is no clean trend that transformer first learn composition of more functions.**\n\nEach line corresponds to the accuracy averaged over composition of a fixed number of functions. The purple line corresponds to the average accuracy, where we evaluate all compositions of 5 functions, while the orange line for compositions of 1 function (the other 4 are identity). Note that compared to the top-middle plot, the colors are reversed in order. The bottom-left plot highlights that at the end of training on 25 random functions, the model has a harder time generalizing to compositions of fewer functions compared to compositions of many of them (yellow and orange lines are always below purple). \n\n**>>> The paper claims that adding permutations aids direct composition; this is a counterintuitive finding that warrants further elaboration.**\n\nWe believe there is a misunderstanding and would like to clarify that this is not the exact claim that we have made in the paper. We restate our result below.\n\nThe direct prompt format is successful if we train on compositions of two functions: a permutation function and a bijection function. Hence the function composition is an element from the set $\\mathcal{F}_b \\circ \\mathcal{F}_p$. It is surprising that direct prompt format fails when we compose 2 functions which are both bijections, i.e. Transformers fail to generalize to compositions in the set $\\mathcal{F}_b \\circ \\mathcal{F}_b$.\n\nWe do not have a precise answer for why this is the case and find this observation surprising. But we speculate why this could occur. Permutations operate on the position of a token and bijections operate on the value of a token; the position and value are (nearly) orthogonal features of the input. Okawa et al., Lewis et al. (https://arxiv.org/abs/2310.09336, https://arxiv.org/abs/2212.10537) make similar observations for vision models and show compositional generalization on an orthogonal set of attributes. We believe a similar phenomenon is at play when a Transformer is forced to compose in a single step."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8467/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552773732,
                "cdate": 1700552773732,
                "tmdate": 1700552773732,
                "mdate": 1700552773732,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RgeVSemrl0",
                "forum": "tHHzfZSP6T",
                "replyto": "XxIGk7Ozpo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8467/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8467/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal to Reviewer xno8 (2/2)"
                    },
                    "comment": {
                        "value": "## Improvements to writing\n\nWe thank the reviewer for the careful reading of our work. We have incorporated the changes to the manuscript and have highlighted them in green.\n\n**>>> The experimental setup lacks sufficient clarity, making it challenging to understand the specific procedures.**\n\nThank you for raising this concern. We have significantly expanded Section 3.1 and Appendix A to include more details. We also commit to releasing the code.\n\n**>>> Section 3.2 lacks a clear definition and differentiation between permutations and bijections. It should be explicitly stated that the results in Section 4.1 pertain only to bijections.**\n\n\nWe have added details to the section (we note it is now section 3.1) and to appendix A.1 where we describe the set of permutations and bijections. We have also clarified in sub-section 4.1 that the results only pertain to bijections.\n\n**>>> Figure 3 needs more explanation about the notations. For instance, what f_{i_j} means?**\n\nWe have tried to explain the notation further in the updated manuscript and have also simplified it for ease of understanding. Specifically, Figure 2(a) now explains the role of $i$ and $j$ for a function $F^{(i)}_j$. The superscript $i$ determines the position in the composition for an in-order composition. For example in a composition of 3 functions, the function $F^{(2)}_3$, must be in the second position for an in-order composition, i.e., an in-order composition of 3 functions should look like $g \\circ F^{(2)}_3 \\circ h$ since $i=2$.\n\nThe index $j$ is used to iterate over the set of all possible functions that are allowed to be at position $i$. For example $F^{(2)}_1$, $F^{(2)}_2$ and $F^{(2)}_3$ (j=1,2,3) are all functions that can appear in the second position in an in-order compositions. All of these functions have the same color in Figure 3.\n\nWe have improved the writing in Section 3.1 to complement Figure 3. We would be glad to further clarify if the reviewer believes that the details are still not entirely clear.\n\n**>>> The explanation between \"random\" and \"21 base\" in Section 4.1 is unclear.**\n\nThank you for letting us know. We have added a section to Appendix A.1 explaining the 2 subsets in more detail.\n\nThe in-order compositions of 5 functions are depicted in Figure 2(a). This figure will be useful for understanding the set of functions \u201crandom\u201d and \u201c21 base\u201d which are both subsets of the set of all bijections $\\mathcal{F}_b$. \n\nWe consider the composition of 5 functions where each position can take one of 5 possible functions with one of the choices being identity. The two sets are defined as follows:\n\n\n\n1. Base 21:  Set of all functions such that at least 4 of the 5 positions are identity functions. There are a total of 21 such compositions in this set and they can be used to generate every other composition.\n2. Random: Set of 25 compositions sampled uniformly at random from the set of all in-order compositions (3125 such compositions)\n\n\u201c21 base\u201d does not have compositions of two or more functions unlike \u201crandom\u201d and forms a basis for the group of all in-order compositions.\n\n**>>> The experimental setup for Figure 6(b) in Section 4.3 is not elucidated. What is the rationale for having 25 bijections and 25 permutations?**\n\nWe would be grateful if the reviewer could clarify which specific details are unclear in Figure 6(b). The experimental setup (architecture, training methodology) is identical to all other experiments in the paper, as discussed in the beginning of the section. Specifically, Figure 6(b) considers the composition of two functions \u2014 one of which is a bijection and the other is a permutation.\n\nSince we consider the composition of only two functions, we increase the number of function choices at each position to 25. This results in a total of 625 compositions of functions. If we instead considered only 5 bijections and 5 permutations, then the total number of compositions is only 25, which would make our evidence for compositional generalization weaker.\n\n**>>> Typos in Page 2 and Figure 11**\n\nThank you for bringing this to our attention."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8467/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552979499,
                "cdate": 1700552979499,
                "tmdate": 1700552979499,
                "mdate": 1700552979499,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "39I1DG7qlo",
            "forum": "tHHzfZSP6T",
            "replyto": "tHHzfZSP6T",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8467/Reviewer_1C3m"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8467/Reviewer_1C3m"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an empirical study on how capable Transformer models are to generalize compositionally, a question that has been broadly discussed in the last 5 years in research literature. The specific task that the paper uses for the empirical study is composing functions defined on the domain of fixed-length works over a fixed vocabulary. The paper considers stepwise (i.e. with intermediate outputs) and direct composition (i.e. without intermediate outputs) setups. Another axis variation is whether the function order can be different at test time than training time (in-order vs out-of-order). The key observations are that (1) provided enough diversity in the training data, Transformers can learn to compose functions (2) stepwise composition is easier to learn."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The paper is mostly clearly written and easy to understand."
                },
                "weaknesses": {
                    "value": "I don\u2019t think that this paper makes a meaningful contribution to the field of deep learning on top of the already available work. The general question of whether Transformers or other neural architectures can generalize compositionally has already been discussed in numerous papers, including similar function compositional setups ([1, 2]). The general consensus in the literature is that with enough diversity, any neural architecture can learn any compositional behaviors. Further interesting questions can be asked: what architectures can learn compositional behavior from less diverse training data, can we get diverse enough training data to achieve compositionality in real-world tasks (GPT-4 seems to partially answer that), how compositional generalization abilities of neural models compare to those of humans. This paper, however, does not go deeper into one of these or any other direction, it discusses compositional generalization and the highest most abstract level, at which the answer is: it depends. While the paper acknowledges that there is ample prior work on the topics, the paper fails to explain what it adds on top. The finding that step-wise composition is easier than direct is rather unsurprising, especially in the view of chain-of-thought prompting of LLM that has been getting popular lately.\n\n[1] https://arxiv.org/abs/1802.06467\n[2] https://aclanthology.org/2022.emnlp-main.662.pdf"
                },
                "questions": {
                    "value": "- Section 3.2 is a bit unclear on what are \u201cbijection\u201d and \u201cpermutation\u201d mappings in the paper\u2019s context. My understanding is that bijection here means per-token bijection, whereas permutation means shuffling tokens of the word.\n- The details of what the vocabulary size and what the size of the word is were difficult to find."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8467/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698629236761,
            "cdate": 1698629236761,
            "tmdate": 1699637056978,
            "mdate": 1699637056978,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AK0cfhRQuL",
                "forum": "tHHzfZSP6T",
                "replyto": "39I1DG7qlo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8467/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8467/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1C3m (1/3)"
                    },
                    "comment": {
                        "value": "We thank the Reviewer for their feedback. We hope to engage in a discussion and convince the Reviewer of the importance of our work. We have answered specific questions below. \n\n**>>> Further interesting questions can be asked \u2026\u2026\u2026..**\n\nAll these questions are indeed very interesting. However, we emphasize that our work has a very different goal. We do not intend to develop novel methods for improving Transformers\u2019 ability to compositionally generalize. Our goals are twofold: 1) to demonstrate that compositionality can occur in autoregressively trained Transformers without changes to standard training pipelines, and if the underlying data domain itself is compositional, and 2) to understand what drives the existence of compositionality in large models. Importantly, we emphasize the motivation for our goals, as discussed in detail in the introduction, is grounded in recent literature on eliciting or predicting capabilities in pretrained models [1, 2]. Specifically, we aim to argue that if via training on a compositional domain (e.g., language), a model learns to compose its capabilities, then empirical benchmarking will be insufficient to characterize what capabilities the model possesses, i.e., what tasks it can perform.\n\n1. Percy Liang, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022\n2. Jordan Hoffmann, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 202\n\n**>>> How compositional generalization abilities of neural models compare to those of humans. This paper, however, does not go deeper into one of these or any other direction**\n\nWe believe it is unfair that the reviewer asks us to tackle questions that compare Transformers to humans. We believe the results in our work are new and compelling and the suggestions seem entirely based on the reviewers interests, which we feel is very different to the motivations and goals of this work, as specified in detail in the introduction. \n\nWe restate the contributions of our work in a broader context. There are many prior works that attempt to estimate the capabilities of Language models [1,2,3]. One aspect of understanding LLMs is to study inductive biases and properties of Transformer architectures on minimal synthetic setups, similar in spirit to [4, 5, 6]. Motivated by this, our work demonstrates that autoregressive Transformers (with commonly used training pipelines) learn to compose functions across several different scenarios; LSTMs fail on this same task. Our results on the synthetic task add credence to the hypothesis that language models can have far more capabilities than the ones directly present in the training data. Estimating these capabilities can be hard since the models need to be prompted appropriately\u2014which is possible in our synthetic setup. To better understand why this occurs, we also study many variations to the properties of the training data and offer a preliminary mechanistic analysis.\n\nTo address reviewer\u2019s comment, we have explained this narrative more clearly in the revised version of the manuscript.\n\n1. Aarohi Srivastava, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.\n2. Percy Liang, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022\n3. Alex Tamkin, et al.. Understanding the capabilities,limitations, and societal impact of large language models. arXiv preprint arXiv:2102.02503, 2021.\n4. Bingbin Liu et al., . Transformers learn shortcuts to automata. arXiv preprint arXiv:2210.10749, 2022.\n5. Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.1, knowledge storage and extraction. arXiv preprint arXiv:2309.14316, 2023a.\n6. Shivam Garg, et al.. What can transformers learnin-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583\u201330598, 2022"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8467/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552344253,
                "cdate": 1700552344253,
                "tmdate": 1700552344253,
                "mdate": 1700552344253,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JUaULLbyQS",
                "forum": "tHHzfZSP6T",
                "replyto": "39I1DG7qlo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8467/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8467/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1C3m (2/3)"
                    },
                    "comment": {
                        "value": "**>>> The general consensus in the literature is that with enough diversity, any neural architecture can learn any compositional behaviors.**\n\nWe disagree with the reviewer\u2019s claim that any neural architecture can learn compositional behaviors with enough diversity. We present a new result in the manuscript where we train an LSTM on the same task as used for our Transformer models in both the direct and step-by-step prompt formats. LSTMs perform significantly worse compared to Transformers (~30% vs ~100%), highlighting that **the architecture has a large impact on compositional generalization for the same training data**.  \n\n**We would also like to point out that \u201cenough diversity\u201d is ill-defined and our work also attempts to characterize the same.** We study how precisely defined properties of the data affect downstream generalization: we show how factors like spurious correlations in the training data (in-order compositions), details of the prompt format (step-by-step and direct), and the number of compositions in the training data have vastly different effects on compositional generalization. Importantly, we emphasize that diversity in the data does not necessarily guarantee compositional behavior. In our work, we find that training on 2000 of the 3125 bijections (Figure 6 (left)) does not result in compositional behavior if we use the direct prompt format. However we can compositionally generalize using the direct prompt format if we use permutations and bijections. **Hence, our results indicate that there are different types of compositional data and Transformers can fail to learn some of them even with sufficient diversity.** \n\nFinally, we would like to add that **Transformers trained with the step-by-step data format are able to compose with very little diversity** in the training data. In Figure 5, we can train on as few as 125 functions (**0.003% of the total number of functions**) but we can still generalize to a combinatorial set of functions not present in the training data (4 million of them). This is a surprising result and points to how data diversity, while important, can be rather minimal when the appropriate architecture and data format are used. \n\n**>>> I don\u2019t think that this paper makes a meaningful contribution to the field of deep learning on top of the already available work.**\n\n**>>> While the paper acknowledges that there is ample prior work on the topics, the paper fails to explain what it adds on top**\n\n**>>> including similar function compositional setups ([1, 2])**\n\nThank you for pointing us to these papers. We have included them in our discussion in the Related work section. However, we hope to convince the reviewer of the importance of our work in the context of these prior works. \n\nOur work differs from prior work like [1, 2] (and several others that are reviewed in Section 2) in the following ways:\n\n\n1. The goal of works like [1, 2] is to design new or improve existing architectures such that they compose better. While this is an important problem, we re-iterate that **our goal is different**: we would like to demonstrate that Transformers can learn to compose functions with very little training data and minimal data diversity, and we intend to understand what drives the existence of compositionality in autoregressive Transformers trained using the standard training pipeline. \n2. [1, 2] observe that Transformers and RNNs do not learn to compose on datasets like CTL and CTL++. However, **we present results that, on the contrary**, show that the **standard Transformer** architecture is capable of almost perfect compositional generalization. It would be interesting to understand why these differences exist and this warrants further exploration. We hypothesize that a reliable training pipeline with all the bells and whistles is largely responsible for this difference. No prior work has shown minimal demonstrations, that show this is possible for Transformers that perform stepwise computation\u2013which is very natural in autoregressive models. \n3. Our **work evaluates functions outside of table lookups** and identifies that compositionality depends on the nature of the composed functions (permutations vs. bijections), which isn\u2019t considered in prior works like [1, 2].\n4. **The scale of our experiments is larger.** We evaluate these models systematically on as many as 4 million functions (even if the model was trained on as few as 100 of these functions) and take steps towards identifying precise conditions under which compositionality occurs and fails. In comparison some table-lookup tasks like [1, 2] are often limited to evaluation on just 128\u2013256 functions\n\nWe also note that we have significantly expanded the Related work section to emphasize some of these points."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8467/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552504290,
                "cdate": 1700552504290,
                "tmdate": 1700552504290,
                "mdate": 1700552504290,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "y7GEqU0mlU",
                "forum": "tHHzfZSP6T",
                "replyto": "39I1DG7qlo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8467/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8467/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1C3m (3/3)"
                    },
                    "comment": {
                        "value": "**>>> finding that step-wise composition is easier than direct is rather unsurprising,**\n\nWe arguably agree that this result is unsurprising, but emphasize that the step-by-step prompt format does better with extremely little data: we find that training on as few as 50 functions (1% of total) is enough to perfectly generalize to all other compositions not present in the training data. While prior works on the CTL dataset have identified that Transformers are poor at compositional generalization, our experiments in the step-by-step format highlight why the same need not be true for an autoregressive model, and how it significantly changes such a conclusion.\n\nThe compositional abilities of chain-of-thought in language models are still not well understood, For example works like Dziri et al. ([https://arxiv.org/abs/2305.18654](https://arxiv.org/abs/2305.18654)) argue that  language models have limited compositional abilities. Our work is a demonstration that it doesn\u2019t seem to be the case in the synthetic setup even with limited data diversity. We hypothesize that the community may be severely underestimating the capabilities of pretrained language models, since we do not know the ideal prompt (unlike in the synthetic setup) to elicit a capability, even though it may exist in the model.\n\n**>>>  it discusses compositional generalization and the highest most abstract level, at which the answer is: it depends.**\n\nA strong claim requires strong evidence. We disagree that our experiments and discussion is only at the abstract level and we request the reviewer to justify why they think this is the case. Our work demonstrates that Transformers can easily learn to compose under a precisely defined data generating process.\n\nTo the best of our knowledge, we are the first to demonstrate how drastically small the training data can be, such that a Transformer learns to compositionally generalize. Our work also characterizes some properties of the training data (in-order compositions, step-by-step format, base21 vs. random) which contribute to compositional generalization. There is little to no other work that precisely characterizes when Transformers learn to compose, let alone demonstrate that it occurs with so little data. Our work makes progress on both these fronts.\n\n**>>>  My understanding is that bijection here means per-token bijection, whereas permutation means shuffling tokens of the word.**\n\nThat is indeed correct. We apologize if this wasn\u2019t clear. We have added Figure 10 to Appendix A.1 that hopefully clarifies this detail.\n\n**>>> The details of what the vocabulary size and what the size of the word is were difficult to find**\n\nThe details are present in Appendix A. We have also included them in Section 3.2 in the updated version of the manuscript."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8467/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552655007,
                "cdate": 1700552655007,
                "tmdate": 1700552655007,
                "mdate": 1700552655007,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JyeyTwejWT",
            "forum": "tHHzfZSP6T",
            "replyto": "tHHzfZSP6T",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8467/Reviewer_h95b"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8467/Reviewer_h95b"
            ],
            "content": {
                "summary": {
                    "value": "The authors study how well Transformers can learn to compose functions via a synthetic task of composing bijective functions. They study in-order and out-of-order generalization; and also step-by-step and direct computation the composed functions. They show that Transfomers can generalize well with step-by-step computation but not as well at direct computation. Similarly, in-order compositions are easier to generalize to compared to out-of-order compositions."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is clearly written and easy to follow.\n - The paper introduces a new and useful synthetic task to understand compositional generalization, that will be of use to the research community. \n - The paper illustrates new compositional capabilities and limitations of Transfomers."
                },
                "weaknesses": {
                    "value": "- I think an ablation over the number of layers in the Transformer is a key missing study here. Many studies (e.g. [Weiss et al](https://arxiv.org/abs/2106.06981), [von Oswald et al](https://arxiv.org/abs/2212.07677)) show that the depth of a Transfomer is key to what it can compute, so I'd like to see a sweep over the number of layers in the Transfomer and how that affects compositional generalization.\n - I am not fully convinced that step-by-step computation implies compositional generalization. If the Transfomer is supervised with the outputs of the intermediate function results, isn't it then just learning 1. the individual functions and 2. that it should apply them sequentially? Maybe I'm missing something, but I'd definitely appreciate some sort of discussion around this in the paper. \n - (nitpick, did not affect review score) Suggestion: I think the title of the paper could be a lot better to reflect what's in the paper. Perhaps \"Studying compositional generalization in transfomers via synthetic bijective functions\" or something along those lines."
                },
                "questions": {
                    "value": "- To test if the Transfomer learned a certain composition of functions, how many examples (i.e. permutations of tokens) are used, and how are they constructed? Are experiments restricted to 6 non-unique tokens per example like in Figure 3? Unless I missed something, I think it'd be useful to clarify this. \n - Will the authors release code/data to reproduce this paper? I imagine the code is not hard to write, but one benefit of such papers that use synthetic tasks and small models is that they should be easy to reproduce."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8467/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8467/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8467/Reviewer_h95b"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8467/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698829771991,
            "cdate": 1698829771991,
            "tmdate": 1699637056857,
            "mdate": 1699637056857,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7l59XrrRo5",
                "forum": "tHHzfZSP6T",
                "replyto": "JyeyTwejWT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8467/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8467/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer h95b (1/1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their time and are glad that they find the paper well-written, the synthetic setup to be useful and our results on compositional capabilities to be new. We hope that the reviewer will champion our paper. \n\n**>>> I\u2019d like to see a sweep over the number of layers in the Transformer and how that affects compositional generalization.**\n\nThank you for this suggestion! We have two new results that vary the number of layers in a Transformer (Appendix B.1). We find that Transformers **compositionally generalize even with 1 layer and up to 28 layers** in the step-by-step format. Models also compositionally generalizes in the **direct prompt format** but **requires at least 2 to 3 layers** and works only for a composition of a permutation and a bijection.\n\n**>>> isn't it then just learning 1. the individual functions and 2. that it should apply them sequentially?**\n\nWe emphasize that the step-by-step prompt format does not necessarily imply compositional generalization and it was not our intent to indicate that this is the case. We have updated the manuscript to address this (for example, see Section 4.1, green text) and have also highlighted two results (one of them new) which show that the ability to compose also depends on the architecture and training data, not just the use of step-by-step prompting format.\n\n\n* On Architecture: We have conducted new experiments (see Appendix B.2)  with **LSTMs and observe that they do not compositionally generalize in the step-by-step and direct prompt formats**. Transformers seem to have an inductive bias to capture compositional structures in the training data. As a result, they can have a significantly larger set of capabilities compared to what\u2019s present in the training data.\n* On training data: The training data also influences if the model learns to compose. Figure 4 shows that training on just the individual functions (21 base, which acts like a null model for the choice of the training data) does not result in compositional generalization. In addition, the model must see compositions of some of these functions in the training data\u2014albeit very few of them\u2014for it to generalize to unseen compositions. \n\n**>>> To test if the Transfomer learned a certain composition of functions, how many examples (i.e. permutations of tokens) are used, and how are they constructed? Are experiments restricted to 6 non-unique tokens per example like in Figure 3?**\n\nThese details are present in Appendix A, but to emphasize them further, we have also added them to Section 3.2 (highlighted in Green) in an updated version of the manuscript. In brief: The vocabulary is of size 10 and we sample 6 tokens from this vocabulary to create the input string in all our experiments (like in Figure 3). To test if a Transformer has learnt a certain composition, we compute the accuracy of prompt completions on 1000 samples (10% of the entire input domain).\n\n**>>> (nitpick) I think the title of the paper could be a lot better to reflect what's in the paper.**\n\nThanks for this suggestion! We will try to select a better title and are currently workshopping a few options. Our choice of current title was motivated by recent works that try to estimate the existence of a capability in a pretrained Language model, or predict its existence in a future version of the models [1,2]. We argue that if a model learns to compose its capabilities, such capability predictions will face challenges and likely underestimate the set of tasks that a pretrained Transformer can learn to perform, i.e., it is difficult to judge \u201chow capable it can become\u201d.\n\n1. Percy Liang, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022\n\n2. Jordan Hoffmann, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 202\n\n**>>> Will the authors release code/data to reproduce this paper?**\n\nWe commit to releasing all code and data."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8467/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700552121734,
                "cdate": 1700552121734,
                "tmdate": 1700552121734,
                "mdate": 1700552121734,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NqGrNM5nXf",
            "forum": "tHHzfZSP6T",
            "replyto": "tHHzfZSP6T",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8467/Reviewer_dYRf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8467/Reviewer_dYRf"
            ],
            "content": {
                "summary": {
                    "value": "The research delves into the compositional capabilities of transformers, examining their potential to generalize to functions not present in their training data. Using a synthetic setup, the authors found that transformers can learn and generalize to an exponential or even combinatorial number of functions based on the compositional structure in the data. The nature of the training data plays a pivotal role in this generalization, with step-by-step compositions proving more effective than direct ones. Additionally, attention layers, particularly between layers 6-10, were identified as crucial for compositional generalization. While the study underscores the promise of transformers' compositional abilities, it also highlights the challenges and nuances of using synthetic data and poses further questions about the underlying mechanisms in transformers."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "None"
                },
                "weaknesses": {
                    "value": "It is conceptually wrong to evaluate a  capability of \"a Transformer\" since it will depend on architecture, training data, training methodology etc.\nMoreover I failed to identify which transformer was used in the paper.\nCapability of each layer in a transformer is again depends on the number of heads, hidden dimension etc. therefore it is not correct to identify layers 6-10 as crucial layers for compositional generalization.\nOverall, I perceive the paper as lacking scientific depth, offering merely a mechanical examination of an ambiguous model without a clear interpretation."
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8467/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698835810029,
            "cdate": 1698835810029,
            "tmdate": 1699637056740,
            "mdate": 1699637056740,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PYiPUrTqYW",
                "forum": "tHHzfZSP6T",
                "replyto": "NqGrNM5nXf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8467/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8467/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dYRf (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for the feedback and for improving the quality of our work. We hope to engage in a discussion and convince the reviewer of the importance of our work. We have made changes to the paper which hopefully addresses the reviewer\u2019s concerns. \n\n**>>> It is conceptually wrong to evaluate a capability of a Transformer since .....**\n\nWe would be happy to incorporate suggestions from the reviewer to make the title more precise. Our title draws inspiration from a recent string of works with similar titles (\u201cTransformers learn shortcuts to automata\u201d, \u201cWhat can transformers learn in-context? a case study of simple function classes\u201d, \u201cOn the ability and limitations of transformers to recognize formal languages\u201d). Similar to the motivations of our work, these papers study transformers on synthetic datasets to understand the phenomenology in a controlled setting. Our title also draws inspiration from literature on estimating the \u201ccapabilities\u201d of LLMs [1,2,3].\n\n1. Aarohi Srivastava et al.l. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022\n2. Percy Liang, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022\n3. Alex Tamkin, et al.. Understanding the capabilities,limitations, and societal impact of large language models. arXiv preprint arXiv:2102.02503, 2021.\n\n**>>> ..... it will depend on architecture, training data, training methodology**\n\nWe agree! In fact, we emphasize that our paper\u2019s title is \u201cHow capable can a transformer *become*?\u201d; that is, we\u2019re focused on the setting where a Transformer model is trained, via a relatively standard pipeline, on a data domain that matches the compositional nature of language to see if the model can learn to compose its capabilities. While we already rigorously ablate the influence of structure in training data in our experiments, we have added further experiments that we believe, addresses all three aspects raised by the reviewer (architecture, training data, and training methodology).\n\n_Training data:_ As stated in the reviewer\u2019s summary of our work, our work presents experiments that show how \u201cthe nature of the training data plays a pivotal role in this generalization\u201d. We precisely characterize how changes to the training data (in-order and out-of-order compositions, base21 and random subsets, step-bv-step and direct prompts) can either result in a combinatorial explosion of capabilities, no compositional generalization or somewhere in between. A surprising observation is that a very small amount of functions in the training data (0.003%  of the total number of functions) is enough to observe compositional generalization when Transformers performs the computations step-by-step.\n\n_Training Methodology:_ We study auto-regressive Transformers trained using the cross-entropy loss. We have made changes to the manuscript to make this more prominent in the introduction and abstract (highlighted in green). We have also added more details to Appendix A.\n\n_Architecture:_ We have added new experiments to Appendix B.1 where we change the number of layers, the number of attention heads, and the embedding dimension. Transformers exhibit compositional compositional generalization (step-by-step and direct formats) even if we vary the number of layers or the number of attention-heads. This ability deteriorates if we reduce the embedding dimension. **This is evidence that our observations are not specific to one single Transformer architecture.**\n\nIn addition to these results, we add that many influential previous works, that study Transformers in similarly designed synthetic setups like ours (e.g., [https://arxiv.org/abs/2208.01066](https://arxiv.org/abs/2208.01066)), present experiments on a single Transformer architecture. The tacit assumption in these works is that deep networks of different sizes and embedding dimensions learn similar functions even if they are represented differently using the weights. As a result, studying one architecture translates to other variants of the same architecture.\n\nWe also present new experiments (Appendix B.2) that train a set of LSTM on these tasks. **While LSTMs achieve perfect training accuracy, we observe that it fails to compositionally generalize.** This is evidence that auto-regressive Transformers in the step-by-step prompt format have an inductive bias for compositional generalization and all architectures do not work in our synthetic setup.\n\n**>>> Moreover I failed to identify which transformer was used in the paper.**\n\nThe details are presented in Appendix A. We use nanoGPT ([https://github.com/karpathy/nanoGPT](https://github.com/karpathy/nanoGPT)) with 12 layers, 12 attention heads and an embedding dimension of size 120."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8467/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700551562460,
                "cdate": 1700551562460,
                "tmdate": 1700551867672,
                "mdate": 1700551867672,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LVs8RwheEc",
                "forum": "tHHzfZSP6T",
                "replyto": "NqGrNM5nXf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8467/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8467/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dYRf (2/2)"
                    },
                    "comment": {
                        "value": "**>>>  it is not correct to identify layers 6-10 as crucial layers for compositional generalization**\n\nThank you for raising this point and we accept we poorly phrased this claim! We intended to say that attention layers in the *latter half* of a Transformer play a crucial layer in compositional generalization. We have modified the claims in the introduction to better address this. We have also run two new experiments to further strengthen our claims empirically. \n\nLinear probing of Transformers of different depths: We conduct  the linear probe analysis on Transformers of different sizes (see Figure 14, Appendix B.4). Across 15 Transformers of different depths, we make two observations: (1) The accuracy increases sharply after an MLP layer for most architectures (2) The composition seems to occur in later layers of a Transformer. We have modified the manuscript to state that: \u201cOur experiments indicate that attention layers in the latter half of a Transformer play a role in compositional generalization\u201d\n\nAttention maps: We have also added Figure 7 (right), where we draw the attention maps for a 1-layer transformer. The map clearly highlights two tokens: the task-token (function to compute) and the intermediate output token to operate on. This experiment could help us build a precise mechanistic description of the role of attention in compositionality. Currently, it provides further credence to our claims on the critical role of attention in compositionality.\n\n**>>>  perceive the paper as lacking scientific depth, offering merely a mechanical examination of an ambiguous model without a clear interpretation.**\n\nWe hope to convince the reviewer of the importance of our work. Our goal is not to offer a mechanical examination of compositionality and precisely characterize why it occurs in Transformers. Our goal is demonstrative and we show how Transformers can learn to compose using a small number of functions training data, and we take initial steps to precisely characterize when this occurs.\n\nWe restate some our main contributions:\n\n\n\n1. Our work proposes a **new synthetic setup to systematically study compositional generalization** in autoregressive Transformers\n2. We show that transformers (with different depths, heads) can **generalize to combinatorially many functions which are entirely out-of-distribution**. The results are surprising: we identify that the step-by-step format with small amounts of diversity in the training data (as few as 0.003% of all the functions) is enough to achieve compositional generalization in Transformers. The ability of a Transformer to output intermediate steps of the computation makes it significantly easier to perform compositions.\n3. We **precisely characterize conditions on the training data** under which Transformers struggle to compose. We also show that autoregressive LSTMs struggle to compose in the same scenarios where autoregressive Transformers succeed.\n4. In addition, we offer a preliminary analysis highlighting that attention in the later layers seems critical for compositionality. We also present results that analyze the attention maps of the Transformer which reveal mechanistic insights into how the attention operates."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8467/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700551756514,
                "cdate": 1700551756514,
                "tmdate": 1700551756514,
                "mdate": 1700551756514,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]