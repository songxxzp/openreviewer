[
    {
        "title": "Graph2Tac: Learning hierarchical representations of math concepts in theorem proving"
    },
    {
        "review": {
            "id": "xPpOz2noDT",
            "forum": "XyB4VvF01X",
            "replyto": "XyB4VvF01X",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1459/Reviewer_WdSC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1459/Reviewer_WdSC"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a graph based approach for theorem proving with Coq. To do so, the authors propose to extract graph representations from existing Coq packages, and train a GNN that learns the meaning of definitions and other math symbols, namely G2T. The authors also enable G2T to incorporate unseen definitions by predicting their embeddings on the fly. The proposed method is evaluated on the separate Coq dataset, where it performs similarly to (sometime slightly better than) k-NN method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This work proposes to represent existing Coq proofs into graph and proposes a new GNN+RNN model to learn the graphs---This approach is interesting and seems novel to me, though I'm not familiar with the literature to be certain that no similar works were proposed before.\n\nThe overall methodology proposed in this work seems sound to me, though I'm not able to fully inspect the details due to lack of expertise in Coq and presentation issues (see weakness)."
                },
                "weaknesses": {
                    "value": "Despite being a highly sophisticated model, Fig 5 suggests that G2T does not perform better than simple baselines such as k-NN:\n- K-NN outperforms all pure G2T variants in terms of pass rate/sec for the most part, as the author pointed out, maybe for more complicated task G2T will have advantage, but it is not sufficiently demonstrated in the experiments.\n- The only method outperforms k-NN is the hybrid one G2T+k-NN, which really cannot prove that G2T is a superior method as it is difficult to tell the contribution apart.\n \nThat said, I find the empirical significance of this work to be minor.\n \nOther than that, the dataset created from the Coq package seems to be a good addition to the literature but there lacks too many details on how the dataset is created and processed, and further comparison to other datasets are needed for readers to assess its novelty and properly position it in the literature."
                },
                "questions": {
                    "value": "My main concern with the method is the online addition of definition embeddings. It is unclear to me how unknown definition is encoded and predicted without a text encoder. Judging from 3.1, definition nodes that are seen in the training set all get assigned with an embedding, then if an unseen definition comes, supposedly there is no embedding in the table that matches this definition, then how does one encode it into an embedding in the first place? Furthermore, it seems to me the definition task is essential for learning to represent unknown definitions, then for the G2T-NoDef model, how is unseen definition handled if there was not a definition task?\n\nAs a reader not familiar with Coq syntax, I find some technical parts very difficult to follow\n \nIt is Difficult to comprehend the diagrams in Fig 2, 3 and 4:\n- For example, While the caption says Fig 2 is the graph representation of Fig 1, it is difficult to see the connections: what do \"@\" do \"up_arrow\" correspond to? What does different color mean in the graph? How is online update reflected in the graph representation? Why are there triangles with no symbols?\n \n3.1 misses formal definitions of many important components making it difficult to track the architecture. For example:\n- The RNN for local and global argument prediction\n- The loss for definition and prediction task\n- The formal representation of how new definitions are added to the current state, and how are they calculated into an embedding using the definition task.\n- Fig 4 is not very helpful either---notations and names do not match very well with the text: what is \"const embs\"? are they trainable node labels? Which part corresponds to edge embeddings? Which parts correspond to \"entry-point\" nodes and \"local context\" nodes?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "none"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1459/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698720633241,
            "cdate": 1698720633241,
            "tmdate": 1699636074737,
            "mdate": 1699636074737,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oXprZMifJJ",
                "forum": "XyB4VvF01X",
                "replyto": "xPpOz2noDT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1459/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1459/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to WdSC"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review.\n\n**On the empirical significance of our work**\n\nAs we mention in the common reply to all reviewers,\nthe k-NN solver was actually among state of the art in this field, better than existing works in this field.\nWe are the first paper to systematically compare it to other approaches and point this out.\nWe provide evidence that Graph2Tac outperforms any other neural or symbolic solvers available for Coq.\n\nThe k-NN is a strong approach with two advantages that many other solvers don't have.\nEach call is fast, allowing it to explore an order of magnitude more states in a given time\n(see Figure 9 in Appendix G _The Relative Speed of the Models_).\nAlso, it incorporates online proof information not seen during training\n(see Figure 1).\nOur G2T model not only rivals the state of the art k-NN despite being slower,\nit is also a complementary approach to the online setting\nof working in a package not seen during training.\n\nAlso, we see the main significance of our work is the introduction of a novel method that learns from the entire hierarchy of mathematical objects,\nespecially in an online setting. Our definition task improves G2T from 17.4% to 26.1% which is a 1.5x increase in theorems proved.\n\n**On the definition task**\n\nAs for the definition task, we think there are some misunderstandings,\nwhich we will try to make more clear in the paper:\n\n> It is unclear to me how unknown definition is encoded and predicted without a text encoder.\n\nFor each new definition we have its definition body.\nWe uses graphs instead of text to encode new definitions, but otherwise it is similar to a text encoder.\nThe (graph of) the definition is put into our graph model, and the new definition embedding is the (pooled) output vector.\nGraphs also let us build hierarchical representations, where the representation of a definition is\nbuilt from the embeddings of the component definitions.\n\nWhen predicting a definition as an argument to a tactic, we select it from among all possible arguments in the global context.\nThis attention-like mechanism is step G in Figure 4.\n(We will rename constant embeddings to definition embeddings in the Figure to be more clear.)\n\n> Judging from 3.1, definition nodes that are seen in the training set all get assigned with an embedding,\n> then if an unseen definition comes, supposedly there is no embedding in the table that matches this definition,\n> then how does one encode it into an embedding in the first place?\n\nDefinitions seen during training use embeddings learned during training,\nsimilar to how, say, a transformer model learns vocabulary embeddings.\nOur definition task trains a graph model which can predict those learned embeddings.\nIf we encounter a new definition not seen during training, \nwe use our definition graph model to predict its embedding.\n\n> Furthermore, it seems to me the definition task is essential for learning to represent unknown definitions,\n> then for the G2T-NoDef model, how is unseen definition handled if there was not a definition task?\n\nThe G2T-NoDef model uses learned embeddings for definitions seen during training.\nAt inference, any new embeddings get assigned random vectors (the `-Frozen` setting described in Section 3.1).\n\n**Regarding clarifications of the figures**\n\nWe agree that Figure 2 could use more explanations. Unfortunately, the page limit prevents us from adding this. \nHowever, the graph dataset has been extensively\ndocumented in a separate publication. An anonymized version can be found\n[here](http://64.71.146.254:8000/SFyud_C5TmEq7AwnK9jaLsfTFMzgBl54cQ0pl2FJB-x9o2Hk24F4jO_W75RqGdOJ/papers/web-paper.pdf).\nRegarding Figures 3 and 4, we agree they are complex.  We will attempt to clean them up and align them more with the text.\nTo answer your questions:\nThe edge and node embeddings are used in step A to initialize the embeddings of the graphs,\nbut we don't see how to put them in the figure.\nWe will relabel \"constant embeddings\" to be \"definition embeddings\" to match the text.\nThe entry point of the definition graph is the node labeled `f`.  Do you think we should label that?\nIt is unclear to us how to improve Figure 3.  Do you have suggestions on what could make the figures more clear?\n\n**More technical details in 3.1**\n\nWe will clean up Section 3.1 and add more details (to that section or the appendix)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1459/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700081786766,
                "cdate": 1700081786766,
                "tmdate": 1700088009392,
                "mdate": 1700088009392,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5zexxC7GoL",
                "forum": "XyB4VvF01X",
                "replyto": "xPpOz2noDT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1459/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1459/Authors"
                ],
                "content": {
                    "title": {
                        "value": "New update to the paper for Reviewer WdSC"
                    },
                    "comment": {
                        "value": "In Appendix P: *Further details on the G2T architecture* we add more details about the model architecture including more descriptions about the graph inputs going into the model and the explicit formulas for the calculations.  We color coded it in dark red.\n\nWe also fixed a number of small inconsistencies between the images and the text in Section 3.1 describing the model.  For example, we renamed entry point nodes to root nodes, and in the text pointed out where to find them in Figure 4.  We also explicitly labeled the definition embedding table in Figures 3 and 4.  It will hopefully be easier to follow."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1459/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691205944,
                "cdate": 1700691205944,
                "tmdate": 1700691243927,
                "mdate": 1700691243927,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7q96QS7zKd",
            "forum": "XyB4VvF01X",
            "replyto": "XyB4VvF01X",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1459/Reviewer_Ursv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1459/Reviewer_Ursv"
            ],
            "content": {
                "summary": {
                    "value": "Authors propose a novel method for automated theorem proving in Coq.\n\nThey utilize graph neural networks (GNN) to construct a hierarchical representation of Coq theorems and definitions.\nThey support utilizing the same GNN at inference stage, on previously unseen Coq theorems and definitions.\nEmbeddings their GNN model constructs aids in theorem proving.\n\nThey evaluate their technique and show that in combination with the k-NN method it achieves superior results given a limited time frame of 10 minutes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Well written\n- Clear motivation\n- Involved technique. Architecture authors constructed is complex and involves many steps.\n- Authors plan to open source the technique, including both code and dataset."
                },
                "weaknesses": {
                    "value": "- Evaluation. \n    - It is unclear that proposed technique is superior to others. E.g., in Figure 7, most of the times k-NN performs better.\n    - Authors show that their technique in combination with k-NN is superior to others. But, what if we combine some other technique with k-NN?"
                },
                "questions": {
                    "value": "- I am curious to see, given a large time limit (say 24 hours), what is the distribution of theorems solved by different systems?\n- Have you noticed some patterns in theorems solved by different systems? You show a Venn diagram and note that different systems solve significantly different sets of theorems.\n- Can you show some visualization of embeddings produced by your GNNs? Are there some interesting relations, groups, etc, in the embedding space?\n- Can you run your tool on CoqGym, or is it not compatible?\n- Typos and minor comments\n    - page 1: \"are able make\" -> \"are able to make\"\n    - page 2: \"can not\" -> \"cannot\" (cannot is more common)\n    - page 2: \"run a on\" -> \"run on a\"\n    - Figure 5: Please make the plot clearer, so one can easily distinguish which line corresponds to which setting."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1459/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1459/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1459/Reviewer_Ursv"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1459/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698748352290,
            "cdate": 1698748352290,
            "tmdate": 1699636074649,
            "mdate": 1699636074649,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HIGiRRmQ0v",
                "forum": "XyB4VvF01X",
                "replyto": "7q96QS7zKd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1459/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1459/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Ursv"
                    },
                    "comment": {
                        "value": "Thank you for your review and for highlighting the strengths of our paper.\n\nAs for performance of our G2T model versus the k-NN, see the common response to all the reviewers.\nOur G2T rivals the already state-of-the-art k-NN, suggesting our model is also state-of-the-art.\n\n> But, what if we combine some other technique with k-NN?\n\nThis is highlighted in the Venn diagram in Figure 6.\nThe two approaches which are most disjoint are the k-NN and G2T solvers.\nNonetheless, we are happy to include an appendix highlighting additional combinations of models.\nThis will be added to a future revision of the paper.\n\n> I am curious to see, given a large time limit (say 24 hours),\n> what is the distribution of theorems solved by different systems?\n\nUnfortunately, we don't have the computational budget to run our solver for 24\nhours per theorem for a statistically significant sample. \nNonetheless, Figure 5\nis on a log scale and it shows a strong log-linear trend. Extrapolating to 24\nhours, our plot suggests that G2T-Anon-Update might increase from 26.1% to\n30-35%.\n\n> Have you noticed some patterns in theorems solved by different systems?\n> You show a Venn diagram and note that different systems solve significantly different sets of theorems.\n\nThis is partially highlighted in Figure 7 where different approaches perform better on different packages.\nFurther in Appendix L _Impact of the Online Setting_ we show the significance of the online setting.\nWhen there are more dependencies the k-NN and G2T solvers show their strength.\nTime permitting, we will try to look into this more, but we also plan to release our theorem-level results,\nso that others can also perform this analysis without having to rerun the benchmarks.\n\n> Can you show some visualization of embeddings produced by your GNNs?\n> Are there some interesting relations, groups, etc, in the embedding space?\n\nThank you for the suggestion.\nWe are currently doing an analysis with UMAP (similar to t-SNE) embeddings.\nWe will include it in the paper when complete.\nPreliminary findings show that there are strong patterns in the embeddings.\nThe embeddings using the definition task are more structured and interpretable.\n\n> Can you run your tool on CoqGym, or is it not compatible?\n\nIn the new Appendix B _Informal comparison on CoqGym test packages_ we make an informal comparison of our models to CoqGym,\nwhich suggests the k-NN and G2T models are state of the art for Coq in the online setting\nof a new project not seen during training."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1459/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700081758837,
                "cdate": 1700081758837,
                "tmdate": 1700081758837,
                "mdate": 1700081758837,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PjsQdgCh9w",
                "forum": "XyB4VvF01X",
                "replyto": "7q96QS7zKd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1459/Reviewer_Ursv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1459/Reviewer_Ursv"
                ],
                "content": {
                    "comment": {
                        "value": "> But, what if we combine some other technique with k-NN?\n\n>> This is highlighted in the Venn diagram in Figure 6. The two approaches which are most disjoint are the k-NN and G2T solvers. Nonetheless, we are happy to include an appendix highlighting additional combinations of models. This will be added to a future revision of the paper.\n\n- Considering that the Venn diagram is important, can you make it more clear? (Current Figure is cut and even the caption can't be seen properly.)\n\n- **Figure 5**.\nIt's still challenging to distinguish between different settings.\nCan you include distinct symbols for distinct lines; e.g.: triangles, circles, rectangles, etc.\n\n- Based on Figure 5, I see that **k-NN still performs better than your technique**; and **only combining your technique with k-NN gives superior results**.\nI do thus think that testing out **combining other techniques with k-NN is crucial to testify about effectiveness of your work**.\nAs if it is the case that combining other technique with k-NN gives same or better results as combining yours with it,\nthen there's no need for the complicated learning approach you take.\n\n- I do believe that running the system in Figure 5 for more time would be useful. It might be the case that in the longer run your approach goes over the curve of k-NN, but it's unclear for how much."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1459/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588961800,
                "cdate": 1700588961800,
                "tmdate": 1700589001508,
                "mdate": 1700589001508,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "W3wKHDjWgs",
                "forum": "XyB4VvF01X",
                "replyto": "pD0pioPb9W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1459/Reviewer_Ursv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1459/Reviewer_Ursv"
                ],
                "content": {
                    "comment": {
                        "value": "> I think the main contribution of this paper is to learn to generate definition embeddings so that the model could handle *never-seen definitions* during test.\n\nThis is a good point. Do you think that the authors have done experiments to **confirm effectiveness of handling never-seen definitions**?\nOr, what experiments would you suggest them to perform to confirm the importance of that feature?\n\nBased on the current results, I am not sure that I see the competitive edge of their approach.\n\nI do see that there are some experiments related to \"never-seen definitions\" in the Appendix.\nBut, considering that this should be the crucial advantage, it should definitely be in the main paper, and well explained.\nAlso, I would expect to see that their approach has a clear edge over k-NN as number of never-seen definitions increases."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1459/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700589383102,
                "cdate": 1700589383102,
                "tmdate": 1700589542220,
                "mdate": 1700589542220,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1zaUFoU7Ne",
                "forum": "XyB4VvF01X",
                "replyto": "jBEMnJmF5R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1459/Reviewer_Ursv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1459/Reviewer_Ursv"
                ],
                "content": {
                    "title": {
                        "value": "Responses provided"
                    },
                    "comment": {
                        "value": "I now included my response to the comment \"Reviewer ePHg\" made, as well as the responses authors provided to my review."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1459/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700589672606,
                "cdate": 1700589672606,
                "tmdate": 1700589672606,
                "mdate": 1700589672606,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YmOJJG5wDl",
                "forum": "XyB4VvF01X",
                "replyto": "MuzXy9Wxkb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1459/Reviewer_Ursv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1459/Reviewer_Ursv"
                ],
                "content": {
                    "comment": {
                        "value": "> To verify the effectiveness of incorporating unseen definitions, one should not compare to the k-NN solver, because both G2T and k-NN are online learners. It is more appropriate to compare to offline learners, such as the transformer and G2T-NoDef-Frozen.\n\n> Both the k-NN solver and the G2T solver incorporate new information on-the-fly as shown in Figure 1. However, the information employed by these models is highly orthogonal:\n\n> The k-NN solver learns from new proof scripts and can exploit unseen user-defined tactics. On the other hand, while G2T learns from proof scripts in an offline setting, it cannot incorporate information from unseen proof scripts into its model on-the-fly. The k-NN does indeed only leverage surface syntax similarity, but in an online setting this is extremely powerful.\nThe G2T solver can build an understanding of new math concepts, even when those concepts have never been seen during training. This understanding can then be exploited to gain a better understanding of the state of a proof and to predict novel arguments for tactics. On the other hand, the k-NN is completely oblivious to math concepts (it does not inspect the body of definitions at all) and is unable to predict novel tactic arguments (it sees tactics and their arguments as a black box).\n\nThanks for clarifications! I believe you should add these statements to the paper. Brief and clear description. Possibly in the related work. Additionally, If there are good references that support these claims, even better."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1459/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699730286,
                "cdate": 1700699730286,
                "tmdate": 1700699730286,
                "mdate": 1700699730286,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tx4Niu52DG",
            "forum": "XyB4VvF01X",
            "replyto": "XyB4VvF01X",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1459/Reviewer_UA5d"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1459/Reviewer_UA5d"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Graph2Tac, a graph neural network-based approach to automating interactive theorem proving, particularly targeting the Coq proof assistant. Graph2Tac constructs a graph representing objects (e.g., local hypotheses) relevant to a proof goal in the Coq environment as well as global definitions. Four types of embeddings will be learned for edge labels, node labels, base tactics, and definitions in the training dataset, respectively.  The dataset is a set of Coq packages extracted from the Opam package manager, which consists of 520K definitions and 4.6 million proof state transformations. Graph2Tac communicates with the Coq theorem prover via _synth_ tactic and _Suggest_ command. The experimental evaluation shows that Graph2Tac achieves comparable performance with the classic k-NN-based machine learning approach and a combination of Graph2Tac and k-NN would outperform each individual approach."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "In general, the contributions of the current work are more about tooling and data collection rather than technical novelties of either theorem proving or machine learning. I can see the following strengths of this work:\n\n- The graph construction systematically considers various information including proof states in the Coq kernel, global definitions, local variables, and equal terms (which are crucial in proofs by construction).\n- The collected dataset represents the real-world applications for certified software\n- Like kNN-based approaches, the tight integration with Coq's Tactician framework as well as solid engineering work make Graph2Tac practical to run on a consumer-grade computer. \n- Multiple baselines have been used in experimental evaluation on a wide selection of ITP tasks."
                },
                "weaknesses": {
                    "value": "The idea of using graph neural networks for automating interactive theorem proving is not new and the improvement is relatively minor, which is the main weakness of this work. More specifically,\n\n- the general idea seems fairly incremental, however, on the other hand, the model architecture is quite complicated, which consists of GNN, several MLPs, argument RNNs, as well as bidirectional LSTM. It is difficult to see the effectiveness of different parts (e.g., are they all essential? Is GNN the most important component?)\n- The title and abstract highlight learning hierarchical representations, however, there is no evidence supporting that hierarchy representations really make a big difference. \n- the improvement of Graph2Tac is not very significant, although combining Graph2Tac and k-NN by running them in parallel does give some interesting improvements, which is more like a simple engineering solution rather than a technical one."
                },
                "questions": {
                    "value": "Can you elaborate a bit more on the objective of the definition task? Shouldn't it be a (separate) pre-training task for learning good representations? \n\nHaving an embedding for each definition might be too expensive. What is the size of the embedding table for definitions? \n\nPremise selection is one important step for theorem proving, which is not discussed in this work. How to guarantee that all relevant theorems/lemmas are properly included when constructing the graph representation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1459/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698865744553,
            "cdate": 1698865744553,
            "tmdate": 1699636074568,
            "mdate": 1699636074568,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6KZ1ABTL8p",
                "forum": "XyB4VvF01X",
                "replyto": "tx4Niu52DG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1459/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1459/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to UA5d"
                    },
                    "comment": {
                        "value": "Thank you for your review.  We plan to improve our writeup to address the important points you made.\n\n**Response regarding incrementality and novelty**\n\nWe believe that our work provides strong novelty in that we are one of the first\nthat learns from the entire hierarchy of mathematical objects, especially in an\nonline setting. Our definition task improves G2T from 17.4% to 26.1% which is a\n1.5x increase in theorems proved.\n\nAs we mention in the common reply to all reviewers,\nthe k-NN solver was actually among state of the art in this field, better than existing works in this field.\nWe are the first paper to systematically compare it to other approaches and point this out.\n\n**Response regarding concerns regarding architectural complexity**\n\nMuch of the complexity is needed for tactic prediction.\nSince we do not use a language model to generate tactics,\nwe need to predict both a base tactic, e.g. `rewrite _ in _`\nand a sequence of arguments to go with that tactic,\nwhich is why we need the RNN (or some other sequence model).\nAs for the bidirectional LSTM,\nit is only used to embed definition names in the G2T-Named model.\nWe indeed show in our ablations that it does not improve results\n(both in the experimental results\nand in Appendix H _Ablation of G2T solvers and symmetry breaking with names_).\n\n**Response regarding lack of evidence for effectiveness of hierarchical representations**\n\nWe didn't have an opportunity to explore in great detail the importance\nof dependency depth in our hierarchical representations,\nand we agree that would be interesting follow-up work.\nNonetheless, our ablations show that the hierarchical definition embedding task improves the G2T model from 17.4% to 26.1%,\nhighlighting the power of calculating new definitions embeddings from their definitions.\nFurther, in Figure 15, Appendix L _Impact of the Online Setting_,\nwe show that using the definition task doubles the pass rate if there are at least 10 new\ndependencies in the global context.\nThis advantage increases as the number of unseen dependencies increases.\nIf your comment pertains to the fact that we are using learned embeddings for\ndefinitions seen during training and computed hierarchical embeddings for definitions\nnot seen during training,\nnote that the learned definitions are trained to resemble the computed hierarchical embeddings\n(and vice versa).\nAdditionally, we perform experiments\nin Appendix H _Ablation of G2T solvers and symmetry breaking with names_\nwhere for inference we recompute all embeddings to use the computed hierarchical embeddings."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1459/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700081704407,
                "cdate": 1700081704407,
                "tmdate": 1700081704407,
                "mdate": 1700081704407,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bRYo3xIUaD",
                "forum": "XyB4VvF01X",
                "replyto": "tx4Niu52DG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1459/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1459/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answers to questions"
                    },
                    "comment": {
                        "value": "> Can you elaborate a bit more on the objective of the definition task?\n> Shouldn't it be a (separate) pre-training task for learning good representations?\n\nThe definition task serves the purpose of aligning the learned definitions during training to the calculated definitions.\nThis way, during inference, if we encounter a new definition we can compute its embedding.\nAs for pretraining, would could have alternately learned our definition embedding computation as a pre-training task\nvia contrastive or non-contrastive self-supervised methods (like SimCLR, BYOL, VICReg, etc.).\nHowever, by training them along with the final tactic prediction task it gives us a few advantages:\n1. It lets us use learned embeddings when available and only swap to calculated embeddings for new definitions.\n2. It ensures our calculated embedding is useful for the downstream task.\n3. It prevents mode collapse where all embeddings become the same (since the embeddings have to be useful for the downstream task).\n4. It allows us to share the weights of our GNN backbone on both the definition task and prediction task.\n5. Since definitions are hierarchical (one definition depends on many previous ones),\n   we need to store (or constantly recompute) embeddings of all intermediate definitions.\n   (This is unique to our setting and not found in most other self-supervised learning papers.)\n   The learned definition embedding table serves as a way to store and update intermediate embeddings.\n\n> Having an embedding for each definition might be too expensive.\n> What is the size of the embedding table for definitions?\n\nOur table is indeed large, with about 459k entries (about an order of magnitude larger than the vocabulary table in small language model).\nHowever, during argument selection we mask out all definition embeddings not in the current global context, significantly reducing the table size. Empirically, our approach is both fast and effective.\nIn Appendix H _Ablation of G2T solvers and symmetry breaking with names_,\nwe do show that if we used the G2T-Named model,\nwe could replace all the learned embeddings with calculated embeddings,\nreducing the size of our model.\nOther approaches in the literature based on premise selection also must compute embeddings for every theorem,\neither beforehand or on-the-fly,\nand the HOList models also learned embeddings for every definition.\n\n> Premise selection is one important step for theorem proving, which is not discussed in this work.\n> How to guarantee that all relevant theorems/lemmas are properly included when constructing the graph representation?\n\nThere might be a misunderstanding we will try to clarify.\nWe don't put background definitions in the graph passed to the model.\nThere are two types of graphs we deal with.\nThe large monograph contains the entire available mathematical universe, including all definitions (and theorems).\nDuring training it contains all definitions in our training data.\nDuring inference, it contains all definitions in Coq's current global context and is how data is communicated between Coq and the model.\nThis monograph is too large for our GNN models.\nWe instead break it up into its subgraphs (the different colors in Figure 2).\nOur definition task uses only the subgraph for that particular definition cluster and no other.\nThe prediction task uses the subgraph for that proof state and no other definitions.\n\nWe don't need to do any preselection of premises.\nOur models select arguments from all definitions (including lemmas) in the global context.\nThis argument selection is a form of premise selection predicated on the current proof state and the chosen base tactic.\nIt is accomplished via an attention-like mechanism in step G of Figure 4.\n(Note: We will fix Figure 4 to say \"definition embeddings\" instead of \"constant embeddings\" to be more clear.)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1459/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700081725795,
                "cdate": 1700081725795,
                "tmdate": 1700081725795,
                "mdate": 1700081725795,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5RjFmfBV8R",
                "forum": "XyB4VvF01X",
                "replyto": "W3wKHDjWgs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1459/Reviewer_UA5d"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1459/Reviewer_UA5d"
                ],
                "content": {
                    "comment": {
                        "value": "I share a similar concern. If the learned embedding really captures something interesting, it ought to outperform k-NNs, as the latter simply leverages surface syntax similarity. Another relevant concern I have is that the emphasis on \"hierarchical\" representation by authors is not backed up by evidence (i.e., no studies about hierarchical representations vs non-hierarchical representations). The so-called hierarchical representation in this work is essentially preprocessing with good premise selections by manually designed heuristics."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1459/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700604448023,
                "cdate": 1700604448023,
                "tmdate": 1700604448023,
                "mdate": 1700604448023,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ianJCa7xA3",
            "forum": "XyB4VvF01X",
            "replyto": "XyB4VvF01X",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1459/Reviewer_ePHg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1459/Reviewer_ePHg"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Graph2Tac, a novel neural-based theorem proving method that can better understand the new definitions defined in the test packages. Besides generating the next proof steps, Graph2Tac is also optimized to generate the embeddings of definitions so that for this definition the predicted embeddings are similar to the learned embeddings. This new training objective equip Graph2Tac the ability to embed the new definitions not seen during training. Graph2Tac is compared with baselines on a collection of Coq packages that are not presented during training. Compared to the Graph2Tac model trained for predicting the next steps only, the new definition prediction training could improve the performance on test set from 17.4% to 26.1%. Combined with a KNN baseline, Graph2Tac+KNN achieve the new start of the art results on the test benchmark."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1 Define new concepts/theorems/methods is a fundamental feature of mathematics. This paper captures this key feature and works on an important and interesting online theorem proving settings where the theorem to prove contain unseen definitions presented in training. \n2 The proposed method is technically sound. By learning to predict the definition embeddings, the model could embed the novel definitions during inference. The design of GNN architecture is also reasonable.\n3 Experiments demonstrate the benefits of training to generate definition embeddings."
                },
                "weaknesses": {
                    "value": "Figure 4 is hard to follow for a new reader. It may help to remove some less important steps and intermediate results to make the figure more clear and helpful."
                },
                "questions": {
                    "value": "1 It is interesting to see if training to predict definition embeddings could be helpful to in general. Like for the test theorems without unseen definitions, or a Coq package that is largely covered by the training data, could the new training loss improve the results?\n2 As mentioned in the paper, this new training objective utilize the hierarchical structure of definitions. It is interesting to see if the learning definition embeddings contain any implicit structures compared to training to generate the next steps only. For example, try to plot the definition embeddings using t-sne."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concern."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1459/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699040498077,
            "cdate": 1699040498077,
            "tmdate": 1699636074500,
            "mdate": 1699636074500,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Wg56WfwD1i",
                "forum": "XyB4VvF01X",
                "replyto": "ianJCa7xA3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1459/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1459/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to ePHg"
                    },
                    "comment": {
                        "value": "Thank you for you thoughtful review.\n\nRegarding Figure 4, we will work on cleaning it up as well as unifying it better with the surrounding text.\nWe agree it is complex, but we also have difficultly seeing what can be removed without adding more confusion.\nIs there anything in particular you would suggest changing?\n\n> It is interesting to see if training to predict definition embeddings could be helpful to in general.\n> Like for the test theorems without unseen definitions, or a Coq package that is largely covered by the training data, could the new training loss improve the results?\n\nIn Appendix L _Impact of the Online Setting_ we indeed look at exactly this.\nIn Figure 15 you can see that even when there are no new dependencies,\nour definition task still gives a modest bump to our G2T solver,\nsuggesting our definition task also acts as an auxillary training task.\n\n>  As mentioned in the paper, this new training objective utilize the hierarchical structure of definitions.\n> It is interesting to see if the learning definition embeddings contain any implicit structures compared to training to generate the next steps only. For example, try to plot the definition embeddings using t-sne.\n\nThank you for the suggestion.\nWe are currently doing an analysis with UMAP (similar to t-SNE) embeddings.\nWe will include it in the paper when complete.\nPreliminary findings show that there are significant differences in the embeddings.\nThe embeddings using the definition task are more structured and interpretable."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1459/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700081652344,
                "cdate": 1700081652344,
                "tmdate": 1700081652344,
                "mdate": 1700081652344,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LOVgR5WLvw",
                "forum": "XyB4VvF01X",
                "replyto": "pD0pioPb9W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1459/Reviewer_ePHg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1459/Reviewer_ePHg"
                ],
                "content": {
                    "title": {
                        "value": "Response to other reviewers' comments."
                    },
                    "comment": {
                        "value": "I agree with other reviewers that the proposed approach doesn't bring significant improvement compared with the kNN baseline. However, I think the main contribution of this paper is to learn to generate definition embeddings so that the model could handle never-seen definitions during test. As I mentioned in my review, I consider this as an important characteristic of mathematics but missing in existing neural-based provers. The method proposed in this paper seems to be simple and reasonable to solve this problem, and performs a lot better than the baseline using randomly initialized definition embeddings (26.1% vs 17.4%). Therefore, I consider this paper has made decent contributions."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1459/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576606829,
                "cdate": 1700576606829,
                "tmdate": 1700576606829,
                "mdate": 1700576606829,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]