[
    {
        "title": "Probabilistic Self-supervised Representation Learning via Scoring Rules Minimization"
    },
    {
        "review": {
            "id": "74EeuILyQJ",
            "forum": "skcTCdJz0f",
            "replyto": "skcTCdJz0f",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5704/Reviewer_JrU7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5704/Reviewer_JrU7"
            ],
            "content": {
                "summary": {
                    "value": "This paper propose ProSMin, a probabilistic self-supervised learning approach that can mitigate the problem of  collapsing representations in self-supervised learning. It follows the basic framework of training representations on augmented views of give images, like in contrastive learning. The core components are 1) an online network that predicts the representation as a distribution rather than a deterministic vector; 2) a target network that can be seen as an mean teacher.  Learning is accomplished based on proper scoring rules."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.  Give a new probabilistic modeling for self-supervised representation learning. Compared with the deterministic modeling, the probabilistic modeling effectively mitigates collapsing representations.\n2. The convergence of the proposed method is theoretically proved. The theooritical justification brings new insights on how representation quality is effectively improve\n3. Solid experiments results on in-domain case and out-of-domain case show the proposed method achieves better represnetation, and the learned scores are well calibrated."
                },
                "weaknesses": {
                    "value": "This paper reads smooth and everything looks good to me except for one concern: The authors are trying to sell that the proposed methods avoid the collapsing problem in self-supervised representation learning, but I didn't see explict justification or evaluation on this point. \nI have no idea that representations learned with previous method, such as contrastive learning, collapsed to what extend, and no idea on how good the proposed method improved on this. Maybe the performance on in-domain and out-of-domain expereiments show the representation is better, but I cannot justify if or not this were because the collapse problem is mitigated. \nI believe this work would be definitely more technically sound If authors provide more qualititive or quantatitive results that can directly reflect the level of collapse,"
                },
                "questions": {
                    "value": "Please see [Weakness]"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5704/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698461910724,
            "cdate": 1698461910724,
            "tmdate": 1699636596886,
            "mdate": 1699636596886,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FB3ahDsiLO",
                "forum": "skcTCdJz0f",
                "replyto": "74EeuILyQJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5704/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5704/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5704/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5704/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5704/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5704/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5704/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JrU7"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive acknowledgment and valuable comments. We refer to General Response 1 for the impact of our proposed method on mitigating collapsing representation."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5704/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700426736601,
                "cdate": 1700426736601,
                "tmdate": 1700426736601,
                "mdate": 1700426736601,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hPIZ1HQJwq",
                "forum": "skcTCdJz0f",
                "replyto": "74EeuILyQJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5704/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5704/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5704/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5704/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5704/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5704/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5704/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up"
                    },
                    "comment": {
                        "value": "Dear Reviewer JrU7,\n\nWe sincerely appreciate your valuable time and effort spent reviewing our manuscript. As the deadline for the discussion nears, we would like to ask you to participate. We just wonder whether there is any further concern and hope to have a chance to respond before the discussion phase ends.\n\nMany thanks, Authors"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5704/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506211708,
                "cdate": 1700506211708,
                "tmdate": 1700506569451,
                "mdate": 1700506569451,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "M8dWxyJNVq",
                "forum": "skcTCdJz0f",
                "replyto": "hPIZ1HQJwq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5704/Reviewer_JrU7"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5704/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5704/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5704/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5704/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5704/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5704/Reviewer_JrU7"
                ],
                "content": {
                    "title": {
                        "value": "Response to author rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the rebuttal, I understand that various of experiments on multiple tasks, such as in-domain generalization, low-shot learning, semi-supervised learning, transfer learning, and OOD detection, show that ProSMin is superior to previous SSL representations. And the results with $\\lambda=0, \\sigma=0$ address some of my concerns. Therefore I remain my possitive rating.\n\nHowever, it would be better if we can see more intuitive justification on the improvement for the  mode collapse problem. Maybe some visualizations?"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5704/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636757558,
                "cdate": 1700636757558,
                "tmdate": 1700636757558,
                "mdate": 1700636757558,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kmABznx4Dg",
            "forum": "skcTCdJz0f",
            "replyto": "skcTCdJz0f",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5704/Reviewer_pLHM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5704/Reviewer_pLHM"
            ],
            "content": {
                "summary": {
                    "value": "The article focuses on the dimensional collapse problem in SSL. To address this issue, the authors propose a probabilistic approach via self-distillation to build robust representations. Detailed proofs confirm the convergence of the proposed method. The experimental results demonstrate the effectiveness of the method across various scenarios, including in-distribution, out-of-distribution, transfer learning, and more."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper proposes to learn robust feature representations through a probabilistic approach.\n2. The theoretical proofs presented in the article, along with the explanations using scoring rules, demonstrate the convergence of the proposed algorithm. \n3. The authors conduct experiments in multiple settings, and the experimental results validate the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1. There are some typos in the article, affecting the overall readability of the paper. \n- In Table 3, should \"PN\" be replaced with \"PL\"? The experimental setups in the first and last rows of Table 3 are exactly the same. Is this an error?\n- In the last sentence of the third paragraph in Chapter 4, should \"$\\mu $\" and \"$\\sigma $\" have the superscript \"i\"? As per my understanding, \"$\\mu $\" and \"$\\sigma $\" vary for each data point.\n- In the eighth line of the abstract, there is a semicolon. I believe using a comma would be more appropriate.\n\n2. The article uses a self-distillation mechanism for learning, but the description of the learning mechanism is not clear. The third paragraph of Chapter 5 contains a sentence that says, \"The target network has the same backbone and projection as the online network and it learns through self-distillation\". The target network is updated through the strategy of EMA, and I think the online network could be regarded as learning through self-distillation. Is there a mistake here, or is my understanding off? In addition, there are some mistakes in the descriptions of the distillation mechanism of DINO in the second paragraph of Chapter 2. The parameters of the teacher model, rather than the student model, are obtained through EMA.\n\n3. The association between the probabilistic method and the prevention of dimensional collapse still requires further elaboration and demonstration. In the second paragraph of Chapter 1, the authors give two possible causes for the dimensional collapse. Is the author's solution inspired by either of the two causes or did it address either of the two factors? I think this needs to be further explained. I suggest two additional experiments in the following areas:\n- Ablation experiments with the removal of \"$\\sigma $\" and the second term Equation 2. I think it is necessary to illustrate the effectiveness of the probabilistic approach.\n- Performance comparison with various baseline methods for the same feature dimensions. \n- In Figure 3d, the dimension of the largest embedding vector is 16000, which I think is not large enough and should be further increased for experiments. As far as I know, many self-supervised methods, such as Barlow Twins, are capable of not collapsing completely in a dimension of 16000. I suggest that the authors can show that their proposed method does not collapse completely in larger dimensions. It would be better if the authors could somehow demonstrate that their method improves the effective dimensions of the learned features."
                },
                "questions": {
                    "value": "1. The network structure of the proposed method is identical to BYOL. What do you think is the most important difference between these two approaches?\n2. The term ${S^2}$ in Equation 4, Equation 5 and Equation 6 is only related to $\\theta $. Why do you define the energy score as ${S^2}({P_\\theta },{z_\\xi })$?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5704/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698653465963,
            "cdate": 1698653465963,
            "tmdate": 1699636596774,
            "mdate": 1699636596774,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "640jzuDuKM",
                "forum": "skcTCdJz0f",
                "replyto": "kmABznx4Dg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5704/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5704/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5704/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5704/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5704/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5704/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5704/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pLHM"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments and suggested experiments that highlighted our contributions. Please refer to General Responses 1 and 2 for W3.1 and W3.3 respectively.  \n\n**Q1.** ... The most important difference between BYOL and ProSMin\n\n>* We employ a similar architecture to DINO (ViT-S) for both the encoder and the projection head, incorporating three linear layers with Gelu activation. Notably, our projection head does not include batch normalization. We then add a prediction layer similar to BYOL. Even without a batch normalization layer, we use a higher representation of 16k compared to 4096 for BYOL. However, we have shown that the performance does not suffer drastically at lower dimensions. Then, we have two layers for mean value and standard deviation. Our loss function is the proper scoring rule loss based on energy score L1 loss, and BYOL uses MSE loss. So we use a prediction layer like BYOL in terms of architecture, but we don't have batch normalization and use a different encoder and projection head, and we use another loss function based on a probabilistic approach. So, our method is different from BYOL, and we also showed that batch normalization and the prediction layer are not necessary components to prevent the collapse scenario, unlike BYOL.\n\n\n\n**Q2., W1., W2.** Suggested improvements and typos \n\n>* We thank the reviewer for pointing out the typos and we have updated our manuscript and incorporated these suggestions.\n\n**W3.** ...Is the author's solution inspired by either of the two causes or did it address either of the two factors? \n>* Thank you for asking this question, we address collapsing representation without overparametrization and heavy data augmentation instead we push the boundaries of self-supervised learning by embracing the richness of probabilistic models. By adopting a probabilistic framework, our proposed method encourages the neural network to assimilate a comprehensive distribution of representations, rather than being constrained to a singular, fixed representation.\n\n**W3.2.** Performance comparison with various baseline methods for the same feature dimensions.\n\n>* We have the experiments for different embedding sizes in Figure 3-d for 100 epochs, and we also add a new result with the same embedding size as DINO.  We summarize the knn results in the following Table:\n| Methods | embedding in the loss |knn|\n|----------:|------:| -----:|\n| iBOT | 8192 | 71.4|\n| Our method  | 8000 |71.2|\n| DINO | 65536 | 69.7|\n| Our method | 65536 |69.9|"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5704/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700429087841,
                "cdate": 1700429087841,
                "tmdate": 1700429087841,
                "mdate": 1700429087841,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w9muPRvTLl",
                "forum": "skcTCdJz0f",
                "replyto": "kmABznx4Dg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5704/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5704/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5704/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5704/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5704/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5704/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5704/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up"
                    },
                    "comment": {
                        "value": "Dear Reviewer pLHM,\n\nWe sincerely appreciate your valuable time and effort spent reviewing our manuscript. As the deadline for the discussion nears, we would like to ask you to participate. We just wonder whether there is any further concern and hope to have a chance to respond before the discussion phase ends.\n\nMany thanks, Authors"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5704/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506142897,
                "cdate": 1700506142897,
                "tmdate": 1700506553474,
                "mdate": 1700506553474,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TKfg7TkeYX",
                "forum": "skcTCdJz0f",
                "replyto": "640jzuDuKM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5704/Reviewer_pLHM"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5704/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5704/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5704/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5704/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5704/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5704/Reviewer_pLHM"
                ],
                "content": {
                    "title": {
                        "value": "Response to author rebuttal"
                    },
                    "comment": {
                        "value": "The author has addressed some of my concerns. Some important experimental results are not provided. So, I keep my rating."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5704/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645685669,
                "cdate": 1700645685669,
                "tmdate": 1700645685669,
                "mdate": 1700645685669,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JNcoYiL7jQ",
            "forum": "skcTCdJz0f",
            "replyto": "skcTCdJz0f",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5704/Reviewer_QnX8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5704/Reviewer_QnX8"
            ],
            "content": {
                "summary": {
                    "value": "The paper approaches the problem of learning self-supervised representation learning as a parametric probability density estimation problem in the representation space. The authors utilize an encoder to estimate the parameters of the distribution of the encoded sample similar to Kingma et al. They utilize self-distillation similar to BYOL and DINO where the distillation loss is a proper scoring rule for distributions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors present a novel perspective toward solving the problem of learning self-supervised representations as a distribution over the representation space.\nThey achieve state-of-the-art performance on imagenet-1K for linear evaluation and K-nearest neighbour search."
                },
                "weaknesses": {
                    "value": "See Questions"
                },
                "questions": {
                    "value": "1. How do you ensure $q_\\theta$ is not an identity network w.r.t $\\mu$ with $\\sigma = 0$ since the dimensionality of $z_\\theta$ and $t_\\theta$ \n2. Since your scoring function is a variation of $L_2$ loss between $z_i$ and $z_{\\xi}$, can you show that the variational encoding actually makes a difference by perturbing $t_\\theta$ by a Gaussian noise of $\\sigma = \\epsilon$\n3. The authors claim that their method is superior in terms of preventing the collapse of representation, can you provide an experiment specific to this to highlight that the representations learned by ProSMin mitigate this problem? This is important because the authors claim that strong augmentations distort the underlying distribution, hence promoting the collapse of the representations learned. If learning the representations via a parametric distribution that utilizes the resampling trick of sampling from a Gaussian distribution, is it not effectively an augmentation in the representation space?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5704/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5704/Reviewer_QnX8",
                        "ICLR.cc/2024/Conference/Submission5704/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5704/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698732051896,
            "cdate": 1698732051896,
            "tmdate": 1700579318881,
            "mdate": 1700579318881,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IzRjqeb8da",
                "forum": "skcTCdJz0f",
                "replyto": "JNcoYiL7jQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5704/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5704/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5704/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5704/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5704/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5704/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5704/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QnX8"
                    },
                    "comment": {
                        "value": "We thank reviewer QnX8 for the useful comments and suggested improvements. Please refer to General Response 1 for the first part of Q3. \n\n**Q1.** How do you ensure $q_{\\theta}$ is not an identity network w.r.t $\\mu$  with $\\sigma$ = 0 since the dimensionality of  $z_{\\theta}$ and $t_{\\theta}$ \n\n>* Thanks for pointing out this issue. To avoid this case, the $q_{\\theta}$ contains two linear layers with GELU as nonlinearity and uses ReLU nonlinearity between $q_{\\theta}$ and $\\mu$, so $q_{\\theta}$ is a nonlinear function between $t_{\\theta}$ and $\\mu$ in case $\\sigma$ is zero.\n\n**Q2.** Since your scoring function is a variation of L2 loss between $z_i$  and $z_{\\xi}$, can you show that the variational encoding actually makes a difference by perturbing $t_{\\theta}$ by a Gaussian noise of ${\\sigma}= \\epsilon$\n\n>* The hyperparameter ${\\lambda}$ is the determining factor for the effect of the perturbation. Thus, a lower value of ${\\lambda}$ shows a more significant effect on the second-term portion of the loss, which is dominated by the injected Gaussian noise. Figure 3-a shows the effect of ${\\lambda}$, lower ${\\lambda}$ shows better performance compared to a higher value of ${\\lambda}$. This means that injecting Gaussian noise improves the performance of the model. However, we adjust ${\\lambda}$ to keep the loss function positive, which is essential for the proper scoring rule. Therefore, there is a limit to the lower value of ${\\lambda}$, depending on the case. \n\n**Q3.2.** ...This is important because the authors claim that strong augmentations distort the underlying distribution, hence promoting the collapse of the representations learned.\n\n>* The effect of augmentation on collapsing representation has been discussed extensively for contrastive learning by Jing et al.[1].\n\nReference:\n[1] Jing, L., Vincent, P., LeCun, Y., & Tian, Y. (2022). Understanding dimensional collapse in contrastive self-supervised learning. ICLR."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5704/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700427408492,
                "cdate": 1700427408492,
                "tmdate": 1700427408492,
                "mdate": 1700427408492,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SIQ1udBpvp",
                "forum": "skcTCdJz0f",
                "replyto": "JNcoYiL7jQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5704/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Submission5704/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5704/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5704/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5704/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5704/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5704/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up"
                    },
                    "comment": {
                        "value": "Dear Reviewer QnX8,\n\nWe sincerely appreciate your valuable time and effort spent reviewing our manuscript. As the deadline for the discussion nears, we would like to ask you to participate. We just wonder whether there is any further concern and hope to have a chance to respond before the discussion phase ends.\n\nMany thanks, Authors"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5704/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506049597,
                "cdate": 1700506049597,
                "tmdate": 1700506534155,
                "mdate": 1700506534155,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jTyHY3z39v",
                "forum": "skcTCdJz0f",
                "replyto": "IzRjqeb8da",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5704/Reviewer_QnX8"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5704/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5704/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5704/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5704/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5704/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5704/Reviewer_QnX8"
                ],
                "content": {
                    "title": {
                        "value": "Response to author rebuttal"
                    },
                    "comment": {
                        "value": "The authors have answered questions 1, 3.2 to my satisfaction and hence I have increased my score to 6. \nAlso kindly fix the font sizes in all the figures."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5704/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579838993,
                "cdate": 1700579838993,
                "tmdate": 1700579838993,
                "mdate": 1700579838993,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]