[
    {
        "title": "LILO: Learning Interpretable Libraries by Compressing and Documenting Code"
    },
    {
        "review": {
            "id": "a4xv5lXi0w",
            "forum": "TqYbAWKMIe",
            "replyto": "TqYbAWKMIe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3844/Reviewer_dfJc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3844/Reviewer_dfJc"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a neuro-symbolic framework for learning \"code libraries\" for lambda-calculus code.\nThe idea is to iteratively synthesize code using an LLM, compress it using an existing approach (\"Stitch\") and then document it using an LLM.\nBy repeating this approach multiple times, the overall framework learns repetitive \"libraries\" that can be further useful in the next iteration.\n\nThe overall framework, called Lilo, is evaluated on three tasks: CLEVR (visual scene reasoning), REGEX (string editing with regular expressions), and LOGO (graphics composition in the 2D Logo graphics language), and shows improvements over a vanilla LLM and over DreamCoder (Ellis et al., 2021)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea is interesting, combining both LLMs and traditional program synthesize.\n2. It is refreshing to see tasks and approaches in which using vanilla LLMs still works much worse than a clever approach."
                },
                "weaknesses": {
                    "value": "1. The tasks are a quite contrived, and it feels like the problem definition is tailored exactly to this kind of solution. All the tasks are synthetic and based on lambda calculus. What practical problems can the proposed approach be helpful for? Who is expected to use the proposed approach? \n2. Evaluation - I am not sure that it is fair to compare that proposed approach to a *prompted* LLM as a baseline when this baseline is prompted with the lambda-calculus language, which it probably was not trained on. Since the task is defined in a domain-specific-language (DSL), a prompted LLM is expected to fail. Can the task be \"translated\" to Python? If the LLM would be allowed to use Python, that would be a more fair comparison. Alternatively, a more fair baseline could be fine-tuning an LLM on the language, rather than just prompting, since the proposed approach also does kind-of training within its compression step. \n3. Novelty - I am not familiar with the directly related work, and I am thus not sure about the novelty: the paper claims that the framework consists of three modules: \n    1. A synthesis module - which is a prompted LLM\n    2. A compression module (\"Stitch\"), which is adopted as is from [Bowers et al., 2023](https://mlb2251.github.io/stitch_jul11.pdf).\n    3. An \"auto-documentation module\" which uses a prompted LLM to document the resulting code pieces, for better interpretability and improvement of the next iteration.\n\n   It seems like steps (1) and (3) are standard code generation using LLM prompting, and the heavy lifting is done by the 2nd step. Since the 2nd step is identical to Bowers et al. (2023), I am not sure how novel is this paper compared to Bowers et al. (2023)."
                },
                "questions": {
                    "value": "### Questions\n\n1. What practical problems can the proposed approach be helpful for? Who is expected to use the proposed approach? \n2. Can the task be \"translated\" to Python, and then tested on an LLM?\n3. Can an LLM such as LLaMA be finetuned on the training examples, to better understand their language? Prompting an LLM with a language it was not trained on is a bit unfair.\n\n### Comments\n1. Regarding the AutoDoc module - more than *having* an auto-documentation module, the interesting part to me is that this generated documentation improve the next iteration, when fed to the LLM. That is, the self-improvement part (the fact that the LLM generated doc improves the LLM itself in the next iteration) is the interesting from an ML perspective, to me, while the paper mostly emphasizes the \"existence\" of this module to generate documentation, which is not significantly novel or interesting on its own.\n\n### Summary\nThe main weaknesses of the paper are its lack of practicality, the tasks are quite synthetic, and I believe that an LLM-based baseline can perform better than presented, if given fair chances.\nNonetheless, the paper proposes a simple approach (simpler than DreamCoder, which is good), it is well written, easy to follow, and the ideas are conceptually pleasing, and I thus vote for acceptance."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3844/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697721972164,
            "cdate": 1697721972164,
            "tmdate": 1699636342313,
            "mdate": 1699636342313,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ocFo9adThq",
                "forum": "TqYbAWKMIe",
                "replyto": "a4xv5lXi0w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3844/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3844/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your positive review!"
                    },
                    "comment": {
                        "value": "We are glad that you found the ideas in the paper to be \u201cinteresting\u201d and \u201cconceptually pleasing\u201d and that you found the overall paper to be \u201crefreshing\u201d and \u201cwell-written.\u201d Thank you for your vote for acceptance.\n\nPlease allow us to address the points that you raised below:\n\n**The tasks are a quite contrived, and it feels like the problem definition is tailored exactly to this kind of solution. All the tasks are synthetic and based on lambda calculus.**\n\nIt is true that some program synthesis tasks have the synthetic quality that you are describing (as Reviewer NLqL points out, this is \u201ca common limitation of program synthesis models/systems\u201d). However, we don\u2019t feel it is accurate to say that these tasks have somehow been tailored to our solution\u2014**we\u2019ve selected a canonical set of benchmarks that have been studied extensively in prior program synthesis work** [1, 2]. This affords the ability compare performance against prior results (see Appendix B.3) to a degree that we would not have been able to had we constructed an original benchmark. Evaluating on a novel benchmark would also have been more likely to raise concerns about \u201ctailoring\u201d of the kind described here.\n\n**What practical problems can the proposed approach be helpful for? Who is expected to use the proposed approach?**\n\nFirst, we want to start by addressing a possible misperception that systems based on DSLs of the kind we study here lack real-world application. Ideas from this line of inductive program synthesis research have resulted in many high-profile applications over the years, including:\n\n- **Tabular autocompletion** [5], which has powered Microsoft\u2019s billion dollar Excel product line since 2013 and continues to do so a decade later [6]. The REGEX domain in our work is a restricted version of the DSL used for systems like FlashFill.\n- **Scientific data analysis**, which has been applied to analyze diverse phenomena ranging from large scale video datasets of social behavior in laboratory mice [7, 8] to protein binding and activity prediction for RNA splicing [9]. Our CLEVR domain, which requires reasoning about spatial relationships between objects in scenes, presents many of the same challenges needed for analyzing this kind of graph-structured data.\n- **Long-horizon robotics tasks** of the kind that are performed by industrial/warehouse robots [10]. Our LOGO domain, which requires synthesizing programs to instruct a \u201cturtle\u201d how to move on a 2D surface by specifying angles and velocities, can be viewed as a version of the kind of planning problems involved in robotics domains.\n\nThese recent advances also a key part of the motivation for the neurosymbolic & hybrid AI systems track at ICLR this year. Situated in this context, the ideas and methods we introduce in LILO should have broad appeal, not just for this audience, but for the ML community in general. In particular, we believe that LILO\u2019s general design pattern around synthesis, refactoring, and documentation has relevance to practical software engineering tools like Copilot that are used by millions of programmers every day.\n\nPractically speaking, our code implementation (withheld for anonymity) has already garnered attention from the GitHub community and students at multiple universities are presently using LILO as the basis for research projects. Concretely, we put significant effort into making the LILO codebase beginner-friendly through documentation and by providing (for the first time in this line of work) a Dockerized development environment. Lastly, we strongly resonate with your assessment that LILO is **\u201csimpler than DreamCoder, which is good.\u201d**"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3844/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700093906467,
                "cdate": 1700093906467,
                "tmdate": 1700505209566,
                "mdate": 1700505209566,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w0Y2U8Kt9v",
                "forum": "TqYbAWKMIe",
                "replyto": "SwS4njiMxQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3844/Reviewer_dfJc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3844/Reviewer_dfJc"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you for your response.\n\nI am still concerned about the applicability of these tasks. Excel's FlashFill has been used by the program synthesis community to justify these kinds of tasks over a decade, while machine learning research had gone through groundbreaking breakthroughs in the meantime.\nIn other words, I think that these tasks are defined in such a specific and unnatural way to allow new approaches to make progress, but they have lost connection to the real world long ago.\n\nHowever I agree that these benchmarks are canonical and have been studied extensively in prior program synthesis work, and the authors should not be penalized for it.\n\nRegarding the potential translation to Python:\n>Can the tasks be translated into Python?\n\n>Yes\u2014and this is something that we considered early in the project. In practice, however, we found that syntax errors with Codex were rare (e.g., virtually no issues with parenthesis matching). Given that the LLM Solver already showed strong performance on the online synthesis experiments when working in the original syntax, we opted not to introduce additional system complexity of translating the programs into a Python-like syntax. Irrespective of the syntax, translation would not address the fact that the main difficulty for LLM-guided synthesis arises from the novel semantics of the domains and the novel abstractions introduced by compression. Accordingly, our emphasis in this work is on methods to address issues with semantic comprehension (i.e., via AutoDoc) and not syntactic comprehension.\n\nI don't understand why the fact that syntax errors with Codex are rare matters? I would argue that even if Codex can generate syntactically valid DSL, it would still perform much better semantically if the input was Python.\nThe benefit of models such as Codex in processing Python-translated inputs goes far beyond syntax only.\n\nI am left with the conclusion that in the way these benchmarks are currently defined - the approach is useful, and thus the paper should be accepted.\nHowever, if anyone really wanted to solve the corresponding real-world problems, they could just translate the problems to Python, and the marginal benefit of approaches such as Lilo and DreamCoder would be minimal."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3844/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587279331,
                "cdate": 1700587279331,
                "tmdate": 1700587279331,
                "mdate": 1700587279331,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AA7sPBZ3tJ",
            "forum": "TqYbAWKMIe",
            "replyto": "TqYbAWKMIe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3844/Reviewer_NLqL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3844/Reviewer_NLqL"
            ],
            "content": {
                "summary": {
                    "value": "The paper presented a dual strategy system utilizing LLM and symbolic systems for program synthesis. Unlike conventional programming synthesis models, which implicitly learn the common program fragment (library), the proposed LILO explicitly generates a library with human-readable function names. The code generator is informed with the library and is able to generate potentially more concise program. \n\nThe experimental results show its improvement against the straightforward LLM + search approach and the LLM free baseline, DreamCoder."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The work demonstrated a successful combination of LLM and symbolic tools to efficiently generate a library and set of programs given a DSL and program synthesis tasks."
                },
                "weaknesses": {
                    "value": "As the author also mentioned, the pipeline needs an additional self-verification step for the auto-documentation. The validity and conciseness of the language-guided program synthesis is enforced by input-output pairs and the stitch compression, while the library auto-doc has no verification or feedback loop to update the LLM or other trainable part of the pipeline. \n\nThe application of the method is limited in functional programming with relatively simple DSL. Though it's almost a common limitation of program synthesis models/systems."
                },
                "questions": {
                    "value": "1. Could the author elaborate on algorithm 1 and Dual-system program search? The relationship between the enumerate search and LLM-guided search is quite unclear. Are they parallel? cascaded? or combined in another manner?\n\n2. What does PCFG stand for? Could the author describe the input-output, architecture, and training of this multi-layer perceptron? Is this model pre-trained? DSL-specific? \n\n3. In the Table 1, only in LOGO, the LILO method significantly outperforms the LLM solver + search. Could the author explain the performance difference among the different DSLs? If the reason is that LLM is not pre-trained on only LOGO, it could be an interesting and promising observation about the generalization capability of both LLM and LILO."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3844/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698100166916,
            "cdate": 1698100166916,
            "tmdate": 1699636342222,
            "mdate": 1699636342222,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RvkHUNgGT8",
                "forum": "TqYbAWKMIe",
                "replyto": "AA7sPBZ3tJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3844/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3844/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your positive review!"
                    },
                    "comment": {
                        "value": "We are glad that you found this work to demonstrate a \u201csuccessful combination of LLM and symbolic tools\u201d and appreciate the strong rating for acceptance.\n\nThank you for the valuable feedback! **Based on your questions, we\u2019ve updated several areas of Section 3 to clarify details about the search module.** We hope these changes\u2014and the clarifications below\u2014will address any issues you encountered in the \u201cPresentation\u201d category.\n\n# Response to questions\n\n**In the dual-system program search module, what is the relationship between the enumerative search and LLM-guided search?**\n\nGood question! It\u2019s a straightforward serial implementation: first, we run LLM-guided search as a fast \u201cSystem 1\u201d first pass (Alg. 1, lines 5-6) and then run enumerative search as a slow \u201cSystem 2\u201d process to try to solve tasks that were not solved by the LLM. We agree that this is not as clear as it could be; **we will update Sec. 3 of our submission to emphasize that LLM-guided search and enumerative search are separate procedures.**\n\nVarious approaches have been proposed for combining neural and symbolic search methods (e.g., [1] for a review), but most of this work is from the pre-LLM era. Smarter ways of integrating LLMs and enumeration for program search is something we\u2019re currently exploring \u2014 of particular interest are methods for grammar-constrained generation with LLMs (e.g., [2]) \u2014 but this is probably beyond the scope of the current paper.\n\nLastly, it\u2019s worth noting that DreamCoder\u2019s enumerative search, which we use in LILO, trains a neural network to predict PCFG weights, so \u201cenumeration\u201d is itself an instance of neurally-guided symbolic search. We provide more detail on this below, in response to your questions.\n\n**What does PCFG stand for?**\n\nA probabilistic context free grammar (PCFG) is a generalization of a context-free grammar that assigns a real-valued weight to each production rule. **We define this term towards the start of the Preliminaries section on p. 3**: \u201cthe prior is defined under a probabilistic context free grammar (PCFG; Johnson 1998)...\u201d\n\nMore formally, a PCFG can be defined as a quintuple $G = (M, T, R, S, P)$ where $M$ is a set of non-terminal symbols, $T$ is a set of terminal symbols, $R$ is a set of production rules, $S$ is the start symbol, and $P$ is a function that assigns a probability to each rule in $R$. This probabilistic element allows for the generation of a diverse range of structures. Moreover, as we describe below, training a task-conditioned neural network to infer $P$ is a natural way to guide search in the PCFG.\n\n**Could the author describe the input-output, architecture, and training of this multi-layer perceptron? Is this model pre-trained? DSL-specific?**\n\nHappy to provide more clarification! Here, we are talking about a piece of DreamCoder\u2019s enumerative search algorithm that trains a **\u201crecognition network\u201d** (i.e., an RNN encoder + a MLP) to guide enumerative search by inferring $P$: the probabilities on production rules in the PCFG, as defined above.\n\n***All of this machinery is part of prior work (i.e., DreamCoder)*** and uses off-the-shelf Pytorch components, so we don\u2019t allocate much space in the main paper body to describe these details. Nevertheless, these are natural questions for a reader interested in implementation, so we provide an overview of the architecture below. **We refer to Appendix I in the [DreamCoder supplement](https://dl.acm.org/doi/10.1145/3453483.3454080) for further details details and will add a pointer to this appendix in the paper.**\n\n- **Inputs and outputs:** The recognition network takes as input a set of input/output examples for a specific task and produces an $|\\mathcal{L}| \\times |\\mathcal{L}|$ tensor with values in $[0, 1]$ specifying transition probabilities for every production in the PCFG.\n- **Architecture/training:** The input/output examples are encoded using an RNN (specifically, a bidirectional GRU) for text-based domains and a CNN for image-based domains. The actual MLP module is very small (only two layers, with a hidden size of 64) so the whole architecture can be learned efficiently in-the-loop. During each learning iteration, the recognition network is trained for a maximum of 10K gradient steps.\n- **Pre-training:** As implemented, the model doesn\u2019t use any pre-trained components. Following prior work with these datasets, we assume ground truth access to the CLEVR scene graphs, which allows us to use a text-based encoder for that domain. If we wanted to train on the original images, one could easily swap in a pre-trained CNN, or even a SOTA segmentation model, for the encoder.\n- **DSL-specificity:** Overall, the recognition network architecture is fairly general\u2014the main domain-specific pieces are the choice of encoder and the dimensionality of the output, which depends on $|\\mathcal{L}|$."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3844/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700093692045,
                "cdate": 1700093692045,
                "tmdate": 1700102716907,
                "mdate": 1700102716907,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9nJWONg5iG",
                "forum": "TqYbAWKMIe",
                "replyto": "39ZQaKVaBf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3844/Reviewer_NLqL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3844/Reviewer_NLqL"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "I acknowledge that I have read the authors' responses and other reviewer's discussion with the authors. \n\nOverall I believe this work is above the acceptance threshold of ICLR, and also above the quality of other papers I'm assigned in this venue, despite the domain specific limitation. Given the computation cost and tremendous pre-training/finetuning/prompting options in LLM, it's kind of hard to make really \"fair\" comparison now, so my judgement is more based on the completeness and robustness of the entire pipeline/system. From this aspect, the only concern I had was the auto-documentation, and I agree with the authors that it should be beneficial to comparing and potentially leverage few shot prompting to enhance this part. \n\nMy rating remains the same."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3844/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590513580,
                "cdate": 1700590513580,
                "tmdate": 1700590513580,
                "mdate": 1700590513580,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Et9GduOxzZ",
            "forum": "TqYbAWKMIe",
            "replyto": "TqYbAWKMIe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3844/Reviewer_FkLi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3844/Reviewer_FkLi"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a neurosymbolic framework 'LILO' for learning interpretable libraries of reusable code abstractions. \n* A dual-system synthesizer that uses both LLM-guided search and enumerative search.\n* A compression module based on 'STITCH' that extracts common abstractions from synthesized programs.\n* An auto-documentation module that adds human-readable names/descriptions to make abstractions more interpretable."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**Originality**:\n* The integration of LLMs and symbolic program compression is a novel approach toward library learning.\n* Propose auto-documentation for improving abstraction interpretability.\n\n**Quality**:\n* Well described architecture.\n* The analysis and experiments are comprehensive.\n* Comparision to multiple baselines on 3 program synthesis benchmarks demonstrate the advantages of the proposed framework.\n\n**Clarity**:\n* The overall framing and individual components of LILO are well motivated.\n\n**Significance**:\n* The proposed 'LILO' provides a generalizeable blueprint for integrating language models with formal program synthesis."
                },
                "weaknesses": {
                    "value": "* In Table 1, the proposed 'LILO' framework has substantially higher standard deviation than the baseline across three domains. It indicates LILO's performance varies more widely across different runs, suggesting less stability in this field. \n* The dual-system search module combines LLM-guided and enumerative search, yet the tradeoff and relative contributions are not deeply analyzed.\n* The proposed 'LILO' is strongly based on LLM and it mentions using different models like Codex, gpt-3.5-turbo, gpt-4, but did not provide direct comparison of the their performance."
                },
                "questions": {
                    "value": "* Please address my concern in the above weakness section.\n* In Figure 10 caption D, the authors mentions LLM output are sampled without clearly specify the filtering/selection standard.  I advise the author can give a more detailed explanation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3844/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3844/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3844/Reviewer_FkLi"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3844/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698702723594,
            "cdate": 1698702723594,
            "tmdate": 1700522224881,
            "mdate": 1700522224881,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "izykd6IjB5",
                "forum": "TqYbAWKMIe",
                "replyto": "Et9GduOxzZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3844/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3844/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review!"
                    },
                    "comment": {
                        "value": "We are glad that you found this work to be \u201cnovel,\u201d \u201cwell-described\u201d and \u201cwell-motivated;\u201d and that you felt our experiments and analysis were \u201ccomprehensive.\u201d We are also delighted that you agree LILO provides a \u201cgeneralizable blueprint for integrating language models with formal program synthesis,\u201d as this is exactly what we set out to accomplish with this work.\n\nThank you for your review and comments! Please allow us to address some of the points that you raised below:\n\n**In Table 1, the proposed 'LILO' framework has substantially higher standard deviation than the baseline across three domains. It indicates LILO's performance varies more widely across different runs, suggesting less stability in this field.**\n\nPart of the novelty of our method is in the integration with a LLM, which inherently introduces variability across runs. In response to your comments and those of the other reviewers, we have updated Table 1 to more transparently present this variability by underlining all results that fall within $1\\sigma$ of the best (bolded) result.\n\nWhile some of the online synthesis conditions approach LILO\u2019s performance, we note that none of the results from offline synthesis (Table 1, bottom half) fall within $1\\sigma$ of the best (bolded) result. Thus, **we are confident that our results indicate that LILO meaningfully outperforms the other models considered in offline synthesis.**\n\nWe also would like to highlight that $\\sigma$ is computed with respect to relatively small $N=3$ runs due to the computational cost of each full run of online program synthesis, so it is not a very tight approximation of the variability in performance. Prior work (e.g., DreamCoder, LAPS) does not  report this variability as part of the main results, but we felt it important to include some kind of variability estimate in our Table 1.\n\n**The dual-system search module combines LLM-guided and enumerative search, yet the tradeoff and relative contributions are not deeply analyzed.**\n\nThank you for the suggestion. We have added two new baselines to the \u201coffline synthesis\u201d section for the LLM Solver and LLM Solver (+ Search). While these are primarily intended to address a specific concern from Reviewer oPbs, these baselines also help to further pinpoint the relative contributions of LLM-guided search, enumerative search, and compression within LILO.\n\nBeyond these new baselines, our existing results set already gives significant attention to the question of the relative contributions of the two search methods. As a recap, we include the following baselines/ablations as part of our original experiments:\n\n- LILO (No Search)\n- LLM Solver (+ Search)\n\nThis pair of conditions (removing enumerative search from LILO / adding enumerative search to the LLM-only baseline) is directly targeted at analyzing the tradeoffs and relative contributions of the two search methods. We discuss these findings on p. 7 (\u201dTo isolate the effects of search\u2026\u201d). In addition to the quantitative results, we also mention several qualitative examples of tasks that are solved with enumerative search but not LLM-guided search (e.g., how to draw a \u201csnowflake\u201d or \u201cstaircase\u201d in the LOGO domain; see Fig. 5)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3844/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700093570896,
                "cdate": 1700093570896,
                "tmdate": 1700093570896,
                "mdate": 1700093570896,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QUqohQ8wOL",
                "forum": "TqYbAWKMIe",
                "replyto": "Et9GduOxzZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3844/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3844/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**The proposed 'LILO' is strongly based on LLM and it mentions using different models like Codex, gpt-3.5-turbo, gpt-4, but did not provide direct comparison of the their performance.**\n\nWhile we would have liked to be in a position to systematically benchmark various LLMs for LLM-guided synthesis, as we discuss in Footnote 1, cost considerations required us to restrict our analysis to Codex.\n\nAs a concrete example, based on our numbers in **Appendix C.1, Table 6**:\n\n- An equivalent benchmark of **gpt-3.5-turbo** would have cost **$1,272.36** at the current price of $0.002 / token.\n- An equivalent benchmark of **gpt-4** would have cost **$38,170.80** at the current price of $0.06 / token.\n\nIn contrast, we accessed Codex through OpenAI\u2019s free beta program for researchers, which saved thousands of USD over the project lifetime and afforded higher rate limits than paid GPT models.\n\nAs discussed in implementation details, we were able to make use of gpt-3.5-turbo and gpt-4 for AutoDoc, which requires orders of magnitude fewer queries. This is why these models are mentioned in the paper, but we make no claims about having systematically benchmarked these other models for synthesis. We also do not think that exhaustive and costly benchmarking of an ever-changing set of foundation models should be a standard for publication at a conference such as ICLR.\n\n**In Figure 10 caption D, the authors mentions LLM output are sampled without clearly specify the filtering/selection standard. I advise the author can give a more detailed explanation.**\n\nThank you for the attention to detail. This filtering/selection pipeline is already described in Section 3 (\"For each completion, we run parsing, type inference, and execution checks to identify valid programs that solve the target task.\u201d). To provide more detail, our post-processing pipeline for LLM completions contains the following steps:\n\nGiven a `completion: str`\n\n1. Check whether `completion` parses to a valid program in the DSL. If not, discard.\n2. Check whether we can perform type inference on `completion`. If not, discard. At this point, `completion` represents a valid, well-typed `program` in the DSL.\n3. Check whether `program` satisfies the target task specification (i.e., execute it against all I/O examples). If it does, add `program` to the set of task solutions.\n\nIf there are any specific aspects that you would like more detail on, please let us know. Our code on GitHub (currently anonymized for review) will also provide a clear reference for how this pipeline is implemented."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3844/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700093591749,
                "cdate": 1700093591749,
                "tmdate": 1700505244101,
                "mdate": 1700505244101,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HAtEJwbvcD",
                "forum": "TqYbAWKMIe",
                "replyto": "QUqohQ8wOL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3844/Reviewer_FkLi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3844/Reviewer_FkLi"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detail information. I have increased the score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3844/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700522309360,
                "cdate": 1700522309360,
                "tmdate": 1700522309360,
                "mdate": 1700522309360,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AltDr1oWpS",
            "forum": "TqYbAWKMIe",
            "replyto": "TqYbAWKMIe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3844/Reviewer_oPbs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3844/Reviewer_oPbs"
            ],
            "content": {
                "summary": {
                    "value": "LILO is a system for library learning that leverages large language models. It iterates through a series of modules that allow it to automatically discover a collection of interpretable and documented abstraction functions that help solve held-out tasks across three domains: string regexes, turtle graphics, and visual-question answering. First, given the current version of a DSL, LILO asks an LLM to infer programs that solve prompted input tasks (this step is augmented by an enumerative search). Then, an abstraction discovery algorithm (STITCH) is employed to identify and refactor common patterns of code use into abstraction functions that are added to the base DSL to form a new library. These abstractions are then presented to an LLM, so that they can be given interpretable names and a doc-string description. Compared with prior library learning techniques (DreamCoder) LILO is able to solve more held out tasks with its discovered abstractions."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well-written and proposes a compelling method for a difficult and important problem. The methodology employed is sound and clearly described, even without a reference implementation much of the system seems reproducible. On the whole, the experimental design is reasonable and comprehensive, modulo a few missing conditions (detailed in the next section). I was pleasantly surprised by the detailed computational efficiency analysis at the end of the appendix, it was a well-formulated and interesting addition to the paper.\n\nIn terms of contribution, the paper situates itself as a complete system that offers an improvement over DreamCoder across a varied collection of tasks; I think the evidence the paper presents on this point is compelling. There are two main insights/observations that support this improvement. (1) Given a task description, LLM\u2019s can usefully solve inductive programming tasks and (2) the success of (1) is dependant on operating over a DSL with \u201cgood documentation\u201d, e.g. without semantic names, even otherwise helpful functions can contribute to model obfuscation. However, in some ways the most surprising results from the paper are that, even without library learning, LLMs can solve these domain-specific tasks much better than prior works (e.g. for regex, comparing DreamCoder to and LLM Solver that infers with respect to a base DSL offers a large improvement).\n\nThe paper notes that invoking an LLM at inference time can actually be more computationally efficient than enumerative search, which is an interesting observation. That said, I think the strongest experimental evidence in favor of the proposed method is in the bottom half of Table 1, which shows that LILO produced libraries outperform other libraries when only enumerative search is used. This is a clever, although not perfect, way of evaluating how well-suited each discovered library is for a particular domain."
                },
                "weaknesses": {
                    "value": "My main concern is that while LILO does offer an improvement over DreamCoder, I\u2019m not convinced the claimed reasons for the improvement are conclusive based on the experimental design. \n\nFor instance, the version of LILO without enumerative search actually does worse than only using LLM solver for all three of the listed domains, even though LILO has access to an abstraction discovery method (i.e. STITCH). So then, is the evidence that LILO w/ search outperforms LLM Solver+search related to LILO\u2019s use of LLMs (to infer programs and document abstractions) or its use of STITCH to identify common patterns of code-use. On this note, the comparisons between the LLM solver+search variant and LILO appear a bit more murky than the narrative reads; based on the listed standard deviation numbers it\u2019s unclear if there are statistically significant differences in any of these cases. \n\nTo better separate out how the contributions of LILO (LLM program induction in the context of iterative library learning, along with auto-documentation of abstractions) affect system performance, I think the following conditions should be considered:\n\n- (A) Base DSL + STITCH\n- (B) LLM Solver+Search+STITCH\n\nCondition (A) would be an ablated version of LILO, where no LLM calls are made. \u201cWake\u201d would be performed through enumerative search, and STITCH would still be used to generate new libraries. This would be similar to a DreamCoder + STITCH integration, where STITCH replaces DreamCoder\u2019s sleep phase. From my perspective, this ablation condition is critical to supporting the claims of the paper \u2013 at present, its possible that the delta improvement between LILO and Dreamcoder is due entirely to LILO\u2019s use of STITCH, and ablating out of the use of LLMs in this manner would provide a definitive answer on this point.\n\n\nCondition (B) would first employ the LLM Solver+Search for a set amount of iterations, and then would run STITCH over the final set of inferred programs to output a new library. This would be important to support the implicit claim of the method that library learning needs to be performed in the intermediate stages of the algorithm. If LILO outperforms this ablation on offline search performance (e.g. bottom half of Table 1), then it would be a strong mark in favor of the method."
                },
                "questions": {
                    "value": "While I think the paper does a good job of validating that AutoDoc is really helpful in getting the LLM to be able to employ the discovered abstractions, I\u2019m a bit curious to know more about how mistakes in documentation might affect performance (as a few mistakes from AutoDoc were noted). Would it be possible to compare AutoDoc generated documentation versus \u201coracle\u201d documentation (e.g. expert provided)? Even if done just once, it could provide some useful insights into how close the AutoDoc module is to human performance, and whether the LLM solver that gets \u201cexpert\u201d documentation would be any better than the one generated by AutoDoc.\n\n# Minor comments\n\nWhile I appreciate the goals of the system are to work for 'real-world\u2019 domains, I would consider reframing the introductory statement of the results section, as I wouldn\u2019t consider any of the domains studied in this work to match that description.\n\nI think a bit more effort should be made into clarifying that STITCH is not a contribution of this work \u2013 this is clear to a careful reader, but should be made obvious. Additional citations to STITCH in Figure 1 and the respective paragraph in Section 3 would seem appropriate. \n\nIt looks like Table 1 has a minor bolding issue in the CLEVR mean column, bottom half. More generally, I\u2019m unsure if bolding only the highest number is the right thing to do in this table, when the standard deviations have a great deal of overlap in many cases.\n\nMaybe I\u2019m misunderstanding this, but the standard inductive program synthesis problem statement at the beginning of Section 2, doesn\u2019t seem to match the problem setting that LILO uses. Instead of being conditioned on a specification with a set of input/output examples that come from the same program, my understanding is that the LLM is conditioned on task-description/program examples (e.g. the in-context examples), and then is prompted with a single new task-description. Is there a reason for this disconnect?\n\nWhile the performance differences between LAPS and LILO are discussed in A.5, is there a reason that this comparison isn\u2019t put directly into either Table 3 or Table 4?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3844/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3844/Reviewer_oPbs",
                        "ICLR.cc/2024/Conference/Submission3844/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3844/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698779540110,
            "cdate": 1698779540110,
            "tmdate": 1700512144141,
            "mdate": 1700512144141,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wSj76dwvQs",
                "forum": "TqYbAWKMIe",
                "replyto": "AltDr1oWpS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3844/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3844/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your insightful review!"
                    },
                    "comment": {
                        "value": "We are heartened that you found value in so many facets of this work: that the problem space is \u201cimportant\u201d and \u201cdifficult\u201d; that the methods are \u201csound\u201d, \u201ccompelling\u201d, and \u201cclearly-described\u201d; that the experiment design is \u201creasonable\u201d; and that the overall paper is \u201cwell-written\u201d.\n\nThank you for your insightful comments and detailed suggestions. **In response to your review, we have:**\n\n- Run two new baselines (See Baseline B, below)\n- Started working on a new AutoDoc evaluation\n- Addressed many of your minor comments, with plans to address all of them in the final submission\n\n# New Baselines\n\nWe appreciate your attention to detail around the experiment design and hope to fully address your concerns below. Concretely, the following two experimental conditions were proposed.\n\n## Baseline A: Base DSL + Stitch\n\nProposal: This baseline is similar to a DreamCoder + Stitch integration, where Stitch replaces DreamCoder\u2019s compression module. The motivation for this condition is to address the possibility of a confound due to the choice of compression module, which could account for the performance delta between LILO and DreamCoder in the experiments.\n\n*Response:* Your concerns about this confound highlight an oversight in our methods presentation: **Stitch is already used as the compression module for all models reported in the paper. This includes our DreamCoder condition, which corresponds exactly to the proposed \u201cBaseline A.\u201d** Specifically, we swapped out DreamCoder\u2019s compression module to use Stitch while preserving the rest of the architecture. There were two motivations for this change:\n\n(1) To improve the efficiency of the \u201csleep\u201d phase: for even modestly-sized datasets (i.e., hundreds of programs), Stitch makes the difference between compression running in tens of seconds vs. tens of hours (see the performance benchmarks in [1]; we find similarly striking efficiency improvements on our domains).\n\n(2) To facilitate direct comparability between LILO and DreamCoder and remove the possibility of the confound described here.\n\n**Thank you for highlighting for this oversight\u2014this is precisely the kind of background assumption that would be difficult for the authors to catch in our own work. We promise to clarify this point in our updated submission!**\n\n## Baseline B: LLM Solver + Search + Stitch\n\nProposal: This baseline is an extension of the existing LLM Solver (+ Search) condition, where Stitch is run on the final iteration. The motivation for this condition is to produce a library that can be evaluated in offline synthesis so as to isolate the effect of intermediate compression on performance. The reviewer states that if LILO were to outperform this ablation in offline synthesis, it would be a \u201cstrong mark in favor of the method.\u201d\n\n****Response:**** We immediately saw the value of this proposal and ran this baseline over the weekend. For completeness/consistency with the online synthesis conditions, we also ran the analogous version of this baseline for the LLM Solver (no Search) condition, which we expect to be weaker than LLM Solver (+ Search).\n\nAfter running the experiment, we can now confidently report that LILO outperforms both of these baselines by a good margin ($>1\\sigma$) on all domains. As intended, the only implementational difference between LILO and the new LLM Solver (+ Search) baseline with post-hoc Stitch compression is the intermediate compression + documentation steps. Therefore, we agree that this result makes a strong argument in favor of performing library learning in-the-loop.\n\n|  |  | REGEX |  |  | CLEVR |  |  | LOGO |  |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n|  | max | mean | std | max | mean | std | max | mean | std |\n| LLM Solver | 48.60 | 43.00 | 5.17 | 91.26 | 89.64 | 2.02 | 36.04 | 27.33 | 7.56 |\n| LLM Solver (+ Search) | 63.40 | 55.67 | 7.51 | 91.26 | 89.00 | 3.92 | 28.83 | 27.63 | 1.04 |\n\n**We have updated our submission with these new results (Table 1 and Figure 4)\u2014please see the updated PDF!** Thank you for this insightful suggestion\u2014we certainly feel these new baselines strengthen the empirical claims of the paper and hope you feel similarly.\n\n# Additional experiments\n\n**Evaluating the quality of AutoDoc documentation against human experts.**\n\nWe appreciated your suggestion to compare AutoDoc outputs against expert-generated documentation. Internally, we had discussed something similar as a follow-up to this paper and were inspired by your suggestion to start working on a limited version of this experiment. So far, we have manually corrected AutoDoc errors (e.g., the kind of issues highlighted in Fig. 5) and and are working on a head-to-head evaluation of downstream LLM-guided synthesis performance. We are not sure whether we will have something ready in time for the review response deadline, but will keep you posted."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3844/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700093350191,
                "cdate": 1700093350191,
                "tmdate": 1700102518101,
                "mdate": 1700102518101,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LhJGJ1BQR7",
                "forum": "TqYbAWKMIe",
                "replyto": "rqUnq7mNqq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3844/Reviewer_oPbs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3844/Reviewer_oPbs"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for their detailed responses and engagement! After the rebuttal I am much more positive on the paper: the strong performance against the baselines discussed above (along with the clarification that STITCH is used throughout) clearly demonstrates the value of the proposed method. \n\nI have updated my score accordingly, and at this point recommend acceptance without reservation. \n\nMy only outstanding thought is that it would while footnote 1 is much more helpful in making the distinction between the original DreamCoder method and the DreamCoder ablation considered in this paper, I'm still a bit apprehensive that many readers might miss this reference. To make sure this key piece of information is not lost, it would seem best to make this point in the main text, somewhere in Section 4 (perhaps the discussion)."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3844/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512606388,
                "cdate": 1700512606388,
                "tmdate": 1700512606388,
                "mdate": 1700512606388,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]