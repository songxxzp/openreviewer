[
    {
        "title": "Improving Compositional Text-to-image Generation with  Large Vision-Language Models"
    },
    {
        "review": {
            "id": "LgoFz7xTyi",
            "forum": "E2ePtpKJpy",
            "replyto": "E2ePtpKJpy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1901/Reviewer_dXyt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1901/Reviewer_dXyt"
            ],
            "content": {
                "summary": {
                    "value": "This paper enhances image-text alignment by fine-tuning the diffusion models through LVLM based evaluations during the training period.\nand designs an LVLM-guided iterative correction process to systematically rectify any misalignments in the generated images during the inference period."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper establishes a robust and plug-and-play framework for improving compositional text-to-image diffusion models."
                },
                "weaknesses": {
                    "value": "1. Lack of comparative experiment\n2. Lack of critical ablation experiments."
                },
                "questions": {
                    "value": "1\u3001In LVLM-guided Editing,  the author introduce a image-editing algorithm which is applied iteratively to rectify the image until no alignment is detected.  Authors should provide corresponding ablation experiments without Model Fine-tuning. In other words, such method should also work directly on pre-trained Stable Diffusion, and the authors should prove it.\n\n2\u3001 The author introduces four limitations in  Compositional Text-to-Image Generation: (a)  Object Number (b) Attribute Binding (c) Spatial Relationship (d) Aesthetic Quality. However,  I think these limitations have been addressed to some degree in Instructpix2pix [1], Prompt-to-Prompt [2] and the authors lack relevant comparative experiments\n\n[1] Brooks T, Holynski A, Efros A A. Instructpix2pix: Learning to follow image editing instructions[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 18392-18402.\n[2] Hertz A, Mokady R, Tenenbaum J, et al. Prompt-to-Prompt Image Editing with Cross-Attention Control[C]//The Eleventh International Conference on Learning Representations. 2022."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1901/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698221057629,
            "cdate": 1698221057629,
            "tmdate": 1699636120489,
            "mdate": 1699636120489,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "hp1rDftVqz",
            "forum": "E2ePtpKJpy",
            "replyto": "E2ePtpKJpy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1901/Reviewer_A5xo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1901/Reviewer_A5xo"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a pipeline to address the well-known problem of text-to-image generation\u2014compositional T2I generation. This paper analyzes the object number, attribute binding, spatial relationship, and aesthetic quality problem and proposes to employ LLM to solve this problem."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper tries to address a very important problem\u2014compositional text-to-image generation, which is a notorious problem in image generation area. This paper proposes a framework that employs LLM to deal with this issue. Some subjective and objective experiements show its effectiveness."
                },
                "weaknesses": {
                    "value": "1. The method proposed in this paper lacks novelty. It seems like a combination of different components\u2014a large vision-language model (e.g. llama, bard), a Reward Feedback Learning (e.g. ImageReward) and an image segmentation module (e.g. SAM). Actually, the framework is not so hard to come up with, and the overall three processes are extremely time-consuming since all the large models (language, vision, or multimodal) are huge and prohibitively expensive to implement. The contribution of this paper is not so clear.\n\n2. Since there exist many compositional text-to-image generation algorithms based on LLM [1] or not [2, 3, 4], some of which are mentioned by you in the introduction section, no further discussion or comparisons are provided in the experimental results part. I can see only some subjective cases and a CLIPScore value, as in the experiments. You must spend more space discussing the algorithm complexity and performance comparisons with the SOTA methods.\n\n\n[1] Lian, Long, et al., \"LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models.\" arXiv preprint arXiv:2305.13655 (2023).\n\n[2] Chefer, Hila, et al. \"Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models.\" ACM Transactions on Graphics (TOG) 42.4 (2023): 1-10.\n\n[3] Feng, Weixi, et al. \"Training-free structured diffusion guidance for compositional text-to-image synthesis.\" arXiv preprint arXiv:2212.05032 (2022).\n\n[4]Wang, Ruichen, et al. \"Compositional text-to-image synthesis with attention map control of diffusion models.\" arXiv preprint arXiv:2305.13921 (2023)."
                },
                "questions": {
                    "value": "Please refer to the weakness part"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1901/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1901/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1901/Reviewer_A5xo"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1901/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698232719075,
            "cdate": 1698232719075,
            "tmdate": 1699636120417,
            "mdate": 1699636120417,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "DyTiCG4S55",
            "forum": "E2ePtpKJpy",
            "replyto": "E2ePtpKJpy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1901/Reviewer_dnr1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1901/Reviewer_dnr1"
            ],
            "content": {
                "summary": {
                    "value": "The paper attempts to integrate various large models, including LLM (LLaMA), LVLM (Bard), a Diffusion Model (Stable Diffusion), and an Editing Model (Blended Diffusion), to establish a finetuning and inference pipeline aimed at enhancing the capability of compositional text-to-image generation. The proposed pipeline is structured in three distinct stages: LVLM-based Evaluation, Diffusion Model Finetuning, and LVLM-guided Editing. While the paper provides examples of the pipeline\u2019s application, it relies solely on qualitative evaluation methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The concept of integrating several sophisticated models into a single pipeline that potentially complement each other's functionalities is commendable and could inspire more research in the field."
                },
                "weaknesses": {
                    "value": "1. The premise of amalgamating various complex models is appealing, but the justification for such a pipeline, along with comprehensive evaluations of its efficacy, is lacking. The current results do not substantiate the effectiveness of the pipeline. Particularly, the necessity for both stage 2 (Diffusion Model Finetuning) and stage 3 (LVLM-guided Editing), which seemingly aim for the same goal, is not clear. There is a lack of clarity about the contributions of each stage. An in-depth ablation study would be valuable to discern the individual and collective impact of these stages.\n\n2. The paper\u2019s evaluation approach is insufficiently rigorous. The success cases are limited in number and are repeatedly used, without the support of robust quantitative analysis. Examples only contains simples cases, e.g., \u201cthree black dogs\u201d, \u201ca white cat and a black dog\u201d. The scenarios presented could potentially be addressed by Blended Diffusion Editing alone, raising questions about the added value of the other stages.\n\n3. The computational demands of the pipeline, particularly the LVLM-guided iterative refinement in stage 3, are not discussed. An analysis of the computational costs and the typical number of iterations required for image generation would be beneficial.\n\n4. The quality of writing and the clarity of figures require improvement to enhance the paper's comprehensibility and professional presentation."
                },
                "questions": {
                    "value": "1. There is a typographical error in Section 3.3 (LVLM-based Question Answering) involving the missing parenthesis around the $Q_i, A_i$ pairs.\n\n2. There seems to be a mismatch between LLM (LLaMA) and LVLM (Bard) in the experiments, potentially leading to discrepancies in the answers and consequently, a lack of reliability in the accuracy measures. Would it not be more consistent to just use Bard, as it supports both textual and visual modalities?\n\n3. Equation 7 mentions a term $\\tilde{Q_i}$ which lacks a definition. Could you provide a clarification for this term?\n\n4. In Figure 5, there is an inconsistency with the input text depicted in the first panel. The input text should be \u201cthree black dogs\u201d? \n\n5. References to Blended Diffusion and SAM appear to be missing when these terms are introduced in the paper.\n\n6. The examples in Figure 4, such as the fifth case, seem to indicate that stage 2 is ineffective. This observation supports my concerns regarding the relative impact of stage 2.\n\nRecommendation:\n\nGiven the aforementioned concerns, particularly the lack of robust evaluation and the unclear contribution of each pipeline stage, I recommend that the paper be revised before consideration for publication. The authors should address the weaknesses and questions detailed above, ensuring that the pipeline's design and efficacy are convincingly demonstrated through comprehensive experiments and quantitative assessments."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1901/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699551841843,
            "cdate": 1699551841843,
            "tmdate": 1699636120341,
            "mdate": 1699636120341,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "NFf8eg6ql9",
            "forum": "E2ePtpKJpy",
            "replyto": "E2ePtpKJpy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1901/Reviewer_TYf6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1901/Reviewer_TYf6"
            ],
            "content": {
                "summary": {
                    "value": "Three main components for this paper:\n\nUse VLM to evaluate image-text alignment by breaking down text into multiple QA\n\nFinetune diffusion models with image-text pair that are weighted by score predicted by above methods \n\nTest time editing (e.g., add/remove a object via inpainting) by signal from VLM"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The writing is clear and the idea of the whole framework is straightforward"
                },
                "weaknesses": {
                    "value": "The paper does not have novelty and lacks experiment study \n\n1, For their evaluation method, the idea of evaluation alignment by breaking down text into QA and then calling external model is first proposed by TiFA. This paper simply uses a different model (Bard) which I don't think there is anything new here. \n\n2, for the score-weighted finetuning, it is very like: \"Aligning Text-to-Image Models using Human Feedback by Kimin et al\". How is it different than this paper's finetuning idea (The first part of Eq2 in Kimin's paper)?\n\n3, I love the third component to improve alignment by editting. However, they just breifly mentioned the high-level pipeline. For exmaple, to solve incorrect attribute, they just give the high-level idea: detect the wrong object and then use inpainting method to regenerate a correct one. This is just a system, however, in a research paper, they should've include more study or in-depth analysis. For example, how accurate is the VLM model? do we need to finetune the model? if so, how to get data? Any insignful observation from prompting? etc. \n\n4. This paper does not have a proper experiment section to compare with baselines, or ablation, and it feels like the effort was half-hearted.   \n\n\nI suggest the authors identify one aspect of the problem and conduct a proper and in-depth study of that issue, rather than giving a general description of a system"
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1901/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699592086395,
            "cdate": 1699592086395,
            "tmdate": 1699636120251,
            "mdate": 1699636120251,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]