[
    {
        "title": "Text-Driven Image Editing using Cycle-Consistency-Driven Metric Learning"
    },
    {
        "review": {
            "id": "REQ7i1JrZ6",
            "forum": "8Cc6qOPvFo",
            "replyto": "8Cc6qOPvFo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission669/Reviewer_k61j"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission669/Reviewer_k61j"
            ],
            "content": {
                "summary": {
                    "value": "This work presents a training-free approach for text-driven image-to-image translation, building on a pre-trained text-to-image diffusion model. The authors revise the process to align better with the target task. They introduce a new guidance objective, which combines maximizing similarity to the target prompt (measured by CLIP score) and minimizing the distance to the source latent variables. Moreover, they employ a cycle-consistency objective to maintain the source image background by iteratively optimizing source and target latent variables. Experimental results demonstrate the exceptional performance of this method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The article introduces a simple yet effective approach for text-driven image-to-image translation. \n1. In contrast to other methods, this approach places a strong emphasis on preserving the structure and background of the source image during image editing. It accomplishes this by revising the process for generating target images to better align with the target task.\n2.  The article introduces a new guidance objective that combines maximizing similarity to the target prompt (measured by CLIP scores) and minimizing the distance to the source latent variables, resulting in improved quality of generated outputs.\n3. To maintain the background of the source image, the article utilizes a cycle-consistency objective. This involves iteratively optimizing source and target latent variables, enhancing the feasibility of the method."
                },
                "weaknesses": {
                    "value": "I find this method to be intuitive, but it appears to lack enough technical innovation, as similar concepts have been previously mentioned in prior works. My primary concerns are related to the experimental aspects:\n\n1. The authors should also conduct experiments on some of the datasets or images provided in their previous work.\n\n2. The quantitative experiments in the study appear to be insufficient. Since the authors have collected a dataset, it would be better for them to report average metrics on this dataset.\n\n3. There is a shortage of comparison with other methods. Given the wide attention in this field, it would be beneficial to compare this approach with more recent works. It is also better to include some fine-tuning-based methods (like SINE, Text-Inversion)  to provide a more comprehensive evaluation.\n\n4. The running costs, such as time and GPU resource consumption, should be reported and compared to help readers understand the resource requirements when using this method.\n\n5. The authors have not listed the limitations of their method. As this approach is positioned as a general method, it is essential to clarify whether it supports general scenarios, like the removal or addition of specific elements in the target image, to better inform users about its applicability.\n\n6. When comparing \"Prompt-to-prompt,\" it seems that the authors have not adopted a strategy that specifically considers the background region. This might impact the accuracy of the experimental results."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission669/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission669/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission669/Reviewer_k61j"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission669/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698577015367,
            "cdate": 1698577015367,
            "tmdate": 1699635994261,
            "mdate": 1699635994261,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gfL7wyweOV",
                "forum": "8Cc6qOPvFo",
                "replyto": "REQ7i1JrZ6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission669/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission669/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We truly thank you for your constructive comments and below are our responses to the main questions.\n\n\nQ1. Dataset for evaluation\n\n\nA1. Following the state-of-the art method [D1], we used the LAION-5B dataset to evaluate the text-driven image editing performance. \n\nQ2. Measurement of metrics using the entire images \n\nA2. As we mentioned in the main paper, we simply selected about 250 images for all tasks from the LAION-5B dataset based on the CLIP similarity for quantitative experiments since most of the remaining images can be irrelevant to the given tasks.  \n\nQ3.Comparison with SINE and Null-text Inversion \n\n\nA3. Since Null-text Inversion is orthogonal to our method, Null-text Inversion can be incorporated into our framework to further enhance the performance. We emphasize that Null-text Inversion is originally proposed to encourage the reconstructed image to align with the source image.  In case of SINE, it requires an additional fine-tuning process on the pretrained Stable Diffusion different from the proposed method. For fair comparisons, we focused on performing comparisons with training-free methods. \n\nQ4. Limitations\n\n\nA4. As we mentioned in the main paper, the proposed method can generate harmful or misleading samples due to the pre-trained model. For example, the pre-trained network can generate realistic samples that can potentially violate the privacy. \n\nQ5.Strategy about preserving the background region\n\nA5. Our triplet-based distance objective is effective to maintain the background region. Also, the cycle-consistency objective encourages the target images to preserve the structural and background information of the source images.\n\nReference\n\n[D1] G. Parmar et al., Zero-Shot Image-to-Image Translation, SIGGRAPH 2023."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission669/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699957839252,
                "cdate": 1699957839252,
                "tmdate": 1699957839252,
                "mdate": 1699957839252,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qqTM0DkRqb",
            "forum": "8Cc6qOPvFo",
            "replyto": "8Cc6qOPvFo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission669/Reviewer_GXjj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission669/Reviewer_GXjj"
            ],
            "content": {
                "summary": {
                    "value": "The paper  presents a training-free approach for text-driven image-to-image translation using a pretrained text-to-image diffusion model."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.The paper introduces a new guidance objective term, which combines maximizing similarity to the target prompt (based on the CLIP score) and minimizing the distance to the source latent variables.\n\n2.Unlike many existing methods based on diffusion models, the paper leverages a cycle-consistency objective to preserve the background of the source image."
                },
                "weaknesses": {
                    "value": "1. The time consumption of the proposed method compared to other methods should be given.\n\n\n2. Comparable works such as \"Negative-prompt Inversion\" and \"Null-text Inversion for Editing Real Images using Guided Diffusion Models\" demonstrate robust image reconstruction and content editing capabilities while preserving the original background. These works also support flexible target category transformations. A comprehensive comparison with these similar works could further support the paper's novelty and performance in relation to existing solutions.\n\n3. A user study is encouraged to be carried out."
                },
                "questions": {
                    "value": "see above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission669/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698760747001,
            "cdate": 1698760747001,
            "tmdate": 1699635994174,
            "mdate": 1699635994174,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rRr5eKRq2p",
                "forum": "8Cc6qOPvFo",
                "replyto": "qqTM0DkRqb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission669/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission669/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We truly thank you for your constructive comments and below are our responses to the main questions.\n\nQ1. Comparison with Negative-prompt Inversion and Null-text Inversion \n\n\nA1. Since Negative-prompt Inversion and Null-text Inversion are orthogonal to our method, they can be incorporated into our framework to further enhance the performance. We emphasize that Negative-prompt Inversion and Null-text Inversion are originally proposed to encourage the reconstructed image to align with the source image\n\n\nQ2. User study \n\n\nA2. We acknowledged the importance of the user study, however, please understand the difficulty to perform it in the rebuttal period."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission669/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699957253990,
                "cdate": 1699957253990,
                "tmdate": 1699957253990,
                "mdate": 1699957253990,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RqIN5Yth7L",
            "forum": "8Cc6qOPvFo",
            "replyto": "8Cc6qOPvFo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission669/Reviewer_q62i"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission669/Reviewer_q62i"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a unique method for free text-driven image editing by utilizing pre-trained text-to-image diffusion models. Central to this approach is a new guidance objective term, which maximizes similarity to the target prompt (as opposed to the source prompt) based on the CLIP score. In tandem, it minimizes the distance to the source latent variables. Additionally, the authors incorporate a cycle consistency objective to retain the background details."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- **Simplicity & Effectiveness**: The proposed method is both straightforward and seemingly efficacious, as evidenced by the results presented in the paper."
                },
                "weaknesses": {
                    "value": "- **Evaluation Methods**: The evaluation could be more robust. The prompts used for evaluation are closely related to the original noun, reducing diversity and potentially biasing results.\n- **Aspect Ratio Concerns**: The samples used for evaluation have been altered from their original aspect ratios. This could inadvertently disadvantage competing methods.\n- **Comparison Choices**: The results from prompt-to-prompt evaluations seem to perform well on generated images rather than inverted real ones. The absence of a comparison with Null-text-inversion, which might be a more apt benchmark, raises questions.\n- **Efficiency Metrics**: The paper would be more informative with a runtime efficiency comparison against other methods."
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission669/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission669/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission669/Reviewer_q62i"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission669/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698803186839,
            "cdate": 1698803186839,
            "tmdate": 1699635994073,
            "mdate": 1699635994073,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6AAuXU0WgV",
                "forum": "8Cc6qOPvFo",
                "replyto": "RqIN5Yth7L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission669/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission669/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We truly thank you for your constructive comments and below are our responses to the main questions.\n\n\nQ1. Evaluation protocol \n\nA1. We tried to follow the experiment protocol of the state-of-the-art methods [B1, B2] in terms of the text-drivn image-to-image translation tasks.\n\nQ2. Aspect ratio concerns\n\n\nA2. We are sorry for confusing you. We used the original data for all comparison methods including the proposed algorithm by keeping their original aspect ratios, but we cropped them only for the visualizations to clearly show the presented images given by the comparison algorithms.\n\nQ3. Comparison with Null-text Inversion\n\n\n A3. Since Null-text Inversion is orthogonal to our method, Null-text Inversion can be incorporated into our framework to further enhance the performance. We emphasize that Null-text Inversion is originally proposed to encourage the reconstructed image to align with the source image.   \n\n\n\nReference\n\n[B1] A. Hertz et al., Prompt-to-Prompt Image Editing with Cross-Attention Control, ICLR 2023.\n\n\n\n[B2] G. Parmar et al., Zero-Shot Image-to-Image Translation, SIGGRAPH 2023."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission669/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699957137384,
                "cdate": 1699957137384,
                "tmdate": 1699957137384,
                "mdate": 1699957137384,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VCdh9ZTrHj",
            "forum": "8Cc6qOPvFo",
            "replyto": "8Cc6qOPvFo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission669/Reviewer_Cs1x"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission669/Reviewer_Cs1x"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method for editing without training using additional cycle-consistency and triplet-based distance guidance. The triplet-based distance ensures that source and target images at the same time step are mapped closer together than those at different time steps, in addition to using a general feature similarity-based distance. The cycle-consistency objective is employed to ensure that two images, one with guide in the forward process and the other with guide in the backward process, produce identical results."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The method produces better structure-preserved editing results. Also, compared to other training-free algorithms the proposed method achieves better quantitative results."
                },
                "weaknesses": {
                    "value": "Cycle-constistency may overly fix the structure and may make the result unnatural with object with different structure. Also, the argument that different time-step target images should be farther apart than same time-step source and target images seems to lack sufficient justification. And there is no comparison with papers such as null-text inversion."
                },
                "questions": {
                    "value": "It seems there are only subtle differences with naive distance and the triplet distance guidance results. Is there more basis for the triplet loss that makes different time-step images of the same image distant from each other?\nAlso, how does the performance compare to recent papers such as null-text inversion and similar approaches?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission669/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission669/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission669/Reviewer_Cs1x"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission669/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834773122,
            "cdate": 1698834773122,
            "tmdate": 1699635993993,
            "mdate": 1699635993993,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oWrEqvblMW",
                "forum": "8Cc6qOPvFo",
                "replyto": "VCdh9ZTrHj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission669/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission669/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We truly thank you for your constructive comments and below are our responses to the main questions.\n\n\nQ1. Basis for the triplet loss that makes different time-step images of the same image distant from each other\n\n\nA1. Since the reverse process destroys the structure of the source image, the distance between $F( \\bar{\\mathbf{x}} _t^{\\text{tgt}} )$ and $F ( \\mathbf{x} _t^{\\text{src}} ) $ should be relatively closer compared to the distance between $F( \\bar{\\mathbf{x}} _t^{\\text{tgt}} )$ and $F ( \\bar{\\mathbf{x}} _{t+1}^{\\text{tgt}} )$ to preserve the structure or background in the source image.\n\n\nQ2. Comparison with Null-Text Inversion \n\n\nA2. Since Null-text Inversion is orthogonal to our method, Null-text Inversion can be incorporated into our framework to further enhance the performance. We emphasize that Null-text Inversion is originally proposed to encourage the reconstructed image to align with the source image."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission669/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699957079600,
                "cdate": 1699957079600,
                "tmdate": 1699957079600,
                "mdate": 1699957079600,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]