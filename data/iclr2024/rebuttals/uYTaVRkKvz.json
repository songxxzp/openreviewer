[
    {
        "title": "Interpretable and Convergent Graph Neural Network Layers at Scale"
    },
    {
        "review": {
            "id": "GN44ekMyHo",
            "forum": "uYTaVRkKvz",
            "replyto": "uYTaVRkKvz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8988/Reviewer_tz3b"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8988/Reviewer_tz3b"
            ],
            "content": {
                "summary": {
                    "value": "The paper describes a novel architecture for graph data. It extends the \"unfolded GNN\" line of research, where GNNs are formulated by an outer loss function to be minimized, and an inner energy function which is minimized layer-by-layer. In this inner energy function, a \"base model\" (e.g., an MLP over the nodes) is defined, and the graph layer is defined as a balance between deviating from this base model and smoothing the signal over the graph. \n\nTo scale it up, in the proposed MuseGNN the authors combine the idea of sampling subgraphs from the original graph with the unfolded GNN model. A new unfolded model is defined by running the original unfolded model on each subgraph, augmented by an additional regularization term that enforces similarity across subgraphs of the nodes' representations.\n\nThey show several convergence analyses on the model of different types. On the experimental side, they show the model is able to achieve better results on very large datasets with slightly higher training time than the baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well written and easy to follow. The idea is described clearly. The model is a combination of two known ideas (unfolded GNNs and subgraph sampling), but it is complemented by a good theoretical analysis and good experimental results. Overall, this is an interesting contribution for the field."
                },
                "weaknesses": {
                    "value": "The biggest weakness of the paper is that the authors keep saying that the model is \"interpretable\" (e.g., \"while maintaining the interpretability attributes of an unfolded GNN\"), but this is never truly motivated. The idea is that a user can look at the difference between the base model and the true output to understand whether the prediction was done by looking at the features or at the graph's structure, but this is a very weak notion of \"interpretability\". In addition, the authors are never showing examples of this.\n\nI have not found any analysis on the memory of the approach, which requires storing predictions and auxiliary embeddings for multiple subgraphs. In addition, the authors should provide an ablation study on the number of subgraphs that are chosen.\n\nIn the related works, several papers have proposed subgraph-based GNNs to improve the expressiveness of standard message-passing. Can you compare your approach, either methodologically or experimentally?"
                },
                "questions": {
                    "value": "Based on the listed weaknesses:\n1. Add a true explainability analyis or remove most of these claims.\n2. Add an analysis of the memory required by the model.\n3. Add some ablations on the number of chosen subgraphs.\n4. Improve the related works section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8988/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698661110209,
            "cdate": 1698661110209,
            "tmdate": 1699637131708,
            "mdate": 1699637131708,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DbWX00XqHt",
                "forum": "uYTaVRkKvz",
                "replyto": "GN44ekMyHo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8988/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8988/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tz3b"
                    },
                    "comment": {
                        "value": "Thanks for the helpful comments.  We address each point in turn below.\n\n**Comment:**\n*[General Weakness and Question 1.] Add a true explainability analysis or remove most of these claims.*\n\n**Response:**\nThanks for the suggestion; please see our general response to all reviewers presented above (and we are happy to modify the draft as needed).\n\n\n**Comment:**\n*[General Weakness and Question 2.] Add an analysis of the memory required by the model.*\n\n**Response:** \nMemory requirements were deferred to the supplementary to save space; however, we can summarize here.  For the lightweight $\\gamma=0$ case, the memory requirement is the same as  baselines like GCN with (online) neighbor sampling. Namely, $O(nd)$ of memory is required to store the input features in  main memory / disk, and $O(n_sdK)$ of GPU memory is required to store intermediate results (embeddings and gradients) in the device. Here $n$ and $n_s$ are the number of nodes in the full graph and subgraphs, respectively, $d$ is the dimension of input features / hidden embedding, and $K$ is the number of layers. For $\\gamma>0$, an extra $O(nd)$ of memory is needed to store the mean vectors $M$, while there is no extra memory needed on the GPU side. As a point of reference, this extra memory requirement is much lower than the $O(ndK)$ required by the GAS-based baselines (Fey et al., 2021) presented in Table 2.\n\n\n**Comment:**\n*[General Weakness and Question 3.] Add some ablations on the number of chosen subgraphs.*\n\n**Response:**\nWe note that the number of subgraphs is not a unique issue to MuseGNN, and could potentially affect the performance of any GNN model based on sampling.  Still, this is a useful stability metric to check as the reviewer suggests to rule out any appreciable sensitivity.  As such, we performed such an ablation with ogbn-products data using 50%, 60%, 70%, 80%, 90%, and 100% of the subgraphs that we originally used in the experiments from Table 2.  The accuracy differences were less than 1% across all cases, indicating a degree of robustness to the number of subgraphs.  We can add these results to the supplementary in a later revision.\n\n\n**Comment:**\n*[General Weakness and Question 4.] In the related works, several papers have proposed subgraph-based GNNs to improve the expressiveness of standard message-passing. Can you compare your approach, either methodologically or experimentally?*\n\n**Response:**\nThis is an interesting point.  We surmise from our submission that the reviewer may be referring to the reference (Zeng et al., 2021) or related, which discusses subgraph-based sampling that can influence expressiveness by decoupling depth (number of model layers) from scope (the receptive field size).  Similar to Zeng et al., MuseGNN also naturally facilitates the decoupling of depth and scope, where the scope is effectively determined by the underlying sampling method.  With regard to depth though, we differ from Zeng et al. in the sense that additional MuseGNN layers push node embeddings closer and closer to a specific target, namely, a minimizer of the objective from Eq.(4).  Another more subtle distinction exists w.r.t. oversmoothing:  While Zeng et al. relies directly on decoupling to avoid oversmoothing, MuseGNN will be immune to oversmoothing regardless of the particular sampler as long as minimizers of the original energy (4) do not oversmooth."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8988/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700261469226,
                "cdate": 1700261469226,
                "tmdate": 1700261469226,
                "mdate": 1700261469226,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3103v1wBGu",
                "forum": "uYTaVRkKvz",
                "replyto": "DbWX00XqHt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8988/Reviewer_tz3b"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8988/Reviewer_tz3b"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the answer.\n1. On interpretability: it seems 3/4 reviewers (me, ik1J, PP1g) were confused by your use of \"interpretability\", especially given its focus on the title. I believe the paper can improve significantly be removing this, at least from the title.\n2. Subgraph-based NNs: I was referring to the papers by Frasca et. al, 2022 (https://arxiv.org/abs/2206.11140) and following.\nFor the rest, the comments were addressed and I am happy to keep my current evaluation."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8988/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700474565216,
                "cdate": 1700474565216,
                "tmdate": 1700474565216,
                "mdate": 1700474565216,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gDPKSaXnaB",
                "forum": "uYTaVRkKvz",
                "replyto": "GN44ekMyHo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8988/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8988/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up Response to Reviewer tz3b"
                    },
                    "comment": {
                        "value": "Thanks for the quick response.  Regarding the title, we agree that removing \"interpretability\" is a good idea.  As a more reflective replacement, one option could be something like: \"Forming Scalable, Convergent GNN Layers that Minimize a Sampling-Based Energy.\"\n\nAlso, thanks for pointing to the Frasca et al. (2022) reference. Upon quick inspection, we notice that this paper is rich with details that require further time to digest.  As one speculative possibility though, the ideas from Frasca et al. could potentially be leveraged to more precisely characterize the expressiveness of MuseGNN's sampling-based energy function."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8988/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700515710487,
                "cdate": 1700515710487,
                "tmdate": 1700516561819,
                "mdate": 1700516561819,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uJvsEl1uP8",
            "forum": "uYTaVRkKvz",
            "replyto": "uYTaVRkKvz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8988/Reviewer_ceNB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8988/Reviewer_ceNB"
            ],
            "content": {
                "summary": {
                    "value": "The authors of the paper present a new approach to address the scalability issues of Graph Neural Networks (GNNs) when constructed with sampling-based energy functions. The paper also discusses the motivation behind unfolded GNNs, their advantages in terms of interpretability, and the challenges of scaling such models. It introduces the MuseGNN model, which addresses these challenges by incorporating efficient subgraph sampling into the energy function design. It demonstrates increased performance over the baselines on large-scale node classification tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper introduces a novel approach to address the scalability challenges faced by GNNs. The incorporation of offline subgraph sampling into the energy function design is a unique and innovative concept.\n\n2. The paper provides a theoretical analysis of the convergence properties of MuseGNN, when \u03b3 is set to 0.\n\n3. The paper includes experimental results showing that MuseGNN performs competitively in terms of accuracy and scalability, especially on large graphs exceeding 1TB in size. This demonstrates the practical feasibility of the proposed approach in large scale graphs."
                },
                "weaknesses": {
                    "value": "1. One notable weak point of the paper's experimental evaluation is its limited comparison to a rather outdated set of baseline models. While the paper does present compelling results in terms of the proposed MuseGNN's performance, the absence of more contemporary and diverse baseline models hinders the thorough assessment of the method's competitiveness and applicability in the current research landscape. Graph neural networks have seen significant advancements in recent years, resulting in a multitude of state-of-the-art models and techniques that offer superior performance across various graph-related tasks. Focusing solely on older and limited baseline models (GCN, GraphSage, GAT) from the past can potentially lead to a skewed perspective of MuseGNN's relative performance in the current state of the art. A more comprehensive comparison against a broader range of modern baseline models would provide a more accurate and up-to-date assessment of the strengths and weaknesses of MuseGNN. I provide some example papers [1,2,3] that can be used as baselines below. Moreover, in the ogb leaderboard https://ogb.stanford.edu/docs/leader_nodeprop/#ogbn-arxiv there are many new baselines.\n\n2. Another weak point in the paper lies in its motivation and justification for employing unfolded GNNs, especially for those who may not be well-versed in this specific research area. While the paper briefly discusses the benefits of using unfolded GNNs and emphasizes their role in enhancing explainability by distinguishing the relative importance of node features and graph structure in predictive tasks, the argument remains somewhat vague and underdeveloped. The paper primarily suggests that the use of unfolded GNNs can help reveal whether node features or graph structure hold more significance for predictions. However, it lacks a more thorough and nuanced discussion of why this is an important aspect to investigate, and how this contributes to the broader field of graph-based machine learning or the potential practical implications.\n\n\n\nReferences: \n\n[1] Rusch, T. Konstantin, et al. \"Graph-coupled oscillator networks.\" International Conference on Machine Learning. PMLR, 2022.\n\n[2]  Rusch, T. Konstantin, et al. \"Gradient gating for deep multi-rate learning on graphs. 2022\n\n[3]  Nikolentzos, Giannis, Michail Chatzianastasis, and Michalis Vazirgiannis. \"Weisfeiler and Leman go Hyperbolic: Learning Distance Preserving Node Representations.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2023."
                },
                "questions": {
                    "value": "1. Could the authors provide more context on their choice of limited and older baselines for comparison? Are there more recent or relevant methods that could have been included for a more comprehensive evaluation?\n\n2. The paper claims that using unfolded GNNs enhances explainability, but the argument remains somewhat abstract. Can the authors provide specific instances or use cases where this enhanced explainability has a direct impact on decision-making or model interpretability?\n\n3. Can the authors elaborate on the practical scenarios where distinguishing the importance of node features and graph structure is particularly valuable? How might this knowledge influence real-world applications, and can the paper provide more concrete examples?\n\n4. The paper would benefit from a more comprehensive and well-structured explanation and rationale for the concept of unfolded GNNs.\n\nI am more willing to increase my score, if the authors properly address the above concerns."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8988/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698755819596,
            "cdate": 1698755819596,
            "tmdate": 1699637131582,
            "mdate": 1699637131582,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NkmeUITGw1",
                "forum": "uYTaVRkKvz",
                "replyto": "uJvsEl1uP8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8988/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8988/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ceNB (Part I)"
                    },
                    "comment": {
                        "value": "Thanks for the detailed comments.  We address each point in turn below.\n\n**Comment:**\n*[From Weakness 1. and Question 1.] One notable weak point of the paper's experimental evaluation is its limited comparison to a rather outdated set of baseline models ... I provide some example papers [1,2,3] that can be used as baselines below. Moreover, in the ogb leaderboard https://ogb.stanford.edu/docs/leader_nodeprop/#ogbn-arxiv there are many new baselines. Could the authors provide more context on their choice of limited and older baselines for comparison? Are there more recent or relevant methods that could have been included for a more comprehensive evaluation?*\n\n**Response:**\nThis is a valid question, and it is definitely true that there now exists a considerable array of different GNN architectures.  The ogbn-arxiv leaderboard and references [1,2,3] are nice examples of this diversity as the reviewer mentions.  However, many/most of these methods have not been integrated with sampling (or historical caching like GAS) or shown to be competitive on truly large graph datasets as is our focus.  Also please kindly note that for the two largest datasets, MAG240M and IGB-full, we are already comparing against the top-ranked homogeneous graph leaderboard model, which is GAT (NS).\n\nAs for [1,2,3], these are quite interesting references proposing powerful models (especially for heterophily graphs); however, upon inspection of the empirical evaluations, none of the included benchmarks are especially large or require sampling; in fact, all are amenable to full-graph training such that scalability (our focus) is not a significant factor.  As for top-performing models on OGB leaderboards such as ogbn-arxiv, please see the discussion at the beginning of Section 6. \n\nOf course, we in no way intended to imply that [1,2,3] or top ogbn-arxiv models could not eventually be scaled to our large benchmarks. However, it remains an open research question outside of our scope of how to do so, and would likely require considerable engineering bandwidth and resources that we presently do not have.  Regardless, our central aim is really to scale unfolded GNNs, so in this regard, the metric of success is more tied to the two criteria mentioned at the beginning of Section 6, which our results thus far have achieved."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8988/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700261242342,
                "cdate": 1700261242342,
                "tmdate": 1700261242342,
                "mdate": 1700261242342,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XJD4MRowni",
            "forum": "uYTaVRkKvz",
            "replyto": "uYTaVRkKvz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8988/Reviewer_tqgM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8988/Reviewer_tqgM"
            ],
            "content": {
                "summary": {
                    "value": "Unfold GNNs are those whose forward pass iteratively reduces a graph-regularized energy function of interest. The node embeddings of unfolded GNNs serve as both predictive features and energy function minimizers. This paper proposes a sampling-based energy function and designs a scalable unfolded GNN (MuseGNN). The authors also theoretically analyze the convergence behavior of MuseGNN."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper is well-structured and well-written. I really enjoy reading this paper.\n2. The proposed method makes sense to me.\n3. The experiment results are good compared to the baselines."
                },
                "weaknesses": {
                    "value": "1. Lack of comparison with representative sampling-based GNNs, such as [a, b].\n2. The authors did not conduct experiments on datasets that can illustrate the importance of unfolded GNNs.\n\n***\n[a] Chiang, Wei-Lin, et al. \"Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks.\" Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining. 2019.\n\n[b] Zeng, Hanqing, et al. \"GraphSAINT: Graph Sampling Based Inductive Learning Method.\" International Conference on Learning Representations. 2019."
                },
                "questions": {
                    "value": "Please see \"weakness\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8988/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698766303432,
            "cdate": 1698766303432,
            "tmdate": 1699637131466,
            "mdate": 1699637131466,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2qiyLZlgoZ",
                "forum": "uYTaVRkKvz",
                "replyto": "XJD4MRowni",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8988/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8988/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tqgM"
                    },
                    "comment": {
                        "value": "Thanks for the constructive comments.  We address each point in turn below.\n\n**Comment:**\n*[Weakness 1.] Lack of comparison with representative sampling-based GNNs, such as Cluster-GCN and GraphSAINT.*\n\n**Response:**\nFor the largest-scale benchmarks, neighbor sampling (NS) is typically the method of choice, which is why we adopted it for our baseline comparisons (along with GAS as a more recent popular alternative).  Still, as the reviewer suggests, for completeness it can be beneficial to explore other options.  While we did not have time to run both during the rebuttal period, we provide new results with Cluster-GCN below. \n\n| model       | arxiv      | IGB-tiny   | products   | papers100M | MAG240M    | IGB-full   |\n| ----------- | ---------- | ---------- | ---------- | ---------- | ---------- | ---------- |\n| Cluster-GCN | 68.11\u00b10.7  | 71.03\u00b10.1  | 74.4\u00b10.4   | 54.0\u00b10.6   | OOM        | OOM        |\n| MuseGNN     | 72.50\u00b10.19 | 72.80\u00b10.02 | 80.47\u00b10.16 | 66.82\u00b10.02 | 67.26\u00b10.06 | 60.21\u00b10.18 |\n\nNote that if the reviewer feels it is important, we can easily add GraphSAINT as well to a later revision, although this approach is not seen on the leaderboards for any of the largest datasets. \n\n\n**Comment:**\n*[Weakness 2.] The authors did not conduct experiments on datasets that can illustrate the importance of unfolded GNNs.*\n\n**Response:**\nThis is actually true by design; please see our general response to all reviewers presented above which addresses this issue."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8988/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700261091297,
                "cdate": 1700261091297,
                "tmdate": 1700261091297,
                "mdate": 1700261091297,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EG0vuLqnVX",
            "forum": "uYTaVRkKvz",
            "replyto": "uYTaVRkKvz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8988/Reviewer_PP1g"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8988/Reviewer_PP1g"
            ],
            "content": {
                "summary": {
                    "value": "A sampling-based energy function and scalable GNN layers, MuseGNN, is proposed. Convergence guarantees (under certain assumptions) are provided. Experiments on the large dataset IGB-full and MAG240M demonstrate the scaleability of MuseGNN and its competitive performance with GATs (combined with neighbourhood sampling)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The background of unfolding GNNs, related energy functions, etc. is well explained.\n- Advantages of the proposed offline sampling approach are discussed that also enable a convergence analysis.\n- Assuming that there exists a unique solution, a theoretical convergence analysis derives a convergence rate of $O(1/\\sqrt(t) + \\exp(-Ck))$.\n- The method enables training on very large graphs and achieves state-of-the-art performance on IGB-full."
                },
                "weaknesses": {
                    "value": "- Novelty: What are the practical benefits of the proposed sampling method over neighbourhood sampling?\n- MuseGNN is integrated into the energy function and thus a less general approach than neighborhood sampling, which can be applied to most message passing GNNs.\n- In comparison to GAT with neighbourhood sampling, the proposed MuseGNN requires a few more training epochs. Also the performance of GAT with neighbourhood sampling is often competitive. \n- Significance intervals are not provided for GAT with neighbourhood sampling on the very large graphs, yet, they are computed for the proposed MuseGNN. The reasoning by the authors are long run times. However, GAT with neighbourhood sampling is reported to be slightly faster than MuseGNN. Thus, significance intervals should also be attainable in this case. Without them, the statement that MuseGNN achieves a new state of the art is not actually accurate.\n- The convergence analysis relies on the strong assumption of a unique solution.\n\n\nMinor points:\n- The discussed concept of interpretability seems of minor relevance, as it does not relate to explaining how trained GNNs solve a task. It actually refers more to a concept of consistency over samples. What should the practical benefit of this be?"
                },
                "questions": {
                    "value": "-What are the limitations of neighborhood sampling that require the development of the proposed offline sampling scheme? From a practical point of view, what are the actual disadvantages of NS that motivate the development of MuseGNN?\n-Please add significance intervals for GAT with neighbourhood sampling for GAT (NS) and SAGE (NS) and GCN (NS). \n- What are the number of training epochs for GCN (NS)?\n- What are the actual training and inference times for the baselines and MuseGNN? This is also a relevant question because the sampling schemes themselves could take different amounts of time.\n- What are the memory requirements of MuseGNN versus the baselines?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8988/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698791870600,
            "cdate": 1698791870600,
            "tmdate": 1699637131349,
            "mdate": 1699637131349,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZYlzgbYWxe",
                "forum": "uYTaVRkKvz",
                "replyto": "EG0vuLqnVX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8988/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8988/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer PP1g (Part I)"
                    },
                    "comment": {
                        "value": "Thanks for the detailed comments.  We address each point in turn below.\n\n**Comment:**\n*[From 1st, 2nd Weakness and 1st Question] Novelty: What are the limitations of neighborhood sampling that require the development of the proposed offline sampling scheme? From a practical point of view, what are the actual disadvantages of NS that motivate the development of MuseGNN?*\n\n**Response:**\nJust to clarify, we are not proposing a new sampling scheme per se.  Rather, we are integrating an existing offline sampling approach within a new role, namely, as the basis for the new energy function from Eq.(4), which then leads to a new scalable unfolded GNN architecture and MuseGNN.  In this sense, the motivation of MuseGNN is unrelated to any disadvantages of NS.  On this last point though, some additional context may be useful.\n\nThere are two completely separate aspects of sampling considered in our paper that are relevant to understanding MuseGNN:  \n1. There is the distinction between online vs offline sampling as first mentioned in Section 2.3, where the online version involves dynamically obtaining a new set of random samples *during each training epoch*, while the offline version is predicated on *a fixed set of samples obtained prior to training*.  We chose the latter for MuseGNN, and the reasons are detailed in Section 3.1. \n2. There is the actual design of the sampler itself, which can be applied in *either* an online or offline fashion.  Neighbor sampling (NS) is arguably the most common choice in the literature, but there are many others such as those used by Cluster-GCN or Shadow-GNN etc.  Regardless, as pointed out in our paper, MuseGNN can adopt *any* such sampler, and the convergence guarantees introduced in Section 5 will still hold.\n\nFrom this distinction then, we hope the nature of MuseGNN relative to prior uses of sampling is more clear, but we are happy to provide further details if needed.\n\n\n**Comment:**\n*[From 4th Weakness and 1st Question (last part)] Please add significance intervals for GAT with neighbourhood sampling for GAT (NS) and SAGE (NS) and GCN (NS).*\n\n**Response:**\nWe apologize that the original submission inadvertently did not provide sufficient details regarding these numbers.  In fact, for the two largest datasets, we relied on numbers from the official IGB/OGB leaderboards for GAT (NS), SAGE (NS), and GCN (NS); we did not compute them all from scratch because of the significant time and cost involved (a common practice in the GNN literature).  For example, just a single training run for these models requires a very large GPU instance (such as a `p4dn.24xlarge` AWS instance) with extra large RAM (over 1TB).  So instead we just focused on computing MuseGNN results for these datasets without bandwidth for multiple runs across all baseline models as would be needed for significance intervals.  We will include these details in the revision.  Thanks for pointing out our oversight.\n\nIn contrast, for speed results (i.e., epoch timing) alone we do not require training until convergence. Instead, we only need to average over a few epochs to obtain the results in Table 3, which is much more affordable.\n\n\n**Comment:**\n*[2nd Question] What are the number of training epochs for GCN (NS)?*\n\n**Response:**\nThe number of epochs changes from dataset-to-dataset, and can even vary a bit from trial-to-trial due to the use of early stoppage (based on the validation set) as commonly adopted during training. As reference though, for ogbn-arxiv it is roughly 30 epochs, for IGB-tiny about 20 epochs, and for ogbn-papers100M it is about 40 epochs.  Incidentally, MuseGNN is similar (e.g., roughly 40 epochs for ogbn-papers100M). \n\n\n**Comment:**\n*[3rd Question] What are the actual training and inference times for the baselines and MuseGNN? This is also a relevant question because the sampling schemes themselves could take different amounts of time.*\n\n**Response:**\nThe average training time for each epoch is reported in Table 3, where MuseGNN is shown to be roughly comparable to the commonly-used NS baselines (the total numbers of epochs are similar; see above).  And indeed, while different sampling schemes may require different amounts of time, NS is arguably the fastest (at least conditioned on those in common use), e.g., in our tests, it is faster than ShadowKHop on baseline GNNs.  Hence there are unlikely to be any significantly faster variants than what we have already shown in Table 3.  Please also see Sections B.2 and B.3 for additional ablations related to sampling.\n\nAs for inference, although we did not report specific results in the paper, they follow the same trend as training.  Basically, both MuseGNN and the baseline GNNs all skip the backward pass and remain similar in run-time for the forward inference pass.  However, if the reviewer feels these numbers are important, we can easily add them to a revision."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8988/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700260913356,
                "cdate": 1700260913356,
                "tmdate": 1700260913356,
                "mdate": 1700260913356,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s4zHZGzUXP",
                "forum": "uYTaVRkKvz",
                "replyto": "WVZYRFFnD0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8988/Reviewer_PP1g"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8988/Reviewer_PP1g"
                ],
                "content": {
                    "title": {
                        "value": "No further questions"
                    },
                    "comment": {
                        "value": "I thank the authors for their response. I do not have any open questions. \nThe authors might want to consider revising their paper within the discussion period when they still have the chance, as their strong emphasis on interpretability in the current form seems problematic.  \n\nWhile I appreciate the contribution by the authors to scale unfolded GNNs to larger graphs, the conceptual novelty with respect to the literature seems limited.\nBased on discussions with other reviewers, I will potentially adapt my score at a later stage, as the spread of opinions is quite high."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8988/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690956490,
                "cdate": 1700690956490,
                "tmdate": 1700690956490,
                "mdate": 1700690956490,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wmx40u7ybF",
                "forum": "uYTaVRkKvz",
                "replyto": "EG0vuLqnVX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8988/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8988/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up Response to Reviewer PP1g"
                    },
                    "comment": {
                        "value": "Thanks for the follow-up comments. Per the reviewer's suggestion, we have quickly uploaded a new version of the paper with interpretability de-emphasized.  This mainly involved editing Section 2.2 and modifying a few other phrases.  There is now no overlap/conflation with existing GNN work on interpretability, with supporting claims for unfolded GNNs deferred to prior work as needed.  The intended message of our paper is surely much more clear now, good suggestion.\n\nIn terms of conceptual novelty, we emphasize that, out of all the possible options for scaling GNNs (and unfolded GNNs in particular), we uniquely offer the following:\n1. Introduce a novel offline-sampling-based energy formulation, including a penalty term for favoring consistency among the  different node representations with the same node index across sampled subgraphs.  To our knowledge, this form of regularization has never been used before.\n2. Derive an unbiased estimate of the full graph energy for certain choices of sampling (Proposition 3.1).\n3. Accommodate new convergence guarantees that are *independent of the specific choice of sampling method itself* (Section 5), and account for sampling and the regularization mentioned above.  \n4. Demonstrate SOTA performance on the two largest publicly-available graph benchmarks, and competitive performance on others using just a single, simple/transparent architecture."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8988/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700721279218,
                "cdate": 1700721279218,
                "tmdate": 1700732654433,
                "mdate": 1700732654433,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Nu9RVbDSNz",
            "forum": "uYTaVRkKvz",
            "replyto": "uYTaVRkKvz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8988/Reviewer_ik1J"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8988/Reviewer_ik1J"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies how to scale unfold GNNs to large-scale graph benchmarks while maintaining acceptable computational and memory overheads, aligning with common GNN alternatives. Specifically, the authors integrate offline subgraph sampling into the energy function to propose a novel sampling-based energy function and derive convergence guarantees for the novel objective, demonstrating its theoretical feasibility. The authors also empirically demonstrate the effectiveness of MuseGNN constructed using the subgraph sampling-based energy function across datasets of widely varying sizes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This article is well-organized.\n2. The author has presented sufficient theoretical proof to ensure the convergence of the novel sampling-based energy function during training.\n3. The proposed MuseGNN framework achieves state-of-the-art performance on the largest graph benchmark IGB-full."
                },
                "weaknesses": {
                    "value": "1. The paper emphasizes \\textbf{Interpretable} in the title and illustrates the explanality of energy minimizers. The key concern is the energy minimization is not aligned with the common concept of GNNs interpretation. For example, some of existing works of interpretable GNNs highlight neighborhood structure leading to the node label classification. The superficial conclusion of node embedding information stemming from the node itself or neighbors makes no sense to the real interpretable applications. \n2. In addition, the absence of case study to validate the explanations provided by the energy function raises questions about one of the paper's main claims.\n3. The incorporation of the energy minimization into GNNs is not novel. The Dirichilet energy of node embeddings, i.e., the second term in Eq.(2), has been extensively studied in graph domains and GNNs. For example, it has been used to analyze over-smoothing issue in [1]. Following this work, several novel GNNs to optimize this energy function have been proposed. The constraint of closeness to base model embedding, i.e., the first term of Eq. (2), has been implicitly included in models like SIGN [2], SAGN [3], DAGNN[4]. \n\n[1] Cai, Chen, and Yusu Wang. \"A note on over-smoothing for graph neural networks.\" arXiv preprint arXiv:2006.13318 (2020).\n[2] Rossi, Emanuele, et al. \"Sign: Scalable inception graph neural networks.\" arXiv preprint arXiv:2004.11198 7 (2020): 15.\n[3] Sun, Chuxiong, Hongming Gu, and Jie Hu. \"Scalable and adaptive graph neural networks with self-label-enhanced training.\" arXiv preprint arXiv:2104.09376 (2021).\n[4] Liu, Meng, Hongyang Gao, and Shuiwang Ji. \"Towards deeper graph neural networks.\" Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining. 2020."
                },
                "questions": {
                    "value": "1. Please address the previous concerns.\n2. Present more results about ablations on $\\gamma$, i.e. results on IGB-full, the dataset where traditional GNNs with sampling technique obviously suffer a significant performance degradation, to further demonstrate the effectiveness of $M$.\n3. In Proposition 3.1, what is the definition of l(M)? What is the purpose of this proposition?\n4. According to my understanding, the main novelty of this paper is to incorporate the shared summary embedding matrix $M$ into energy to facilitate controllable linkage between the multiple embeddings that may exist for a given node appearing in different subgraphs (Because the Formula 6 is a commonly used technique for large-scale training). But existing ablation on $\\gamma$ are not sufficient to demonstrate the general effectiveness of this additional constraint term."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8988/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8988/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8988/Reviewer_ik1J"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8988/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698801754402,
            "cdate": 1698801754402,
            "tmdate": 1699637131226,
            "mdate": 1699637131226,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "s30iBPhyxC",
                "forum": "uYTaVRkKvz",
                "replyto": "Nu9RVbDSNz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8988/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8988/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ik1J (Part I)"
                    },
                    "comment": {
                        "value": "Thanks for the detailed comments.  We address each point in turn below.\n\n**Comment:**\n*[From Weakness 1.] The paper emphasizes Interpretable in the title and illustrates the explainability of energy minimizers. The key concern is the energy minimization is not aligned with the common concept of GNNs interpretation. For example, some of existing works of interpretable GNNs highlight neighborhood structure leading to the node label classification. The superficial conclusion of node embedding information stemming from the node itself or neighbors makes no sense to the real interpretable applications.*\n\n**Response:**\nWe apologize for any confusion the title may have caused relative to existing work on interpretable GNNs.  Please see our general comments to all reviewers above which directly address this issue.  In any event, we are happy to modify the title and framing to increase clarity.\n\n\n**Comment:**\n*[From Weakness 2.] In addition, the absence of case study to validate the explanations provided by the energy function raises questions about one of the paper's main claims.*\n\n**Response:**\nPlease see the general comments above to all reviewers which address this issue in depth.  But in brief, our starting assumption is that prior work has *already* demonstrated the value of unfolded GNNs formed from interpretable energy functions; we make no claims of demonstrating this ourselves.  Instead, our focus herein is entirely on scaling this class of GNN models to the largest possible graphs, with provable convergence guarantees.\n\n\n**Comment:**\n*[From Weakness 3.] The incorporation of the energy minimization into GNNs is not novel.*\n\n**Response:**\nAbsolutely, we 100% agree. These models were originally invented, motivated, and validated in prior work, e.g., the many references we cite in Sections 2.1 and 2.2.  That being said, our contribution is *not* the invention or motivation of these models, rather, it is scaling these models to huge graphs with convergence guarantees that account for the required sampling involved. \n\n\n**Comment:**\n*[From Weakness 3.] The Dirichilet energy of node embeddings, i.e., the second term in Eq.(2), has been extensively studied in graph domains and GNNs. For example, it has been used to analyze over-smoothing issue in [1]. Following this work, several novel GNNs to optimize this energy function have been proposed. \n\\[1\\] Cai, Chen, and Yusu Wang. \"A note on over-smoothing for graph neural networks.\" arXiv preprint arXiv:2006.13318 (2020).*\n\n**Response:**\nExactly, the Dirichlet energy, often in combination with other terms as in Eq.(2) of our submission, is the starting point for inspiring a wide variety of existing unfolded GNN architectures, e.g., see references in Section 2.1.  And indeed as the reviewer correctly suggests (and the reviewer provided reference [1] shows), this Dirichlet energy can be used to analyze oversmoothing.  In fact, this is a good example of the very type of interpretability that these energy-based models possess.  In this case, the Dirichlet energy alone (akin to the second term in Eq.(2)) leads to oversmoothing since constant node embeddings (i.e., oversmoothing) will achieve the minimum.  In contrast, it has also been observed in prior work that when additional terms involving a base model are included as in Eq.(2), oversmoothing no longer happens precisely because global minimizers transparently no longer oversmooth. Thanks for pointing out reference [1], this is a great example to add to our related work.\n\n\n**Comment:**\n*[From Weakness 3.] The constraint of closeness to base model embedding, i.e., the first term of Eq. (2), has been implicitly included in models like SIGN [2], SAGN [3], DAGNN[4].*\n\n**Response:**\nWhile these architectures may incorporate some form of base model or precomputed features, to our knowledge the forward pass in each case does not actually minimize an explicit energy function.  Hence we cannot directly interpret their output embeddings as energy function minimizers as we can with the GNN architectures formed by minimizing Eq.(2).  Even so, they can be cited for added context if the reviewer suggests."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8988/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700260183428,
                "cdate": 1700260183428,
                "tmdate": 1700260183428,
                "mdate": 1700260183428,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RJcNENsxYw",
                "forum": "uYTaVRkKvz",
                "replyto": "Nu9RVbDSNz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8988/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8988/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ik1J (Part II)"
                    },
                    "comment": {
                        "value": "**Comment:**\n*[From Question 2.] Present more results about ablations on $\\gamma$, i.e. results on IGB-full, to further demonstrate the effectiveness of $M$.*\n\n**Response:**\nIGB-full is massive and we do not have the time or computing resources to conduct a careful ablation during the short rebuttal period.  However, for now, we can nonetheless use IGB-tiny and IGB-medium as surrogates, as well as ogbn-products as another reference point (note that IGB-medium is actually quite large, with more labeled nodes than even ogbn-papers100M).  From the new ablation table below, we observe that it is indeed possible to improve performance with $\\gamma > 0$ in all three cases. (Note that for ogbn-products, this accuracy is actually better than reported in our original submission, where we did not carefully explore tuning $\\gamma$.)\n\n| dataset       | $\\gamma=0$ | $\\gamma=0.1$ | $\\gamma=0.5$ |\n| ------------- | ---------- | ------------ | ------------ |\n| IGB-tiny      | 72.66      | 72.78        | **72.81**    |\n| IGB-medium    | 75.18      | 75.80        | **75.83**    |\n| ogbn-products | 80.42      | 80.92        | **81.23**     |\n\n\n**Comment:**\n*[From Question 3.] In Proposition 3.1, what is the definition of $\\ell(M)$? What is the purpose of this proposition?*\n\n**Response:**\nThe function $\\ell$ is defined by Eq.(2), and $\\ell(M)$ merely refers to Eq.(2) with $M$ as the input argument.  The paragraph directly above Proposition 3.1, where we describe how the role of $M$ equates to $Y$ when $\\gamma = \\infty$, provides context for why we introduce $\\ell(M)$.\n\nIn terms of purpose, Proposition 3.1 serves a key role by quantifying the relationship between ideal full-graph training as instantiated via Eq.(2), and offline sampling using Eq.(4) as used by MuseGNN in our submission.  More precisely, when $\\gamma = \\infty$, Eq.(4) reduces to Eq.(7).  From here, Proposition 3.1 then proves that Eq.(7) becomes equivalent to Eq.(2) in expectation (i.e., it is unbiased), provided we conduct sampling independently as described in the proposition statement. We hope this helps to clarify the value of Proposition 3.1, and we are happy to update the draft if necessary to do so. (The end of Section 3.2 very briefly mentions these ideas, but further detail could be helpful.)\n\n\n**Comment:**\n*[From Question 4.] According to my understanding, the main novelty of this paper is to incorporate the shared summary embedding matrix $M$ into energy to facilitate controllable linkage between the multiple embeddings that may exist for a given node appearing in different subgraphs (Because the Formula 6 is a commonly used technique for large-scale training).*\n\n**Response:**\nOur novelty extends beyond incorporating the shared summary embedding matrix $M$ into the stated energy as follows: \n* Although offline sampling has been proposed previously, to our knowledge Eq.(6) has not been used in prior work as an energy function for producing GNN layers through minimization (our focus). Please also see Section 3.1 for further details regarding why we chose offline sampling, which differs from prior work. \n* Our convergence results, which cover both the $\\gamma = 0$ case, meaning Eq.(6), and the more general $\\gamma > 0$ case are new.  Moreover, by relying on offline sampling these results are agnostic to the underlying sampling method itself. \n* Finally, even if we exclude consideration of the empirical efficacy of $\\gamma > 0$, Proposition 3.1 quantifies a precise regime whereby our sampling-based energy provides an unbiased estimator of the full-graph energy.  This is notable because, generally speaking, it is infeasible to establish notions of unbiasedness in arbitrary GNN models instantiated with sampling (with the exception of unbiasedness narrowly defined within a single model layer).\n\n\n**Comment:**\n*[From Question 4.] But existing ablation on $\\gamma$ are not sufficient to demonstrate the general effectiveness of this additional constraint term.*\n\n**Response:**\nRegarding additional ablations, please see our response to Question 2 above."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8988/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700260515825,
                "cdate": 1700260515825,
                "tmdate": 1700738686650,
                "mdate": 1700738686650,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]