[
    {
        "title": "Connect, Collapse, Corrupt: Learning Cross-Modal Tasks with Uni-Modal Data"
    },
    {
        "review": {
            "id": "AbwZbxUEc0",
            "forum": "ttXg3SKAg5",
            "replyto": "ttXg3SKAg5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission813/Reviewer_XDw2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission813/Reviewer_XDw2"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the space's geometry of the features learned by contrastive learning and finds there is a modality gap in such feature space. To bridge the modality gap, based on the analysis of the geometry, they propose a three-step method called C^3 (Connect, Collapse, Corrupt). They conduct experiments on zero-shot image/audio/video captioning and text-to-image generation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The discussion about the gradient direction in contrastive optimization in Lemma 1 sheds light on how the features learned by contrastive learning are processed.\n2. The theoretical analysis and experiment results of the alignment noise are reasonable and effective."
                },
                "weaknesses": {
                    "value": "1. The abstract and introduction emphasize the importance of the modality gap of the feature space, \nhowever, the results in the experiments seem show that the key component is the alignment noise. \nFor instance, in Table 2, C_2^2 (only use connect and corrupt) can achieve a similar performance as the C^3.\nCompared to C^1, only adding collapse boost 16.3 but 41 when only adding alignment noise.\n\n2. In section 3.3, it provides some statistic values to show the space geometry. \nI think such statistics may not actually reveal the space geometry since it averages all possible values.\nIs that better to provide a histogram of such statistics to demonstrate the space geometry?\nFurthermore, based on the constant modality gap analysis, is that the E_{i,j}[cos(d_i,d_j)] should have a value close to 1 since they should be parallel except for the noise effect.\n\n3. The experiments are conducted on generation tasks, the quantitative performance is similar for the method and baseline,\nwhile in the qualitative examples, I can not tell which method is better based on three examples. It will be better to provide more qualitative examples."
                },
                "questions": {
                    "value": "1. In the analysis of the space geometry of contrastive features, the paper proposes that the modality gap is a constant vector and orthogonal to each modality. But it can not be derived easily that why the modality gap is the constant vector from the dimension collapse as mentioned in section 3.1. Are there any formal propositions for the constant modality gap vector in the initialization stage?\n\n2. The paper aims to mitigate the modality gap between feature space of different modalities, but in the generation task or encoder/decoder-based architecture, is that the closeness of the features from different modalities indicate a better generation performance? I think it would be better to refer to previous works or conduct this kind of experiment to show the assumption of this paper is true in real applications."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission813/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission813/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission813/Reviewer_XDw2"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission813/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698507269468,
            "cdate": 1698507269468,
            "tmdate": 1700577013844,
            "mdate": 1700577013844,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gTpFzSRO2B",
                "forum": "ttXg3SKAg5",
                "replyto": "AbwZbxUEc0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission813/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank Reviewer XDw2 for reviewing our paper and providing helpful feedback on our work. We address Reviewer XDw2\u2019s concerns below.\n\n**Histogram of statistics revealing the space geometry**\n\n> In section 3.3, it provides some statistic values to show the space geometry. I think such statistics may not actually reveal the space geometry since it averages all possible values. Is that better to provide a histogram of such statistics to demonstrate the space geometry? Furthermore, based on the constant modality gap analysis, is that the $E_{i,j}[\\cos(d_i,d_j)]$ should have a value close to 1 since they should be parallel except for the noise effect.\n\nThank you for the suggestion and question. We have now included the histogram, as well as a detailed explanation of how to interpret these statistics in Appendix C.\n\nFor $E_{i,j}[\\cos(d_i,d_j)]$, because there is a noise entangled in $d_i = c_\\perp + \\epsilon_i$ and $d_j = c_\\perp + \\epsilon_j$, the cosine similarity will not be 1. However, if we compute the group-level $E_{i,j}[\\cos(d_{c_i},d_{c_j})]$, where $c_i$ and $c_j$ are 100 randomly sampled images, the value reaches 0.99. We provide a detailed explanation in Appendix C.\n\n**Why corrupt ($C_2^2$) seems more effective than collapse ($C_1^2$)?**\n\n> The abstract and introduction emphasize the importance of the modality gap of the feature space, however, the results in the experiments seem show that the key component is the alignment noise. For instance, in Table 2, $C_2^2$ (only use connect and corrupt) can achieve a similar performance as the $C^3$. Compared to $C^1$, only adding collapse boost 16.3 but 41 when only adding alignment noise.\n> \n\nThanks for the question. While $C_2^2$ (corrupt) seems to be more effective than $C_1^2$ (collapse) in Table 2 (image captioning on CLIP embedding space), they seem to be similarly effective in Table 3 (text-to-image generation on CLIP embedding space) and Table 4 (image / video / audio captioning on ImageBind embedding space).\n\nWe hypothesize that the greater effectiveness of $C_2^2$ is because $C_2^2$ injects random noise in the embedding space during training, including the direction of the modality gap, which potentially reduces the model's sensitivity to this gap. Nevertheless, given the substantial size of the modality gap, $C_1^2$ remains necessary to diminish this gap, thereby enhancing the overall performance of $C^3$. \n\nIt is also worth noting that $C^3$ adds near-zero additional computational cost over baselines. As our experiments demonstrate its effectiveness across a wide range of tasks, it can be a template for future work.\n\n**Significance of the quantitative improvement and more qualitative examples**\n\n> The experiments are conducted on generation tasks, the quantitative performance is similar for the method and baseline, while in the qualitative examples, I can not tell which method is better based on three examples. It will be better to provide more qualitative examples.\n> \n\nThank you for the suggestion. We have added more qualitative examples for image captioning and text-to-image generation in Appendix Figure 10 and Figure 11. We hope these examples provide additional evidence of our method\u2019s effectiveness.\n\nWe also emphasize that the improvement of our method is significant. Per Reviewer bNKP\u2019s suggestion, we have run $C^3$ in Table 2 three times with random seeds 1234, 5678, and 910 and reported the numbers below. There is a very small variance across different runs, and $C^3$ consistently outperforms all the baselines.\n\n| Seed | BLEU-1 | BLEU-4 | METEOR | ROUGE-L | CIDER | SPICE |\n| --- | --- | --- | --- | --- | --- | --- |\n| 1234 | 71.0 | 27.6 | 25.0 | 52.0 | 93.2 | 18.3 |\n| 5678 | 71.1 | 27.6 | 24.9 | 52.0 | 93.6 | 18.4 |\n| 910 | 70.9 | 27.8 | 25.0 | 52.1 | 93.0 | 18.2 |\n| Avg | 71.0 | 27.7 | 25.0 | 52.0 | 93.3 | 18.3 |\n| Std | 0.1 | 0.1 | 0.0 | 0.0 | 0.3 | 0.1 |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700406156879,
                "cdate": 1700406156879,
                "tmdate": 1700406156879,
                "mdate": 1700406156879,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6m73b6IteW",
                "forum": "ttXg3SKAg5",
                "replyto": "vdBoTDKHrJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission813/Reviewer_XDw2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission813/Reviewer_XDw2"
                ],
                "content": {
                    "title": {
                        "value": "Reply"
                    },
                    "comment": {
                        "value": "Thanks for the explanation and demonstration.\n\nI think some issues have been addressed well like the W1,3, Q2. \n\nFor W2, I think more analysis both in terms of methodology and experiments that can explain why corrupt is so effective will be helpful for me to understand the question.\n\nFor Q1, I realize why the modality gap is constant if the image and text share no effective dimensions. And the example just follows that there are no overlapped effective dimensions. But are that some propositions to guarantee this phenomenon?\n\nOverall, I will change the score from 5 to 6 based on the current rebuttal."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576997655,
                "cdate": 1700576997655,
                "tmdate": 1700576997655,
                "mdate": 1700576997655,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LgTYk59lDT",
                "forum": "ttXg3SKAg5",
                "replyto": "AbwZbxUEc0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission813/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer XDw2,\n\nThank you again for providing us with constructive feedback. We appreciate your consideration of our response and willingness to adjust your rating based on our improvements!\n\nHere we provide additional clarification to address your concerns:\n\n> For Q1, I realize why the modality gap is constant if the image and text share no effective dimensions. And the example just follows that there are no overlapped effective dimensions. But are that some propositions to guarantee this phenomenon?\n> \n\nThanks for the question. We hope to clarify that\u00a0**the theory still works when image and text share effective dimensions**.\n\nHere we provide a modified example to help understand the process:\n\nSuppose there are two image-text pairs in 3D space; both the 1st and 2nd dimensions of the image embeddings are effective, and the same applies to text embeddings, and their 3rd dimension both are ineffective.\n\nDuring initialization, let us denote these embeddings as $x_1, x_2, y_1, y_2 \\in \\mathbb{R}^3$:\n\n$x_1 = [0.7, 0.5, -0.5]$ (the 3rd dimensions are constant across $x_i$)\n\n$x_2 = [-0.7, -0.5, -0.5]$ (the 3rd dimensions are constant across $x_i$)\n\n$y_1 = [0.5, 0.7, 0.5]$ (the 3rd dimensions are constant across $y_i$)\n\n$y_2 = [-0.5, -0.7, 0.5]$ (the 3rd dimensions are constant across $y_i$)\n\nDuring optimization, only the union of effective dimensions (i.e., 1st and 2nd) will be aligned, and the intersection of ineffective dimensions (i.e., 3rd) will remain constant:\n\n$x_1 = [0.6, 0.6, -0.5]$ (the 3rd dimension remains constant because of no gradient)\n\n$x_2 = [-0.6, -0.6, -0.5]$ (the 3rd dimension remains constant because of no gradient)\n\n$y_1 = [0.6, 0.6, 0.5]$ (the 3rd dimension remains constant because of no gradient)\n\n$y_2 = [-0.6, -0.6, 0.5]$ (the 3rd dimension remains constant because of no gradient)\n\nTherefore, we have a constant gap with a distance of 1.0, which is also orthogonal to the embedding spans.\n\nInstead,\u00a0**the key assumption of the theory is that image and text share ineffective dimensions, which is empirically verified in various models such as CLIP.**\u00a0For example, in CLIP, the image has only 25 effective dimensions, and the text has only 230 effective dimensions. Therefore, **the image and text must share ineffective dimensions in 512D space**, as they sum up to at most 255 effective dimensions. Therefore, these ineffective dimensions will be fully determined by initialization, and will not change during optimization, creating a constant orthogonal modality gap.\n\nPlease let us know if you have further questions or concerns!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638852597,
                "cdate": 1700638852597,
                "tmdate": 1700639326747,
                "mdate": 1700639326747,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MSiRxwKJXd",
            "forum": "ttXg3SKAg5",
            "replyto": "ttXg3SKAg5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission813/Reviewer_CpsN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission813/Reviewer_CpsN"
            ],
            "content": {
                "summary": {
                    "value": "The paper provides a theoretical understanding of the geometry of the multi-modal contrastive representation space, which is related to the modality gap and alignment noise. Based on this, it presents a new 3-stage framework, called C3 (Connect, Collapse, Corrupt), for solving cross-modal tasks using single-modal data. C3 can effectively bridge the modality gap and enhance the interchangeability of embeddings in the shared representation space. The paper demonstrates the empirical effectiveness of C3 by showing that it achieves state-of-the-art results in various cross-modal tasks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Theoretical Insight: The paper's theoretical understanding of the geometry of the multi-modal contrastive representation space is a significant contribution. It also helps shed light on the challenges related to the modality gap (by showing that it is attributed to the joint effect of initialization and optimization), which is the key issue in multi-modal/cross-modal learning.\n\n2. C3 Algorithm: The rationale behind the proposed C3 method is sound and well-explained. Even though each individual step has been explored by previous work, the combination of them leads to very competitive performance on a variety of tasks compared with recent strong baselines (as shown in Table 2-3). The paper also provides comprehensive ablation studies and qualitative examples to understand the effect of each component.\n\n3. Presentation: The presentation is clear, and the ideas are easy to follow. The visuals also help illustrate the effectiveness of the proposed method. The current submission does not include code, hopefully the authors can release them later to facilitate future research."
                },
                "weaknesses": {
                    "value": "The proposed C3 algorithm has limited novelty on its own given that each step has been studied in previous work. However, the combination of these steps is new and well-motivated by the theoretical framework developed in this paper, which mitigates the lack-of-novelty issue."
                },
                "questions": {
                    "value": "1. How is the \"Collapse\" step implemented (i.e., computing e_x' and e_y')? Is it the same as batchnorm?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission813/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission813/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission813/Reviewer_CpsN"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission813/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698608970854,
            "cdate": 1698608970854,
            "tmdate": 1699636008540,
            "mdate": 1699636008540,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LTjaQOwy4g",
                "forum": "ttXg3SKAg5",
                "replyto": "MSiRxwKJXd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission813/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank Reviewer CpsN for their positive comments and for providing thoughtful feedback on our work. We address Reviewer CpsN\u2019s concerns below.\n\n**Novelty of $C^3$ algorithm**\n\n> The proposed $C^3$ algorithm has limited novelty on its own given that each step has been studied in previous work. However, the combination of these steps is new and well-motivated by the theoretical framework developed in this paper, which mitigates the lack-of-novelty issue.\n\nThank you for the question. Yes, as you mentioned, while existing works have empirically proposed collapse and corrupt operation separately, we differ from them in three aspects:\n\n1. Our method is built on the theoretical understanding of multi-modal contrastive representation space geometry.\n2. We unified these insights and showed the combination of them leads to superior performance than each of them.\n3. We proved the effectiveness of our method in a wide range of tasks, modalities, data, and embedding spaces, which can become a standard recipe for all future works built on multi-modal contrastive embedding space.\n\n**Collapse operation implementation**\n\n> How is the \"Collapse\" step implemented (i.e., computing e_x' and e_y')? Is it the same as batchnorm?\n\nThanks for the question. The \u201ccollapse\u201d step can be viewed as a distribution norm, where the image embedding mean $\\bar{e}_x$ and text embedding mean $\\bar{e}_y$ are pre-computed on the entire training set and subtracted during training and inference. The batch norm is an approximation with a larger variance. We provided an algorithm formulation of $C^3$ in Appendix C for further clarification.\n\n**Codebase**\n\n> The current submission does not include code, hopefully the authors can release them later to facilitate future research.\n\nThanks for the question. We included an anonymous GitHub link in the Page 10 reproducibility statement, which should be able to reproduce all the results. Reviewer Vv1o has checked this codebase and commented that \u201cthe codebase looks carefully developed and seems free from glaring bugs\u201d.\n\nThank you again for your feedback, which is very helpful in improving the paper. We hope the above responses and changes to our manuscript adequately address your concerns. Please let us know if you have further questions or concerns!"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700405999436,
                "cdate": 1700405999436,
                "tmdate": 1700405999436,
                "mdate": 1700405999436,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FpLuOvr6yP",
                "forum": "ttXg3SKAg5",
                "replyto": "LTjaQOwy4g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission813/Reviewer_CpsN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission813/Reviewer_CpsN"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' response and my concerns are addressed. I'm happy to keep my rating."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694386433,
                "cdate": 1700694386433,
                "tmdate": 1700694386433,
                "mdate": 1700694386433,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AlGdVejbNQ",
            "forum": "ttXg3SKAg5",
            "replyto": "ttXg3SKAg5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission813/Reviewer_Vv1o"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission813/Reviewer_Vv1o"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates the geometry of embedding spaces obtained through multi-modal contrastive learning (e.g. CLIP), collecting interesting insights and using these to motivate a simple-yet-effective 3-step approach to improve the performance of cross-modal tasks learned using uni-modal data.  In particular, the study suggests, both empirically and theoretically, that the difference between embeddings from different modalities originates from two components: i) a modality gap caused by ineffective dimensions being initialized differently in the two modalities and remaining constant during optimization, and ii) alignment noise that can be approximated as gaussian. The paper then suggests reducing this gap by centering the embeddings at both training time and inference time and adding noise during training. Experiments finally show that the suggested modifications result in state-of-the-art results across a wide set of cross-modal tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "### Originality\n\n- The paper studies the poorly-understood geometry of latent spaces obtained through multi-modal contrastive learning, building on top of existing works and integrating new insights into an overall recipe to improve cross-modal tasks using such spaces.\n- While two out of 3 steps in the CCC method are not novel, the motivation behind the overall framework is a valid contribution, as well as the need of collapsing the ineffective dimensions by subtracting the mean to the embeddings.\n- The finding of the modality gap being orthogonal to the text and image spaces is interesting and well motivated.\n\n### Clarity\n\n- The paper is well written and pleasing to read.\n- The concepts are explained both rigorously and more colloquially.\n- The experiments are well motivated, and their results properly discussed.\n\n### Significance\n\n- The theoretical framework looks solid: the difference of initialization and the lack of gradients for the ineffective dimensions convincingly explains the modality gap.\n- The empirical analyses make intuitive sense.\n- The framework results in improvements over a wide set of tasks (image/audio/video captioning and text-to-image generation), proving its general applicability.\n- The codebase looks carefully developed and seems free from glaring bugs.\n\nOverall, the paper tackles an extremely interesting question that many practitioners share: \u201chow does, and possibly how should, a multi-modal space look like?\u201d and attempt to characterize its geometry with simple yet convincing tools. Both the theoretical and empirical analyses make intuitive sense, and the empirical results on the considered tasks confirm the utility of its findings."
                },
                "weaknesses": {
                    "value": "- The discussion on the alignment noise could be improved: in particular, the results in Table 1 are left for the reader to infer. The same statistics could also be easily computed upon any other modality combination in the appendix, it would be useful to see if it still applies."
                },
                "questions": {
                    "value": "- Since the modality gap is due to the dimensional collapse, would reducing the dimensionality to the effective one help overcoming the issue?\n- Is there any relation between the decomposition of the modality gap with the content-style-modality specific decomposition assumed e.g. in [1]? Briefly, each latent vector in a multi-modal contrastive learning space is there assumed to have a part that is shared across modalities, i.e. the content, one that is shared but with some distortion, i.e. the style, and one that is not shared at all, i.e. the modality-specific component. Is it possible that the modality specific component in [1] is just the constant component caused by the different initializations seen in this work?\n- The solution to the modality gap is to center the embeddings, implying the modality gap is just a shift. Isn\u2019t it possible that the difference in modality may also result in different scales?\n\n[1] Daunhawer, I., Bizeul, A., Palumbo, E., Marx, A., & Vogt, J. E. (2022, September). Identifiability Results for Multimodal Contrastive Learning. In The Eleventh International Conference on Learning Representations."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission813/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission813/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission813/Reviewer_Vv1o"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission813/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698735880219,
            "cdate": 1698735880219,
            "tmdate": 1699636008464,
            "mdate": 1699636008464,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "koDVPMntzK",
                "forum": "ttXg3SKAg5",
                "replyto": "AlGdVejbNQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission813/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank Reviewer Vv1o for their positive comments and for providing thoughtful feedback on our work. We address Reviewer Vv1o\u2019s concerns below.\n\n**More discussion about Table 1**\n\n> The discussion on the alignment noise could be improved: in particular, the results in Table 1 are left for the reader to infer. The same statistics could also be easily computed upon any other modality combination in the appendix, it would be useful to see if it still applies.\n\nThank you for your suggestion. We have now included a detailed explanation of how to interpret these statistics in Appendix C. We also included statistics of embeddings from different modalities in Appendix C. We hope these edits will help readers easily understand Table 1.\n\n\n**Would reduce the dimensionality mitigate the modality gap?**\n\n> Since the modality gap is due to the dimensional collapse, would reducing the dimensionality to the effective one help overcoming the issue?\n\nThanks for the question. To understand how dimensionality affects the gap, we initialized CLIP with different dimensions. We find that, in lower dimensions, the ratio of effective dimensions increases for both encoders (but cannot reach 100%) and the modality gap decreases.\n\n| Dimension | Effective Image Dimension | Effective Text Dimension | Init Gap Distance |\n| --- | --- | --- | --- |\n| 64 | 18 | 60 | 0.386 |\n| 128 | 20 | 110 | 0.554 |\n| 256 | 23 | 176 | 0.803 |\n| 512 | 25 | 230 | 1.136 |\n\nNonetheless, we hope to clarify that while the relationship between the dimensionality and the gap is an interesting question, this gap can be well addressed by the collapse operation based on our theoretical analysis. Therefore, the gap actually does not matter.\n\n**Connection to [1]**\n\n> Is there any relation between the decomposition of the modality gap with the content-style-modality specific decomposition assumed e.g. in [1]? Briefly, each latent vector in a multi-modal contrastive learning space is there assumed to have a part that is shared across modalities, i.e. the content, one that is shared but with some distortion, i.e. the style, and one that is not shared at all, i.e. the modality-specific component. Is it possible that the modality specific component in [1] is just the constant component caused by the different initializations seen in this work?\n\nThank you for pointing out this interesting work. The modality-specific component in [1] appears to be more closely related to the \"alignment noise\" than the \"modality gap\". For instance, [1] identifies \"object rotation\" as a modality-specific component in the visual domain. This suggests that changes like rotating an image would solely affect image embeddings, with no parallel effect in text embeddings. Therefore, this phenomenon seems to align more with alignment noise, where image and text embeddings are not perfectly aligned in the shared representation space.\n\n**Can modality gap differ in scale?**\n\n> The solution to the modality gap is to center the embeddings, implying the modality gap is just a shift. Isn\u2019t it possible that the difference in modality may also result in different scales?\n\nThanks for your question. Based on our theoretical analysis, the modality gap can only be a shift, not a scale difference. Our analysis reveals that the embeddings\u2019 effective dimensions from different modalities will be aligned, and the ineffective dimensions will remain constant, which creates a constant gap. If there is a scale difference, gradients will be propagated into the effective dimensions and make the scale the same.\n\nThank you again for your feedback, which is very helpful in improving the paper. We hope the above responses and changes to our manuscript adequately address your concerns. Please let us know if you have further questions or concerns!\n\n**References**\n\n[1] Daunhawer, I., Bizeul, A., Palumbo, E., Marx, A., & Vogt, J. E. (2022, September). Identifiability Results for Multimodal Contrastive Learning. In The Eleventh International Conference on Learning Representations."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700405887107,
                "cdate": 1700405887107,
                "tmdate": 1700405887107,
                "mdate": 1700405887107,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lgDmchh4Ff",
                "forum": "ttXg3SKAg5",
                "replyto": "koDVPMntzK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission813/Reviewer_Vv1o"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission813/Reviewer_Vv1o"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their response. I find my concerns to be addressed, and confirm my Accept score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700464802503,
                "cdate": 1700464802503,
                "tmdate": 1700464802503,
                "mdate": 1700464802503,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kgztcwgP27",
            "forum": "ttXg3SKAg5",
            "replyto": "ttXg3SKAg5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission813/Reviewer_bNKP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission813/Reviewer_bNKP"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the modality gap phenomenon in multimodal learning. Specifically, the authors claim that the modality gap emerges and is preserved due to a) dimensional collapse during initialization and training, and b) alignment noise controlled by temperature. To overcome the modality gap, this paper proposes the C^3 paradigm by subtracting mean of features and add Gaussian noise before decoding the features. Experiments on four tasks involving three modalities show the efficacy of C^3."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe paper is well written and easy to follow. The empirical analysis well presents and supports the claims on modality gap, a significant problem in multimodal learning.\n2.\tExperiments are extensive. The authors experiment on image, text and audio modalities, and the results prove the method is applicable across various tasks."
                },
                "weaknesses": {
                    "value": "1.\tMy major concern is about novelty. [1] has pointed out that random initialization and contrastive learning causes and preserves modality gap, and [2] has modeled modality gap as a constant orthogonal to image and text span. The proposed C^3 is also an ensemble of existing methods [2][3][4]. Especially, the cross-modal transferability in [2] seems quite similar to the ``interchangeability of embeddings from different modalities\u2019\u2019 in this paper. Please justify.\n2.\tThis paper proposes to align representations from different modalities but without convincing justification. In fact, it remains uncertain what effects are relevant to aligning modalities. [1] reports that making the modality gap too small or too large harms performance. [5] proves that strictly aligning modality representations is suboptimal. Therefore, I suggest adding reasons for aligning modalities.\n3.\tDespite that the authors have conducted experiments on various tasks, the comparison with existing methods is limited. Most comparisons in this paper are ablating over different components in C^3. Tab.2 shows marginal improvement over CapDec without reporting std over independent runs, which is not convincing. Tab.3, Tab.4 and Tab.5 report few comparisons with SOTA methods.\n\n[1] Liang, Victor Weixin, et al. \"Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning.\" \n[2] Zhang, Yuhui, et al. \"Diagnosing and rectifying vision models using language.\" \n[3] Radford, Alec, et al. \"Learning transferable visual models from natural language supervision.\" \n[4] Zhou, Yufan, et al. \"Towards language-free training for text-to-image generation.\" \n[5] Jiang, Qian, et al. \"Understanding and constructing latent modality structures in multi-modal representation learning.\""
                },
                "questions": {
                    "value": "1.\tFrom the experiment results, $C_2^2$ seems to be much more effective than $C_1^2$. Why?\n2.\tSection 3.2 mentions the effect of temperature, but no discussion is given in experiments. What are the effects of modifying temperature in stage 1 and std of Gaussian in stage 3?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission813/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698837370859,
            "cdate": 1698837370859,
            "tmdate": 1699636008380,
            "mdate": 1699636008380,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "e9NCTSixJu",
                "forum": "ttXg3SKAg5",
                "replyto": "kgztcwgP27",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission813/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank Reviewer bNKP for their positive comments and for providing thoughtful feedback on our work. We address Reviewer bNKP\u2019s concerns below.\n\n**Novelty compared to existing works**\n\n> My major concern is about novelty. [1] has pointed out that random initialization and contrastive learning causes and preserves modality gap, and [2] has modeled modality gap as a constant orthogonal to image and text span. The proposed C^3 is also an ensemble of existing methods [2][3][4]. Especially, the cross-modal transferability in [2] seems quite similar to the ``interchangeability of embeddings from different modalities\u2019\u2019 in this paper. Please justify.\n\nThank you for the question. While some previous works have found the modality gap exists and empirically observed that addressing the gap can improve performance, no paper so far can provide **a comprehensive and complete theoretical understanding of why there is a gap, how the geometry looks like, and what we should do for the gap**. In this work, we provide a unified theoretical understanding of the gap and provide a simple yet effective method that addresses the gap in a principled way. Our experiments demonstrate its effectiveness across a wide range of tasks, which can be a template for future work.\n\nHere we further provide a detailed comparison with previous works, which is also included in Appendix A:\n\nIn terms of\u00a0**theory**\u00a0about multi-modal representation space geometry:\n\n[1] explained there is a modality gap caused by initialization and optimization, but its theory failed to justify its geometry.\n\n[2] empirically identified the modality gap geometry, but it failed to explain how the geometry arises.\n\nWe are the first work that explains how the geometry arises. We identified the *dimensional collapse* phenomenon in initialization and the *collapsed gradients* in optimization, both of which are not explored by [1,2], and their combination well explained that the modality gap geometry would be a constant orthogonal vector. We also connect\u00a0*dimensional collapse*\u00a0back to the\u00a0*cone effect*\u00a0proposed in [1] (Appendix F.2), while the former is a more general and well-explored analytical method.\n\nIn terms of\u00a0**methods**\u00a0for enhancing the interchangeability of embeddings from different modalities:\n\n[2] proposed the\u00a0*collapse*\u00a0operation (i.e., removing embedding mean from each modality) in classification settings, based on an empirical understanding of the modality gap geometry.\n\n[3,4] empirically found that the\u00a0*corrupt*\u00a0operation (i.e., adding noise to embeddings) works in generation settings.\n\nOur work unified these insights and showed the combination of them leads to superior performance than each of them. Our method is built on our solid theoretical understanding of multi-modal representation space geometry. We proved our method's effectiveness in a wide range of tasks, modalities, data, and embedding spaces, which can become a standard recipe for all future works built on multi-modal contrastive embedding space.\n\nIn terms of the **relation**\u00a0between\u00a0*cross-modal transferability*\u00a0and\u00a0*embedding interchangeability*:\n\n*Embedding interchangeability* is a more fundamental concept that\u00a0enables\u00a0*cross-modal transferability*\u00a0on both classifiers (demonstrated in [2]) and generators (shown in this work). Therefore, in this work we focus on enhancing embedding interchangeability."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700405523350,
                "cdate": 1700405523350,
                "tmdate": 1700429853326,
                "mdate": 1700429853326,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]