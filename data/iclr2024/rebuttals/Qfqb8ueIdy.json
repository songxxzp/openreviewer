[
    {
        "title": "A Unified Framework for Consistency Generative Modeling"
    },
    {
        "review": {
            "id": "fpYamYsSZ7",
            "forum": "Qfqb8ueIdy",
            "replyto": "Qfqb8ueIdy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3337/Reviewer_Qpyg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3337/Reviewer_Qpyg"
            ],
            "content": {
                "summary": {
                    "value": "This paper propose a way to unified three different views to formulate consistency model. This framework can inspires different design of consistency losses, and the paper demonstrate that such design can potentially lead to better consistency models. The paper is largely evaluated on three tasks, one is a toy 2D dataset, and then two image generations tasks. The results show that the new"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This theoretical framework that could unify multiple works (Poisson, Coupling, etc) and provide insight for developing new algorithm. To the best of my knowledge, this can be a useful contribution to the community. But I\u2019m not an expert in generative model theory so I will need to defer this to other reviewers as well."
                },
                "weaknesses": {
                    "value": "- Both proposed algorithm (PCM and CCM-OT) seems to require additional computes during training. For PCM, the weighted sum is computed through all x_i in the batch and for CCM-OT, the optimal transport is computed among the batch. These operations are not scaling very trivially with the batch-size, while to the best of my knowledge, consistency model seems to work better with larger batch size (e.g. 512 and in this paper case 256)."
                },
                "questions": {
                    "value": "- the final algorithm is not very clear to me by reading the main paper, there are several tricks proposed. It would be great to have the algorithm written out in the main paper rather than in the appendix."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3337/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698818401815,
            "cdate": 1698818401815,
            "tmdate": 1699636282992,
            "mdate": 1699636282992,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ud9JUClDlt",
                "forum": "Qfqb8ueIdy",
                "replyto": "fpYamYsSZ7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3337/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3337/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review and suggestions"
                    },
                    "comment": {
                        "value": "Dear Reviewer Qpyg,\n\nThank you for recognizing our contribution to our work and the constructive feedback. Here is our response to your concerns:\n*******************************************\n\n**Q: Both proposed algorithm (PCM and CCM-OT) seems to require additional computes during training. For PCM, the weighted sum is computed through all $x_i$ in the batch and for CCM-OT, the optimal transport is computed among the batch. These operations are not scaling very trivially with the batch-size, while to the best of my knowledge, consistency model seems to work better with larger batch size (e.g. 512 and in this paper case 256).**\n\n**A:** Thank you for highlighting the potential increase in computational complexity introduced by the proposed enhancements in our paper. To address this concern, we conducted an analysis of the training time on the CIFAR-10 dataset with batchsize 256: \n\n1) **Batch Weighted Summation.** We observed a modest 1.8% increase in training time related to batch summation for both the DCM-MS and PCM models.\n\n2) **Optimal Transport.** In the case of the CCM-OT model, which incorporates optimal transport for sample pairing, we noted a 3.3% rise in training time associated with optimal transmission.\n\nWe noted that these operations all occur independently of the network and, in our evaluation, did not result in significantly prolonged computation times for practical training scenarios. Given the improvements in model performance, we believe the impact on overall computational efficiency is reasonable.\n\nThank you again for your concerns, and we will include a discussion of computational overhead in the revised version.\n*******************************************\n**Q: The final algorithm is not very clear to me by reading the main paper, there are several tricks proposed. It would be great to have the algorithm written out in the main paper rather than in the appendix.**\n\n**A:** We appreciate your thoughtful feedback and recognize the importance of providing a clear understanding of our proposed algorithm. Considering the constraints of page limitations, we regret that we are unable to present the complete training and sampling algorithm in the main text.\n\nTo address this concern and enhance the accessibility of our work, we will incorporate a jump link in the main paper, directing readers to the corresponding pseudocode in the appendix. This approach will enable readers to seamlessly navigate between the main text and the detailed algorithmic representations for each model, ensuring a more comprehensive understanding of the proposed methodology.\n\nWe look forward to your continued insights and hope that this adjustment enhances the clarity and accessibility of our work."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3337/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700277189489,
                "cdate": 1700277189489,
                "tmdate": 1700277189489,
                "mdate": 1700277189489,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fzqrVJyjdx",
                "forum": "Qfqb8ueIdy",
                "replyto": "fpYamYsSZ7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3337/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3337/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your time and any follow up questions?"
                    },
                    "comment": {
                        "value": "Dear Reviewer Qpyg,\n\nTowards the end of the discussion phase, we are optimistic that our response has effectively addressed the queries raised. We eagerly await your feedback to ascertain if our reply adequately resolves any concerns you may have or if further clarification is required.\n\nThank you for your time and consideration.\n\nSincerely,\n\nPaper 3337 Authors"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3337/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644448345,
                "cdate": 1700644448345,
                "tmdate": 1700644796606,
                "mdate": 1700644796606,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Q1tuthka2w",
            "forum": "Qfqb8ueIdy",
            "replyto": "Qfqb8ueIdy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3337/Reviewer_drKn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3337/Reviewer_drKn"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a unified and comprehensive framework for consistency generative modeling. In particular, it introduces two novel models: Poisson Consistency Models (PCMs) and Coupling Consistency Models (CCMs), which extend the prior distribution of latent variables beyond the Gaussian form. Additionally, it incorporates optimal transport (OT) to improve the performance of these models further. Through empirical experiments, it demonstrates the effectiveness of the proposed framework across a range of generative tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper generalizes consistency models beyond the Gaussian form. In particular, it introduces two novel models: Poisson Consistency Models (PCMs) and Coupling Consistency Models (CCMs) with theoretical analysis. And comprehensive experiments show advantages compared to other diffusion models on both synthetic and real-world datasets, such as unconditional generation of CIFAR-10 and unpaired image-to- image translation (I2I) using AFHQ."
                },
                "weaknesses": {
                    "value": "1. the notation is a little confusing, which makes the paper hard to follow.\n2. The  reverse diffusion process with gaussian noise can reconstruct the original clear image from the noisy version step by step, it would be better to show some figures using PCMs and CCMs."
                },
                "questions": {
                    "value": "From Eq. 22 and 23, it shows that the continuity equation holds with or without condition z, then what is the physical meaning for z?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3337/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819838727,
            "cdate": 1698819838727,
            "tmdate": 1699636282909,
            "mdate": 1699636282909,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5088sDqVUO",
                "forum": "Qfqb8ueIdy",
                "replyto": "Q1tuthka2w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3337/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3337/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review and suggestions"
                    },
                    "comment": {
                        "value": "Dear Reviewer drKn,\n\nThank you for the detailed review and thoughtful feedback. Below we address specific questions.\n***************************************\n**Q: the notation is a little confusing, which makes the paper hard to follow.**\n\n**A:** I apologize for any confusion caused by our current notation choices. In the revised version, we are actively working to enhance the use of symbols for a clearer and more easily understandable presentation. If you have specific suggestions or examples where the notation can be improved, please feel free to provide them, as your input is invaluable in refining our paper for better comprehension.\n***************************************\n**Q: The reverse diffusion process with gaussian noise can reconstruct the original clear image from the noisy version step by step, it would be better to show some figures using PCMs and CCMs.**\n\n**A:** Thank you for raising this point. While our consistency modeling framework shares similarities with the diffusion model, it distinguishes itself by directly recovering the image from noise through a single-step mapping, bypassing intermediate stages. \n\n********************************************\n**Q: From Eq. 22 and 23, it shows that the continuity equation holds with or without condition z, then what is the physical meaning for $z$?**\n\n**A:**  I appreciate your insightful question. You are correct; the continuity equation describes the relationship between the probability path and the velocity vector field, and its establishment does not depend on $z$. In this context, $z$ lacks a clear physical meaning; we use it as a tool to indirectly construct the desired probability path for consistency training."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3337/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700277090706,
                "cdate": 1700277090706,
                "tmdate": 1700277090706,
                "mdate": 1700277090706,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zYENpGZLa0",
                "forum": "Qfqb8ueIdy",
                "replyto": "Q1tuthka2w",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3337/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3337/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your time and any follow up questions?"
                    },
                    "comment": {
                        "value": "Dear Reviewer drKn,\n\nTowards the end of the discussion phase, we are optimistic that our response has effectively addressed the queries raised. We eagerly await your feedback to ascertain if our reply adequately resolves any concerns you may have or if further clarification is required.\n\nThank you for your time and consideration.\n\nSincerely,\n\nPaper 3337 Authors"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3337/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644270689,
                "cdate": 1700644270689,
                "tmdate": 1700644786893,
                "mdate": 1700644786893,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yHKNdZozge",
            "forum": "Qfqb8ueIdy",
            "replyto": "Qfqb8ueIdy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3337/Reviewer_i9Sm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3337/Reviewer_i9Sm"
            ],
            "content": {
                "summary": {
                    "value": "This work proposed a generalized formulation of the Consistency Model [CM, Song et al. 2023], a recent advanced in diffusion generative modeling that enable few number of function evaluation (usually only 1 or 2 NFEs)-sampling but high-quality samples. The authors show that CM can be generalized into a general form with affine probability path that admits a velocity field. This results in an equivalent form continuity equation that describes the dynamic of the probabiliy density path. Equip with this general form, the authors proposed two additional extension of CM, called Poisson consistency model (PCM), based on formulation of the probability path showed in [Xu et al 2022], and Coupling consistency model (CCM), based on the linear probability path of flow matching framework [Lipman et al 2023, Liu et al 2023, Albergo & Vanden-Ejinden 2023]. The authors demonstrate the effectiveness of their proposed methodologies with numerical experiments on unconditional image generation task (CIFAR10) and unpaired image-to-image translation (AFHQ Cat-Dog/Wild-Dog). \n\n\n\nSong, Y., Dhariwal, P., Chen, M. &amp; Sutskever, I.. (2023). Consistency Models. In Proceedings of Machine Learning Research202:32211-32252 Available from https://proceedings.mlr.press/v202/song23a.html.\n\nXu, Y., Liu, Z., Tegmark, M., & Jaakkola, T. (2022). Poisson flow generative models. Advances in Neural Information Processing Systems, 35, 16782-16795.\n\nYaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, ICLR 2023.\n\nLiu, Xingchao, Chengyue Gong, and Qiang Liu. \"Flow straight and fast: Learning to generate and transfer data with rectified flow.\" arXiv preprint arXiv:2209.03003. ICLR 2023.\n\nAlbergo, Michael Samuel, and Eric Vanden-Eijnden. \"Building Normalizing Flows with Stochastic Interpolants.\" In The Eleventh International Conference on Learning Representations, 2023."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Well written and structured paper that is easy to pass through. The main goal of improving and generalizing consistency model framework is well-motivated."
                },
                "weaknesses": {
                    "value": "I have two main concerns for this paper.\n\n1. **Question mark on novelty:** the idea of writing continuity equation and probability path is not new, and many derivations  from the generalized CM in this paper are followed exactly from [Lipman et al 2023] on flow matching generative models. The formulation of Couping Consistency Model (CCM) -- section 3.4, which is this work's most well-performed framework empirically, the authors again borrowed heavily from previous work of [Lipman et al. 2023] with the linear path $x_t = tx_1 + (1-t)x_0$. Equation (20) and the idea of CCM-OT on learning probability path with joint distribution to reduce training loss variance is straightforward taken from Multisample Flow matching paper [Pooladian et al. 2023, Section 3], but surprisingly there was no mention of this citation around this equation.\n\n2. **Question mark on the results of empirical evaluation:** although the results provided with 1 NFE sampling for CIFAR10 in table 2 and 3 showed that CCM-OT outperformed original CM (denoted DCM in this paper) of [Song et al. 2023], I do not understand where the authors got the FID score of 18.4 for DCM to begin with. In [Song et al. 2023], it is clearly stated that their model reached 1-NFE FID 3.55 & 8.70 with distillation and without distillation, respectively. Therefore, I urge the authors provide a clear explanation on the discrepancy between the reported numbers of the baseline. Moreover, DCM also provided more extensive experiments with ImageNet 64x64 and LSUN Bedroom 256x256, and although I understand the lack of computational resources, I advise the authors to provide results on more datasets for better understanding the gains of their proposed methods compared with baselines, which should also included distillation techniques such as Progressive distillation [Salisman & Ho 2022].\n\nYaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, ICLR 2023.\n\nPooladian, Aram-Alexandre, Heli Ben-Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron Lipman, and Ricky Chen. \"Multisample flow matching: Straightening flows with minibatch couplings.\" arXiv preprint arXiv:2304.14772 (2023).\n\nSalimans, T. and Ho, J. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=TIdIXIpzhoI"
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3337/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826810794,
            "cdate": 1698826810794,
            "tmdate": 1699636282830,
            "mdate": 1699636282830,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8O412Z4h9O",
                "forum": "Qfqb8ueIdy",
                "replyto": "yHKNdZozge",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3337/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3337/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review and suggestions"
                    },
                    "comment": {
                        "value": "Dear Reviewer i9Sm,\n\nWe sincerely appreciate your positive comments and feedback, as well as the suggestions for improvement. In response to your questions, we have prepared the answers below:\n**************************************************\n**Q: Many derivations from the generalized CM in this paper are followed exactly from [1] on flow matching generative models. The formulation of Couping Consistency Model (CCM) -- section 3.4, which is this work's most well-performed framework empirically, the authors again borrowed heavily from previous work of [1] with the linear path .**\n\n**A:** Thank you for highlighting this observation. In Section 3.1, we explicitly acknowledge that our approach to constructing probabilistic paths draws inspiration from recent advances in Flow-based models [1]. It is essential to recognize, however, that our primary focus diverges significantly.\n\nFirstly, our major contribution lies in unveiling the intrinsic relationship between probabilistic paths and consistency models (CMs). This unique perspective enables us to approach consistency training from a foundational angle, potentially catalyzing the development of innovative single-step generative models. The introduction of the linear path into consistency models facilitates a distinct single-step Im2Im translation. In contrast, [1] typically employs around 100~200 steps, underscoring the non-trivial nature of our contribution.\n\nSecondly, it's imperative to emphasize that the conclusions in [1] are limited to Gaussian paths. In our work, we generalize these conclusions to a broader form, a crucial step for incorporating Poisson Consistency Models (PCMs) into our proposed framework. This generalization significantly enhances the adaptability and applicability of our approach beyond the constraints of Gaussian paths established by [1].\n******************************************************\n\n**Q:Equation (20) and the idea of CCM-OT on learning probability path with joint distribution to reduce training loss variance is straightforward taken from Multisample Flow matching paper [2]**\n\n**A:** We appreciate your insightful observation and acknowledge the connection between our CCM-OT and optimal transport flow matching in [2]. In our related section (Section 4), we have cited and expounded upon the relevance of their work, underscoring the differences in our motivation.\n\nWhile [2] demonstrates the efficacy of coupling distributions in reducing the variance of the flow matching target, we place a specific emphasis on refining the estimation of vector fields within our proposed framework, constituting a unique contribution in the realm of CMs. To the best of our knowledge, there exists no prior work proposing the link of optimal transport and CMs. Consequently, our work fills a significant gap in this domain, introducing a novel application of optimal transport principles to elevate the performance of CMs.\n\nWe are grateful for your suggestion, and as you recommended, we will refer back to the work [2] in the corresponding section to ensure that readers gain a comprehensive understanding of our contributions.\n***********************************************\n\n*[1] Lipman Y, Chen R T Q, Ben-Hamu H, et al. Flow matching for generative modeling[J]. arXiv preprint arXiv:2210.02747, 2022.*\n\n\n*[2] Pooladian A A, Ben-Hamu H, Domingo-Enrich C, et al. Multisample flow matching: Straightening flows with minibatch couplings[J]. arXiv preprint arXiv:2304.14772, 2023.*"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3337/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700276735148,
                "cdate": 1700276735148,
                "tmdate": 1700276735148,
                "mdate": 1700276735148,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ATf9vUZp0h",
                "forum": "Qfqb8ueIdy",
                "replyto": "yHKNdZozge",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3337/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3337/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part II. Rebuttal"
                    },
                    "comment": {
                        "value": "**Q:I urge the authors provide a clear explanation on the discrepancy between the reported numbers of the baseline.**\n\n**A:** Thank you for bringing attention to the differences in our reported metrics compared to [1]. Specifically, we meticulously replicated the work of [1], maintaining consistency in **time discretization, network design, learning rate, moving average rate, loss computation, and other key aspects**. However, due to computational constraints, we made necessary adjustments, including reducing the batch size from 512 to 256, and the number of iterations from 800K to 400K. Additionally, we implemented mixed-precision training for further acceleration. \n\nDespite these modifications, we want to emphasize our comparisons are fair. All models were configured with identical experimental hyperparameters. We believe that the primary source of the observed discrepancies may be attributed to the use of a smaller batchsize in our implementation. We will further elucidate our experimental setup and discrepancy compared with [1] in the revised version.\n****************************************************\n\n**Q:I advise the authors to provide results on more datasets for better understanding the gains of their proposed methods compared with baselines.**\n\n**A:** We appreciate your valuable feedback and fully acknowledge the merit of conducting evaluations on a broader set of datasets. However, it's worth noting that datasets like Imagnet and LSUN Bedroom, used by [1], involved training a consistency model on 64 A100 GPUs clusters\u2014such computational resources are beyond our current capacity.\n\nDespite these constraints, we have extended our evaluation to include another widely recognized image-generated benchmark dataset, CelebA. All methods were trained with a batch size of 64 and updated for 200K iterations and the results are as follows,\n\n| method | | FID | |\n|  ----  | ----  | ----  | ----  |\n| | NFE=1|NFE=2|NFE=5|\n| DCM | 41.0 | 27.3 | 25.3 |\n| DCM-MS | 38.3 | 22.2 | 18.2|\n| PCM | 32.9 | **17.8** | **16.1** |\n| CCM-OT | **30.8** | 28.0| 25.5|\n\nWe observed consistent improvements with our proposed method, reinforcing its efficacy. We understand the importance of comprehensive evaluations and hope that these additional results contribute to a better understanding of the strengths of our approach.\n*****************************************************************\n**Q:Comparison with distillation technology such as Progressive distillation [2]**\n\n**A:** We appreciate the suggestion to compare our approach with distillation technologies like Progressive Distillation [4]. However, it's important to note that our primary focus in this paper is on the consistency training introduced by [1]. Our objective is to achieve a single-step generative model directly, without relying on pre-trained score networks.\n\nUnlike distillation approaches based on score networks, our consistency framework can be positioned as an independent family of generative models that are computationally resource-friendly. \n****************************************************\n*[1] Song Y, Dhariwal P, Chen M, et al. Consistency models[J]. 2023.*\n\n*[2] Salimans T, Ho J. Progressive distillation for fast sampling of diffusion models[J]. arXiv preprint arXiv:2202.00512, 2022.*"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3337/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700277036274,
                "cdate": 1700277036274,
                "tmdate": 1700396876524,
                "mdate": 1700396876524,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "t434xjGL3M",
                "forum": "Qfqb8ueIdy",
                "replyto": "yHKNdZozge",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3337/Reviewer_i9Sm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3337/Reviewer_i9Sm"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the rebuttal, but I am keeping my original opinion."
                    },
                    "comment": {
                        "value": "I have read the rebuttal of the author. I appreciate the efforts that the authors put into it. However, I don't think my two major concerns are addressed. \n\n1. Unfair comparison: if I am not wrong, the authors cited a misleading information about computational requirement for consistency model [1], as I could not find the line in that paper that explicitly said they use exactly 64 A100 GPUs (which I agree is a lot), but only state a cluster of A100 GPUs. I would be happy to be clarified about this. For a check, I could also suggest the authors share their code so I and other reviewers can check the reproducibility of their experiments. \n\n2. Novelty: I restate the fact that flow matching and diffusion model (more exactly probability flow) are in fact very closely related to each other (with both sharing the same affine probability path) has been pointed out by flow matching papers [2,3]. Combining them with consistency model for diffusion probabiliy path in [1] makes it become the CCM in this paper, and adding an layer of OT coupling that has been introduced in [4] to make it become CCM-OT. I do agree that the framework the authors presented is new, but is still a combination of multiple existing ideas, which makes the novelty limited. \n\n\n[1] Song Y, Dhariwal P, Chen M, et al. Consistency models. ICML 2023.\n\n[2] Lipman Y, Chen R T Q, Ben-Hamu H, et al. Flow matching for generative modeling[J]. arXiv preprint arXiv:2210.02747, 2022.\n\n[3] Liu, Xingchao, Chengyue Gong, and Qiang Liu. \"Flow straight and fast: Learning to generate and transfer data with rectified flow.\" arXiv preprint arXiv:2209.03003. ICLR 2023.\n\n[4]  Pooladian, Aram-Alexandre, Heli Ben-Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron Lipman, and Ricky Chen. \"Multisample flow matching: Straightening flows with minibatch couplings.\" arXiv preprint arXiv:2304.14772 (2023)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3337/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544372432,
                "cdate": 1700544372432,
                "tmdate": 1700544372432,
                "mdate": 1700544372432,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LsZYFAo4K3",
                "forum": "Qfqb8ueIdy",
                "replyto": "TEsoAggDjr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3337/Reviewer_i9Sm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3337/Reviewer_i9Sm"
                ],
                "content": {
                    "title": {
                        "value": "Re: Further clarification"
                    },
                    "comment": {
                        "value": "Thank you for publishing the code and further clarification. Could you quickly clarify why there are difference between variance schedulers config (sigma_min, sigma_max) of the two model type `DCM` and `CCM/CCM-OT`?"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3337/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625810619,
                "cdate": 1700625810619,
                "tmdate": 1700625810619,
                "mdate": 1700625810619,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZarwtEez8Z",
                "forum": "Qfqb8ueIdy",
                "replyto": "yHKNdZozge",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3337/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3337/Authors"
                ],
                "content": {
                    "title": {
                        "value": "variance schedulers"
                    },
                    "comment": {
                        "value": "Thank you for your question. sigma_min and sigma_max in the code represent the range of the anchor variables $t$ in our paper. For DCM, we followed the settings of [1] with sigma_min=$0.02$, sigma_max=$80$, and $t\\in [0.02, 80]$.  For CCM/CCM-OT, we compute the interpolation to sample from the conditional probability paths, i.e. $tx_1+(1-t) x_0$, where $t\\in [0, 1]$. In order to prevent numerical errors, we set it as sigma_min=$0.0001$, sigma_max=$0.9999$.\n\n\n*[1] Song Y, Dhariwal P, Chen M, et al. Consistency models. ICML 2023.*"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3337/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631146155,
                "cdate": 1700631146155,
                "tmdate": 1700632065379,
                "mdate": 1700632065379,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QBeGdomTuL",
                "forum": "Qfqb8ueIdy",
                "replyto": "ZarwtEez8Z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3337/Reviewer_i9Sm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3337/Reviewer_i9Sm"
                ],
                "content": {
                    "title": {
                        "value": "Re: Further clarification"
                    },
                    "comment": {
                        "value": "Hi, what I meant is that would you be able to clarify that this discrepancy in setting variance schedulers would not results in discrepancy of the performance of the two methods, and not just because the difference in probability path formulation? Thanks."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3337/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633683559,
                "cdate": 1700633683559,
                "tmdate": 1700633683559,
                "mdate": 1700633683559,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ponbCFXTTP",
                "forum": "Qfqb8ueIdy",
                "replyto": "yHKNdZozge",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3337/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3337/Authors"
                ],
                "content": {
                    "title": {
                        "value": "variance schedulers"
                    },
                    "comment": {
                        "value": "We are sorry that we initially misunderstood your question. We think that these two variance schedulers are similar to the two setups of variance preservation and variance explosion in the field of diffusion model, both of them aim at corrupting the original image signals by a prior distribution, and we believe that both of them do not make a significant difference on the model performance, as can be seen from our experimental results on DCM and CCM. We hope our answers address your concerns."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3337/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635097700,
                "cdate": 1700635097700,
                "tmdate": 1700636086913,
                "mdate": 1700636086913,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aDlH428KEW",
                "forum": "Qfqb8ueIdy",
                "replyto": "yHKNdZozge",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3337/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3337/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your time and any follow up questions?"
                    },
                    "comment": {
                        "value": "Dear Reviewer i9Sm,\n\nTowards the end of the discussion phase, we are optimistic that our response has effectively addressed the queries raised. We eagerly await your feedback to ascertain if our reply adequately resolves any concerns you may have or if further clarification is required.\nThank you for your time and consideration.\n\nSincerely,\n\nPaper 3337 Authors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3337/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644236173,
                "cdate": 1700644236173,
                "tmdate": 1700644776487,
                "mdate": 1700644776487,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VREqUHjiu4",
                "forum": "Qfqb8ueIdy",
                "replyto": "ponbCFXTTP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3337/Reviewer_i9Sm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3337/Reviewer_i9Sm"
                ],
                "content": {
                    "title": {
                        "value": "Re: variance schedulers"
                    },
                    "comment": {
                        "value": "Hi, I am asking this question because as far as I am aware, using different variance schedulers __do__ make big diffrences in terms of performance of the models in both sampling and training (in this case the authors said VP and VE SDE), see e.g. simple diffusion paper [1] for stochastic path and EDM paper for ode deterministic path [2, Section 3]. There are enough discrepancies in the performance regarding FID between DCM (the baseline) and CCM, for example on NFE=2 on CIFAR10, or CelebA-64 where actually CCM is performing better (and I suspect it will be bigger if we increase the resolution of the datasets).   \n\nI appreciate the authors' intuitive answer, but I am keeping my original evaluation.\n\n[1] Hoogeboom, E., Heek, J. &amp; Salimans, T.. (2023). simple diffusion: End-to-end diffusion for high resolution images. Proceedings of the 40th International Conference on Machine Learning 202:13213-13232.\n\n[2] Karras, T., Aittala, M., Aila, T., & Laine, S. (2022). Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35, 26565-26577."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3337/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645655099,
                "cdate": 1700645655099,
                "tmdate": 1700645655099,
                "mdate": 1700645655099,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Banq1K04xW",
            "forum": "Qfqb8ueIdy",
            "replyto": "Qfqb8ueIdy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3337/Reviewer_Dtgk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3337/Reviewer_Dtgk"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a unified framework for consistency generative modeling by introducing two models: Poisson Consistency Models(PCMs) and Coupling Consistency Models(CCMs). The overall training pipeline stays similar to the original Consistency Models. The PCMs attempt to replace the Gaussian distribution of the latent variables in the diffusion models with the Poisson distribution while the CCMs introduce a tuple of random variables to replace the Gaussian distributions. The paper also utilizes Opticmal Transport to further improve the performance of CCMs. Results are shown on a toy dataset and CIFAR-10."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Speeding up the diffusion models is an important problem and consistency training has been shown to be an effective way to achieve this.\n2. Results on the toy experiments and CIFAR-10 show that the proposed methods improve over the baselines regarding of FID."
                },
                "weaknesses": {
                    "value": "1. The evaluation is weak. The paper only conducts quantitative comparisons on a toy dataset and CIFAR-10.\n2. As in the Consistency Models paper, the consistency distillation method generally performs better than consistency training. However, the paper does not provide any results or comparisons for this."
                },
                "questions": {
                    "value": "The key contribution that goes beyond the gaussian distribution by replacing the gaussian distribution with either poisson distribution or a joint distribution seems to not specific to the training of consistency models but also the vanilla diffusion models. Could the authors clarify more for on this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3337/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3337/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3337/Reviewer_Dtgk"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3337/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698830095795,
            "cdate": 1698830095795,
            "tmdate": 1699636282752,
            "mdate": 1699636282752,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "29lxFVJR5t",
                "forum": "Qfqb8ueIdy",
                "replyto": "Banq1K04xW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3337/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3337/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review and suggestions"
                    },
                    "comment": {
                        "value": "Dear Reviewer Dtgk,\n\nThank you for the detailed review and thoughtful feedback. Below we address specific questions.\n************************\n**Q: The evaluation is weak. The paper only conducts quantitative comparisons on a toy dataset and CIFAR-10.**\n\n**A:** We appreciate your pointer and acknowledge the need for a more evaluation. In response, we will include another benchmark dataset, Celeba64 $ \\times $64, in the revised version. All methods were trained with a batch size of 64 and updated for 200K iterations, yielding the following results:\n\n| method | | FID | |\n|  ----  | ----  | ----  | ----  |\n| | NFE=1|NFE=2|NFE=5|\n| DCM | 41.0 | 27.3 | 25.3 |\n| DCM-MS | 38.3 | 22.2 | 18.2|\n| PCM | 32.9 | **17.8** | **16.1** |\n| CCM-OT | **30.8** | 28.0| 25.5|\n\nThe result further highlights the advantages of our proposed algorithm over the original DCM, affirming the effectiveness of our framework. We commit to promptly include these tables and visualizations in the revised version.\n\nIt is crucial to emphasize that our work extends the modeling capabilities of consistency models (CMs) [1]. In addition to unconditional generation, we introduced a unique single-step unpaired im2im experiment on AFHQ, addressing limitations in previous CMs related to the diffusion process.\n************************\n**Q: As in the Consistency Models paper, the consistency distillation method generally performs better than consistency training. However, the paper does not provide any results or comparisons for this.**\n\n**A:** Thank you for bringing up this point. Our framework is designed to establish a self-sufficient and self-reliant approach for consistency generative modeling, without dependence on pre-trained score networks.\n\nWhile we do mention consistency distillation in our work, it was not the primary focus for several reasons:\n\n1) Incorporating consistency distillation necessitates an additional pre-training step, introducing computational overhead and somewhat deviating from our original design philosophy for this framework. \n\n2) The effectiveness of consistency distillation is inherently depend on the quality of the pre-trained model used. Concurrent research [2] has shown that independent consistency modeling can achieve significant improvements through various advanced training tricks, potentially surpassing the performance of consistency distillation. We believe our framework can also benefit from these strategies.\n*******************************************************\n\n**Q: The key contribution that goes beyond the gaussian distribution by replacing the gaussian distribution with either poisson distribution or a joint distribution seems to not specific to the training of consistency models but also the vanilla diffusion models. Could the authors clarify more for on this?**\n\n**A:** I appreciate the reviewer's thoughtful question. Our work indeed is inspired by prior efforts that have explored modifications to the Gaussian distribution in vanilla diffusion models, such as the Poisson flow [3] and Flow matching [4]. However, it is crucial to highlight that our contribution extends beyond a simple replacement of distributions.\n\nOur distinctive contribution lies in establishing a profound connection between the self-consistency property inherent in Consistency Models (CMs) and the probabilistic paths. This profound connection not only enhances our understanding of CMs but also facilitates the design of more effective single-step generative models.\n\n*********************************************************\n*[1] Song Y, Dhariwal P, Chen M, et al. Consistency models[J]. 2023.*\n\n*[2] Song Y, Dhariwal P. Improved Techniques for Training Consistency Models[J]. arXiv preprint arXiv:2310.14189, 2023.*\n\n*[3] Xu Y, Liu Z, Tegmark M, et al. Poisson flow generative models[J]. Advances in Neural Information Processing Systems, 2022, 35: 16782-16795.*\n\n*[4] Lipman Y, Chen R T Q, Ben-Hamu H, et al. Flow matching for generative modeling[J]. arXiv preprint arXiv:2210.02747, 2022.*"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3337/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700275992701,
                "cdate": 1700275992701,
                "tmdate": 1700276758613,
                "mdate": 1700276758613,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6yH63v0maI",
                "forum": "Qfqb8ueIdy",
                "replyto": "Banq1K04xW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3337/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3337/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the review and any follow up questions?"
                    },
                    "comment": {
                        "value": "Dear Reviewer Dtgk,\n\nTowards the end of the discussion phase, we are optimistic that our response has effectively addressed the queries raised. We eagerly await your feedback to ascertain if our reply adequately resolves any concerns you may have or if further clarification is required.\n\nThank you for your time and consideration.\n\nSincerely,\n\nPaper 3337 Authors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3337/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644113988,
                "cdate": 1700644113988,
                "tmdate": 1700644760906,
                "mdate": 1700644760906,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]