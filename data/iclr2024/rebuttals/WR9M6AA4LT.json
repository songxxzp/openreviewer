[
    {
        "title": "Fit Like You Sample: Sample-Efficient Generalized Score Matching from Fast Mixing Diffusions"
    },
    {
        "review": {
            "id": "3Xj1mBYKzU",
            "forum": "WR9M6AA4LT",
            "replyto": "WR9M6AA4LT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5757/Reviewer_BbBg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5757/Reviewer_BbBg"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the problem of using score matching to learn the probability distribution in energy-based models.\nIn energy-based models, when we learn the probability distribution, we often encounter an intractable normalizing factor.\nTo avoid this intractable factor, one can use score matching instead.\nHowever, score matching can be statistically less efficient.\nThis paper works on the connection between the mixing time of a broad class of continuous, time-homogeneous Markov processes with stationary distribution and generator, and the statistical efficiency of an appropriately chosen generalized score matching loss."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The problem seems well-motivated."
                },
                "weaknesses": {
                    "value": "- The presentation is fairly technical and may pose challenges for readers who are not an expert in this particular area."
                },
                "questions": {
                    "value": "Note:\n- Theorem 2: What are $\\Gamma_{SM}$ and $\\Gamma_{MLE}$?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5757/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5757/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5757/Reviewer_BbBg"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5757/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698610823116,
            "cdate": 1698610823116,
            "tmdate": 1699636604362,
            "mdate": 1699636604362,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9HAGo9PwwY",
                "forum": "WR9M6AA4LT",
                "replyto": "3Xj1mBYKzU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5757/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5757/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your feedback"
                    },
                    "comment": {
                        "value": "Thank you for your feedback! We are glad you find the problem well-motivated. We are happy to make changes to the presentation if you have concrete suggestions on how to make it more approachable. \n\nRegarding your question: $\\Gamma_{SM}$ and $\\Gamma_{MLE}$ are the covariance matrices of the asymptotic limit for the score matching and maximum likelihood estimators respectively (i.e. the matrix $\\Gamma$, s.t. for the estimator $\\hat{\\theta}_n$, we have $\\sqrt{n}(\\hat{\\theta}_n - \\theta^*) \\to \\mathcal{N}(0, \\Gamma)$)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5757/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699933316288,
                "cdate": 1699933316288,
                "tmdate": 1699933316288,
                "mdate": 1699933316288,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dlfTXmtUqN",
                "forum": "WR9M6AA4LT",
                "replyto": "3Xj1mBYKzU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5757/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5757/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Any outstanding concerns?"
                    },
                    "comment": {
                        "value": "Dear reviewer BbBg, \n\nthank you again for your feedback ! \n\nWe'd like to remind you that the discussion period is ending tomorrow (11/22), and wanted to follow up and check if there are any outstanding concerns or questions we could answer. If our response (as well as the response to the other reviewers) answered your questions, we hope you'll consider raising your score. \n\nBest, \n\n--Authors"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5757/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576188844,
                "cdate": 1700576188844,
                "tmdate": 1700576188844,
                "mdate": 1700576188844,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EW6VneGhEY",
                "forum": "WR9M6AA4LT",
                "replyto": "dlfTXmtUqN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5757/Reviewer_BbBg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5757/Reviewer_BbBg"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. I will take it into consideration during the AC discussion."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5757/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680047444,
                "cdate": 1700680047444,
                "tmdate": 1700680047444,
                "mdate": 1700680047444,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kTwZYinlxF",
            "forum": "WR9M6AA4LT",
            "replyto": "WR9M6AA4LT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5757/Reviewer_ufaE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5757/Reviewer_ufaE"
            ],
            "content": {
                "summary": {
                    "value": "The authors study generalized score matching loss, that uses an arbitrary linear operator instead of $\\nabla_x$ in the standard score matching objective. They generalize the result of  Koehler et al. (2022) to this setting. Concretely, they show that for a Markov process with stationary distribution $p$ proportional to $\\exp(-f(x))$ and a generator $\\mathcal{L}$ with Poincare constant $C_P$, one can choose a linear operator such that the error of the corresponding generalized score matching estimator (more precisely, the spectral norm of the covariance of limit distribution, assuming asymptotic normality) can be bounded in terms of $C_P$ and the error of MLE (more precisely, the spectral norm of the covariance of the limit distribution). In addition, they use generalized score matching with additional temperature variable for learning means of Gaussian mixtures with the same covariance matrix."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The results are new and non-trivial. The statements are clear, as well as comparison with results from prior works on score matching."
                },
                "weaknesses": {
                    "value": "There are a few things that concern me. So far I'm not convinced that the paper is above the acceptance threshold. Please see the questions below."
                },
                "questions": {
                    "value": "Regarding Theorem 2: Could you please explain if there is any interesting technical contribution compared to Theorem 2 in Koehler et al. (2022)? The proof looks like a straightforward generalization of their proof to your settings, or did I miss anything important?\n\nRegarding Theorem 5:\n\n1) The estimator that you use here doesn't seem to be efficiently computable, is that correct? The score matching estimator for exponential families from Koehler et al. (2022) is efficiently computable (please correct me if I'm wrong), so their motivation to study it and compare with MLE is clear to me. What is the motivation of usage of your estimator for this problem if it is not efficiently computable? \n\n2) As I understood, you are interested in the regime $K \\gg d$, so the fact that $C$ from your bound $\\Vert \\Gamma_{SM} \\Vert_{OP} \\le C \\Vert \\Gamma_{MLE}\\Vert_{OP}^2$ does not depend on $K$ is important and nice. However, as you said in the footnote, $ \\Vert \\Gamma_{MLE}\\Vert_{OP}$ may depend on $K$, so $K$ can appear in the end in the error, i.e. $\\Vert \\hat{\\mu_i} - \\mu^*_i \\Vert$ \nmay depend on $K$ even when the corresponding error for MLE doesn't. So it is not clear to me why this dependence on $K$ was important from the very beginning. \n\nIf it was really important, then it would make sense to bound not \n$\\Vert \\Gamma_{SM} \\Vert_{OP}$, but the largest diagonal entry of $\\Gamma_{SM}$ in terms of the largest diagonal entry of $\\Gamma_{MLE}$. In this case , if there is such a bound with a factor that does not depend on $K$, then it should imply a bound on $\\Vert \\hat{\\mu_i} - \\mu^*_i \\Vert$ that does not depend on $K$ (as long as corresponding errors of MLE do not depend on $K$). Is it possible to derive such a bound?\n\n3) There is no comparison with prior works on Gaussian mixtures. While you refer to some of these works in the paper, it is not immediately clear how your result is comparable with them. I think it makes sense to add such a comparison.\n\n4) Can your approach be generalized to more general mixtures of Gaussians, when not all of them have the same covariance (but, say, when all covariances have condition number bounded by $O(1)$)?\n\nAnd a minor thing:\n\nIn Definition 1, is it really fine to use linear operators between the spaces of *all* functions? E.g. in Lemma 1 you use adjoint operators and assume that the operators are between Hilbert spaces."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5757/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5757/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5757/Reviewer_ufaE"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5757/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698848755260,
            "cdate": 1698848755260,
            "tmdate": 1699636604267,
            "mdate": 1699636604267,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SpvIexErqM",
                "forum": "WR9M6AA4LT",
                "replyto": "kTwZYinlxF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5757/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5757/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your feedback!"
                    },
                    "comment": {
                        "value": "Thank you for your feedback and questions! We are glad you found our results new and non-trivial, and our writing clear. Hopefully the clarifications below ameliorate your concerns.\n\n**Contribution of Theorem 2**: You are correct that our results are a generalization of Koehler et al. (2022). While the overall proof strategy is similar, Theorem 2 is important in two ways: \n\n(1) **Conceptually** our result makes it clear that there is nothing special about *Langevin diffusion* (and its mixing time, as captured through the standard Poincare constant). In particular, for *any diffusion* of the form (7), one can connect the statistical complexity of the corresponding generalized score matching loss to the mixing time of the diffusion. This provides a \u201cdictionary\u201d to translate between fast-mixing diffusions and corresponding score losses with good statistical complexity. Furthermore, the results in Koehler et al. (2022) also only apply to exponential families\u2014our proof shows this restriction is not needed.            \n\n(2) **Technically** the main new ingredient is Lemma 4 (along with the calculations in Lemmas 3, 11 and Lemma 13) which connects the Hessian of the generalized score matching loss to the Dirichlet form of the corresponding diffusion. Again, these calculations are completely new compared to Koehler et al. (2022).  \n\n\n**Clarification on Theorem 5**: \n\n*(Q1) Computational efficiency of training score loss, motivation*: The general motivation behind score-matching methods is to avoid calculating a partition function, by fitting the score function through the integration by parts formula (eq (2)). In our case, as in Proposition 6, the CTLD score loss can also be rewritten by integration by parts as an expectation of expressions involving the fitted scores.   \n\nNote that even for basic score matching, the corresponding loss may not be convex, so all that\u2019s guaranteed is that a gradient-based method can be efficiently run (which is not the case for maximum likelihood due to the partition function). Nevertheless, there **is** an algorithmic gain compared to maximum likelihood (the possibility of running a gradient-based algorithm efficiently)\u2014so the question of understanding the \"statistical cost\u201d (how much worse it is compared to maximum likelihood) is meaningful.  \n\n*(Q2) Dependence on K*: the reason why we phrase the result as $\\lVert\\Gamma_{SM}\\rVert_{OP} \\leq C \\lVert\\Gamma_{MLE}\\rVert^2_{OP}$ is that we are trying to compare how much (statistically) less efficient score matching is compared to MLE. We stress the point about $C$ not depending on $K$ for two reasons: \n\n(1) While MLE could potentially depend on $K$\u2014the dependence is likely to be mild (we are estimating $K$ vectors, so the dependence would be no worse than linear); if $C$ depends poorly (e.g. exponentially in $K$) it would render our result uninteresting for mixtures with large number of components. Moreover, since MLE is asymptotically statistically optimal, whatever dependence on $K$ it incurs is unavoidable. \n\n(2) Theorem 2 proceeds via a decomposition result, similar as prior analyses of simulated tempering (Lee et al, Ge et al (2018)) \u2014 and all these results incur a dependence on $K$. This is because their setup is somewhat different \u2014 in particular, they analyze discrete tempering, where the annealing is done via temperature scaling (rather than Gaussian convolution).\n\n*(Q3) Comparison with prior work on Gaussian mixture models*: We are happy to expand on the discussion in Appendix I. Mostly though, these results are incomparable: they focus on end-to-end provable algorithms for learning mixtures in various regimes\u2014typically trying to deal with the case of mixtures with not-well-separated means. Our focus is on understanding the *statistical complexity of score matching losses*\u2014not on providing new end-to-end provable algorithms for Gaussian mixtures. (Note, the CTLD loss is not convex, and we don\u2019t analyze what gradient descent or related algorithms would converge to.) Gaussian mixtures are just chosen as a very natural toy model for multimodal data, and as a \u201cproof of concept\u201d that annealed score matching is statistically well-behaved for many distributions for which vanilla score matching is not (via the lower bound in Koehler et al (2022)). \n\n*(Q4) Gaussians with different covariance*: it is quite likely that our results apply to mixtures with covariances which are $1+O(1/d)$ away from each other, but not larger (were d is the ambient dimension). The reason for this is that annealing \u201cdistorts\u201d exponentially the relative volumes of the Gausians (technically, what breaks is the chi-squared divergence bound in Lemma 21). However, we note that for universal approximation using mixtures, one can use a mixture of (exponentially many) Gaussians with covariance $\\epsilon I$ to $\\epsilon$-approximate the distribution of choice."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5757/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699897969882,
                "cdate": 1699897969882,
                "tmdate": 1699898403656,
                "mdate": 1699898403656,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "60uc0xJ6Id",
                "forum": "WR9M6AA4LT",
                "replyto": "kTwZYinlxF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5757/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5757/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Any outstanding concerns?"
                    },
                    "comment": {
                        "value": "Dear reviewer ufaE, \n\nthank you again for your feedback and questions ! \n\nWe'd like to remind you that the discussion period is ending tomorrow (11/22), and wanted to follow up and check if there are any outstanding concerns or questions we could answer. If our response clarified your original concerns, since you otherwise thought our results are clear, new and non-trivial, we hope you'll consider raising your score. \n\nBest, \n\n--Authors"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5757/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576072391,
                "cdate": 1700576072391,
                "tmdate": 1700576072391,
                "mdate": 1700576072391,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "k46JtnYsOw",
                "forum": "WR9M6AA4LT",
                "replyto": "60uc0xJ6Id",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5757/Reviewer_ufaE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5757/Reviewer_ufaE"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nThank you very much for the clarification! I don't have further questions. Currently I don't increase the score, but I will take into account your response during the Reviewer/AC Discussion."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5757/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733683383,
                "cdate": 1700733683383,
                "tmdate": 1700733683383,
                "mdate": 1700733683383,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "I24P6y7P0U",
            "forum": "WR9M6AA4LT",
            "replyto": "WR9M6AA4LT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5757/Reviewer_v9Ko"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5757/Reviewer_v9Ko"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a general framework for designing generalized score matching losses with good sample complexity from fast-mixing diffusions. More precisely, for a broad class of diffusions with generator $\\mathcal{L}$ and Pincare constant $C_P$, they can choose a linear operator $\\mathcal{O}$ such that  the generalized score matching loss $E[\\|\\mathcal{O} p / p - \\mathcal{O} p_{\\theta} / p_{\\theta}\\|_2^2] / 2$ has a statistical complexity that is a factor $C_P^2$ worse than that of maximum likelihood. In addition, they analyze a lifted diffusion, which introduces a new variable for temparature and provably show statistical benefits of annealing for score matching. They apply their approach to sample from Gaussian mixture distributions.Their first result generalizes that of Koehler 2022."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well motivated and well written. It generalizes a previous paper on score matching (Koehler 2022) to generalized linear operator and correspondingly general score matching loss. The authors are also able to design a Markov chain termed CTLD based on the idea of anneling. Motivated by this chain, they are able to estimate the score function for Gaussian mixture distribution that has multiple modes and control the generalized score matching loss. The framework they propose is novel and quite interesting."
                },
                "weaknesses": {
                    "value": "Several assumptions in the paper seem abit strong, and it would be good if the authors can elaborate a bit more on them. For the GMM application, it would be good to compare their result with the previous ones. Finally, I would love to see an experiment that supports their result, but the result itself is also interesting enough."
                },
                "questions": {
                    "value": "1. Assumption 1 and 2 of Theorem 2 seems pretty strong. Could the authors give an example where these assumptions hold when $\\mathcal{O} \\neq \\nabla_x$ and not from CTLD? In general, how do we validate these assumptions?\n\n2. What is $\\mathcal{O}$ for CTLD? \n\n3. Maybe the authors can comment a bit on how Theorem 5 compares with the previous results, in particular, can the results of Koehler 2022 also be applied to get an upper bound?\n\n4. Can we apply generalized score matching loss to diffusion sampling? Maybe the authors can comment a bit on the feasiblity of that."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5757/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5757/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5757/Reviewer_v9Ko"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5757/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699290983790,
            "cdate": 1699290983790,
            "tmdate": 1699636604178,
            "mdate": 1699636604178,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "a65nFgc8ak",
                "forum": "WR9M6AA4LT",
                "replyto": "I24P6y7P0U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5757/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5757/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the encouraging feedback!"
                    },
                    "comment": {
                        "value": "Thank you for your encouraging review! We are glad you found our framework novel and interesting, and our paper well-written. \n\n*(Q1) Strength of Assumptions 1 and 2*: Assumption 1 is meant to capture a broad class of multimodal distributions. Moreover, by the universal approximation property of mixtures, any distribution can be approximated by a mixture of sufficiently many Gaussians. Assumption 2 is meant to check that for the \u201ccanonical\u201d parametrization of a mixture, the smoothness term in the asymptotic sample complexity (Theorem 4) can be effectively bounded. Some kind of weight-tieing in the parametrization of the score at different temperatures has to be assumed to bound the smoothness term (else, the scores the model estimates at different temperatures need not have anything to do with each other!) It would be a great direction for future research to understand what assumption suffices to have a good bound on this term (we certainly don't think Assumption 2 is necessary). Our goal was to check that the \u201ccanonical\u201d parametrization results in a good bound.    \n\n*(Q2) What is O for CTLD*: $\\mathcal{O}$ is $\\nabla_x$ for CTLD (the Markov Chain is however over the augmented space that includes the temperature random variable). \n\n*(Q3) Theorem 5, relation to Koehler et al (2022)*: The results in Koehler et al (2022) are for the basic version of score matching (eq (2)), and they incur a bound that depends on the Poincare constant of the distribution, which can be quite bad for well-separated mixtures. For instance, for a mixture of even two Gaussians with means $-\\mu$ and $\\mu$ and variance $\\sigma^2$, the Poincare constant scales exponentially in $\\mu^2/\\sigma^2$ [1]. By contrast, our results scale polynomially in the norms of the means, the ambient dimension, and minimum and maximum eigenvalues of the covariance matrices. \n\n*(Q4) Using CTLD for diffusion models*: The CTLD loss can plausibly be used for better diffusion model training: the primary obstacle is the fact that it involves derivatives of the score, which is a $d \\times d$ matrix. As we note after Proposition 3, similar \u201chigh-order\u201d analogues of score losses have been proposed in some works (e.g. [2]), with some success. Some preconditioning strategies for sampling from diffusion models have also been recently proposed [3], again with some success. \n\n[1] Chafai, Djalil, and Florent Malrieu. \"On fine properties of mixtures with respect to concentration of measure and Sobolev type inequalities.\" In Annales de l'IHP Probabilit\u00e9s et statistiques, vol. 46, no. 1, pp. 72-96. 2010.\n\n[2] Meng, Chenlin, Yang Song, Wenzhe Li, and Stefano Ermon. \"Estimating high order gradients of the data distribution by denoising.\" Advances in Neural Information Processing Systems 34 (2021): 25359-25369.\n\n[3] Zhang, Qinsheng, and Yongxin Chen. \"Fast Sampling of Diffusion Models with Exponential Integrator.\" In The Eleventh International Conference on Learning Representations. 2022."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5757/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699932195398,
                "cdate": 1699932195398,
                "tmdate": 1699932195398,
                "mdate": 1699932195398,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]