[
    {
        "title": "Solving Multiobjective Combinatorial Optimization via Learn to Improve Method"
    },
    {
        "review": {
            "id": "gFso0iR0sn",
            "forum": "le1UUMd45T",
            "replyto": "le1UUMd45T",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5602/Reviewer_ru1d"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5602/Reviewer_ru1d"
            ],
            "content": {
                "summary": {
                    "value": "One of the traditional methods for solving MOCOPs involves the MOEA approach. In this approach, individual solutions in a population pool are continuously updated through cross-overs and enhanced via local search. The authors suggest replacing the local search component within the MOEA. Previously, the local search was driven by a random selection of node pairs; however, the authors now employ a neural network for this pair selection."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This is the first L2I approach applied to MOCOPs.\n\n2. A new neural net architecture is introduced, which can accommodate the weight factor used in identifying the Pareto set.\n\n3. A new RL training method is introduced, leveraging the population pool of the MOEA approach.\n\n4. A new quality enhancement method suitable for the MOEA approach is presented."
                },
                "weaknesses": {
                    "value": "While the authors have commendably applied the L2I approach to MOCOPs, yielding impressive results, the novelty of the ideas underpinning this work doesn't fully meet the expectations I have for ICRL publications.\n\n1. The concept of utilizing a trained neural model to bolster the local search component of a genetic algorithm isn't novel.\n\n2. The presented neural net architecture appears to be a minor variation of an existing one, specifically DACT.\n\n3. The employment of a shared baseline for REINFORCE isn't groundbreaking, as seen in POMO.\n\n4. Similarly, quality enhancement via instance augmentation isn't a pioneering approach.\n\nWhile I acknowledge that the specific methodologies deployed in the paper are novel, especially given this is the inaugural L2I application to MOCOPs, the broader insights readers can derive from this work seem somewhat limited."
                },
                "questions": {
                    "value": "Branding the methodology in this paper as \"L2I\" might lead to confusion in the future. As subsequent research emerges that applies the L2I approach to MOCOPs, referencing this work simply as \"L2I\" could create ambiguity. Future works that aim to compare their results with this paper would face challenges in distinguishing between this specific approach and other \"L2I\" methodologies for MOCOPs."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5602/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5602/Reviewer_ru1d",
                        "ICLR.cc/2024/Conference/Submission5602/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5602/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698477683698,
            "cdate": 1698477683698,
            "tmdate": 1700465411067,
            "mdate": 1700465411067,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8gMpoV0dhw",
                "forum": "le1UUMd45T",
                "replyto": "gFso0iR0sn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5602/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5602/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable comments.\n\nWhile MOCOPs can be decomposed into multiple scalar optimization subproblems and solved individually using existing local improvement algorithms, this approach often overlooks the dominance relationships among Pareto solution sets. Each solution is improved independently, which can result in neighboring subproblems converging to similar solutions and a loss of diversity. Furthermore, existing local improvement algorithms require separate sub-models to be trained for each subproblem.\n\nTo illustrate the difference, consider the table below, which compares the number of model parameters required by the classical single-objective local improvement algorithm DACT and our L2I when solving multi-objective problems (e.g. Bi-TSP decomposed into 101 subproblems). It is evident that DACT requires a significantly larger number of model parameters compared to L2I, resulting in a more challenging model to train and longer training time.\n\n| Method | #(model) | #(parameter) |\n|-|-|-|\n| DACT | 101 | 30B|\n| L2I | 1 | 0.298B |\n||||\n\nOur L2I approach offers a more flexible and efficient neural network that can solve multiple subproblems simultaneously and improve their solutions collaboratively. This mitigates the issue of a lack of diversity in the Pareto solution set. In addition, L2I incorporates a shared baseline and quality enhancement mechanism, which captures implicit connections among Pareto solutions during both the training and inference processes. The ablation experiments in Section 5.3 further validate the effectiveness of our approach.\n\nWe further respond to your concerns as follows.\n\n**To W1:**\n\nAlthough there are similar methods previously proposed for single-objective combinatorial optimization problems, it is challenging to modify these methods for MOCOPs. We present the initial attempt of using the learn-to-improve framework. It supports collaboratively improvement of multiple solutions and has achieved promising results.\n\n**To W2:**\n\nDACT is one of the learn-to-improve methods for single-objective COPs, which cannot directly address MOCOPs. In principle, any advanced neural model can be aggregated in our weighted-related policy network, where DACT is just a building block. When directly training multiple DACT models to deal with the decomposed subproblems for MOCOPs, it costs more computational resources and exhibits poor performance compared with our model, as shown in Table 3.\n\n**To W3:**\n\nThe shared baseline in L2I is derived by solving multiple subproblems for a MOCOP simultaneously and averaging their solutions. On the other hand, the shared baseline in POMO is obtained by solving a single-objective problem multiple times and averaging the results. However, applying POMO strategy to the multi-objective domain is challenging, because individually solving each scalar subproblem multiple times would consumes a significantly increasing amount of memory and runtime, making it impractical.\n\n**To W4:**\n\nThe vanilla instance augmentation approach for single-objective COPs exploits equivalent instance transformation, which could be directly applied to the decomposed subproblems for MOCOPs, as in [1]. Different from the vanilla instance augmentation approach, our quality enhancement approach is specifically tailored for MOCOPs, further utilizing the external population of MOEAs and the Pareto dominance of solutions. The comparison results in Table 4 show our quality enhancement approach can better improve the solution quality compared with the vanilla instance augmentation approach.\n\n[1] Xi Lin, Zhiyuan Yang, and Qingfu Zhang. Pareto set learning for neural multi-objective combinatorial optimiza- tion. In International Conference on Learning Representations, 2022. \n\n\n\n**To Questions:**\n\nThank you for your suggestion. We have taken it into consideration and decided to revise the name of our model to Weighted-Related Policy Network (WRPN). This change aims to prevent potential confusion with other L2I approaches."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5602/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700462837637,
                "cdate": 1700462837637,
                "tmdate": 1700462837637,
                "mdate": 1700462837637,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cbOPpywzoQ",
                "forum": "le1UUMd45T",
                "replyto": "8gMpoV0dhw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5602/Reviewer_ru1d"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5602/Reviewer_ru1d"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your clarification. I have changed my score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5602/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700465642599,
                "cdate": 1700465642599,
                "tmdate": 1700465642599,
                "mdate": 1700465642599,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "K5KpAVQRQl",
            "forum": "le1UUMd45T",
            "replyto": "le1UUMd45T",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5602/Reviewer_Rq8H"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5602/Reviewer_Rq8H"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new deep reinforcement learning method that involves a learning-based improvement method for solving multi-objective combinatorial optimization problems. A weight-related policy network is embedded into multi-objective evolutionary algorithm frameworks to guide the search. Experimental studies on multi-objective traveling salesman problems and multi-objective vehicle routing problems show the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tA learning-based improvement method is proposed for solving multi-objective combinatorial optimization problems with deep reinforcement learning.\n2.\tAn ablation study is conducted to study the proposed method and show its effectiveness."
                },
                "weaknesses": {
                    "value": "1.\tThe way the proposed weight-related policy network is embedded is not clearly described. A pseudo-code of the complete method for solving one multi-objective combinatorial optimization problem should be provided.\n2.\tThe proposed method contains several hyperparameters, e.g., the number of transformer-style stacked encoders and the number of attention heads. A summary of them needs to be provided. \n3.\tHow many times does each algorithm run independently in the experiment?"
                },
                "questions": {
                    "value": "1.\tHow does the algorithm perform when using complex operators?\n2.\tWhat are the numbers of variables for the test problems in the experiments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5602/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698754288912,
            "cdate": 1698754288912,
            "tmdate": 1699636577403,
            "mdate": 1699636577403,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fSXPXW8uTl",
                "forum": "le1UUMd45T",
                "replyto": "K5KpAVQRQl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5602/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5602/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**To W1**: \n\nThanks for your suggestions. Initially, we presented a detailed explanation of the proposed weight-related policy network in Sections 4.2 and a pseudo-code outlining the L2I inference process in Algorithm 2 in Appendix G. In response to your feedback, we will include pseudo-code detailing the computation of the weight-related policy network when addressing an MOCOP with L2I, as demonstrated in Algorithm 2.\n\n**To W2**:\n\nThank you for your suggestion. We will include a list of hyperparameters in Appendix I, aiming to enhance the understanding of the model's specific parameters.\n\n**To W3**:\n\nAll learning-based methods employing the greedy strategy are executed only once, as their outcomes exhibit no randomness. For WS-LKH and WS-ORTools, we conduct a single run, because their results from 10 runs, despite consuming more runtime, are close to those from a solitary run, as discussed in [1].\n\nFor our L2I and the baseline MOEAs, we also run them only once, as the standard deviations are tiny. To show this, we have further supplemented the results with 10 independent executions for Bi-TSP-20/50/100, as presented in Table 11.\n\n| Method | Bi-TSP-20 | | Bi-TSP-50 | | Bi-TSP-100 | |\n|-|-|-|-|-|-|-|\n| | mean |std. |  mean | std.  | mean | std. | \n| MOEA/D | 0.6247 | 1.1$\\times 10^{-4}$  | 0.6338 | 1.4$\\times 10^{-4}$ |  0.6956 | 7.5$\\times 10^{-5}$ |\n| NSGA-II | 0.6284 | 9.1$\\times 10^{-5}$ | 0.6147 | 1.1$\\times 10^{-4}$ | 0.6708 | 6.2$\\times 10^{-5}$ | \n| MOGLS | 0.6286 | 4.1$\\times 10^{-5}$ | 0.6296 | 9.9$\\times 10^{-5}$ | 0.6504 | 1.6$\\times 10^{-4}$  | \n| MOGLS+L2I | **0.6297** | 1.7$\\times 10^{-6}$ | **0.6446** | 4.4$\\times 10^{-6}$ | **0.7069** | 1.4$\\times 10^{-5}$ | \n||||||||\n\n[1] Wouter Kool, Herke van Hoof, and Max Welling. Attention, learn to solve routing problems! In International Conference on Learning Representations, 2019.\n\n\n**To Q1:**\n\nIt is possible to employ more complex operators to further enhance the search process, but this may require additional computational resources during both training and inference. In our methodology, we utilize an ensemble operator consisting of three basic ones. The results of the ablation study are presented in Table 8 in Appendix F.1. It is evident from the results that the ensemble operator, being a more complex operator, surpasses the performance of three basic operators.\n\n**To Q2:**\n\nThe number of decision variables is closely related to the length of the decision sequence. In the case of MOTSP, the number of decision variables equals the number of nodes. In our paper, we consider three specific numbers: 20, 50, and 100. For MOCVRP, additional dummy depots are included in the decision sequence, as depicted in Appendix B.2. In our analysis, we evaluate three sizes of decision variables: 30, 70, and 120, corresponding to 20, 50, and 100 nodes, respectively."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5602/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700462832707,
                "cdate": 1700462832707,
                "tmdate": 1700462832707,
                "mdate": 1700462832707,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UHNtElTaxZ",
            "forum": "le1UUMd45T",
            "replyto": "le1UUMd45T",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5602/Reviewer_BJaC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5602/Reviewer_BJaC"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces \"Learn to Improve\" (L2I), a deep reinforcement learning (DRL) technique designed to address multiobjective combinatorial optimization problems (MOCOPs). L2I contrasts traditional DRL methods by embedding a weight-related policy network into multiobjective evolutionary algorithm (MOEA) frameworks. This assists in directing the search, reduces training variance, and offers an enhanced quality mechanism for better model inference. Computational experiments on classic MOCOPs like multiobjective traveling salesman and vehicle routing problems highlight the superiority of L2I over existing methods."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "L2I introduces a new DRL-based approach for MOCOPs. Unlike the traditional \"Learn to Construct\" methodology, L2I emphasizes iterative improvements. The L2I module has demonstrated adaptability as it can be integrated into various MOEA frameworks, such as NSGA-II, MOEA/D, and MOGLS. This mechanism, applying instance augmentation techniques, improves both the proximity and diversity of the Pareto set. The L2I methodology outperforms other state-of-the-art techniques on standard MOCOPs, even showing better performance than renowned solutions like the LKH solver for specific problems."
                },
                "weaknesses": {
                    "value": "Since the paper deals with the combinatorial optimization, decomposition methods are not sufficiently elaborated/reviewed."
                },
                "questions": {
                    "value": "How does L2I compare with methods follows a general scheme of \"learn-divide-and-conquer\" or \"divide-learn-and-conquer\"? In a sense, there are approaches that learn how to decompose a problem and there are approaches that decompose a problem before learning. I qualitative assessment may be sufficient."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5602/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5602/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5602/Reviewer_BJaC"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5602/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698810976234,
            "cdate": 1698810976234,
            "tmdate": 1699636577290,
            "mdate": 1699636577290,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ETBy6MvUfa",
                "forum": "le1UUMd45T",
                "replyto": "UHNtElTaxZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5602/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5602/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable comments.\n\nMany multiobjective algorithms first decompose the MOCOP into a number of scalar optimization subproblems, and then optimize them simultaneously. To the best of our knowledge, approaches that learn how to decompose a problem are rare. Exploring this concept could present an interesting direction for further investigation. On the other hand, our L2I approach focuses on learning improvement strategies for each individual subproblem. It belongs to the scheme of \u201cdivide-learn-and-conquer\u201d."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5602/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700462795266,
                "cdate": 1700462795266,
                "tmdate": 1700462795266,
                "mdate": 1700462795266,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Vmqo7RZwfo",
            "forum": "le1UUMd45T",
            "replyto": "le1UUMd45T",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5602/Reviewer_P9jg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5602/Reviewer_P9jg"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new paradigm \"learn to improve\" in the context of solving Mult objective combinatorial optimization problems (MOCOPs). The proposed approach adds an improvement operation, based on a deep policy network, that works in parallel with individual solutions, using evolutionary technique. The deep network is based on an Encoder-Decoder, where encoder is Transformer-style stacked encoders, with Dual-Aspect Collaborative Attention (DAC-Att). Comparisons with SOA and ablation studies are done."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "a. Mathematical fomulation of the proposed method\nb. Details of the proposed deep network based on Encoder-Decoder based on Transformer.\nc. The use of Dual-Aspect Collaborative Attention (DAC-Att)\nd. Details of the algorithm\ne. Comparisons with state of the art (SOA) on MOCOPs.\nf. Ablation Study"
                },
                "weaknesses": {
                    "value": "a. In the result tables, the proposed method is not highlighted\nb. The results in tables 1 & 2 are not discussed on why the proposed approach is better only in the last entries?"
                },
                "questions": {
                    "value": "Why the proposed method show better results? Please explain the specificality of the proposed method."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5602/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5602/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5602/Reviewer_P9jg"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5602/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698909067645,
            "cdate": 1698909067645,
            "tmdate": 1700548752905,
            "mdate": 1700548752905,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IxhRQM9tUK",
                "forum": "le1UUMd45T",
                "replyto": "Vmqo7RZwfo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5602/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5602/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer\u00a0P9jg,\n\nThanks\u00a0for\u00a0the\u00a0detailed\u00a0review.\n\nIt\u00a0looks\u00a0like\u00a0the\u00a0review\u00a0is\u00a0about\u00a0another\u00a0paper.\u00a0E.g.,\u00a0It\u00a0emphasizes\u00a0a\u00a0fault\u00a0prediction\u00a0method,\u00a0but\u00a0it\u00a0is\u00a0not\u00a0mentioned\u00a0in\u00a0our\u00a0paper.\u00a0We would appreciate it if you could check it and offer an appropriate review.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5602/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699767445565,
                "cdate": 1699767445565,
                "tmdate": 1699767445565,
                "mdate": 1699767445565,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "U9aWhHN4x3",
                "forum": "le1UUMd45T",
                "replyto": "IxhRQM9tUK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5602/Reviewer_P9jg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5602/Reviewer_P9jg"
                ],
                "content": {
                    "title": {
                        "value": "MISTAKE CORRECTION"
                    },
                    "comment": {
                        "value": "I accidentally entered the wrong (another paper) review. Now I have corrected it. I sincerely apologize for this mistake."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5602/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549668794,
                "cdate": 1700549668794,
                "tmdate": 1700549668794,
                "mdate": 1700549668794,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "A5hIPEIjbm",
                "forum": "le1UUMd45T",
                "replyto": "Vmqo7RZwfo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5602/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5602/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your updates and valuable comments.\n\n**To Weakness a:**\n\nAccording to your suggestions, we have highlighted our proposed method in italic in the tables.\n\n**To Weakness b:**\n\nIn terms of the improved outcomes reported in the tables, there are two points to clarify.\n\nFirstly, various MOEA frameworks, when augmented with our L2I approach, retain their distinct original characteristics. Notably, MOGLS, designed specifically for multi-objective combinatorial optimization, achieves the best result. However, MOEA/D and NSGA-II, originally proposed for general multi-objective optimization, exhibit comparatively lower performance.\n\nSecondly, one of the results obtained from MOGLS+L2I in the last entry benefits from a larger number of iterations and the incorporation of a quality enhancement mechanism. As a result, this particular configuration demonstrates the best performance.\n\n**To Q:**\n\nOn the one hand, our L2I approach differs from traditional MOEAs as it leverages deep reinforcement learning techniques. Through training on extensive datasets, L2I acquires the ability to selectively invoke local operators that enhance solutions in a specified direction.\n\nOn the other hand, L2C methods directly construct solutions for individual subproblems, which can lead to a diversity problem due to the inadequate search for potential Pareto solutions in the neighborhood of each subproblem. In contrast, our L2I approach adopts a different learning paradigm. It collaboratively improves multiple solutions, effectively addressing the aforementioned issues and enhancing the solution quality."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5602/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700624782565,
                "cdate": 1700624782565,
                "tmdate": 1700624963406,
                "mdate": 1700624963406,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]