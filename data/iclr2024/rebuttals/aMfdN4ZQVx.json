[
    {
        "title": "Training-free Deep Concept Injection Enables Language Models for Crossmodal Tasks"
    },
    {
        "review": {
            "id": "5HoXZz69cm",
            "forum": "aMfdN4ZQVx",
            "replyto": "aMfdN4ZQVx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4148/Reviewer_Hqos"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4148/Reviewer_Hqos"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Deep Concept Injection (DCI) as an approach to achieving zero-shot cross-modal tasks without the need for additional training. This work conducts a comprehensive set of experiments and analyses to validate the effectiveness of this approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The method of constructing projection networks to replace fine-tuning is intriguing. The authors successfully inject observed concepts, facilitating cross-modal fusion in self-attention layers and feed-forward networks."
                },
                "weaknesses": {
                    "value": "The paper claims to be the first to demonstrate the ability of Pre-trained Language Models (PLMs) to perform zero-shot cross-modal tasks without any training. However, there exist similar works, such as Tip-Adapter [1], which should be discussed and compared to provide context and clarify the novelty of this approach.\n\n[1] Zhang, Renrui, et al. \"Tip-adapter: Training-free adaption of clip for few-shot classification.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022."
                },
                "questions": {
                    "value": "Does the Deep Concept Injection process incorporate additional information? While there is no explicit training involved in the design of this paper, has any training data been indirectly introduced into this process?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4148/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4148/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4148/Reviewer_Hqos"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4148/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698556505870,
            "cdate": 1698556505870,
            "tmdate": 1699636380283,
            "mdate": 1699636380283,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tzBCSGuSBV",
                "forum": "aMfdN4ZQVx",
                "replyto": "5HoXZz69cm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4148/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4148/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Hqos"
                    },
                    "comment": {
                        "value": "Dear Reviewer Hqos,\n\nThank you for your detailed comments and suggestions. We appreciate your recognition of the soundness, contribution, and presentation of our work. We tried our best to address all the concerns and questions, and update the main paper and appendix in the new version. Please let us know if you have any further concerns or questions to discuss.\n\nBest,\n\nPaper 4148 Authors\n\n---\n\n### Our position w.r.t. existing work\n- Thanks for the reference. But the provided reference doesn\u2019t change the position of our paper. **Our work is focused on crossmodal tasks that require modality fusion, but the Tip-adapter is designed only for unimodal classification tasks, which is fundamentally different from ours**. We have cited it in the revised main paper.\n\n---\n\n### Information from training data\n- For DCI using the genric vocabulary and DCI-LM, there is no information from the training set at all. \n- As we discussed in Sec 3.3, for the variant DCI-A, the most frequent answers from the training set provided in the open-ended setting are used as the vocabulary. We also noted that this does not leak any additional information for 8 datasets of open-ended video question answering since the compared FrozenBiLM also uses the answer vocabulary as part of the input to the model."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4148/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700074675736,
                "cdate": 1700074675736,
                "tmdate": 1700074691697,
                "mdate": 1700074691697,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UOy7YuCtWC",
                "forum": "aMfdN4ZQVx",
                "replyto": "tzBCSGuSBV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4148/Reviewer_Hqos"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4148/Reviewer_Hqos"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your detailed response. I have no other questions."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4148/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700697749313,
                "cdate": 1700697749313,
                "tmdate": 1700697749313,
                "mdate": 1700697749313,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rVotfnHd5Q",
            "forum": "aMfdN4ZQVx",
            "replyto": "aMfdN4ZQVx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4148/Reviewer_Eqqp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4148/Reviewer_Eqqp"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a method to inject visual concepts into pretrained LLMs without any training for vision-language tasks. The authors leveraged a pre-extracted concept library to aggregate vision-related concepts into feed forward pretrained LLMs with probabilistic weighting. By properly constructing the concept library, the proposed DeepConceptInjection (DCI) model achieve state-of-the-art results on several vqa and video-qa tasks with significant training overhead reduction."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed model is training-free, by properly constructing the concept library and weighting the forward features of words in concept library, the resulting DCI can correctly adapt vision-related information into pretrained-LLMs for vision-language tasks. The resulting model achieve state-of-the-art results compared with existing VQA or Video-QA models with similar model scale."
                },
                "weaknesses": {
                    "value": "Despite the good performance and simplicity of the method, this paper seems to be missing several critical key points:\n- First, the concept library seems to be extremely important in the whole DCI pipeline, however, how to construct this concept library given different input datasets or domains seems to be too simple for evaluating if this pipeline could be adapted to more general settings. The authors should consider adding more details to this concept library construction process for the reviewers to evaluate the contribution of this paper. \n\n- The overall architecture seems extremely simple and efficient. Given the good performance, the authors should have considered providing more insights on why this simple augmentation strategy, or put it another way, weighting input and features from the concept library, could help improve vision-language tasks. Is this augmentation enough?"
                },
                "questions": {
                    "value": "See the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4148/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698755648442,
            "cdate": 1698755648442,
            "tmdate": 1699636380197,
            "mdate": 1699636380197,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YFPNCLG77U",
                "forum": "aMfdN4ZQVx",
                "replyto": "rVotfnHd5Q",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4148/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4148/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Eqqp"
                    },
                    "comment": {
                        "value": "Dear Reviewer Eqqp,\n\nThank you for your detailed comments and suggestions. We tried our best to address all the concerns and questions, and update the main paper and appendix in the new version. Please let us know if you have any further concerns or questions to discuss.\n\nBest,\n\nPaper 4148 Authors\n\n---\n\n### Vocabulary construction process\n- From our results in Tables 1 and 2 and our analysis in Sec 4.2 and 4.3, the dataset-specific variants DCI-A and DCI-LM achieve better results, but we would also like to highlight that: **even with the generic vision vocabulary, our training-free DCI can already achieve comparable results with SOTA that requires training, which has already validated the main message that current paradigm of training projection and adaptation layers are not necessary.**\n- We indeed included the details of the vocabulary construction in Sec. 3.3 and G.1. The generic vocabulary is obtained from existing computer vision benchmarks \u2013 VisualGenome and OpenImages. The DCI-A dataset-specific vocabulary is obtained from the answer vocabulary used in the open-ended setting for each dataset.\n- We indeed included detailed quantitative results and analysis in Sec. F.2 for ablation study on DCI-LM. We generally observe that DCI-LM helps to remove from the concept recognition results some too generic visual concepts that are not relevant to the question.\n\n---\n\n### Insights and limitations\n- We indeed provided insightful analysis in Sec. 4.4 and 4.5. In Sec. 4.4, we specifically study how the projection layers can be replaced with different training-free variants, and we find that most of the ability of the learned projection layers and the adaptation layers can be instantly\nobtained with the simple constructed projection layer, as long as we use concepts as the bridge. This observation is aligned with our first insight that concepts can be directly understood by the language model, as summarized in the Introduction.\n- In Sec 4.5, we find that the augmentation in the intermediate feature space helps the model attend more to extracted concepts that are relevant to the correct answer, which is aligned with our second insight that the feed-forward process is the other ignored complementary pathway for modality fusion.\n- In Sec. 3.2.2, we also show the insights on how the injection process is equivalent to a feed-forward network with constructed weights, which again shows that as long as we speak the \u201clanguage\u201d that PLMs understand, the training process can be removed.\n- To further understand the potential of the proposed method, we also provide Plug-and-play results, results using ViT pretrained on ImageNet instead of CLIP, and Fine-tuning results in Appendix B, D, and F.1. We find that for models that have already been trained, our model can still provide help. When there is finetuning involved, DCI is also a better strategy to fuse visual information compared to the existing approach. More importantly, when there is no high-quality vision-language contrastive model like CLIP available, we show that with a generic classifier, our DCI method can still achieve better results compared to the existing approach.\n- Our method certainly has its limitations, which we discuss in detail in Appendix E. Overall, the main aim of this paper is not to completely replace existing approaches but to open up new possibilities for future research on better designs of modality fusion and new paths towards more efficient and versatile utilization of PLMs."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4148/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700074563362,
                "cdate": 1700074563362,
                "tmdate": 1700074563362,
                "mdate": 1700074563362,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ivf9WMSq9d",
            "forum": "aMfdN4ZQVx",
            "replyto": "aMfdN4ZQVx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4148/Reviewer_HYCX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4148/Reviewer_HYCX"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel approach for zero-shot video/visual question answering leveraging the capabilities of a Large Language Model. To achieve this, the authors translate relevant visual concepts into textual representations using a predefined vocabulary set. Subsequently, they introduce the Deep Concept Injection Module, which integrates these textual visual concepts into the input and feed-forward networks. The effectiveness of this method is validated through extensive experiments conducted on eight video question datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper proposes to inject the textual visual input into the feed-forward network, with experimental results affirming the efficacy of this approach.\n2. Extensive experiments on 8 benchmark video question answering datasets demonstrate the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1. As shown in Eqn. (8), the input features of feed-forward networks guide the aggregation of the output features of feed-forward network, which is hard to comprehend. More explanations are required.\n2. The authors adopt PLM to extract the features of textual visual concepts. However, when the length of the textual visual concepts exceeds one, it would be better to elucidate the specific feature extraction process.\n3. There exists \u2018?\u2019 and \u2018-\u2019 in Table 1. It would be better to explain the meaning of these quotes.\n4. While the ablation studies regarding hyper-parameters are included in the appendix, it would be beneficial to mention the best hyper-parameter settings in the implementation details to make the paper more concrete.\n5. This method demonstrates remarkable performance. Could the authors consider releasing the source code upon acceptance?"
                },
                "questions": {
                    "value": "Please refer to the weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/a"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4148/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4148/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4148/Reviewer_HYCX"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4148/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698756714049,
            "cdate": 1698756714049,
            "tmdate": 1699636380119,
            "mdate": 1699636380119,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "puo1fg8T90",
                "forum": "aMfdN4ZQVx",
                "replyto": "Ivf9WMSq9d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4148/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4148/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer HYCX"
                    },
                    "comment": {
                        "value": "Dear Reviewer HYCX,\n\nThank you for your detailed comments and suggestions. We appreciate your recognition of the soundness, contribution, and presentation of our work. We tried our best to address all the concerns and questions, and update the main paper and appendix in the new version. Please let us know if you have any further concerns or questions to discuss.\n\nBest,\n\nPaper 4148 Authors\n\n---\n\n\n### Explanation of Eq. 8\n- Thanks for the suggestion. Eq. 8 adds the representation of concepts weighted based on the conditional distribution from Eq. 5 to the representation of the current word being processed, in the output space of each feed-forward network. $\\lambda$ controls the strength of the augmentation. We have added this explanation in the revised paper.\n- We indeed included the intuitive explanation of this process in the last paragraph of Sec 3.2.2: \n> Intuitively, as verified in Figure 4, the intermediate representation of \u201c[mask]\u201d could not be close to the answer \u201chat\u201d but after adding the representation of observed concepts, the model can make the correct prediction.\n\n\n---\n\n### When the number of tokens is larger than one for one concept\n- We include this detail in Appendix G.1 due to space limit. We simply apply averaging over the tokens when the concept consists of multiple tokens.\n\n---\n\n### Explanation of \u201c?\u201d and \u201c-\u201d\n- Thanks for the suggestion. \u201c-\u201d means not applicable and \u201c?\u201d means unclear from the original paper.\n- We have included it in the revised version.\n\n---\n\n### Hyper-parameter setting\nThanks for the suggestion. We have added it to the revised main paper.\n\n---\n\n### Code release\n- Thanks for the question. We indeed plan to, as we mentioned in Sec. G.1."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4148/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700074411292,
                "cdate": 1700074411292,
                "tmdate": 1700074411292,
                "mdate": 1700074411292,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jkISxN8Wxq",
            "forum": "aMfdN4ZQVx",
            "replyto": "aMfdN4ZQVx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4148/Reviewer_YWMx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4148/Reviewer_YWMx"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a feed forward method to inject text prompts from visual inputs, for visual understanding tasks using large language model. Three variants of the method are presented, which are tested on different visual QA and dialogue datasets. Comparisons show that even without fine tuning the LLM with visual inputs, the system performs comparable or slightly better than competing algorithms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The strengths of the paper are as follows:\n\n1. The method does not require any training, and works simply augmenting the forward path of LLM by injecting semantic visual concepts. \n2. Experiments have been conducted on various QA datasets assessing the performance of the method. The method looks like performing at par or better than some of the prior methods which do joint visual and text encoder training. It beats Flamingo, a visual-text encoder comprehensively on visual QA.\n3. The method can be used easily with different PLMs and paper shows application of the method in multi-modal dialogue system, where the method outputs look reasonable."
                },
                "weaknesses": {
                    "value": "The weaknesses are as follows:\n\n1. The paper describes two variants of the DCI method in Section 3.2.1 and Section 3.2.2, however the variations presented in the experiments are based on the vocabulary selection method (DCI, DCI-A, DCI-LM). The authors have not assessed the performances of the previously described variants.\n2. In experiment section, the visual encoder has not been mentioned explicitly. Authors need to add that information in the tables as well as description.\n3. The vocabulary addition is a major step of the algorithm. Certain details and variations in vocabulary are missing: 1. What is the total number of visual concepts taken in the experiments, 2. What is the difference in output vocabulary of DCI-LM vs other variants. In equation 10, how are authors going from output of LLM to visual concepts. Question by itself can generate very open ended responses from LLM. Similarly for DCI-A, what is the number of top frequent words taken as dictionary. \n4. In Table 2, there is not much difference between the results of BLIP-2 and proposed DCI variants. Authors have not explained the reason behind no significant change in output accuracy wrt original BLIP-2.\n5. Examples of multimodal dialgoue systems show several images with named entities like Great Wall of China, orchid, etc. How is the current framework accounting for named entities in their method? Just visual input can potentially generate hallucinations in the LLM output. Authors have not explored the quality of the system in any details in the paper, hence the section does not add to the contribution of the paper."
                },
                "questions": {
                    "value": "Questions to authors have been posted in weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Privacy, security and safety"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Vocabulary generation is a major step in the algorithm. The source and potential biases of the vocabulary is not clear in the paper."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4148/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698847577503,
            "cdate": 1698847577503,
            "tmdate": 1699636380048,
            "mdate": 1699636380048,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KfYLcJD2XX",
                "forum": "aMfdN4ZQVx",
                "replyto": "jkISxN8Wxq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4148/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4148/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YWMx"
                    },
                    "comment": {
                        "value": "Dear Reviewer YWMx,\n\nThank you for your detailed comments and suggestions. We appreciate your recognition of the soundness, contribution, and presentation of our work. We tried our best to address all the concerns and questions, and update the main paper and appendix in the new version. Please let us know if you have any further concerns or questions to discuss.\n\nBest,\n\nPaper 4148 Authors\n\n---\n\n\n### Two Injection Mechanisms\n- As discussed in the introduction, Figures 1 and 3, Sec. 3.2, Sec. 3.2.1 and Sec. 3.2.2 are complimentary pathways that combine to form Deep Concept Injection.\n- We indeed included qualitative analysis in Figure 4 and Sec. 4.5 to analyze the effect of the constructed adaptation layers. We find that, with the help of the constructed adaptation layers, we successfully augment the intermediate feature to help the model attend more to extracted concepts relevant to the correct answer.\n- We also included quantitative analysis in the appendix due to space limit. In Sec. F.3, we quantitatively analyze the contribution of the two mechanisms and observe that combining the two injection mechanisms yields the best result.\n\n---\n\n### Visual encoders used\n- We indeed included the visual encoders used in the experiments in Sec. 4.1. As a summary, to facilitate fair comparisons, we use the same CLIP ViT-L/14 as FrozenBiLM when comparing with it, and we use the same Q-Former based on ViT-g as BLIP-2 when comparing with it.\n- We also included the information in the caption of Tables 1 and 2.\n- We are happy to indicate this further if the reviewer kindly provides more detailed suggestions.\n\n---\n\n### Vocabulary construction and ethic question\n- We indeed include the details of the vocabulary construction in Sec. 3.3 and G.1. We don\u2019t expect additional bias to be introduced beyond that of the VisualGenome ontology.\n- We indeed included detailed quantitative results and analysis in Sec. F.2 due to the space limit. We generally observe that this process helps remove from the concept recognition results some too generic visual concepts that are irrelevant to the question. We will further add some qualitative examples of the difference between the retrieved concepts when the generic and narrowed vocabulary are used.\n- For DCI-LM, as shown in Eq. (10), we didn\u2019t let the PLM decode in an open-ended manner. We use it to measure the relevance of each word in the generic vocabulary to the question.\n- Once we obtain the conditional probability given Eq. (10), as explained in the last sentence of page 6, we just select the most probable $n_C$ words as the concept vocabulary for retrieval and injection usage.\n- We didn\u2019t change the size of the vocabulary for DCI-A. We just use it as it is, based on the setting of open-ended video/visual question answering. Therefore, it varies from dataset to dataset.\n\n---\n\n### Comparison with BLIP2\n- Thanks for the good question. As we mentioned in Sec G.2, all the hyper-parameters are tuned based on iVQA validation set to avoid fitting other datasets for zero-shot evaluation, which means that current hyper-parameters are not optimal for comparisons with BLIP-2.\n- Our results do not rely on any training, which is not expected to achieve significantly higher accuracy because of its great efficiency. The comparable results we have obtained are already quite surprising and validate the research hypothesis that the projection and adaptation layers are not necessary to be learned.\n\n---\n\n### Named entity\n- Thanks for the good question. Currently, we don\u2019t directly handle named entities in our vocabulary, but this ability can be further integrated if we can also provide a list of named entities that we want the model to recognize.\n- For the Great Wall image, the recognized concepts include \u201cchina\u201d, \u201cfortification\u201d, \u201ctourism\u201d etc. The PLM successfully inferred the most famous Great Wall based on these concepts.\n- For the orchid image, we respectfully disagree that \u201corchid\u201d is a named entity. An \"orchid,\" in general, is a type of flower and falls under a common noun. For this example, the recognized concepts directly contain the concept \u201corchid\u201d.\n- We have also included more discussion about named entity in the revised Appendix H."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4148/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700074153030,
                "cdate": 1700074153030,
                "tmdate": 1700074153030,
                "mdate": 1700074153030,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]