[
    {
        "title": "Neural functional a posteriori error estimates"
    },
    {
        "review": {
            "id": "MpAmXyi1OI",
            "forum": "z62Xc88jgF",
            "replyto": "z62Xc88jgF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8021/Reviewer_h1aP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8021/Reviewer_h1aP"
            ],
            "content": {
                "summary": {
                    "value": "This paper brings \"functional a posteriori error analysis\" from the mathematics community to machine learning. Similar to Hillebrecht et al and many others, the goal is to obtain error certificates, i.e, some formal statement on the upper bound of the error produced by physics-informed neural networks. In functional a posteriori error analysis, the upper bound of the error depends on approximate solution, data, and additional fields that can potentially tighten the bound. This work builds on this direction and proposes a loss function Astral. Experiments show that in an unsupervised setting, Astral outperforms one baseline (Li et al)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I note that I am not an expert in the related domain of the paper. Hence, I requested area chairs for additional reviewers. My score reflects that I am ignorant in this field and do not want to reject papers.\n\nThere are many papers on certifiable machine learning, where the goal is to analyze the error, traditionally from generalization theory like PAC-Bayes bounds. For physics-informed neural networks, I think there might be meaning in investigating practical techniques to upper bound the error.\n\nWhat this paper provides is perhaps the idea of bringing \"functional a posteriori error analysis\", which can be interesting to certain parts of the community."
                },
                "weaknesses": {
                    "value": "On the other hand, I would have preferred if the presentation of the paper was more kind to readers. For example:\n\nThe introduction provides minimum information about astra -- only that it is a loss function with certain benefits like more robustness over residuals and variational losses. In my view, there should be a high-level description of (a) what motivates this specific loss function, and (b) what exactly is this loss function. There should also be reasoning behind it at a high level. It is difficult to grasp the concept and also get interested otherwise. I think the paper can also be more kind to the readers by defining certain terminology before using them, e.g., majorants, posterior error, priori error, etc.\n\nI found section 2.3 to be difficult to parse. The paper states several equations without explaining them in sufficient depth. For example, in equation (9), I did not understand where the definition of astral is from. The paper states \"It is possible to derive\" but at least in the appendix, these derivations should be shown. Equation 12 is also similar. I could not understand the derivation from equation 9 to equation 12. There should also be sufficient reasoning \"why\", for example, equation 12 is a good measure of the predicted solution.\n\nI hope other reviewers can comment more in-depth about technical part of the paper and quality of the experiments."
                },
                "questions": {
                    "value": "My questions for clarification are embedded in the comments above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8021/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8021/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8021/Reviewer_h1aP"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8021/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698435588960,
            "cdate": 1698435588960,
            "tmdate": 1699636989476,
            "mdate": 1699636989476,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "klfEtMdufp",
                "forum": "z62Xc88jgF",
                "replyto": "MpAmXyi1OI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8021/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8021/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the review and thank the reviewer for providing comments on the article.\n\nWe understand that in parts the presented work may be hard to follow. This is, however, subjective, since we can see that pN9X, and SLC5 rank presentation as good and excellent, CFkx ranks it as fair and finally, the present reviewer h1aP ranks it as poor. Given that we have a full spectrum of opinions.\n\n> The introduction provides minimum information about astra -- only that it is a loss function with certain benefits like more robustness over residuals and variational losses. In my view, there should be a high-level description of (a) what motivates this specific loss function, and (b) what exactly is this loss function. There should also be reasoning behind it at a high level. It is difficult to grasp the concept and also get interested otherwise.\n\nThere is a part of the paper that provides high-level details on the Astral loss. We tried to convey this information in 2.1, where the rationale behind using the functional error estimate as a loss function is given (certain favorable properties are listed).\n\n> I think the paper can also be more kind to the readers by defining certain terminology before using them, e.g., majorants, posterior error, priori error, etc.\n\nWe agree that these terms are not properly defined. We will make sure to provide brief comments when appropriate. More specifically, we will comment on the terms majorant and minorant right after equation (2) where they first appeared. Similarly, we will comment on a priori and a posteriori errors in the introduction where these terms appeared first.\n\n> I found section 2.3 to be difficult to parse. The paper states several equations without explaining them in sufficient depth. For example, in equation (9), I did not understand where the definition of astral is from. The paper states \"It is possible to derive\" but at least in the appendix, these derivations should be shown. Equation 12 is also similar. I could not understand the derivation from equation 9 to equation 12\n\nWe agree that the presentation is sketchy here. The main reason why is the limit on the length of the paper. We provided references in the text sufficient for the reproduction of equations but we agree that it is better to make paper as self-sufficient as possible. Shortly we will provide a revised version of the manuscript with two additional appendices: the first one with the derivation of upper bound (9), and the second one with the explanation of how to arrive at (12) from (9).\n\n> There should also be sufficient reasoning \"why\", for example, equation 12 is a good measure of the predicted solution.\n\nWe agree that this is an important question and thank the reviewer for bringing this up. Indeed, the manuscript does not explain properly why the energy norm (and its upper bound) is a good measure of accuracy. In some sense this material is standard, but we will review it properly. For details we refer to the answer https://openreview.net/forum?id=z62Xc88jgF&noteId=SQ0cDlqod2, where we address this question in some detail."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8021/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700396625022,
                "cdate": 1700396625022,
                "tmdate": 1700396625022,
                "mdate": 1700396625022,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HcVDko8zkJ",
            "forum": "z62Xc88jgF",
            "replyto": "z62Xc88jgF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8021/Reviewer_SLC5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8021/Reviewer_SLC5"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new loss function for supervised and physics-informed training of neural networks and operators that incorporates a posteriori error estimate. The trained model can guarantee the approximation error as this is done by the new loss function aiming at minimizing the theoretical posterior error estimate. The later has been established for a number of PDEs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.  It is a novel idea to adopt the theoretical  functional a posteriori error estimates for the learning objective. The functional a posteriori error estimate was initially established in the conventional finite element method analysis for PDE.\n\n2. The entire framework has been clearly described (at least I can follow the main stream of the paper, although I am not the expert in this particular area).\n\n3. The paper is well motivated with the goal to mitigate neural PDE solvers' inability guarantee good accuracy in practice."
                },
                "weaknesses": {
                    "value": "Although this is a viable approach in a guaranteed way to produce reliable neural PDE solvers, the application could be limited, e.g., in the case for the problems where there is no theoretical posteriori estimates available."
                },
                "questions": {
                    "value": "Not sure why the full GitHub repository is not made available for review."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8021/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8021/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8021/Reviewer_SLC5"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8021/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698534710467,
            "cdate": 1698534710467,
            "tmdate": 1700686523452,
            "mdate": 1700686523452,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "L8Rln3shGY",
                "forum": "z62Xc88jgF",
                "replyto": "HcVDko8zkJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8021/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8021/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We want to thank the reviewer for taking part in the discussion of the manuscript.\n\nWe agree that the approach is limited to the PDEs where the functional upper bound can be derived, and this is the main downside. However, for many practically relevant equations, the functional error estimate exists. A non-exhaustive list of these equations is given at the end of 2.3 of the present manuscript.\n\nHere, we would like to mention that error majorants are available for the Maxwell equation and a wide class of variational problems with convex functions.\n\nBesides that, we provide additional examples of how to apply functional error estimate: $D=2$ elliptic equation in the L-shaped, $D=1+1$ convection-diffusion equation. More details are available in\u00a0https://openreview.net/forum?id=z62Xc88jgF&noteId=Wy4TDErd0T.\n\nAs for the code, we decided not to share the GitHub repository because direct sharing is prohibited by rules. On the other hand, anonymization of the repository is tedious. The Google Colab notebooks that we share are anonymous and can be immediately used to reproduce the results since they run in the browser. We will make the code available after the review stage."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8021/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700396397756,
                "cdate": 1700396397756,
                "tmdate": 1700396397756,
                "mdate": 1700396397756,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "74nG1ulET2",
                "forum": "z62Xc88jgF",
                "replyto": "L8Rln3shGY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8021/Reviewer_SLC5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8021/Reviewer_SLC5"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for addressing my comments.  \n\nI still think releasing code is helpful in review. To protect your identity, you have a lot of ways to do so, e.g., https://anonymous.4open.science/ or as supplementary to your paper submission on openreivew."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8021/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700620766434,
                "cdate": 1700620766434,
                "tmdate": 1700620766434,
                "mdate": 1700620766434,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iPFaqoAxgf",
                "forum": "z62Xc88jgF",
                "replyto": "HcVDko8zkJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8021/Reviewer_SLC5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8021/Reviewer_SLC5"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for releasing the code.   I am revising my rating up."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8021/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700686430102,
                "cdate": 1700686430102,
                "tmdate": 1700686479941,
                "mdate": 1700686479941,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "D0YNWWjmRK",
            "forum": "z62Xc88jgF",
            "replyto": "z62Xc88jgF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8021/Reviewer_pN9X"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8021/Reviewer_pN9X"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method for training neural networks that provide solutions to PDEs (including PINNs, Neural Operator networks, and vanilla surrogate models). The idea is to upper bound the error of the predicted solutions and incorporate this upper bound into the training loss of the neural network. Constructing the the upper bound on the solution error is non-trivial and requires expert knowledge, but this provides a way of incorporating domain knowledge into the model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well-written and easy to follow. Figures are clear and informative.\n- The paper addresses a method for computing error bounds on solutions to PDEs, a challenge of high interest to the ML for physics community.\n- Experiments support the claims."
                },
                "weaknesses": {
                    "value": "- It seems like this approach will have limited applicability. The first limitation is that the U function must be specified by the practitioner, but this is addressed in the paper, seems reasonable, and is a way to incorporate domain knowledge. The bigger issue is that in most scenarios, I imagine that predicting the error certificates is at least as difficult (and usually more difficult) than predicting the solution."
                },
                "questions": {
                    "value": "- My intuition is that the error certificate w_U can be thought of as an *explanation* of how the solution emerges. For example, the certificate could explain the predictions of a day-ahead weather forecast model by providing the evolution of the weather variables over the intermediate time steps. When a good certificate can be generated, the practitioner can feel confident that the solution is correct (with hard error bounds!), but otherwise the error bounds will be loose and the practitioner will not have much confidence in the solution. This all seems desirable, but I wonder if there exists a large set of problems for which the surrogate model can produce good solutions but not produce good certificates?\n\nThere are some minor typos throughout:\n- Figure 1: should approximate solution u by u^tilde?\n- Section 2.1 \"deep learning is to\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8021/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698870460811,
            "cdate": 1698870460811,
            "tmdate": 1699636989269,
            "mdate": 1699636989269,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vXeY7YSJoY",
                "forum": "z62Xc88jgF",
                "replyto": "D0YNWWjmRK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8021/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8021/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for the time spent reading the paper and for the feedback.\n\nTypos:\n\n- Indeed, the approximate solution should be $\\widetilde{u}$. We will fix that in the revision.\n- \u201cdeep learning is to\u201d --> \u201cdeep learning to\u201d\n\nAs for the question. Indeed, the solution may be good be the certificate produced by the surrogate model is not good, i.e., they lead to an upper bound that is much larger than the energy norm. In our experiments, however, we did not observe this behavior when input data is in distribution. Out of distribution, this may happen, but the situation is hard to analyze. In this unfortunate case when certificates are useless, the practitioner has two choices:\n\n1. Recompute the solution with a more reliable method.\n2. Optimize the loss with a fixed solution to find better certificates.\n\nSo the certificates are useful only in the situation when the predicted upper bound is smaller than the desired tolerance. The situation in machine learning and statistics when the method provides a measure of certainty for the obtained result (e.g., vote counting, document sorting). When the measure is good enough, no intervention is needed. When the measure is bad, the sample is processed with a more reliable method (e.g., by a human being).\n\n> Constructing the the upper bound on the solution error is non-trivial and requires expert knowledge, but this provides a way of incorporating domain knowledge into the model.\n\nIndeed, we agree that the construction of the upper bound is not straightforward and requires expert knowledge. We recognize this as a weakness of the approach and provide some discussion in these lines in\u00a0https://openreview.net/forum?id=z62Xc88jgF&noteId=L8Rln3shGY. This discussion includes the list of equations where the upper bound is known and the additional example of the convection-diffusion equation that we introduce to showcase that not only elliptic equations can be treated with functional a posteriori error estimate (see\u00a0https://openreview.net/forum?id=z62Xc88jgF&noteId=Wy4TDErd0T)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8021/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700396501998,
                "cdate": 1700396501998,
                "tmdate": 1700396501998,
                "mdate": 1700396501998,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IqeUmQHla6",
                "forum": "z62Xc88jgF",
                "replyto": "D0YNWWjmRK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8021/Reviewer_pN9X"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8021/Reviewer_pN9X"
                ],
                "content": {
                    "title": {
                        "value": "Update after reading comments from other reviewers and the responses"
                    },
                    "comment": {
                        "value": "I have one of the most positive reviews for this paper because I think it is well-written and brings new ideas to the ML community. I admit that it is out of my area of expertise and the other reviewers likely have more knowledge of this area than me, but I think the authors have responded well to the criticisms and comments, so I see no reason to change my positive review. \n\nIn particular, the public comments from an expert in this area were critical of the paper's novelty. (https://openreview.net/forum?id=z62Xc88jgF&noteId=1MwG3D88ca) The responses from the authors seem thorough and make sense to me, but I admit I do not have the depth of expertise to be confident in my judgement."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8021/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708290306,
                "cdate": 1700708290306,
                "tmdate": 1700708290306,
                "mdate": 1700708290306,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Y0ysDtRU5i",
            "forum": "z62Xc88jgF",
            "replyto": "z62Xc88jgF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8021/Reviewer_CFkx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8021/Reviewer_CFkx"
            ],
            "content": {
                "summary": {
                    "value": "This paper propose a loss function for training PINNs with theoretically guaranteed error bounds. Experiments are done for verifying their claims."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. By the theory from functional a posteriori error analysis, using Astral loss gives theoretical guarantee that the output of neural network is the exact solution in the sense that their distance is zero in some function space.\n\n2. Astral loss can be computed explicitly for common PDE problems. The authors use elliptic equation as an example.\n\n3. Some experiments are done to compare Astral loss with the commonly used residual loss using different models/equations."
                },
                "weaknesses": {
                    "value": "1. Although the use of this type of loss in this setting might be new, this work does not prove any new theoretical results.\n\n2. That being said, experiment is a very important component in this paper, however, I find the evaluation metric of the solution very interesting. More specifically, let $u$ be the output of neural networks and $u^*$ be the exact solution. The test error is usually computed using relative $L^2$ norm (See for example [1][2]), i.e.\n$$|| u - u^*||_2^2 / ||u^*||_2^2 = \\int|u - u^*|^2dx / \\int |u^*|^2 dx.$$\nHowever, in Figure 4, when evaluating solutions, the mean error is computed using equation (15), the energy norm. \n\n(i). why not using the relative $L^2$ norm? How does Astral loss perform if the evaluation is done in $L^2$? \n\n(ii). The a posteriori error bound is in the energy norm, i.e. \n$$L(u, w_L) \\leq |||u-u^*||| \\leq U(u, w_U).$$\nso I would naturally expect Astral loss to achieve fairly small error in this energy norm, but this does not necessarily imply the solution is \"better\". Equations can be solved in different spaces. In fact, I think the space $L^2$ is more commonly used when people study existence and uniqueness of PDE solutions. \n\n(iii). There could be a relation between the energy norm and $L^2$ norm. More explanation is needed for the specific choice of the evaluation metric since it differs from the previous literature. \n\n\n[1] Li et al., Physics-Informed Neural Operator for Learning Partial Differential Equations\n\n[2] Wang et al., An Expert's Guide to Training Physics-informed Neural Networks"
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8021/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8021/Reviewer_CFkx",
                        "ICLR.cc/2024/Conference/Submission8021/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8021/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699414137103,
            "cdate": 1699414137103,
            "tmdate": 1700667877697,
            "mdate": 1700667877697,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SQ0cDlqod2",
                "forum": "z62Xc88jgF",
                "replyto": "Y0ysDtRU5i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8021/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8021/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank the reviewer for bringing up an important question. We agree that the relation between errors computed in energy and the $L_2$ norm is an important topic not covered in the article. There are two parts to consider: the theoretical one \u2014 how the losses are related, and the practical one \u2014 what are typical magnitudes of errors in each norm when networks are trained with Astral and residual losses.\n\nThe theoretical part is as follows. Suppose the error is $e$, the diffusion coefficient is $a$, and the reaction term is $b^2$. In this case, we can find\n\n$$|||e|||^2 = \\int_{\\Gamma} dx \\sum_{ij} \\frac{\\partial e}{\\partial x_{i}}\\frac{\\partial e}{\\partial x_{j}}a_{ij} + ||be||^2 \\geq \\lambda_{\\min}^2 ||e||^2 + ||be^2|| \\geq \\lambda_{\\min}^2 ||e||^2 + \\inf_{x} b(x)^2||e^2||,$$\n\nwhere $\\lambda_{\\min}$ is a minimal eigenvalue of the elliptic problem $-\\frac{\\partial}{\\partial x_{i}} a_{ij} \\frac{\\partial}{\\partial x_{j}}u(x) = \\lambda u(x)$ defined on the same domain and with the same boundary conditions as the original elliptic problem. From the expression above, we obtain the bound\n\n$$||e||\\_{2}^{2} \\\\leq \\\\frac{1}{\\\\lambda_{\\\\min}^2 + \\\\inf_{x} \\\\left(b(x)\\right)^2} |||e|||.$$\n\nThis bound can be simplified and made more practical (e.g., by removing minimal eigenvalue that may be hard to compute) by various means but the bound above is already sufficient to claim that when error is sufficiently small in the energy norm, this is also the case for the standard $L_2$ norm.\n\nTo answer the practical part, we conducted the set of additional experiments described in the reply\u00a0https://openreview.net/forum?id=z62Xc88jgF&noteId=Wy4TDErd0T. There, we compared relative errors and energy norms for two additional test cases ($D=2$ elliptic equation in L-shaped domain, $D=1+1$ convection-diffusion equation) for Astral and residual losses.\n\nWe would like to also add that, indeed, solutions of PDE can be considered in various spaces. Most typically, however, not a space $L_2$ is used but Sobolev spaces to properly define weak solutions (see [E] Part II). Errors in natural Sobolev norms are sometimes used in deep learning literature. For example, a recent paper with FNO extension uses this norm [K]. Other metrics besides relative $L_2$ norms are also used, e.g., $L_1$ norm divided by $L_{\\infty}$ [B], relative RMSE of energy spectrum (turbulence) [C], etc\n\nThe main reason we use energy norms is because it is a natural norm for elliptic problems. Namely, the energy norm will weigh the error according to the importance of spaces spanned by eigenvalues of the elliptic operator in question (eigenspaces with larger eigenvalues receive more weight). The energy norm is a de facto standard for elliptic PDEs. For example, FEM error estimate is done with error energy norm [SB, for example, 1.5.2 and 1.5.3]; the convergence theory for Conjugate Gradient method (Krylov subspace method used to solve a linear system with symmetric positive-definite matrices including the ones originated from elliptic equations) is naturally formulated with energy norm [S].\n\nFinally, we would like to point out that we do not claim that the solution obtained with Astral loss is better. Our main claim is that Astral loss provides a natural way to construct an error majorant that provides a tight bound on the error. Note that the residual does not have this property. It is easy to see that the residual can be arbitrarily bad for the solution that is arbitrarily close to the exact one. Namely, we can consider a small high-frequency perturbation $\\\\delta(x) = \\\\epsilon\\\\sin(2\\\\pi L x)$ to the exact solution. This perturbation will introduce an error of the order $\\epsilon$ and residual of order $\\left(2 \\\\pi L\\right)^2 \\epsilon$. Now one can take $L$ large enough to have an arbitrarily large residual.\n\nWe hope that this discussion clarifies the relation between $L_2$ and energy norms and explains our choice of metric. We will incorporate this material in the corrected version of the paper that will appear shortly.\n\n[E] \u2014 Evans LC. Partial differential equations. American Mathematical Society; 2022 Mar 22.\n\n[K] \u2014 https://arxiv.org/abs/2310.00120v1\n\n[B] \u2014 https://arxiv.org/abs/2202.03376\n\n[C]\u2014 https://arxiv.org/abs/2112.01527\n\n[SB] \u2014 Szab\u00f3 B, Babu\u0161ka I. Finite Element Analysis: Method, Verification and Validation.\n\n[S] \u2014 Shewchuk JR. An introduction to the conjugate gradient method without the agonizing pain."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8021/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700396262676,
                "cdate": 1700396262676,
                "tmdate": 1700437237329,
                "mdate": 1700437237329,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QmQHJ7eXZU",
                "forum": "z62Xc88jgF",
                "replyto": "Wy4TDErd0T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8021/Reviewer_CFkx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8021/Reviewer_CFkx"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the additional experiments. Could you please comment on why the residual loss achieved better results for the L-shaped domain? As the theory suggests, I would expect Astral to always give better energy norms."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8021/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700667088714,
                "cdate": 1700667088714,
                "tmdate": 1700667088714,
                "mdate": 1700667088714,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]