[
    {
        "title": "ReSimAD: Zero-Shot 3D Domain Transfer for Autonomous Driving with Source Reconstruction and Target Simulation"
    },
    {
        "review": {
            "id": "aFu9Q3S2RB",
            "forum": "1d2cLKeNgY",
            "replyto": "1d2cLKeNgY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1573/Reviewer_Fz1j"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1573/Reviewer_Fz1j"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces ReSimAD, a unified reconstruction-simulation-perception paradigm that addresses the domain shift issue when 3D detectors face cross-domain performance degradation in the field of autonomous driving.\nThe experimental results conducted on multiple 3D detectors and datasets verify that the proposed paradigm can maintain cross-domain performance in the simulated target domain. In addition, it can also assist and accelerate model training optimization, which has a certain application value."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Regarding the phenomenon of cross-domain performance degradation in 3D detectors in the field of autonomous driving, this paper proposes a paradigm of reconstruction-simulation-perception from the perspective of data sources. This paradigm can alleviate or partially solve the problem of cross-data domain discrepancies. The proposed approach, named ReSimAD, has significant research value."
                },
                "weaknesses": {
                    "value": "1)\tThe readability of the paper needs improvement, such as the illustrations, explanations, and better formatting.\n2)\tSome necessary elaborations and supporting evidence need to be added."
                },
                "questions": {
                    "value": "1)\tFigure 1 and 3 can be made more illustrative of the proposed paradigm.\n2)\tThe formatting of the paper needs adjustment, for example, ensure that figures and corresponding sections are not too far apart.\n3)\tTo eliminate artifacts in Point-to-mesh Implicit Reconstruction, it should be further explained how the authors performed point cloud registration when consolidating all frames from the corresponding sequence.\n4)\tThe interpretation of \"Closed Gap\" in Table 2 and its analysis are not provided in the paper.\n5)\tThe paper does not mention the significance or the specific impact of \"the matching of vehicle traffic flow density\" mentioned in Section 4 (Mesh-to-point Rendering).\n6)\tIn Table 5, what is the reason of the significant gap between using zero-shot and oracle methods? The details regarding the sample quantity and other experimental settings can be supplemented in the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1573/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698398625657,
            "cdate": 1698398625657,
            "tmdate": 1699636085751,
            "mdate": 1699636085751,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kcUCRoBsAP",
                "forum": "1d2cLKeNgY",
                "replyto": "aFu9Q3S2RB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1573/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1573/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Fz1j"
                    },
                    "comment": {
                        "value": "**Q1**: Figures 1 and 3 can be made more illustrative of the proposed paradigm.\n\n**A1**: We are grateful to the reviewers for pointing out these problems in the manuscript. We have polished Figures 1 and 3 of the manuscript to further clearly our method.\n\n&ensp;\n\n**Q2**: The formatting of the paper needs adjustment, for example, ensure that figures and corresponding sections are not too far apart.\n\n**A2**: We have noticed the placement issue with Tables 2, 3, and 4. Thank you very much for pointing that out, and we will make the necessary corrections in the upcoming updated version.\n\n&ensp;\n\n**Q3**: To eliminate artifacts in Point-to-mesh Implicit Reconstruction, it should be further explained how the authors performed point cloud registration when consolidating all frames from the corresponding sequence.\n\n**A3**: We utilize the well-annotated Waymo dataset, which provides precise vehicle and LiDAR poses for each frame in all sequences. It is worth mentioning that, to avoid motion blur, prior to consolidating all frames, we first remove dynamic objects from the point cloud and only stitch together multiple frames of the static background. We further filter out outlier point clouds in the scene using point neighborhood statistics. Subsequently, we align the point cloud data from each frame in the current sequence to the starting frame of the sequence by projecting them using the LiDAR poses, thus achieving point cloud registration.\n\n&ensp;\n\n**Q4**: The interpretation of \"Closed Gap\" in Table 2 and its analysis are not provided in the paper.\n\n**A4**:  Thanks. As stated in caption of Table 2 of the manuscript, the \"Closed Gap\" denotes the detection accuracy drop rate closed by various methods along the Source Only and Oracle results, which is a common evaluation metric used by previous Unsupervised Domain Adaptation (UDA) works [1, 2]. \"Closed Gap\" can be calculated by: $(AP_{methods} - AP_{Source-only}) / (AP_{Oracle} - AP_{Source-only})$.\n\n**References**:\n\n[1] Jihan Yang, Shaoshuai Shi, Zhe Wang, Hongsheng Li, and Xiaojuan Qi. St3d: Self-training for unsupervised domain adaptation on 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10368\u201310378, 2021.\n\n[2] Jiakang Yuan, Bo Zhang, Xiangchao Yan, Tao Chen, Botian Shi, Yikang Li, and Yu Qiao. Bi3d: Bi-domain active learning for cross-domain 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15599\u201315608, 2023b.\n\n&ensp;\n\n**Q5**: The paper does not mention the significance or the specific impact of \"the matching of vehicle traffic flow density\" mentioned in Section 4 (Mesh-to-point Rendering).\n\n**A5**: Due to the strong correlation between vehicle movement and road network structure, vehicles generally cannot travel to non-driving areas. In this paper, we rely on the Waymo Open Dataset as the source domain dataset to complete background reconstruction, including roads. If the matching of vehicle traffic flow cannot be ensured in rendering, it may result in abnormal vehicle movement, such as collisions with surrounding buildings. Therefore, it is crucial to ensure the same density of vehicle traffic flow as the source domain dataset.\n\n&ensp;\n\n**Q6**: In Table 5, what is the reason of the significant gap between using zero-shot and oracle methods? The details regarding the sample quantity and other experimental settings can be supplemented in the paper.\n\n**A6**: Thanks for your valuable suggestions. In Table 5, the main reason for the significant accuracy gap between using zero-shot and Oracle is that, the Waymo-to-nuScenes cross-dataset setting is very challenging for 3D UDA study, as reported in other works [1, 2, 3], since Waymo and nuScenes datasets have totally different LiDAR beam and scene layout.\n\n**References**:\n\n[1] Jihan Yang, Shaoshuai Shi, Zhe Wang, Hongsheng Li, and Xiaojuan Qi. St3d: Self-training for unsupervised domain adaptation on 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10368\u201310378, 2021.\n\n[2] Jiakang Yuan, Bo Zhang, Xiangchao Yan, Tao Chen, Botian Shi, Yikang Li, and Yu Qiao. Bi3d: Bi-domain active learning for cross-domain 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15599\u201315608, 2023b.\n\n[3] Zhuoxiao Chen, Yadan Luo, Zheng Wang, Mahsa Baktashmotlagh, Zi Huang. Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and Class-balanced Pseudo-Labeling. In\u00a0*Proceedings of the IEEE/CVF International Conference on Computer Vision*\u00a0(pp. 3714-3726)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1573/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538665299,
                "cdate": 1700538665299,
                "tmdate": 1700538665299,
                "mdate": 1700538665299,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mk4s7iHXJ8",
            "forum": "1d2cLKeNgY",
            "replyto": "1d2cLKeNgY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1573/Reviewer_bhEL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1573/Reviewer_bhEL"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces 3d domain transfer pipeline, ReSimAD.\nIt focuses on bridging the gap between old autonomous driving datasets and newly collected real-life datasets.\nThe pipeline is threefold, 1) point-to-mesh implicit reconstruction, 2) mesh-to-point rendering, and 3) zero-shot perception process.\nFirst two stages are designed to simulate target domain's LiDAR configuration using source domains. For better granulity compared to using raw sparse LiDAR points, it utilizes implicit SDF representation.\nThe last stage is to use the simulated dataset for 3D detection method and perform zero-shot inference on the real dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper suggests dataset generation/simulation pipeline that may enrich/adjust old annotated dataset to the new target domain."
                },
                "weaknesses": {
                    "value": "1. Poor presentation\nOverall, the placement of figures and tables is not aligned with the text, thereby the whole paper is difficult to follow.\n\n2. Technical novelty & Writing\nThe paper mainly focuses on the dataset simulation process using old annotated dataset.\nTherefore, the most of the methodology is restricted to step-by-step instructions, rather than providing theoretical insights or verifications.\nI believe the paper's contribution on introducing new dataset simulation pipeline does not exceeds its lack of technical novelty.\n\n3. Questionable dataset selection\nSince Waymo dataset is more recent and contains more LiDAR sensors all around the vehicle, compared to nescenes or KITTI, wouldn't it be more plausible to simulate Waymo from KITTI, rather than KITTI from Waymo, to be more in coherence with the paper's motivation?\nIt seems like the pipeline only focuses on interchanging sensor configuration, using the richest point cloud information. Please elaborate.\n\nOn the minor note, I believe that this paper is more related to computer vision or robotics field than machine learning."
                },
                "questions": {
                    "value": "Addressed in weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1573/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1573/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1573/Reviewer_bhEL"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1573/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698770592041,
            "cdate": 1698770592041,
            "tmdate": 1700705241201,
            "mdate": 1700705241201,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Oiiboe4qBr",
                "forum": "1d2cLKeNgY",
                "replyto": "mk4s7iHXJ8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1573/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1573/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bhEL (Part 1/2)"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the reviewer's valuable comments. We have also tried our best to improve the quality and presentation of this manuscript, according to this reviewer\u2019s comments.\n\n**Q1**: Poor presentation.\n\n**A1**: Thanks for your suggestions. The usage of English language and grammar of the revised manuscript has been double-checked thoroughly, and all the mistakes and errors which we can find have been revised and corrected. And the layout and order of the various tables and diagrams in the manuscripts have been better organized.\n\n**Q2**: Technical novelty: This paper focuses on the dataset simulation process.\n\n**A2**: In order to provide a clearer view of why and how the proposed ReSimAD is novel in autonomous driving community, we give an in-depth discussion of **the following three aspects**:\n- Motivation: First, based on the fact that the model pre-trained on source-domain still faces serious performance drop when deployed on the target domain (e.g., different weather or different LiDAR beam), this paper provides a new view of the source-domain reconstruction followed by the new-domain simulation. Basically, we assume that the data from source-domain are well-annotated and massive. However, direct training on the data from the source domain may result in the model overfitting to the data distribution of the source domain. Thus, we produce 3D mesh from the source domain to **decouple the domain characteristics**, and simulate the target-domain-like data according to the produced 3D mesh.  We are the first to achieve zero-shot target-domain 3D object detection.\n- Framework: Second, directly generating the 3D mesh using the off-the-shelf reconstruction method such as VDBFusion is challenging and cannot bring satisfactory performance gains for zero-shot 3D object detection, as demonstrated in Table 2 and Figure 6 of the revised manuscript. This is mainly because we assume that only point clouds are available and the 3D scene-level reconstruction based on 3D points is very challenging, due to: a) Sparse 3D points, and b) Insufficient reconstruction evaluation.\n\n  a) To address the issue of sparse 3D points, we aggregate all frames from the same scene sequence in Waymo dataset. Besides, we merge point clouds from side LiDARs to fill the blind spots around the vehicle, utilizing denser point clouds as input for reconstruction. \n  \n  b) To address the issue of insufficient reconstruction evaluation, we render the Waymo points based on the generated 3D mesh, and calculate the Root Mean Square Error and Chamfer Distance between the simulated Waymo points and the original Waymo points. The results in Table 6 of the Appendix indicate that the sequences we used have low errors and exhibit high reconstruction quality.\n- Task: Finally, aiming to reduce the annotation cost of autonomous driving manufacturers for an unseen domain, we present the zero-shot target-domain 3D object detection task, where it is assumed that all annotated data from the source domain are available and no target domain data can be accessed. The presented task setting is practical, and as illustrated in Tables 2 and 4, we have verified that the proposed ReSimAD can achieve a satisfactory detection performance on the target domain compared to baseline models and UDA models."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1573/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538266416,
                "cdate": 1700538266416,
                "tmdate": 1700538266416,
                "mdate": 1700538266416,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bP9AzEBwl5",
                "forum": "1d2cLKeNgY",
                "replyto": "mk4s7iHXJ8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1573/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1573/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bhEL (Part 2/2)"
                    },
                    "comment": {
                        "value": "**Q3**: Questionable cross-domain setting selection.\n\n**A3**: Thanks for your concerns. However, we would like to clarify that the proposed cross-domain setting (namely, adaptation from Waymo to nuScenes) ****is rational****, due to the following reasons:\n\n1) Fair comparison with the existing Unsupervised Domain Adaptation (UDA) methods: The existing UDA works [1, 2, 3, 4, 5] employ the Waymo-to-KITTI, Waymo-to-nuScenes setting to study the model cross-domain ability under the 3D scenario. In order to make fair comparisons with previous cross-dataset works, we align their cross-dataset setting, selecting the Waymo-to-KITTI, Waymo-to-nuScenes, and Waymo-to-ONCE as the baseline setting.\n\n2) Reconstruction process: In order to achieve high-quality reconstruction results, we have to merge multiple frames sampled from the same 3D sequence to perform the reconstruction process, which is also inspired by UniSim and StreetSurf. Different from these works, we also utilize the side LiDARs to fill the blind spots around the vehicle to boost the reconstruction quality. Besides, we are the first to explore the so-called one-for-all setting, meaning that the source-domain reconstruction process is only performed once using the proposed ReSimAD, and simulated on different downstream datasets including KITTI, nuScenes, and ONCE.\n\n3) ONCE is a challenging dataset, publicly available in 2022. We also select ONCE as the target domain, evaluating the cross-dataset performance of the proposed ReSimAD. On Waymo-to-ONCE setting, we also observe some interesting findings as reported in Table 2 of the manuscript: a) The existing UDA work, such as ST3D[1], only achieve 68.13\\% mAP compared to 68.82\\%  mAP of the source only (baseline result). This is mainly due to that the data distribution of ONCE dataset is very different from the Waymo dataset, including category definition and scene layout. b) Our ReSimAD exceeds the cross-dataset performance achieved by UDA work under the challenging Waymo-to-ONCE setting. Based on this, we believe that ReSimAD would pave the way for future exploration in zero-shot cross-dataset 3D study.\n\nAccording to the reviewer\u2019s comments, we have supplemented detailed descriptions and explanations on Pages 7and 8 of the revised manuscript.\n\n**References**:\n\n[1] Jihan Yang, Shaoshuai Shi, Zhe Wang, Hongsheng Li, and Xiaojuan Qi. St3d: Self-training for unsupervised domain adaptation on 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10368\u201310378, 2021.\n\n[2] Jiakang Yuan, Bo Zhang, Xiangchao Yan, Tao Chen, Botian Shi, Yikang Li, and Yu Qiao. Bi3d: Bi-domain active learning for cross-domain 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15599\u201315608, 2023b.\n\n[3] Zhuoxiao Chen, Yadan Luo, Zheng Wang, Mahsa Baktashmotlagh, Zi Huang. Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and Class-balanced Pseudo-Labeling. In\u00a0*Proceedings of the IEEE/CVF International Conference on Computer Vision*\u00a0(pp. 3714-3726).\n\n[4] Yi Wei, Zibu Wei, Yongming Rao, Jiaxin Li, Jie Zhou, Jiwen Lu. Lidar distillation: Bridging the beam-induced domain gap for 3d object detection. In\u00a0*European Conference on Computer Vision*\u00a0(pp. 179-195).\n\n[5] Jiageng Mao, Shaoshuai Shi, Xiaogang Wang, Hongsheng Li. 3d object detection for autonomous driving: A review and new outlooks. arXiv preprint arXiv:2206.09474."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1573/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700538312976,
                "cdate": 1700538312976,
                "tmdate": 1700538690430,
                "mdate": 1700538690430,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CshluZ4rWq",
                "forum": "1d2cLKeNgY",
                "replyto": "bP9AzEBwl5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1573/Reviewer_bhEL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1573/Reviewer_bhEL"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their response.\nI still think the main contribution of this paper is majorly focused on data simulation pipeline, which, still seems unfair since as the authors denoted, there exists sparsity issue for older LiDAR datasets.\nHowever, I admit that other approaches and suggested benchmark follow same domain-shift scenarios, and is not the authors' fault.\nI change my rating to 5. I am not against the acceptance of this paper, but I still believe there exists a lot of room for improvement."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1573/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705611944,
                "cdate": 1700705611944,
                "tmdate": 1700705611944,
                "mdate": 1700705611944,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tU3ufSpZL4",
            "forum": "1d2cLKeNgY",
            "replyto": "1d2cLKeNgY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1573/Reviewer_kWr7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1573/Reviewer_kWr7"
            ],
            "content": {
                "summary": {
                    "value": "This is a system paper. The authors study a weakened domain generalization setting for 3d object detection from lidar point clouds. For example, training only on Waymo dataset and only accessing the target domain's lidar statistics, the setting pursues good performance on nuScenes. Since information about the target domain is not completely unknown, this is a weakened domain generalization setting. The proposal is to reconstruct the mesh using aggregated LIDAR (or RGB? as NeuS is mentioned). Then the background mesh is put into Carla and car assets are placed according to object size matching. The lidar signal is simulated using the composed scene in Carla. Then the authors train detectors using these simulated data and show the results out-perform the UDA baseline."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ The idea generally makes sense and the authors benchmark it in a large-scale, showing meaningful margins."
                },
                "weaknesses": {
                    "value": "- The biggest issue is a lack of clarify, for both the mesh reconstruction and the sampling part:\n* The reconstruction does not define input, output and losses formally. The only equation is depth rendering. Aggregated lidar piont clouds are used for training? (this is highlighted in bold texts) The authors mention NeuS and I am not sure whether RGB rendering losses are used. \n* Lidar rendering is difficult and there are some sphiscated methods to simulate second-returns [A]. Again the lidar rendering part does not contain any formal mathematical exposition so I cannot understand what the lidar rendering formulation is.\nSince two major algorithmic parts are not understandable, I am rating presentation as poor.\n\n- Let alone the presentation issue about algorithms, I will just assume that these two parts invoke some black-box functions and only consider the system. In this regard, I find the mesh of poor quality. This is understandable as recent papers from my group can only reconstruct meshes from lidar with similar quality. Having that said, I cannot understand why points generated from them are meaningful for detection. The authors should present a systematic evalution for mesh quality (Table.6 does not make sense to me) and rendered point cloud quality. Some comparisons are also needed, e.g., comparing with point clouds rendered from VDBF?\nThis concern makes me rate soundness as fair.\n\nMinor but still confusing issues:\n- Fig.2 is confusing. I cannot understand what the differences are except for the figure color.\n- Table.1 gives literally no additional information since target domains are already mentioned in texts. And Waymo is also target domain\uff1f\n\n[A] Neural LiDAR Fields for Novel View Synthesis"
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1573/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1573/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1573/Reviewer_kWr7"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1573/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698773982068,
            "cdate": 1698773982068,
            "tmdate": 1700673982063,
            "mdate": 1700673982063,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "K0umiBm79E",
                "forum": "1d2cLKeNgY",
                "replyto": "tU3ufSpZL4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1573/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1573/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kWr7"
                    },
                    "comment": {
                        "value": "Thank you for your comments. We provide discussions and explanations about your concerns as follows.\n\n**Q1**: Clarifying issues related to mesh reconstruction.\n\n**A1**: The main contribution claimed in this paper is the unified reconstruction-simulation-perception paradigm that addresses the domain shift issue. Considering previous work, StreetSurf [1], current advanced reconstruction methods are already capable of achieving high-quality point cloud rendering in the target domain, and the specific reconstruction methods are not our main contribution. In the mesh reconstruction section, we opt for a LiDAR-only setting, which allows us to be unaffected by lighting conditions and to obtain a richer and larger dataset of the scene. Drawing inspiration from StreetSurf [1], **the reconstruction input is derived from LiDAR rays, and the output is the predicted depth**. \n\nOn each sampled LiDAR beam $\\mathbf{r}\\_{\\text{lidar}}$, we apply a logarithm L1 loss on $\\hat{D}^{(\\mathrm{cr}, \\mathrm{dv})}$, the rendered depth of the combined close-range and distant-view model:\n$$\n{\\mathscr{L}}\\_{geometry} = \\ln \\\\left ( |\\hat{D}^{(\\mathrm{cr}, \\mathrm{dv})} (\\mathbf{r}\\_{lidar}) - D(\\mathbf{r}\\_{lidar }) | + 1\\\\right )\n$$\n\nTo achieve improved reconstruction quality, we aggregate all frames from the same scene sequence in the Waymo dataset. Additionally, we merge point clouds from side LiDARs to fill the blind spots around the vehicle, utilizing denser point clouds as input for reconstruction.\n\nSimultaneously, we have conducted a quantitative evaluation of the reconstruction. Within the target domain LiDAR FOV (which is smaller than the source domain LiDAR FOV), we sample and calculate the Root Mean Square Error and Chamfer Distance for reconstruction errors. The results in Table 6 of the ****Appendix**** indicate that, the sequences we used have low errors and exhibit high reconstruction quality.\n\nDue to space constraints, the evaluation of reconstruction quality and the explanation of LiDAR rendering are placed in the Appendix section. We greatly appreciate your review of this material.\n\n&ensp;\n\n**Q2**: Clarifying issues related to LiDAR rendering. What the lidar rendering formulation is?\n\n**A2**: In the LiDAR rendering section, we utilize the **official specifications** of the LiDAR used in the target domain to complete the modeling of scanning characteristics, while also considering the simulation of physical properties. The key technical details of this part are fully elaborated in previous work on LiDAR simulation [2]. A brief overview is also provided in Appendix A.2.\n\n&ensp;\n\n**Q3**: Comparation with point clouds rendered from VDBF.\n\n**A3**: Regarding your mention of comparing mesh quality with the VDBF method, it is true that we have only conducted a preliminary comparison in terms of visualization. However, considering the numerous voids and artifacts introduced by VDBF, it is evident that this would impact the quality of LiDAR rendering in our downstream section, making it challenging to achieve the results of our current reconstruction method.\n\n&ensp;\n\n**Q4**: Why points generated from mesh are meaningful for detection?\n\n**A4**: We decouple the sensor characteristics of the source domain through mesh reconstruction and employ sensors conforming to the target domain characteristics during the LiDAR rendering. This leads to generated points with smaller domain differences from the target domain, which is more conducive to learning the features of the target domain point cloud. Consequently, this can greatly enhance the performance of downstream tasks.\n\n&ensp;\n\n**Q5**: Confusion issue of Figure 2.\n\n**A5**: With regards to Figure 2, our original intention was to intuitively illustrate that, the LiDAR data obtained through our method closely resembles the real data, without any noticeable variations between domains. We acknowledge your suggestion and will remove this figure in the upcoming updated version.\n\n&ensp;\n\n**Q6**: Confusion issue of Table 1.\n\n**A6**: As for Table 1, which discusses the Waymo-like dataset, it refers to the target domain Waymo dataset obtained by rendering on the mesh reconstructed from the source domain Waymo dataset. We use this to **assess reconstruction errors**, as detailed in Appendix Table 6. If this causes any ambiguity, we will provide a more detailed explanation in the legend of the upcoming version.\n\n**References**:\n\n[1] Jianfei Guo, Nianchen Deng, Xinyang Li, Yeqi Bai, Botian Shi, Chiyu Wang, Chenjing Ding, Dongliang Wang, and Yikang Li. Streetsurf: Extending multi-view implicit surface reconstruction to street views. arXiv preprint arXiv:2306.04988, 2023.\n\n[2] Xinyu Cai, Wentao Jiang, Runsheng Xu, Wenquan Zhao, Jiaqi Ma, Si Liu, and Yikang Li. Analyzing infrastructure lidar placement with realistic lidar simulation library. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pp. 5581\u20135587, 2023. doi: 10.1109/ICRA48891.2023.10161027."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1573/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546667295,
                "cdate": 1700546667295,
                "tmdate": 1700548188601,
                "mdate": 1700548188601,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PEntIW4h7E",
                "forum": "1d2cLKeNgY",
                "replyto": "tU3ufSpZL4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1573/Reviewer_kWr7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1573/Reviewer_kWr7"
                ],
                "content": {
                    "title": {
                        "value": "Quick Feedback."
                    },
                    "comment": {
                        "value": "Q1. Thanks for clarification. Now I understand the part is based upon another paper [1].\n\nQ2. Thanks for clarification. Now I understand the part is based upon another paper [2].\n\nQ3. Thanks for clarification. Now I understand that the VDBF quality is so bad that it does not deverse a quantitative comparisons.\n\nQ4. Now I understand that although the mesh quality of ReSimAD seems low, e.g., bottom row of Fig.5, it may be enough for LiDAR simulation.\n\nQ5&6. Thanks, although I don't see the updated version. There are still 24 hours. Looking forward to these updates before the update panel closes."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1573/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700649751835,
                "cdate": 1700649751835,
                "tmdate": 1700649793557,
                "mdate": 1700649793557,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2W5oKbLgcx",
                "forum": "1d2cLKeNgY",
                "replyto": "tU3ufSpZL4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1573/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1573/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your rapid response"
                    },
                    "comment": {
                        "value": "Dear Reviewer:\n\nThanks for your precious time and valuable suggestions to improve the quality of our paper. We have also carefully revised the manuscript, with modifications marked with blue text.\n\nWe hope that our revision makes our presentation clearer and covers your concerns. We are happy to discuss any remaining questions about our work.\n\nBest regards,\nAuthors of Paper 1573"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1573/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669662617,
                "cdate": 1700669662617,
                "tmdate": 1700670202212,
                "mdate": 1700670202212,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KShkh6DjEv",
                "forum": "1d2cLKeNgY",
                "replyto": "2W5oKbLgcx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1573/Reviewer_kWr7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1573/Reviewer_kWr7"
                ],
                "content": {
                    "title": {
                        "value": "feedback"
                    },
                    "comment": {
                        "value": "I have raised the recommendation to score to ba. This system paper may have some impact on industrial people."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1573/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674113021,
                "cdate": 1700674113021,
                "tmdate": 1700674113021,
                "mdate": 1700674113021,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mgePA656mZ",
            "forum": "1d2cLKeNgY",
            "replyto": "1d2cLKeNgY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1573/Reviewer_7ZCS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1573/Reviewer_7ZCS"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes RESIMAD, which aims to base source 3D reconstruction for target-domain point clouds data generation. Specifically, using implicit neural fields with accumulated point clouds per sequence, a high-quality source-domain 3D mesh could be extracted as background of AD scenes. Foreground synthetic assets like vehicles are added within simulation guided by source GT information. Therefore, target-domain point clouds could be further extracted directly or rectified with domain-specific sensor specs. Experiments compared against UDA baseline and pre-training effectiveness demonstrate the potential usage of RESIMAD."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The overall paper is well motivated and easy to follow. The RESIMAD pipeline of is composed of three key components and clearly discussed with adequate implementation description.\n\n- The experiments on several Waymo to other datasets setup shows the effectiveness of RESIMAD."
                },
                "weaknesses": {
                    "value": "1. Effectiveness on larger detection models. One potential benefit of a \u2018reconstruction, simulation and perception\u2019 pipeline is that numerous data could be generated/rendered. As only a typical point RCNN baseline model is chosen for evaluation, it would further highlight the advantages of such methods considering more powerful and data-hungry models (e.g., ViT, DETR like detection models). \n\n2. Only \u2018zero-shot\u2019 performance is given. It would still be valuable to see whether RESIMAD could benefit from increasingly more target-domain information, from sensor specs (almost zero shot), to few-shot set-up or even with more target samples available)\n\nAbove tow points also applies to the pre-training experiment in Table3.\n\n3. Comparison against more recent DA/UDA methods. UDA is a popular topic attracting much attentions, it is expected to compare against more recent SOTA DA/UDA approaches to further support the evaluation.\n\n4. Although an almost \u2018zero-shot\u2019/unsupervised DA could be achieved, such zero-shot performance, I would say, requires the underlying similarity of driving scenarios and a large amount of geometric observations (to form background 3d meshes). I am wondering if the source domain shifts from Waymo to nuScenes or KITTI, will the proposed method still able to work well?\n\n5. Related to last point, one of my main concern is that the acquisition of 3D meshes. It may require a lot of computational costs and multi-lidar sensor specs for pre-training (per-scene optimization of LINR) to get relatively complete and accurate 3d meshs?\n\nEffectiveness and contributions of the proposed methods could be further improved in the above aspects in my opinion. I would like to hear from from authors in the rebuttal phase."
                },
                "questions": {
                    "value": "Please see the weaknesses section above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1573/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1573/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1573/Reviewer_7ZCS"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1573/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699614978412,
            "cdate": 1699614978412,
            "tmdate": 1700702337742,
            "mdate": 1700702337742,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IRdM4YkVE9",
                "forum": "1d2cLKeNgY",
                "replyto": "mgePA656mZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1573/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1573/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7ZCS (Part 1/2)"
                    },
                    "comment": {
                        "value": "**Q1**: Effectiveness on larger detection models.\n\n**A1**: Thanks for your valuable comment. Considering that we are the first work to explore the zero-shot cross-dataset ability of 3D detection model, we align with previous cross-dataset works [1, 2, 3, 4], where they often employ PV-RCNN and PV-RCNN++ as the baseline models (as reported in Table 1 of their papers). The purpose of employing the PV-RCNN model is to make a fair comparison with previous cross-dataset works [1, 2, 3, 4].\n\n**References**:\n\n[1] Jihan Yang, Shaoshuai Shi, Zhe Wang, Hongsheng Li, and Xiaojuan Qi. St3d: Self-training for unsupervised domain adaptation on 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10368\u201310378, 2021.\n\n[2] Jiakang Yuan, Bo Zhang, Xiangchao Yan, Tao Chen, Botian Shi, Yikang Li, and Yu Qiao. Bi3d: Bi-domain active learning for cross-domain 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15599\u201315608, 2023b.\n\n[3] Zhuoxiao Chen, Yadan Luo, Zheng Wang, Mahsa Baktashmotlagh, Zi Huang. Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and Class-balanced Pseudo-Labeling. In\u00a0*Proceedings of the IEEE/CVF International Conference on Computer Vision*\u00a0(pp. 3714-3726).\n\n[4] Yi Wei, Zibu Wei, Yongming Rao, Jiaxin Li, Jie Zhou, Jiwen Lu. Lidar distillation: Bridging the beam-induced domain gap for 3d object detection. In\u00a0*European Conference on Computer Vision*\u00a0(pp. 179-195).\n\n**Q2**: It would still be valuable to see whether the proposed method could benefit from increasingly more target-domain information.\n\n**A2**: We are very grateful to the reviewer for the insightful suggestion. According to the reviewer\u2019s comment, we have illustrated the experimental results using ReSimAD under the fully-supervised target-domain setting. As reported in Table 3 of the revised manuscript, ReSimAD can provide better network initialization parameters, compared to the baseline trained from scratch. Specifically, PV-RCNN model is firstly trained from scratch on the target domain (nuScenes) using fully annotated data, and its performance is 53.07\\% on the target domain. By comparison, PV-RCNN model is initialized using the checkpoint pre-trained by the proposed ReSimAD, and fine-tuned on the target domain. The target-domain performance ($AP_{BEV}$) can outperform the one trained from scratch, achieving ****54.48\\%****.\n\n**Q3**: Expected to compare against more recent SOTA DA/UDA approaches to further support the evaluation.\n\n**A3**: Thanks for the valuable recommendation. As you observe in Table 1, there is still a performance gap between our ReSimAD (81.01\\%) and the UDA method (84.10\\%) on the target domain (KITTI). This is mainly due to that, our ReSimAD aims to zero-shot cross-dataset setting, where it is assumed that ****no**** data from the target domain are **available**. But for the UDA method, they assume that the data from the target domain are available for model training. Considering this, we select the ST3D [1], the representative 3D UDA method, to show the cross-dataset detection accuracy of the network under different input conditions (zero-shot or UDA settings). Besides, we select two zero-shot methods, which are CARLA-default and Sensor-like, as the comparison baselines. The experimental results are shown in Table 2.\n\n**Reference**:\n\n[1] Jihan Yang, Shaoshuai Shi, Zhe Wang, Hongsheng Li, and Xiaojuan Qi. St3d: Self-training for unsupervised domain adaptation on 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10368\u201310378, 2021."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1573/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570053325,
                "cdate": 1700570053325,
                "tmdate": 1700570053325,
                "mdate": 1700570053325,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kn3gx98m80",
                "forum": "1d2cLKeNgY",
                "replyto": "j2WpNMZe7x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1573/Reviewer_7ZCS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1573/Reviewer_7ZCS"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors' feedback"
                    },
                    "comment": {
                        "value": "First I appreciate the detailed feedback from authors.\nAs mentioned by other reviewers, a lot of implementation and components details are not mentioned and cause confusion when understanding the overall pipeline of ReSimAD, e.g., mesh reconstruction/preparation. I hope necessary discussions and analysis could be added into a revised version to strengthen the writing quality and contributions.\n\nSecondly, after this round of discussion, I better understand the motivations and limitations of the proposed method regarding its applicable asepcts. \n\nOverall, the rebuttal addressed most of my concerns about detailed information and hope all helpful materials could be added into revision. I would like to increase my score to BA considering such both clarification and inherent limitations of the proposed method."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1573/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700702247569,
                "cdate": 1700702247569,
                "tmdate": 1700702247569,
                "mdate": 1700702247569,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]