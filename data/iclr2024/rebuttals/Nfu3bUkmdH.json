[
    {
        "title": "Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts"
    },
    {
        "review": {
            "id": "GcS5nItygB",
            "forum": "Nfu3bUkmdH",
            "replyto": "Nfu3bUkmdH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1357/Reviewer_8RdV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1357/Reviewer_8RdV"
            ],
            "content": {
                "summary": {
                    "value": "This paper suggests employing back-translation for few-shot, in-context learning of machine translation for low-resource languages. Initially, synthetic examples are generated by instructing the model to translate into English, utilizing few-shot, in-context learning with a variety of examples."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea is simple to follow and the author tested the idea across diverse set of languages. They also show improvement compared to some baselines."
                },
                "weaknesses": {
                    "value": "A major weakness in this work is that the author essentially reintroduces the concept of back-translation, a well-established technique in machine translation. Yet, this paper does not make any reference to the original, popular back-translation work by Sennrich in 2016, which raises questions about the author's familiarity with prior research and the potential reinvention of existing concepts. \n\nThe primary distinction is that it is now presented in the form of a few-shot in-context learning, rather than for training purposes. One of the proposed comparisons involves fine-tuning using synthetic data in the opposite direction, which basically is the original back-translation concept. The absence of this reference is significant because it represents the core idea of this paper. See also my 2nd question.\n\nThe synthetic back-translation data was generated by providing in-context examples across a diverse set of languages (Figure 1a). However, I believe that this crucial idea is not thoroughly explored, considering the potential variability in the types of languages, examples, diversity that could be explored. Conducting an ablation study involving different examples and languages would strengthen the paper's claims. Also, while the author comments that LDP method (Figure 1c) is superior to the standard few-shot approach (Figure 1b), there is a lack of experimental results to substantiate this claim."
                },
                "questions": {
                    "value": "- Is there any specific reason on choosing BLOOM over BLOOMZ (instruct-tuned version of BLOOM)? I think it will be more fair comparison vs InstructGPT.\n\n- One of the strengths of the original back-translation approach lies in its ability to generate synthetic data at scale, and the size of the generated data can influence performance. However, I am uncertain about the data size used for your fine-tuning comparison."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1357/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1357/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1357/Reviewer_8RdV"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1357/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697536641243,
            "cdate": 1697536641243,
            "tmdate": 1700716314821,
            "mdate": 1700716314821,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HIgEvt4RLV",
                "forum": "Nfu3bUkmdH",
                "replyto": "GcS5nItygB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1357/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1357/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response from authors"
                    },
                    "comment": {
                        "value": "We thank the reviewer for your thorough response.\n\nBefore we clarify you concerns in the below section. We want to inform that, with new experimental results presented below, our method also works for language-understanding and pure knowledge-based question answering with XQUAD and no-context TydiQA benchmarks. We use LDP with En,Vi,Zh for XQUAD and En,Id,Ko for TydiQA.\n|XQUAD/chatgpt/F1 | Arabic | Hindi | Thai\n| --- | --- | --- | --- | \n| 0-shot | 52.91 | 45.96 | 26.34\n| 3-shot | 69.96 | 69.34 | 53.80\n| 0-shot-LDP(EnViZh) | 69.80 | 68.94 | 53.98\n\n| TydiQA/chatgpt/F1 | Arabic | Bengali | Finnish | Russian\n| --- | --- | --- | --- |  --- | \n| 0shot             | 19.17 | 5.72 | 21.79 | 12.31\n| 3-shot            | 27.78 | 20.21 | 34.72 | 16.82\n| 0-shot-LDP(EnIdKo) | 23.21 | 18.90 | 32.60 | 16.97\n\nLet us clarify your concerns:\n1. **Our paper does not, in absolutely anyway, claim to invent back-translation!**  We cited numerous back-translation papers (Edunov et al., 2018; Lample et al., 2018; Conneau & Lample, 2019; Liu et al., 2020; Nguyen et al., 2022b). (Sennrich et al., 2016) was the first, and has been improved and refined in various works we cited above. Each of them is better than (Sennrich et al.) or complement it in other settings. **We thank the reviewer for bringing up this well-known work, and we will cite (Sennrich et al., 2016) in the paper!** But we would like to emphasize that we initially skipped its citation in the same way many papers nowadays have also stop citing (Vaswani, et al, 2017) Transformer paper due to its utmost popularity, but instead cite other improved versions, and not because we are not aware of it. We sincerely plead that missing an amendable citation is not punishable with rejection.\n2. Our work tackles the questions of **(1) How to do back-translation with LLMs (2) How to do back-translation without any training because some LLMs are closed-source (3) How to do it with zero or just 3 unlabelled data samples from the target languages. (4) How to do it when the pretraining data for a target language is possibly less than 1MB.**\n3. Training on synthetic back-translation data is a well-established idea and this is not our novelty claim. But how to create those data depends on settings (unsupervised or supervised), constraints (data size), tasks (summarization or translation), or target languages (high or low-resource, Latin or non-Latin), and have been explored in the many works we cited. Our work explore BT in the conditions explained above.\n4. **Ablation study**: **Table 4a** examines the different types of prompts, language tags and with/without synthetic backtranslation. **Table 4b** explored different language choices, from the number of languages to use, which groups of languages (distant or close), and repeating the same languages. We also conduct other ablation studies in **figure 3, figure 4, table 5, figure 5, and figure 6**.\n5. **Figure 1b vs 1c:** **Table 4a** compares Fig. 1b (without BT) vs Fig. 1c (with BT) and shows that Fig. 1c outperforms Fig. 1b greatly for En-->X direction, but observes no difference for X-->En direction. This is consistent with the explanations in figures 1a, 1b, and 1c. Please note that Fig. 1b is **not** standard supervised few-shot prompting, but our LDP method **without** synthetic intra-lingual backtranslation data. \n6. **Why BLOOM?** Because LDP is a fully unsupervised prompt technique, as we explain in page 6, we do not want the examined models to be contaminated with supervised data and especially test sets. BLOOMz was trained on test sets! InstructGPT experiments are to show that our method works on instruction-tuned models. We also tested on GPT-3 base and observe similar results, but its performance was much worse than BLOOM in absolute terms for these low-resource languages across different settings. We explain GPTs' weaknesses in figure 6.\n7. **Finetuning data size:** sizes are very small! **<500-1000** samples for extremely low-resource languages like Tsonga, and **~50K** samples for more popular ones like Hindi. We understand BT needs millions of samples in data size, but it was difficult to obtain such data for low-resource languages. We will put these details in paper.\n\nGiven our explanations and additional experimental results, we sincerely hope the reviewer will reconsider your evaluation. If you have other questions or requests for additional experiments, we will try our best to accommodate and perform such experiments."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699697410483,
                "cdate": 1699697410483,
                "tmdate": 1699697410483,
                "mdate": 1699697410483,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CCPCgMsmVx",
                "forum": "Nfu3bUkmdH",
                "replyto": "HIgEvt4RLV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1357/Reviewer_8RdV"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1357/Reviewer_8RdV"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarification, I bumped my score.\n\nThose BT papers were mainly discussed briefly in the related work, rather than being the core of the idea design and introduction. One might argue not citing Vaswani if they use some kind of more modern model as a black box (e.g., BERT), but in your case, your main idea itself is based on BT. Therefore, I would hope the authors re-clarify this in the updated manuscript."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716267464,
                "cdate": 1700716267464,
                "tmdate": 1700716267464,
                "mdate": 1700716267464,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OUTb9Fptgo",
            "forum": "Nfu3bUkmdH",
            "replyto": "Nfu3bUkmdH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1357/Reviewer_y5rc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1357/Reviewer_y5rc"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces \u201dlinguistically diverse prompting\u201d (LDP), a method aimed at improving prompt-based generative task performance in languages for which there are no available few-shot exemplars. In this method, few-shot in-context learning is enabled through leveraging in-context exemplars from various (higher-resource) languages to \u201dlocate the task\u201d. The authors find that their approach achieves at least comparable performance w.r.t. supervised methods for translation and multilingual summarization."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The presented method is well-motivated by the observations from the literature and addresses a concrete problem in low-resource NLP (i.e. lack of in-context exemplars for some languages).\n\n2. Evaluation is rigorous and the analyses (section 4.4) provide valuable insights for this line of research."
                },
                "weaknesses": {
                    "value": "Summary of Weaknesses\n\n1) Flawed linguistic diversity: this paper claims linguistic diversity mainly on the basis of selecting languages with different scripts: \u201dto ensure diversity \u2026 characters are used\u201d (page 1). However, this is not done systematically (the authors \u201dinclude various script types\u201d and later mention \u201ddissimilar lexical and regional characteristics\u201d but do not explain the exact selection process). Moreover, this misses important aspects of linguistic diversity that are captured by for instance taking into account phylogeny.\n\n2) This paper has reproducibility issues. The results in the paper cannot be replicated, as the approaches are evaluated on 200 randomly sampled sentences from the test set, while there is no explanation or source provided that details which sentences are included or how to reproduce this selection (e.g. which random seed). Random data selection is also used in one of the baselines, namely supervised prompting (A.2) without providing details."
                },
                "questions": {
                    "value": "1. Is LDP truly an unsupervised prompting method, or are some aspects more like obtaining data for a kind of weak supervision?\n\n2. What are your criteria for distinguishing high-resource from low-resource languages?\n\n3. In what way does improving prompting performance for certain low-resource scenarios \u2019democratize\u2019 LLMs (title)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1357/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698756301728,
            "cdate": 1698756301728,
            "tmdate": 1699636063021,
            "mdate": 1699636063021,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ueyHnzGDbm",
                "forum": "Nfu3bUkmdH",
                "replyto": "OUTb9Fptgo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1357/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1357/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response from authors"
                    },
                    "comment": {
                        "value": "We thank the reviewer for your thorough response.\n\nBefore we clarify you concerns in the below section. We want to inform that, with new experimental results presented below, our method also works for language-understanding and pure knowledge-based question answering with XQUAD and no-context TydiQA benchmarks. We use LDP with En,Vi,Zh for XQUAD and En,Id,Ko for TydiQA.\n|XQUAD/chatgpt/F1 | Arabic | Hindi | Thai\n| --- | --- | --- | --- | \n| 0-shot | 52.91 | 45.96 | 26.34\n| 3-shot | 69.96 | 69.34 | 53.80\n| 0-shot-LDP(EnViZh) | 69.80 | 68.94 | 53.98\n\n| TydiQA/chatgpt/F1 | Arabic | Bengali | Finnish | Russian\n| --- | --- | --- | --- |  --- | \n| 0shot             | 19.17 | 5.72 | 21.79 | 12.31\n| 3-shot            | 27.78 | 20.21 | 34.72 | 16.82\n| 0-shot-LDP(EnIdKo) | 23.21 | 18.90 | 32.60 | 16.97\n\nLet us clarify your questions:\n1. **Results are reproducible!** We stated that in the paper **\"For each of the 68 language pairs, we sample randomly and evaluate 200 sentences from each test set with the same seed to limit the cost of API calls\"** . The seed is **0**! We also used seed **0** to collect the prompts for supervised prompting (A.2). In short, **every test set in the paper is the same across all experiments and baselines**. \n2. **Linguistic diversity**: As explained in the paper, the name \"linguistically diverse\" simply means we use prompts from different languages and distant from the target language, and not going any deeper or further into sophisticated aspects of linguistics, such as phylogeny. \n3. *Selecting the optimal languages* is a combinatorial search problem with search space expands in factorial order! The paper's main point is to use prompts from diverse languages for many downstream tasks, while leaving the optimal search problem for future work. Thus, we only conduct experiments with languages choices based on common knowledge, such as \"English is different from Chinese\". We will amend the paper to reflect this point.\n4. **Table 4b explains the guidelines for language selection process empirically**. Our method work best if all languages in prompts are *different from each other* and **should be distant from the target language** we want to solve tasks in. \n5. Other than that, **as we have tried different combinations of language choices**, there is no significant difference in terms of performances. The choice of Ar,Zh,Vi,Fr is totally a random decision by the authors at the beginning. As we tried other language sets and experiments in Table 4b, we believe will get roughly the same results as long as the guidelines above are followed.\n\nFor questions:\n1. LDP itself is **truly a fully and completely unsupervised** method, both from prompting or data collection perspectives. The only thing that makes the entire process not *fully unsupervised*  is which underlying model is used. InstructGPT is not an unsupervised model and we do not know which data it was trained on. Still, we demonstrate that our method can improve InstructGPT zero-shot performances for the target language where we do not have any supervised data at hand! \n2. Which language is high or low resource is subjective. In our paper, we assume language with < 0.1% pretraining data or <1GB data size from the CC100 corpora as very low-resource.\n3. **Democratizing**: our method is a simple strategy that can boost LLM's performance for low-resource languages that we would not have been able to achieve with naive zero-shot prompting, and without supervised data from those low-resource languages. For example, practitioners can build a simple summarization app for Swahili-speaking community, without ever needing to collect any supervised Swahili data, which would be hard to collect.\n\nIn this way, under-represented communities can benefits from existing LLMs without needing to invest in human annotations, training computations."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699696580812,
                "cdate": 1699696580812,
                "tmdate": 1699771760844,
                "mdate": 1699771760844,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jv85dyzNtv",
            "forum": "Nfu3bUkmdH",
            "replyto": "Nfu3bUkmdH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1357/Reviewer_qSbo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1357/Reviewer_qSbo"
            ],
            "content": {
                "summary": {
                    "value": "In this paper the authors tried to improve the LLMs performance on low-resource languages by creating synthetically diverse prompts in high resource languages. The authors show the effectiveness of their approach in translation and summarisation tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "It is an interesting approach to get good performance on Low-Resource set up. The experiments are promising."
                },
                "weaknesses": {
                    "value": "The results seems promising. The authors should provide more details about:\n\n(a) How the diverse language sets are selected? Do they observe any correlation on linguistically similar language selection vs a random set of languages?\n\n(b) Did they study the relation of number of languages to be selected and number of examples in the prompt?\n\n(c) Were the prompt set fixed for every test instance? \n\nAlso, it would be interesting to see the performance difference of selecting prompts from diverse languages vs creating synthetic prompts for just the pair of languages of interest."
                },
                "questions": {
                    "value": "The paper would be sound if the authors can explain / provide experimental evidences on prompt selection as pointed out in the previous section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1357/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1357/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1357/Reviewer_qSbo"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1357/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698824071451,
            "cdate": 1698824071451,
            "tmdate": 1700716579415,
            "mdate": 1700716579415,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ti8rgclr6k",
                "forum": "Nfu3bUkmdH",
                "replyto": "jv85dyzNtv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1357/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1357/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response from authors"
                    },
                    "comment": {
                        "value": "We thank the reviewer for your thorough response.\n\nBefore we clarify you concerns in the below section. We want to inform that, with new experimental results presented below, our method also works for language-understanding and pure knowledge-based question answering with XQUAD and no-context TydiQA benchmarks. We use LDP with En,Vi,Zh for XQUAD and En,Id,Ko for TydiQA.\n|XQUAD/chatgpt/F1 | Arabic | Hindi | Thai\n| --- | --- | --- | --- | \n| 0-shot | 52.91 | 45.96 | 26.34\n| 3-shot | 69.96 | 69.34 | 53.80\n| 0-shot-LDP(EnViZh) | 69.80 | 68.94 | 53.98\n\n| TydiQA/chatgpt/F1 | Arabic | Bengali | Finnish | Russian\n| --- | --- | --- | --- |  --- | \n| 0shot             | 19.17 | 5.72 | 21.79 | 12.31\n| 3-shot            | 27.78 | 20.21 | 34.72 | 16.82\n| 0-shot-LDP(EnIdKo) | 23.21 | 18.90 | 32.60 | 16.97\n\nLet us clarify your questions:\n1. As demonstrated in **table 4b**, our method work best if all languages in prompts are different from each other and **must be very distant from the target language** you want to solve. Other than that, **as we have tried different combinations of language choices**, there is no significant difference in terms of performances.\n2. Number of languages or number of examples are limit by the context length, which is around 6-8. **table 4b** also shows that there is no difference if we put 3 (Fr,Es,Pt) or 7 (Ar,Fr,Es,Pt,Vi,Zh,Id) languages in the prompt. Meanwhile, repeating the same languages (Zh,Zh,Zh,Zh) may hurt performance.\n3. Yes, we use the **same set of prompts for every test instance, across all languages**. For selecting back-translation data, we also use the same **seed 0** to randomly select data from the train/valid set. For test set, we also use seed 0 to select 200 samples to ensure **all 200 samples being tested are exactly same!**\n4. *Diverse vs Synthetic intra-lingual Prompts*: **table 4a** explains the requested results, indicated as LDP *without* vs *with* intralingual back-translation prompts. In short, there is no difference between them for X-->En direction, but ones with synthetic back-translation significantly outperforms the other and match supervised prompting for En-->X direction.\n5. Prompt selection: We randomly pick unlabeled sentences from CC25 dataset for the diverse languages, and used the same ones through the entire paper. We have to tried different random selections and found results are on average the same. As indicated in the paper that the setting is about low-resource languages, so we assume no optimal prompt selection technique is used and **random selection** is the only default choice."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699693806411,
                "cdate": 1699693806411,
                "tmdate": 1699771529392,
                "mdate": 1699771529392,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zdD38wNCwa",
                "forum": "Nfu3bUkmdH",
                "replyto": "ti8rgclr6k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1357/Reviewer_qSbo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1357/Reviewer_qSbo"
                ],
                "content": {
                    "title": {
                        "value": "Reply to authors"
                    },
                    "comment": {
                        "value": "Thanks for the clarification. I have updated the score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1357/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716821168,
                "cdate": 1700716821168,
                "tmdate": 1700716821168,
                "mdate": 1700716821168,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]