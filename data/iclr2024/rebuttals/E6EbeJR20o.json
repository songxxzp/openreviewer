[
    {
        "title": "A Large-Scale 3D Face Mesh Video Dataset via Neural Re-parameterized Optimization"
    },
    {
        "review": {
            "id": "FoSF8FZzcw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission12/Reviewer_B8S8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission12/Reviewer_B8S8"
            ],
            "forum": "E6EbeJR20o",
            "replyto": "E6EbeJR20o",
            "content": {
                "summary": {
                    "value": "The paper proposes a novel method, i.e. the neural re-parameterized optimization, for optimizing FLAME parameters from a video sequence. Then, the paper applies the proposed technique to several large-scale in-the-wild video datasets. The fitting results form a novel dataset called NeuFace-dataset, which is another contribution of the paper. The paper also demonstrates the usage of the proposed dataset on face reconstruction and motion-prior learning."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* I do like the proposed neural re-parameterized technique for optimizing FLAME parameters, it provides insights for me. Sparse gradients are not desired for geometry optimization. By optimizing the neural network weights to indirectly optimize the FLAME geometry, dense gradients can be obtained from sparse landmark loss as shown in Fig.3.\nHowever, in the context of optimizing per-vertex displacement (this task is more challenging than optimizing 3DMM parameters), similar techniques [1][2] are proposed to obtain dense gradients. I suggest the authors discuss these more relevant works in the main paper.\n* The proposed dataset would benefit future research in this field.\n\n[1] Neural head avatars from monocular rgb videos, CVPR 2022.\n\n[2] Large steps in inverse rendering of geometry, SIGGRAPH Asia 2021."
                },
                "weaknesses": {
                    "value": "My main concerns are listed as follows.\n\n1. I agree that the paper proposes the first method to introduce the neural re-parameterized technique to optimize FLAME parameters. As I know, previous work [1] has already introduced this technique to solve very relevant (actually more challenging) tasks; they adopt a neural network to re-parameterize the per-vertex displacement of a FLAME mesh. However, the submission does not discuss these very relevant works.\n\n2. I think the video FLAME fitting algorithm is not well-designed. One of the contributions of the paper is to use the proposed fitting algorithm to provide pseudo GT for the video dataset, so I think is necessary to push the quality of the fitting method as high as possible. I list some questions about the design choice here:\n* Why not use a photometric loss term? Many previous works have demonstrated that photometric loss can improve the geometry reconstruction quality, see Figure 6 in [2]. I'm curious when photometric loss is used, will the proposed neural re-parameterized technique still improve the results a lot?\n* Why not use a shared identity code?\n* From the supp. video, I find that the fitting results are still jittering, although better than the naive baseline of applying DECA to each frame separately. More advanced strategies beyond the temporal moving average method should be exploited, like the common-used optical flow loss, or stabilize the detected landmarks [3].\n* Thus, it gives me the impression that the paper does not do its best to improve the video-fitting results. I would like to hear from the authors to change my impression.\n\n3. Apply DECA to each frame is too weak to serve as a baseline for video fitting. There are many previous works [4-6] proposed to fit 3DMM into a video. Why not compare with these more strong baselines specialized in video fitting? In the supp. material, the method is compared to MICA-T, but I think more competitors are expected. At least they should be discussed in the paper. So I think the paper does not well evaluate the proposed technique for video fitting.\n\n[1] Neural head avatars from monocular rgb videos, CVPR 2022.\n\n[2] State of the Art on Monocular 3D Face Reconstruction, Tracking, and Applications\n\n[3] High-Resolution Neural Face Swapping for Visual Effects\n\n[4] 3D Shape Regression for Real-time Facial Animation\n\n[5] Real-time high-fidelity facial performance capture\n\n[6] Face2Face: Real-time face capture and reenactment of rgb videos"
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission12/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission12/Reviewer_B8S8",
                        "ICLR.cc/2024/Conference/Submission12/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission12/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697182202148,
            "cdate": 1697182202148,
            "tmdate": 1700707130444,
            "mdate": 1700707130444,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qJBiTToPqc",
                "forum": "E6EbeJR20o",
                "replyto": "FoSF8FZzcw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer B8S8 (Part 1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the time and constructive comments that strengthen our paper. We discuss the concerns and questions of the reviewer below and in the revision (please check pdf, highlighted in pink).\n\nPlease let us know if our answers satisfy the reviewer\u2019s concerns. We would be happy to provide further discussions and clarifications.\n\n### **Weaknesses**\n\n> **W1. A previous work [C1] has already introduced this technique to solve very relevant (actually more challenging) tasks; they adopt a neural network to re-parameterize the per-vertex displacement of a FLAME mesh. However, the submission does not discuss these very relevant works.** \n\nWe appreciate the reviewer for recommending the reference. We have revised the paper to include the discussion about the suggested reference in the 4th paragraph of Sec. 2 of the revision, highlighted in pink.\n\nHowever, please note that our work and [C1] are focus on very different perspectives as:\n\n1. We first remark that, in contrast to ours, Neural Head Avatars [C1] does not re-parameterize FLAME parameters with neural networks, but directly optimizes FLAME parameters which corresponds to the baseline we presented.\n2. In contrast, we provided the empirical analysis about the gradient densification (Sec. 3.3, p. 5 in the initial submission and Sec. A.1, p. 17 in the Appendix), which can be attributed to the proposed neural re-parameterization, and hasn\u2019t been explored in the fields. While [C2] also tries to make the dense gradient for geometry optimization, it uses a different technique, pre-conditioning, which is different from our neural re-parameterization. Also, we briefly showed the robustness and high-probability convergence guarantee of the optimization to global optima in the theoretical proof sketch of the initial submission (Sec. A.2, p. 18 in the Appendix).\n3. They parameterize residual fine-detail geometries of face using an MLP, which is not our scope for reconstructing details of face shapes. Our scope is to reconstruct facial geometries, well complying with input facial gestures and motions as discussed in the response to W1 of Reviewer axFo.\n\nGiven these, [C1,C2] do not harm our contribution. Thanks for suggesting references. \n\n[C1] Grassal et al., Neural head avatars from monocular rgb videos. In CVPR 2022.\n\n[C2] Nicolet et al., Large Steps in Inverse Rendering of Geometry. In SIGGRAPH ASIA 2021.\n\n---\n\n> **W2. The video FLAME fitting algorithm is not well-designed.** \n\n- **W2-1.** **Why not use a photometric loss term?** \n    \nAs requested, we report the effect of the photometric loss ($\\mathcal{L}\\_{photo}$) in the metrics for the MEAD dataset we used in the main paper. We compare the optimization that uses three different loss configurations as below:\n\n| Loss configuration  | $\\text{MSI}\\_{3D}\\^L (\\uparrow)$ | $\\text{MSI}\\_{3D}\\^V (\\uparrow)$ | $\\text{CVD} (\\downarrow)$ | $\\text{NME} (\\downarrow)$ |\n| --- | --- | --- | --- | --- |\n| **NeuFace (Original)** | **0.206** | **0.305** | **0.103** | **2.58** |\n| NeuFace + $2\\cdot\\mathcal{L}\\_{photo} $ | 0.205 | 0.299 | 0.106 | 2.62 |\n| NeuFace + $5\\cdot\\mathcal{L}\\_{photo}$ | 0.195 | 0.282 | 0.112 | 3.69 |\n\nWe tried this in our preliminary experiment, and even with the tuning of balancing, we obtain negligible effects of using  $\\mathcal{L}\\_{photo}.$ Using $\\mathcal{L}\\_{photo}$ rather degrades the performance, and the larger $\\mathcal{L}\\_{photo}$ results in more degradation. \n\nWe postulate that, since $\\mathcal{L}\\_{photo}$ is designed to enforce the consistency of colors, i.e., albedo assumption, thus it has been known to be very prone to self-shadows, and non-Lambertian reflection by lighting, and noise existing in videos, which distract the optimization. Furthermore, it takes more than twice the GPU memory than the original configuration, due to the requirement of the differentiable rendering. Thus, we claim our design choice is sufficient to reconstruct dynamic 3D faces despite its compact combination. \n\nWe have newly added the results and discussion in the revision (Sec. A.4, p. 21 in the Appendix)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission12/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536949410,
                "cdate": 1700536949410,
                "tmdate": 1700551292951,
                "mdate": 1700551292951,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pk3141sagy",
                "forum": "E6EbeJR20o",
                "replyto": "FoSF8FZzcw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer B8S8 (Part 2/2)"
                    },
                    "comment": {
                        "value": "- **W2-2. Why not use a shared identity code?**\n    \nWe tried this in our preliminary experiment, but found negligible differences in our early test. \nIn our newly conducted investigation, we found that the standard deviation of the identity codes ($\\boldsymbol{\\beta}$) of all frames, gradually decreases and converges during NeuFace optimization.\n    \n| Optim. step | 0 | 25 | 50 | 75 | 100 (End) |\n| --- | --- | --- | --- | --- | --- |\n| $\\boldsymbol{\\beta}$ std. | 3.152 | 1.489 | 1.372 | 1.370 | 1.366 |\n    \nThis means, the identity codes across video frames and views automatically converge, although we do not manually force the shared identity codes with the losses. Accordingly, in the visual results, we obtain identical results in both 1) original NeuFace optimization and 2) NeuFace optimization with consistent $\\boldsymbol{\\beta}$. \n    \nWe have revised the paper and added this discussion and results in the revision (Sec. A.4, p. 21 in the Appendix)\n    \n\n- **W2-3.** **More advanced strategies beyond the temporal moving average method should be exploited, like the common-used optical flow loss, or stabilize the detected landmarks [C3].**\n    \nWe thank the reviewer for suggesting diverse methods for improving each component in our optimization. \nAs discussed in the response to [W2-1], our design choice is sufficient to reconstruct dynamic 3D faces despite its compact combination. It is also sufficient to validate the idea of neural re-parameterization with bootstrapping. Thus, adding more losses does not change our message and finding of our work we conclude (This is the goal of a piece of the academic paper). \n    \nOn the other hand, we agree with the reviewer\u2019s point that using advanced losses would increase the accuracy. Note that we can readily add more advanced losses for our proposed optimization. Given the limited rebuttal period, we\u2019ll investigate other advanced combinations of methods later, and add the relevant discussion in the camera-ready version.\n    \n\n[C3] Naruniec et al., High-Resolution Neural Face Swapping for Visual Effects. In Eurographics Symposium on Rendering 2020.\n\n---\n\n> **W3. Why not compare with these more strong baselines specialized in video fitting? In the supp. material, the method is compared to MICA-T, but I think more competitors are expected.** \n\nWe compared with MICA+Tracker as the representative competitor because of the following reasons:\n\n1. MICA+Tracker [C4] is the most competitive video fitting work, which already demonstrate the surpassing performance over Face2Face [C5], and other previous baselines [C6,C7]. Ours better performs than MICA+Tracker, which implies ours performs also better than [C5,C6,C7]. \n2. For [C8,C9], their codes are not publicly available. During the discussion period, we reached out to the authors of [C8,C9] and requested to share the code, but we couldn\u2019t get a reply. We cannot even quote their quantitative results, because they did not present the benchmarks shared with both MICA and ours.\n\nTheir quantitative results cannot be directly quoted, because the benchmarks they presented are not compatible with ours. Despite our efforts, we could not manage to add them. Instead, we have newly added the reference in the revision (Sec. C.2, p. 24 in the Appendix).\n\n[C4] Zielonka et al., MICA - Towards Metrical Reconstruction of Human Faces. In ECCV 2022.\n\n[C5] Thies et al., Face2Face: Real-time face capture and reenactment of rgb videos. In CVPR 2016.\n\n[C6] Deng et al., Accurate 3d face reconstruction with weakly-supervised learning: From single image to image set. In CVPRW 2019.\n\n[C7] Feng et al., Learning an animatable detailed 3D face model from in-the-wild images. In SIGGRAPH 2021.\n\n[C8] Cao et al., 3D Shape Regression for Real-time Facial Animation. In SIGGRAPH 2013.\n\n[C9] Cao et al., Real-time high-fidelity facial performance capture. In SIGGRAPH 2015."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission12/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700537866416,
                "cdate": 1700537866416,
                "tmdate": 1700551320788,
                "mdate": 1700551320788,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "irt6qhNSWL",
                "forum": "E6EbeJR20o",
                "replyto": "pk3141sagy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission12/Reviewer_B8S8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission12/Reviewer_B8S8"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for providing more experiments, it addressed most of my concerns about the experiment setup. However, my main concern is still the main techniqical contribution, i.e. the re-parameterized optimization.\n\n1. Is the proposed re-parameterized technique still useful if the FLAME fitting energy involves a dense term, i.e. the photometric loss? If the proposed technique only benefits the \"landmark loss + regularization\" fitting setup, I think it is not a significant step as most modern 3DMM fitting algorithm involves a photometric loss. I suggest the author include more experiments in the revised paper to demonstrate their technique can benefit the 3DMM fitting problem, not only the \"landmark loss + regularization\" setup.\n\n1. I am still not sure about the significance of the proposed re-parameterized optimization as Neural Head Avatar has already applied it to a more challenging problem to optimize per-vertex displacement. However, I also noticed that in the Neural Head Avatar paper, they do this thing implicitly; they even did not mention the re-parameterized technique can densify the gradient to benefit optimization. I need to discuss with other reviewers to re-evaluate the significance of the proposed technique."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission12/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635632410,
                "cdate": 1700635632410,
                "tmdate": 1700635632410,
                "mdate": 1700635632410,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1vc6swF8C9",
                "forum": "E6EbeJR20o",
                "replyto": "BOnwwlf6Fl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission12/Reviewer_B8S8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission12/Reviewer_B8S8"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the rebuttal. I think my concerns are addressed. I would change my rating to weak accept now."
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission12/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707113938,
                "cdate": 1700707113938,
                "tmdate": 1700707113938,
                "mdate": 1700707113938,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "USsm9RVAsM",
            "forum": "E6EbeJR20o",
            "replyto": "E6EbeJR20o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission12/Reviewer_C76P"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission12/Reviewer_C76P"
            ],
            "content": {
                "summary": {
                    "value": "This article introduces a new video dataset with 3D face mesh pseudo-labels and provides a method for annotating spatio-temporally consistent 3D face meshes for existing multi-view facial video data. Based on the results provided by the authors, this dataset is valuable for related research."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The dataset introduced by the authors exhibits clear advantages in terms of data quantity, annotation accuracy, and spatio-temporal consistency, as evidenced by the provided data examples. These strengths are valuable for advancing research in the relevant field. Additionally, the optimization method proposed for achieving spatio-temporal consistency seems effective."
                },
                "weaknesses": {
                    "value": "1. Unfair Comparison: The comparison with methods like DECA and EMOCA, which operate on single-view data (DECA-dataset and EMOCA-dataset), cannot utilize multi-view information. It can be argued that the proposed method leverages more information by utilizing multi-view data. Therefore, comparing the proposed multi-view approach to these single-view reconstruction methods may not provide a fair evaluation.\n\n2. The novelty is limited. The proposed temporal-consistency-loss and multi-view-consistency-loss seem more like separate regularizations (or averages) applied to pose, camera parameters, or face shape and expression coefficients to achieve reduced jitter in the reconstructed videos.\n\nI have doubts about the effectiveness of the multi-view-consistency-loss.  In the training set, only the MEAD dataset consists of multi-view video data, while VoxCeleb2 and CelebV-HQ have only single-view video data. Consequently, it appears that only the MEAD dataset can effectively leverage the multi-view consistency loss.  Table 1 illustrates that MEAD comprises a mere 1% of the total duration, suggesting that the majority of the proposed NeuFace-dataset primarily derives from VoxCeleb2 and CelebV-HQ.  In essence, it seems to be a data processing outcome achieved by applying inter-frame smoothing to existing methods. Although I appreciate the authors' effort and the contribution of NeuFace-dataset to the community, the paper's level of innovation may fall slightly below the standard typically expected at ICLR."
                },
                "questions": {
                    "value": "1. As mentioned in the paper, the proposed dataset contains a large amount of data, and the preliminary 3D mesh results generated based on DECA (EMOCA) may have errors. Have the authors considered how to filter out failed reconstruction results?\n2. The quality of the reconstructed results for extreme facial expressions appears suboptimal. For instance, in Figure 5's top-left corner, where the open-mouth expression is depicted, the reconstruction of the mouth region does not seem consistent with the original input. Additionally, there appear to be imperfections in the reconstruction of closed-eye expressions.\n3. Given the analysis above, while the dataset's scale is certainly commendable, there seems to be room for improvement in terms of reconstruction accuracy. It might be worthwhile for the authors to consider utilizing such data as annotations for 3D landmarks rather than 3D mesh data. Additionally, have the authors explored the possibility of applying their proposed method to a different face model, such as the Basel Face Model (BFM), or investigating alternative pre-trained models instead of DECA or EMOCA?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission12/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698638237250,
            "cdate": 1698638237250,
            "tmdate": 1699635924802,
            "mdate": 1699635924802,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LFgtoK42pa",
                "forum": "E6EbeJR20o",
                "replyto": "USsm9RVAsM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer C76P (Part 1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the time and the thorough review, which helped us improve our paper. We address the concerns and the questions below and in the revision (please check pdf, highlighted in pink).\n\nPlease let us know if our answers satisfy the reviewer\u2019s concerns. We would be happy to provide further discussions and clarifications.\n\n\n### **Weaknesses**\n---\n> **W1. Unfair Comparison with DECA/EMOCA, which operates on single-view data.**\n\nWe confirm that the experiments were fair.\n\nOur NeuFace optimization is a **test-time optimization method.** The test-time optimization is a method that updates the model to fit into the deployment environment at test time, without access to training data [C1]. Test-time optimization methods utilize the losses with self-supervision or pseudo supervision, available at test-time. Prior arts [C1,C2,C3,C4] also compared their proposed test-time optimization methods and test-time losses with the baseline methods that do not involve test-time optimization. It is regarded as fair and valid comparison.\n\nWhen evaluating the test-time optimization methods, the configuration of the pre-trained backbone network is important. Our NeuFace optimization starts from the pre-trained DECA/EMOCA checkpoint same as the compared baselines, which makes our settings and comparison fair.\n\nAlso, we would have tried if there was a fair setting that the reviewer specifically thinks. Please leave the comment if there is a specific fair setting to compare with DECA/EMOCA. We\u2019d like to further discuss and willing to include it. \n\n[C1] Yi et al., Temporal Coherent Test-Time Optimization for Robust Video Classification. In ICLR 2023.\n\n[C2] Zeng et al., Test-Time Optimization for Video Depth Estimation Using Pseudo Reference Depth. In Computer Graphics Forum 2023.\n\n[C3] Luo et al., Consistent Video Depth Estimation. In SIGGRAPH 2020.\n\n[C4] Schneider et al., Improving robustness against common corruptions by covariate shift adaptation. In NeurIPS 2020.\n\n-----\n> **W2. Novelty; The proposed temporal-consistency-loss and multi-view-consistency-loss seem more like separate regularizations (or averages) to achieve reduced jitter in the reconstructed videos.** \n\nWe haven\u2019t claimed the novelty of our losses. While our loss configurations are simple, we obtain a sufficiently good quality dataset, thanks to our neural re-parameterized optimization and its favorable properties. We can readily add more advanced losses for our proposed optimization.\n\nAlthough our loss configuration is sufficient, we share the experimental result that reviewer B8S8 asked: NeuFace optimization + photometric loss ($\\mathcal{L}_{photo}$).\n\nBelow we report the metrics for the MEAD dataset we used in the main paper. We compare the optimization that uses three different loss configurations as follows: \n\n| Loss configuration  | $\\text{MSI}_{3D}^L (\\uparrow)$ | $\\text{MSI}_{3D}^V (\\uparrow)$ | $\\text{CVD} (\\downarrow)$ | $\\text{NME}(\\downarrow)$ |\n| --- | --- | --- | --- | --- |\n| **NeuFace (Original)** | **0.206** | **0.305** | **0.103** | **2.58** |\n| NeuFace + $2*\\mathcal{L}_{photo} $ | 0.205 | 0.299 | 0.106 | 2.62 |\n| NeuFace + $5*\\mathcal{L}_{photo}$ | 0.195 | 0.282 | 0.112 | 3.69 |\n\nWe obtain negligible effects of using $\\mathcal{L}_{photo}$. The results show our minimal loss combination is sufficient and essential to reconstruct dynamic 3D faces. We have added the results and discussion in the revision (Sec. A.4, p. 21 in the Appendix)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission12/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700535473798,
                "cdate": 1700535473798,
                "tmdate": 1700551156216,
                "mdate": 1700551156216,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IaAnEFw1BA",
                "forum": "E6EbeJR20o",
                "replyto": "USsm9RVAsM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer C76P (Part 2/3)"
                    },
                    "comment": {
                        "value": "> **W3. Effectiveness of the multi-view consistency loss. In the training set, only the MEAD dataset consists of multi-view video data, while VoxCeleb2 and CelebV-HQ have only single-view video data. Consequently, it appears that only the MEAD dataset can effectively leverage the multi-view consistency loss.**\n\nWe\u2019d like to remind that our work focuses on the effectiveness of our proposal, the neural re-parameterization with bootstrapping (temporal and multi-view consistency).  \n\nThus, we show that\n\n1. Even with the temporal bootstrapping alone in the majority of data, we demonstrate the effectiveness of our NeuFace dataset by significantly enhancing the 3D reconstruction quality in the public benchmark (Table 3a, Sec.5.1. in the initial submission). \n2. Also, Table 3a, Sec.5.1. in the initial submission shows that the model trained with our NeuFace dataset significantly improves 3D reconstruction quality for extreme side view cases, where the temporal consistency alone is weak while the multi-view consistency effectively works. This suggests that even with the small portion of data involving multi-view consistency as the reviewer mentioned, the multi-view consistency helps.\n\nRegardless of which bootstrapping (temporal or multi-view consistency) is, it does not alter our conclusion that the neural re-parameterization with bootstrapping is effective. Also, while the multi-view consistency is important, for scalability of the dataset, it is a practical consideration to leverage large-scale monocular video data.\n\nWe will consider processing and enlarging the portion of multi-view datasets, e.g., HUMBI [C5], to provide FLAME parameters when releasing the dataset.\n\n[C5] Yu et al., HUMBI: A Large Multiview Dataset of Human Body Expressions. In CVPR 2020.\n\n \n\n### **Questions**\n\n---\n> **Q1. The proposed dataset contains a large amount of data, and the preliminary 3D mesh results generated based on DECA (EMOCA) may have errors. Have the authors considered how to filter out failed reconstruction results?** \n\nOur NeuFace optimization is robust to the initial 3D face estimation so that small number of failure does not affect the performance. In the newly conducted experiment, we have compared the multi-view consistency of the final reconstruction on two scenarios: 1) Standard cases (use original images) and 2) Extreme cases (use perturbated images). For extreme cases, we randomly apply perturbation with large black boxes to the facial areas for 2~3 views in the multi-view videos to mimic significant corruption scenarios. \n\n| Loss configuration |  Standard cases (CVD)\u2193 | Extreme cases (CVD)\u2193 |\n| --- | --- | --- |\n| Weighted average (ours) | 0.103 | 0.113 |\n\nThe results show our optimization is robust to initial failure that occurs in the extreme cases.\n\nFurthermore, to filter out the potentially remaining erroneous samples or outliers, we conducted 1) automatic filtering and 2) manual human verification when we constructed our NeuFace dataset. Please refer to Sec. B in the appendix of the initial submission.\n\n---\n\n> **Q2. The quality of the reconstructed results for extreme facial expressions appears suboptimal.** \n\nThe failure cases stem from the limited performance of the off-the-shelf 2D landmark detection algorithm for extreme facial expressions. Since 2D landmark human annotations are relatively cheaper than any other signals, we think using better 2D landmarks can mitigate this limitation. We newly added this discussion in the revision (Sec. E, p. 24 in the Appendix).\n\nAlso note that these failure cases are rare, and we showed our annotation method\u2019s effectiveness even in the presence of these rare cases, by enhancing the existing 3D reconstruction model with our dataset (Table 3. in Sec. 5.1.). We hope the reviewer acknowledges this perspective."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission12/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536657939,
                "cdate": 1700536657939,
                "tmdate": 1700551184050,
                "mdate": 1700551184050,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VlK72IAxBy",
                "forum": "E6EbeJR20o",
                "replyto": "USsm9RVAsM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer C76P (Part 3/3)"
                    },
                    "comment": {
                        "value": "> **Q3-1. Room for improvement in reconstruction accuracy; It might be worthwhile for the authors to consider utilizing such data as annotations for 3D landmarks rather than 3D mesh data.** \n\nThank you for the suggestion regarding the use of our dataset for improving the another task, 3D landmark reconstruction. As our dataset includes accurate pseudo 3D mesh data along with corresponding 3D landmarks, which indeed makes it a valuable resource for this purpose. Due to the limited time of the rebuttal, we aim to include the results of these experiments in the camera-ready version of our paper.\n\n> **Q3-2. Have the authors explored the possibility of applying their proposed method to a different face model, such as the Basel Face Model (BFM), or investigating alternative pre-trained models instead of DECA or EMOCA?** \n\nWe think our losses and optimization are applicable to other 3DMMs and neural models. We chose FLAME since it is the most recent and popular 3D linear face model.\n\nFor the alternative pre-trained models, we can use our method and the same benefits would hold as long as the model predicts low dimensional parameters and the neural network is overparameterized (from Proposition 1 in Sec. 3.3.). Also, using a neural model with better accuracy would provide better initial points that can lead to the faster convergence of NeuFace optimization."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission12/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536746333,
                "cdate": 1700536746333,
                "tmdate": 1700551201721,
                "mdate": 1700551201721,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2LaQR3OFHg",
                "forum": "E6EbeJR20o",
                "replyto": "USsm9RVAsM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to reviewer's reply"
                    },
                    "comment": {
                        "value": "Dear reviewer C76P, \n\nThank you for reviewing our work to enhance the quality of the paper. \nPlease check our rebuttal. We think our rebuttal addressed all of the reviewer's comments. If there is anything more we can do to improve the paper, please leave a comment. We'd be happy to discuss further.\n\nBest regards, Authors of 12"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission12/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652946866,
                "cdate": 1700652946866,
                "tmdate": 1700653006482,
                "mdate": 1700653006482,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jcE0GvPhtZ",
                "forum": "E6EbeJR20o",
                "replyto": "USsm9RVAsM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "content": {
                    "title": {
                        "value": "[Final] Looking forward to reviewer C76P's final reply!"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful and encouraging feedback. We think that all the comments by the reviewer have been addressed. If there is no other concern, we would like to respectfully ask the reviewer to re-assess our work and increase the rating."
                    }
                },
                "number": 35,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission12/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737730829,
                "cdate": 1700737730829,
                "tmdate": 1700737787548,
                "mdate": 1700737787548,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DC6e0UeMhb",
            "forum": "E6EbeJR20o",
            "replyto": "E6EbeJR20o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission12/Reviewer_GPao"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission12/Reviewer_GPao"
            ],
            "content": {
                "summary": {
                    "value": "In this proposed method, a 3D face database is built based on the neural radiance fields method. In general, the neural face representation tries to find the best 3D mesh representation through the neural network parameters to best fit the multiple views and temporal consistent faces. Multiview and temporal consistency losses are added on top of the 2D landmark loss in the EM-like optimization process. Based on the proposed method, a significantly larger 3D face database is built using existing public 3D videos. The authors also demonstrated the possible applications of the proposed database to improve 3D face reconstruction and to learn the 3D face prior."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed 3D face database is beneficial to the research community. The proposed database is significantly larger than the typical 3D face datasets. Experimental results also demonstrate good 3D estimation results."
                },
                "weaknesses": {
                    "value": "The theoretical novelty of face reconstruction using Neural network parameterization is incremental."
                },
                "questions": {
                    "value": "What are the typical failure cases of the proposed method? In the EM-like optimization, does the reconstruction always go to the right optional result?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission12/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698641748665,
            "cdate": 1698641748665,
            "tmdate": 1699635924709,
            "mdate": 1699635924709,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zPhSZsp5Nc",
                "forum": "E6EbeJR20o",
                "replyto": "DC6e0UeMhb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GPao"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the interest in our work and the valuable feedback that strengthens our paper. We address the concern below and in the revision (please check pdf, highlighted in pink).\n\n### **Weaknesses**\n\n---\n\n> **W1. The theoretical novelty of face reconstruction using Neural network parameterization is incremental.** \n\nWe haven\u2019t claimed the theoretical novelty itself. \n\nThe theoretical context in Sec. A.2 helps the deep understanding of the analysis and algorithmic behavior of our proposed optimization system. We respectfully request the reviewer to consider our contributions below:\n\n- Proposing the first and insightful concept of neural re-parameterized 3D face optimization, which mitigates the undesirable sparse gradient for face optimization (acknowledged by Reviewer B8S8).\n- Providing a NeuFace-dataset, the first large-scale 3D face mesh pseudo-labels for existing large-scale 2D face video datasets. The dataset would benefit future research in this field (acknowledged by all the other reviewers).\n- Extensive experiments to compare the quality and reliability of the proposed optimization and the dataset, and some empirical analysis.\n\n### **Questions**\n\n---\n\n> **Q1. What are the typical failure cases of the proposed method?** \n\nThe failure cases could occur when the 2D video itself contains extreme degradations, e.g., motion blur, low resolution, extremely (> 50%) occluded, so that the 2D keypoint detection fails. Please note that, when we construct the NeuFace-dataset, we tackle these cases with automatic filtering followed by human verification, discussed in Appendix Sec. B of the initial submssion, which guarantees the reliability of the dataset.\n\n---\n\n> **Q2. In the EM-like optimization, does the reconstruction always go to the right optional result?** \n\nThe NeuFace optimization and its losses are designed in a self-improving manner. Although the initial estimate of DECA could be noisy, the strong measurement of detected 2D landmarks and robust target supervision at each iteration gradually corrects the initial noisy predictions. The theoretical analysis shows that our neural re-parameterized optimization is highly likely to converge to global optima (Appendix Sec. A.2 of the initial submission). This hints that, with our neural parameterization, the optimization is robust to noisy initials and guarantees to exhibit at least stabler optimization behaviors than the compared baseline methods."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission12/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536022567,
                "cdate": 1700536022567,
                "tmdate": 1700551128622,
                "mdate": 1700551128622,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5PqTsFpqtE",
                "forum": "E6EbeJR20o",
                "replyto": "zPhSZsp5Nc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission12/Reviewer_GPao"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission12/Reviewer_GPao"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal review"
                    },
                    "comment": {
                        "value": "My review towards this paper still hold. Overall, the proposed method is straightforward but dataset is still a good contribution. However, the authors mentioned they specifically did QA to review challenging examples in the data set which is a bit concerning. Overall, I keep my original rating due to the data set contribution."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission12/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631331523,
                "cdate": 1700631331523,
                "tmdate": 1700631331523,
                "mdate": 1700631331523,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VKkfZ9WfDx",
                "forum": "E6EbeJR20o",
                "replyto": "DC6e0UeMhb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Correction and clarification for Reviewer GPao"
                    },
                    "comment": {
                        "value": "Dear reviewer GPao, \n\nWe would like to clarify the reviewer's last comment: \n> \"However, the authors mentioned they specifically did QA to review challenging examples in the data set which is a bit concerning.\"\n\nIf the reviewer meant our 'human verification process' for the word QA, we'd like to correct and clarify that:\n- We didn't involve the human verification process at all, when assessing our dataset's quality in experiments (Secs. 3.4, 4, and A.3, Fig 4, Table 2, and Table S1).\n- We conduct the human verification process only for making the release version of our dataset.\n\nWe hope this addresses your concern about QA. If there is no other concern, we would like to respectfully ask the reviewer to re-assess our work and consider increasing the rating."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission12/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678481491,
                "cdate": 1700678481491,
                "tmdate": 1700678543863,
                "mdate": 1700678543863,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zKsm2f2SVe",
                "forum": "E6EbeJR20o",
                "replyto": "DC6e0UeMhb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer GPao,\n\nWe wish to highlight that Reviewer B8S8 has acknowledged the value of our work and updated the rating to a weak accept. We have thoroughly addressed all the comments raised by Reviewer GPao, thus we kindly request Reviewer GPao to review our revision and the responses of the rebuttal once more. We hope this might prompt a reconsideration and adjustment of your rating.\n\nBest regards,\nAuthors of 12."
                    }
                },
                "number": 30,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission12/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718565002,
                "cdate": 1700718565002,
                "tmdate": 1700718565002,
                "mdate": 1700718565002,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "md9JBrR4va",
            "forum": "E6EbeJR20o",
            "replyto": "E6EbeJR20o",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission12/Reviewer_axFo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission12/Reviewer_axFo"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents NeuFace, an optimization algorithm for fitting a morphable model to a sequence of multi-view face images. To this end it refines a pre-trained NN to fit the target images. The optimized loss includes temporal and multi-view regularization terms minimizing the distance of the reconstructed mesh to the temporal moving average, in the case of the temporal term, and to the aligned average, in the multi-view loss case. The model is iteratively refined by alternating the estimation of the reconstructions used in the regularization terms with the optimization of the network parameter that minimize the loss.\n\nThe experimentation quantitatively compares the reconstructions with two competing algorithms, DECA and EMOCA, on MEAD, VoxCeleb2 and CelebV-HQ video datasets.\n\nFinally, the algorithm is used to build the \"NeuFace dataset\" as the result of the reconstruction of the 3D face meshes in these three datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper reads well and is properly set in the research context. It addresses a relevant problem, namely, 3D face landmark estimation, with many practical applications and open challenges. The paper contributes with a new dataset and shows that by using it we may improve the accuracy of different face processing algorithms. This will be of interest to the face processing community."
                },
                "weaknesses": {
                    "value": "The the paper claims to investigate the reconstruction of image-aligned facial details on 3D meshes. However, the approach is based on optimizing the parameters of a 3DMM model, with the limitations of a linear model to represent fine facial details.\n\nIn the vertex accuracy evaluation experiments described in Sec. 3.4 and shown in Fig. 4, NeuFace optimization is compared with DECA and plain FLAME fitting. The paper does not describe the details of this experiment, specifically what are the train, validation and test data used for evaluating each algorithm. DECA results were produced by the plain pre-trained DECA model. We may assume that NeuFace optimization, as described in Sec. 3.2, was trained with some part of VOCASET and evaluated on a different part of it. It does not seem like a fair comparison, since DECA did not have the chance to see any part of VOCASET.\n\nFor the same reason, the quantitative comparisons in Table 2 seem also unfair, since the optimizations in NEUFACE-*-datasets could see part of MEAD, VoxCeleb2 and CelebV-HQ data, whereas those involving DECA and EMOCA datasets did not. In Sec. A.3 and Table S1 we can see that if we give DECA the chance to be refined on these datasets, the NME in MEAD reduces to 2.44, much lower than 4.65 shown in Table 2."
                },
                "questions": {
                    "value": "There are important details missing:\n- Section 3.1 It would be good if you extended the explanation by adding the dimension of each FLAME parameter. Also, the backbone network, e.g. DECA, not only estimates the 3DMM parameters, but also texture, lighting and a displacement map to model details outside the 3DMM linear model. I understand that the approach discards the texture and lighting part, but what about the displacement map? \n- Section 3.2 does not explain how the ground-truth landmarks for equation 2 were obtained. Also, in the Multi-view consistency loss it does not explain where the confidence values for each vertex come from.\n- Experiments. The paper must clearly explain what is the train/validation/test data used in each experiment and confirm that the results shown in the accuracy evaluation in Fig.4 and Table 2 are correct and fair.\n\nOften the base pre-trained model fails dramatically. In this situation, averaging the estimated mesh with others in the regularization terms would ruin the optimization, since the average operation is not robust. Would an alternative robust operation, e.g. median, improve the results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Privacy, security and safety",
                        "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The paper reads \"Since our dataset is acquired based on the existing public video datasets (Wang et al., 2020; Chung et al., 2018; Zhu et al., 2022), all the rights, licenses, and permissions follow the original datasets.\" However, some of these datasets were automatically gathered from the internet. So, it is unclear whether the new dataset is legally compliant."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission12/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698785449631,
            "cdate": 1698785449631,
            "tmdate": 1699635924631,
            "mdate": 1699635924631,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cOkYwMmqbz",
                "forum": "E6EbeJR20o",
                "replyto": "md9JBrR4va",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer axFo (Part 1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the time and the thorough review. By addressing the reviewer\u2019s questions and comments, we could strengthen our paper. We address the concerns and the questions below and in the revision (please check pdf, highlighted in pink).\n\nWe\u2019d like to ask the reviewer to re-assess the value of our work with the following clarification.\n\nPlease let us know if our answers address the reviewer\u2019s concerns. We would be happy to provide further discussions and clarifications.\n\n### **Weaknesses**\n\n---\n\n> **W1. Claims to investigate facial details. The approach is based on 3DMM with limitations of a linear model to represent fine facial details.** \n\nWe would like to clarify the scope of our approach. This is a misleading point due to the term we used, which we have revised and tone-downed the terms across the paper in this revision.\n\nOur work aims to *facial details* of facial gestures and motions, not mesoscopic facial geometry of facial skin, which is the point the misleading happens at. Thus, we do not reconstruct the displacement-level facial details which is not the target of our work. We focus on 3D facial geometries complying with input facial gestures and motions. \n\nWe revised the overall paper and tone-down our claim of \u201c*reconstructing image-aligned facial details*\u201d into \u201c***reconstructing facial geometries, well complying with input facial gestures and motions***\u201d, highlighted in pink. Thanks for the comment for specifying our scope better.\n\n---\n\n> **W2. Concerns on fair comparison and missing descriptions of experiments (Sec. 3.4, Fig. 4).** \n\nWe respectfully note that this concern misled by the reviewer's misunderstanding of the premise of our work. We hope the explanation below clearly addresses the points leading to the reviewer's misconcern.\n\nFirst of all, the evaluation of our NeuFace test-time optimization is indeed fair, which we carefully designed, as all the other test-time approaches are. Specifically, \n\n1. Our\u00a0NeuFace optimization, as elaborated in Sec. 3.2,\u00a0is a TEST-TIME approach and does not involve training with train/validation/test splits. Our NeuFace step itself only exploits the input samples at test-time, but no additional data is used for the NeuFace itself.\n2. We use the the pre-trained base model in NeuFace, e.g., DECA, for test-time fine-tuning on the input samples at test-time. Thus, the comparison with the pre-trained base model and our NeuFace (base model + NeuFace test-time optimization) is indeed in the fair setting; they have seen the exactly same data).\n3. Our NeuFace is evaluated on each sample sequence independently. In Sec.3.4 and Fig. 4, for NeuFace optimization, we do not split the VOCASET into train/validation/test splits. We performed the test-time optimization for each sequence of VOCASET, starting from the pre-trained DECA checkpoint (pre-trained on VGGFace2 [C1], BUPT-Balancedface [C2], VoxCeleb2 [C3]), and detected 2D landmarks (we didn't use any GT information including mesh or 2D landmarks from VOCASET). Note that 2D landmark detections are not the ground-truth, as well. Thus, we claim that both DECA and ours do not have the chance to see any test dataset.\n4. Likewise, our method is evaluated and compared for each test samples in each dataset (including VOCASET, MEAD, Voxceleb, and Celebv-HQ) independently without using their ground-truth mesh.\n\nOur goal of experiments in Sec 3.4 and Fig. 4 was to compare the quality of pseudo 3D mesh annotations obtained from existing methods. In other words, the experiment in Sec 3.4 and Fig. 4 did not aim to compare the performance of each trained model on test datasets.\n\nIn the 3D face community, recent works [C4,C5] still naively use a pre-trained DECA or FLAME fitting (baseline) for annotator. In our experiments, we wanted to show how accurate and reliable NeuFace optimization is in generating pseudo 3D face annotations compared to existing methods.\n\nTo conclude,\u00a0**NeuFace is not a learned model but a test-time optimization method; thus, it does not require training/validation/test sets.**\u00a0Therefore, we confirm that our experiments were correct and fair, which we carefully designed.\n\n[C1] Cao et al., VGGFace2: A dataset for recognising faces across pose and age. In International Conference on Automatic Face & Gesture Recognition (FG) 2018.\n\n[C2] Wang et al., Racial Faces in the Wild: Reducing Racial Bias by Information Maximization Adaptation Network. In ICCV 2019.\n\n[C3] Chung et al., VoxCeleb2: Deep Speaker Recognition. In INTERSPEECH 2018.\n\n[C4] Ng et al., Learning to listen: Modeling non-deterministic dyadic facial motion. In CVPR 2022.\n\n[C5] Paraperas et al., Neural Emotion Director: Speech-preserving semantic control of facial expressions in \"in-the-wild\" videos. In CVPR 2022."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission12/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534966697,
                "cdate": 1700534966697,
                "tmdate": 1700550576422,
                "mdate": 1700550576422,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RdoegQq0JB",
                "forum": "E6EbeJR20o",
                "replyto": "md9JBrR4va",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer axFo (Part 2/3)"
                    },
                    "comment": {
                        "value": "> **W3. Unfair comparison in Table 2, Sec A.3., & Table S1.** \n\nOur goal in Table 2 was to compare the quality of our pseudo 3D mesh annotations (NeuFace-D-dataset, NeuFace-E-dataset) with those obtained from existing methods (Base-dataset, DECA-dataset, EMOCA-dataset). \n\n**We emphasize that both DECA and ours do not have the chance to see any test dataset,** given the fact that 1) for Table 2, we do NOT train models using MEAD, VoxCeleb2, and CelebV-HQ at all, and 2) 2D landmark detections are not the ground-truth but estimated from the off-the-shelf module.\n\nIn Sec. A.3 and Table S1, we have compared the dataset quality obtained with our different TEST-TIME loss configurations. When we use only $\\mathcal{L}_\\text{2D}$ to refine the DECA estimations, the NME indeed improves than DECA\u2019s initial prediction. However, as mentioned in [W2], NeuFace optimization is not a learned nor a fine-tuned model on MEAD, VoxCeleb2, and CelebV-HQ datasets. NeuFace optimization is a test-time optimization method for fitting 3D face for each video. Also, the 2D landmark detections itself is a pseudo ground-truth. Therefore, we conclude that our experiment for Table S1 was correct and fair as well.\n\n\n\n### **Questions**\n\n---\n\n> **Q1-1. It would be good if you extended the explanation by adding the dimension of each FLAME parameter.** \n\nThanks for the thoughtful comment. We have added the detailed explanation in the revision (Sec. 3.1), highlighted in pink, as:\n\n\"We use FLAME, a renowned 3DMM, as a 3D face representation. 3D face mesh vertices $\\mathbf{M}$ and facial landmarks $\\mathbf{J}$ for $F$ frame videos can be acquired with the differentiable skinning: $\\mathbf{M}, \\mathbf{J}{=}\\texttt{FLAME}(\\boldsymbol{\\mathbf{r}, \\boldsymbol{\\theta}, \\boldsymbol{\\beta}, \\boldsymbol{\\psi}})$, __where $\\mathbf{r}\\in{\\mathbb{R}^{3}}$, $\\boldsymbol{\\theta}\\in{\\mathbb R}^{12}$, $\\boldsymbol{\\beta}\\in{\\mathbb R}^{100}$ and $\\boldsymbol{\\psi}\\in{\\mathbb R}^{50}$__ denote the head orientation, face poses, face shape and expression coefficients, respectively.\"\n\n\n> **Q1-2. Considering the displacement map in addition to the FLAME parameter?** \n\nWe think optimizing the mesoscopic detailed face geometry by re-parameterizing the displacement map is definitely an interesting future direction. \n\nAs discussed in [W1], the face skin detail reconstruction is not our scope of this work, but \u201c***reconstructing facial geometries, well complying with input facial gestures and motions.***\u201d More importantly, we are motivated by the fact that there are lack of existing large-scale in-the-wild or multi-view video datasets that contain high-level face geometry, head motion, identity, and expressions that comply with input videos, in the community.\n\nWe believe our NeuFace dataset, which contains large-scale, diverse, natural, high-level human 3D face motion, would invigorate the 3D face community.\n\n---\n\n> **Q2-1. Sec. 3.2: how were the ground-truth landmarks for equation 2 obtained?** \n\nWe used the off-the-shelf 2D landmark detection model, FAN [C6]. We also performed manual human verifications to reject the failure cases when constructing the NeuFace-dataset.\n\nWe have added this to the revision (Sec 3.2, second paragraph, page 4), highlighted in pink. Thanks for checking the details.\n\n[C6] Face Recognition (FAN), https://github.com/1adrianb/face-alignment\n\n> **Q2-2. In the Multi-view consistency loss, where do the confidence values come from?** \n\nWe assign the confidence score per vertex by measuring the angle between the vertex normal and the camera ray. We set the vertices as invisible if the angle is larger than the threshold $\\tau_a$, and the vertex has a deeper depth than $\\tau_{z}$ (i.e., $z<\\tau_z$). We empirically choose $\\tau_a = 72^\\circ$, $\\tau_z=-0.08$. We have revised the paper accordingly (p5, Sec 3.2), highlighted in pink."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission12/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700535659849,
                "cdate": 1700535659849,
                "tmdate": 1700551083626,
                "mdate": 1700551083626,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5X2IPKljTN",
                "forum": "E6EbeJR20o",
                "replyto": "md9JBrR4va",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer axFo (Part 3/3)"
                    },
                    "comment": {
                        "value": "> **Q3. Would an alternative robust operation, e.g., median, improve the results?** \n\nFollowing the reviewer's suggestion, we have compared the mean, median, and weighted average (ours) in the multi-view loss. We have compared the results on two scenarios: 1) Standard cases (use original images) and 2) Extreme cases (use perturbated images). For extreme cases, we randomly apply perturbation with large black boxes to the facial areas for 2~3 views in the multi-view videos to mimic significant corruption scenarios. \n\n| Loss configuration |  Standard cases (CVD)\u2193 | Extreme cases (CVD)\u2193 |\n| --- | --- | --- |\n| Average | 0.106 | 0.124 |\n| Median | 0.104 | 0.112 |\n| **Weighted average (Ours)** | 0.103 | 0.113 |\n- Results of standard cases: All three methods (mean, median, and weighted average) showed similar performance, with no marked difference in optimization results evaluated on the MEAD subset.\n- Results of extreme cases: The median outperforms the mean in perturbed MEAD data.\n- Our Method: Our methodology, which employs a weighted average grounded in the confidence scores from multiple view vertices, not only shows favorable performance over the simple average but also aligns closely with the median's results. This indicates that our approach maintains robustness in extreme cases, akin to the median operation highlighted by the reviewer.\n\nThe reviewer's suggestion about the median was insightful. Both our weighted average and the median work for extreme scenarios. One difference is that our visibility-based weighted average has a control parameter to adjust the robustness through hyperparameters (as in Q2-2.).\n\n---\n\n> **Q4. Ethics Concerns: Some of these datasets were automatically gathered from the internet. So, it is unclear whether the new dataset is legally compliant.** \n\nWe will not release the video dataset itself, but will release only the optimized 3DMM parameters obtained by our method without the video frames that might have been gathered from the internet. \n\nThe optimized 3DMM parameter does not contain identity-specific metadata or facial texture maps. Also, we will release the code that can generate pseudo ground-truth datasets like the NeuFace dataset for generic applications."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission12/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700535812266,
                "cdate": 1700535812266,
                "tmdate": 1700551104385,
                "mdate": 1700551104385,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vcI85NqTpr",
                "forum": "E6EbeJR20o",
                "replyto": "md9JBrR4va",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to reviewer axFo's reply!"
                    },
                    "comment": {
                        "value": "Dear reviewer aXFo,\n\nThank you for reviewing our work. \nWe sincerely ask reviewer axFo to check our rebuttal.  \nWe believe that we have thoroughly addressed all the comments and suggestions reviewer axFo had raised. \nConsidering the remaining time, we welcome any further comments or feedback that may help to enhance our work. \nWe'd be happy to further discuss.\n\nBest regards, \nAuthors of 12"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission12/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653052473,
                "cdate": 1700653052473,
                "tmdate": 1700653052473,
                "mdate": 1700653052473,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CMQ3lNM15o",
                "forum": "E6EbeJR20o",
                "replyto": "md9JBrR4va",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "content": {
                    "title": {
                        "value": "[Final] Looking forward to reviewer axFo's final reply!"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful and encouraging feedback.\nWe think that all the comments by the reviewer have been addressed, and the reviewer also confirmed the reviewer's misled concerns being corrected. If there is no other concern, we would like to respectfully ask the reviewer to re-assess our work and increase the rating."
                    }
                },
                "number": 31,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission12/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736376326,
                "cdate": 1700736376326,
                "tmdate": 1700736376326,
                "mdate": 1700736376326,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WAUZF4eI4s",
                "forum": "E6EbeJR20o",
                "replyto": "CMQ3lNM15o",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission12/Reviewer_axFo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission12/Reviewer_axFo"
                ],
                "content": {
                    "title": {
                        "value": "Still waiting for your reply"
                    },
                    "comment": {
                        "value": "Dear authors.\nThanks for your message. Perhaps you missed my previous message of Nov 22nd at 16:03.  Let me attach it here ...\n\n\"Thanks for your detailed answer. I can see that you do not train your model with any \"extra\" images from the datasets, but rather optimize eq (1).\n\nI assume that in this case the predictions of DECA or EMOCA models in Fig 4 and Tables 2 and S1 only analyze one image, whereas those of NeuFace are optimized with eq (1) using all frames in each sequence. Is this correct?\n\nAlso, it is not yet clear to me where you get the ground truth landmarks to compute the NME in Tables 2 and S1.\"\n\nI would like a response to these questions. Best regards."
                    }
                },
                "number": 33,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission12/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736814134,
                "cdate": 1700736814134,
                "tmdate": 1700736814134,
                "mdate": 1700736814134,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sb6Un9adF0",
                "forum": "E6EbeJR20o",
                "replyto": "md9JBrR4va",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer axFo, \n\nWe kindly note that \bthe reviewer's comments were at the top (not linked to this thread), and we already answered above. \n\n**1) Answer about the experiments**\n\n[22 Nov 2023, 16:31] Reply link: https://openreview.net/forum?id=E6EbeJR20o&noteId=bsAfVVwlqg\n\n\n**2) Answer about temporal consistency loss**\n\nAlso, please check our response on temporal consistency loss at above (the reviewer's comment), as well. \n\n[22 Nov 2023, 17:53] Reply link: https://openreview.net/forum?id=E6EbeJR20o&noteId=n5KZS1btNz\n\n\nWe respectfully request the reviewer axFo to have a look at our responses, re-assess our work, and increase the rating, if the reviewer's concerns are addressed."
                    }
                },
                "number": 34,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission12/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737631798,
                "cdate": 1700737631798,
                "tmdate": 1700737989094,
                "mdate": 1700737989094,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EACVJNruPs",
                "forum": "E6EbeJR20o",
                "replyto": "sb6Un9adF0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission12/Reviewer_axFo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission12/Reviewer_axFo"
                ],
                "content": {
                    "title": {
                        "value": "No clear answer to my last two questions"
                    },
                    "comment": {
                        "value": "Dear authors. \nThanks for your message and for sending me the links to your previous responses.\nI cannot find in them a clear and straight answer to these questions:\n\n\"I assume that in this case the predictions of DECA or EMOCA models in Fig 4 and Tables 2 and S1 only analyze one image, whereas those of NeuFace are optimized with eq (1) using all frames in each sequence. Is this correct?\n\nAlso, it is not yet clear to me where you get the ground truth landmarks to compute the NME in Tables 2 and S1.\"\n\nPlease do not miss the opportunity to do so at this time. Best regards."
                    }
                },
                "number": 36,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission12/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738051408,
                "cdate": 1700738051408,
                "tmdate": 1700738051408,
                "mdate": 1700738051408,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SIfwlONuZN",
                "forum": "E6EbeJR20o",
                "replyto": "md9JBrR4va",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission12/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer axFo,\n\nWe believe that all the comments by the reviewer have been addressed. \nIf there is no other concern, we respectfully request Reviewer axFo to review our revision and the responses of the rebuttal once more. We hope this might prompt a reconsideration and adjustment of Reviewer axFo's rating.\n\nBest regards,\n\nAuthors of 12."
                    }
                },
                "number": 39,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission12/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740112891,
                "cdate": 1700740112891,
                "tmdate": 1700740112891,
                "mdate": 1700740112891,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]