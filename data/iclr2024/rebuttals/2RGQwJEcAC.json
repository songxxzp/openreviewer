[
    {
        "title": "Visual Transformer with Differentiable Channel Selection: An Information Bottleneck Inspired Approach"
    },
    {
        "review": {
            "id": "IgUaA8csbG",
            "forum": "2RGQwJEcAC",
            "replyto": "2RGQwJEcAC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6878/Reviewer_wQJ5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6878/Reviewer_wQJ5"
            ],
            "content": {
                "summary": {
                    "value": "The study introduces an innovative and streamlined transformer block named DCS-Transformer, which facilitates channel selection for both attention weights and attention outputs. The inspiration for this channel selection arises from the information bottleneck (IB) principle. This principle strives to diminish the mutual information between the transformer block's input and output, all the while maintaining the mutual information between the output and the task label. This is chiefly realized through the use of a Gumbel-softmax and a channel-pruning loss.\n\nThe overall framework mirrors the structure presented in the Neural Architecture Search (NAS), encompassing both a search phase and a training phase. In the quest for the optimal weights for channel selection, the authors put forth an IB-associated loss for the search process. The proficiency of the presented approach is corroborated by experimental findings on ImageNet-1k and COCO."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well written.\n\n2. The introduction of IB loss into Vision Transformers sounds novel."
                },
                "weaknesses": {
                    "value": "1. The rationale for utilizing an Information Bottleneck loss appears to be somewhat rigid and unclear to me.\n- The authors explain that the reason for employing this loss is that\n\n\n> IB prompts the network to learn features that are more strongly correlated with class labels while decreasing their correlation with the input.\n\n\nHowever, it remains unclear to me why a traditional Softmax cross-entropy loss wouldn't be adequate to address this issue effectively.\n\n2. The primary contribution of this paper doesn't seem to be particularly effective.\n\nAs per Table 6 in the appendix, the implementation of the proposed IB loss results in only a 0.3% improvement when used on the same backbone. Such a minor improvement could also be attained simply by using a more favourable random seed, which might be too trivial to serve as the main contribution of a ICLR paper.\n\n3. Some of the ablation studies on hyper-parameter search are missing.\n\nI'm intrigued by the roles of the hyper-parameters $\\eta$ and $\\lambda$ in the suggested approach. It appears that if $\\eta$ is not set to a small value, the outcomes could be inferior to the baseline. Upon examining the code, I noticed that $\\eta$ is defaulted to 0.1, which contrasts with the paper's assertion that $\\eta$ is set to 50 for ImageNet. This discrepancy could potentially be confusing for many readers.\n\n4. The discussion on related works is not comprehensive enough.\n\nThis paper introduces some techniques, e.g. channel selection with Gumbel SoftMax, and entropy minimization for architecture search, that were first applied in the field of Neural Architecture Search (NAS) and network pruning. However, the section of related work does not include a subsection in this direction, which is inappropriate from my point of view. Some seminal works like [1, 2, 3] should be included and carefully discussed.\n\n### Reference\n\n[1] Xie S, Zheng H, Liu C, et al. SNAS: stochastic neural architecture search.ICLR 2019.\n\n[2] Herrmann C, Bowen R S, Zabih R. Channel selection using gumbel softmax. ECCV 2020.\n\n[3] Liu H, Simonyan K, Yang Y. Darts: Differentiable architecture search. ICLR 2019"
                },
                "questions": {
                    "value": "see weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6878/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698736399018,
            "cdate": 1698736399018,
            "tmdate": 1699636799616,
            "mdate": 1699636799616,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b1bxgFOMog",
                "forum": "2RGQwJEcAC",
                "replyto": "IgUaA8csbG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6878/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6878/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wQJ5 Part 1"
                    },
                    "comment": {
                        "value": "We appreciate the review and the suggestions in this review. The raised issues are addressed below.\n\n(1) **More detailed motivation for the IB Loss and Regular Cross-Entropy Cannot Decrease the IB Loss Enough**\n\n\nMore detailed motivation for the incorporate of information bottleneck loss is presented in Section 1 of the revised paper. \nIt is attached below for your convenience. \n\n**Motivation** \n\nA typical transformer block can be written as \n$\\texttt{Output} = \\texttt{MLP} \\left(\\sigma(QK^{\\top})\\times V \\right)$ \nwhere $Q,K,V \\in \\mathbb{R}^{N \\times D}$ denote the query, key, and value respectively \nwith $N$ being the number of tokens and $D$ being the input channel number. \n$\\sigma(\\cdot)$ is an operator, such as Softmax, which generates the attention weights or affinity between the tokens. \nWe refer to $W  = \\sigma(QK^{\\top}) \\in \\mathbb{R}^{N \\times N}$ as the affinity matrix between the tokens. \n$\\textup{MLP}$ (multi-layer perceptron network) generates the output features of the transformer block. \nThere are $D$ channels in the input and output features of the MLP, and $D$ is also the channel of the attention outputs. \nDue to the fact that the MLP accounts for a considerable amount of FLOPs in a transformer block, the size and FLOPs of a transformer \nblock can be significantly reduced by reducing the channels of the attention outputs from $D$ to a much smaller $\\tilde D$. \n\\textbf{Our goal is to prune the attention output channels while maintaining and even improving the prediction accuracy of the original transformer.} \nHowever, directly reducing the channels attention outputs, even by carefully designed methods, would adversely affect the performance of the model. \nIn this paper, we propose to maintain or even improve the prediction accuracy of a visual transformer with pruned attention outputs channels \nby computing a more informative affinity matrix $W$ through selecting informative channels in the query $Q$ and the key $V$. That is, only selected columns of $Q$, \nwhich correspond to the same selected rows of $K^{\\top}$, are used to compute the affinity matrix $W  = \\sigma(QK^{\\top})$, which is refered to as channel selection \nfor attention weights and illustrated in Figure 1a. We note that the attention outputs, which are also the input features to the MLP, is $W\\times V$, \nand every input feature to the MLP is an aggregation of the rows of the value $V$ using the attention weights in $W$. \nAs a result, pruning the channels of $W\\times V$ amounts to pruning the channels of $V$ in the weighted aggregation. If the affinity $W$ is more informative,\n it is expected that a smaller number of features (rows) in $V$ contribute to such weighted aggregation, and the adverse effect of channel selection on the prediction accuracy \n of the transformer network is limited. Importantly, with a very informative affinity $W$, every input feature of the MLP is obtained by aggregation of the most relevant features (rows) \n in $V$, which can even boost the performance of visual transformers after channel selection or pruning of the attention outputs.\n \n\nThe idea of channel selection for the attention weights can be viewed from the perspective of Information Bottleneck (IB). \nLet $X$ be the input training features, $\\tilde X$ be the learned features by the network, and $Y$ be the ground truth training labels for a classification task. \nThe principle of IB is maximizing the mutual information between $\\tilde X$ and $Y$ while minimizing the mutual information between $\\tilde X$ and $X$. That is, \nIB encourages the network to learn features more correlated with the class labels while reducing their correlation with the input. Extensive empirical and theoretical\nworks have evidenced that models respecting the IB principle enjoys compelling generalization. With channel selection for the attention weights, every feature in the attention \noutputs aggregates less features of the value $V$, so the attention outputs are less correlated with the training images so the IB principle is better adhered. This is reflected \nin Table 5 in Section C.2 of the supplementary, where a model for ablation study with channel selection for attention weights, \nDCS-Arch1 w/o IB Loss, enjoys less IB loss and higher top-1 accuracy than the vanilla transformer, MobileViT-S. It is noted that the model, DCS-Arch1 w/o IB Loss, only uses \nthe regular cross-entropy loss in the retraining step, and smaller IB loss indicates that the IB principle is better respected. In order to further decrease the IB loss, \nwe propose an Information Bottleneck (IB) inspired channel selection for the attention weights $\\mathbf W$ where the learned attention weights can be more informative by explicitly \noptimizing the IB loss for visual transformers. Our model termed ``DCS-MobileViT-S'' in Table 5 is the visual transformer with the IB loss optimized, \nso that more informative attention weights are learned featuring even smaller IB loss and even higher top-1 accuracy."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6878/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722336832,
                "cdate": 1700722336832,
                "tmdate": 1700722336832,
                "mdate": 1700722336832,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZzMlXOLIH2",
            "forum": "2RGQwJEcAC",
            "replyto": "2RGQwJEcAC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6878/Reviewer_Q1sT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6878/Reviewer_Q1sT"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a compact transformer architecture exploring the differentiable channel selection. There are two types of channel selection, which are channel selection for attention weights and channel selection for attention outputs. In addition,  the IB loss is employed to boost the performance of the proposed framework. Extensive experiments on image classification and object detection verifies the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-written and well-motivated. \n\n2. The design of the channel selection for attention weights and attention outputs make sense, corresponding to the matrix multiplication for attention and MLP. The IB loss is further considered to improve the performance.\n\n3. The comparison with the SOTA pruning methods and compact models show that the proposed method is effective on the mobile devices."
                },
                "weaknesses": {
                    "value": "1. There are only comparisons of parameters and FLOPs, I wonder the actual inference time of the proposed method.\n\n2. In Figure 2, there are two points for EfficientViT while only one point for DCS-EfficientViT. What's the performance of another point? In another word, is the proposed method still valuable for a larger model?\n\n3. The hyper-parameters are carefully designed such as the temperature etc. I am doubt about the generalization of the proposed method."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6878/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698752819968,
            "cdate": 1698752819968,
            "tmdate": 1699636799500,
            "mdate": 1699636799500,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2dIbF8yma7",
                "forum": "2RGQwJEcAC",
                "replyto": "ZzMlXOLIH2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6878/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6878/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Q1sT"
                    },
                    "comment": {
                        "value": "We appreciate the review and the suggestions in this review. The raised issues are addressed below.\n\n(1) **Actual Inference Time**\n\nWe compare DCS-Transformer to the current state-of-the-art pruning methods for visual transformers, the results are shown in the table below. \nDCS-Transformer is compared to S$^2$ViTE[2], SPViT[3] and SAViT [4] on EfficientViT-B1 (r224) [1]. \nFor S$^2$ViTE[2], SPViT[3] and SAViT [4], the pruning is performed on the ImageNet training data. After obtaining the pruned networks, we fine-tune the pruned networks using the same setting as [1]. \nIt can be observed that DCS-EfficientViT-B1 outperforms all pruned models by at least 1.9% in top-1 accuracy with even less parameters and FLOPs.\nWe report the actual inference time of all the model on a NVIDIA V100 GPU.\n\n\n| Methods                          | \\# Params | FLOPs   | Inference Time (ms/image) | Top-1 |\n| :------------------------------: | :-------: | :-----: | :---: | :---: |\n| EfficientViT-B1 (r224) [1]       | 9.1 M    | 0.52 G | 2.654 | 79.4 |\n|S$^2$ViTE-EfficientViT-B1 (r224) [2] | 8.2 M    | 0.47 G | 2.438 | 79.0 |\n| SPViT-EfficientViT-B1 (r224) [3]  | 9.2 M    | 0.49 G | 2.451 | 79.3 |\n| SAViT-EfficientViT-B1 (r224) [4] | 8.4 M    | 0.47 G | 2.435 | 79.2 |\n| DCS-EfficientViT-B1 (r224)       | 8.2 M    | 0.46 G | 2.427 | 81.2 |\n\n\n\n(2) **Completed Figure 2**\n\nThe two points for DCS-EfficientViT have been added to Figure 2 in the revised paper.\n\n(3) **Tuning Hyperparameters by Cross-Validation**\n\nThank you for your concern about the hyperparameters. We set the initial value of the temperature $\\tau$ to $4.5$ and decrease it by a factor of $0.95$ every epoch, and this is the setting used for all the experiments in this paper. In the revised paper, DCS-Transformer is trained by an improved training strategy \nwhich tunes the hyperparameters $\\lambda,\\eta,t_{\\texttt{warm}}$ by cross-validation. The details are presented in\nSection B of the supplementary of the revised paper. The value of $\\lambda$ is selected from $\\{0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0,5\\}$. \nThe value of $\\eta$ is selected from $\\{0.1, 0.5, 1, 5, 10, 50, 100\\}$. The value of $t_{\\text{warm}}$, which is the number of warm-up epochs, is selected from $\\{0.1t_{\\texttt{train}}, 0.2t_{\\texttt{train}}, 0.3t_{\\texttt{train}}, 0.4t_{\\texttt{train}}, 0.5t_{\\texttt{train}}, 0.6t_{\\texttt{train}}\\}$, \nwhere $t_{\\texttt{train}}=300$ is the total number of training epochs.  We select the values of $\\eta$, $\\lambda$, and $t_{\\texttt{warm}}$ that \nlead to the smallest validation loss. It is revealed that $t_{\\texttt{warm}} = 90$ is chosen for all the three visual transformers in our experiments.\nUsing the searched hyperparameters by cross-validation which is a principled method for tuning hyperparameters, we have obtained significantly improved results of \nDCS-Transformers shown in Table 1, and the same results are also reflected in the revised paper. For exmaple, compared to the top-1 accuracy of \n79.9% of DCS-MobileViT-S (w/o IB Loss), DCS-MobileViT-S achieves a top-1 accuracy of 81.0%.\n\n**References**\n\n[1] Cai et al. \"Efficientvit: Enhanced linear attention for high-resolution low-computation visual recognition.\" ICCV 2023.\n\n\n[2] Chen, Tianlong, et al. \"Chasing sparsity in vision transformers: An end-to-end exploration.\" NeurIPS 2021.\n\n\n[3] Kong, Zhenglun, et al. \"SPViT: Enabling faster vision transformers via soft token pruning.\" ECCV 2022.\n\n\n[4] Zheng, Chuanyang, et al. \"SAViT: Structure-Aware Vision Transformer Pruning via Collaborative Optimization.\" NeurIPS 2022."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6878/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725568148,
                "cdate": 1700725568148,
                "tmdate": 1700725568148,
                "mdate": 1700725568148,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lgL1HgzylQ",
            "forum": "2RGQwJEcAC",
            "replyto": "2RGQwJEcAC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6878/Reviewer_JKXw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6878/Reviewer_JKXw"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a DCS mechanism, which achieves network pruning via differentiable channel selection. Specifically, two channel selection strategies have been proposed, that is, channel selection for attention weights and channel selection for attention outputs. To ensure that only informative channels have been selected, the authors incorporate IB loss, which is inspired by the information bottleneck theory. Experiments on image classification and object detection have demonstrated the effectiveness of the proposed method, as well as its generalization on multiple Transformer architectures, including EfficientViT and MobileViT."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Basically, the main contribution of the proposed method is two-fold: a straightforward channel selection mechanism, and the intuitive incorporation of information bottleneck theory. Although both ideas have long been proposed, their combination and utilization in Transformer pruning may still be inspiring, especially to researchers in this specific field. The authors' claims have also been well-supported by the extensive experimental results. The manuscript is generally well-written and the English usage is satisfactory."
                },
                "weaknesses": {
                    "value": "There are several aspects of this work that could be further improved:\n\n1. The authors may consider focusing more on illustrating their motivation and ideas instead of explaining the technical details. For example, the usage of information bottleneck in the proposed method is not well-introduced. I was expecting to see how the information bottleneck theory is integrated into the proposed architecture and the rationale behind it, but only to fine detailed derivation of the variational upper bound for the IB loss.\n\n2. Is it possible that the propose module be applied to more classical architecture of ViT, e.g., the vanilla ViT or Swin? And what would be performance if DCS is used in semantic segmentation tasks. More experimental results would make the paper more convincing.\n\n3. It seems that the authors fail to compare their method to other pruning techniques, but only show that DCS is effective as it successfully reduce the number of parameters without sacrificing the performance. I also expect a comprehensive comparison against benchmark pruning methods in term of the overall computational cost.\n\nAfter rebuttal:\nI appreciate the detailed response provided by the authors, which have solved much of my concern and lead to the increase of overall rating. I would recommend the authors to integrate the supplementary results they provide in the rebuttal phase into their manuscript, so as to make it more intuitive and convincing."
                },
                "questions": {
                    "value": "Please refer to the weakness. My concern mainly lies in the experiment section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6878/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6878/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6878/Reviewer_JKXw"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6878/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699240487907,
            "cdate": 1699240487907,
            "tmdate": 1700719615908,
            "mdate": 1700719615908,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nWgghKzrxe",
                "forum": "2RGQwJEcAC",
                "replyto": "lgL1HgzylQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6878/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6878/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JKXw Part 1"
                    },
                    "comment": {
                        "value": "We appreciate the review and the suggestions in this review. The raised issues are addressed below.\n\n(1) **More Detailed Motivation**\n\nMore detailed motivation and ideas for the incorporate of information bottleneck in the proposed architecture and \nthe rationale behind it are presented in Section 1 of the revised paper. It is attached below for your convenience.\n\n**Motivation**  \n\nA typical transformer block can be written as \n$\\texttt{Output} = \\texttt{MLP} \\left(\\sigma(QK^{\\top})\\times V \\right)$ \nwhere $Q,K,V \\in \\mathbb{R}^{N \\times D}$ denote the query, key, and value respectively \nwith $N$ being the number of tokens and $D$ being the input channel number. \n$\\sigma(\\cdot)$ is an operator, such as Softmax, which generates the attention weights or affinity between the tokens. \nWe refer to $W  = \\sigma(QK^{\\top}) \\in \\mathbb{R}^{N \\times N}$ as the affinity matrix between the tokens. \n$\\textup{MLP}$ (multi-layer perceptron network) generates the output features of the transformer block. \nThere are $D$ channels in the input and output features of the MLP, and $D$ is also the channel of the attention outputs. \nDue to the fact that the MLP accounts for a considerable amount of FLOPs in a transformer block, the size and FLOPs of a transformer \nblock can be significantly reduced by reducing the channels of the attention outputs from $D$ to a much smaller $\\tilde D$. \n\\textbf{Our goal is to prune the attention output channels while maintaining and even improving the prediction accuracy of the original transformer.} \nHowever, directly reducing the channels attention outputs, even by carefully designed methods, would adversely affect the performance of the model. \nIn this paper, we propose to maintain or even improve the prediction accuracy of a visual transformer with pruned attention outputs channels \nby computing a more informative affinity matrix $W$ through selecting informative channels in the query $Q$ and the key $V$. That is, only selected columns of $Q$, \nwhich correspond to the same selected rows of $K^{\\top}$, are used to compute the affinity matrix $W  = \\sigma(QK^{\\top})$, which is refered to as channel selection \nfor attention weights and illustrated in Figure 1a. We note that the attention outputs, which are also the input features to the MLP, is $W\\times V$, \nand every input feature to the MLP is an aggregation of the rows of the value $V$ using the attention weights in $W$. \nAs a result, pruning the channels of $W\\times V$ amounts to pruning the channels of $V$ in the weighted aggregation. If the affinity $W$ is more informative,\n it is expected that a smaller number of features (rows) in $V$ contribute to such weighted aggregation, and the adverse effect of channel selection on the prediction accuracy \n of the transformer network is limited. Importantly, with a very informative affinity $W$, every input feature of the MLP is obtained by aggregation of the most relevant features (rows) \n in $V$, which can even boost the performance of visual transformers after channel selection or pruning of the attention outputs.\n \nThe idea of channel selection for the attention weights can be viewed from the perspective of Information Bottleneck (IB). \nLet $X$ be the input training features, $\\tilde X$ be the learned features by the network, and $Y$ be the ground truth training labels for a classification task. \nThe principle of IB is maximizing the mutual information between $\\tilde X$ and $Y$ while minimizing the mutual information between $\\tilde X$ and $X$. That is, \nIB encourages the network to learn features more correlated with the class labels while reducing their correlation with the input. Extensive empirical and theoretical\nworks have evidenced that models respecting the IB principle enjoys compelling generalization. With channel selection for the attention weights, every feature in the attention \noutputs aggregates less features of the value $V$, so the attention outputs are less correlated with the training images so the IB principle is better adhered. This is reflected \nin Table 5 in Section C.2 of the supplementary, where a model for ablation study with channel selection for attention weights, \nDCS-Arch1 w/o IB Loss, enjoys less IB loss and higher top-1 accuracy than the vanilla transformer, MobileViT-S. It is noted that the model, DCS-Arch1 w/o IB Loss, only uses \nthe regular cross-entropy loss in the retraining step, and smaller IB loss indicates that the IB principle is better respected. In order to further decrease the IB loss, \nwe propose an Information Bottleneck (IB) inspired channel selection for the attention weights $\\mathbf W$ where the learned attention weights can be more informative by explicitly \noptimizing the IB loss for visual transformers. Our model termed ``DCS-MobileViT-S'' in Table 5 is the visual transformer with the IB loss optimized, \nso that more informative attention weights are learned featuring even smaller IB loss and even higher top-1 accuracy."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6878/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718358902,
                "cdate": 1700718358902,
                "tmdate": 1700718358902,
                "mdate": 1700718358902,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1I82xN2ES0",
                "forum": "2RGQwJEcAC",
                "replyto": "lgL1HgzylQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6878/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6878/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JKXw Part 2"
                    },
                    "comment": {
                        "value": "(2) **DCS-Transformer for ViT and Swin, and DCS-Transformer for the Task of Segmentation**\n\nThe proposed DCS-Transformer module can be straightforwardly applied to a broader class of visual transformers beyond the examples covered in this paper, including \nthe vanilla ViT [1] and Swin [2]. This is because the two components of a DCS-Transformer, channel selection for attention weights and \nchannel selection for attention outputs can be directly applied to ViT and Swin. Here we replace all the transformer blocks in ViT-S/16 [1] and Swin-T [2], obtaining DCS-ViT-S/16 and DCS-Swin-T respectively.\nWe use the same settings for search and training as described in Section 4.1 of our paper. \nAll the models are trained on the training set of ImageNet-1K and tested on the validation set of ImageNet-1K. The results are shown in the Table below. \n\n\n|    Models    | # Params | FLOPs | Top-1 |\n| :----------: | :------: | :---: | :---: |\n| ViT-S/16 [1] |  22.1 M  | 9.2 G | 81.2  |\n| DCS-ViT-S/16 |  20.2 M  | 8.3 G | **82.8**  |\n|  Swin-T [2]  |  29.0 M  | 4.5 G | 80.6  |\n|  DCS-Swin-T  |  26.1 M  | 4.0 G | **82.1**  |\n\n\nIn Section E of the supplementary of the revised paper, we present the results of DCS-EfficientViT-B1 for the task of instance segmentation on the COCO dataset\nunder the same setting as [3]. Please refer to Section E for more details. The results in Table 7 of the revised paper are copied to the table below for your convenience. \nWe report the mean bounding box Average Precision (APb) and mean mask Average Precision (APm) as well as bounding box Average Precision (APb) \nand mask Average Precision (APm) under IoU thresholds of 0.5 and 0.75. It can be observed that DCS-EfficientViT-B1 consistently improves the performance of segmentation across various thresholds.\n\n\n| Methods           | $AP^{box}$ | $AP^{box}_{50}$ | $AP^{box}_{75}$ | $AP^{mask}$ | $AP^{mask}_{50}$ | $AP^{mask}_{75}$ |\n| :---------------: | :----: | :----: | :----: | :----: |:----:|:----:|\n| EViT[3]               | 32.8   | 54.4 | 34.5 | 31.0    | 51.2 | 32.2 |\n| EfficientViT-B1 [4]     | 33.5   | 55.4 | 34.8 | 31.9    | 52.3 | 32.7 |\n| DCS-EfficientViT-B1 | 34.8   | 56.3 | 35.3 | 33.2    | 53.1 | 33.3 |\n\n\n\n\n\n\n(3) **Comparison with Pruning Methods**\n\nWe compare DCS-Transformer to the current state-of-the-art pruning methods for visual transformers, the results are shown in the table below. DCS-Transformer is compared to S$^2$ViTE[2], SPViT[3], and SAViT [4] on EfficientViT-B1 (r224) [1]. \nFor S$^2$ViTE[2], SPViT[3] and SAViT [4], the pruning is performed on the ImageNet training data. After obtaining the pruned networks, we fine-tune the pruned networks using the same setting as [1]. \nIt can be observed that DCS-EfficientViT-B1 outperforms all pruned models by at least 1.9% in top-1 accuracy with even less parameters and FLOPs.\n\n\nWe also include actual inference time of all the model on a NVIDIA V100 GPU.\n\n\n| Methods                          | \\# Params | FLOPs   | Inference Time (ms/image) | Top-1 |\n| :------------------------------: | :-------: | :-----: | :---: | :---: |\n| EfficientViT-B1 (r224) [4]       | 9.1 M    | 0.52 G | 2.654 | 79.4 |\n|S$^2$ViTE-EfficientViT-B1 (r224) [5] | 8.2 M    | 0.47 G | 2.438 | 79.0 |\n| SPViT-EfficientViT-B1 (r224) [6]  | 9.2 M    | 0.49 G | 2.451 | 79.3 |\n| SAViT-EfficientViT-B1 (r224) [7] | 8.4 M    | 0.47 G | 2.435 | 79.2 |\n| DCS-EfficientViT-B1 (r224)       | 8.2 M    | 0.46 G | 2.427 | 81.2 |\n\n\n**References**\n\n[1] Dosovitskiy et al. \"An image is worth 16x16 words: Transformers for image recognition at scale.\" ICLR 2021.\n\n[2] Liu et al. \"Swin transformer: Hierarchical vision transformer using shifted windows.\" ICCV, 2021.\n\n[3] Liu et al. \"EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention.\" CVPR 2023.\n\n[4] Cai et al. \"Efficientvit: Enhanced linear attention for high-resolution low-computation visual recognition.\" ICCV 2023.\n\n\n[5] Chen, Tianlong, et al. \"Chasing sparsity in vision transformers: An end-to-end exploration.\" NeurIPS 2021.\n\n[6] Kong, Zhenglun, et al. \"SPViT: Enabling faster vision transformers via soft token pruning.\" ECCV 2022.\n\n[7] Zheng, Chuanyang, et al. \"SAViT: Structure-Aware Vision Transformer Pruning via Collaborative Optimization.\" NeurIPS 2022."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6878/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718660924,
                "cdate": 1700718660924,
                "tmdate": 1700724109325,
                "mdate": 1700724109325,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]