[
    {
        "title": "Generalizing Denoising to Non-Equilibrium Structures Improves Equivariant Force Fields"
    },
    {
        "review": {
            "id": "FdP0Pj0lpw",
            "forum": "X7gqOBG8ow",
            "replyto": "X7gqOBG8ow",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission487/Reviewer_NSpY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission487/Reviewer_NSpY"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new auxiliary task to improve the training of networks on the prediction of the energy and forces from the structure of an atomistic system. It proposes to consider the recovery of the 3d structure after perturbing it with Gaussian noise (similar to a denoising autoencoder). Yet, since this plain (structure-to-structure) denoising problem is only well-posed for equilibrium structures, the authors propose to consider the forces of the original structure as an additional input along with a corrupted non-equilibrium structure, which seems to make the denoising problem well-posed again. The resulting ability to also consider  non-equilibrium structures significantly increases the available amount of training data and results in significant and systematic numerical improvements on several data sets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "I am not familiar with machine learning on atomistic systems, such that my evaluation has to be treated with some care. Yet, \n- The idea seems to be novel, very well motivated by language processing and computer vision, and nicely resolves the ambiguity of denoising non-equilibrium structures with the help of additional input. \n- The numerical results (on huge and apparently very competitive datasets) are very promising as they are largely able to improve the state-of-the-art. \n- Ablation studies over hyper-parameters, architectural choices, and loss functions indicate a very well-designed method."
                },
                "weaknesses": {
                    "value": "The presentation of the work could be improved significantly. While part of my difficulty in understanding the presentation is surely due to my lack of knowledge in the particular field, I believe some aspects hold for scientific texts in general \n- Abbreviations should not be used before they are introduced. The abstract already refers to \"S2EF\" and \"IS2RE results\" (clarified in the numerical results), refers to network architectures without citation and even uses variables ($L_\\max = 2$ and $L_\\max=3$) whose meaning is assumed to be known.\n- It is not explained why forces make the denoising problem on non-equilibrium structures well-posed and also not how the forces are obtained. Is it correct to assume that the potential of any structure can be computed and that the forces are the gradient thereof? If so, it would be just two additional sentences of explanation that make the work much more clear. \n- The property \"equivariant\" is frequently used. While I know what it means, it was unclear to me what kind of equivariance (with respect to which transformations) is meant and why that influences the ability to encode forces. Rotations and translations of 3d coordinates/vectors? \n- I do not know the term \"type-L vectors\" - what does it mean? I tried googling but did not have direct success. Thus, I'd recommend defining it. Also, what is \"the projection of $f$ into type-L vectors with spherical harmonics\"? Representing the function f with L-many coefficients corresponding to spherical harmonics basis functions?\n- It would have helped me to cite something for \"SO(3) linear layer\". \n- Please double-check the manuscript for typos. \n\nPlease be aware that I am not part of your main audience. Thus, feel free to adjust the writing for those aspects where you believe your main audience would also agree.\n\nFrom my (quite uncertain) point of view, the strengths seem to clearly outweigh the suboptimal presentation (particularly if the latter is well suited for a more expert audience)."
                },
                "questions": {
                    "value": "In addition to some small things raised above, I have two further questions:\n- It was strange to me to decide for a cost function for each batch during training with some probability. Will this not be equal to a weighted linear combination between the two terms in expectation?\n- The ablation in Table 1 (b) indicates that $p_{DeNS}=0.5$ is the best, but also the largest tested value. Wouldn't it make sense to ablate values $>0.5$?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission487/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698521002719,
            "cdate": 1698521002719,
            "tmdate": 1699635975454,
            "mdate": 1699635975454,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vPFylEQvHw",
                "forum": "X7gqOBG8ow",
                "replyto": "FdP0Pj0lpw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission487/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission487/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NSpY (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for helpful feedback and address the comments below.\n\n---\n\n> 1. [Weakness 1] Abbreviations should not be used before they are introduced. The abstract already refers to \"S2EF\" and \"IS2RE results\" (clarified in the numerical results), refers to network architectures without citation and even uses variables (L_{max} = 2 and L_{max} = 3) whose meaning is assumed to be known.\n\nThanks for pointing these out! We have updated the abstract.\n\n---\n\n> 2. [Weakness 2] It is not explained why forces make the denoising problem on non-equilibrium structures well-posed and also not how the forces are obtained. Is it correct to assume that the potential of any structure can be computed and that the forces are the gradient thereof? If so, it would be just two additional sentences of explanation that make the work much more clear.\n\nThe ground-truth force labels are obtained from DFT / MD calculations depending on the dataset and are from the training set.\n\nFor predicting forces, as mentioned in Section 3.1, we either take the gradient of potential energy or directly predict them from node embeddings.\n\nRestating from the introduction and Section 3.2.2, denoising non-equilibrium structures without encoding forces as input is ill-posed because there are many possible target structures to denoise to. This is unlike the case for equilibrium structures which are at local minima, and so there exists a unique target structure to denoise to given perturbed structures. Encoding atomic forces in DeNS helps identify the unique non-equilibrium structure to denoise to. This is also illustrated in Figure 1. More specifically, since we train GNNs with $S_{\\text{non-eq}}'$ and $F(S_{\\text{non-eq}})$ as inputs and $\\text{Noise}(S_{\\text{non-eq}}, S_{\\text{non-eq}}')$ as outputs, they implicitly learn to leverage $F(S_{\\text{non-eq}})$ to reconstruct $S_{\\text{non-eq}}$ instead of predicting any arbitrary non-equilibrium structures. (Here we use $S_{\\text{non-eq}}'$ to denote corrupted non-equilibrium structures.)\n\n---\n\n> 3. [Weakness 3] While I know what it means, it was unclear to me what kind of equivariance (with respect to which transformations) is meant and why that influences the ability to encode forces. Rotations and translations of 3d coordinates/vectors?\n\nYes, the equivariance in this work refers to equivariance to 3D rotation and invariance to 3D translation, which is commonly implied in the context of learning force fields. Besides, we also discuss some equivariant networks and provide pointers to detailed background in Section A.1.\n\nAs mentioned in Section 3.2.2, the node embeddings of equivariant networks consist of different type-$L$ vectors, where $L$ denotes degree, and forces belong to type-1 vectors. Therefore, to encode forces, we can simply add forces to the part of type-1 vectors in node embeddings. \n\nPlease let us know if we can help clarify anything else. \n\n---\n\n> 4. [Weakness 4] What are type-$L$ vectors? What is \"the projection of f into type-$L$ vectors with spherical harmonics\"? \n\nType-$L$ vectors are (2$L$+1)-dimensional vectors that rotate $L$ times faster than 3D Euclidean vectors when we rotate the 3D coordinates. They can be obtained by taking tensor products of 3D relative positions (type-1 vectors). \n\nThe projection of $f$ (type-1 vector) into type-$L$ vectors with spherical harmonics means that we use spherical harmonics $Y^{(L)}$ to transform type-1 vectors into type-$L$ vectors so that we can consider higher frequency (higher $L$).\n\nWe note that the details can be found in [1] as mentioned in Section A.1.  \n\nReference:  \n[1] Liao et al. Equiformer: Equivariant Graph Attention Transformer for 3D Atomistic Graphs. ICLR 2023.\n\n\n---\n\n> 5.  [Weakness 5] Cite SO(3) linear layers. \n\nThanks! We will add the citation of e3nn [1] after SO(3) linear layers. \n\nReference:  \n[1] Geiger et al. e3nn: Euclidean Neural Networks. ArXiv 2022.\n\n\n---\n\n> 6. [Weakness 6] Double check the manuscript for typos. \n\nThanks! We will fix them.\n\n---\n\n> 7. [Question 1] It was strange to me to decide for a cost function for each batch during training with some probability. Will this not be equal to a weighted linear combination between the two terms in expectation?\n\nYes, that should be equal to a weighted linear combination in expectation. However, the input structures corresponding to the two different loss functions (Equation 1 and Equation 6) are different. For Equation 1, we take the original structures as inputs while for Equation 6, we add noise and then take the corrupted structures as inputs. Therefore, we need to decide whether to add noise, and this introduces some probability and affects the subsequent loss function. We provide the pseudocode in Section E."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission487/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734066717,
                "cdate": 1700734066717,
                "tmdate": 1700734066717,
                "mdate": 1700734066717,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Rt1hWDoj1e",
            "forum": "X7gqOBG8ow",
            "replyto": "X7gqOBG8ow",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission487/Reviewer_erXC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission487/Reviewer_erXC"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method called denoising non-equilibrium structures (DeNS), which generalizes to a larger set of non-equilibrium structures without relying on additional datasets for pre-training, and the effectiveness of DeNS is demonstrated on the OC20, OC22, and MD17 datasets, achieving better results and faster training times compared to existing methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The inverse denoising framework presented in this work is quite interesting, as it proposes a novel possibility of combining structures, forces, and energy (or other properties) in a dual or inverse setting.\n2. The experiments and ablation study conducted are robust and extensive, accompanied by meticulous analysis.\n3. Well-written and easy to understand."
                },
                "weaknesses": {
                    "value": "1. The results show limited improvements in the OC20, OC22, and MD17 datasets.\n2. Maybe could add more results from the denoising framework with other backbones to provide a comprehensive understanding of its performance.\n3. I believe that developing a general AI-based molecular dynamics (MD) method is more crucial than specifically designing and tuning for the OC dataset. A more generalized approach could be beneficial to the community by focusing on the broader application of deep learning-based MD methods across different systems, such as drug molecules, crsytal materials or polymers."
                },
                "questions": {
                    "value": "1. If the improvements are not solely due to data augmentation, but rather are attributed to the inverse or dual denoising setting, it would be valuable to explore the generalization ability of the S2EF system in other systems.\n2. The tunable standard deviation (\u03c3) denoising strategy appears to be crucial, it might be beneficial to incorporate visualizations of \u03c3 during the training process for better illustration."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission487/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission487/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission487/Reviewer_erXC"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission487/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698668622474,
            "cdate": 1698668622474,
            "tmdate": 1699635975391,
            "mdate": 1699635975391,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KNB4lwFaWH",
                "forum": "X7gqOBG8ow",
                "replyto": "Rt1hWDoj1e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission487/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission487/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to  Reviewer erXC (1/1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for helpful feedback and address the comments below.\n\n---\n\n> 1. [Weakness 1] The results show limited improvements in the OC20, OC22, and MD17 datasets.\n\nPlease see **General Response 3** (MD17), **General Response 5** (OC20) and **General Response 6** (OC22) for discussions on the performance gain.\n\n---\n\n> 2. [Weakness 2] Maybe could add more results from the denoising framework with other backbones to provide a comprehensive understanding of its performance.\n\nWe evaluate EquiformerV2 [1] and eSCN [2] on OC20 S2EF-2M dataset, Equiformer [3] and SEGNN [4] on MD17 dataset, and EquiformerV2 on OC22 dataset. These architectures improve the SoTA on each dataset. Of course more can always be done, but we believe 4 architectures spanning 3 datasets provides a sufficiently general evaluation of the proposed denoising approach. As a point of comparison, prior works cover less breadth in architectures \u2013 Noisy Nodes [5] only uses GNS [6], pre-training via denoising [7] uses GNS [6], GNS-TAT and TorchMD-NET [8], and fractional denoising [9] only conducts experiments with TorchMD-NET [8].\n\nReference:  \n[1] Liao et al. EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations. ArXiv 2023.  \n[2] Passaro et al. Reducing SO(3) Convolutions to SO(2) for Efficient Equivariant GNNs. ICML 2023.  \n[3] Liao et al. Equiformer: Equivariant Graph Attention Transformer for 3D Atomistic Graphs. ICLR 2023.   \n[4] Brandstetter et al. Geometric and Physical Quantities Improve E(3) Equivariant Message Passing. ICLR 2022.  \n[5] Godwin et al. Simple GNN Regularisation for 3D Molecular Property Prediction and Beyond. ICLR 2022.   \n[6] Sanchez-Gonzalez et al. Learning to simulate complex physics with graph networks. ICML 2020.  \n[7] Zaidi et al. Pre-training via Denoising for Molecular Property Prediction. ICLR 2023.   \n[8] Th\u00f6lke et al. TorchMD-NET: Equivariant Transformers for Neural Network based Molecular Potentials. ICLR 2022.  \n[9] Feng et al. Fractional Denoising for 3D Molecular Pre-training. ICML 2023.   \n\n---\n\n> 3. [Weakness 3] I believe that developing a general AI-based molecular dynamics (MD) method is more crucial than specifically designing and tuning for the OC dataset. A more generalized approach could be beneficial to the community by focusing on the broader application of deep learning-based MD methods across different systems, such as drug molecules, crystal materials or polymers.\n\nFirst, we show that DeNS improves performance on MD17 (see **General Response 3** and Table 4 in the paper).\n\nSecond, we do not incorporate any constraints related to catalysis into DeNS, and therefore DeNS can be applied to all non-equilibrium structures, including those in MD simulations. From that perspective, DeNS is indeed generally applicable. Moreover, as mentioned in Section D.1 and D.2 (partial denoising, decaying the DeNS coefficient), DeNS can be specifically tuned for MD to further improve performance.\n\nFinally, now that we have a general method that seems to work well, we agree that pushing further on MD simulations across a broader range of chemistries (drug molecules, crystal materials, polymers) is the logical next step for future work! \n\n---\n\n> 4. [Question 1] If the improvements are not solely due to data augmentation, but rather are attributed to the inverse or dual denoising setting, it would be valuable to explore the generalization ability of the S2EF system in other systems.\n\nThe work of Noisy Nodes [1] has explored this. Comparing the first two rows in  Table 5 in [1], they find that simply adding data augmentation without denoising results in small performance gain, suggesting denoising is necessary. Besides, in Figure 3(B) and 3(C) in [1], they also show that adding noise as data augmentation results in strong regularization.\n\nOur validation and testing S2EF evaluations on OC20 and OC22 already test for generalizability since some of the splits involve out-of-distribution catalysts or adsorbates not seen during training. \n\nReference:  \n[1] Godwin et al. Simple GNN Regularisation for 3D Molecular Property Prediction and Beyond. ICLR 2022. https://arxiv.org/abs/2106.07971  \n\n---\n\n> 5. [Question 2] The tunable standard deviation (\u03c3) denoising strategy appears to be crucial, and it might be beneficial to incorporate visualizations of \u03c3 during the training process for better illustration.\n\nThanks for the suggestion! We visualize how structures in OC20, OC22 and MD17 datasets become after adding noise of different scales in Section F in the appendix of the revision."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission487/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733225951,
                "cdate": 1700733225951,
                "tmdate": 1700733225951,
                "mdate": 1700733225951,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NRW41oH65n",
            "forum": "X7gqOBG8ow",
            "replyto": "X7gqOBG8ow",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission487/Reviewer_AVUJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission487/Reviewer_AVUJ"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an auxiliary task (not a pre-training) to help learn molecular tasks: denoising not only equilibrium (few are available) structures, but also denoising the not-yet-at-mechanical-equilibrium structures (much more numerous).\n\nThe key new idea is to provide also the forces (of the non-corrupted input) together with the corrupted structures, to make the problem well posed.\n\nRather heavy experiments show that the proposed method can sometimes beat the state of the art."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality:\n\nThe idea of the auxiliary task applied to non-equilibrium structures is great, here the paper allows to make this task well-defined by feeding the (uncorrupted) forces as input.\n\n\nClarity:\n\nThe paper explains carefully how the auxiliary task is added to the \"default\" model on which one wants to work.\nI like a lot figure 1, which explains very clearly and concisely the idea.\n\n\n\nSignificance:\n\nI think the paper makes the point for releasing more non-equilibrium structures (although, this is already the case in OC20 and OC22 if I understood well), which is important to state clearly to the community.  As authors say:\n\n> We hope that the ability to leverage more from non-equilibrium structures as proposed in this work can encourage researchers to release data containing intermediate non-equilibrium structures in addition to final equilibrium ones."
                },
                "weaknesses": {
                    "value": "Originality:\n\nThe original idea is not groundbreaking: auxiliary tasks are known, the specific case of denoising as well, here the novelty is only in feeding also the forces as input.\n\nQuality:\n\nit is not always clear from the results shown in tables, that the proposed method improves the SOTA significantly.\nAlso, since there are two measures of success (energy and forces), it's sometimes difficult to make a final decision.\n\nClarity:\n\nThe paper has some typos, but most importantly, experiments are discussed very quickly (too quickly). More space should be devoted to discuss results. For instance table 4 shows a nice improvement for DeNS on OOD splits (i.e. better generalization if I understand well that OOD is Out of Distribution as opposed to ID=In Distribution).\n\nSignificance:\n\nSince the results are not strikingly better when using DeNS on the SOTA, and given it involves a number of additional hyperparameters (that obviously do not need very narrow fine-tuning, admitedly, but still, they involve more work and potential for problems), it is not clear yet whether the contribution would be used widely.\n\nTable 1d is probably the most convincing result, to me (differences between models seem more significant). Is it computed on the validation set(s) ? (or train set ? I have a doubt).\n\nIn summary, this is red AI and the results are not significantly better than SOTA, thus not convincing for publication."
                },
                "questions": {
                    "value": "I note that:\n- trainings are very heavy.\n- there is some hyper-param tuning, and DeNS is used on top of the best models (equiformerV2).\n- energy and force: sometimes only one is better, but as you say, it's a balance.\n- All that considered, most of the differences reported are not very significant. Maybe you could outline in green when a metric is significantly better in one model than in others (when this happens).\nCan you answer and/or improve the discussions (and/or presentation of results) in the Experiments section, to show that indeed, the improvement is significant ?\n\nIf some tables show no significant improvement, it should be explained why.\n\nIf the method is mostly able to speed up training to achieve equivalent accuracy, state it (and it will weaken the paper's claim, but at the same time strengthen the submission).\n\n\n\nNote:\nTable 2 does not show a significant improvement from using DeNS\n\n\n\n\ntypos:\ndevication -> deviation\n\"L2 different\" -> \"L2 difference / L2 norm / squared difference \""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Potentially harmful insights, methodologies and applications"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Methodology:\n\nBad ecologial impact of this red AI-style research: for instance one of the experiment reports 3495hours of GPU of training time.\n\n(Table I: The training time is in GPU-hours and measured on V100 GPUs.)\n\nAgain in Table 3: 3446 hours of GPU-time for training.\n\nOverall, this research aims at shortening training time by using a smart data-augmentation strategy. But the results are very moderate, for an investment cost that is quite consequent.\n\nBut, at least, they do report these training times !"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission487/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698705587953,
            "cdate": 1698705587953,
            "tmdate": 1699635975286,
            "mdate": 1699635975286,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OmbMEf8hWy",
                "forum": "X7gqOBG8ow",
                "replyto": "NRW41oH65n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission487/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission487/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer AVUJ (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for helpful feedback and address the comments below.\n\n---\n\n> 1. [Significance] I think the paper makes the point for releasing more non-equilibrium structures (although, this is already the case in OC20 and OC22 if I understood well), which is important to state clearly to the community.\n\nThanks for recognizing this! Indeed, single-point DFT calculations on non-equilibrium structures are cheaper to calculate than full DFT relaxations. Many publicly available datasets (e.g. PCQM4Mv2 [1]) only release equilibrium structures while those structures are obtained by relaxing intermediate non-equilibrium structures. DeNS training can further improve performance and will benefit from the release of more non-equilibrium structure datasets.\n\nReference:  \n[1] Nakata et al. PubChemQC Project: A Large-Scale First-Principles Electronic Structure Database for Data-Driven Chemistry. Journal of Chemical Information and Modeling 2017.\n\n---\n\n> 2. [Weakness \u2013 Originality] The original idea is not groundbreaking: auxiliary tasks are known, the specific case of denoising as well, here the novelty is only in feeding also the forces as input.\n\nWe compare the contributions of previous works and this work in **General Response 2**. In summary, the main contribution is that we generalize denoising to all atomistic datasets, including both non-equilibrium and equilibrium structures, with a simple modification of force encoding. All prior works on denoising do not encode forces, which is an ablation of our proposed approach and performs worse (Table 1(e) on OC20 and **General Response 4** on MD17).\n\n---\n\n> 3. [Weakness \u2013 Quality] it is not always clear from the results shown in tables, that the proposed method improves the SOTA significantly. \n\nTraining with DeNS improves state-of-the-art models on all the datasets we consider. The improvements are clear on MD17 (Table 5 and **General Response 3**), OC20 S2EF-2M (Table 1(d) and **General Response 5**) and OC20 S2EF-All+MD datasets (Table 2 and **General Response 5**). The performance gain is sometimes less on OC22 (Table 4 and **General Response 6**). The main point is that with this simple force encoding, we can generalize denoising to the broader set of non-equilibrium structures and improve all the state-of-the-art models. \n\n---\n\n> 4. [Weakness \u2013 Quality] Since there are two measures of success (energy and forces), it's sometimes difficult to make a final decision.\n\nPlease see **General Response 8**. \n\n---\n\n> 5. [Weakness \u2013 Clarity] The paper has some typos.\n\nThanks! We will fix them. \n\n---\n\n> 6. [Weakness \u2013 Clarity] More space should be devoted to discussing results.\n\nWe move the results of different hyper-parameters on OC22 (Table 4) to the appendix and incorporate some discussions on the results in **General Response 3, 5, 6** into the manuscript. \n\n---\n\n> 7. [Weakness \u2013 Significance] Since the results are not strikingly better when using DeNS on the SOTA, and given it involves a number of additional hyperparameters (that obviously do not need very narrow fine-tuning, admittedly, but still, they involve more work and potential for problems), it is not clear yet whether the contribution would be used widely.\n\nWe disagree on results not being substantially better than SoTA. To restate, please see **General Response 3** (MD17), **General Response 5** (OC20) and **General Response 6** (OC22).\n\nAny new method involves at least some additional hyper-parameters [1, 2, 3, 4]. The hyperparameters for DeNS are easily transferable across architectures without much tuning. Specifically, when training eSCN with DeNS onOC20 S2EF-2M (Table 1(d)), we use the same hyper-parameters as training EquiformerV2, and that achieves better performance, matching the results of EquiformerV2 without DeNS and saving 1.94$\\times$ training time without any hyper-parameter tuning. Similarly, for MD17 in Table 5, we directly use the same hyper-parameters tuned for Equiformer and training with DeNS improves two architecture variants on all the molecules. Besides, some hyper-parameters such as $p_{\\text{DeNS}}$ and $\\sigma_{\\text{high}}$ also apply across datasets like OC20 and OC22, and this makes tuning hyper-parameters easier. \n\nReference:  \n[1] Wang et al. Denoise pretraining on nonequilibrium molecules for accurate and transferable neural potentials. Journal of Chemical Theory and Computation, 2023.   \n[2] Godwin et al. Simple GNN Regularisation for 3D Molecular Property Prediction and Beyond. ICLR 2022.  \n[3] Zaidi et al. Pre-training via Denoising for Molecular Property Prediction. ICLR 2023.  \n[4] Feng et al. Fractional Denoising for 3D Molecular Pre-training. ICML 2023."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission487/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732192803,
                "cdate": 1700732192803,
                "tmdate": 1700732192803,
                "mdate": 1700732192803,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5aw98hLZEI",
            "forum": "X7gqOBG8ow",
            "replyto": "X7gqOBG8ow",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission487/Reviewer_tmjX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission487/Reviewer_tmjX"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors proposed a denoising non-equilibrium structures (DeNS) training strategy to improve force field learning. Different from previous denoising approaches limited to equilibrium structures, DeNS enables the utilization of non-equilibrium structures for the denoising task by encoding the corresponding non-zero forces for specifying the denoising targets. Extensive experiments are conducted to demonstrate the effectiveness of DeNS."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The target problem is of interest to the machine learning force field community. \n- The proposed approach to encode the forces for specifying target structures when denoising perturbed non-equilibrium structures is a new modification compared to previous methods like noisy-node.\n- The experimental evaluation covers both small and large-scale benchmarks. The ablation studies on the introduced hyperparameters are informative for practitioners to try the proposed DeNS approach in their applications."
                },
                "weaknesses": {
                    "value": "- **Regarding the performance improvement** . Serving as a simple modification upon previous denoising training strategy on molecular modeling, performance improvement is the most important aspect to measure the quality of this work. However, there exist several issues that need to be further clarified:\n  - The gains brought by DeNS diminish with the dataset scales up. From Table 1 and Table 2, we can see that models trained on the 2M subset of the OC20 S2EF dataset benefit a lot more from the DeNS auxiliary task compared to the OC20 S2EF-All+MD split.\n  - The improvement on MD17 is limited compared to the OC series experiments.\n  - The improvement on the IS2RE task is also limited (sometimes DeNS even hurts the performance).\n  - The gains on the energy and force metrics are not consistent across different datasets with different scales. In the OC20 tasks, models trained with the DeNS task achieve lower force errors and competitive energy errors compared to the standard training and vice versa for the OC22 tasks.\n\nThe first issue relates to the scaling property of the proposed training strategy. I recommend the authors further design experiments to investigate whether such a phenomenon is due to the inability of the proposed DeNS to bring performance gains when large-scale data is provided or other factors of the model, optimization, and so on. The second issue relates to the generality of the proposed strategy. MD17 contains simple molecules instead of adsorbate-catalyst complex in OC20/22. This dataset is much smaller than OC20/22 dataset, on which the proposed DeNS is expected to bring more gains according to the phenomenon mentioned in the first issue. However, the results in Table 5 and 6 show the gains are limited. It is suggested to further verify the generality of DeNS. The third issue relates to the significance of the force field learning task. In OC and other similar applications, what we really care about is to obtain the property of the equilibrium states like relaxed energy and structure. The error of force field model is an indirect metric. In this sense, the significance of improvement brought by DeNS on energy and force error of S2EF task should be reexamined based on the IS2RE performance.\n\nOverall, I recommend the authors to carefully clarify the proposed issues above with further experimental evidence to make some aspects of the proposed DeNS training strategy more clear for readers. I would like to increase my scores if the authors could address my concerns in this section and questions in the next section."
                },
                "questions": {
                    "value": "1. In each iteration, the model uses either the standard training or the DeNS training. As DeNS training requires the forces to be encoded into the input atom features, I wonder how the force encoding module would be used in the standard training which instead uses the forces as labels.\n\n2. In Table 5, the authors demonstrate that the DeNS training is more efficient and results in larger performance gains than increasing max degrees of irreps. I wonder why the authors changed the model from EquiformerV2 to EquiformerV1 for this investigation. After all, the EquiformerV2 model is claimed to largely benefit from scaling the max degrees of irreps. How would EquiformerV2 with different max degrees of irreps behave in the same setting of Table 5?\n\n3. Could you provide more discussion on why you chose Equation 7 for the multi-scale noise scheduler?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission487/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827045152,
            "cdate": 1698827045152,
            "tmdate": 1699635975188,
            "mdate": 1699635975188,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7W92KqW406",
                "forum": "X7gqOBG8ow",
                "replyto": "5aw98hLZEI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission487/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission487/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tmjX (1/1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for helpful feedback and address the comments below.\n\n---\n> 1. [Weakness 1] The gains brought by DeNS diminish with the dataset scales up. From Table 1 and Table 2, we can see that models trained on the 2M subset of the OC20 S2EF dataset benefit a lot more from the DeNS auxiliary task compared to the OC20 S2EF-All+MD split.\n\nWe update the results of training on the OC20 All+MD dataset (**General Response 7**) and discuss the performance gain on OC20 in **General Response 5**. \n\n---\n\n> 2. [Weakness 2] The improvement on MD17 is limited compared to the OC series experiments.\n\nPlease see **General Response 3**. \n\n---\n\n> 3. [Weakness 3] The improvement on the IS2RE task is also limited (sometimes DeNS even hurts the performance).\n\nWith the updated OC20 All+MD results, EquiformerV2 with DeNS is better on all validation and testing S2EF splits, and about the same on IS2RE, compared to EquiformerV2 without DeNS. Consistent with findings in prior works, where eSCN achieves lower energy and force MAE but higher IS2RE MAE than SCN, we find that improvements on S2EF don\u2019t always translate to improvements on IS2RE. We posit that IS2RE performance might be bottlenecked by other design choices when running structural relaxations (optimizer settings, stopping criterion, etc.) and not S2EF performance. We will look into this in future work. \n\nFor OC22, please see **General Response 6**.\n\n---\n\n> 4.  [Weakness 4] The gains on the energy and force metrics are not consistent across different datasets with different scales. In the OC20 tasks, models trained with the DeNS task achieve lower force errors and competitive energy errors compared to the standard training and vice versa for the OC22 tasks.\n\nPlease see **General Response 8**.\n\n---\n\n> 5. [Question 1] In each iteration, the model uses either the standard training or the DeNS training. As DeNS training requires the forces to be encoded into the input atom features, I wonder how the force encoding module would be used in the standard training which instead uses the forces as labels.\n\nDuring standard training, we do not feed in the forces. Specifically, the force encoding module is an SO(3) linear layer, which is fed a tensor filled with all zeros during standard training. Force encoding is only used during DeNS training. We do not use any force labels in the validation and testing sets.\n\n---\n\n> 6. [Question 2] In Table 5, the authors demonstrate that the DeNS training is more efficient and results in larger performance gains than increasing max degrees of irreps. I wonder why the authors changed the model from EquiformerV2 to EquiformerV1 for this investigation. After all, the EquiformerV2 model is claimed to largely benefit from scaling the max degrees of irreps. How would EquiformerV2 with different max degrees of irreps behave in the same setting of Table 5?\n\nWe use Equiformer(V1) simply because EquiformerV2 [1] does not report results on MD17 and that makes the comparison difficult.\n\nThe benefits of using higher degrees depend on datasets. As we can see in Table 5, on MD17, increasing $L_{max}$ from 2 to 3 in Equiformer(V1) leads to small gains in energy and force MAE. Therefore, this makes increasing $L_{max}$ further and using EquiformerV2 less well-motivated. It is likely that for this small dataset, using higher $L_{max}$ just leads to margin gains. However, training with DeNS achieves better energy and force errors across all the molecules and improves training efficiency by 3.1$\\times$ compared to increasing $L_{max}$ from 2 to 3.\n\n[1] Liao et al. EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations. ArXiv 2023.\n\n---\n\n> 7. [Question 3] Could you provide more discussion on why you chose Equation 7 for the multi-scale noise scheduler?\n\nThis was an empirical choice. We first experimented with a single noise scale (Index 4 in Table 1(e)) and then a multi-scale schedule similar to prior work in denoising score matching [1], which we found to work significantly better. We believe that a multi-scale schedule is more likely to span the distribution of meaningful non-equilibrium structures across a diverse range of atom types and geometries compared to a fixed $\\sigma$. We add this intuition to the revision.\n\n[1] Song et al. Generative modeling by estimating gradients of the data distribution. NeurIPS 2019."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission487/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731525668,
                "cdate": 1700731525668,
                "tmdate": 1700731525668,
                "mdate": 1700731525668,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Fy3AdK6RDA",
            "forum": "X7gqOBG8ow",
            "replyto": "X7gqOBG8ow",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission487/Reviewer_eCvt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission487/Reviewer_eCvt"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces DeNS, an approach to improve energy and force predictions with the aid of non-equilibrium structures' denoising as an auxiliary task. Its implementation feeds forces from original structures as inputs, contributing to a well-structured problem. Demonstrated results indicate minor improvements on EquiformerV2 for datasets like OC20, OC22, and MD17."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-The paper addresses an essential challenge: the development of a self-supervised learning methodology using non-equilibrium molecules.\n\n-The proposal offers a unique perspective on non-equilibrium denoising by discussing the ill-posed mapping. The handling of input encoding using forces seems logically feasible.\n\n-The documentation is unambiguous and comprehensible."
                },
                "weaknesses": {
                    "value": "-The paper could benefit from a broader theoretical discussion and a comparative analysis with other self-supervised techniques for non-equilibrium structures, such as denoising pretraining in [1], Noisy Nodes (using the OC20 dataset)[2], and improved noisy nodes (using MD17)[3]. These techniques have demonstrated efficacy for energy or force predictions for non-equilibrium molecules, hence their significance.\n\n-The motivation behind the paper's approach needs additional validation.\na) The concept of \u201cencoding force as input\u201d finds extensive discussion in the paper. However, this approach needs corroborative proof from experiments. Results from Table 1e indicate energy prediction outcomes remain the same without force encoding. The possibility of label leakage and its contribution to improvement in force prediction, as discussed under question 2, needs examination.\nb) Similar problem-solving approaches have been published. A comparative discussion highlighting the distinctions and superiority of this paper's proposed methodology would prove advantageous.\n\n-The significant results were, to a large extent, achieved through Equiformer. Against Equiformer's backdrop, the improvements contributed by DeNS appear minimal.\n\n[1] Yuyang Wang, Chang Xu, Zijie Li, and Amir Barati Farimani. Denoise pretraining on nonequilibrium molecules for accurate and transferable neural potentials. Journal of Chemical Theory and Computation, 2023. \n[2] Feng, S., Ni, Y., Lan, Y., Ma, Z. &amp; Ma, W.. (2023). Fractional Denoising for 3D Molecular Pre-training. Proceedings of the 40th International Conference on Machine Learning, in Proceedings of Machine Learning Research 202:9938-9961 Available from https://proceedings.mlr.press/v202/feng23c.html.\n[3] Jonathan Godwin, Michael Schaarschmidt, Alexander L Gaunt, Alvaro Sanchez-Gonzalez, Yulia Rubanova, Petar Velickovi \u02c7 c, James Kirkpatrick, and Peter Battaglia. Simple GNN regularisation for 3d molecular property prediction and beyond. In International Conference on Learning Representations, 2022."
                },
                "questions": {
                    "value": "-While encoding input forces can mitigate the non-equilibrium denoising's ill-posed problem, [1] shows that denoising without force input is also plausible. Does this undermine your motivation and imply the redundancy of input force encoding?\n-Could there be label leakage when encoding forces as input for energy and force predictions of structures?\n-Given that force prediction is one of your experiments, should you consider adding the force prediction loss to eq. (6)?\n-Provision of the pseudocode for DeNS training would be beneficial. This can offer insights into the usage of Multi-Scale Noise and other hyperparameters like p_{ DeNS }."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission487/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698892736204,
            "cdate": 1698892736204,
            "tmdate": 1699635975119,
            "mdate": 1699635975119,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1hGn9GbqbE",
                "forum": "X7gqOBG8ow",
                "replyto": "Fy3AdK6RDA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission487/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission487/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer eCvt (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for helpful feedback and address the comments below.\n\n---\n> 1. [Weakness 1] The paper could benefit from a broader theoretical discussion and a comparative analysis with other self-supervised techniques for non-equilibrium structures, such as denoising pretraining in [1], Noisy Nodes (using the OC20 dataset) [2], and improved noisy nodes (using MD17) [3]. These techniques have demonstrated efficacy for energy or force predictions for non-equilibrium molecules, hence their significance.\n\nPlease see **General Response 2** for the comparison to previous works.\n\n---\n\n> 2. Could there be label leakage when encoding forces as input for energy and force predictions of structures?\n\nNo, there is no label leakage for forces since we do not encode any force labels during evaluation (i.e., on the validation and testing sets). We only use force labels during denoising on the training set. In fact, on the OC20 testing set, the energy and force labels are not even publicly available. Evaluation on the testing sets is performed by a centralized server on EvalAI maintained by the OC20 authors.\n\nTo clarify the training procedure, we have both the upper blue block and the lower red block in Figure 2 during training but only have the upper blue block when evaluating the results on the validation and testing sets.\n\nWe clarify this in the revision.\n\n---\n> 3. [Weakness 2] The concept of \u201cencoding force as input\u201d needs corroborative proof from experiments. Results from Table 1e indicate energy prediction outcomes remain the same without force encoding.\n\nWe conduct additional ablations with and without force encoding on MD17 in **General Response 4**. In summary, denoising with force encoding achieves the best results across all molecules. Denoising without force encoding is worse than no denoising for most molecules and is worse than with force encoding.\n\nOn OC20 S2EF-2M (Table 1(e)), indeed the energy MAE is the same with and without force encoding, but the force MAE is significantly worse (~4%) without force encoding. We should look at both energy and force prediction at the same time when comparing two methods since there is always a trade-off between energy MAE and force MAE. For example, we can slightly increase energy coefficient so that we further reduce energy MAE but increase force MAE. In this case, we can have both lower energy and force MAE when we use force encoding. Moreover, using force encoding also leads to significantly better sample efficiency \u2013 training DeNS with force encoding for 12 epochs improves force MAE by 4% compared to without force encoding (Index 1 and Index 2 in Table 1(e)), as opposed to training EquiformerV2 without DeNS for 20 epochs instead of 12 (3% better force MAE for 66% more compute, Table 1(d)), suggesting that force encoding is quite important. \n\n---\n\n> 4. [Weakness 2] Similar problem-solving approaches have been published. A comparative discussion highlighting the distinctions and superiority of this paper's proposed methodology would prove advantageous.\n\nPlease see **General Response 2** for a comparison to previous works. \n\nThe main difference in our denoising formulation lies in that we additionally encode forces as input. This generalizes denoising to non-equilibrium structures since the atomic forces help identify the unique non-equilibrium structure to denoise to. Without conditioning on forces, the problem of denoising non-equilibrium structures is ill-posed since there are many possible target structures to denoise to (see Figure 1).\n\nPrevious works [1, 2, 3] are designed for equilibrium structures and are the same as our ablation of denoising without force encoding. While [4] applies the same approach to non-equilibrium structures in ANI-1 and ANI-1x datasets, we show that denoising non-equilibrium structures without force encoding is worse than with force encoding, and can sometimes even be worse than training without denoising altogether (**General Response 4**). \n\nReference:  \n[1] Godwin et al. Simple GNN Regularisation for 3D Molecular Property Prediction and Beyond. ICLR 2022.  \n[2] Zaidi et al. Pre-training via Denoising for Molecular Property Prediction. ICLR 2023.   \n[3] Feng et al. Fractional Denoising for 3D Molecular Pre-training. ICML 2023.  \n[4] Wang et al. Denoise pretraining on nonequilibrium molecules for accurate and transferable neural potentials. Journal of Chemical Theory and Computation 2023."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission487/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729982860,
                "cdate": 1700729982860,
                "tmdate": 1700729982860,
                "mdate": 1700729982860,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]