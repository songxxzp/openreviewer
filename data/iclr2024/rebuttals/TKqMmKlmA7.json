[
    {
        "title": "Modulate Your Spectrum in Self-Supervised Learning"
    },
    {
        "review": {
            "id": "GDvLvjwTtC",
            "forum": "TKqMmKlmA7",
            "replyto": "TKqMmKlmA7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4214/Reviewer_UjJ7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4214/Reviewer_UjJ7"
            ],
            "content": {
                "summary": {
                    "value": "- The authors propose a framework referred to as Spectral Transformation (ST) to modulate the spectrum of embedding and to seek functions beyond whitening that can avoid dimensional collapse.\n- Additionally, The authors introduce a novel ST instance named IterNorm with trace loss (INTL) to prevent collapse and modulate the spectrum of embedding toward equal eigenvalues during optimization.\n- The extensive experiments on ImageNet classification and COCO object detection demonstrate the effectiveness of INTL in learning superior representations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- (+) The authors show the novel points of the proposed methods, INTL, while comparing them with previous methods such as hard and soft whitening.\n- (+) The authors show the empirical observations of IterNorm, which map all non-zero eigenvalues to approach one, with large enough iterations (T)."
                },
                "weaknesses": {
                    "value": "- (-) The authors seem to have a missing baseline [1] in SSL. The baseline looks similar to INTL from the viewpoint of spectral adjusting.\n    - [1] Exploring the Gap between Collapsed & Whitened Features in SSL-ICML2022"
                },
                "questions": {
                    "value": "- Could the authors compare the INTL method with the baseline [1] if possible?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4214/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698563857578,
            "cdate": 1698563857578,
            "tmdate": 1699636388625,
            "mdate": 1699636388625,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gIwDQAh9z1",
                "forum": "TKqMmKlmA7",
                "replyto": "GDvLvjwTtC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4214/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4214/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you sincerely for your valuable suggestions and insightful comments."
                    },
                    "comment": {
                        "value": "## **Responses**\nThanks sincerely for your encouraging and insightful comments. Please find our responses to specific concerns below.\n\n**Concern 1:** The authors seem to have a missing baseline [1] in SSL. The baseline looks similar to INTL from the viewpoint of spectral adjusting.\n\n**Response:** \nThank you sincerely for your valuable suggestions. The theoretical contribution of [1] has been discussed in the related work of our submission, here we further list the key distinctions between our work and [1] as follows.\n1. The fundamental concept in [1] revolves around the impact of feature whitening on generalization, with an emphasis on adjusting the whitening degree of pre-trained SSL models based on the power law behavior of eigenspectra to **enhance downstream performance**. In contrast, our paper seeks to introduce a novel SSL training algorithm, leveraging our ST framework, to **address dimensional collapse** without relying solely on whitening.\n2. Our paper focuses on analyzing how adjusting the spectrum during the **training phase** can mitigate collapse-related issues. Conversely, [1] primarily investigates how the feature spectrum influences generalization during the **evaluation stage**.\n3. The method proposed in [1], named PMP, serves as a **post-processing evaluation technique** aimed at adjusting the spectrum of a pre-trained SSL encoder for improved downstream performance. In contrast, our proposed method, INTL, aligns with **SSL training algorithms** like SimCLR, Barlow Twins, etc. It is designed to prevent collapse within joint embedding architectures during the training process.\n\n[1] demonstrates that the proposed PMP method is effective in enhancing the linear evaluation performance of encoders pre-trained by SimCLR and Barlow Twins, particularly on datasets with limited labeled samples. In line with your recommendations, we conducted experiments on 1% and 10% subsets of ImageNet to assess the performance of our INTL using PMP. The results, presented in the table below, include LP (standard linear probe) and PMP (the evaluation method proposed in [1]). The findings indicate that PMP contributes to the improved evaluation performance of INTL.   \n&emsp; However, it's essential to note that our experiments in Section 5 are conducted based on the standard SSL benchmark using a standard linear probe, and therefore, the baseline from [1] is not incorporated in our experimental setup.\n\n|  Method   | Eval | Top-1 | Top-5  |\n| :-------- | :--------:| :--------:| :--------:| \n|    |  |1% &nbsp; 10%| 1% &nbsp; 10%|\n| SimCLR  | LP |48.1  &nbsp;   61.0 |  73.8   &nbsp;  84.3|\n| SimCLR  | PMP |50.9   &nbsp;  62.5 | 76.6  &nbsp;   85.2  |\n |  |  |  | \n| INTL (ours) | LP  | 53.9   &nbsp;  66.4 | 79.1 &nbsp;   87.7  | \n| INTL (ours) | PMP  | 54.9  &nbsp;   67.8 | 79.6   &nbsp;   89.1  |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4214/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699974948115,
                "cdate": 1699974948115,
                "tmdate": 1699974948115,
                "mdate": 1699974948115,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "82PviRYSda",
            "forum": "TKqMmKlmA7",
            "replyto": "TKqMmKlmA7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4214/Reviewer_AFis"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4214/Reviewer_AFis"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a new self-supervised learning framework called spectral transformation (ST) to modulate the spectrum of embedding to avoid dimensional collapse. To be specific, they introduced a novel ST instance named IterNorm with trace loss (INTL). Theoretically, this paper proved that INTL can modulate the spectrum of embeddings toward equal eigenvalues and prevent dimensional collapse. Empirically, the authors showed that INTL can obtain state-of-the-art performance for SSL on real-world datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The dimensional collapse is an important problem in contrastive learning and the analysis in this paper is insightful.\n2. The theoretical analysis and empirical results cooperate well. The improvements on real-world datasets are significant, especially in transfer learning tasks."
                },
                "weaknesses": {
                    "value": "1. As analyzed in this paper, both whitening methods and INTL are instances of spectral transformations. However, it seems that INTL outperforms whitening in every task. So what are the disadvantages of whitening methods? It would be better to provide more theoretical and empirical comparisons between them.\n2. The motivation behind the trace loss is a little confusing. Is it possible to provide a more detailed discussion?\n3. It seems that INTL shows superior performance in 5-nn accuracy than linear probing accuracy. Are there any intuitive explanations for that?\n4. There are some typos. For example, in p.4, \u2018Eqn. 13 can be viewed as an optimization problem over \u2026\u2019 should be replaced with \u2018Eqn.6 \u2026\u2019."
                },
                "questions": {
                    "value": "see my comments above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4214/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698647049308,
            "cdate": 1698647049308,
            "tmdate": 1699636388526,
            "mdate": 1699636388526,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OLqPfpXdP7",
                "forum": "TKqMmKlmA7",
                "replyto": "82PviRYSda",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4214/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4214/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your constructive comments and suggestions."
                    },
                    "comment": {
                        "value": "## **Responses (1/2)**\n\nYour comments and suggestions are exceedingly helpful to improve our paper. Our point-to-point responses to your questions are given below.\n\n**Question 1:** It seems that INTL outperforms whitening in every task. So what are the disadvantages of whitening methods?\n\n**Response:** Following your constructive suggestions, we further analysis the disadvantages of whitening methods and explain why our INTL can outperform whitening methods in downstream tasks.\n1. **The disadvantages of whitening methods.**\n>&emsp; In addition to the numerical instability issues discussed in Appendix D.3 for existing whitening methods, the unsatisfactory performance of these methods can be attributed to their weak constraints on embedding which are challenging to control and enhance effectively.  \n>&emsp; Recent theoretical work [1] proposes a power law to analyze the relationship between eigenspectrum and representation quality. Meanwhile, [2] demonstrates that the degree of feature whitening influences generalization. Both of these works emphasize that **a steep spectrum diminishes representation quality, a smooth spectrum compromises generalization, while a moderate spectrum ensures optimal generalization ability**. SSL methods impose their constraints on embedding to prevent collapse, and the strength of these constraints directly influences the spectrum distribution.  \n>&emsp; Hard whitening methods, such as W-MSE and CW-RGP, impose a full-rank constraint on embedding [3]. However, [3] indicates that the full-rank constraint is too weak to achieve optimal performance, leading to the proposal of a random group partition technique to enhance constraints and improve results. Nevertheless, this enhancement is unpredictable and comes with additional computational costs, especially on large datasets.  \n>&emsp; Soft whitening methods, like Barlow Twins and VICReg, impose a strong whitening constraint on embedding, urging the covariance matrix of the embedding to be identity. The results in Table 3 show that these methods (Barlow Twins) perform even worse than hard whitening methods (W-MSE) in transfer learning, which indicates that excessive constraints have already compromised their generalization ability for downstream tasks.\n\n2. **Why our INTL can outperform whitening methods in downstream tasks**\n>&emsp; The commendable performance of INTL can be attributed to its moderate constraint on the embedding. Unlike whitening methods, our INTL provides a moderate equal-eigenvalues constraint on embedding, which is stronger than hard whitening (full-rank constraint) but weaker than soft whitening (whitening constraint) as illustrated in Section 4. This moderate constraint results in a satisfactory spectrum distribution, thereby endowing the representation with better generalization capabilities.\n\n**Question 2:** The motivation behind the trace loss is a little confusing. Is it possible to provide a more detailed discussion?\n\n**Response:** As illustrated in Theorem 1, IterNorm implicitly maps  $\\\\forall \\\\lambda\\_i \\\\in \\\\lambda(\\\\mathbf{Z})$ to $\\\\widehat{\\\\lambda}\\_i = \\\\frac{\\\\lambda\\_i}{tr(\\\\Sigma)} {f\\_T}^2(\\\\frac{\\\\lambda\\_i}{tr(\\\\Sigma)})$. Based on Formula 4 and 5, we know  $\\\\forall T\\\\in \\\\mathbb{N}, \\\\lambda\\_i > 0 \\\\Longrightarrow 0 < \\\\widehat{\\\\lambda}\\_i < 1 $, and $\\\\lim\\_{T\\\\to\\\\infty} \\\\widehat{\\\\lambda}\\_i = 1 $. This suggests that the eigenvalues of the covariance matrix $\\\\Sigma\\_{\\\\widehat{\\\\mathbf{Z}}}$ resulting from the transformation output by IterNorm all fall within the range of 0 to 1, with an ideal convergence to 1 in the limit of infinite iterations ($T$). Therefore, $\\\\forall T\\\\in \\\\mathbb{N}, trace(\\\\Sigma\\_{\\\\widehat{\\\\mathbf{Z}}})=\\\\sum\\\\limits\\_{i=1}^d \\\\widehat{\\\\lambda}\\_i < d$.  \n&emsp; Consequently, in experiments, we have noted that IterNorm encounters severe dimensional collapse and struggles to effectively train the model in self-supervised learning, regardless of the chosen value for T. In such instances, we observe some eigenvalues of the covariance matrix $\\\\Sigma\\_{\\\\widehat{\\\\mathbf{Z}}}$ collapse into 0, causing a substantial gap between $trace(\\\\Sigma\\_{\\\\widehat{\\\\mathbf{Z}}})$ and $d$.  \n\n&emsp; To address this issue, the motivation behind the trace loss is to introduce an additional penalty that encourages  $trace(\\\\Sigma\\_{\\\\widehat{\\\\mathbf{Z}}})$ to approach its ideal maximum, where $\\\\sum\\\\limits\\_{i=1}^d \\\\widehat{\\\\lambda}\\_i = d$ and $\\\\forall i, \\\\widehat{\\\\lambda}\\_i =1$ . By doing so, the collapse is mitigated since it becomes improbable for eigenvalues of embedding to be 0."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4214/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699974171937,
                "cdate": 1699974171937,
                "tmdate": 1699974324880,
                "mdate": 1699974324880,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Zj6T8KNTVp",
            "forum": "TKqMmKlmA7",
            "replyto": "TKqMmKlmA7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4214/Reviewer_YNa9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4214/Reviewer_YNa9"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new approach, Spectral Transformation (ST), for self-supervised learning, and proposes a new training algorithm named IterNorm with trace loss (INTL). The basic idea of the paper is to balance the spectrum of the covariance matrices for the learned features which is often ill-posed. Theoretical and empirical results are provided as well for demonstrating the performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Clear writing with many experimental results."
                },
                "weaknesses": {
                    "value": "To me, I do not see any obvious weakness of the proposed approach. Motivated by the whitening, the paper presents a nice and logical development of the approach. However, I do not see a very strong point neither that can make this paper stand out compared with the literature. I suggested the authors to further emphasize the key contributions: What really makes your approach better than others such as MoCo and SimCLR? How about computational speed (this seems not to be discussed in both paper and appendix)? \n\nI\u2019d like to increase my rate if the authors can convince me at this point."
                },
                "questions": {
                    "value": "see my comment"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4214/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698941593871,
            "cdate": 1698941593871,
            "tmdate": 1699636388440,
            "mdate": 1699636388440,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "G1mXvolC2r",
                "forum": "TKqMmKlmA7",
                "replyto": "Zj6T8KNTVp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4214/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4214/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks sincerely for your encouraging words and constructive comments."
                    },
                    "comment": {
                        "value": "## **Responses (1/2)**\n\n**Question 1:** What really makes your approach better than others such as MoCo and SimCLR? \n\n**Response:** Following your constructive suggestions, we further emphasize the advantages of our proposed INTL compared to other methods step by step.\n1. **Without negative pairs: Our INTL eliminates the need for negative example construction, mitigating potential challenges inherent in contrastive learning which enlarges the dissimilarity among samples belonging to the same latent label.**\n> &emsp; In contrastive methods like MoCo and SimCLR, negative samples play an important role and need to be well-designed. However, the designed negative samples are likely to have the same latent label as the positive samples. In this case, contrastive methods will enlarge the dissimilarity among these samples, and could potentially lead to the disruption of the integrity of the potential manifold during the training process [1]. As a non-contrastive approach, our INTL circumvents this issue by eliminating the need for negative examples.\n\n2. **Robust to hyperparameters: Our INTL demonstrates robustness across various hyperparameters compared to the non-contrastive counterparts, making it adaptable to diverse application scenarios.**\n> &emsp; Although other non-contrastive methods also eliminate the need for constructing negative examples, many of these approaches display sensitivity to hyperparameters (including training parameters like batch size and algorithm parameters like penalty coefficient), posing challenges in their adaptation to diverse scenarios.  \n> &emsp; Some methods are sensitive to training parameters. For instance, BYOL, SwAV, and W-MSE are sensitive to batch size; they require a large training batch size to work well [2, 3]. As dimension de-correlation methods, Barlow Twins and VICReg are sensitive to embedding dimensions; they require a large embedding dimension to work well [4, 5]. These requirements increase the demand for computational resources.  \n> &emsp; Moreover, certain SSL methods are sensitive to algorithm parameters and necessitate substantial parameter tuning when applied to diverse datasets or network architectures. For example, SimSiam, DINO, and SwAV exhibit poor performance on CIFAR-100 and ImageNet-100 when using the parameters provided in their settings on ImageNet-1K [6].  \n> &emsp; In contrast, our INTL are robust to hyperparameters. It achieves commendable performance with small batch size (Table 2). It also exhibits robustness to embedding dimensions (Figure 8). Notably, INTL does not necessitate parameter tuning (the iteration number $T$ of IterNorm and the coefficient $\\beta$ are fixed when applied to different datasets and network architectures), as shown in Appendix C. These characteristics underscore its adaptability and effectiveness across diverse scenarios.\n\n3. **Moderate constraints: Our INTL imposes a moderate constraint on embedding which results in a satisfactory spectrum distribution, thereby endowing the representation with good generalization capabilities.**\n> &emsp; Recent theoretical work [7] proposes a power law to analyze the relationship between eigenspectrum and representation quality. Meanwhile, [8] demonstrates that the degree of feature whitening influences generalization. Both of these works emphasize that a steep spectrum diminishes representation quality, a smooth spectrum compromises generalization, while **a moderate spectrum ensures optimal generalization ability**. SSL methods impose their constraints on embedding to prevent collapse, and the strength of these constraints directly influences the spectrum distribution.  \n> &emsp; Hard whitening methods, such as W-MSE and CW-RGP, impose a full-rank constraint on embedding [2]. However, [2] indicates that the full-rank constraint is too weak to achieve optimal performance, leading to the proposal of a random group partition technique to enhance constraints and improve results. Nevertheless, this enhancement is unpredictable and comes with additional computational costs, especially on large datasets.  \n> &emsp; Soft whitening methods, like Barlow Twins and VICReg, impose a strong whitening constraint on embedding, urging the covariance matrix of the embedding to be identity. The results in Table 3 show that these methods (Barlow Twins) perform even worse than hard whitening methods (W-MSE) in transfer learning, which indicates that excessive constraints have already compromised their generalization ability for downstream tasks.  \n> &emsp; In contrast, our INTL provides a moderate equal-eigenvalues constraint on embedding, which is stronger than hard whitening (full-rank constraint) but weaker than soft whitening (whitening constraint). Our INTL performs much better than whitening methods in transfer learning, which indicates this moderate constraint enables the representation to attain a favorable spectrum distribution, consequently endowing it with good generalization capabilities."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4214/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699973669409,
                "cdate": 1699973669409,
                "tmdate": 1699973813570,
                "mdate": 1699973813570,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Bc7InJwh5h",
                "forum": "TKqMmKlmA7",
                "replyto": "Zj6T8KNTVp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4214/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4214/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We sincerely appreciate your time in reading the paper"
                    },
                    "comment": {
                        "value": "## **Responses (2/2)**\n\n4. **Theoretical guarantee: Our INTL is supported by theoretical evidence to avoid collapse.**\n> &emsp; Although some non-contrastive methods such as BYOL, SimSiam, and DINO work well to avoid collapse, it remains unclear how these asymmetric networks effectively prevent collapse without the inclusion of negative pairs. Instead, our INTL is theoretically guaranteed to avoid collapse and our theoretical analysis presents a new thought in demonstrating how to avoid dimensional collapse.\n\nIn summary, **our INTL method is free to negative pairs, robust to hyperparameters,  moderate to eig-spectrum constraints, and guaranteed to avoid collapse**. These characteristics of INTL make it have the potential to stand out in self-supervised learning. We hope the detailed response above can convince you. Thanks! \n\n**Question 2:** How about computational speed (this seems not to be discussed in both paper and appendix)?\n\n**Response:** We have reported the computational cost of INTL (time and GPU memory requirements of our implementation for INTL trained per epoch on ImageNet with ResNet-50) in Table 9 of the appendix. Our INTL with EMA requires a total of around 23.6 GB GPU memory and 24min46s running time per epoch, which is comparable to 20 GB memory and 23min11s running time per epoch required by MoCo-v2 which also uses EMA. Meanwhile, compared to other methods that use an additional predictor (such as BYOL), or use eigen-decomposition (such as W-MSE), our INTL requires less memory and computation time.\n\n> **References**  \n> [1] HaoChen, J.Z., Wei, C., Gaidon, A., Ma, T.: Provable guarantees for self-supervised deep learning with spectral contrastive loss. In: NeurIPS (2021)  \n> [2] Xi Weng, Lei Huang, Lei Zhao, Rao Muhammad Anwer, Salman Khan, and Fahad Khan. An investigation into whitening loss for self-supervised learning. In NeurIPS, 2022.  \n> [3] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In CVPR, 2021.  \n> [4] Jure Zbontar, Li Jing, Ishan Misra, Yann Lecun, and Stephane Deny. Barlow twins: Self-supervised learning via redundancy reduction. In ICML, 2021.  \n> [5] Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization for self-supervised learning. In ICLR, 2022.  \n> [6] Victor Guilherme Turrisi da Costa, Enrico Fini, Moin Nabi, Nicu Sebe, and Elisa Ricci. solo-learn: A library of self-supervised methods for visual representation learning. Journal of Machine Learning Research, 23(56):1\u20136, 2022.  \n> [7] Ghosh, Arna, et al. Investigating power laws in deep representation learning. arXiv preprint arXiv:2202.05808 (2022).  \n> [8] Bobby He and Mete Ozay. Exploring the gap between collapsed and whitened features in selfsupervised learning. In ICML, 2022"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4214/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699973760453,
                "cdate": 1699973760453,
                "tmdate": 1699974526636,
                "mdate": 1699974526636,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]