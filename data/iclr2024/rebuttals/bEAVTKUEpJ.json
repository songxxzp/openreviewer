[
    {
        "title": "SARI: SIMPLISTIC AVERAGE AND ROBUST IDENTIFICATION BASED NOISY PARTIAL LABEL LEARNING"
    },
    {
        "review": {
            "id": "SFVceIsBSs",
            "forum": "bEAVTKUEpJ",
            "replyto": "bEAVTKUEpJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7866/Reviewer_WNqE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7866/Reviewer_WNqE"
            ],
            "content": {
                "summary": {
                    "value": "To deal with the problem of noisy partial label learning (NPLL), this paper proposes a framework called SARI via combining the strengths of Average Based Strategies and Identification Based Strategies. Experimental results validate its effectiveness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.This paper proposes an effective frameworks, which is illustrated from the experimental results.\n\n2.The figures and tables are well presented.\n\n3.The proposed method is easy to understand and follow."
                },
                "weaknesses": {
                    "value": "1.My main concern is the novelty of this paper. As it claims, the novelty lies in the potential of a simpler alternatives for NPLL instead of the architecture. \n\n2.The simplicity of the proposed framework is partly reflected in no demand for warm-up. However, one of the key techniques is to perform pseudo-labeling via a weighted KNN algorithm, and the KNN algorithm often needs an effective feature extractor to perform well. Without the process like warm-up or pre-training, we could not obtain such an effective feature extractor."
                },
                "questions": {
                    "value": "1.As it is mentioned above, how can the pseudo-labeling be implemented well in the proposed method with the weighted KNN? Please give more details about it."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7866/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698222959616,
            "cdate": 1698222959616,
            "tmdate": 1699636964781,
            "mdate": 1699636964781,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "r8tSoc9lyV",
                "forum": "bEAVTKUEpJ",
                "replyto": "SFVceIsBSs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7866/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7866/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their positive feedback and address their concerns below:\n\n* **Q1: Novelty of the proposed framework.** Please refer to the global response. If minimal and existing ideas work well, it would be unjust to push for unnecessary complexities and fancy architectures.\n\n* **Q2: Need for pre-training to obtain effective feature extractor.** \n    1. We also initially thought that an unsupervised pre-training strategy would be required for effective feature extraction. We employed a few epochs of MoCo to learn the initial set of features (giving a warm start). However, we found no difference in performance with and without such unsupervised pre-training. We illustrate the results below. \n    1. Recent studies [G] suggest that a randomly initialised backbone can also provide good features due to its inductive bias. We hypothesize that this is the reason that the network is able to learn in the initial training phase. \n    1. Further, we highlight that, for the other datasets: CUB-200, Treeversity, Benthic and Plankton, we initialize the network with Imagenet pre-trained weights (in accordance with prior methods) which provides good initial features.\n    1. We would also like to clarify that warm-up in other methods is not unsupervised pre-training. For example, ALIM initially trains PiCO for several epochs (calling it warm-up), and then applies their own ideas on top of this pre-trained model. Hence their method at least needs the complexity of PiCO. \n\n    | CIFAR-100| $q=0.05$, $\\eta=0.3$ | $q=0.05$, $\\eta=0.5$ |\n    | --- | ----------- | -------|\n    | With MoCo |76.76 |74.29 |\n    | Without MoCo|77.61 | 75.15 |\n\n[G] Ulyanov, Dmitry, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. CVPR. 2018."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7866/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700237806424,
                "cdate": 1700237806424,
                "tmdate": 1700632411736,
                "mdate": 1700632411736,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wYrBMK0zKQ",
                "forum": "bEAVTKUEpJ",
                "replyto": "r8tSoc9lyV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7866/Reviewer_WNqE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7866/Reviewer_WNqE"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. It is suggested to further explore the motivation to integrate existing tricks and provide more insight into the problem.  Otherwise, the entire article reads with a sense of patchwork. Consequently, I will maintain my current scoring."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7866/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713921073,
                "cdate": 1700713921073,
                "tmdate": 1700713921073,
                "mdate": 1700713921073,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3uAfY3HV6A",
            "forum": "bEAVTKUEpJ",
            "replyto": "bEAVTKUEpJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7866/Reviewer_EnLY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7866/Reviewer_EnLY"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript considers a new setting of PLL, i.e., NPLL, and presents a minimalistic framework called SARI that initially assigns pseudo-labels to images by exploiting the noisy partial labels through a weighted nearest neighbour algorithm. The authors conduct some experiments to validate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The manuscript is well-organized, and easy to follow."
                },
                "weaknesses": {
                    "value": "PLL itself is a noisy learning case, so it is very confusing to name Noisy PLL as a new noisy learning case. Also, it is worth thinking about whether PLL is learnable even though assuming that the ground-truth resides in the candidate label set, let alone removing the constraint as a new setting NPLL.\n\nThe simulated data in the experiments can not well reflect the real-world scenarios, thus It is meaningless to compare the results on these datasets.\n\nBTW, I didn't see any novelty in the proposed framework. It is time to stop and think what is a meaningful research, not just pursuit for paper quantities with meaningless work."
                },
                "questions": {
                    "value": "The techiques in the paper still combines those methods of PLL, what is your technique novelty for NPLL?\n\nThe dataset in the experiment is not reflectable on real-world scenarios, and the setting of the noise rate is too subjective. How to improve it? If no real-world data can be used for experiments, how to prove the value of NPLL setting and your proposed method without theorectical guarantee?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7866/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7866/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7866/Reviewer_EnLY"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7866/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698557093220,
            "cdate": 1698557093220,
            "tmdate": 1699940985284,
            "mdate": 1699940985284,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yDIS2nTL2m",
                "forum": "bEAVTKUEpJ",
                "replyto": "3uAfY3HV6A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7866/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7866/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their comments and address their concerns below:\n\n* **Q1: Simulated dataset and not a real world one.**\n    1. There are atleast a dozen papers in top forums (e.g. Neurips, ICML and ICLR) on the same setting in the last couple of years. Hence, we followed and presented the results in the same way, assuming that it is accepted by the community. However, we understand and respect your concern that these datasets are synthetically curated and do not necessarily reflect upon the practical applicability of the PLL/NPLL solutions.\n    1. Your comments encouraged us to explore alternative datasets where the method can be applicable. Last four days, we investigated its applicability in the crowd sourced datasets and performed experiments on three such publicly available real world datasets called Treeversiy, Benthic and Plankton (please refer to the global response). SARI achieves state-of-the-art performance in these real world datasets as well. We demonstrate that these crowd sourced datasets can be approached both from a noise robust learning or a partial label learning perspective. And that adds one practical use case for the PLL/NPLL. We thank you for your comments, which led us to this exploration. We hope, this changes your opinion on our paper. We would be happy to clarify any further queries from your end. \n    1. The quick initial experiments also showcase the adaptability of SARI on varying datasets and scenarios. \n    1. Further, we refer you to a recent work S-CLIP [F], a semi-supervised learning method for training CLIP that utilizes additional unpaired images. This work employs PLL in their pseudo-labelling pipeline, demonstrating another interesting, real-world use case.\n\n\n* **Q2: Novelty of proposed framework.** We request the reviewer to refer to the global response. \n\n[F] Mo, Sangwoo, et al. \"S-CLIP: Semi-supervised Vision-Language Learning using Few Specialist Captions.\" Thirty-seventh Conference on Neural Information Processing Systems. 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7866/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700237761756,
                "cdate": 1700237761756,
                "tmdate": 1700237761756,
                "mdate": 1700237761756,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AOiLvCZph7",
            "forum": "bEAVTKUEpJ",
            "replyto": "bEAVTKUEpJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7866/Reviewer_WW8x"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7866/Reviewer_WW8x"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new framework called SARI for noisy partial label learning (NPLL) that combines the benefits of both average-based and identification-based strategies through the utilization of a weighted nearest neighbor method. The proposed framework achieves a good performance compared with the existing state-of-the-art methods, occasionally delivering impressive results, especially in fine-grained classification scenarios. Ablation studies and quantitative experiments convincingly showcase the effectiveness of this approach. Overall, this work is characterized by its simplicity and reader-friendly nature, ensuring accessibility and comprehension for a wider audience."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper introduces a new noisy partial label learning method called SARI, which performs label disambiguation by the weighted nearest neighbor method.\n2. This work is straightforward and easily understandable, making it reader-friendly for potential readers.\n3. Extensive experimental results demonstrated the effectiveness of the proposed SARI method."
                },
                "weaknesses": {
                    "value": "The paper focus on an interesting noisy partial label learning problem and has several issues that can be improved: \n\n(1) The author scrutinized the deficiencies inherent in current methodologies, highlighting their complexity, warm-up requirements, and error propagation. However, it remains ambiguous how these shortcomings are tackled within the proposed method by the authors.\n\n(2) The proposed method is easy to understand but lacks novelty. It seems no technological innovation in this approach instead of integrating the existing technologies.\n\n(3) The proposed method appears to heavily depend on parameter selection, rendering the approach more empirical than technical."
                },
                "questions": {
                    "value": "(1) My main concern about the paper lies in the novelty: The proposed method seems just integrates existing technologies: like small loss trick, label smoothing, mixup, and consistency regularization by using weak and strong augmentation, which leads to the novelty limited.\n\n(2) The proposed method computes the pseudo-laebl $\\hat y_i$ using weighted KNN form the entire dataset, which make the approach memory consuming and time consuming.\n\n(3) In noisy partial label learning, the true label may not in the candidate label set. Thus, the computation of the pseudo-laebl $\\hat y_i$ and class posterior probabilities $\\hat q_c$ can be noisy, since the assumption proposed in the paper \u2018samples in the neighbourhood have the same class label\u2019 can hardly held.\n\n(4) Many experimental results are missing in Table 1, Table 3, and Table 4.\n\n(5) In ablation study, when k=25, the performance improves in $\\eta=0.3$ while degrades $\\eta=0.5$. However, when k=50, one degrades while the other improves. Can you explain why?\n\n(6) Please see the weakness above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7866/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698776078536,
            "cdate": 1698776078536,
            "tmdate": 1699636964544,
            "mdate": 1699636964544,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xdEHtrp8a7",
                "forum": "bEAVTKUEpJ",
                "replyto": "AOiLvCZph7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7866/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7866/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their detailed feedback and address their concerns below:\n\n* **Q1: Novelty.** Please refer to the global response. If minimal and existing ideas work well, it would be unjust to push for unnecessary complexities and fancy architectures. \n\n* **Q2: On complexity, warm-up requirements, and error propagation. It remains ambiguous how the shortcomings are tackled within the proposed method :**\n    1. Our approach does not need warm up using a base PLL method. Many methods like ALIM first train PICO for several epochs and then add upon it. \n    1. SARI is a single branch network with a single loss function. It uses standard regularization techniques. \n    1. Reliable image-label pairs and label smoothing makes our method significantly robust to noise. Furthermore, in every iteration we freshly pick the reliable image-label pairs. In contrast, several other methods (like Pico+, IRNet etc.) maintain an explicit state/probability vector, which is sequentially updated at every iteration. Our approach is free of such state maintenance and hence reduces error propagation. \n    1. Label smoothing helps to reduce the impact of noise caused by sample-selection bias, which is another known reason for error accumulation [D] [E].\n\n* **Q3: K-NN is memory and time consuming.**  We would like to emphasize that the K-NN is only used in the training phase. We use FAISS library for optimized nearest neighbour computation. On the CIFAR-100 dataset, our model trains within 3 hours. In contrast, the base PiCO model training takes over 10 hours on the same GPU. Please note that several methods build on top of the PiCO model and may inherit the slow training. We are committed to release the code post paper acceptance to strengthen our case. \n\n* **Q4: Assumption of \u2018samples in the neighbourhood have the same class label\u2019.** We assume that the samples of the same class would be closer in feature space. As long as most samples in the neighbourhood contain the ground truth, the pseudo-label will be computed correctly. We perform Partial Label Augmentation which can potentially add the correct label into the partial set. As time progresses, this leads to better pseudo-label computation.\n\n* **Q5: Many experimental results are missing in Table 1, Table 3, and Table 4.** We do a comprehensive set of experiments, compared to some of the other papers. For the matching configuration, we take the numbers from the corresponding original paper. Since the codes were not publicly available at the time of submission, we could not compute the results for other configurations. However, given the available results, there is consistent evidence on the superior performance of SARI.  \n\n* **Q6: In ablation study, when K=25, the performance improves in $\\eta =0.3$ while degrades $\\eta=0.5$. However, when $K=50$, one degrades while the other improves.** The variance remains within 1pp, so results are largely similar. In a highly noisy scenario, the highest value of $K$ seems to benefit the performance, as it would allow to have a larger consensus. \n\n\n[D] Michal Lukasik, Srinadh Bhojanapalli, Aditya Menon, and Sanjiv Kumar. Does label smoothing mitigate label noise? In International Conference on Machine Learning, pp. 6448\u20136458. PMLR, 2020.\n\n[E] Jian et al. Mentornet: Learning data driven curriculum for very deep neural networks on corrected labels. ICML 2018"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7866/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700237720090,
                "cdate": 1700237720090,
                "tmdate": 1700237720090,
                "mdate": 1700237720090,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "p7D4vDBp0F",
                "forum": "bEAVTKUEpJ",
                "replyto": "xdEHtrp8a7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7866/Reviewer_WW8x"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7866/Reviewer_WW8x"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. I still think integrating existing tricks as the new method is contribution-limited for this conference. Thus, I will keep my score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7866/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703570205,
                "cdate": 1700703570205,
                "tmdate": 1700703570205,
                "mdate": 1700703570205,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "s9lsgblsFm",
            "forum": "bEAVTKUEpJ",
            "replyto": "bEAVTKUEpJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7866/Reviewer_YQoY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7866/Reviewer_YQoY"
            ],
            "content": {
                "summary": {
                    "value": "The author proposes a new approach in the field of weakly supervised learning in the domain of NPLL. He argues that the current methods three main challenges: 1. complexity 2. the need for warm-up 3.error propagation. In response, the author proposes the SARI model, which integrates the advantages of Average-based strategies and Identification-based strategies. Specifically, the author employs K-nearest neighbors(KNN) to obtain weighted pseudo-labels for unlabeled samples. Then, a classifier is trained using these pseudo-labels, and the training process is enhanced with techniques such as smoothing and consistency regularization to improve robustness. Finally, the author updates the existing labels of the KNN with highes probability class predicted by the current model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1\uff09The author conducts an analysis of the limitations in previous methods for NPLL and identifies complexity, the need for warm-up, and error propagation as the three main challenges in this field. The author's arguments are well-founded and reasonable.\n\n2\uff09The structure of the paper is reading friendly and easy to understand.\n\n3\uff09The author's experiments are comprehensive and thorough."
                },
                "weaknesses": {
                    "value": "1.The author only compares their method on ResNet 18. Given the fact that of several updated backbone networks has been proposed in recent years. It would provide a more comprehensive evaluation of the model's performance under different backbone architectures.\n\n2.I doubt whether the author really addresses the above three issues. The author mentions that the previous method can cause error propagation. However, KNN classifiers according to the overall features from the current encoders, and if there is too much noise features, KNN will also accumulates noise. In addition, the author mentions that the previous method has too many hyper parameters, but according to the author's proposed method, $ K, \\delta, \\gamma, \\lambda$ and other hyper parameters are also required. Further more, according to the ablation study, Mix-up and CR significantly improves the performance. I doubt that the advantage of the author's model rooted by these existing techniques rather than new training methods, which reduces the innovation of this paper.\n\n3.This article needs a more professional writing language."
                },
                "questions": {
                    "value": "As the problems in the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I have no ethics concerns."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7866/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698916209739,
            "cdate": 1698916209739,
            "tmdate": 1699636964427,
            "mdate": 1699636964427,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TKTHpm5djo",
                "forum": "bEAVTKUEpJ",
                "replyto": "s9lsgblsFm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7866/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7866/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their thoughtful feedback and address their concerns below:\n\n* **Q1: Results with other backbones.** In alignment with prior research and for fair comparisons, we limited our experiments to the resnet18 backbone. However, as per the reviewers request we conducted experiments with two other backbones and present the results below. We use the same parameters as in ResNet18 experiments.\n\n    | CIFAR-100| $q=0.05$, $\\eta=0.3$ |\n    | --- | ----------- |\n    | Wide-ResNet-34x10 | 81.3 |\n    | DenseNet-121 | 78.75 |\n    | ResNet-18 | 77.61 |\n\n    The performance  improves over the resnet18 backbone in both the cases. We will add experiments with more backbones in the final version of the paper. \n\n* **Q2: Hyperparameters.** SARI has four key hyperparameters $K$, $\\delta$, $\\lambda$ and $r$ ($\\gamma$ is derived from $\\delta$).\n    1. We keep the same parameters in all experiments of the paper, across datasets and noise settings.\n    1. Barring extreme values, the performance remains similar with change in variables (variance within 1-2pp, Table 5).\n    1. Even with suboptimal parameters SARI outperforms previous state of the art methods. \n    1. We performed experiments on three new datasets (see global response). With the same parameters we achieve state of the art performance on them as well. Showcasing the simplicity and adaptability of our approach.\n\n* **Q3: On complexity, warm up, error propagation.**\n    1. Our approach does not need warm up using a base PLL method. Many methods like ALIM first train PICO for several epochs and then add upon it. \n    1. SARI is a single branch network with a single loss function. It uses standard regularization techniques. \n    1. Reliable image-label pairs and label smoothing makes our method significantly robust to noise. Furthermore, in every iteration we freshly pick the reliable image-label pairs. In contrast, several other methods (like PiCO+, IRNet etc.) maintain an explicit state/probability vector, which is sequentially updated at every iteration. Our approach is free of such state maintenance and hence reduces error propagation. \n    1. Label smoothing helps reduce the impact of noise caused by sample-selection bias, which is another known reason for error accumulation [D] [E].  \n    1. Most previous PLL methods also utilize CR/Mix-up\n\n\n[D] Michal Lukasik, Srinadh Bhojanapalli, Aditya Menon, and Sanjiv Kumar. Does label smoothing mitigate label noise? In International Conference on Machine Learning, pp. 6448\u20136458. PMLR, 2020.\n\n[E] Jian et al. Mentornet: Learning data driven curriculum for very deep neural networks on corrected labels. ICML 2018"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7866/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700237675573,
                "cdate": 1700237675573,
                "tmdate": 1700237675573,
                "mdate": 1700237675573,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "awZzBB0RG4",
                "forum": "bEAVTKUEpJ",
                "replyto": "TKTHpm5djo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7866/Reviewer_YQoY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7866/Reviewer_YQoY"
                ],
                "content": {
                    "comment": {
                        "value": "I do not think that the novelty is sufficient according to the response and I keep my socre 5."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7866/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699171089,
                "cdate": 1700699171089,
                "tmdate": 1700699171089,
                "mdate": 1700699171089,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]