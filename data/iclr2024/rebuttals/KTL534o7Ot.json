[
    {
        "title": "Programmable Synthetic Data Generation"
    },
    {
        "review": {
            "id": "5dl1GM37ym",
            "forum": "KTL534o7Ot",
            "replyto": "KTL534o7Ot",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7367/Reviewer_keAe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7367/Reviewer_keAe"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a tabular data generation method called ProgSyn where one can vary fairness, privacy, and logical constraints. The three constraints are relaxed into differentiable loss terms and used to fine tune a generative model. Experiments show that it is possible to generate synthetic data that satisfies compound constraints while maintaining high downstream accuracy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* This is a timely work that addresses the important problem of configurable tabular data generation satisfying multiple constraints.\n* The presentation is straightforward to understand.\n* Experiments show promising results on generating data satisfying multiple constraints."
                },
                "weaknesses": {
                    "value": "* The support for fairness seems limited. There are many fairness definitions in the literature beyond demographic parity including equalized odds, equal opportunity, predictive parity, equal error rates, individual fairness, and causal fairness, which seem to be ignored here. The proposed work would be much more interesting if it could also be configured for the other fairness measures as well. Supporting demographic parity only gives the impression that only the easiest fairness measure is supported, and there is no discussion on how to possibly extend the framework to other measures either.\n\n* Emphasizing the programmability of ProgSyn sounds a bit exaggerated. For example, Figure 3 looks like a conventional config file instead of say a Python program. When using DP, a user always specify epsilon and delta, but this is not called programming.\n\n* It is not clear why DP should be optimized together with fairness holistically. Why not generate a fair dataset using an existing technique or a fairness-only version of ProgSyn and then add random noise to satisfy DP? This two-step approach should be a baseline and compared with ProgSyn empirically.\n\n* It would be more interesting to see the limitations of this method where the accuracy actually has to drop in order to satisfy various constraints. The current experiments only show success cases, but fairness and privacy are not necessary aligning, so there has to be a point where the accuracy cannot be maintained. In Table 3, there is almost no reduction of accuracy after applying DP, which suggests that the proposed method may have not been stressed tested enough. Hence, there should be more extensive experiments showing what happens in truly challenging scenarios."
                },
                "questions": {
                    "value": "Please address the weak points."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7367/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697628613573,
            "cdate": 1697628613573,
            "tmdate": 1699636881297,
            "mdate": 1699636881297,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Vr4OsRQAPV",
                "forum": "KTL534o7Ot",
                "replyto": "5dl1GM37ym",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7367/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7367/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "First of all, we would like to thank the reviewer for their detailed assessment of our paper, and for their constructive feedback. We are also pleased to read the reviewer\u2019s recognition of the timeliness and importance of the problem setting, the quality of presentation, and the strong empirical results. Below, we address the reviewers comments and questions:\n\n**Q1: Can ProgSyn support fairness notions beyond demographic parity?**\n\nYes, there is no technical restriction in ProgSyn that would only allow for demographic parity, and the modularity of ProgSyn allowed us to easily implement and test two more fairness measures: equality of opportunity (EoO), and equalized odds (EO). We repeat the fairness experiment on Adult on these two new measures in all settings and report our results in the tables below:\n\nEqualized Odds (non-private) \u2013 True data acc.: 85.4% EO: 0.08\n\n| |Accuracy [%]|EO|  \n|:--|:--:|--:|  \n|Decaf DP|66.8|0.07|  \n|Decaf FTU|69.0|0.14|  \n|Decaf CF|67.1|0.08|  \n|TabFairGAN|82.6|0.04|  \n|ProgSyn|**84.5**|**0.031**|\n\nEqualized Odds (differentially private):\n\n| |Accuracy [%]|EO|  \n|:--|:--:|--:|  \n|PreFair Greedy|80.2|**0.01**|  \n|PreFair Optimal|75.7|0.03|  \n|ProgSyn|**83.4**|0.02|\n\nEquality of Opportunity (non-private) \u2013 True data acc.: 85.4% EoO: 0.09\n\n| |Accuracy [%]|EoO|  \n|:--|:--:|--:|  \n|Decaf DP|66.8|0.07|  \n|Decaf FTU|69.0|0.15|  \n|Decaf CF|67.1|0.10|  \n|TabFairGAN|82.6|**0.02**|  \n|ProgSyn|**84.5**|**0.02**|\n\nEquality of Opportunity (differentially private):\n\n| |Accuracy [%]|EoO|  \n|:--|:--:|--:|  \n|PreFair Greedy|80.2|**0.02**|  \n|PreFair Optimal|75.7|0.04|  \n|ProgSyn|**83.3**|0.04|\n\nAs we can observe from the above tables, ProgSyn still prevails as the method providing the best accuracy at a low bias level. In the non-private setting ProgSyn achieves both the best fairness and accuracy from all methods, while in the differentially private setting, it achieves comparably low bias to PreFair while having >3% higher accuracy.\n\nIn general, any fairness measure (as well as any other specification) that can be expressed as a continuous function or a good enough relaxation is naturally supported by ProgSyn. Therefore, there is no fundamental reason why individual fairness notions could not be integrated into ProgSyn either, however, we do not consider it here as (i) there are conflicting notions of individual fairness, (ii) the design of individual fairness pipelines involve challenges that are orthogonal to the objective of our work, and (iii) the fair synthetic tabular data generation methods we have surveyed all concern group fairness, which is also where we position our work. We definitely believe that developing a flexible individually fair synthetic data generation pipeline is an interesting research challenge that could lead to highly valuable contributions for the community.\n\nFurther, regarding causal fairness, we did not consider this fairness notion as it requires knowledge of the causal structure of the dataset, introducing a tight, second information bottleneck in the pipeline, and additional challenges worth their own contributions, such as the question of how can the elucidation of the causal structure be united with differential privacy. The requirement for the causal graph is a serious limitation of causal fairness methods, as also exemplified by DECAF, one of the fair synthetic data generation methods we compare ProgSyn against, where we were only able to conduct comparisons on Adult, and not on the other three datasets, as the method\u2019s implementation included only the causal graph of Adult."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7367/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700166652814,
                "cdate": 1700166652814,
                "tmdate": 1700166652814,
                "mdate": 1700166652814,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "H2C93mnd0D",
            "forum": "KTL534o7Ot",
            "replyto": "KTL534o7Ot",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7367/Reviewer_XJ8v"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7367/Reviewer_XJ8v"
            ],
            "content": {
                "summary": {
                    "value": "This article introduces the ProgSyn framework, which is the first framework designed for programmable table data generation.  The overall architecture of this framework is based on a two-stage process, starting with pre-training of the generated model using sampling and decoder structures, and then fine-tuning specific downstream tasks and adapting to their requirements.  At the same time, in the process of concrete implementation, ProgSyn uses the relaxed version of differential privacy, descriptive requirements and specification to achieve the programmability of the whole process.  The structure of this paper is reasonable and the content is clear, which provides a good model for the research of controlled table data generation.  In subsequent experiments and appendices, the authors also intelligently present their contributions and ideas by using selected datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe paper maintains a clear and focused main theme, presenting novel research in the realm of programmable generation for table data. The proposed ProgSyn framework, utilizing a pretraining and fine-tuning architecture, effectively caters to the requirements of downstream tasks in practical scenarios.\n2.\tIn terms of methodology, the authors employ three approaches: differential privacy constraints, logical constraints, and statistical formulations. These strategies support the programmable nature of ProgSyn and demonstrate careful consideration for its differentiability, thereby enhancing its practical implement ability.\n3.\tWithin this paper, the proposed differential computation for binary masks addresses the challenge of non-differentiable hard logical constraints and counts. This method provides an effective means for controlling the content generated by the model during the generation process.\n4.\tThe supporting materials provided in this paper exhibit well code writing standardization and good applicability. The code content aligns well with the paper's core ideas, making it an excellent resource for readers seeking a deeper understanding of the author's concepts."
                },
                "weaknesses": {
                    "value": "1.\tTheoretical Framework: Although the research area explored in this paper holds promise for further investigation, the technical aspects in the paper appear somewhat dated, primarily covering foundational theories and methods. Considering the goal of ProgSyn is to generate sufficiently realistic simulated data with privacy protection properties, the authenticity aspect is addressed mainly in terms of experimental accuracy (given that XGBoost is a robust classifier), without validating its reliability from a statistical hypothesis testing perspective. This arrangement may lead to a somewhat one-sided argument in the paper.\n2.\tWriting Clarity: The content in the paper's introduction appears somewhat disorganized, as it combines background introduction with an overview of the framework's methodology. It is recommended to organize the sections logically as \"introduction,\" \"related work,\" and \"formulation\" to help readers better understand the core content of the paper. Additionally, there are instances of non-standard writing in the paper, such as lengthy formulas (page 6), pseudocode (page 5), page breaks (page 15, 20, 25), and inconsistencies in paper formatting (page 10-12). Also, it's important to address the improper use of color in tables.\n3.\tCode: To facilitate the research framework's wider adoption, it's advisable to update the code version requirements. Personally, I encountered issues with running the code on an NVIDIA GeForce RTX 4090 with CUDA capability sm_89, and it would be beneficial to address compatibility concerns to ensure broader accessibility and usability."
                },
                "questions": {
                    "value": "Regarding this study's research, I have the following queries:\n1.\tTypically, table data is more widespread and common than image data. Does the controllable generation method proposed in this paper aim to fill the missing distributions in real data, as opposed to directly applying conditional filtering within the table? Because it is cheaper to filter tabular data through simple rules than to generate data compared to other data structures.\n2.\tIn the field of image generation, we can rely on our own visual judgment or specific discriminators or metrics, such as FID, for authenticity assessment. Apart from the loss function control methods mentioned in this paper, are there more reliable approaches for verifying the authenticity of the generated data?\n3.\tIn practical scenarios involving table data, recommendation systems present a more realistic application. Since relying solely on a highly generalizable model like XGBoost might not provide strong model performance validation, can ProgSyn consider further methodological validation (e.g., using models from other domains like ONN, xDeepFM, or more general models like SVM, GBDT)?\n4.\tIn the main text, I didn't come across rigorous proofs related to statistical control of table data generation. Could you provide some information on the formulation process for statistical control? This would greatly assist readers in understanding controllable generation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7367/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7367/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7367/Reviewer_XJ8v"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7367/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698658571437,
            "cdate": 1698658571437,
            "tmdate": 1699636881184,
            "mdate": 1699636881184,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3i9ROpyMjf",
                "forum": "KTL534o7Ot",
                "replyto": "H2C93mnd0D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7367/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7367/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "First of all we would like to thank the reviewer for their thorough investigation of our work, including the supplementary materials, and for their far-reaching and constructive feedback. We also appreciate the reviewer\u2019s recognition of the novelty of our work, its strong empirical performance, and the quality of the provided artifacts. Below we address your questions and points raised:\n\n**Q1: Does the empirical evaluation in the paper provide a compelling argument?**\n\nYes, we believe so. The evaluation method used in the paper to obtain our results follows the practices established in the literature on synthetic tabular data generation (e.g., [1, 2, 3, 4]). Although we also believe that providing a robust and principled statistical hypothesis test for validating the synthetic data would be ideal, this is currently not the method of evaluation in this field, due to it being fundamentally hard. Due to the high complexity and dimensionality of the involved distributions, there is currently no gold-standard method that is tractable and reliable to a degree that would allow it to be adopted by the field of synthetic tabular data research for evaluation. The closest to this comes the comparison of low dimensional marginals, which is the basic building block of our method already. All in all, we believe that strong contributions can be made here that would benefit the community, but addressing this problem is beyond the scope of our work.\n\nIn more detail, we evaluate mainly using the downstream accuracy of a state-of-the-art classifier, and, as shown in the experiments in Appendix C, we also measure the TV-distance between the learned and the true k-way marginals, effectively calculating a low-dimensional approximation of the divergence between the true and the modeled distribution. Note that we measured both metrics for all conducted experiments, merely, for presentational brevity, we opted to include the accuracy metric in the main paper, due to its easy understanding, and higher interest for practitioners. Furthermore, a strong classifier is also a meaningful proxy summarizing how well basic to very high order correlations have been preserved in the synthetic sample with respect to the original data.\n\nAs for assessing the alignment of the synthetic data with the custom specifications, we directly measure their satisfaction rates/degrees on the generated synthetic samples, providing the closest metric of interest.\n\n**Q2: Would using different models to evaluate the quality of the data change the overall picture of ProgSyn\u2019s strong performance compared to other methods?**\n\nNo, and for the sake of demonstration, we have re-run the fairness experiment on Adult adding more classifiers. In the table below we compare the performance of ProgSyn to the next best non-private method TabFairGAN on the classifiers Logistic Regression (LogReg, linear model), SVM, Random Forest (RF, tree ensemble), XGBoost (XGB, gradient boosted tree-based, used in the paper), and CatBoost (CB, gradient boosted tree-based):\n\n|Classifier|ProgSyn [Acc. / Dem. Parity]|TabFairGAN [Acc. / Dem. Parity]|  \n|:---|:--:|:--:|  \n|LogReg|82.1 / 0.01|76.1 / 0.01|  \n|SVM|80.0 / 0.04|76.0 / 0.00|  \n|RF|81.4 / 0.01|78.8 / 0.02|  \n|XGB|82.1 / 0.01|79.8 / 0.02|  \n|CB|82.0 / 0.01|80.5 / 0.01|\n\nAs we can see, the overall picture is unchanged: ProgSyn prevails as the best method in terms of fairness-accuracy trade-off. Note that for LogReg and SVM, TabFairGAN is unable to produce data that is suitable for training and therefore results in classifiers that largely only predict the majority class for every instance, irrespective of any feature (including the protected feature, therefore such classifiers appear as fair, however are not useful as they are only able to assign every instance to the same class).\n\nFurther, note that [5] asserts that for evaluating synthetic tabular data, one should use a state-of-the-art classifier, as weaker models are unable to exploit the edge in distributional detail that better synthesization methods provide, painting a false picture.\n\nFinally, we do not test on recommendation models, as, again in line with prior work, the datasets used in this paper are classification datasets. Note that this is representative, especially in the context of our work, as none of the referenced methods in which ProgSyn is positioned examine recommendation datasets, but focus mainly on classification datasets (e.g., [1, 2, 3, 4, 5, 6, 7, 8, 9]). Also, as table 10 in [10] highlights, albeit in the context of federated learning, only 2 out of 61 tabular data applications concern recommendation systems."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7367/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700164969040,
                "cdate": 1700164969040,
                "tmdate": 1700164969040,
                "mdate": 1700164969040,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UI5Ev8bMjG",
                "forum": "KTL534o7Ot",
                "replyto": "H2C93mnd0D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7367/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7367/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "**Q5: \u201cCould you provide some information on the formulation process for statistical control?\u201d**\n\nYes, let us illustrate the technique using the following example:\n\n```\nENFORCE: STATISTICAL:\n\tE[2 * age - capital_gain | education == Doctorate] >= 500;\n```\n\nThe above specification consists of a conditional expectation in a comparison term. Our method first calculates the statistical operation and then combines them in the formula using t-norms and DL2 [11] primitives.\n\nProceedingly, let us first calculate the term `E[2 * age - capital_gain | education == Doctorate]`: here, the two involved features are `age` and `capital_gain`, therefore $\\mathcal{S} = \\\\{\\\\texttt{age}, \\\\texttt{capitalgain}\\\\}$ (following the notation in the paper). The operation is the expectation $E$ , the features are involved in the function $f(\\mathcal{S}: x, y) = 2 * x - y$ , and the condition $\\phi$ is `education == Doctorate`. First, we calculate the binary mask belonging to the condition following the technique described for logical constraints, obtaining $b_{\\phi}(\\hat{X})$. Then, we apply the mask to  the sample $\\hat{X}\\_{\\phi} = b_{\\phi}(\\hat{X}) \\odot \\hat{X}$, zeroing out all the rows where the condition does not apply. Using $\\hat{X}\\_{\\phi}$, we calculate the joint marginal of the `age` and `capital_gain` features, obtaining: $\\bar{\\mu}(\\\\texttt{age},\\\\texttt{capitalgain}, \\hat{X}\\_{\\phi})$, which is the empirical estimate of the distribution $p_{\\theta}(\\\\texttt{age}, \\\\texttt{capitalgain}|\\phi)$. Now, we can insert the obtained distribution into the mathematical definition of the expectation operation and obtain the value for our first term: \n\n$$t\\_1 = \\sum_{\\\\texttt{age}}\\sum_{\\\\texttt{captialgain}} (2 * \\texttt{age} + \\\\texttt{captialgain})\\, \\bar{\\mu}(\\\\texttt{age},\\\\texttt{capitalgain}, \\hat{X}\\_{\\phi})$$\nNotice that due to the careful steps taken, this process is fully differentiable, even though it involves conditioning and arithmetic expressions in discrete features.\n\nNext, we are left with the expression $t_1 > 500$ which is a logical expression of reals, for which we have to construct a differentiable loss term. Here we use the primitives introduced in [11] to construct a differentiable loss term:\n\n$$\\mathcal{L}\\_{\\\\texttt{stat}}(\\hat{X} \\leftarrow g\\_{\\theta}(z)) = max(0, 500 - t\\_1).$$\nNotice that the term is positive, and therefore adds a loss penalty only if $t_1$ is less than $500$, i.e., the desired constraint is not satisfied. Otherwise, when the constraint is met, the loss is zero.\n\n**Q6: Please clarify the organization of the introduction section, and address other comments regarding the formatting of certain elements in the paper.**\n\nWe believe that a good presentation in a paper is extremely important, therefore, we appreciate any feedback and comments on it, and are eager to incorporate them, further improving the paper.\n\n**Introduction:** We believe that the introduction section of our paper follows the common pattern of how introductions are and have to be structured, at least what is commonly adopted in the machine learning literature. We first start by setting a general scene to our problem (first paragraph), then we introduce the background and most related work, highlighting the research gap our work will target (the two paragraphs connected to \u201cSynthetic data\u201d), finally, we briefly introduce our method, structured in a short technical introduction, a working example, and a summary of our results. For some examples on this structure, see [4], [7], [12], [13], and [14], all published papers at prestigious venues. Nevertheless, we are always looking to improve our paper, therefore, if the reviewer has any concrete suggestions for improving the introduction, we would be highly appreciative of that.\n\n**Lengthy formulas:** Although due to the space restrictions of the paper format it will be unavoidable to sometimes have inline math formulas, we thank the reviewer for raising our awareness on readability concerns, and have uploaded a revised version of the paper.\n\n**Pseudocode:** In general, we believe that including excerpts of code in the paper, especially if it is core to its message, is not non-standard writing. Additionally, the referenced code snippet is not just mere pseudocode, but an actual example of how specifications can be defined and passed to ProgSyn, written in a domain specific language constructed by us for this task. If this has been unclear from the presentation in the paper, we are eager to improve it, and would be very grateful if the reviewer could share their concrete suggestions with us."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7367/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700165966568,
                "cdate": 1700165966568,
                "tmdate": 1700209329808,
                "mdate": 1700209329808,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gBiJDSFPBu",
            "forum": "KTL534o7Ot",
            "replyto": "KTL534o7Ot",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7367/Reviewer_Jzns"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7367/Reviewer_Jzns"
            ],
            "content": {
                "summary": {
                    "value": "The authors tackle the interesting problem of adding constraints to the synthetic data generation. They provide a framework where they consider both statistical and logical constraints arising from privacy, fairness and the domain. Experiments are conducted on real-datasets to showcase the benefits of their approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall:\n\nThe paper is well-written and easy to digest. The approach is simple and the experiments are convincing but the novelty factor is a bit missing. \n\nPros:\n\n(a) The problem setup is very timely and relevant to the literature and the community. The framework solves a real issue of generating high-quality synthetic data with constraints.\n(b) The experiments are extensive and showcase the framework in a wide variety of constraints while contrasting with the current state of the art approaches."
                },
                "weaknesses": {
                    "value": "Cons:\n\n(i) The main approach of fine tuning and adding differentiable constraints is relatively straightforward. \n(ii) The approach is not adaptive to changing the constraint set and is not even discussed in the paper."
                },
                "questions": {
                    "value": "1. How would you generate a variety of synthetic datasets with varying constraint specifications without retuning your model?\n\n\n* On the Constrained Time-Series Generation Problem  https://openreview.net/forum?id=KTZttLZekHa"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7367/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699199899425,
            "cdate": 1699199899425,
            "tmdate": 1699636881085,
            "mdate": 1699636881085,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1gbrN6JiTv",
                "forum": "KTL534o7Ot",
                "replyto": "gBiJDSFPBu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7367/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7367/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their acknowledgment of our timely and strong empirical contribution and quality of writing, their informative feedback, the informative resource provided, and their favorable overall assessment of the paper. Below we address their questions and points raised:\n\n**Q1: What novel technical and conceptual contributions does ProgSyn make?**\n\nAdditionally to ProgSyn\u2019s empirically demonstrated strong performance and high level of versatility, we firmly believe that ProgSyn brings both significant technical and conceptual novel contributions to the field of synthetic tabular data generation.\n\nConceptually, it is the first work to facilitate extended customizability of the generated data, allowing for row-wise, distributional, and downstream-dependent specifications. Our framework cannot only be used to improve the synthetic data quality, but also to hand-tailor its distribution and other properties to specific, custom use-cases defined by the user. At the same time, ProgSyn effectively captures the capabilities of prior works across different aspects of tabular data generation.\n\nTechnically, ProgSyn introduces several novel elements in the differentiable computation of the specification loss-terms, tackling several difficult challenges on the way. The common key challenge of each specification type arises from the fundamentally non-differentiable nature of categorical features in the produced tables. Once the conversion from complex constraints involving discrete features is achieved, using them as a regularizer in a two-staged optimization pipeline is indeed a natural approach for manipulating the modeled distribution to our liking. Nevertheless, note that ProgSyn makes its key technical contributions addressing the challenge of arriving at these differentiable loss terms (something that is for example in [1] in the domain of time series data is treated as a given). For instance, for logical constraints and conditions, we address this through our masking scheme introduced in Section 4. Further, we are the first to offer a technical solution to incorporate complex formulas involving conditional statistical expressions acting directly on the distribution of the data, going beyond row-wise constraints. We are also first to provide a general framework of incorporating downstream classifiers in the synthetic data generation training process; tackling the difficult additional challenges of training-in-the-loop, and uniting it with the rigorous requirements of differential privacy. Notably, this allowed us to set a new state-of-the-art on fair synthetic data generation, both in the non-private and the differentially private setting, while this objective not being the main focus of our work."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7367/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700164484559,
                "cdate": 1700164484559,
                "tmdate": 1700164484559,
                "mdate": 1700164484559,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]