[
    {
        "title": "GenSim: Generating Robotic Simulation Tasks via Large Language Models"
    },
    {
        "review": {
            "id": "HswuhN1oHy",
            "forum": "OI3RoHoWAN",
            "replyto": "OI3RoHoWAN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3906/Reviewer_LF4g"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3906/Reviewer_LF4g"
            ],
            "content": {
                "summary": {
                    "value": "In summary, this study introduces GenSim, a scalable framework harnessing large language models (LLMs) to enhance robotic policy simulation tasks. \n\nThe primary goal is to distill LLM capabilities into practical, low-level policies for robots. We delved into LLM finetuning and prompting methods, exploring both goal-directed and exploratory approaches to generate novel simulation task codes. \n\nThe study focused on table-top pick-and-place tasks, and addressing challenges in generating intricate robotic tasks remains an area for future exploration. Moreover, future research could explore algorithms for training multitask policies capable of efficiently adapting to these larger-scale generated task benchmarks, thereby paving the way for more sophisticated and realistic robotic simulations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors successfully leverage Large Language Models (LLMs) to tackle the issue of limited task-level diversity in robotic simulation. By tapping into the reasoning and coding capabilities of LLMs, the proposed GenSim framework autonomously designs and validates task asset arrangements and progressions, leading to the creation of over 100 tasks.\n\nSecondly, the paper presents a meticulous evaluation of the generated tasks using tailored metrics, ensuring the tasks' quality and achievability. The authors go further to compare different LLM models, including GPT4 and Code-Llama, and demonstrate that prompting and finetuning techniques based on the task library significantly enhance the LLMs' ability to generate tasks of higher quality. \n\nAdditionally, the paper showcases the practical impact of the generated tasks on language-conditioned visuomotor policies. The rigorous evaluation and comparison of different LLM models, and the tangible improvements demonstrated in the generalization capabilities of language-conditioned visuomotor policies."
                },
                "weaknesses": {
                    "value": "Although the paper discusses sim-to-real transfer briefly, there is a lack of in-depth evaluation regarding the effectiveness of policies trained on LLM-generated tasks in real-world robotic applications. Generating over 100 tasks is a positive step, but the computational resources, time, and effort required for this scale of task generation are crucial aspects, especially when considering real-world applications. Without addressing these scalability issues, the framework's practical utility might be limited.\n\nThe paper acknowledges the imperfections in the evaluation metrics used for code generation, including misaligned language descriptions. However, it does not delve deeply into the potential implications of these imperfections on the quality of the generated tasks. A more comprehensive analysis of how these imperfections affect task quality and subsequent policy training is essential.\n\nCompared with the vision modules, the language planning modules are much more sophisticated. The potential of this model is not fully unlocked."
                },
                "questions": {
                    "value": "Could you provide more details on the types of syntax errors and lack of grounding observed in the generated code? Understanding these specific issues would be valuable for refining the framework. Additionally, how do these errors impact the subsequent policy training and task performance?\n\nConsidering the limitations and challenges identified, what are the key areas of future research you are planning to explore? Are there specific aspects of GenSim that you aim to enhance or refine based on the feedback and insights gathered from this study?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3906/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698428651930,
            "cdate": 1698428651930,
            "tmdate": 1699636350762,
            "mdate": 1699636350762,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "d81xqMTCDb",
                "forum": "OI3RoHoWAN",
                "replyto": "HswuhN1oHy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3906/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3906/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your constructive feedback!"
                    },
                    "comment": {
                        "value": "Thank you for finding the topic of our work is rigorous and practical impacts, and appreciating our contribution. We have revised the manuscript and will address your questions below. \n\n> Although the paper discusses sim-to-real transfer briefly, there is a lack of in-depth evaluation regarding the effectiveness of policies trained on LLM-generated tasks in real-world robotic applications. Without addressing these scalability issues, the framework's practical utility might be limited.\n\nThanks for asking this and we see our work as a first step toward the direction of scalability in simulation and deployment in the real world. We have a full real-world experiment section to discuss how the pre-trained models in simulation might improve the real-world performance in both seen and unseen tasks. We have also added Appendix Section D.7  to discuss specifically this scalability issue. Moreover, we believe that the costs and performance of generating new tasks with foundation models will go down as LLM evolves.  \n\n> The paper acknowledges the imperfections in the evaluation metrics used for code generation, including misaligned language descriptions. However, it does not delve deeply into the potential implications of these imperfections on the quality of the generated tasks. A more comprehensive analysis of how these imperfections affect task quality and subsequent policy training is essential.\n\nThanks for asking this. For instance, misaligned language instructions and tasks can potentially lead to a good single-task policy but potentially can hurt when we conduct joint multitask policies (as it can become a data point with wrong labels). However, we have also observed some robustness in experiments. For instance, when we scale tasks pretraining from 5 to 70 tasks, we did not apply particular per-task noise filtering and the multitask policy seems to generalize well in our evaluation. Finally, we have added some discussions in Appendix Section D.5 and Section D.6 on concrete errors and limitations. \n\n\n>  Could you provide more details on the types of syntax errors and lack of grounding observed in the generated code? Understanding these specific issues would be valuable for refining the framework. Additionally, how do these errors impact the subsequent policy training and task performance?\n\nThere are low-level mistakes and high-level mistakes. The low mistakes can be simple syntax errors such as \u201cTypeError: can only concatenate str (not \"list\") to str'' and \u201cIndexError: arrays used as indices must be of integer (or boolean) type\u201d or failures to set the right goal for task completion. The high-level mistakes can be language misalignment and hallucination. Luckily, we have a full automatic mechanism of task filtering such that these tasks will likely be rejected during the task generation process, and therefore will not impact policy training. We have added concrete error messages (Figure 3) and provided more summaries as well as discussions on how LLM errors propagate to policy training in Appendix D.4 and D.5. \n\n> Considering the limitations and challenges identified, what are the key areas of future research you are planning to explore? Are there specific aspects of GenSim that you aim to enhance or refine based on the feedback and insights gathered from this study?\n\nThanks for asking this. We have mentioned some limitations and future works in both the main paper and the Appendix. There are three stages in this work, generating the simulation tasks, solving these simulation tasks, and distilling them into a policy. We believe that all of these three problems are in their infancy, and are actively exploring these directions. Automatically generating and scaling these tasks require careful engineering and insights into the LLM pipeline, which has lots of room for improvement. For instance, how to balance between diversity of the tasks and the success rates of generating these tasks. Solving these top-down pick and place tasks is not difficult, but extending to more general dexterous 6 dof tasks are also very challenging problems. Finally, once we have many tasks, how to efficiently distill them into a general policy is also a research question.  \n\nThank you very much again for your constructive feedback. Please do not hesitate to let us know if you have any other questions and/or comments."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3906/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700364766791,
                "cdate": 1700364766791,
                "tmdate": 1700404861098,
                "mdate": 1700404861098,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qKzVMkBiY5",
                "forum": "OI3RoHoWAN",
                "replyto": "d81xqMTCDb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3906/Reviewer_LF4g"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3906/Reviewer_LF4g"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. \n\nThanks for your efforts in this area."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3906/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674685133,
                "cdate": 1700674685133,
                "tmdate": 1700674685133,
                "mdate": 1700674685133,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QPc8iZFKCT",
            "forum": "OI3RoHoWAN",
            "replyto": "OI3RoHoWAN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3906/Reviewer_hwME"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3906/Reviewer_hwME"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a framework for generating new tasks given a fixed number of existing tasks by leveraging the code generation capability of LLMs for robotics simulation environments. It further uses the generated over 100 tasks for training multi-task policies and evaluates them in both simulation and real world. Stronger generalization is observed by training across more diverse tasks. The idea is novel and profound for breaking the hardness of creating new robot learning tasks in simulation."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The task-level diversity is indeed a hard problem due to the required human efforts and this paper proposes a promising approach for solving it, by a novel usage of LLMs. It verifies the effectiveness of this approach and is inspiring.\n\nThe strong generalization of the multi-task policies even with zero-shot transfer on new tasks is impressive. \n\nThe paper is well-written.\n\nThe experiments are thorough for verifying the generalization capability of the policies."
                },
                "weaknesses": {
                    "value": "Most of the current tasks are within the domain of top-down pick and place, and the generalization of policies within such a domain is relatively easy. It could be more impressive to see more dexterous manipulation tasks and policy generation over those.\n\nFor the failure cases of task generation, perhaps providing more examples will draw a better boundary on the limitation of the current approach, or for each model."
                },
                "questions": {
                    "value": "In Sec. 3.1, for exploration task generation, which LLM is used? Is it fine-tuned or GPT models without fine-tuning? Also for the generation of 120 tasks, are the newly generated tasks used for fine-tuning as well or just as prompt examples? Please provide more explanations.\n\nFor Fig. 6, it seems GPT-4 without fine-tuning outperforms all other fine-tuned models, so is it true that using fine-tuned smaller models is just for consideration of inference cost?\n\nAs mentioned in Sec. 1 the task library is initialized with only 10 tasks, for fine-tuning the LLMs to gain context of robotic simulation tasks, is the overfitting to these 10 tasks a problem? If so, how is it handled?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3906/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698534245346,
            "cdate": 1698534245346,
            "tmdate": 1699636350660,
            "mdate": 1699636350660,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ES2kOGip2z",
                "forum": "OI3RoHoWAN",
                "replyto": "QPc8iZFKCT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3906/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3906/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your constructive feedback!"
                    },
                    "comment": {
                        "value": "Thank you for finding the topic of our work inspiring and impressive, and appreciating our contribution. We have revised the manuscript and will address your questions below. \n\n\n>  Most of the current tasks are within the domain of top-down pick and place, and the generalization of policies within such a domain is relatively easy. It could be more impressive to see more dexterous manipulation tasks and policy generation over those.\n\nWhile the domain of top-down pick-and-place, especially short-horizon tasks, are *seemingly* not challenging to most roboticists, we note that the  **task-level policy generalization** explored in GenSim, such as long-horizon tasks and compositional tasks, remains under-explored and non-trivial. For example, \"build-a-car\" is a nontrivial task that requires semantic understanding of natural language instructions and more than 10 steps of precise pick-and-place and stacking. We are actively experimenting with extending our pipeline to handle more challenging tasks such as dexterous, 6-dof, contact-rich tasks.\n\n\n> For the failure cases of task generation, perhaps providing more examples will draw a better boundary on the limitation of the current approach, or for each model.\n\nThanks for asking this.  Overall, we acknowledge that even with the strongest model GPT-4 in the current approach, there are still gaps in achieving generation of infinite *high-quality* tasks without any human efforts, due to **the bottleneck of grounding and the hallucination** problems in LLMs at its current state. We have added detailed error messages to Appendix D.4. The GPT-4 model sometimes hallucinates concepts such as \u201cboundary\u201d and \u201cascending size\u201d without implementing these in code. For weaker models such as Code-LLama and GPT-3.5, the gap from GPT-4 is in generating creative and complex tasks, and also in correctly implementing them in code. \n\n\n> In Sec. 3.1, for exploration task generation, which LLM is used? Is it fine-tuned or GPT models without fine-tuning? Also for the generation of 120 tasks, are the newly generated tasks used for fine-tuning as well or just as prompt examples? Please provide more explanations.\n\n**GPT-4 without finetuning** is used in the exploration task generation to generate the task library. This is because the base GPT-4 model is the current strongest LLM model (See Figure 6 right) and has the desired creativity and reasoning ability to generate new tasks in exploratory task generation stage. In addition to being retrieved in the prompts for GPT-4, these 120 tasks are also used for finetuning the open-source model such as Code LLama and closed-source model such as GPT 3.5. However, as shown in Figure 6 right, even fine-tuning the GPT3.5 or codellama with 120 tasks, the success rate cannot outperform GPT4. Therefore, we choose GPT-4 as the base model to generate gensim tasks.\n\n> For Fig. 6, it seems GPT-4 without fine-tuning outperforms all other fine-tuned models, so is it true that using fine-tuned smaller models is just for consideration of inference cost?\n\nLower costs, because of fewer prompt tokens and cheaper models, and better efficiency, because of more structured and domain-specific output, are important motivations for finetuning LLMs. Moreover, we show that scaling up simulation tasks can also work well with open-source LLM models, where we have access to the model itself and potentially can be used for many other purposes. This finetuning process also enables self-instruct improvements as the task library is expanding.\n\n> As mentioned in Sec. 1 the task library is initialized with only 10 tasks, for fine-tuning the LLMs to gain context of robotic simulation tasks, is the overfitting to these 10 tasks a problem? If so, how is it handled?\n\nIn finetuning experiments, we used **all the 100 tasks** from the generated task library from GPT-4 to finetune LLMs. Indeed, during the LoRA finetuning process, we have observed some overfitting problems. We expect further scaling up training tasks to conduct self-improvement will improve this issue.\n\nThank you very much again for your constructive feedback. Please do not hesitate to let us know if you have any other questions and/or comments."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3906/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700364665207,
                "cdate": 1700364665207,
                "tmdate": 1700364811505,
                "mdate": 1700364811505,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xlYB943Ey9",
                "forum": "OI3RoHoWAN",
                "replyto": "ES2kOGip2z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3906/Reviewer_hwME"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3906/Reviewer_hwME"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response.\n\nThis is an essential contribution of scaling-up experiments for policy generalization in table-top manipulation."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3906/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674318549,
                "cdate": 1700674318549,
                "tmdate": 1700674318549,
                "mdate": 1700674318549,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yc3yaNAF5c",
            "forum": "OI3RoHoWAN",
            "replyto": "OI3RoHoWAN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3906/Reviewer_1vRz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3906/Reviewer_1vRz"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the problem of automatically generating robotic tasks in synthetic simulation environments. A method relying on LLMs is presented, where a LLM is used to generate the code that describes a task (assets selection, initial configuration of the assets, and language/robot motion instructions). A task library is initialized using 10 examples written by a human, and the LLM is asked to generate new tasks given carefully designed prompts. The generated tasks are added to the task library if they pass validation tests (e.g. the syntax is correct, the success rate of a policy trained on this task is high enough, it passes some human inspection etc), or the feedback is explicitly given to the LLM as additional instructions in a few-shot prompting scheme. 120 tasks are generated and policies are pre-trained on these tasks. The experimental evaluation shows that pre-training on a large number of tasks increases zero-shot generalization performance on new tasks (held-out test set also generated) in both simulation and real environments after fine-tuning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "S1. The problem addressed in this paper is important. Multi-task robot policies strongly rely on diverse training data (in terms of environments and task) but access to such training data is limited by real collection efforts, or having to manually invent and design new tasks. Automatically generating such training data could have a significant impact.\n\nS2. The overall method is technically sound. The appendix provides many details. The details of the prompts which are provided to the LLM is appreciated. Their design design is key as demonstrated in Fig 6 left). However, additional intuition why the two-stage prompt chain is better than the single-prompt would be appreciated.\n\nS3. The evaluation of Table 1 is convincing. In this experiment, multiple models pre-trained on synthetic data are fine-tuned using few real demonstrations. The models pre-trained on 70 generated tasks performs significantly better than a model pre-trained on 50 generated tasks and on the 10 original tasks from CLipport. This experiment demonstrates the practical applications of the proposed approach."
                },
                "weaknesses": {
                    "value": "W1. Figure 3 mentions that human inspection of a generated task is required. Was feedback collected for each of the generated tasks ? How many tasks were manually annotated in total. In addition, what is the granularity of feedback provided by the human ?\n\nW2. The experiments presented in Table 1 use a different set of tasks than the real tasks used to evaluate CLIPort. In particular, the tasks do not involve sweeping, or folding of objects. The variety of asset is also smaller than what is shown in CLIPort where scissors or chess pieces are manipulated. A similar observation is made regarding the synthetic experiments, e.g. the held-out simulation tasks used for evaluation mostly use blocks and omit more complex objects like ropes. Additional explanations on how these tasks were selected are needed to convince the readers that the method indeed generalizes to a wide variety of tasks. Experiments presented in Fig 6 left) evaluate models on the same tasks as CLIPort but it is not clear if the models were pre-trained on all generated tasks in this experiment (see W3.)\n\nW3. The \u201cfew-shot policy generalization to related tasks\u201d experiment needs additional description to understand the meaning of \u201csingle-original, single+2, single+4 etc\u201d. Were the models pre-trained on original clipport tasks, original clipport tasks + 2 generated tasks etc ? Or are the models pre-trained on all CLIPort tasks + all the ones that were generated automatically ? It is important to clarify this experiment as the current version lets the reader think that only a small subset of of the generated tasks were used to pre-train the policy, and the question of how these were selected arises.\n\nW4. Page 6 mentions \u201cWhen only pretraining on the 10 tasks in clipport, the policy does not generalize well on the GPT4 tasks.\u201d In fig 7 right) the model pre trained on tasks generated with LLMs is better than the one trained on CLipport tasks, but the difference is 15-20% of relative improvements, the claim should therefore be mitigated. It is also suggested to justify how the set of testing tasks (given in Appendix A.1) was selected. See also W2.\n\n W5. Details of the metrics used in Fig 6 would be appreciated. In particular, how is \u201ccode reasoning capability\u201d measured in \u201cruntime-verified\u201d, and is the \u201ctask completed\u201d an average over several environment resets ?"
                },
                "questions": {
                    "value": "The paper addresses an important problem and the method is technically sound. The experiments demonstrate that policies pre-trained on the automatically generated tasks can generalize to novel tasks in both synthetic and real environments. I am however concerned about the diversity of the tasks which the method is evaluated on. In particular, the evaluation tasks seem to be less varied compared to the ones used in CLIPort (W2). I am also questioning the \u201cautomatic\u201d nature of the method as human intervention is required (W1). I will gladly update my review if the authors can address these concerns and respond to my questions (see weaknesses)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3906/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3906/Reviewer_1vRz",
                        "ICLR.cc/2024/Conference/Submission3906/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3906/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698772869680,
            "cdate": 1698772869680,
            "tmdate": 1700488420584,
            "mdate": 1700488420584,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nyraIwkhXF",
                "forum": "OI3RoHoWAN",
                "replyto": "yc3yaNAF5c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3906/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3906/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Title: Thank you for your constructive feedback! (1)"
                    },
                    "comment": {
                        "value": "Thank you for finding the topic of our work is an important problem and the method is technically sound, and appreciating our contribution. We have revised the manuscript and will address your questions below. \n\n\n> Additional intuition why the two-stage prompt chain is better than the single-prompt would be appreciated.\n\nCreating simulation task descriptions and implementing the code for the tasks require different language capabilities. For creating the task, we would like to generate tasks as *creative and interesting* as possible (with high temperature for decoding) while being physically feasible, so it tests the language reasoning capabilities of LLM. For implementing the task code in simulation, it requires *rigorous coding execution* (with zero temperature for decoding) and needs to be thoughtful in the details of syntax and goal generation. Therefore a two-stage prompt chain with two subtasks is favored. We also have an ablation study on the task performance generated by single-shot and two-stage prompts in Figure 6 to show that a two-stage prompt chain with few-shot examples and task library can effectively improve the code generation success rates.\n\n\n>W1. Figure 3 mentions that human inspection of a generated task is required. Was feedback collected for each of the generated tasks ? How many tasks were manually annotated in total. In addition, what is the granularity of feedback provided by the human ?\n\nWe note that human inspection is *not required* for each task and the effort of annotation only involves watching rendered videos (without the need for checking code).  First, around 90% of erroneous tasks can be filtered out via automatic metrics (e.g., execution errors). Second, for the rest of the tasks, humans only need to check whether the simulated tasks are correct or not by checking whether the rendered videos correspond to the natural language description of the tasks.  In our experiments, LLMs generate around *200* tasks in total, and humans (i.e. the authors) checked and filtered around 150 tasks and kept 100 tasks.  Our new experiment below also shows that the human filtering rate is around 50%.\n\nDuring the response period, we assess human effort (time and pass rates) quantitatively by re-generating 30 tasks for human feedback. In Table 3 (attached below, also in Appendix Section D.6), we find that the required human efforts are around 10 seconds (for CS students) on average for each task. Note again that this is 10% auto-filtered tasks and the amounts of total inspectations are much smaller than all generated tasks. For future work, we are also exploring whether non-CS humans can check the correctness so that we can scale up the process of collecting human feedback. \n\n| Task       | User 1 Time | User 1 Pass | User 2 Time |User 2  Pass | User 3 Time |User 3 Pass |\n|------------|------------|------------|------------|------------|------------|------------|\n| Average    | 12.83 | 0.8 | 5.11  | 0.63 | 17.4 | 0.41 |\n\nYou can also get a sense of how much effort is needed for verification by trying out our Gradio demo (https://huggingface.co/spaces/Gen-Sim/Gen-Sim)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3906/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700364289243,
                "cdate": 1700364289243,
                "tmdate": 1700404298147,
                "mdate": 1700404298147,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JVE4N5gD3p",
                "forum": "OI3RoHoWAN",
                "replyto": "yc3yaNAF5c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3906/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3906/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your constructive feedback! (2)"
                    },
                    "comment": {
                        "value": "> W2. The experiments presented in Table 1 use a different set of tasks than the real tasks used to evaluate CLIPort. In particular, the tasks do not involve sweeping, or folding of objects. The variety of assets is also smaller than what is shown in CLIPort. A similar observation is made regarding the synthetic experiments, e.g. the held-out simulation tasks used for evaluation mostly use blocks. Additional explanations on how these tasks were selected are needed.\n\n\n We have conducted additional experiments to verify that our framework can also extended to more varied tasks and assets as suggested. In simulation, we evaluate new tasks such as **separating-piles**.\n|            | single task+0     | Single task + 2    | Single task +4    |\n|------------|------------|------------|------------|\n| Average    | 29.7 | 40.1| 49.3 |\n\nAs shown in the above table, the experiment results are also consistent. The GPT-generated tasks demonstrate efficacy across a more diverse array of objects. \n In the real world, we have also evaluated novel tasks with assets such as fruits and spoons.\n|            |   organize-kits  | sort-garbage  | place-bread   |\n|------------|------------|------------|------------|\n| Average    | 60.0 | 40.0 | 30.0 |\n\nWe have updated Figure 8 and Table 1 in the main paper to reflect the new experiments, we showed that training and evaluating on more diverse tasks such as **spoons and piles** are also possible, while the main bottleneck is the hardware. Given our time budget, we showed that the policies pre-trained from 70 GenSim tasks can outperform policies pre-trained from 10 CLIPort tasks and training from scratch. Due to the limited time and hardware limitations (only suction cup), we chose these tasks (see [website](gen-sim.github.io) for visualizations), and we believe our pipeline and pretrained weights can be useful for more tasks.\n\nThere are two reasons behind our task selection: task-level generalization and sim-to-real transfer.\n\n1. We highlight that we did not focus our evaluation on diverse assets such as rope, because we focus more on *task-level generalization*. We consider \u201cbuild-a-car\u201d to also be a challenging task that involves. We note that in *Table 2*, we have already shown performance on tasks on \u201cmove-piles-along-line\u201d and \u201cmanipulating-two-ropes\u201d. We believe that when scaling is improved, we will see more interesting generalizations to both diverse tasks and scenes due to the robustness of ``common-sense`` representations. \n\n2. We point out that our experiment focuses on pretraining on large simulation data and transferring it to the real world. For CLIPort, it only focused on sim-to-sim and real-to-real for demonstration purposes. In fact, in Table 1, similar zero-shot sim-to-real baselines will have 0 success rates due to the perception gaps. However, we are interested in evaluating novel-task generalization in simulation and conducting sim-to-real by using policies bootstrapped from the simulation. We also use a different hardware than the CLIPort (we use xARM with a suction cup), so dexterous tasks with very complex geometry in the real world can be difficult. \n\n> Experiments presented in Fig 6 left) evaluate models on the same tasks as CLIPort but it is not clear if the models were pre-trained on all generated tasks in this experiment (see W3.)\n\nThanks for asking. We would like to know clarifications on the Figures. It looks like that you might be referring to *Figure 7* since *Figure 6* does not include CLIPort experiments. We do not pre-train on the evaluation tasks or pre-train on any tasks in this few-shot generalization experiment. Moreover, the GPT-generated training tasks are selected based on task code embeddings produced by LLM model \u201ctext-embedding-ada-002\u201d. Please feel free to clarify the question and let us know if this needs a follow-up. \n\n> W3. The \u201cfew-shot policy generalization to related tasks\u201d experiment needs additional description to understand the meaning of \u201csingle-original, single+2, single+4 etc\u201d. Were the models pre-trained on original clipport tasks, original clipport tasks + 2 generated tasks etc? Or are the models pre-trained on all CLIPort tasks + all the ones that were generated automatically? It is important to clarify this experiment.\n\nThanks for your suggestion.  We have added additional descriptions for Figure 7 and the few-shot policy generalization section in the main paper. The x-axis denotes single CLIPort task + N GPT tasks where N=0,2,4 and multiple CLIPort tasks + N GPT tasks where N=0,4,8. We use the GPT embedding to pick related tasks from the evaluation tasks. The goal of the experiment is to see if the additional related tasks and data generated by GPT-4 can be used to improve the training performance. We clarify that the models are not pre-trained on any tasks. This experiment does not have large-scale pre-training, we train few-shots on CLIPort (single/multiple)+N tasks from GenSim and evaluate the CLIPort tasks."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3906/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700364451085,
                "cdate": 1700364451085,
                "tmdate": 1700404592194,
                "mdate": 1700404592194,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EEdh8d0TJK",
                "forum": "OI3RoHoWAN",
                "replyto": "yc3yaNAF5c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3906/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3906/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your constructive feedback! (3)"
                    },
                    "comment": {
                        "value": "> W4. Page 6 mentions \u201cWhen only pretraining on the 10 tasks in CLIPort, the policy does not generalize well on the GPT4 tasks.\u201d In fig 7 right) the model pretrained on tasks generated with LLMs is better than the one trained on CLipport tasks, but the difference is 15-20% of relative improvements, the claim should therefore be mitigated. It is also suggested to justify how the set of testing tasks (given in Appendix A.1) was selected. See also W2.\n\nWe acknowledge that the claim on the performance of pre-training on CLIPort tasks should be mitigated and we have removed that sentence. However, we would like to point to the additional results, which state that pre-trained CLIPort tasks indeed only have limited zero-shot generalization in our simulation experiments (only 1.6% success rate among 4 GPT generated tasks). We also note that the right subfigure in Figure 7 is few-shot joint training instead of zero-shot generalization. The tasks in Appendix A.1 are used for evaluating LLM task generation across different LLMs (from GPT-3 finetuned to GPT-4). The set of testing tasks is not particularly selected.  \n\n\n> W5. Details of the metrics used in Fig 6 would be appreciated. In particular, how is \u201ccode reasoning capability\u201d measured in \u201cruntime-verified\u201d, and is the \u201ctask completed\u201d an average over several environment resets ?\n\nThere are two levels of mistakes that LLMs make. The metric mostly measures low-level mistakes. Our metric has an incremental structure from syntax to runtime to successfully generate demonstrations where failing the former metrics implies failing the latter metrics. \u201cRuntime verified\u201d measures if the code can be runnable without any runtime problems such as out of bound, \u201cTask completed\u201d is computed as 1 if we have 3 trials managing to generate demonstrations over 5 trials (sometimes there could be simulation issues). Therefore, they test whether the LLM can successfully write the code to generate demonstrations. Our additional human experiments also measure that the human pass rate is around 0.5. We also note that evaluating LLM judiciously is still an open research question. \n\nI am however concerned about the diversity of the tasks which the method is evaluated on. In particular, the evaluation tasks seem to be less varied compared to the ones used in CLIPort (W2). I am also questioning the \u201cautomatic\u201d nature of the method as human intervention is required (W1). I will gladly update my review if the authors can address these concerns and respond to my questions (see weaknesses).\nWe have conducted additional experiments for these questions (please also check the main paper). We summarize our answers to these questions below and please see our full answers to W1-W5 above:\nDiversity of task evaluation: \nWe have conducted additional experiments to verify that our framework can also be extended to more varied tasks as suggested. In the simulation, we evaluate new tasks such as \"separating-piles\". In the real world, we have also evaluated novel tasks with assets such as fruits and spoons. Please see our detailed answers with experiments to W2. Our additional experiment shows that the policy can also generalize to diverse tasks and assets.\n \nFor task selection rationale, we emphasize that the evaluation tasks are selected to focus on sim-to-real transfer and task-level generalization. For simulation, we choose interesting long-horizon tasks such as build-a-car over diverse assets such as rope and kits in both task generation and policy evaluations. However, we note that we have evaluated on manipulating-two-ropes and and move-piles-along line in table 2. For the real world, we are interested in sim-to-real transfer (in contrast to sim-to-sim and real-to-real in CLIPort) and are limited by the real-world hardware (suction cup) and thus focus on tasks involving simple structures. Finally, we want to highlight that we are still limited by compute budgets, where each experiment can take a week. \n\nAutomatic nature and human effort: please see our general response and response to your W1.\n\nThank you very much again for your constructive feedback and insightful questions. We appreciate any follow-up questions or comments."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3906/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700364605072,
                "cdate": 1700364605072,
                "tmdate": 1700405127550,
                "mdate": 1700405127550,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "v0cYIdq277",
                "forum": "OI3RoHoWAN",
                "replyto": "yc3yaNAF5c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3906/Reviewer_1vRz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3906/Reviewer_1vRz"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed explanations and additional experiments.\n\nMy concerns regarding:\n- The human efforts required to annotate the generated tasks.\n- The selection of tasks for the experiments in Fig 7.\n- The variety of tasks.\n\nhave been addressed. I now better understand that the paper focuses on task generalization across long-horizon tasks, and that the real experiments are limited by the available hardware. Thanks also for pointing to the experiments in Table 2 which indeed already evaluate the method for rope manipulation tasks.\n\nSeveral clarifications have been also added to the paper. Finally, I suggest to clarify the following sentence in the paper.\n> The set of testing tasks is not particularly selected. \n\nFollowing the authors response, I have increased the rating of my review and recommend to accept this paper."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3906/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700488372566,
                "cdate": 1700488372566,
                "tmdate": 1700488399863,
                "mdate": 1700488399863,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PVckJUssoG",
            "forum": "OI3RoHoWAN",
            "replyto": "OI3RoHoWAN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3906/Reviewer_XopG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3906/Reviewer_XopG"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a methodology to automatically generate tasks in simulation and corresponding expert policies to increase the task diversity when training robot policies. Existing large language models are utilized for this, and a number of ablations are conducted to study various design choices of the proposed method. Experimental evaluation both in simulation and in the real world reveals that the proposed methodology leads to better generalization than the baseline."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- Interesting idea. The proposed idea is very interesting and novel.\n- Well-written. The paper is very well-written and easy to follow. The figures very much aid in understanding the paper.\n- Experimental Evaluation. The experiments reveal that the proposed methodology outperforms baseline methods by non-trivial margins. In particular, the proposed method enables both few-shot policy generalization to related tasks and zero-shot policy generalization to unseen tasks. The experiments are conducted in both simulation and in the real world, which further speak to the strength of the proposed method.\n- Ablations. Ablations are conducted to better understand various design choices of the proposed methodology. Visualizations and qualitative results also aid in this."
                },
                "weaknesses": {
                    "value": "- Simplistic tasks. The experimentation is limited to table-top pick-and-place task. Furthermore, the use of a suction gripper is a weakness -- this simplifies the already simple task, and limits generalizability to other kinds of tasks. Unclear whether the proposed methodology can be adapted for more complex tasks, and how well it would do. Experiments on more complex tasks would be more convincing.\n- Experimental Evaluation. While a diverse set of experiments are conducted, only one setting (ie environment) is adopted. The results would be more convincing if the presented trends were shown to hold true across a number of settings."
                },
                "questions": {
                    "value": "- A number of approaches exist in the literature for data augmentation using foundation models (e.g. ROSIE, CACTI, etc.). How do such approaches compare to the proposed methodology when it comes to generalization to unseen tasks? Arguably, some of these are simpler to implement and use.\n- What is done in the cases where the output of the LLM has a syntactical error and cannot be run?\n- How much prompt engineering went into this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3906/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3906/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3906/Reviewer_XopG"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3906/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827863422,
            "cdate": 1698827863422,
            "tmdate": 1699636350505,
            "mdate": 1699636350505,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kytsEnRq0N",
                "forum": "OI3RoHoWAN",
                "replyto": "PVckJUssoG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3906/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3906/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your constructive feedback!"
                    },
                    "comment": {
                        "value": "Thank you for finding the topic of our work is of increasing importance, and appreciating our contribution. We have revised the manuscript and will address your questions below. \n\n> Simplistic tasks and Experimental Evaluation. The experimentation is limited to table-top pick-and-place task.While a diverse set of experiments are conducted, only one setting (ie environment) is adopted. The results would be more convincing if the presented trends were shown to hold true across a number of settings.\n\nWhile the domain of top-down pick-and-place, especially short-horizon tasks, are *seemingly* not challenging to most roboticists, we note that the  **task-level policy generalization** explored in GenSim, such as long-horizon tasks and compositional tasks, remains under-explored and non-trivial. For example, \"build-a-car\" is a nontrivial task that requires semantic understanding of natural language instructions and more than 10 steps of precise pick-and-place and stacking. Such task-level generation cannot be achieved by previous methods without LLMs, as future work we plan to extend our work to handle more challenging tasks such as dexterous, 6-dof, contact-rich tasks.\n\n\n> A number of approaches exist in the literature for data augmentation using foundation models (e.g. ROSIE, CACTI, etc.). How do such approaches compare to the proposed methodology when it comes to generalization to unseen tasks? Arguably, some of these are simpler to implement and use.\n\nThanks for bringing up these interesting works; we will include these references in the related work section. From our understanding, ROSIE and CACTI focus more on **scene-level generalization** on a single task, utilizing image diffusions for data augmentation. However, in our work, we focus on **task-level** generalization with LLM for generating more tasks. As these works are orthogonal to our approach, it would be interesting to explore a combination of both to achieve generalization across unseen tasks in diverse scenes.\n\n>  What is done in the cases where the output of the LLM has a syntactical error and cannot be run?\n\nOne of the properties of coding simulation tasks is that we can just execute the robot simulation and check the results. Therefore, if the output of LLM is non-executable, we will do **rejection sampling** and move to the next trial of LLM generation. We have added a new Appendix Section D.5 on the feedback in GenSim. Feel free to check that section out.\n\n> How much prompt engineering went into this?\n\nDuring the LLM pipeline development process, we went through prompt engineering, retrievals and reflections, and finetuning and used techniques such as chain-of-thoughts and few-shot prompting. The final version of the prompt is *not* very complicated and contains two subtasks of generating the task descriptions and the task code (Figure 3). Figure 6 Left shows the necessity of this subtask separation in prompt chains to give LLM enough \u201ctime\u201d to think. The prompts are attached in Appendix Section D.2. One intuition is that creating simulation task descriptions and implementing the code for the tasks require different language capabilities. For creating the task, we would like to generate tasks as creative and interesting as possible (with high temperature for decoding) while being physically feasible, so it tests the language reasoning capabilities of LLM. For implementing the task code in simulation, it requires rigorous coding execution (with 0 temperature for decoding) and needs to be thoughtful in the details of syntax and goal generation. In summary, the two subtask pipeline is important and this can avoid complicated prompt engineering.\n\nThank you very much again for your constructive feedback. Please do not hesitate to let us know if you have any other questions and/or comments."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3906/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700364171727,
                "cdate": 1700364171727,
                "tmdate": 1700364845334,
                "mdate": 1700364845334,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]