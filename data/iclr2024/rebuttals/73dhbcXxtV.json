[
    {
        "title": "LOLAMEME: LOGIC, LANGUAGE, MEMORY, MECHANISTIC FRAMEWORK"
    },
    {
        "review": {
            "id": "rS4sgdb7gT",
            "forum": "73dhbcXxtV",
            "replyto": "73dhbcXxtV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2772/Reviewer_JYqN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2772/Reviewer_JYqN"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel framework called LOLAMEME including two instantiations, the LoLa language, and MeMe languages. Then it introduces a hybrid architecture T HEX and compares it with transformer-based GPT-2 and convolution-based Hyena based on the LOLAMEME framework. Furthermore, this work conducts comprehensive experiments to demonstrate the effectiveness of T HEX in different tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The new framework LOLAMEME similar to natural language is impressive and interesting.\n2. This work builds multiple datasets with several billion tokens based on the LOLAMEME framework, which would contribute to future research.\n3. This work performs comprehensive experiments over these datasets and a related benchmark dataset to show the effectiveness of the new framework."
                },
                "weaknesses": {
                    "value": "1. The motivation for the model design is not clearly discussed in this work. I am confused about the differences among T HEX, GPT-2, and Hyena. \n2. The structure of this paper is not clear enough, which is very hard to follow.\n3. I would suggest that an illustration figure be provided to clearly show the main idea of the LOLAMEME framework, which will make this work easier to understand.\n4. Some sentences should be revised and the format should be unified. For instance, in the Abstract, \"We extend current mechanistic schemes to incorporate Logic, memory, and nuances of Language such as latent structure\", I am curious why the first letter of Logic and Language in this sentence are capitalized."
                },
                "questions": {
                    "value": "1. What are the differences among T HEX, GPT-2, and Hyena? \n2. What are the advantages of T HEX? \n3. Why not provide an illustration figure to show the model framework?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2772/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817351097,
            "cdate": 1698817351097,
            "tmdate": 1699636220237,
            "mdate": 1699636220237,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "H8EI9oJhrc",
                "forum": "73dhbcXxtV",
                "replyto": "rS4sgdb7gT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2772/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2772/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thorough review and valuable feedback on our paper. We appreciate the opportunity to address your concerns and clarify some aspects of our work.\n\n1. What are the differences among T HEX, GPT-2, and Hyena?\n    1. In the paper, we use GPT-2 model which uses attention operator, Hyena model which uses hyena operator and T-Hex which is similar to hyena model in which 1 layer of hyena operator is replaced with attention operator. \n2. What are the advantages of T HEX?\n    1. Using LOLAMEME framework, we tested the GPT-2 and hyena model architectures. We identified various areas where GPT-2 using attention operator works better and Hyena using hyena operator works better. Guided by the experiment results using LOLAMEME framework, we show that attention and hyena have complementary strengths using T-HEX. Using attention operator together with hyena operator, boosts the model architecture\u2019s performance on various aspects such as memorization, in-context learning, language mixing, longer input length and more listed in the paper. \n    2. We show T-Hex performs better than Hyena and GPT-2 on various aspects of the language and we also show results on existing dataset \u201cListops\u201d. Using LOLAMEME framework, better architectural design changes can be developed and tested on various aspects of the language. \n3. Why not provide an illustration figure to show the model framework?\n    1. Thanks for the feedback, we will definitely add model architecture as well as LOLAMEME framework illustration in the next revision. This is the anonymous link to a draft of LOLAMEME framework illustration:  https://ibb.co/qWCPRnW . We will add a revised version to the paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2772/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700701891325,
                "cdate": 1700701891325,
                "tmdate": 1700701891325,
                "mdate": 1700701891325,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0IXhuVaaLJ",
            "forum": "73dhbcXxtV",
            "replyto": "73dhbcXxtV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2772/Reviewer_5Z2e"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2772/Reviewer_5Z2e"
            ],
            "content": {
                "summary": {
                    "value": "This paper talks a lot about mechanistic interpretability and evaluation of models with different architectures on synthetic benchmarks to generate more understanding of how they work. I fail to understand completely what insight is gained here."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "."
                },
                "weaknesses": {
                    "value": "The only changes done to the transformer architecture is to replace a single layer by a layer from the hyena model. The variations include only replacing a different layer of the transformer with the same hyena layer. Lots of experiments are done to compare the performance of variants and measure the impact on the quality under different input lengths, on some synthetic datasets, etc. But I don't see any insight that could be won from these experiments."
                },
                "questions": {
                    "value": "None."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2772/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698964265910,
            "cdate": 1698964265910,
            "tmdate": 1699636220151,
            "mdate": 1699636220151,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "B0wCkEWaDz",
                "forum": "73dhbcXxtV",
                "replyto": "0IXhuVaaLJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2772/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2772/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thorough review and valuable feedback on our paper. We appreciate the opportunity to address your concerns and clarify some aspects of our work.\n\n\n1. Natural languages has several aspects to it and it has become extremely difficult to test a model architecture on one or a set of aspects of the natural language. LOLAMEME framework is built to evaluate model architectures on various aspects of language such as grammer, vocabulary, logic/reasoning, memorization, multiple languages and more listed in the paper.\n2. Using LOLAMEME framework, we tested the GPT-2 and hyena model architectures. We identified various areas where GPT-2 using attention operator works better and Hyena using hyena operator works better. Guided by the experiment results using LOLAMEME framework, we show that attention and hyena have complementary strengths using T-HEX. Using attention operator together with hyena operator, boosts the model architecture\u2019s performance on various aspects such as memorization, in-context learning, language mixing, longer input length and more listed in the paper. \n3. We show T-Hex performs better than Hyena and GPT-2 on various aspects of the language and we also show results on existing dataset \u201cListops\u201d. Using LOLAMEME framework, better architectural design changes can be developed and tested on various aspects of the language."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2772/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700701879839,
                "cdate": 1700701879839,
                "tmdate": 1700701879839,
                "mdate": 1700701879839,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "n6PUXYc1HN",
            "forum": "73dhbcXxtV",
            "replyto": "73dhbcXxtV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2772/Reviewer_iHzK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2772/Reviewer_iHzK"
            ],
            "content": {
                "summary": {
                    "value": "This article introduces a novel framework called LOLAMEME that expands current mechanistic schemes to incorporate Logic, memory, and nuanced aspects of Language, such as latent structure. By using this framework, the authors compare three generative language model architectures: GPT-2 (transformer-based), Hyena (convolution-based), and proposed hybrid architecture T HEX which are constructed by replacing certain layer of the Hyena model with the GPT-2 layer. To instantiate LOLAMEME, the authors introduce two different manifestations, LoLa and MeMe, and evaluate the performance of the architectures across various aspects of language. The findings demonstrate that T HEX surpasses GPT-2 and Hyena on select tasks as well as a related benchmark dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This work proposes a new hybrid architecture based on transformer-based GPT-2 and convolution-based Hyena. Experiments demonstrate the superiority of this architecture."
                },
                "weaknesses": {
                    "value": "\u2022\tThe motivation and problem formulation of this work is unclear. And the novelty and contribution of this paper are somewhat limited. The proposed new architectures are simply constructed by replacing certain layers of the Hyena model with the GPT-2 layer. Although some experiments demonstrate better performance on the proposed two test datasets, there may be a lack of validation experiments on other existing datasets. Additionally, providing some interesting findings or interpretations about the experiments through a deeper analysis of the proposed architectures should be better.\n\u2022\tThe construction procedure of the two datasets should be explained more clearly, and deeper consideration should be given to whether the experimental settings can reliably reflect the behavior of the related models, such as memorization and in-context learning.\n\u2022\tThere are quite a few typo errors, including grammar and table issues, in this paper. For example, in the abstract, it states \"We propose the hybrid architecture T HEX and use LOLAMEME framework is used to compare three architectures.\" There are also typo errors in tables 3, 4, and 5. The grammar, figures, and tables in this paper may require some polishing.\n\u2022\tThe related work on mechanistic interpretability is not comprehensive. In fact, there is a considerable amount of work such as [1], [2], [3] attempting to interpret and understand the mechanisms of LLMs.\n\u2022\tIn section 6.5, it is unclear why TH EX-11 to T HEX-15 showed a loss of 0 after a few epochs but showed an exact match of 0. Further clarification or explanation is needed for this inconsistency.\n[1] A Mathematical Framework for Transformer Circuits\n[2] Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\n[3] Investigating Gender Bias in Language Models Using Causal Mediation Analysis"
                },
                "questions": {
                    "value": "Please see the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2772/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2772/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2772/Reviewer_iHzK"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2772/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699372626121,
            "cdate": 1699372626121,
            "tmdate": 1699636220090,
            "mdate": 1699636220090,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HJHIEgodVm",
                "forum": "73dhbcXxtV",
                "replyto": "n6PUXYc1HN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2772/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2772/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thorough review and valuable feedback on our paper. We appreciate the opportunity to address your concerns and clarify some aspects of our work.\n\nAddressing Strengths:\nWe thank the reviewer for recognizing the potential of one of our contributions new hybrid architecture. Although , our contribution also include framework called LOLAMEME which is a mechanistic framework closer to natural language. LOLAMEME framework is used to explore the capabilities of different language model architectures, through which we show various findings of GPT-2, Hyena and show that in T-HEX architecture both attention and hyena operator are complementary to each other and show greater performance on various aspects of language. \n\nAddressing Weaknesses:\n\n\n1. The motivation and problem formulation of this work is unclear. Natural languages has several aspects to it and it has become extremely difficult to test a model architecture on one or a set of aspects of the natural language. LOLAMEME framework is built to evaluate model architectures on various aspects of language such as grammer, vocabulary, logic/reasoning, memorization, multiple languages and more listed in the paper.\n2. The proposed new architectures are simply constructed by replacing certain layers of the Hyena model with the GPT-2 layer. Although some experiments demonstrate better performance on the proposed two test datasets, there may be a lack of validation experiments on other existing datasets. Additionally, providing some interesting findings or interpretations about the experiments through a deeper analysis of the proposed architectures should be better.  \n    1. Using LOLAMEME framework, we tested the GPT-2 and hyena model architectures. We identified various areas where GPT-2 using attention operator works better and Hyena using hyena operator works better. Guided by the experiment results using LOLAMEME framework, we show that attention and hyena have complementary strengths using T-HEX. Using attention operator together with hyena operator, boosts the model architecture\u2019s performance on various aspects such as memorization, in-context learning, language mixing, longer input length and more listed in the paper. \n    2. We show T-Hex performs better than Hyena and GPT-2 on various aspects of the language and we also show results on existing dataset \u201cListops\u201d. Using LOLAMEME framework, better architectural design changes can be developed and tested on various aspects of the language. \n3. The construction procedure of the two datasets should be explained more clearly, and deeper consideration should be given to whether the experimental settings can reliably reflect the behavior of the related models, such as memorization and in-context learning. Thank you for your feedback, we will explain the dataset more clearly in the next revision. \n4.  There are quite a few typo errors, including grammar and table issues, in this paper. For example, in the abstract, it states \"We propose the hybrid architecture T HEX and use LOLAMEME framework is used to compare three architectures.\" There are also typo errors in tables 3, 4, and 5. The grammar, figures, and tables in this paper may require some polishing. Thank you for your feedback, we will address this in the next revision.\n5. The related work on mechanistic interpretability is not comprehensive. In fact, there is a considerable amount of work such as [1], [2], [3] attempting to interpret and understand the mechanisms of LLMs.  We realize the importance of a comprehensive review of related work. Our paper will be updated to include a more exhaustive list of research in the domain of LLMs interpretability, referencing works such as [1], [2], [3] cited by the reviewer\n6. In section 6.5, it is unclear why TH EX-11 to T HEX-15 showed a loss of 0 after a few epochs but showed an exact match of 0. Further clarification or explanation is needed for this inconsistency. One intuition for this is that, attention in higher layers requires more training data than in lower layers for more complex dataset. In section 6.4 we observed pre-training on larger dataset boosted the performance. In future work, we will address this with more research."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2772/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700701864521,
                "cdate": 1700701864521,
                "tmdate": 1700701864521,
                "mdate": 1700701864521,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]