[
    {
        "title": "Stochastic Controlled Averaging for Federated Learning with Communication Compression"
    },
    {
        "review": {
            "id": "0DR7MGLa06",
            "forum": "jj5ZjZsWJe",
            "replyto": "jj5ZjZsWJe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission546/Reviewer_fdMe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission546/Reviewer_fdMe"
            ],
            "content": {
                "summary": {
                    "value": "The paper delves into the challenge of minimizing the objective, which is defined as a finite sum of smooth, and potentially non-convex, functions within a Federated Learning setting. The primary focus is addressing the significant workers-to-server communication costs arising in a centralized distributed framework, which involves one main server and multiple nodes. The authors' contributions can be summarized as:\n\n- They present a novel formulation of the foundational SCAFFOLD algorithm, which effectively cuts uplink communication expenses by half. This lays a more straightforward foundation for integrating communication compression.\n- The SCALLION algorithm is introduced, leveraging the new SCAFFOLD formulation combined with unbiased compressors.\n- The SCAFCOM algorithm is developed to facilitate biased compressors in Federated Learning via local momentum. Convergence analysis for standard contractive compressors is provided.\n- It is demonstrated that both SCALLION and SCAFCOM either match or surpass the communication and computation complexities of current compressed FL baselines.\n- Through experiments, it's evident that SCALLION and SCAFCOM deliver performance akin to full-precision techniques, boasting compression savings exceeding 100x."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The writing of the paper is clear, the main claims are outlined, and the text is easily read;\n- Literature review is solid and contains relevant papers on the topic;\n- Proposed novel FL algorithms, SCALLION and SCAFCOM, are provably shown to be robust to heterogeneous data and partial participation and require only standard assumptions to establish first-order stationary point guarantees;\n- SCALLION attains superior convergence guarantees compared to prior compressed FL methods with unbiased compression under minimal assumptions;\n- Local momentum in SCAFCOM overcomes the adverse effects of biased compression. SCAFCOM improves communication complexity by a factor of 1/(1-q) over the prior art;\n- Both SCALLION and SCAFCOM exhibit robustness to heterogeneous data and partial participation, unlike existing approaches;\n- The paper provides a principled way to integrate communication compression into federated learning through the new SCAFFOLD formulation;\n- Algorithms are simple to implement and empirically achieve significant compression savings."
                },
                "weaknesses": {
                    "value": "1) Some potentially relevant papers are missing in the references. See \"Suggestions\" below. \n\n## Typos:\n1) In the RELATED WORK section, instead of \"TurnGrad\" it should be \"TernGrad\".\n\n## Suggestions:\nI would recommend authors to reconsider some of the phrases that they wrote, in particular\n\n>>Can we design FL approaches that accommodate arbitrary data heterogeneity, local updates, and partial participation, as well as support communication compression?\n>>\n>>In the literature, none of the existing algorithms have successfully achieved this goal, to the best of our knowledge.\n\nTo the best of my knowledge, this is true, but for the general class of non-convex loss functions. However, this is not true for strongly convex functions. There are already some works [1, 2, 3, 4] on that topic, providing a provably beneficial combination of compression, partial participation, and local updates.\n\n[1] Grudzie\u0144, M., Malinovsky, G., & Richt\u00e1rik, P. (2023). Improving Accelerated Federated Learning with Compression and Importance Sampling.\u00a0_arXiv preprint arXiv:2306.03240_.\n\n[2] Youn, Y., Kumar, B., & Abernethy, J. (2022, October). Accelerated Federated Optimization with Quantization. In\u00a0_Workshop on Federated Learning: Recent Advances and New Challenges (in Conjunction with NeurIPS 2022)_.\n\n[3] Condat, L., Malinovsky, G., & Richt\u00e1rik, P. (2023). TAMUNA: Accelerated federated learning with local training and partial participation.\u00a0_arXiv preprint arXiv:2302.09832_.\n\n[4] Sadiev, A., Malinovsky, G., Gorbunov, E.A., Sokolov, I., Khaled, A., Burlachenko, K., & Richt'arik, P. (2022). Federated Optimization Algorithms with Random Reshuffling and Gradient Compression.\u00a0_ArXiv, abs/2206.07021_."
                },
                "questions": {
                    "value": "There are several questions related to the prof of the Theorem 3:\n1) (Page 25) Could authors please clarify why this inequality holds?\n\n$$9 e^2 K^2 \\eta_l^2 L^2\\left(24+\\frac{4(1+\\omega) \\alpha^2}{S}+\\frac{522(1+\\omega) \\alpha}{N}\\right) \\leq \\frac{\\alpha(1+\\omega)}{N} \\leq \\frac{1}{4}$$\n\n2) Why does this property hold?\n$$\\| {x^0 - x^1}\\|=0$$\n\n## Conclusion\nI would happily give this paper a higher grade for its theoretical contributions and reliable experiments. However, at this moment, I can not do so since the paper still contains things that need to be clarified. I am ready to reconsider my current rate during rebuttals once you respond to me on Weaknesses and Questions.\n\n## Update after the author's rebuttal\nI increased the score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission546/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission546/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission546/Reviewer_fdMe"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission546/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698775984420,
            "cdate": 1698775984420,
            "tmdate": 1700561343378,
            "mdate": 1700561343378,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0eOHUNWsLq",
                "forum": "jj5ZjZsWJe",
                "replyto": "0DR7MGLa06",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission546/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission546/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer fdME:\n\nThanks for your valuable feedback. All questions have been clarified as best as we can and all the revisions in the updated manuscript are highlighted with the red color. \n\n- **Softening the claim.** Thanks for the suggestion and for mentioning the related papers. We have modified the claim in our updated manuscript with the additional references by saying ``In the literature, none of the existing algorithms have successfully achieved this goal in non-convex\nFL, despite a few studies in the strongly convex scenarios (Grudzie \u0301n et al., 2023; Youn et al., 2022;\nCondat et al., 2023; Sadiev et al., 2022)''. We hope this addresses your question.\n\n- **Clarification for the inequality.** We highly appreciate your efforts in carefully reading through the proofs. There is indeed a miscalculation here in our earlier manuscript. We have fixed it and highlighted the revision with the red color on page 25. Note that this minor miscalculation does not affect the soundness of our theories as we essentially only need $\\eta_l^2 K^2L^2 \\lesssim \\alpha (1+\\omega)/N$. The existence of numerical numbers does not affect the final convergence rate.\n\n- **Clarification for $x^{-1}=x^{0}$.**\nWe use $x^{-1}:=x^{0}$ notationally in our proofs instead of $x^1=x^{0}$. We think reviewer fdME meant to ask the clarification regarding $x^{-1}=x^{0}$.\n The variable $x^{-1}:=x^0$ is introduced to simplify the notations in telescoping and does not appear in our algorithmic iterations which indeed start with $t=0$. Specifically, our proof frequently uses the term $\\mathbb{E}[\\\\|x^t-x^{t-1}\\\\|^2]$, which typically emerges from breaking $\\mathbb{E}[\\\\|c_i^t-\\nabla f_i(x^{t})\\\\|^2]$ and $\\mathbb{E}[\\\\|c^t-\\nabla f(x^{t})\\\\|^2]$ for $t\\geq 1$ as\n\\begin{equation}\\mathbb{E}[\\\\|c_i^t-\\nabla f_i(x^{t})\\\\|^2]\\lesssim \\mathbb{E}[\\\\|c_i^t-\\nabla f_i(x^{t-1})\\\\|^2]+L^2\\mathbb{E}[\\\\|x^t-x^{t-1}\\\\|^2]\\end{equation}\nand\n$$\\mathbb{E}[\\\\|c^t-\\nabla f(x^{t})\\\\|^2]\\lesssim \\mathbb{E}[\\\\|c^t-\\nabla f(x^{t-1})\\\\|^2]+L^2\\mathbb{E}[\\\\|x^t-x^{t-1}\\\\|^2],$$\nsee, e.g., Lemma 4, 5, 6.\nBy hypothetically letting $x^{-1}=x^0$, the above inequalities hold trivially for $t=0$. We hope this notation is clear now.\n\nThanks again for your feedback on our work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission546/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700178477825,
                "cdate": 1700178477825,
                "tmdate": 1700178477825,
                "mdate": 1700178477825,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1BB3q3tPkH",
            "forum": "jj5ZjZsWJe",
            "replyto": "jj5ZjZsWJe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission546/Reviewer_17jD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission546/Reviewer_17jD"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors propose two new algorithms SCALLION and SCAFCOM. Those new FL algorithms show robustness to data heterogeneity, partial participation, local updates and use communication compression. They are based on SCAFFOLD algorithm.\n\nThe authors provide convergence analysis for these two algorithms in non-convex case and show that the convergence rate is faster than rates of previous algorithms. The experiments support theoretical guarantees obtained by the authors."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Interesting idea related broadcasting the compressed difference. This new view on the updates from SCAFFOLD help to design new proposed algorithms and to understand why the work.\n2. Only two assumptions are used for convergence analysis.\n3. Well written paper and good presentation of results. It is easy to follow."
                },
                "weaknesses": {
                    "value": "1. For me there is no reasonable weaknesses. Possible, in camera ready version it would be better compare your methods with this work\nhttps://arxiv.org/pdf/2310.07983.pdf ."
                },
                "questions": {
                    "value": "I do not have questions. Probably, I will ask some questions during the discussion period. \n\nTypos:\n1. In the third row of the chain of inequalities, $L^2$ is missed in the second term."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "-"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission546/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827593420,
            "cdate": 1698827593420,
            "tmdate": 1699635981558,
            "mdate": 1699635981558,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "y8xhmqbc5g",
                "forum": "jj5ZjZsWJe",
                "replyto": "1BB3q3tPkH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission546/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission546/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 17jD:\n\nThanks for your valuable feedback and support.\n\nWe have cited and discussed the paper https://arxiv.org/pdf/2310.07983.pdf in the section of related works in the updated manuscript. However, since this method does not support communication compression, we do not\ncompare our methods with it directly.\n\nWe tried our best to look for the typo you pointed out. However, we did not find the location. Could you please kindly inform us of the location where $L^2$ is missing? \n\nThanks again for your feedback on our work."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission546/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700177945449,
                "cdate": 1700177945449,
                "tmdate": 1700178010322,
                "mdate": 1700178010322,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ELizy70BqN",
                "forum": "jj5ZjZsWJe",
                "replyto": "y8xhmqbc5g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission546/Reviewer_17jD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission546/Reviewer_17jD"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response! \n> Could you please kindly inform us of the location where $L^2$  is missing?\n I apologize for forgetting to mention the exact page: p. 28 eq. (39)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission546/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679958808,
                "cdate": 1700679958808,
                "tmdate": 1700679958808,
                "mdate": 1700679958808,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KiefM1YqG9",
            "forum": "jj5ZjZsWJe",
            "replyto": "jj5ZjZsWJe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission546/Reviewer_aALW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission546/Reviewer_aALW"
            ],
            "content": {
                "summary": {
                    "value": "The paper suggests a federated learning algorithm that finds a stationary point while handling arbitrary client heterogeneity, partial client participation, local updates, and gradient compression. This work introduces an algorithm based on SCAFFOLD which communicates to the server only a single compressed vector. The paper shows convergence to a stationary point under very weak communication and compression assumptions and provides results for both biased and unbiased compressors."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The proof looks sound.\n* I believe that the results for biased compressors are a substantial contribution."
                },
                "weaknesses": {
                    "value": "1) I believe the paper lacks comparison (both theoretical and experimental) with \"MARINA: Faster Non-Convex Distributed Learning with Compression\". Both papers pursue the same goals, namely handling the following FL issues:\n* arbitrary client heterogeneity,\n* partial client participation,\n* gradient compression.\n\nOverall, as far as I know, MARINA (one of itsvariations) is the closest result in terms of settings (and I think it achieves similar bounds), and I'm not sure whether the authors are aware of it.\n\n2) Theorems 1 and 2: \"set learning rates $\\eta_l$ and $\\eta_g$ as well as scaling factor $\\alpha$ properly\" - the parameters should be specified in the main body.\n\n3) The paper handles local updates (K local updates per round), but this is achieved by dividing the learning rate by a factor of K. In other words, local updates don't provide provable improvement compared to a single larger gradient step."
                },
                "questions": {
                    "value": "How does your paper compare with MARINA?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission546/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission546/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission546/Reviewer_aALW"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission546/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699359998807,
            "cdate": 1699359998807,
            "tmdate": 1700356070235,
            "mdate": 1700356070235,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3poQsMvsDQ",
                "forum": "jj5ZjZsWJe",
                "replyto": "KiefM1YqG9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission546/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission546/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer aALW:\n\nThanks for your valuable feedback on our work. All questions have been clarified as best as we can and the revisions in the updated manuscript are highlighted with the red color.\n\n**1. Comparison with MARINA**\n\nWe did not compare our algorithms with MARINA in the original manuscript for a couple of reasons. (i) MARINA does not support local updates, the fundamental characteristic of federated learning algorithms. (ii) MARINA lacks the guarantee of supporting partial client participation and stochastic gradients simultaneously. Specifically, VR-MARINA supports stochastic gradients but not partial participation while  PP-MARINA supports partial participation but not stochastic gradients. (iii) MARINA requires  the averaged smoothness (see Assumption 3.2 therein), which is beyond the scope of our manuscript. Nevertheless, we are happy to cite the paper and add some discussions.\n\nWe have supplemented the complexities of MARINA in Table 1 in the updated manuscript. We observe that SCAFCOM outperforms MARINA in terms of computation complexity but is inferior in terms of communication complexity. Despite the comparison of theoretical rates, we again would like to emphasize that the problem setups are different, and SCALLION and SCAFCOM can be advantageous in the practical federated learning setup where local updates, stochastic gradients, and partial participation are entangled together.\n\n \n**2. Specifications of learning rates and scaling factors**\n\nWe have specified the choices of $\\eta_l$, $\\eta_g$, $\\alpha$, and $\\beta$ in the updated manuscript (in red color).\n\n\n**3. No provable improvement compared to a single larger gradient step**\n\nTo our knowledge, the shrinkage of the learning rate due to local updates is customary in non-convex and stochastic FL literature.  The main focus of this work is to address the entanglement of local updates, stochastic gradients, partial participation, compression, and data heterogeneity that appears naturally in practical FL.\n\nWe remark that the provable benefit of local updates is a long-standing problem particularly in the stochastic and non-convex scenarios, without a bounded-similarity-type condition being imposed over $\\\\{f_i\\\\}_{i=1}^N$. While there is a stream of works demonstrating the benefit as mentioned by Reviewer fdMe in strongly convex scenarios, we are not aware of any existing work achieving this goal in the stochastic and non-convex scenarios. We thus intend to leave this ambitious goal for future work.\n\nPlease kindly let use know if there are any further questions. Thanks again for your review of our work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission546/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700178031391,
                "cdate": 1700178031391,
                "tmdate": 1700178031391,
                "mdate": 1700178031391,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qj6ZwGllYz",
                "forum": "jj5ZjZsWJe",
                "replyto": "3poQsMvsDQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission546/Reviewer_aALW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission546/Reviewer_aALW"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your replies. I updated my score.\n\n> Comparison with MARINA\nThank you for the explanation. I had PP-MARINA in mind, but I overlooked the lack of stochastic gradients.\n\n> We have specified the choices of [parameters]  in the updated manuscript:\n\nThank you. I think it is fine to specify the parameters only up to big-O notation.\nI think it would be great to explain where all the terms come from.\nI also suspect that you can present a specific choice of parameters that would preserve the complexity while drastically simplifying the presentation."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission546/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700356052100,
                "cdate": 1700356052100,
                "tmdate": 1700356052100,
                "mdate": 1700356052100,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3MQSudOVTT",
                "forum": "jj5ZjZsWJe",
                "replyto": "KiefM1YqG9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission546/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission546/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are delighted that your concerns have been resolved, and we sincerely appreciate your positive feedback. We will incorporate your suggestions to streamline the choices of parameters in our later revisions. Thank you again for your valuable input."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission546/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700364208792,
                "cdate": 1700364208792,
                "tmdate": 1700364259737,
                "mdate": 1700364259737,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]