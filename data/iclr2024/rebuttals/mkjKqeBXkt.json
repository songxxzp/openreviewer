[
    {
        "title": "KITS: Inductive Spatio-Temporal Kriging with Increment Training Strategy"
    },
    {
        "review": {
            "id": "v14BkQiZAu",
            "forum": "mkjKqeBXkt",
            "replyto": "mkjKqeBXkt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission129/Reviewer_FRYk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission129/Reviewer_FRYk"
            ],
            "content": {
                "summary": {
                    "value": "I've reviewed this paper for a previous conference and have discussed at length its merits and negative aspects with a long back and forth discussion with the authors. Unfortunately, none of my comments (and that of the other reviewers) were apparently taken into account as this submission is basically unchanged w.r.t. the previous iteration. Furthermore, several of the additional results presented during the previous discussion haven't been included in this version.\n\nThe paper introduces KITS, a novel approach for kriging based on graph neural networks. The main contribution of the paper is the introduction of an augmentation strategy based on the idea of adding virtual nodes to the input graph at training time. The augmentation strategy is then paired with a self-supervised training strategy yielding good results on several benchmark datasets. The method is presented as a solution to what the paper defines as the \"graph gap\", i.e., a mismatch between the graph at training and test time. Overall, the paper paper has merits, however, I do have some serious concerns that prevent me from recommending acceptance."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The introduced data augmentation strategy paired with the self-supervised training routine is novel and appealing.\n* Good empirical performance.\n* Very good presentation."
                },
                "weaknesses": {
                    "value": "* There is a conceptual flaw in the main motivation behind the introduced methodology. While it is straightforward to see why using a drastically different graph at training and inference time is a problem (\"graph gap\" in the paper), I do not understand why every target node should be reconstructed in a single forward pass.\n* Reconstructing a single node at a time would remove the \"graph gap\", this would be the proper way of carrying out the evaluation.\n* After removing nodes for training, graphs can become sparse. However, this issue is only caused by the removal of nodes for evaluation, i.e., it is an issue of the training/evaluation procedure and not an inherent issue of kriging methods. This would not be a problem in any real-world application as you would eventually train the model on the full graph.\n* The considered datasets are quite small, removing many nodes at random might result in disconnected graphs that would explain the poor performance for some of the baselines.\n\nI believe that the paper has good methodological novelty, although is not framed in the proper way as there is no inherent \"graph gap\" issues in spatio-temporal kriging. The paper should either be rewritten targeting a specific operational setting (heavily scaling back the current claims and significance of the work) or presenting the method as a data augmentation strategy on graphs that are not made artificially sparse."
                },
                "questions": {
                    "value": "--"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission129/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission129/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission129/Reviewer_FRYk"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission129/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698401509482,
            "cdate": 1698401509482,
            "tmdate": 1699635938340,
            "mdate": 1699635938340,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KJ5uVW0Mjl",
                "forum": "mkjKqeBXkt",
                "replyto": "v14BkQiZAu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission129/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission129/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response [1/3]"
                    },
                    "comment": {
                        "value": "> **Summary**: Unfortunately, none of my comments (and that of the other reviewers) were apparently taken into account as this submission is basically unchanged w.r.t. the previous iteration.\n\nWe would like to emphasize that **we have addressed all comments from the previous conference**, which we explain as follows.\n- **Reviewer 1 for the previous submission (weak accept - score 6)**:\n    - *W1*: This comment is that the reviewer did not observe significant improvement of our KITS over an existing method INCREASE due to some misinterpretation. We provided explanations on this in the rebuttal, with which the reviewer is satisfied. **No changes are necessary for this comment.**\n    - *W2*: This comment is on the robustness aspect of the paper. We provided explanations on this in the rebuttal, with which the reviewer is satisfied. **No changes are necessary for this comment.**\n    - *Q1, Q2*: The comments are some questions about the details of virtual node generation (Section 3.2). **We have re-written this part to make it clearer.**\n    - *Comment on our rebuttal*: **\"I would like to thank the authors for their detailed and clear rebuttal. I am convinced with the provided answers. I keep my rating intact.\"** This comment shows that the reviewer is convinced with our responses to the comments in the rebuttal.\n- **Reviewer 2 for the previous submission (weak accept - score 6)**:\n    - *W1*: This comment suggests to include some empirical evidence about \"graph gap issue\" and \"fitting\" issue. **We have mentioned them in introduction and included the details in Appendix B.**\n    - *W2*: This comment suggests to include more error bars evaluation (like Table 9 in Appendix H.2). **We have followed this suggestion and further included Table 10 in Appendix H.2.**\n    - *W3, W4 (minor)*: These comments are about two typos in Equation 1 and 2. **We have fixed the typos.**\n    - *Comment on our rebuttal*: **\"Thank you for your rebuttal! I updated my score as my concern raised in the review was resolved. I would recommend the discussion in the author's rebuttal be involved appropriately in the revised manuscript.\"** This comment shows that the reviewer is satisfied with our rebuttal overall. We have updated the draft as suggested by this reviewer.\n- **Reviewer 3 for the previous submission (you, borderline reject - score 4)**:\n    - *W1, W2, Q1*: Kriging single node at a time. **We have included and compared your suggested idea in Appendix H.8, which shows that the suggested idea does not work as well as our method for the problem settting we target in this paper.**\n    - *W3.1*: Isolated nodes introduced after dropping nodes. We replied to you with some statistics to answer this question during the rebuttal of the previous conference. **We believe this issue has been settled in our rebuttal, and thus there is no change for this comment.**\n    - *W3.2*: More discussions about region missing. **We have enriched the corresponding part, which is near Table 6.**\n    - *W5 (minor)*: More baselines such as SPIN by Marisca et al. NeurIPS 2022. We appreciated your comment, and replied to you with some experimental results about this baseline, but we finally decided not to include it for the following reasons. (1) This method targets transductive setting, which is not the main target setting of our paper; (2) It does not work as well as GRIN; (3) This method costed much in terms of GPU onboard memory, and we met out-of-memory (OOM) problem in PEMS07 dataset (with 883 nodes). In case that you still think we should include these results, we shall include them in a new version of our draft.\n    - *Q2*: Average degree of training and inference graphs. **We have included some statistics in Appendix B.1.**\n    - *Q3 (minor)*: Why removing self-loops in Equation 1. We have replied to you in the previous rebuttal.\n    - *Q4 (minor)*: Initialization of virtual nodes' representations. **We have re-written Section 3.2 to make it more clear.**\n    - *L1*: The possible negative effects of adding noise to training graphs. We previously replied to you with statistics in Figure 5."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission129/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700065428000,
                "cdate": 1700065428000,
                "tmdate": 1700065622166,
                "mdate": 1700065622166,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vwQDIaIT5J",
                "forum": "mkjKqeBXkt",
                "replyto": "EacT0ck5Yv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission129/Reviewer_FRYk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission129/Reviewer_FRYk"
                ],
                "content": {
                    "comment": {
                        "value": "As for our previous interactions, thank you for your comprehensive feedback. \n\nWhile I see your points and still find the paper methodologically significant, I keep my position regarding the \"graph gap\" issue. I do not think the mismatch between the graph used for training and evaluation is the relevant issue here as it can easily be avoided as previously discussed. The increment training strategy effectively acts as a data augmentation and graph rewiring routine, which can explain the improved performance in an inductive setting. This point is unchanged w.r.t. the previous version of the paper and, clearly, it seems that we cannot come to an agreement on this aspect.\n\nMy position on the paper is unchanged, I believe that the introduced method is significant (the combination of the increment strategy with NCR is very appealing); however, I do not agree with your analysis of the above-mentioned aspects and think that the paper (and its impact on the research community) would benefit from a different framing. I'll keep my current score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission129/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700473875089,
                "cdate": 1700473875089,
                "tmdate": 1700473875089,
                "mdate": 1700473875089,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "M33J1YdPPz",
            "forum": "mkjKqeBXkt",
            "replyto": "mkjKqeBXkt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission129/Reviewer_env1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission129/Reviewer_env1"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses a spatio-temporal krigging problem. This is modeled as a node prediction task using graph convolution neural network (GCN) whose convolution operator is learned to compute the feature embeddings for each node, taking into account its topological connection to others (i.e., a training graph). \n\nThe learned weights of the convolution operator can be applied to any topological graphs that contain the training graph, thus enabling inductive inference on new node: a forward pass of the (learned) convolution over an expanded graph that includes incorporate new connections to unobserved nodes will predict the features for those nodes, which can be subsequently input to a feed-forward net for node prediction.\n\nPrevious approaches addressing this problem often trains the GCN on the observed (training) part of the entire graph, which is later used to perform inductive inference on the unobserved (test) part of the graph. Their performance might therefore suffered from the disparity or gap between the train and test graphs. This is what this paper aims to address.\n\nThis is based on three main ideas:\n\nGraph Augmentation (Increment Strategy): Observed nodes are sampled. For each observed node, a virtual node and a connection to the observed node is created. The virtual node's connection to the neighborhood of the observed node is also randomly generated. Multiple such augmented graphs are generated to train the GCN so its learned convolution is robust to potential variation in the structure of the (unseen) test graph. Original features for the virtual nodes are set to be zero. Also, features of the same node reported or computed within a window of +/- m steps are also concatenated before passing through the GCN\n\nFeature Fusion between Virtual & Observed Nodes: Generated features of virtual nodes and observed nodes from the above steps are paired based on a similarity notion. Paired features are concatenated and passed through a (learnable) neural net, which returns the fused features for the virtual node. \n\nPseudo-Label Generation: Pseudo-labels for virtual nodes are first generated using the learned GCN-based node prediction model. The model is then re-trained using only the pseudo-labels to predict labels of the observed nodes. The losses in two training phases can be combined to be optimized together."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The presented ideas are refreshingly interesting & novel to me. \n\nThe problem being addressed is also practically significant. The gap between the train and test graphs in the inductive setting of node prediction is always a fundamental issue, which needs an in-depth treatment.\n\nThe empirical studies are sufficiently extensive, showing good results across a variety of datasets. \n\nComprehensive ablation studies showing the effectiveness of each idea is also included."
                },
                "weaknesses": {
                    "value": "Overall, I like this paper. It motivates well a fundamental issue of inductive inference with GCN.\n\nBut I do have a few concerns or questions regarding both the position, presentation and empirical evaluation of this paper that I want to discuss with the authors (mostly out of curiosity & for constructive feedback)\n\n1. The main position of this paper is grounded in the spatio-temporal setting but the proposed treatment does not seem to have anything specific to the temporal aspect. The aggregation of temporal feature within +/- m steps is kind of a random treatment to me. I do not see a very particular reasoning why doing so makes sense. What if such aggregation erases important small-scale variation patterns in the time-series data? Furthermore, m is a value dependent on the nature of the data so even though the ablation studies conclude that m = 2 gives best result, it is still specific to the few set of experimental data and that should not be a generic guideline to choose m. \n\n2. Although comparison with recent pure graph-based approaches has been well presented, I am still curious to know how well the proposed approach improves over a hybrid approach that integrates GCN with traditional krigging techniques. For example, the series of work on graph convolutional Gaussian processes. There is quite a substantial volume on that & I think the authors have not discussed that in the literature review. Such techniques can be used to address this krigging problem. After all, if I remember correctly, Gaussian processes were referred to as krigging in the past (in geo-statistics). \n\n3. Last, in terms of the presentation, I believe it would be better if the authors spend some space on summarizing GCN which will highlight better the essence of transferability in the inductive setting of krigging. Otherwise, while the current presentation is fine for people who are familiar with graph neural network, it will still leave a lot of gap for generic readers."
                },
                "questions": {
                    "value": "Based on the above, I have the following questions:\n\n1. Could the authors position this work with the other literature on graph convolutional GPs, which is an integrative approach combining both elements of GCN & a traditional non-graph krigging method (i.e., GPs)?\n\n2. Could the authors elaborate more on why this paper is specifically positioned in the temporal-spatial setting even though the proposed technique does not really have any new innovations for temporal modeling? I might have missed something important here.\n\n3. It will be good to run some comparative experiments with a few representative graph convolutional GPs."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission129/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698996505038,
            "cdate": 1698996505038,
            "tmdate": 1699635938264,
            "mdate": 1699635938264,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BZ1N5139Re",
                "forum": "mkjKqeBXkt",
                "replyto": "M33J1YdPPz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission129/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission129/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your review! Response [1/2]"
                    },
                    "comment": {
                        "value": "> **W1, Q2**: Intuition behind Spatio-Temporal Graph Convolution (STGC) Module.\n\nThanks for raising this good question! We reply to this question as follows:\n1. For Q2 (why the setting is spatio-temporal), we claim that for the $i$-th node $X_i^t$ obtained from timestamp $t$:\n    - if it aggregates information from other nodes $X_j^t$ under the same timestamp $t$, where $j \\neq i$, then the **spatial** aspect will be included in the setting;\n    - if it obtains information from different timestamps, e.g., $t-1$, $t+1$, then the **temporal** aspect will be captured in the setting.\n2. More details about STGC module:\n    - Firstly, we explain the differences between normal GCN (spatial only) and STGC (spatial + temporal).\n        - In Figure 3(a), for node-1 under timestamp $T_i$, normal GCN would only aggregate node-1 with node-2,3,5's (node-1's neighbors) information under **the same timestamp $T_i$**;\n        - Differently, STGC would aggregate not only **neighbor information within the same timestamp**, but also **information from each node's neighbors across time**.\n    - Then, we explain the design and rationale behind STGC module:\n        - **Forbid each node to aggregate information from itself (including the same and different timestamps)**:\n            - As introduced in the paper, the nodes can be divided into 2 groups, namely observed nodes (with sensors) and target nodes (no sensors). Observed nodes have sensor readings all the time, while target nodes can never have sensor readings. The essence of kriging is to learn the pattern only on observed nodes during training, and directly apply the learned pattern to target nodes for value estimation. Due to the fact that observed nodes not only have sensor readings but also have loss regulations, their kriging patterns can be easily learned, i.e., their learned representations/features can be quite good.\n            - With the above viewpoint in mind, if we allow self-loops, then the observed nodes would keep aggregating *good-quality* features, while the target nodes would keep aggregating *bad-quality* features from itself across time. Therefore, the pattern learned on observed nodes cannot suit target nodes well, and the performance would be suboptimal.\n            - Therefore, we (1) remove self-loop in the adjacency matrix (**verified in row 1&3 of Table 11**); and (2) do not utilize RNN-like modules to aggregate standard temporal information (**verified in row 1&2 of Table 11**). For example, in Figure 3, node-1 ($T_i$) will not receive information from node-1($T_i$) and node-1($T_{i+1}, T_{i-1}$).\n        - **Aggregate each node with its neighbors information (from the same timestamp)**: The intuition is the same as that of the normal GCN.\n        - **Aggregate each node with its neighbors information (from different timestamps)**: This design is less common yet effective (**verified in row 4-8 of Table 11**). We provide a few examples to explain the intuition as follows:\n            - Consider that we have road 1 and road 2, and the vehicles would drive from road 1 to road 2. Suppose at 8:00 am, a traffic jam occurs at road 2, and affected by road 2, road 1  gets crowded at 8:05 am. Therefore, the data distributions (e.g., vehicle speed data) of road 1 (8:05) have high correlations with those of road 2 (8:00).\n            - Similarily, suppose a group of vehicles drive on road 1 at 8:00 am, and the same group drive to road 2 at 8:05 am. The driving behavior of the same group might be uniform, thus, the data distributions (e.g., vehicle speed data) of road 2 (8:05) have high correlations with those of road 1 (8:00).\n    - Finally, the hyperparameter $m$ is used to define the scope, e.g., if $m=2$, then we consider each node at $T_i$ has high correlations with its neighbors at $\\{T_{i-2}, T_{i-1}, T_i, T_{i+1}, T_{i+2}\\}$, and would aggregate the former with the latter."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission129/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700064896754,
                "cdate": 1700064896754,
                "tmdate": 1700064896754,
                "mdate": 1700064896754,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DRhPs76zWZ",
                "forum": "mkjKqeBXkt",
                "replyto": "v35NtfsCRJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission129/Reviewer_env1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission129/Reviewer_env1"
                ],
                "content": {
                    "title": {
                        "value": "Re: Thank you for the detailed response"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response & the extra empirical work.\n\nAll look good to me & I maintain my original support for this paper.\n\n--\n\nThat being said, I do want to push back a little on the statement that \"there is no existing GCN+GP method particulary designed for the kriging task\". Could the authors discuss this paper on graph GP: https://arxiv.org/pdf/1809.04379\n\nIt seems like it has not been used in the exact context here but I am curious if it can be repurposed for that? \n\nThis question is meant for constructive feedback only (my support will not change) so please feel free to expand a detailed discussion as necessary."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission129/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700373634116,
                "cdate": 1700373634116,
                "tmdate": 1700373634116,
                "mdate": 1700373634116,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aMw4W1PM42",
                "forum": "mkjKqeBXkt",
                "replyto": "LQevERSWXq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission129/Reviewer_env1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission129/Reviewer_env1"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the detailed response"
                    },
                    "comment": {
                        "value": "Thank you for the response. Please consider including this brief discussion in the revised paper (either in a related work section or as a remark in the experiments) for thoroughness.\n\nAs mentioned previously, I will vote for acceptance."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission129/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634366292,
                "cdate": 1700634366292,
                "tmdate": 1700634366292,
                "mdate": 1700634366292,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BPPRXip2Hq",
            "forum": "mkjKqeBXkt",
            "replyto": "mkjKqeBXkt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission129/Reviewer_Ge1P"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission129/Reviewer_Ge1P"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the challenge of inferring unobserved nodes using observed source nodes. While several inductive spatio-temporal kriging methods utilizing graph neural networks have been previously proposed, they often do not account for the discrepancies between the sparsity of training data and inference data (as the 'graph gap').\n\nThe author introduces an approach that adds virtual nodes into the training graph. This is done by 1) improve bad-learned features by finding similar nodes/feature fuse (Reference-based Feature Fusion module) 2) improve supervision signals by construct reliable pseudo labels for virtual nodes (Node-aware Cycle Regulation). Authors also show extensive experiments on 8 datasets with ablation study to support the method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Admittedly, I am not a domain expert for inductive spatio-temporal kriging methods based on GNN, I do find this paper is:\n* Well-written, and enjoyable to read.\n* This looks like a novel method to address the sparsity gap between training graph and inference graph."
                },
                "weaknesses": {
                    "value": "* My main concern is that the introduction of virtual nodes does not add additional information to the dataset. Consequently, my intuition is that this strategy is more effective when the dataset size is small. In the case of the benchmark datasets, which contain a limited number of spatial data points (ranging from a maximum of 883 to a minimum of 80), the approach seems advantageous. However, in scenarios where there is a larger denser spatial dataset, it's worth considering whether the issue of sparsity remains as significant. Would the gap in sparsity still present a challenge in extensive populated spatial data environments? But again, I am not a domain expert for inductive spatio-temporal kriging methods based on GNN, maybe this is indeed the usual dataset size for this line of work. I am happy to change my score if this gets justified.\n\n* This paper employs a thresholded Gaussian kernel to construct the adjacency matrix A among nodes, setting the values to zero when distances exceed a certain threshold. Have the authors explored the possibility of using non-stationary kernels or neural network-based kernels as alternatives?"
                },
                "questions": {
                    "value": "* When aggregating spatio-temporal features from neighboring nodes, is it worth exploring some attention mechanism to gain more global information (especially with so many timesteps in the datasets)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission129/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission129/Reviewer_Ge1P",
                        "ICLR.cc/2024/Conference/Submission129/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission129/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700457744963,
            "cdate": 1700457744963,
            "tmdate": 1700930109503,
            "mdate": 1700930109503,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "32pbV8Pa7F",
                "forum": "mkjKqeBXkt",
                "replyto": "BPPRXip2Hq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission129/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission129/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your review!"
                    },
                    "comment": {
                        "value": "> **W1**: The sparsity of deployed sensors.\n\nThanks for this comment. Our responses are as follows:\n\nAs written in the 1st paragraph of introduction (\"Nevertheless, due to the **high cost of devices and maintenance expenses**, the actual **sparsely deployed sensors**...\"), we target the kriging setting where the sensors are sparsely deployed (mainly for cost saving considerations). This setting has been studied in many existing kriging methods, including IGNNK (Wu et al., 2021a), LSJSTN (Hu et al., 2021) and INCREASE (Zheng et al., 2023). For this setting, the **graph gap issue** would remain valid. We further provide a few **practical application scenarios** for justifying the targetted setting.\n   1. The PEMS07 dataset comes from the Caltrans Performance Measurement System (PeMS) deployed in California (in use). It covers a large district, but the average node degree is 1.79 for the full graph (883 nodes), which means the graph is indeed locally sparse.\n   2. Similarily, the AQI-36 dataset is collected from an air quality monitoring application, where only 36 stations are installed for the whole Beijing City.\n   3. In our own business data of traffic, only 71 traffic sensors (to measure traffic flow) are sparsely installed to a large region, centered at Jinghu Amusement Park, Shaoxing, Zhejiang, China. The average node degree is less than 3.\n   4. In many water quality management systems, the sensors are sparsely installed. For example, the Rhein river in Europe only has 2-3 sensors in each city it flows through.\n   5. In a landslides monitoring application, there are only 7 landside tilt sensors installed in Yushan mountain in Taiwan.\n\nWe are not aware of any existing studies/datasets/practical scenarios, which target the kriging task with desnsely distributed sensors. We believe the kriging with densely distributed sensors would naturally be an easier task than the one targeted in this paper since each node would have more neighbors to leverage for the kriging task. \n\n> **W2**: Non-stationary or neural network-based kernels.\n\nIt's a very reasonable and interesting idea! When we worked on this paper, we did not come up with this idea, but we do agree it can potentially be  applied to this task. Particularly, we also wonder if the virtual nodes-related graph edges (in adjacency matrix) can be learnable instead of being randomly determined (in the current version). To keep consistency with baselines, we would still use Gaussian-thresholded static kernels in this paper, and consider your idea as a very good future direction. Thanks again for this precious comment!\n\n> **Q1**: Use attention mechanism for global spatio-temporal features aggregation.\n\nThank you for your suggestion! Our understanding of the suggested idea is as follows - in case we misunderstood the idea, please kindly let us know. Suppose node-1's neighbors are node-2, 3, 4, and their data from 24 consecutive timestamps are available in a data sample. Your suggested idea is to use attention to aggregate node-1's features (in 1 of 24 timestamps) with node-2, 3, 4's features (from all 24 timestamps), so as to capture global dependency (in the temporal dimension).\n\nOur responses are as follows:\n- **Temporal scope (from local to global)**: We have studied the effects of the temporal scope, which we represent with a parameter $m$ (i.e., the information over $2 \\times m + 1$ timestamps is aggregated). According to the results shown in Table 11, we can observe that larger $m$ (more global temporal information) cannot guarantee better performance.\n- **GCN v.s. GAT/Attention**: When we started this work, we also thought about which to use among GCN, GAT or standard Self-Attention (without adjacency matrix). We finally chose GCN, and we explain as follows:\n    - GAT and Self-Attention would firstly perform projection operations to project input features to $Q$, $K$ and $V$ in attention blocks. However, we think such projections cannot work well in spatio-temporal kriging. This is because in kriging, observed nodes would consistently have loss supervisions, while target nodes do not have loss supervisions all the time. Hence, the complex projections learned on observed nodes may not be suitable for target nodes during inference since it would get overfitting on observed nodes.\n    - We conduct experiments by replacing GCN (M-0 of Table 5) with GAT and Self-Attention under the same condition. We show the MAE scores on METR-LA dataset in the table below (we've already tuned the hyperparameters). We could see that attention-like methods cannot outperform GCN-like methods.\n\n|Method|MAE|\n|-|-|\n|GCN|**6.7597**|\n|GAT|7.3220|\n|Attention|8.2679|"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission129/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700484473230,
                "cdate": 1700484473230,
                "tmdate": 1700484473230,
                "mdate": 1700484473230,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]