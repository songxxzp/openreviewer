[
    {
        "title": "Robustness Over Time: Understanding Adversarial Examples\u2019 Effectiveness on Longitudinal Versions of Large Language Models"
    },
    {
        "review": {
            "id": "eOlYhiEh6k",
            "forum": "eC4WlSZc4H",
            "replyto": "eC4WlSZc4H",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2951/Reviewer_6TAn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2951/Reviewer_6TAn"
            ],
            "content": {
                "summary": {
                    "value": "This paper conducts a comprehensive experimental investigation to evaluate the robustness of updated large language models (LLMs) in comparison to their earlier versions. Utilizing established adversarial benchmarks, the research employs two distinct experimental setups: zero-shot and few-shot prediction paradigms. Contrary to expectations, the findings reveal that the newer versions of LLMs do not demonstrate a significantly enhanced level of robustness against adversarial attacks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "This paper is clearly written and straightforward to understand. \n\nIt focuses on the intriguing question of comparing the robustness between earlier and later versions of the model.\n\nThis paper offers a thorough evaluation across various scenarios, encompassing benign and adversarial descriptions, questions, and demonstrations."
                },
                "weaknesses": {
                    "value": "1. My primary concern with this paper is its limited scope in terms of technical innovation. While the paper considers a range of models against existing benchmarks, albeit with some modifications and combinations, it fails to introduce new evaluation benchmarks or methodologies. Therefore, I think the paper does not meet the standards of ICLR.\n\n2. Another issue is the selection of datasets for evaluation. The benchmarks employed are commonly used, potentially even in the training of the GPT models under scrutiny. This compromises the conclusions of the results.\n\n3. Furthermore, the objective behind updating from GPT-3.5 v0301 to GPT-3.5 v0613 may not exclusively target robustness enhancement. Other factors such as reasoning ability, following prompts, and computational efficiency could also be taken into account. Thus, expecting substantial improvements in robustness may not be reasonable."
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2951/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2951/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2951/Reviewer_6TAn"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2951/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697833186795,
            "cdate": 1697833186795,
            "tmdate": 1700505664530,
            "mdate": 1700505664530,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "F7s8roBK3G",
                "forum": "eC4WlSZc4H",
                "replyto": "eOlYhiEh6k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2951/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2951/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your elaborate review.  In the following, your comments are first stated and then followed by our point-by-point responses.\n\n>**My primary concern with this paper is its limited scope in terms of technical innovation.**\n\nWe believe we introduced a new evaluation methodology, i.e.,  adversarial robustness should be evaluated `over time`. The other three reviewers highly appreciate this new angle of research. Developing new attacks/benchmarks is of general interest but would not solve (and is not relevant to) the problem that LLMs are vulnerable even to existing attacks in existing benchmarks. We also believe that uncovering the LLMs\u2019 vulnerability to existing (simple) attacks poses more severe threats than newly designed (advanced) attacks.\n\n> **The benchmarks employed are commonly used, potentially even in the training of the GPT models under scrutiny.**\n\nThis is a good point, and we will add a related discussion in the paper based on the following. In the LLM era, an LLM's training data may (partially and inadvertently) include the evaluation/test data. However, we point out that our evaluation data is not included in the training data of both GPT and LLaMA models because:\n- Regarding GPT-3.5, the GPT-3.5 training data is up to Sep 2021 according to OpenAI's website (see https://platform.openai.com/docs/models/gpt-3-5). The `AdvGLUE` dataset, however, was published in Nov 2021, and the `PromptBench` dataset was published in 2023. Therefore, It is safe to conclude that there is no possibility of training the GPT-3.5 model using these datasets.\n- Regarding LLaMA-1/2, their paper (https://arxiv.org/abs/2302.13971) states they used the GitHub dataset with `Apache`, `BSD`, and `MIT licenses`. However, the `AdvGLUE`'s license is `CC BY-SA 4.0`. Thus, Meta cannot choose `ADVGLUE` to train the model. In addition, `PromptBench` is a brand new dataset generated using different LLMs from 5 months ago, one month before the LLaMA-2 was released. Therefore, it is very unlikely for Meta to use this dataset for training the LLaMA-2 model.\nMore generally, adversarial data are much less likely than clean data to be included in any natural dataset because they are specifically optimized. Our finding that LLMs are vulnerable to our adversarial data also suggests that they are not used for training.\n\n> **The objective behind updating from GPT-3.5 v0301 to GPT-3.5 v0613 may not exclusively target robustness enhancement. Thus, expecting substantial improvements in robustness may not be reasonable.**\n\nWe agree that robustness is not *exclusively* important, and we do not claim that. However, the security of LLMs is an emerging research topic, and achieving adversarial robustness is a key requirement. For example, OpenAI has established the `preparedness` team to build a robust framework for monitoring, evaluation, prediction, and protection against the dangerous capabilities of frontier AI systems [4]. Following this research trend, we provide new insights that updated LLMs are vulnerable even to existing attacks.\n\n\n[4]. https://openai.com/blog/frontier-risk-and-preparedness"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2951/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700054227139,
                "cdate": 1700054227139,
                "tmdate": 1700054311115,
                "mdate": 1700054311115,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "arzHitanYl",
                "forum": "eC4WlSZc4H",
                "replyto": "F7s8roBK3G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2951/Reviewer_6TAn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2951/Reviewer_6TAn"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for the detailed reply. I have some further questions. \n\n> We believe we introduced a new evaluation methodology, i.e., adversarial robustness should be evaluated over time. \n\nI don't think introducing robustness evaluation over time is a significant novelty. I think almost all existing benchmarks include the time of a model. Furthermore, before releasing a model, all companies considered evaluating it with its prior version.  For example, the LLAMA2 paper (https://arxiv.org/pdf/2307.09288.pdf) compares LLAMA2 and LLAMA1. If they do not find any improvements, they won't even release it. Could the authors provide more justifications for why this is novel? \n\n> We point out that our evaluation data is not included in the training data. \n\nI agree with it. However, I assume the proposed benchmark will be used for future models. Recently, OpenAI introduced new models (https://platform.openai.com/docs/models/gpt-4-and-gpt-4-turbo) and the data is up to April 2023. What is the best way to compare the models over time then? \n\n> We agree that robustness is not exclusively important, and we do not claim that.\n\nIn the abstract, the authors state \"Our findings indicate that, compared to earlier versions of LLMs, the updated versions do not exhibit the anticipated level of robustness against adversarial attacks.\" I am not sure what level of improvement we should anticipate. If the model is updated for function calling (https://openai.com/blog/function-calling-and-other-api-updates), what should we expect about the robustness?\n\nOther than those concerns, I think the paper is generally interesting. I raise the score to 5 for now."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2951/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700505648676,
                "cdate": 1700505648676,
                "tmdate": 1700505648676,
                "mdate": 1700505648676,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6ogyfajuSf",
                "forum": "eC4WlSZc4H",
                "replyto": "eOlYhiEh6k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2951/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2951/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your constructive comments and suggestions; they are exceedingly helpful in improving our paper.\n\n>**I don't think introducing robustness evaluation over time is a significant novelty.**\n\nWe advocate for a distinctive perspective in measuring LLM performance by examining the robustness of models updated `over time`. Note that LLMs are publicly available for millions of users. Robustness is, therefore, an important aspect for owners of LLMs since minor user input errors could result in unforeseen consequences. Unfortunately, our empirical findings reveal a prevailing oversight in addressing this concern. We acknowledge that the LLaMA framework incorporates robustness considerations within its training data. Nonetheless, our results indicate that their evaluations may overestimate the robustness of the updated model since even existing attacks considered in our work perform better on the updated model. Therefore, there remains a necessity for a holistic evaluation against adversarial attacks of the LLMs across various longitudinal versions.\n\n>**I assume the proposed benchmark will be used for future models**\n\nIt is an interesting point to consider the future compatibility of our evaluation methodology. This is indeed why we focus on the methodology of  `over-time robustness evaluation` independent of any dataset. Specifically, we indeed adopt existing datasets in our work. Following the same methodology, in the future, we could also choose other (existing) datasets that do not overlap with the training data, e.g., **the dataset with a CC BY-SA 4.0 license** or **constructed by ourselves**. It is a generally very trending topic to construct or select non-overlap datasets for LLM evaluation nowadays [5, 6]. We would like to discuss this topic further in the revised version.\n\n>**I am not sure what level of improvement we should anticipate. If the model is updated for function calling, what should we expect about the robustness?**\n\nWe thank the reviewer for pointing out the function calling service. This service provides a promising angle to incorporate our new finding that a new version of the model may not be more robust than its older counterpart. Specifically, based on our evaluation results, the users can choose the most robust version via function calling instead of sticking to the newest version that is less robust. On the other hand, for platforms that do not provide the function calling service, we would still expect that the new model would be more robust than the old model. In sum, we will provide the specific context when we say `expected robustness` and especially discuss the application of function calling.\n\n[5].Zhou, Kun, et al. \"Don't Make Your LLM an Evaluation Benchmark Cheater.\" arXiv preprint arXiv:2311.01964 (2023)\n\n[6]. https://twitter.com/keirp1/status/1724518513874739618"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2951/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571962087,
                "cdate": 1700571962087,
                "tmdate": 1700572123564,
                "mdate": 1700572123564,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iQCszhqJM8",
            "forum": "eC4WlSZc4H",
            "replyto": "eC4WlSZc4H",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2951/Reviewer_JE4Q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2951/Reviewer_JE4Q"
            ],
            "content": {
                "summary": {
                    "value": "Prior studies have primarily centered on specific versions of the LLMs, neglecting the possibility of new attack vectors emerging for updated versions. \nIn this paper, the authors perform a thorough assessment of the robustness of the longitudinal versions of LLMs with a focus on GPT-3.5 and LLaMA. Their findings indicate that the updated model does not exhibit heightened robustness against the proposed adversarial queries compared to its predecessor."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "* The research problem addressed in this paper is novel and previously underexplored. The findings are interesting and indicate that the majority of newly released LLMs lack robustness considerations. To promote responsible AI, technology giants should take into account the deployment of effective robustness-enhancing techniques and perform strict evaluations before releasing their latest LLMs. In particular, this paper demonstrates that both GPT-3.5 and LLaMA exhibit vulnerability to adversarial queries persistently across different versions. \n\n* The authors employ diverse evaluation metrics to offer a comprehensive assessment of various model versions. They find that the performances of the LLMs need to improve as versions evolve steadily. Specifically, GPT-3.5 v0613 exhibits a discernible decline in performance in some specific tasks.\n\n* This study involves a substantial workload. It encompasses the use of six distinct surrogate models and employs ten different settings for adversarial queries to ensure the thoroughness of the assessment."
                },
                "weaknesses": {
                    "value": "* In this work, the authors primarily focus on assessing adversarial robustness exclusively within various iterations of LLMs. The authors should broaden the scope of their investigation to encompass additional thematic categories across diverse subject matter domains.\nFurthermore, the authors should expand their evaluation efforts to encompass various dimensions of the model iterations they are not considering. Specifically, regarding the LLaMA model family, which includes models of varying architectural sizes, the authors should investigate and provide insights into the robustness of these models in the context of different parameter sizes.\n\n* The motivation of this paper is clear. However, the authors should elaborate more on the process of generating adversarial examples by different surrogate language models\n\n* I didn't find any results of LLaMA-30B, but the authors list this model in Section 4.2. I think the authors should provide some details about that."
                },
                "questions": {
                    "value": "* Could the authors provide an explanation for the underlying reason that caused the LLMs not to exhibit heightened robustness over time? Additionally, could they discuss potential strategies to address this issue? \n\n* Have the authors shared their findings with OpenAI or Meta to report this issue?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2951/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698583167574,
            "cdate": 1698583167574,
            "tmdate": 1699636239048,
            "mdate": 1699636239048,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bAZSvIcqbC",
                "forum": "eC4WlSZc4H",
                "replyto": "iQCszhqJM8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2951/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2951/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your constructive comments and suggestions; they are exceedingly helpful in improving our paper.\n\n>**The authors should investigate and provide insights into the robustness of these models in the context of different parameter sizes.**\n\nThanks for the valuable suggestions. Our current research is focused on the `time` dimension since we consider it the most straightforward dimension for comparing the robustness of two closely related models. We would like to consider the suggested extensions for future work, i.e., `more subject matter domains`, `dimensions`, and `model parameter settings`.\n\n>**The authors should elaborate more on the process of generating adversarial examples by different surrogate language models.**\n\nWe have added more details on how to generate the adversarial samples by different surrogate language models in Appendix.\n\n>**Results of LLaMA-30B**\n\nThis is indeed a typo, and we have removed it from the paper.\n\n> **Could the authors provide an explanation for the underlying reason that caused the LLMs not to exhibit heightened robustness over time? Additionally, could they discuss potential strategies to address this issue?**\n\nThe training details of LLMs are generally not available to the public. However, a simple observation is that the model trainer tends to consider model accuracy over robustness, partially because achieving adversarial robustness requires considerable additional effort. We have added the content to suggest the model trainer to use enhancing adversarial robustness methods in model upgrades in our discussion.\n\n> **Have the authors shared their findings with OpenAI or Meta to report this issue?**\n\nWe have not done it yet, and we would like to do it once the review process is finished."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2951/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700053871394,
                "cdate": 1700053871394,
                "tmdate": 1700478301099,
                "mdate": 1700478301099,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dj27HvIZ6e",
            "forum": "eC4WlSZc4H",
            "replyto": "eC4WlSZc4H",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2951/Reviewer_9mwd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2951/Reviewer_9mwd"
            ],
            "content": {
                "summary": {
                    "value": "This paper concludes that considerations for improving robustness should be integral when updating LLMs. This paper is well-written, with thorough experimental design and argumentation. It provides insightful contributions to the research on LLMs, emphasizing the importance of accounting for model version updates. Additionally, the proposed systematic design of adversarial queries should be considered a vital metric for assessing LLMs' performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. **Variation in Robustness Across Model Versions.** This paper, for the first time, investigates the variability in the robustness of LLMs using model versions as a variable and adversarial samples as input objects. Meanwhile, the study encompasses 12 different versions of two of the most popular LLMs, i.e., ChatGPT and LLaMA.\n\n2. **Comprehensive Analysis of Adversarial Attacks.** To comprehensively evaluate potential adversarial attacks on LLMs, this paper discusses 10 types of malicious queries (including zero-shot in-context learning and few-shot in-context learning scenarios). It also addresses different query elements, including descriptions, demonstrations, and questions by using multiple datasets such as PromptBench, GLUE, and AdvGLUE.\n\n3. **Impact of Model Version Updates.** Experimental results demonstrate that updates in model versions do not significantly improve benign performance on various downstream tasks (e.g., results of CTS in Figures 3 and 4). Simultaneously, the robustness of the models shows a decreasing trend (as observed in Figures 3 and 4 for PDR results)."
                },
                "weaknesses": {
                    "value": "1. **Enhancing Adversarial Robustness in Model Upgrades.** The author(s) should add a discussion on strategies for improving adversarial robustness during version upgrades of LLMs. In fact, in my opinion, this is an important part that can inspire the community to proceed further research on safety and security of LLM.\n\n\n2. **Considering New Features in Model Evolution.** Note that model updates may introduce new features. For example, recent versions of ChatGPT allow internet access. Future research could explore the correlation between online connectivity and robustness."
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2951/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2951/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2951/Reviewer_9mwd"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2951/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698749627435,
            "cdate": 1698749627435,
            "tmdate": 1699636238979,
            "mdate": 1699636238979,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0HdkHtuOsU",
                "forum": "eC4WlSZc4H",
                "replyto": "dj27HvIZ6e",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2951/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2951/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable feedback and pointing out the many positive aspects of our work. Below, we\u2019ll address the negative aspects you mentioned.\n\n>**Enhancing Adversarial Robustness in Model Upgrades.**\n\nThis is a good point. We have discussed the strategies in the paper to suggest the model trainer to use enhancing adversarial robustness methods in model upgrades.\n\n>**Considering New Features in Model Evolution.**\n\nThis is a promising direction for our future work. We have added the content about the new feature of GPT-3.5 in the paper. For the browse with bing of ChatGPT, it may be expected that new features are also vulnerable to existing attacks and may also introduce new attack surfaces that make LLMs even more vulnerable."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2951/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700053560663,
                "cdate": 1700053560663,
                "tmdate": 1700477178159,
                "mdate": 1700477178159,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "euQAyhgrpz",
            "forum": "eC4WlSZc4H",
            "replyto": "eC4WlSZc4H",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2951/Reviewer_4jLb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2951/Reviewer_4jLb"
            ],
            "content": {
                "summary": {
                    "value": "Large language models (LLMs) have significantly improved many cross-domain tasks. However, these models often overlook the impact of security and privacy when upgrading, which can lead to unintended vulnerabilities or biases. Previous studies have predominantly focused on specific versions of the models and disregard the potential emergence of new attack vectors targeting the updated versions. This paper conducts a comprehensive assessment of the robustness of successive versions of LLMs, vis-`a-vis GPT-3.5 and LLaMA."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Well-written.\n- The experiment was comprehensive."
                },
                "weaknesses": {
                    "value": "- Hope the author can provide a more detailed description of \"zero-shot ICL learning\" and \"few-shot ICL learning\" in Figure 2.\n- What does the second column \"Adversarial Query\" in Table 1 mean? Please clarify.\n- \"For instance, on the SST2 dataset, when applying BertAttack (Li et al., 2020) to create the adversarial description, the Robust Test Scores (see Section 4.3) for both versions of GPT-3.5 are almost 0.\" , which table or figure is being described specifically? Please clarify."
                },
                "questions": {
                    "value": "Please see \"Weaknesses\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2951/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2951/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2951/Reviewer_4jLb"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2951/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834739507,
            "cdate": 1698834739507,
            "tmdate": 1699636238914,
            "mdate": 1699636238914,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OuUkVABnth",
                "forum": "eC4WlSZc4H",
                "replyto": "euQAyhgrpz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2951/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2951/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your encouraging words and constructive comments. We sincerely appreciate your time reading the paper, and our point-to-point responses to your comments are below.\n\n>**Hope the author can provide a more detailed description of \"zero-shot ICL learning\" and \"few-shot ICL learning\" in Figure 2.**\n\nIf the reviewer means the description of the two terms, we will further clarify that we follow [1,2,3] on this terminology. Zero-shot learning means that the query includes only the description and the question but **without any demonstrations**, while few-shot learning means that the query also includes a few `demonstrations`. We have added the explanation in the caption of Figure 2. If the reviewer means the `description` component in the query, please see the detailed definitions in Section 2.1.\n\n[1]. Yongqin Xian et al. Zero-Shot Learning - A Comprehensive Evaluation of the Good, the Bad and the Ugly. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2019.\n\n[2]. Tom B. Brown et al. Language Models are Few-Shot Learners. In Annual Conference on Neural Information Processing Systems (NeurIPS). NeurIPS, 2020.\n\n[3]. Jason Wei et al. Finetuned Language Models are Zero-Shot Learners. In International Conference on Learning Representations (ICLR), 2022.\n\n>**What does the second column \"Adversarial Query\" in Table 1 mean? Please clarify.**\n\n`Adversarial Query` refers to the query that contains the adversarial content in any of its three components (`description`, `question`, and `demonstrations`), as defined in *Eq. (1)* and *Eq. (2)*. Specifically, each component can be clean (denoted as C) or adversarial (denoted as A). \nTherefore, in Table 1, the adversarial query `AC` means a zero-shot learning-based query that consists of an adversarial `description` and a clean `question`, and `AAC` means a few-shot learning-based query that consists of an adversarial `description`, an adversarial `question`, and clean `demonstrations`. We will describe it more clearly in Section 5.1.\n\n>**\"For instance, on the SST2 dataset, when applying BertAttack (Li et al., 2020) to create the adversarial description, the Robust Test Scores (see Section 4.3) for both versions of GPT-3.5 are almost 0.\" Which table or figure is being described specifically? Please clarify.**\n\nSorry, we indeed made a mistake here. It should be ''For instance, on the SST2 dataset, the average result of Robust Test Scores of zero-shot learning for both versions of GPT-3.5 dropped from 85.093% and 87.390% to 37.210% and 20.652%, respectively (see Figure 3).''"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2951/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700053286650,
                "cdate": 1700053286650,
                "tmdate": 1700477012830,
                "mdate": 1700477012830,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]