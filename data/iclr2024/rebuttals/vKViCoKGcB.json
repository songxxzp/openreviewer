[
    {
        "title": "Intriguing Properties of Data Attribution on Diffusion Models"
    },
    {
        "review": {
            "id": "jZuKDb43Xb",
            "forum": "vKViCoKGcB",
            "replyto": "vKViCoKGcB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5084/Reviewer_3yAm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5084/Reviewer_3yAm"
            ],
            "content": {
                "summary": {
                    "value": "This work considers the topic of data attribution with diffusion models, which means quantifying the importance of training examples to model generations. Previous work on this topic has explored a range of techniques, including with gradient-based calculations and retraining-based calculations, and this work builds off of TRAK (a gradient-based method that can leverage retraining runs, but can also avoid doing so). \n\nThe proposed method, D-TRAK, deviates from TRAK in ways that the authors describe as theoretically unjustified. Namely, they use a non-standard measure of parameter sensitivity for each generation, and they omit a term that should only be ignored when the loss and model output $\\mathcal{F}$ are identical. Surprisingly, they find that this heuristic version of TRAK works better than the original version, achieving better performance in the LDS metric that's designed around TRAK's original formulation.\n\nThis raises natural questions about why, which the authors don't answer. However, they observe consistently better performance than competing methods across several datasets, a couple evaluation approaches, and that performance improves when several retraining runs are used."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Data attribution is an interesting problem that's increasingly important with widely used generative models trained on web-scraped datasets. It's computationally and theoretically challenging, so developing new methods for this task is a valuable contribution. This work builds off of one the better-performing methods in the literature, TRAK, and observes performance for D-TRAK that makes it, to my knowledge, the most effective method available. And it preserves the advantages of TRAK, particularly the relative efficiency compared to methods that require many model retraining runs (e.g., the original datamodels approach from which TRAK is derived).\n\nThe evaluation is thorough, although it's restricted mostly to small datasets. This is perhaps understandable given the cost of running certain retraining based methods, but it would be helpful for the authors to clarify why they can't use ArtBench itself, for example. It seems like the cost of running TRAK/D-TRAK should be manageable, given that retraining is optional in these methods?\n\nThe experiments are also thorough in terms of the baselines considered. I appreciated the appendix section that concisely describes each method."
                },
                "weaknesses": {
                    "value": "The main weakness is that in terms of data attribution methodology, the contribution here is shallow. The paper essentially finds that a couple heuristics improve performance, and offers no explanation for why. The paper acknowledges this with statements like \"the mechanism of data attribution requires a deeper understanding,\" which are true, but this is not ideal for a publication. A paper proposing a new and improved method should offer some understanding of why it works, and the paper barely attempts to do so. The analysis in Section 3.2 about interpolating between TRAK and D-TRAK provides no insight on why D-TRAK works, it only shows that interpolated versions are strictly worse (which is not an especially interesting point, I don't see why we would have expected this to work).\n\nThe point above is my main concern, and I wonder whether this paper could be more valuable given time to revise it and offer an appropriate explanation for why D-TRAK works.\n\nSeveral other thoughts:\n\n- There is an emphasis in the introduction on the role of non-convexity in this setting, and its potential impact on TRAK not working as expected. After reading the paper, I'm not sure it ultimately addresses this point, or offers any explanation for how D-TRAK circumvents the issue. Can the authors expand on that subject in the paper?\n\n- In Section 2.2, I'm not sure it's correct that Shapley values are proposed to *evaluate* data attributions, they're suggested to define data attributions (or more specifically, data valuation scores). In any case, this would be due to Ghorbani & Zou, not Lundberg & Lee.\n\n- In Section 2.2, the lead-up to Definition 2 is hard to follow. It might be helpful to explicitly state that the LDS score considers the sum of attributions as an additive proxy for $\\mathcal{F}$, as it's explained in the original datamodels work.\n\n- In Section 2.3, the authors write that retraining-based methods offer better efficacy than gradient-based methods. Is there existing work that makes this point, or or is this an allusion to results showing that D-TRAK/TRAK improve when using retraining runs? I'm not clear on whether this point is generally true or only true for TRAK, so it could be helpful to clarify.\n\n- Many of the results reflect that the LDS score depends strongly on the number of time steps used when approximating the expectation $\\mathbb{E}_t$. It therefore seems like the results entangle two separate concerns: the intrinsic correctness of the attribution method, and the efficiency of estimation. Both matter, but interpreting the results is more difficult because we don't know if either method has converged to its theoretical value due to imprecision in the expectation. I wonder if this subject deserves more discussion in the paper. For example, is there a good reason why D-TRAK should be easier to estimate?\n\n- Can the authors explain the consistent drops in LDS scores when shifting from validation examples to generations? \n\n- Related to my main concern described above, do the authors have any other ideas for why their alternative $\\mathcal{L}$ formulations make sense? The point in eq. 8 doesn't tell us much. One intuition I have is that it's fundamentally difficult to predict the exact noise value, because there are many \"denoising paths\" to recover the original image when large noise is added. $\\mathcal{L}_{simple}$ therefore seems like a questionable choice to determine whether the model works well for a given image. I'm not sure the new proposals are more sensible, but perhaps they get at, in a limited sense, whether the predicted noise values are at least Gaussian? Further discussion around why they work seems important for the paper.\n\n- On a broader note, can the authors justify their choice to focus on the LDS metric defined via $\\mathcal{L}_{simple}$? It seems convenient and natural in a way, because it's how the model is trained, but I don't see an argument that this faithfully represents the model's tendency to generate particular images. I see that this design choice is motivated by prior work, but it seems like a strange implementation decision because it's disconnected from what we really care about, which is whether the model will produce certain generations."
                },
                "questions": {
                    "value": "Several questions are mentioned in the weaknesses section above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5084/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698882238627,
            "cdate": 1698882238627,
            "tmdate": 1699636499087,
            "mdate": 1699636499087,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6OAl4MZTQZ",
                "forum": "vKViCoKGcB",
                "replyto": "jZuKDb43Xb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5084/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5084/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5084/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5084/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3yAm [1/2]"
                    },
                    "comment": {
                        "value": "Thank you for your supportive review and suggestions. Below we try to offer appropriate explanations for why D-TRAK works and respond to the detailed comments.\n\n---\n\n***It would be helpful for the authors to clarify why they can't use ArtBench itself, for example. It seems like the cost of running TRAK/D-TRAK should be manageable, given that retraining is optional in these methods.***\n\nIndeed, running TRAK/D-TRAK is computationally efficient and scalable to larger datasets, given that retraining is optional in these methods. However, to compute LDS scores on a large-scale dataset, such as ArtBench, we must first build the corresponding LDS benchmark, which requires retraining a large number of models and accounts for the majority of the computational cost. We tried our hardest and scaled up to CIFAR10 and ArtBench-5 based on our maximum computation budget.\n\n---\n\n***Why do the interpolation in Section 3.2?***\n\nAs reported in Table 1, $\\\\mathcal{L}\\_{\\\\textrm{Square}}$, $\\\\mathcal{L}\\_{\\\\textrm{Avg}}$, $\\\\mathcal{L}\\_{\\\\textrm{$2$-norm}}$, and $\\\\mathcal{L}\\_{\\\\textrm{$1$-norm}}$ consistently outperform $\\\\mathcal{L}\\_{\\\\textrm{Simple}}$. The counterintuitive results in Table 1 motivate us to take a closer look at the difference between $\\\\nabla\\_{\\\\theta}\\\\mathcal{L}\\_{\\\\textrm{Simple}}$ and $\\\\nabla\\_{\\\\theta}\\\\mathcal{L}\\_{\\\\textrm{Square}}$. In Eq. (8), we observe that $\\\\nabla\\_{\\\\theta}\\\\mathcal{L}\\_{\\\\text{Simple}}$ consists of two terms: $\\\\mathbb{E}\\_{t,\\\\boldsymbol{\\\\epsilon}}\\\\left[2\\\\cdot\\\\boldsymbol{\\\\epsilon}^{\\\\top}\\\\nabla\\_{\\\\theta}\\\\boldsymbol{\\\\epsilon}\\_{\\\\theta}\\\\right]$ and $\\\\mathbb{E}\\_{t,\\\\boldsymbol{\\\\epsilon}}\\\\left[2\\\\cdot\\\\boldsymbol{\\\\epsilon}\\_{\\\\theta}^{\\\\top}\\\\nabla\\_{\\\\theta}\\\\boldsymbol{\\\\epsilon}\\_{\\\\theta}\\\\right]$, where the latter term is exactly $\\\\nabla\\_{\\\\theta}\\\\mathcal{L}\\_{\\\\text{Square}}$. Thus, we propose to interpolate between these two terms to see if there is a combination ratio that leads to a higher LDS score.\n\nSurprisingly, as shown in Figure 1, the most theoretically-justified $\\\\nabla\\_{\\\\theta}\\\\mathcal{L}\\_{\\\\textrm{Simple}}$ (i.e., $\\\\eta=0.5$) consistently has the lowest LDS scores. It seems that the poor performance of $\\\\mathcal{L}\\_{\\\\textrm{Simple}}$ is a result of the cancel-out effect of the above two terms, which might be related to the gradient obfuscation/saturation problem (as discussed in Section 5).\n\n---\n\n***The role of non-convexity.***\n\nNon-convexity is always a major factor leading to inferior performance of theoretically justified (in convex settings) attribution methods such as TRAK. However, non-convexity may not explain why D-TRAK empirically outperforms TRAK, because there is no evidence that D-TRAK should be superior in non-convex settings.\n\nIn addition to non-convexity, we notice another factor influencing TRAK performance, which may be associated with gradient obfuscation or saturation. Specifically, as seen in Figure 2, TRAK achieves its highest LDS score on *intermediate* checkpoints, whereas D-TRAK achieves its highest LDS score on *final* checkpoints. This indicates that the gradients of $\\\\nabla\\_{\\\\theta}\\\\boldsymbol{\\\\mathcal{L}}$ used in TRAK tend to saturate as the model converges, which may not be a good design choice for attributing the training objective $\\\\boldsymbol{\\\\mathcal{L}}$ itself, as discussed in Section 5.\n\nGiven these observations, it is hypothesized that D-TRAK benefits from using loss functions that are not identical but are connected to the training objective. This would prevent gradient obfuscation or saturation while retaining sufficient information for attribution.\n\n---\n\n***The role of Shapley values.***\n\nThank you for pointing out, we have corrected the reference to Ghorbani & Zou [1] in the revision. Regarding the use of Shapley values in evaluation, for both feature attribution and data attribution, the exact Shapley value should be able to be viewed as the ground truth to evaluate different attribution methods that aim to efficiently approximate the exact Shapley values.\n\n---\n\n***The lead-up to Definition 2 is hard to follow.***\n\nIn the revision, we explicitly state that the LDS score considers the sum of attributions as an additive proxy for $\\\\boldsymbol{\\\\mathcal{F}}$ following your suggestion."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5084/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700207897129,
                "cdate": 1700207897129,
                "tmdate": 1700207897129,
                "mdate": 1700207897129,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rDUBkeN2w8",
                "forum": "vKViCoKGcB",
                "replyto": "jZuKDb43Xb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5084/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5084/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5084/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5084/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3yAm [2/2]"
                    },
                    "comment": {
                        "value": "***Retraining-based methods offer better efficacy than gradient-based methods.***\n\nThis claim follows the TRAK paper [2]. In terms of optimizing LDS scores, retraining-based methods like the datamodels approach [3] accurately predict the model output on a target test input by retraining at most 1.5 million models on subsets of CIFAR-10. This approach also outperforms various methods in terms of predicting data counterfactuals. In Appendix A.1 of the TRAK paper [2], the authors show that datamodels (300K models) are the best when estimating prediction brittleness. Thus, with unlimited computation, retraining-based methods offer better efficacy than gradient-based methods by simply retraining many models to obtain accurate estimation.\n\n---\n\n***Is there a good reason why D-TRAK should be easier to estimate than TRAK?***\n\nWe deduce that this is because TRAK suffers from gradient obfuscation or saturation as we mentioned above, and therefore TRAK needs to aggregate information from more timesteps to provide effective attribution. In contrast, D-TRAK can obtain sufficient information from much less timesteps.\n\n---\n\n***Can the authors explain the consistent drops in LDS scores when shifting from validation examples to generations?***\n\nThis is because training examples and validation examples are both sampled from the true data distribution, whereas generated examples are sampled from the model distribution. In practice, there is always a gap between the true data distribution and the learned model distribution, so there are consistent drops in LDS scores when attributing generated data to training data, compared to attributing validation data.\n\n---\n\n***Do the authors have any other ideas for why their alternative $\\\\mathcal{L}$ formulations make sense?***\n\nAs reported in Table 1, $\\\\mathcal{L}\\_{\\\\textrm{Square}}$, $\\\\mathcal{L}\\_{\\\\textrm{Avg}}$, $\\\\mathcal{L}\\_{\\\\textrm{$2$-norm}}$, and $\\\\mathcal{L}\\_{\\\\textrm{$1$-norm}}$ consistently outperform $\\\\mathcal{L}\\_{\\\\textrm{Simple}}$, while $\\\\mathcal{L}\\_{\\\\textrm{$\\\\infty$-norm}}$ has worse performance. We deduce that this is because $\\\\mathcal{L}\\_{\\\\textrm{Square}}$, $\\\\mathcal{L}\\_{\\\\textrm{Avg}}$, $\\\\mathcal{L}\\_{\\\\textrm{$2$-norm}}$, and $\\\\mathcal{L}\\_{\\\\textrm{$1$-norm}}$ can better retain the information from $\\\\nabla\\_{\\\\theta}\\\\boldsymbol{\\\\epsilon}\\_{\\\\theta}$, while $\\\\mathcal{L}\\_{\\\\textrm{Simple}}=\\\\mathbb{E}\\_{t,\\\\boldsymbol{\\\\epsilon}}[2\\\\cdot(\\\\boldsymbol{\\\\epsilon}\\_{\\\\theta}-\\\\boldsymbol{\\\\epsilon})\\^{\\\\top}\\\\nabla\\_{\\\\theta}\\\\boldsymbol{\\\\epsilon}\\_{\\\\theta}]$ becomes ill-behaved when model converges (i.e., $\\\\boldsymbol{\\\\epsilon}\\_{\\\\theta}\\\\approx \\\\boldsymbol{\\\\epsilon}$ and thus $\\\\boldsymbol{\\\\epsilon}\\_{\\\\theta}-\\\\boldsymbol{\\\\epsilon}$ becomes a noisy error term). As to $\\\\mathcal{L}\\_{\\\\textrm{$\\\\infty$-norm}}$, it has worse performance since it discards too much information and only retains the maximum value.\n\n---\n\n***Can the authors justify their choice to focus on the LDS metric defined via $\\\\mathcal{L}\\_{\\\\textrm{Simple}}$?***\n\nIndeed, the design choice of defining LDS via $\\\\mathcal{L}\\_{\\\\textrm{Simple}}$ follows prior work [4], and it seems that $\\\\mathcal{L}\\_{\\\\textrm{ELBO}}$ may be a more reasonable choice since it provides a lower bound on log-likelihood. Nevertheless, recent work [5] shows that $\\\\mathcal{L}\\_{\\\\textrm{Simple}}$ can be regarded as a weighted integral of ELBOs over different noise levels, and can be equivalent to ELBO under monotonic conditions.\n\nBesides, in Section 4.4, we empirically perform counterfactual evaluation by measuring the $\\\\ell_{2}$-distance and CLIP similarity between the images generated before/after the exclusion of top-1000 positive influencers identified by D-TRAK and TRAK. Both the quantitative (Figures 3 and 11) and visualized (Figure 4) results show that D-TRAK is better than TRAK, which is consistent with the trend concluded from the LDS metric.\n\n---\n\n***References:*** \\\n[1] Data Shapley: Equitable Valuation of Data for Machine Learning. ICML 2019 \\\n[2] Trak: Attributing Model Behavior at Scale. ICML 2023 \\\n[3] Datamodels: Predicting Predictions from Training Data. ICML 2022 \\\n[4] The Journey, Not the Destination: How Data Guides Diffusion Models. ICML Workshop 2023 \\\n[5] Understanding Diffusion Objectives as the ELBO with Simple Data Augmentation. NeurIPS 2023"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5084/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700207980982,
                "cdate": 1700207980982,
                "tmdate": 1700207980982,
                "mdate": 1700207980982,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "63CZZaQtb1",
                "forum": "vKViCoKGcB",
                "replyto": "jZuKDb43Xb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5084/Reviewer_3yAm"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5084/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5084/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5084/Reviewer_3yAm"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thanks to the authors for their response. The clarifications are helpful, but the degree of insight on 1) why these alternative versions of TRAK are easier to estimate and 2) why they may be intrinsically more useful (i.e., even if all versions were perfectly estimated) remains somewhat unsatisfactory. I wonder if the authors could do a better job with these points given more time to revise the work, and perhaps design new experiments.\n\nFurthermore, regarding the last point about the choice of evaluation via LDS with ${\\mathcal{L}}_{\\text{simple}}$, my question was less about L_simple vs. L_ELBO - I was curious whether either have a clear relationship with the likelihood of a generation under the model's probability distribution. It seems like that's what we should try to assess with data attribution methods for diffusion models: for any generation, we care which training examples made them more likely. The current formulation seems convenient, and I see that it follows from prior work, but this paper doesn't seem to argue that it represents what we really care about.\n\nOverall, I'm not ready to raise my score. In terms of soundness and completeness I'm tempted to lower my score to marginally below acceptance, but I recognize that this is a timely topic that would be of interest to ICLR attendees."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5084/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700515876015,
                "cdate": 1700515876015,
                "tmdate": 1700515949443,
                "mdate": 1700515949443,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3mOjwozVt2",
            "forum": "vKViCoKGcB",
            "replyto": "vKViCoKGcB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5084/Reviewer_nsDq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5084/Reviewer_nsDq"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles the problem of attributing images generated by diffusion models back to the training data of these models. They make simple modifications to existing methods [1, 2] and report significantly better results on a standard evaluation metric (LDS) for this task. The authors do not provide a justification for the discrepancy in empirical results between their method and [1], and call for further exploration of the design choices within data attribution methods for diffusion models.\n\n\n[1] Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry. Trak:\nAttributing model behavior at scale. In International Conference on Machine Learning (ICML),\n2023.\n\n[2] Kristian Georgiev, Joshua Vendrow, Hadi Salman, Sung Min Park, and Aleksander Madry. The\njourney, not the destination: How data guides diffusion models. In Workshop on Challenges in\nDeployable Generative AI at International Conference on Machine Learning (ICML), 2023.\n\n---\n\nScore raised from 5 to 6 after authors' response."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors thoroughly test their proposed method on a number of datasets \n- The authors present strong empirical results across a variety of settings"
                },
                "weaknesses": {
                    "value": "- Out of the listed baselines, to the best of my knowledge only Journey TRAK [1] has been explicitly used for diffusion models in previous work. As the authors note, Journey TRAK is not meant to be used to attribute the *final* image $x$ (i.e., the entire sampling trajectory). Rather, it is meant to attribute noisy images $x_t$ (i.e., specific denoising steps along the sampling trajectory). Thus, the direct comparison with Journey TRAK in the evaluation section is not on equal grounds.\n\n- For the counterfactual experiments, I would have liked to see a comparison against Journey TRAK [1] used at a particular step of the the sampling trajectory. In particular, [1, Figure 2] shows a much larger effect of removing high-scoring images according to Journey TRAK, in comparison with CLIP cosine similarity. \n\n- Given that the proposed method is only a minor modification of existing methods [1, 2], I would have appreciated a more thorough attempt at explaining/justifying the changes proposed by the authors.\n\n[1] Kristian Georgiev, Joshua Vendrow, Hadi Salman, Sung Min Park, and Aleksander Madry. The\njourney, not the destination: How data guides diffusion models. In Workshop on Challenges in\nDeployable Generative AI at International Conference on Machine Learning (ICML), 2023.\n\n[2] Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry. Trak:\nAttributing model behavior at scale. In International Conference on Machine Learning (ICML),\n2023."
                },
                "questions": {
                    "value": "- Why is the rank correlation close to $0$ when $\\mathcal{L}\\_{square}$ is used for both $\\phi^s$ and $\\mathcal{F}$ (Table 4)?\nThe authors have observed that $\\mathcal{L}\\_{square}$ is useful as a model output function when predicting $\\mathcal{L}\\_{simple}$ or $\\mathcal{L}\\_{ELBO}$. It is particularly odd then that it does not do a good job at predicting *itself*. Is there any particular reason why that might be the case?\n\n- Why set $\\mathcal{Q}\\equiv \\frac{\\partial\\mathcal{L}}{\\partial\\mathcal{F}}$ to be the identity matrix even when $\\mathcal{F}\\neq \\mathcal{L}$?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5084/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5084/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5084/Reviewer_nsDq"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5084/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699303344877,
            "cdate": 1699303344877,
            "tmdate": 1700417185083,
            "mdate": 1700417185083,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HMrYqx6vSc",
                "forum": "vKViCoKGcB",
                "replyto": "3mOjwozVt2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5084/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5084/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5084/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5084/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nsDq [1/2]"
                    },
                    "comment": {
                        "value": "Thank you for your valuable review and suggestions, we have uploaded a paper revision including additional experiment results. Below we respond to the comments in Weaknesses (***W***) and Questions (***Q***).\n\n---\n\n***W1: The direct comparison with Journey TRAK in the evaluation section is not on equal grounds.***\n\nAs you mentioned, only Journey TRAK (J-TRAK) [1] has been explicitly used for diffusion models in previous work, but still not meant to be used to attribute the final image. This means that *there is no off-the-shelf baseline for D-TRAK*, so we have to actively adapt previous methods (including TRAK and J-TRAK), in order to construct baselines on attributing final images generated by diffusion models.\n\nAlthough we have made every effort to adapt previous methods as described in Appendix A.3, it is possible that stronger baselines could still be developed. We will make more rigorous clarification on the selection of baselines in the final reversion.\n\n---\n\n***W2: A comparison against Journey TRAK on counterfactual experiments.***\n\nFollowing your suggestions, we perform counterfactual experiments to compare D-TRAK and J-TRAK at specific steps of the sampling trajectory (please see Appendix E for details). As shown in Figures 21, 22, 23, and 24, J-TRAK and its ensembling version J-TRAK(8) can indeed induce a large effect on the generation results via removing high-scoring training images specific to a timestep. Nevertheless, our findings indicate that D-TRAK remains more effective at identifying images that have a larger impact on the generation results at a specific timestep (we evaluate at $t=300,400$ following [1]). Even more counterintuitive are these results in light of the fact that D-TRAK was not originally intended to attribute a specific sampling timestep.\n\n---\n\n***W3: A more thorough attempt at explaining/justifying the changes.***\n\nActually, we happened to find that $\\\\mathcal{L}\\_{\\\\textrm{Square}}$ counterintuitively works much better than $\\\\mathcal{L}\\_{\\\\textrm{Simple}}$ and $\\\\mathcal{L}\\_{\\\\textrm{ELBO}}$ on attributing diffusion models, so we also tried other common $\\\\ell_{p}$-norm and average pooling losses. As reported in Table 1, $\\\\mathcal{L}\\_{\\\\textrm{Square}}$, $\\\\mathcal{L}\\_{\\\\textrm{Avg}}$, $\\\\mathcal{L}\\_{\\\\textrm{$2$-norm}}$, and $\\\\mathcal{L}\\_{\\\\textrm{$1$-norm}}$ consistently outperform $\\\\mathcal{L}\\_{\\\\textrm{Simple}}$, while $\\\\mathcal{L}\\_{\\\\textrm{$\\\\infty$-norm}}$ has worse performance. We deduce that this is because $\\\\mathcal{L}\\_{\\\\textrm{Square}}$, $\\\\mathcal{L}\\_{\\\\textrm{Avg}}$, $\\\\mathcal{L}\\_{\\\\textrm{$2$-norm}}$, and $\\\\mathcal{L}\\_{\\\\textrm{$1$-norm}}$ can better retain the information from $\\\\nabla\\_{\\\\theta}\\\\boldsymbol{\\\\epsilon}\\_{\\\\theta}$, while $\\\\mathcal{L}\\_{\\\\textrm{Simple}}=\\\\mathbb{E}\\_{t,\\\\boldsymbol{\\\\epsilon}}[2\\\\cdot(\\\\boldsymbol{\\\\epsilon}\\_{\\\\theta}-\\\\boldsymbol{\\\\epsilon})\\^{\\\\top}\\\\nabla\\_{\\\\theta}\\\\boldsymbol{\\\\epsilon}\\_{\\\\theta}]$ becomes ill-behaved when model converges (i.e., $\\\\boldsymbol{\\\\epsilon}\\_{\\\\theta}\\\\approx \\\\boldsymbol{\\\\epsilon}$ and thus $\\\\boldsymbol{\\\\epsilon}\\_{\\\\theta}-\\\\boldsymbol{\\\\epsilon}$ becomes a noisy error term). As to $\\\\mathcal{L}\\_{\\\\textrm{$\\\\infty$-norm}}$, it has worse performance since it discards too much information and only retains the maximum value.\n\nFurthermore, non-convexity is always a major factor leading to inferior performance of theoretically justified (in convex settings) attribution methods such as TRAK. However, non-convexity may not explain why D-TRAK empirically outperforms TRAK, because there is no evidence that D-TRAK should be superior in non-convex settings. In addition to non-convexity, we notice another factor influencing TRAK performance, which may be associated with gradient obfuscation or saturation. Specifically, as seen in Figure 2, TRAK achieves its highest LDS score on *intermediate* checkpoints, whereas D-TRAK achieves its highest LDS score on *final* checkpoints. This indicates that the gradients of $\\\\nabla\\_{\\\\theta}\\\\boldsymbol{\\\\mathcal{L}}$ used in TRAK tend to saturate as the model converges, which may not be a good design choice for attributing the training objective $\\\\boldsymbol{\\\\mathcal{L}}$ itself, as discussed in Section 5.\n\nGiven these observations, it is hypothesized that D-TRAK benefits from using loss functions that are not identical but are connected to the training objective. This would prevent gradient obfuscation or saturation while retaining sufficient information for attribution."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5084/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700207655183,
                "cdate": 1700207655183,
                "tmdate": 1700207655183,
                "mdate": 1700207655183,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u9ddJjFFLC",
                "forum": "vKViCoKGcB",
                "replyto": "3mOjwozVt2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5084/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5084/Authors",
                    "ICLR.cc/2024/Conference/Submission5084/Reviewers/Submitted"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5084/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nsDq [2/2]"
                    },
                    "comment": {
                        "value": "***Q1: Why is the rank correlation close to 0 when $\\\\mathcal{L}\\_{\\\\textrm{Square}}$ is used for both  $\\\\phi\\^{s}$ and $\\\\boldsymbol{\\\\mathcal{F}}$ (Table 4)?***\n\nWe have discussed this phenomenon in Appendix B (the paragraph of $\\\\boldsymbol{\\\\mathcal{F}}$ selection). Specifically, as mentioned in Appendices E.1 and E.6 of the TRAK paper [2], there is a latent assumption of *linearity* on the model output function $\\\\boldsymbol{\\\\mathcal{F}}$ in linear datamodeling prediction tasks. If the chosen $\\\\boldsymbol{\\\\mathcal{F}}$ is not well approximated by a linear function of training examples, then that *puts an upper bound on the predictive performance of any attribution method.*\n\nTherefore, note that $\\\\boldsymbol{\\\\mathcal{F}}=\\\\mathcal{L}\\_{\\\\textrm{Simple}}$ can be linearly approximated since its value is small when the model converges (the model is trained by $\\\\boldsymbol{\\\\mathcal{L}}=\\\\mathcal{L}\\_{\\\\textrm{Simple}}$). In contrast, the value of $\\\\boldsymbol{\\\\mathcal{F}}=\\\\mathcal{L}\\_{\\\\textrm{Square}}$ remains large and cannot be well approximated by a linear function.\n\n---\n\n***Q2: Why set $\\\\mathcal{Q}$ to be the identity matrix even when $\\\\boldsymbol{\\\\mathcal{F}}\\\\neq\\\\boldsymbol{\\\\mathcal{L}}$?***\n\nIn our paper, we primarily focus on the settings where  $\\\\boldsymbol{\\\\mathcal{F}}=\\\\boldsymbol{\\\\mathcal{L}}$ and notice that the weighting term $\\\\mathcal{Q}$ becomes an identity matrix in TRAK. When we generalize TRAK to D-TRAK, we simply drop the weighting term $\\\\mathcal{Q}$ and discover that the performance is already satisfactory. As a result, there is *no weighting term $\\\\mathcal{Q}$ in D-TRAK* when we conduct ablation studies on D-TRAK in Table 4.\n\n---\n\n***References:*** \\\n[1] The Journey, Not the Destination: How Data Guides Diffusion Models. ICML Workshop 2023 \\\n[2] Trak: Attributing Model Behavior at Scale. ICML 2023"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5084/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700207749473,
                "cdate": 1700207749473,
                "tmdate": 1700207749473,
                "mdate": 1700207749473,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CNDyhXDDLW",
                "forum": "vKViCoKGcB",
                "replyto": "u9ddJjFFLC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5084/Reviewer_nsDq"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5084/Authors",
                    "ICLR.cc/2024/Conference/Submission5084/Reviewers/Submitted"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5084/Reviewer_nsDq"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "I am satisfied with the authors' response and I raise my score.\n\n## Response to W2\nI appreciate the authors adding these experiments, I find the results very interesting. in particular:\n1. It is interesting to see that D-TRAK remains effective when used at a particular timestep. To me, this further corroborates the authors' findings.\n\n2. Regarding:\n> Take the 4th row in both Figures 22 and 24 as an example, for the timestep 400, J-TRAK (8) leads the diffusion model to generate a man with the white beard. However, for the timestep 300, it is a man with brown bread and an extra hat. In contrast, D-TRAK removes the same set of influential training examples for different timesteps and thus produces a consistent generation: a still-life painting for both timestep 400 and 300. The above phenomenon again highlights the differences between Journey TRAK and D-TRAK.\n\n     While this consistency of D-TRAK across timesteps is somewhat expected given the choice of $\\mathcal{L}$, it is intriguing to see it holding up in practice.\n\nA few follow-up questions:\n1. What is the compute budget used for D-TRAK in Appendix E? Specifically, how many noise resampling (and how many models) is it averaged over?\n2. Regarding D-TRAK producing the same scores across timesteps: does that hold throughout the entire diffusion trajectory? E.g., are the scores when using timestep $t=0$ consistent with the ones obtained from using $t=1000$? (I understand there may not be enough time during the remainder of the rebuttal period to run additional experiments)."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5084/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700417145262,
                "cdate": 1700417145262,
                "tmdate": 1700417145262,
                "mdate": 1700417145262,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vFhRwsFRPk",
            "forum": "vKViCoKGcB",
            "replyto": "vKViCoKGcB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5084/Reviewer_yBWG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5084/Reviewer_yBWG"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes variants of TRAK for diffusion models by calculating the gradients with respect to various different loss functions and discover that alternative loss functions perform better than the original TRAK in terms of LDS, a data attribution metric."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is simple and easy to follow. The extensive experiments on different settings provide solid evidence of D-TRAK performing better than TRAK in terms of LDS. Readers can be easily convinced that there is an issue with either TRAK as a data attribution method or LDS as a data attribution metric."
                },
                "weaknesses": {
                    "value": "Despite the solid experiment results, the desiderata of a data attribution paper is different from an adversarial attack paper. For adversarial attacks, the success of an attack is a sufficient contribution. This could not be said for data attribution. Successfully finding techniques to optimize for a data attribution metric is only meaningful **if the technique reveals insight**, because in practice attackers have no control over the data attribution method. Therefore, unlike writing adversarial attack papers, more insight to explain why changing the loss function would lead to better LDS score should be provided. Is the non-convexity at fault? Is LDS not an appropriate metric? Is there something special about diffusion models that TRAK fails to capture? How does different norm losses lead to better or worse LDS score?\n\nThe observation of this paper is extremely interesting. Providing some insights about the success of D-TRAK over TRAK would make it publication-worthy."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5084/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5084/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5084/Reviewer_yBWG"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5084/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699313968089,
            "cdate": 1699313968089,
            "tmdate": 1700673589584,
            "mdate": 1700673589584,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "biC6qvoSYs",
                "forum": "vKViCoKGcB",
                "replyto": "vFhRwsFRPk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5084/Authors"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5084/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5084/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5084/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yBWG"
                    },
                    "comment": {
                        "value": "Thank you for your valuable review and suggestions. Below we try to provide some insights about the success of D-TRAK over TRAK, and respond to the comments in Weaknesses.\n\n---\n\n***Is LDS not an appropriate metric?***\n\nWe deem LDS as an appropriate metric based on two considerations:\n\n- First, both D-TRAK and TRAK satisfy the *additive* assumption of LDS [1,2], so it is reasonable to follow LDS in evaluating (from the aspect of Spearman rank correlation) the counterfactual predictors constructed from D-TRAK and TRAK.\n\n- Second, in Section 4.4, we empirically perform counterfactual evaluation by measuring the $\\\\ell_{2}$-distance and CLIP similarity between the images generated before/after the exclusion of top-1000 positive influencers identified by D-TRAK and TRAK. Both the quantitative (Figures 3 and 11) and visualized (Figure 4) results show that D-TRAK is better than TRAK, which is consistent with the trend concluded from the LDS metric.\n\n---\n\n***Is the non-convexity at fault?***\n\nPartially yes, non-convexity is always a major factor leading to inferior performance of theoretically justified (in convex settings) attribution methods such as TRAK. However, non-convexity may not explain why D-TRAK empirically outperforms TRAK, because there is no evidence that D-TRAK should be superior in non-convex settings.\n\nIn addition to non-convexity, we notice another factor influencing TRAK performance, which may be associated with gradient obfuscation or saturation. Specifically, as seen in Figure 2, TRAK achieves its highest LDS score on *intermediate* checkpoints, whereas D-TRAK achieves its highest LDS score on *final* checkpoints. This indicates that the gradients of $\\\\nabla\\_{\\\\theta}\\\\boldsymbol{\\\\mathcal{L}}$ used in TRAK tend to saturate as the model converges, which may not be a good design choice for attributing the training objective $\\\\boldsymbol{\\\\mathcal{L}}$ itself, as discussed in Section 5.\n\nGiven these observations, it is hypothesized that D-TRAK benefits from using loss functions that are not identical but are connected to the training objective. This would prevent gradient obfuscation or saturation while retaining sufficient information for attribution.\n\n---\n\n***Is there something special about diffusion models that TRAK fails to capture?***\n\nAs discussed in Section 5, our main insight is that when the model output function of interest $\\\\boldsymbol{\\\\mathcal{F}}$ is the same as the training objective $\\\\boldsymbol{\\\\mathcal{L}}$, the gradients of $\\\\nabla\\_{\\\\theta}\\\\boldsymbol{\\\\mathcal{L}}$ may not be a good design choice for attributing $\\\\boldsymbol{\\\\mathcal{L}}$ itself. Although in this paper we focus on diffusion models, we deduce that *this insight also holds on classification models.* For example, [2] has reported that using $\\\\nabla\\_{\\\\theta}\\\\boldsymbol{\\\\mathcal{L}}$ in TRAK leads to worse performance than using $\\\\nabla\\_{\\\\theta}\\\\log(\\\\exp(\\\\boldsymbol{\\\\mathcal{L}}(\\\\boldsymbol{x};\\\\theta))-1)$ on classification problems, even if the LDS score is computed by setting $\\\\boldsymbol{\\\\mathcal{F}}=\\\\boldsymbol{\\\\mathcal{L}}$. Besides, selecting intermediate checkpoints has also been discussed when attributing classifiers [3].\n\n---\n\n***How do different norm losses lead to better or worse LDS scores?***\n\nWe need to clarify that we did not adversarially search for loss functions to optimize the LDS metric. Actually, we happened to find that $\\\\mathcal{L}\\_{\\\\textrm{Square}}$ counterintuitively works much better than $\\\\mathcal{L}\\_{\\\\textrm{Simple}}$ and $\\\\mathcal{L}\\_{\\\\textrm{ELBO}}$, so we also tried other common $\\\\ell_{p}$-norm and average pooling losses.\n\nAs reported in Table 1, $\\\\mathcal{L}\\_{\\\\textrm{Square}}$, $\\\\mathcal{L}\\_{\\\\textrm{Avg}}$, $\\\\mathcal{L}\\_{\\\\textrm{$2$-norm}}$, and $\\\\mathcal{L}\\_{\\\\textrm{$1$-norm}}$ consistently outperform $\\\\mathcal{L}\\_{\\\\textrm{Simple}}$, while $\\\\mathcal{L}\\_{\\\\textrm{$\\\\infty$-norm}}$ has worse performance. We deduce that this is because $\\\\mathcal{L}\\_{\\\\textrm{Square}}$, $\\\\mathcal{L}\\_{\\\\textrm{Avg}}$, $\\\\mathcal{L}\\_{\\\\textrm{$2$-norm}}$, and $\\\\mathcal{L}\\_{\\\\textrm{$1$-norm}}$ can better retain the information from $\\\\nabla\\_{\\\\theta}\\\\boldsymbol{\\\\epsilon}\\_{\\\\theta}$, while $\\\\mathcal{L}\\_{\\\\textrm{Simple}}=\\\\mathbb{E}\\_{t,\\\\boldsymbol{\\\\epsilon}}[2\\\\cdot(\\\\boldsymbol{\\\\epsilon}\\_{\\\\theta}-\\\\boldsymbol{\\\\epsilon})\\^{\\\\top}\\\\nabla\\_{\\\\theta}\\\\boldsymbol{\\\\epsilon}\\_{\\\\theta}]$ becomes ill-behaved when model converges (i.e., $\\\\boldsymbol{\\\\epsilon}\\_{\\\\theta}\\\\approx \\\\boldsymbol{\\\\epsilon}$). As to $\\\\mathcal{L}\\_{\\\\textrm{$\\\\infty$-norm}}$, it has worse performance since it discards too much information and only retains the maximum value.\n\n---\n\n***References:*** \\\n[1] Datamodels: Predicting Predictions from Training Data. ICML 2022 \\\n[2] Trak: Attributing Model Behavior at Scale. ICML 2023 \\\n[3] Estimating Training Data Influence by Tracing Gradient Descent. NeurIPS 2020"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5084/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700207505773,
                "cdate": 1700207505773,
                "tmdate": 1700207505773,
                "mdate": 1700207505773,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fRjGOOqtDn",
                "forum": "vKViCoKGcB",
                "replyto": "biC6qvoSYs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5084/Reviewer_yBWG"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5084/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5084/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5084/Reviewer_yBWG"
                ],
                "content": {
                    "comment": {
                        "value": "These are fair arguments. The key insight of ``the information $\\nabla_\\theta \\epsilon_\\theta$ of being better retained in the norm-based losses'' should be highlighted in the paper. Currently the difference between the gradient of different loss terms is only briefly mentioned in Sec 3.2. An additional study should also be provided to isolate whether $\\epsilon - \\epsilon_\\theta$ causing the diminishing of $\\nabla_\\theta \\epsilon_\\theta$ signal is the main culprit for worse data attribution."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5084/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700250845237,
                "cdate": 1700250845237,
                "tmdate": 1700250845237,
                "mdate": 1700250845237,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BLSr1vevpS",
                "forum": "vKViCoKGcB",
                "replyto": "aQ8FzqhWXT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5084/Reviewer_yBWG"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5084/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5084/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5084/Reviewer_yBWG"
                ],
                "content": {
                    "comment": {
                        "value": "Can the authors please clarify what does ``the cancel-out effect induced by $\\epsilon_\\theta - \\epsilon$'' mean exactly? Based on context I am guessing it is the cancellation between $\\epsilon_\\theta$ and $\\epsilon$ when the model has converged and is capable of denoising. If this is the case, does the term $\\eta\\epsilon_\\theta - (1 - \\eta)\\epsilon$ actually carry information or does any non-zero random vector suffices the retaining of information from $\\nabla_\\theta \\epsilon_\\theta$?"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5084/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548797552,
                "cdate": 1700548797552,
                "tmdate": 1700548797552,
                "mdate": 1700548797552,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lrsL4T8Yyn",
                "forum": "vKViCoKGcB",
                "replyto": "dvNjM5utqq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5084/Reviewer_yBWG"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5084/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5084/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5084/Reviewer_yBWG"
                ],
                "content": {
                    "comment": {
                        "value": "The extremely low LDS score of applying a random vector is somewhat shocking since according to Figure 1, a wide range of $\\eta$ leads to similarly good LDS score. LDS is quite decent even when $\\eta = 0$ which corresponds to $\\epsilon^{T}  \\nabla_\\theta \\epsilon_{\\theta}$. Isn't $\\epsilon$ simply sampled from a normal distribution? What kind of information is the ``$\\eta \\epsilon_\\theta - (1 - \\eta) \\epsilon$'' term carrying when $\\eta = 0$?"
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5084/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640299626,
                "cdate": 1700640299626,
                "tmdate": 1700640299626,
                "mdate": 1700640299626,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iX2rv3ZziX",
                "forum": "vKViCoKGcB",
                "replyto": "dsfaNnOA0n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5084/Reviewer_yBWG"
                ],
                "readers": [
                    "everyone",
                    "ICLR.cc/2024/Conference/Program_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Senior_Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Area_Chairs",
                    "ICLR.cc/2024/Conference/Submission5084/Reviewers",
                    "ICLR.cc/2024/Conference/Submission5084/Reviewers/Submitted",
                    "ICLR.cc/2024/Conference/Submission5084/Authors"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5084/Reviewer_yBWG"
                ],
                "content": {
                    "comment": {
                        "value": "I see. The symmetry in Figure 1 makes perfect sense then if we view $\\epsilon$ and $\\epsilon_\\theta$ as almost equivalent (when the model converges).\n\nAs there are no more concerns, I have increased the score from 5 to 6. The reason for 6 as opposed to 8 is the lack of theoretical support for justifying the D-TRAK function choice. Currently the sole argument is based on cancellation but the cancellation can potentially be mitigated in other ways as well. Nevertheless, this is an interesting observation that the research community should also find ``intriguing\u2019\u2019."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5084/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673559973,
                "cdate": 1700673559973,
                "tmdate": 1700673559973,
                "mdate": 1700673559973,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]