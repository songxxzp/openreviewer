[
    {
        "title": "Consistency Models as a Rich and Efficient Policy Class for Reinforcement Learning"
    },
    {
        "review": {
            "id": "ZcvHdeMTha",
            "forum": "v8jdwkUNXb",
            "replyto": "v8jdwkUNXb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6598/Reviewer_JCUm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6598/Reviewer_JCUm"
            ],
            "content": {
                "summary": {
                    "value": "It is known that the inference process of the diffusion model can be slow. In the context of RL, the diffusion model has been introduced and widely adopted recently. The authors focus on addressing the slow inference speed issue of diffusion in this paper. Their solution is to replace the diffusion model with the recently proposed consistency model. The authors make some minor adaptations of consistency models to make it fit the RL setting. The authors also test their method across offline RL setting, online RL setting, and offline2online finetuning setting, by building a consistency model on top of behavior cloning and actor-critic structure."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "# Strengths\n\nThis paper pinpoints an interesting question that the current diffusion-based offline RL algorithms (e.g., Diffusion-QL) suffer from slow inference speed. The authors then propose to address this issue by using an existing method, the consistency model, and adapting it to the offline RL setting. This, as far as the reviewer can tell, is the first work that introduces the consistency model into the offline RL setting, online RL setting and offline2online setting. This paper is generally quite well-written, and I enjoy reading this work. The structure of this paper is clear and easy to follow. It is easy for the readers to capture the core points/conclusions in the experiment section. The authors conduct several experiments in D4RL domains like MuJoCo, AntMaze, etc., to show that their method can reduce the training time and inference time while maintaining comparable performance. The authors also do a good job in the related work part."
                },
                "weaknesses": {
                    "value": "# Weaknesses\n\nDespite the aforementioned strengths of this paper, I think this paper is below the acceptance bar of this venue. Please refer to the following comments\n\n- (major) The novelty of this paper is limited. The authors simply borrow an existing method from the vision community and apply it to the RL tasks, with some minor modifications. I do not see much novelty in doing so. This paper seems more like a technical report or experimental report, in that the authors conduct several experiments and summarize the conclusions. One serious flaw of this paper is that the author merely reports some experimental phenomenons while the corresponding explanations and discussion are unfortunately missing. From an ICLR paper of this kind, I would expect to understand why such a phenomenon occurs. This paper leaves me with more questions than answers. For example, why on many offline and offline2online tasks, the consistency model underperform the diffusion model, while on some online hard tasks, the consistency model seems to be better?\n\n- (major) This paper does not consider statistical significance. Written statements and the presentation of the results as tables (often without standard deviations) obscure this flaw. In fact, ALL tables in this paper do not include any signal of statistical significance for baseline methods, e.g., std, IQM [1]. We have reached a point of maturity in the field where claims need to be made in reference to actual statistical evidence, which seems to be lacking in the current presentation.\n\n[1] Deep reinforcement learning at the edge of the statistical precipice. NeurIPS\n\n- (major) The proposed consistency-BC or consistency-AC do not show much improvement over the diffusion-based counterparts. If one looks at the experiments in this paper, it is clear that the consistency-BC and consistency-AC cannot beat diffusion-BC or diffusion-QL on most of the tasks. The authors say that consistency-AC or consistency-BC can achieve less training time. Well, I do not see this as an appealing advantage over the diffusion-based methods, since training cost is somewhat unimportant, while the most critical part, from my perspective, is the inference speed. Let us then take a look at the inference cost. Based on Table 3 in the main text, it is clear that the inference speed of diffusion-QL is quite similar to that of the consistency-AC. For instance, when setting $N=5$, the inference speed of diffusion-QL gives 3.76ms and consistency-AC gives 3.39ms, while their performance differs (diffusion-QL has an average score of 108.2, while consistency-AC only has 101.4). I actually do not see many advantages of utilizing consistency-AC or consistency-BC in practice\n\n- (major) Even worse, it seems that the performance of consistency-AC and consistency-BC are acquired by carefully tuning the hyperparameters. As a piece of evidence, one can see the hyperparameter setup in Appendix C (Table 5). This indicates that the generality and effectiveness of the proposed method are limited\n\n- (minor) On page 3, Eq 3 is the objective using clipped double Q-learning instead of vanilla double Q-learning. The authors ought to cite the TD3 paper here instead of the double Q-learning paper.\n\n- (minor) No codes are provided in this paper, and the authors do not include a reproducibility statement section in the main text"
                },
                "questions": {
                    "value": "- why do you use different baselines on different domains? It is somewhat confusing that you use some quite weak baselines on domains like antmaze, adroit, and kitchen. As an example, why do you use AWR, BRAC, and REM on adroit tasks and kitchen tasks in Table 1? It seems to me that the advantages of consistency-BC and consistency-AC are illustrated by carefully picking the baselines.\n\n- how important is the gradient norm to the consistency-BC and consistency-AC algorithms?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6598/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698401856441,
            "cdate": 1698401856441,
            "tmdate": 1699636751939,
            "mdate": 1699636751939,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1UJj1KAW9o",
                "forum": "v8jdwkUNXb",
                "replyto": "ZcvHdeMTha",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6598/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6598/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to review [1/2]"
                    },
                    "comment": {
                        "value": "We appreciate the insightful comments and suggestions from the reviewer. We will try our best to answer those questions raised by the reviewer to remove his/her concerns.\n\nThe novelty concern is replied in the above general response.\n\nWe appreciate the reviewer for raising the question regarding why consistency models are able to achieve higher online performances than diffusion models. Our conjecture is that the expressiveness of a model is more essential in offline setting than online setting, especially when multi-modal distributions appear in offline dataset and the algorithm is of the behavior-cloning (BC) type. Our results of BC in offline setting as Tab.1 show that the expressive policies can significantly improve the BC performances, which can be helpful for policies to quickly converge in offline RL.  However for online RL in Markov decision process, the theoretically optimal policy can always be deterministic, which indicates that the expressive models will shrink back to unimodal to approach optimality, thus expressiveness may only help with initial exploration instead of final convergence of the policy. This conjecture can be hard to verify consistently due to the heterogeneous distributions of different tasks. However, this is an interesting question and we will explain this in a modified version of the paper.\n\n**Statistical significance**. \nFor all the results derived by ourselves, we reported the standard deviations to show its statistical significance, for both algorithm performances as Tab.1,2,4 and Fig. 3 and time consumption as Tab. 3, also the learning curves in Fig. 4 are shaded by 95% confidence intervals. Other results without standard deviations are adapted from previous papers, where most of the values are reported without statistical significance. The results of BC, BEAR, BRAC, AWR, BCQ, SAC and CQL can be found in D4RL paper and CQL paper, all without standard deviations reported. The results of DT, TD3+BC, Onestep RL, AWAC, IQL can be found in IQL paper, Diffuser and IDQL can be found in IDQL paper, results of $\\mathcal{X}$-QL and MoRel can be found in their own papers, these are all reported without standard deviations as well. For Tab. 1 and 2, we manage to find the standard deviations reported for ARQ and already fill the values in the modified draft. The results of SAC, AWAC, ACA in Tab. 4 are also reported without standard deviations in ACA paper. It turns out our paper is the very few papers reporting the statistical significance of our own algorithms in the domain. We appreciate the attention of the reviewer for the statistical significance of the results and we have tried our best to satisfy this requirement. \n\n**Performances**.\nWe admit the current consistency model does not yield better performances than diffusion models for offline RL settings, and we do not expect it since otherwise the consistency model will not just be an acceleration method of the diffusion model but a total replacement of it. There is always the trade-off of computational efficiency and the model accuracy. We agree that the inference speed is more critical for RL. However, the Tab. 3 may be interpreted in another way by the reviewer as we intended. As also in above general response, in Tab. 3, our major claim is that Consistency-AC can achieve a similar performance as Diffusion-QL with a much smaller $N$, instead of comparing with the same $N$. Consistency-AC saturates its performances with $N=2$ for most of the tasks in our experiments rather than just this one. Specifically, if we look at $N=2$ for Consistency-AC while $N=5$ for Diffusion-QL, the scores are similar but Consistency-AC saves half inference time (1.84/3.76) and training time (31.94/57.06). By default in our experiments, Consistency-AC and Consistency-BC use $N=2$ while Diffusion-BC and Diffusion-QL use $N=5$ for results reported in paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6598/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700169706159,
                "cdate": 1700169706159,
                "tmdate": 1700169818355,
                "mdate": 1700169818355,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tkMOAlPoqL",
                "forum": "v8jdwkUNXb",
                "replyto": "ZcvHdeMTha",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6598/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6598/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to review [2/2]"
                    },
                    "comment": {
                        "value": "**Hyperparameter tuning**. We may disagree with the reviewer that hyperparameter tuning is making the proposed method less convincing. Actually, the hyperparameters in Tab. 5 are all inherited from Diffusion-QL paper [3] except for the Q norm, some are not reported in the previous paper but can be found in their experiments. Most of the present state-of-the-art algorithms for specific domains require hyperparameter tuning, which is quite common in literature, with evidence in Tab. 3 of paper [3] and Tab. 4 of paper [4]. They may not report full hyperparameters in the algorithms, and some other papers do not even report their hyperparameters, but this does not indicate that there is no hyperparameter tuning in their reported results. We don\u2019t see any reason that our reported results with good hyperparameter tuning indicates the generality of the method is bad. We can understand the concern if the reviewer is asking how robust the proposed method is against each hyperparameter. This can be computationally heavy to evaluate and we do not see a lot of previous work reporting it. This is further answered in later question regarding the choice of gradient norm.\n\nWe thank the reviewer for pointing out the mis-reference of Eq. 3, and will modify it in the modified version.\n\nWe plan to open-source the code after the review process and the clean-up of the code. We have added the statement in the modified version, thanks for pointing out.\n\nFor the questions of different baseline on different domains, those baseline results are adapted from previous Diffusion-QL paper [3] but not cherry-picked by us. We guess one reason that those baselines are filled on *Adroit* and *Kitchen* tasks is because some strong baselines (like MoRel, DT, Diffuser, etc) on Gym tasks do not report their values on *Adroit* and *Kitchen* tasks while these 'weak' baselines are reported. Our paper just tries to report thoroughly about previous baselines but not take any cherry-picking.\n\nFor the question regarding the gradient norm, most of the gradient norm values are directly inherited from Diffusion-QL paper [3] given the usage of the same neural network architectures, which means we do not thoroughly search over the values. Exception exists for *Kitchen* tasks where we reduce the gradient norm values to stabilize training. However, this is still a rough tuning instead of a thorough searching over the values. A thorough search over hyperparameters can be computationally very expensive. We found the Q value normalization and coefficient $\\eta$ to be more influential on performances than gradient norm.\n\nReferences:\n\n[3] Wang, Zhendong, Jonathan J. Hunt, and Mingyuan Zhou. \"Diffusion policies as an expressive policy class for offline reinforcement learning.\" arXiv preprint arXiv:2208.06193 (2022).\n\n[4] Garg, Divyansh, et al. \"Extreme q-learning: Maxent RL without entropy.\" arXiv preprint arXiv:2301.02328 (2023)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6598/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700169779453,
                "cdate": 1700169779453,
                "tmdate": 1700169880821,
                "mdate": 1700169880821,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CcXUSCzOG1",
                "forum": "v8jdwkUNXb",
                "replyto": "tkMOAlPoqL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6598/Reviewer_JCUm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6598/Reviewer_JCUm"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your rebuttal. Please find the comments below.\n\n> novelty\n\nI hold my opinion that simply borrowing an existing method from the vision community and applying it to the RL tasks is under the bar of top-tier conferences like ICLR. However, I agree with the authors that it is valuable to propose the usage of more efficient yet expressive models for RL, and agree that there are some modifications to the consistency model. I would not criticize the authors too much on this point.\n\n> why on many offline and offline2online tasks, the consistency model underperform the diffusion model, while on some online hard tasks, the consistency model seem to be better?\n\nThe authors do not explain this well. Offline2online tasks also involve the online phase, while the consistency model can underperform the diffusion model, while on some online tasks from scratch, the consistency model can be better. The inherent reasons are not clear. I would say further experiments are needed. I understand that the rebuttal period of this venue is about to close, and I hope the authors can add these to future versions of this manuscript.\n\n> It turns out our paper is the very few papers reporting the statistical significance of our own algorithms in the domain\n\nCannot agree with that. Numerous existing papers consider statistical significance, e.g., [1,2]. I strongly believe in the realm of RL, it is vital to consider the statistical significance of both baselines and the proposed methods. The authors can find baseline results with statistical significance in other published papers.\n\n[1] Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble\n\n[2] Revisiting the Minimalist Approach to Offline Reinforcement Learning\n\n> performance\n\nThanks for the clarification. But I do not think my previous comments are *interpreted in another way*. I can now see that the consistency model can aid faster convergence, while it still underperforms the diffusion model. The performance of the consistency model does not seem to be better with larger $N$. It would be good if the authors could elaborate on further improving the performance of the consistency model in the offline setting.\n\n> hyperparameter tuning\n\nCannot agree with that. As the authors comment, it is computationally heavy to find the best hyperparameter. My concern is, how can we quickly find the best hyperparameters in some new tasks? If the consistency model can exhibit robustness to some hyperparameters, this concern can be mitigated to some extent. While this seems to be lacking in the current version.\n\n> different baselines for different domains\n\nThe authors can find recent baselines on Adroit in recent papers, e.g., [2]. It would be better if the authors could run baselines on kitchen datasets. I also do not want to blame the authors too much on this point and I can totally understand that training consistency model itself can be computationally heavy.\n\n> gradient norm\n\nI think the influence of gradient norm ought to be included in the paper. It is somewhat strange that you tune this on kitchen datasets but not on other tasks.\n\nAll in all, I am still somewhat negative about this submission. However, I decided to stand neutral during voting, and would not be frustrated if this paper got accepted."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6598/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634694749,
                "cdate": 1700634694749,
                "tmdate": 1700634694749,
                "mdate": 1700634694749,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PescVXBHBq",
                "forum": "v8jdwkUNXb",
                "replyto": "ZcvHdeMTha",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6598/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6598/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks a lot for your constructive feedback and insightful questions.\n\n> why on many offline and offline2online tasks, the consistency model underperform the diffusion model, while on some online hard tasks, the consistency model seem to be better?\n\nWe are very willing to share insights about this phenomenon in future work with more extensive experiments. Our current conjecture is the deficiency within the offline pre-trained models and a straightforward usage of offline models with online data. Future plans involve better ways combining pre-trained models and online data, but it can be beyond the scope of current paper since it mostly focuses on the effectiveness of consistency model from offline data. \n\n> statistical significance \n\nWe totally agree with the reviewer that the statistical significance is essential for RL. [1][2] are methods not included in current paper but can definitely be added as baselines. However there are lots of methods in this domain and we can only list some most representative ones with limited pages.\n\n> gradient norm\n\nThe gradient norm is tuned for tasks that are observed to show less stable training progress.\n\nWe generally appreciate your comments. However, some additionally required experiments like evaluating the robustness of the models against each hyperparameter, or running some baselines on certain tasks for providing the statistical significance can take lots of additional efforts and computation. If our response has addressed some of your concerns, we would highly appreciate it if you could re-evaluate our work and consider raising the score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6598/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677946055,
                "cdate": 1700677946055,
                "tmdate": 1700686116720,
                "mdate": 1700686116720,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LNY2K5oLYw",
            "forum": "v8jdwkUNXb",
            "replyto": "v8jdwkUNXb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6598/Reviewer_gzrQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6598/Reviewer_gzrQ"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes utilizing the consistency model to learn policies in modeling multi-modal data from image generation, which perform more efficiently than diffusion models. The authors evaluate their models on three typical RL settings: offline, offline-to-online, and online, and experiments show that the consistency policy can reach comparable performances than the diffusion policies while reducing half computation costs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well-written, and the experiments are sufficient, which include three different RL settings and four task suites with BC and RL baselines.\n- The motivation behind this paper is natural and valuable."
                },
                "weaknesses": {
                    "value": "The author claims the consistency policy is much more efficient than diffusion policies while keeping comparable results. However,\n- the results in Tab. 1 and 2 can not support the conclusion in a way, which shows there are some significant drops in some tasks (such as halfcheetah-me, kitchen-xxx) between the Diffusion-BC and Consistency-BC or Diffusion-QL and Consistency-AC. \n- Moreover, in Tab. 3, when N = 5, the performance between Diffusion-QL and Consistency-AC is comparable while the time cost is also similar. This indicates that when the denoising step is small, the absolute scores of both methods are good enough. In this case, the consistency model has few advantages, which makes the improvement much more limited."
                },
                "questions": {
                    "value": "(1) The authors claim that \"By behavior cloning alone (without any RL component), using an expressive policy representation with multi-modality like the consistency or diffusion model achieves performances comparable to many existing popular offline RL methods.\", I'm wondering where is the multi-modality. Is the consistency policy trained for all tasks across all suites? Or does the offline dataset have different successful behavior policies? The author should make it more clear. \n\n(2) In the specific tasks in RL, the scenarios are not abundant and the trajectories are quite similar. If more expressive policy representation can result in better performances, what if using some large pre-trained representation models, can this problem be solved? (E.g. R3M)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6598/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6598/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6598/Reviewer_gzrQ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6598/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698742960314,
            "cdate": 1698742960314,
            "tmdate": 1699636751582,
            "mdate": 1699636751582,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iNFIvSeRp0",
                "forum": "v8jdwkUNXb",
                "replyto": "LNY2K5oLYw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6598/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6598/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to review"
                    },
                    "comment": {
                        "value": "We appreciate the insightful comments and questions from the reviewer. \n\nLess comparable performance for consistency models on *halfcheetah-me*, *kitchen-xxx* in Tab. 1 and 2. We admit that there are some tasks showing consistency policies are less performant than diffusion policies, and the computational efficiency is not obtained for free compared with diffusion models at present, as it is an acceleration method without full capability of replacing the diffusion models with a much smaller $N$ yet. However, there are still quite a few tasks showing at least equivalent performances of consistency policies over diffusion policies, like *hopper-medium*, *walker2d-medium*, *hopper-medium-replay*, *walker2d-medium-replay*, *Antmaze* tasks on BC setting, and *halfcheetah-medium*, *halfcheetah-medium-replay*, *walker2d-medium-expert*, *antmaze-umaze-diverse* for actor-critic RL setting.\n\nFor the result in Tab. 3, our major claim is that Consistency-AC can achieve a similar performance as Diffusion-QL with a much smaller $N$. Specifically, if we look at $N=2$ for Consistency-AC while $N=5$ for Diffusion-QL, the scores are similar but Consistency-AC saves half inference time (1.84/3.76) and training time (31.94/57.06). This holds for most of the tasks in our experiments, and by default, Consistency-AC and Consistency-BC use $N=2$ while Diffusion-BC and Diffusion-QL use $N=5$ for results reported in paper.\n\nFor the first question regarding the multi-modality of the data, the model is not trained across tasks but within each task. Even so, the offline dataset for each task is usually multi-modal since it can be collected by a mixing of behavior policies. For example, the 'medium-replay' dataset in Gym tasks is the replay buffer of a RL policy to reach a medium level performance, which contains samples collected by an evolving policy. In our paper Fig. 5 in Appendix A shows the diversity of the data distribution, although it is not the action distribution given a certain state, the shapes also indicate the complexity of the sample distributions. More details about these tasks are provided in the original D4RL paper. We thank the reviewer for pointing out this problem and will add explanations in the modified version.\n\nFor the second question regarding whether large pre-trained representation model will be an alternative of expressive policy,  we believe pre-trained visual representation can definitely lead to improved performance for certain domains of RL, e.g., robotic manipulation, but the improvement can source from different aspects compared with the expressiveness of the policies. Pre-trained visual representation is more helpful for providing image information abstraction with semantic understanding. R3M is encoding images as state representation, while consistency/diffusion model represents the conditional distribution of actions given states, so the representation spaces are not the same. If R3M is modified to encode both images and robot actions, while maintaining a multi-modal model, it might be close to the expressive policies. But for current R3M in imitation learning, if the behavior dataset is multi-modal, and the downstream policy of R3M is unimodal (like Gaussian), we may not expect it to capture the multi-modality."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6598/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700168458986,
                "cdate": 1700168458986,
                "tmdate": 1700170177259,
                "mdate": 1700170177259,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IQpXvZI8Tp",
            "forum": "v8jdwkUNXb",
            "replyto": "v8jdwkUNXb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6598/Reviewer_259J"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6598/Reviewer_259J"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to use the recent \"consistency models\" as a way of parameterizing the policy for deep RL.  This is explored in offline RL, offline-to-online RL, and online RL with an actor-critic setup.  The performance is competitive with diffusion-BC despite requiring far fewer sampling steps.  This seems like a strong empirical advance to me, because the speed of sampling is essential for online reinforcement learning.  While the approach here is unsurprising, combining consistency models with RL, this still seems like an important contribution.  \n\nnotes: \n  -Diffusion inference is slow, so there could be value in using consistency models to define the policy, particularly for an actor-critic style algorithm.  \n  -This paper considers both online, offline-to-online, and offline setups.  \n  -The policy class is important for RL, especially that it be multi-modal.  \n  -The paper lays out policy regularization to prevent out-of-distribution actions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-I think that this project is very critical for the success of RL, as insufficiently rich policy classes are a major limitation.  Additionally, in RL it is important to draw as many samples as possible (either in a model or in the environment) so evaluating the policy quickly is critical.  So I think this work will have a lot of impact.   \n  -I think it's also particularly impressive that the paper shows success in online RL, because in online RL it is important that the policy perform well even when it's imperfect (i.e. early in the training process).  Whereas in offline-RL, we could imagine that the policy only needs to perform well near the end of the training process.  We indeed see that the consistency model outperforms the diffusion model in the purely online setting (Figure 4)."
                },
                "weaknesses": {
                    "value": "-The idea of using consistency models as RL policies is fairly intuitive and not terribly surprising.  \n  -On the harder tasks like Kitchen and Adroit, there is a significant gap with Diffusion-BC baseline."
                },
                "questions": {
                    "value": "-Have you thought about also using the consistency model as the \"model\" in the RL sense, i.e. to learn p(s' | s,a)?  If so, do you see any interesting challenges or opportunities there?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6598/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698806700116,
            "cdate": 1698806700116,
            "tmdate": 1699636750902,
            "mdate": 1699636750902,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Th6x4lv2cy",
                "forum": "v8jdwkUNXb",
                "replyto": "IQpXvZI8Tp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6598/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6598/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to review"
                    },
                    "comment": {
                        "value": "We appreciate the insightful comments and questions from the reviewer. \n\nFor the performance gap on Kitchen and Adroit tasks, we show that the computational efficiency is not obtained for free compared with diffusion models. The dataset of the two domains has poor coverage (Adroit) or long-term trajectories (Kitchen), which makes them harder to learn. How to improve the performance of consistency models on these tasks requires further investigation.\n\nWe appreciate the idea of using a consistency model as a transition model instead of policies and it is definitely worth trying. It could be more interesting to see the consistency model being used for sequence prediction like Decision Diffuser, which requires more expressiveness and multi-modality. In that case the improvement of computational efficiency can be even more significant. We generally expect the significance of leveraging the consistency model for RL to be larger in more complicated models or tasks, and current experiments are showcased on standard D4RL baseline."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6598/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700168261679,
                "cdate": 1700168261679,
                "tmdate": 1700168261679,
                "mdate": 1700168261679,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]