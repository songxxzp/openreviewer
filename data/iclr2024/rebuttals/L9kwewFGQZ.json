[
    {
        "title": "Mitigating Interference in the Knowledge Continuum through Attention-Guided Incremental Learning"
    },
    {
        "review": {
            "id": "zEBVau8OqW",
            "forum": "L9kwewFGQZ",
            "replyto": "L9kwewFGQZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7288/Reviewer_zq89"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7288/Reviewer_zq89"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on mitigating task interference in continual learning by introducing a compact task-attention module. It incorporates a set of lightweight, learnable task projection vectors, equal in number to the tasks, which transform the latent representations of a shared task-attention module into task-specific distributions. Additionally, this approach aims to enhance the model's performance in continual learning by jointly addressing the challenges of within-task and task-id prediction."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The approach presented in this paper differs significantly from previous methods by combining a task-attention mechanism with minimal memory overhead. It explores the feasibility of reducing interference between tasks and surpasses rehearsal-based approaches in several continual learning scenarios."
                },
                "weaknesses": {
                    "value": "A single lightweight task-specific vector may not be sufficient to adequately represent and distinguish the crucial information among multiple tasks. This approach may not effectively address the issue of catastrophic forgetting."
                },
                "questions": {
                    "value": "1)\tThis method is less innovative and mainly focuses on solving the task interference problem. How to weigh the importance of solving the interference problem or solving the forgetting problem in continual learning?\n2)\tThe innovation in this paper is that the task-attention module is used to solve the task-id prediction problem, and within-task prediction problem how can it be solved efficiently?\n3)\tAs the number of tasks continues to grow, is there any interference or conflict between these lightweight task-specific vectors?\n4)\tCan this method be used in other continual learning scenarios, such as Task- free scenario?\n5)\tPlease provide attention-guided visualization experiments showing what the task-specific vector makes the model pay attention to.\n6)\tIn section 3.4 only the extension of the classifiers was carried out, what exactly does the network extension refer to?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7288/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698646047360,
            "cdate": 1698646047360,
            "tmdate": 1699636871179,
            "mdate": 1699636871179,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jLmy3W4gjZ",
                "forum": "L9kwewFGQZ",
                "replyto": "zEBVau8OqW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7288/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7288/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rely to the Reviewer zq89 (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable insights on our paper. We provide the responses to the raised concerns below\n\n> A single lightweight task-specific vector may not be sufficient to adequately represent and distinguish the crucial information among multiple tasks. \n\nWe appreciate the reviewer's perspective, but respectfully disagree regarding the sufficiency of a single lightweight task-specific vector. For every continual learning approach, mitigating catastrophic forgetting is a balancing act between several factors like model capacity, parameter growth, buffer size and so on. While we acknowledge that a solitary lightweight task-specific vector may not completely eliminate catastrophic forgetting, our innovative attention mechanism has demonstrated significant effectiveness, leading to performance improvements across various continual learning scenarios.\n\nAGILE proposes a novel mechanism to integrate task attention and task specificity with minimal computation and memory overhead so that the model is scalable to longer task sequences. We believe AGILE proves to be a valuable step in the quest of zero forgetting in Class-IL by tackling the problem of task interference.\n\n>This method is less innovative and mainly focuses on solving the task interference problem. How to weigh the importance of solving the interference problem or solving the forgetting problem in continual learning?\n\nWe respectfully disagree with the reviewer's assertion that addressing the task interference problem does not contribute to resolving the forgetting issue in continual learning. In the context of continual learning, task interference and forgetting are not mutually exclusive challenges. Inter-task separation is a significant hurdle in Class-IL learning, particularly in realistic scenarios where task-id is unavailable during inference, leading to catastrophic forgetting [1].\n\nThe sequential learning of tasks with limited buffer data tends to blur the boundaries between classes distributed across different tasks. Therefore, minimizing task interference becomes crucial for the model to accurately predict the correct class within the current task, free from the influence of classes from other tasks. This approach aids in mitigating the impact of forgetting in continual learning scenarios.\n\n> The innovation in this paper is that the task-attention module is used to solve the task-id prediction problem, and within-task prediction problem how can it be solved efficiently?\n\nAGILE takes a distinctive approach by not explicitly focusing on task-id prediction. Instead, the task projection vectors play a crucial role in transforming feature vectors with task-specific information. This transformation empowers task-specific classifiers to enhance within-task class predictions. Unlike traditional methods that rely on explicit task-id prediction, AGILE leverages these task projection vectors to navigate and capture essential task-specific nuances, contributing to improved model performance within each task.\n\nFurthermore, our novel attention mechanism is a key component designed to mitigate inter-task interference. By incorporating attention mechanisms, AGILE minimizes the impact of information from one task bleeding into another, thus reducing interference between tasks. What sets AGILE apart is its ability to achieve this with minimal memory and computational overhead, making it scalable and efficient for handling longer sequences of tasks in continual learning scenarios. This dual strategy of leveraging task projection vectors and employing a novel attention mechanism positions AGILE as a robust solution for addressing both within-task class prediction and inter-task interference in continual learning.\n\nIn section 4.1, we discuss how AGILE facilitates learning a good task-prediction and within-task prediction. We also show how task projection vectors are distributed along the principal components using PCA in Figure 3 (middle). AGILE entails a shared task-attention module and as many lightweight, learnable task projection vectors as the number of tasks. As each task projection vector learns the task-specific transformation, they project samples belonging to the corresponding task differently, resulting in less interference and improved WP and TP in CL.\n\n\n\n[1] Timothee Lesort, Andrei Stoian, and David Filliat. Regularization shortcomings for continual learn- \u00b4 ing. arXiv preprint arXiv:1912.03049, 2019."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700550730831,
                "cdate": 1700550730831,
                "tmdate": 1700550730831,
                "mdate": 1700550730831,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Bdg5u3znlF",
            "forum": "L9kwewFGQZ",
            "replyto": "L9kwewFGQZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7288/Reviewer_2y3j"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7288/Reviewer_2y3j"
            ],
            "content": {
                "summary": {
                    "value": "Inspired by the notion that most methods that work in a task-incremental scenario can achieve almost zero forgetting, the authors introduce AGILE (Attention-Guided Incremental Learning). The main idea is to break down a class incremental problem into two sub-problems: Task-ID prediction (TP) and within-task prediction (WP). Once the first one is solved, the problem can be treated as a Task-Incremental, as the predicted task-id is already available. The authors suggest using task-specific projections to condition the feature vector. This conditioned vector passes through a task-specific module: task prediction and feature importance. During inference, the output of each module is concatenated to obtain the prediction. The authors demonstrate good performance in both task and class incremental scenarios."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors work under the assumption that the incremental Class problem can be transformed into a task-incremental problem.\n    - However, I can't entirely agree that this is a \"necessary and sufficient\" solution. In fact, there is a probability that working the problem in this way helps the model lose generalization in the representations it generates, and the only reason why this does not happen in the proposed solution is that they use a buffer to store previous tasks.\n    - Even so it is a problem that is not widely attacked, but that can be a good option in many cases, especially if it's motivated by the idea of GWT.\n- The approach comprises many different components that have a good synergy between them. It is beneficial that the authors add Table 2 to show the importance of each loss."
                },
                "weaknesses": {
                    "value": "- Using EMA is a critical point in the proposal, and the authors do not mention it too much. EMA can also be used to reduce weight modification, meaning that it can mitigate forgetting with a favorable beta. The authors present it to increase generalization.\n    - Experiments showing evidence that it increases generalization could help mitigate the doubts.\n    - Did you have an analysis of the beta value? \n- It is challenging to understand where there are linear layers and where there is soft attention in the proposed methods. The image does not help.\n    - It could be helpful to decrease the amount of terms, names or losses used in the explanation.\n    - For example, from the Figure, one can assume that there is one Task-Attention Module for each task. However, the Task-Attention Module is shared, no?\n- Didn\u2019t find Definition 1 and 2."
                },
                "questions": {
                    "value": "- Is EMA used in every method for Table 1? Or just AGILE?\n- How much overhead in terms of time is added when adding a Task-Attention Module?\n    - Even if the Task-Attention module is shared, it must still be used independently for each task.\n- Are you familiar with the work called Bias Correction (BiC) in Continual Learning? \n    - There are some similarities that you can find interesting.\n    - I don\u2019t remember if it works in class or task-incremental, but there have been extensions that work in class-incremental settings.\n- Do you know how your proposal scales with the memory size? I have seen methods that scale well (such as DER), but others could be better (like iCarl).\n- Have you tried this approach with a fixed pre-trained model?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7288/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698692027617,
            "cdate": 1698692027617,
            "tmdate": 1699636871024,
            "mdate": 1699636871024,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "T4IgUUxoWf",
                "forum": "L9kwewFGQZ",
                "replyto": "Bdg5u3znlF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7288/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7288/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 2y3j (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their insightful feedback. We provide our responses to the raised concerns below.\n> Using EMA is a critical point in the proposal, and the authors present it to increase generalization.\nExperiments showing evidence that it increases generalization could help mitigate the doubts.\n\nFollowing CLS-ER[1], we employ EMA as it functions as a self-ensemble of the intermediate model states that leads to better internal representations. Figure 3 in CLS-ER [1] shows that learning with EMA results in convergence to flatter minima, indicating better generalization [2]. \nWhile EMA helps in better generalization, the inter task inference problem still prevails and hinders the performance of the model. On the other hand, AGILE effectively reduces inter task interference with our novel task attention mechanism. As can be seen in Table 1, we outperform CLS-ER by significant margins across all datasets in Class-IL and Task-IL settings.\n\n>Did you have an analysis of the beta value?\n\nWe have updated the manuscript with the Hyperparameter Tuning results in the Appendix (section D.5.1). The performance of our model reduces as the $\\beta$ value is increased. Higher the $\\beta$, higher the restriction on the model to preserve old knowledge, thus limiting its ability to learn new information.\n>It is challenging to understand where there are linear layers and where there is soft attention\n\nSection 3.3 describes the different components in the task attention module in detail. As mentioned there, both feature encoder and feature selector in task attention module is composed of a linear layer followed by Sigmoid activation. \n> It could be helpful to decrease the amount of terms, names or losses used in the explanation.\n\nWe regret any confusion caused due to the explanations provided. We have taken utmost care to include all the components and losses in the model so that we are fully transparent. Figure 1, reflects notations used in Algorithm 1 and Proposed Method Section for proper correspondence. Furthermore, we provide the code to the reviewers to ensure complete transparency.\n\n>However, the Task-Attention Module is shared, no?\n\nAGILE uses a single task attention module that is shared across multiple tasks, however it requires as many forward passes through the module as the number of tasks (mentioned in Section 3.4). In Figure 1, we show separate task projection vector and task classifiers for each task whereas the same attention module is shared for each forward pass. \n> Didn\u2019t find Definition 1 and 2.\n\nOwing to the space limitations, we have provided Definitions 1 and 2 in the Theoretical Insights section in the Appendix.\n> is EMA used in every method for Table 1? Or just AGILE?\n\nAmong the baseline approaches, CLS-ER is the only approach that uses EMA in its formulation.\n> How much overhead in terms of time is added when adding a Task-Attention Module?\n\nBelow we provide the relative time taken to train on Seq-CIFAR100 for 5 tasks with a buffer size of 200. As can be seen, even with as many forward passes through the attention module as the number of tasks, the additional computational overhead incurred in AGILE is very minimal.\n\nMethod|ER|DER++|AGILE\n-|-|-|-\nTime|1x|1.5x|1.7x\n>Are you familiar with the work called Bias Correction (BiC) in Continual Learning?\n\nBiC attempts to correct the recency bias in continual learning by adapting the output logits for recent classes with two bias parameters. However, BiC does not entail any task specific learning to reduce task interference in continual learning. Task interference is a significant problem in Class IL that lead to catastrophic forgetting. AGILE, on the other hand, tackles the problem of task interference through the novel task attention mechanism.\n\n\n[1] Elahe Arani, Fahad Sarfraz, and Bahram Zonooz. Learning fast, learning slow: A general continual learning method based on complementary learning system. In International Conference on Learning Representations, 2022.\n\n[2] Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient descent into wide valleys"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700589444229,
                "cdate": 1700589444229,
                "tmdate": 1700589444229,
                "mdate": 1700589444229,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OrBVQwArJi",
                "forum": "L9kwewFGQZ",
                "replyto": "Bdg5u3znlF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7288/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7288/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Replying to Reviewer 2y3j (2/2)"
                    },
                    "comment": {
                        "value": "> Do you know how your proposal scales with the memory size? I have seen methods that scale well (such as DER), but others could be better (like iCarl), the exemplar size is also scaled accordingly\n\nIncreasing the memory size largely reduces the catastrophic forgetting problem as you have more exemplars from old classes to closely represent old data distribution. This enables the model to learn the decision boundaries better. We expect the margins between rehearsal based approaches to be minimal with large memory size.\nWe studied AGILE in a much harder scenario of relatively low memory size regimes where the catastrophic forgetting is exacerbated by task interference. For example, in Seq-TinyImageNet with a buffer size of 200, there are hardly 2 examples per class in the buffer for the model to preserve learnt information. Our novel task attention incorporates task specific learning into the model with task projection vectors and outperforms all other baselines by significant margin.\n> Have you tried this approach with a fixed pre-trained model?\n\nWe have not studied this approach with a fixed pre-trained model. We followed the same settings in which the baselines have been studied by training the model from scratch for different datasets.\n\nWe thank the reviewer once again for their valuable insights. We hope that we have addressed all the raised concerns sufficiently. We are happy to address any remaining concerns the reviewer may have. In the absence of further questions, we respectfully request the reviewer to reconsider the assigned score, considering the improved confidence in our paper."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700589674353,
                "cdate": 1700589674353,
                "tmdate": 1700589941655,
                "mdate": 1700589941655,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HgikmoaCSr",
            "forum": "L9kwewFGQZ",
            "replyto": "L9kwewFGQZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7288/Reviewer_5d1d"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7288/Reviewer_5d1d"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a replay-based CL method utilizing a lightweight task attention module. The module receives features from the feature extractor and performs task-id prediction using the projection vectors for each task. This approach aligns with the findings of a prior theoretical study. The authors conduct comprehensive experiments to demonstrate the benefits of their approach compared to existing baselines and show the effectiveness of the proposed techniques."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed approach is grounded in a theoretical study.\n2. The proposed method outperforms the baselines."
                },
                "weaknesses": {
                    "value": "1. I feel like the paper is written in a rush. The experiment setup is not mentioned in the main paper. It's not clear how many tasks are used in the sequential data (e.g., Seq-CIFAR100), and what architecture is used. I couldn't find where I can find the information in the main text.\n2. It's not clear why the shared task-attention module improves WP and TP when this module itself also suffers from forgetting.\n3. I couldn't fully understand why this method is better than the existing task-id prediction methods. [1] also builds a task-id prediction module on top of the feature extractor. A more comprehensive and detailed discussion should be included.\n\nOverall, I think this approach is promising, but needs some improvements.\n\n[1] Conditional channel gated networks for task-aware continual learning"
                },
                "questions": {
                    "value": "1. How does the model make the final class prediction? Does it first predict the task-id using the attention module and make a within-task prediction?\n2. What's the purpose of using the task projection vectors and why is it used to compute both z_s and z_tp?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7288/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809383244,
            "cdate": 1698809383244,
            "tmdate": 1699636870904,
            "mdate": 1699636870904,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5ElfcXlbJ1",
                "forum": "L9kwewFGQZ",
                "replyto": "HgikmoaCSr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7288/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7288/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 5d1d (1/2)"
                    },
                    "comment": {
                        "value": "We express our gratitude to the reviewer for dedicating time to evaluate our paper. Our responses to the raised concerns are presented below.\n\n> I feel like the paper is written in a rush. The experiment setup is not mentioned in the main paper. It's not clear how many tasks are used in the sequential data (e.g., Seq-CIFAR100), and what architecture is used. I couldn't find where I can find the information in the main text.\n\nOwing to space limitations in the paper, we relocated details regarding the experimental setup and datasets to Appendix D. The decision was based on the assumption that, given the adherence to standard model architecture setups similar to other baselines, these details could be appropriately placed in the appendix. However, in the final revision, we will include a reference to these specifics in the Experimental Results section (Section 4) for better accessibility.\n\n> It's not clear why the shared task-attention module improves WP and TP when this module itself also suffers from forgetting.\n\nAs outlined in the paper, the performance of Class Incremental Learning (Class IL) can be understood as a synergy between TP (task prediction) and WP (within-class prediction). Given the sequential nature of learning in Continual Learning (CL), the model tends to excel in distinguishing between classes within the same task compared to those across different tasks. In AGILE, we emulate this characteristic by employing task projection vectors that transform backbone features into task-specific latent spaces, implicitly encouraging TP. Subsequently, these transformed features undergo processing by task-specific classifiers, addressing WP.\n\nWhile it is acknowledged that the task-attention module in AGILE is susceptible to forgetting to some extent, our strategy involves capturing task-specific information within the task projection vectors, while task-agnostic information is preserved in the shared task attention module. As shared, generic information is less prone to forgetting, AGILE exhibits improved performance compared to other approaches.\n\n> I couldn't fully understand why this method is better than the existing task-id prediction methods. [1] also builds a task-id prediction module on top of the feature extractor. A more comprehensive and detailed discussion should be included.\n\nWe thank the reviewer for relevant citation and appreciate the opportunity to provide a more comprehensive and detailed discussion on task id prediction methods.\n\nIn AGILE, our approach diverges by incorporating a task attention mechanism, which utilizes task projection vectors to transform backbone features into task-specific latent spaces. This transformation is designed to implicitly encourage task prediction (TP). Subsequently, these transformed features undergo processing by task-specific classifiers, addressing within-class prediction (WP). This two-step process distinguishes AGILE from methods like [1], which primarily focus on task-id prediction on top of the feature extractor.\n\nSecondly, the relevant citation [1]  prediction relies on a task classifier, which is trained incrementally in a single-head fashion. Notably, the objective in Eq. 3 in [1] shifts the single-head complexities from a class prediction to a task prediction level. Although the end goal focusses on how the shared information can be filtered out for classification for both AGILE and task-id prediction methods, AGILE approaches the problem differently through theoretically grounded hypothesis. \n\nWe agree with the reviewer that a comparison with  task-id prediction modules would add more clarity to the readers. We will update our final revision with extensive related works section focussing on task-id prediction methods."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549813387,
                "cdate": 1700549813387,
                "tmdate": 1700549813387,
                "mdate": 1700549813387,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Hg0CPXykTp",
            "forum": "L9kwewFGQZ",
            "replyto": "L9kwewFGQZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7288/Reviewer_pnnH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7288/Reviewer_pnnH"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel rehearsal based continual learning approach which use a shared task-attention module to mitigate the task interference. The shared task-attention module compresses the task specific information to some trainable parameters."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The framework achieves fairly good results compared with baselines.\n2. The paper is written clearly and easy to follow."
                },
                "weaknesses": {
                    "value": "1. Novelty concern. I would like to point out that the idea of leveraging trainable parameters to store task information has been investigated in previous works [*] [**]. L2P has shown its effectiveness in continual learning areas in recent years. \n\n2. Lack of a comprehensive comparison. There are many works using prompting (learnable parameters) in continual learning and achieving SOTA performance. I suggest the author conduct a comprehensive comparison with these works.\n\n[*] Learning to prompt for continual learning, CVPR 2022.\n\n[**] DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning, ECCV 2022."
                },
                "questions": {
                    "value": "Could the author conduct a comprehensive comparison with CL works using prompting (learnable parameters)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7288/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698815603338,
            "cdate": 1698815603338,
            "tmdate": 1699636870572,
            "mdate": 1699636870572,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0QrhyBYMZ2",
                "forum": "L9kwewFGQZ",
                "replyto": "Hg0CPXykTp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7288/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7288/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer pnnH"
                    },
                    "comment": {
                        "value": "We thank the reviewer for taking the time to review our paper. We provide our responses to the raised concerns below.\n\n> Novelty concern. I would like to point out that the idea of leveraging trainable parameters to store task information has been investigated in previous works [*] [**]. L2P has shown its effectiveness in continual learning areas in recent years.\n\nWhile the idea of leveraging trainable parameters has been investigated, we believe the novelty comes from how the idea is implemented. Transformer architectures allow us to somewhat intuitively extend the input tokens with special tokens (say class token) that capture specific information. Learning to prompt and DualPrompt are both transformer based architectures that use special prompt tokens with pre-trained models to then learn task specific information.\n\nHowever such an extension is not straightforward for CNN architecture. CNN does not have any attention mechanisms or tokens. AGILE leverages parameter isolation to bring in task specificity with little computational or memory overhead. Our novelty comes in emulating such task attention through a shared under complete autoencoder and task projection vectors. These projection vectors then capture task specific information through Our auxiliary task classification loss and pair wise discrepancy loss. Even in CNN architectures, several approaches have tried to capture task specific information through parameter isolation. We compare against such approaches in Figure 2.\n\nIn this study, we have explored the integration of our method into the widely used CNN architecture, specifically ResNet18. We have examined various baseline methods within the CNN framework and conducted a comparative analysis, demonstrating that AGILE achieves superior performance due to its innovative task attention mechanism. While AGILE's task attention mechanism utilizes simple undercomplete autoencoders and tensors, devoid of architecture-specific layers like convolutions, we concur with the reviewer's suggestion that AGILE could potentially be generalized to transformers. However, we acknowledge that extending AGILE to different architectures, such as transformers, is a non-trivial task requiring extensive experiments and hyperparameter tuning. Such endeavors go beyond the scope of this rebuttal period. Recognizing the potential benefits of extending AGILE to transformers for enhanced generalizability and comparison with methods such as L2P, DualPrompt, we propose to address this in future work and will update our limitations section accordingly.\n\n> Lack of a comprehensive comparison. There are many works using prompting (learnable parameters) in continual learning and achieving SOTA performance. I suggest the author conduct a comprehensive comparison with these works.\n\nWe respectfully disagree with the reviewer's assertion that our comparison with other state-of-the-art (SOTA) approaches is not comprehensive. It is crucial to emphasize that our current focus is confined to CNN architecture. Given the well-established exploration of continual learning within CNN architectures, we have conducted a thorough comparison with various popular and recent rehearsal approaches, as detailed in Table 1 under consistent experimental settings. Additionally, our evaluation extends to the comparison with several methods specifically designed to capture task-specific information, as illustrated in Figure 2.\n\n> Could the author conduct a comprehensive comparison with CL works using prompting (learnable parameters)?\n\nThe current scope of our work is primarily centered on CNN architecture. While we have extensively compared our approach with various state-of-the-art rehearsal methods and those focusing on task-specific information capture within the CNN framework, the specific comparison with continual learning works using prompting and learnable parameters falls outside the scope of our study. We acknowledge the importance of this avenue for future exploration and welcome it as a potential direction for further research.\n\nWe have thoroughly addressed all concerns raised by the reviewer. If any lingering concerns remain, we kindly ask you to bring them to our attention. In the absence of further questions, we respectfully request the reviewer to reconsider the assigned score, considering the improved confidence in our paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548592177,
                "cdate": 1700548592177,
                "tmdate": 1700548592177,
                "mdate": 1700548592177,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "houAM6uWre",
            "forum": "L9kwewFGQZ",
            "replyto": "L9kwewFGQZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7288/Reviewer_n4fn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7288/Reviewer_n4fn"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a rehearsal-based method called AGILE to tackle the class-incremental learning setting in continual learning. Specifically, the paper leverages learnable task embedding vectors and shared task-attention module for better mitigating task interference. Experimental results on benchmark datasets demonstrate the effectiveness of the method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper reads well and is easy to follow.\n- Class-incremental learning is indeed a more challenging setting than task-incremental learning."
                },
                "weaknesses": {
                    "value": "- The idea of using task-attention or task embedding vector is not quite novel. For example, DyTox [1] also has a task attention module, L2P [2] leverages task-specific prompts. \n- Following the first one, I think the paper misses several recent competitive methods to compare against. For example, I understand both DyTox and L2P are based on transformers. However, if the proposed method AGILE is generalizable enough, it should be compatible with transformer architectures as well, making comparison with more advance methods like DyTox, L2P possible.\n- \n- The contents in middle and right subfigures in figure 3 seems missing?\n\n[1] Douillard, Arthur, et al. \"Dytox: Transformers for continual learning with dynamic token expansion.\" CVPR 2022\n[2] Wang, Zifeng, et al. \"Learning to prompt for continual learning.\" CVPR 2022"
                },
                "questions": {
                    "value": "- I understand the method is based on rehearsal, what if the rehearsal part is removed. Will the remaining design lead to improvement upon the baselines without rehearsal as well?\n- See weaknesses for the rest questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7288/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699088339304,
            "cdate": 1699088339304,
            "tmdate": 1699636870372,
            "mdate": 1699636870372,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RlDwoGoJCh",
                "forum": "L9kwewFGQZ",
                "replyto": "houAM6uWre",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7288/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7288/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to reviewer n4fn"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback and insights. We provide the responses for the raised concerns below. \n\n> The idea of using task-attention or task embedding vector is not quite novel\n\nTransformer architecture provides a rather straightforward way to extend self attention mechanism with learnable parameters to capture any specific information. However such attention learning is not intuitive in other architectures, like CNNs.Our novelty is in emulating the task attention mechanism with a bottleneck architecture and encouraging task specific learning with task projection vectors by transforming the encoded latent space output in CNNs. Our auxiliary task classification loss and pair wise discrepancy loss enables the projection vectors to capture task specific information. \n\nFor the scope of this work, we have studied the integration of our method into the widely popular CNN architecture (ResNet18). Several baseline methods are all studied in the CNN architecture, We compare AGILE against these methods under the same experimental setting and show that AGILE can achieve better performance through our novel task attention mechanism.\n\nSince AGILE uses simple under complete auto-encoders and tensors( no architecture specific layer like convolutions ) for our task attention mechanism, we agree with the reviewer that AGILE can be generalized to transformers. However, extending it to other architectures like transformers may not be trivial and it warrants quite some experiments and hyperparameter tuning which is beyond the scope of this rebuttal period. We acknowledge that extending AGILE to transformers could in fact strengthen the method for its generalizability. We leave that as a future work and will update our limitation section to reflect the same.\n\n> The contents in middle and right subfigures in figure 3 seems missing?\n\nWe see that the PDF is not loading fully in browser tabs. We request the reviewer to use a PDF viewer application. We will update the PDF in the meantime for proper loading in browser windows. \n\n> I understand the method is based on rehearsal, what if the rehearsal part is removed. Will the remaining design lead to improvement upon the baselines without rehearsal as well?\n\nRemoving rehearsal would drastically affect all the rehearsal based approaches including AGILE. All the methods would be reduced to a simple SGD based fine tuning, a naive baseline provided in Table 1. Even though AGILE has task specific projection vectors, the shared task attention module would undergo catastrophic forgetting without the rehearsal and the captured information in projection vectors may cease to be useful.  \n\n\nWe have diligently addressed all concerns raised by the reviewer. Should any concerns persist, we kindly request that you bring them to our attention. If no further questions arise, we kindly request the reviewer to reconsider the assigned score, taking into account the enhanced confidence in our paper."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7288/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547879106,
                "cdate": 1700547879106,
                "tmdate": 1700547879106,
                "mdate": 1700547879106,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]