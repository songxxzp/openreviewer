[
    {
        "title": "Accelerated Sampling with Stacked Restricted Boltzmann Machines"
    },
    {
        "review": {
            "id": "HoJOc7c9X2",
            "forum": "kXNJ48Hvw1",
            "replyto": "kXNJ48Hvw1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2423/Reviewer_mREg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2423/Reviewer_mREg"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript introduces a stacked RBM parallel tempering training, in which sampling is improved by swapping configurations of hidden and visible units in the vertical stack."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper introduces an interesting idea, that allows to sample more efficiently from RBMs, without the need to specify effective temperatures, as in Parallel Tempering"
                },
                "weaknesses": {
                    "value": "The numerical experiments do not allow to understand the improvement over more standard Parallel Tempering"
                },
                "questions": {
                    "value": "Can the authors show the autocorrelation times of some observable (for example, on the Ising model, the magnetization at the critical point) ? It is hard to understand, from the proposed plots, whether the scheme introduced is faster than standard PT \n\nThe simplified theoretical analysis of mixing and swapping times is very interesting, but as far as I can see there is no direct comparison to simulation data. It would be desirable to have such a comparison, in order to understand whether the qualitative aspects of this analysis carry on to the more complex algorithm"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2423/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698739440242,
            "cdate": 1698739440242,
            "tmdate": 1699636177833,
            "mdate": 1699636177833,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8O79iMqMQo",
                "forum": "kXNJ48Hvw1",
                "replyto": "HoJOc7c9X2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2423/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2423/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the comments.\n\n> The numerical experiments do not allow to understand the improvement over more standard Parallel Tempering.\n\nFig. 3 shows numerical simulations in three different datasets (MNIST, Lattice Proteins, 2D Ising model), where stacked tempering (ST) outperforms parallel tempering (PT) in terms of the rate of transitions effected between minima of the energy landscape. To compare different algorithms in a fair way, we count the number of Alternate Gibbs sampling (AGS) steps. Note that different algorithms perform different numbers of AGS per iteration. For example, PT does one AGS per temperature, and a full sweep over all temperatures involves many AGS. In Fig. 3, the x-axis reports the number of AGS steps (we have modified the caption to emphasize this point). The figure demonstrates that ST transitions more frequently between modes than vanilla AGS or PT.\n\n> Can the authors show the autocorrelation times of some observable (for example, on the Ising model, the magnetization at the critical point)? It is hard to understand, from the proposed plots, whether the scheme introduced is faster than standard PT\n\nWe have computed the autocorrelation times for the Ising model, MNIST0/1, and for the Lattice proteins, in Supplementary Table S4. Generally, the autocorrelation is between one or two orders of magnitude smaller for ST than for PT or AGS, as expected from Fig. 3.\n\n> The simplified theoretical analysis of mixing and swapping times is very interesting, but as far as I can see there is no direct comparison to simulation data. It would be desirable to have such a comparison, in order to understand whether the qualitative aspects of this analysis carry on to the more complex algorithm\n\nThe simplified analysis is meant to demonstrate the existence of a regime where stacked tempering has an exponential advantage over local sampling techniques such as Metropolis or AGS. Moreover, the analysis provides a mechanistic explanation of this advantage, related to an interplay of clustering and separation of the representations extracted by the model. Figure 5 and SI2, 3 show numerical simulations of the simplified model, in excellent agreement with the theoretical calculations.\n\nWe have added SI figures S4, S5, S6, which illustrate how representations along the stack of RBMs cluster and separate, for MNIST0/1, MNIST full, and Ising datasets. Representations at deeper levels of the stack bring closer together data points that are similar, while driving apart more dissimilar points. Our simplified theoretical analysis then allows us to hypothesize that these changes in the representation geometry facilitate sampling."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2423/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700086386987,
                "cdate": 1700086386987,
                "tmdate": 1700086407621,
                "mdate": 1700086407621,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Q9NZJ05Bwk",
                "forum": "kXNJ48Hvw1",
                "replyto": "8O79iMqMQo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2423/Reviewer_mREg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2423/Reviewer_mREg"
                ],
                "content": {
                    "title": {
                        "value": "Performance comparison to PT"
                    },
                    "comment": {
                        "value": "> We have computed the autocorrelation times for the Ising model, MNIST0/1, and for the Lattice proteins, in Supplementary Table S4. Generally, the autocorrelation is between one or two orders of magnitude smaller for ST than for PT or AGS, as expected from Fig. 3.\n\nThank you, apologies for the oversight if the Table was already there, that is what I was looking for. I think that the analysis on the autocorrelation times should take precedence over the more qualitative analysis of Fig. 3, but that is just my own taste. \n\nGenerally speaking, it is nice to see a significant reduction in the autocorrelation times when compared to PT with a reasonable number of copies. However, to be entirely rigorous here, one would need to have at least an idea of the scaling of correlation times (at the critical temperature) with system size. If the authors could add that, it would nicely complete the assessment over PT also in terms of scaling.  \n\nAlso, while I understand the authors emphasize applications based on unsupervised learning, it would be nice to maybe add a comment on the fact that their approach is quite general and can can be used to improve sampling of an RBM more broadly. I.e., those applications in which there is no external data, but weights of an RBM are given, and the task is sampling from those. These are more common for example in applications to quantum physics."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2423/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700114988122,
                "cdate": 1700114988122,
                "tmdate": 1700114988122,
                "mdate": 1700114988122,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pYLRk3m8HR",
            "forum": "kXNJ48Hvw1",
            "replyto": "kXNJ48Hvw1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2423/Reviewer_fKHh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2423/Reviewer_fKHh"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a method called Stacked Tempering (ST) that applies the ideas of deep tempering to restricted Boltzmann machines (RBM). The authors propose to learn stacks of nested RBMs, where the representations of one RBM are used as \"data\" for the next one in the stack. By exchanging configurations between RBMs, the ST method allows for fast transitions between different modes of the data distribution. The paper provides analytical calculations of mixing times and demonstrates the efficiency of the ST method compared to standard Monte Carlo methods on several datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper introduces a new approach called Stacked Tempering (ST) for sampling from Restricted Boltzmann Machines (RBMs).\n  - ST learns nested RBM stacks by using the representation of one RBM as the \"data\" for the next RBM.\n- Efficiency of the ST method is demonstrated through experiments on multiple datasets including MNIST, in-silico Lattice Proteins, and 2D-Ising model.\n- This paper provides the first theoretical analysis supporting the use of deep representations for improving mixing in RBMs, inspired by previous research on deep tempering.\n  - Obtained analytical results are interesting."
                },
                "weaknesses": {
                    "value": "- Learning nested RBMs seems costly compared to simple parallel tempering approach where additional models are not required.\n- This method is only applicable to RBMs."
                },
                "questions": {
                    "value": "- Is it possible to extend this method for sampling from other energy-based models (e.g., deep Boltzmann machines)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2423/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2423/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2423/Reviewer_fKHh"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2423/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699086484140,
            "cdate": 1699086484140,
            "tmdate": 1700791705412,
            "mdate": 1700791705412,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nJ09p8kOID",
                "forum": "kXNJ48Hvw1",
                "replyto": "pYLRk3m8HR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2423/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2423/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the comments. \n\n> Learning nested RBMs seems costly compared to simple parallel tempering approach where additional models are not required.\n\nAlthough parallel tempering (PT) requires no additional training, it requires several (usually hundreds or thousands) copies of the RBM at different temperatures that exchange configurations. The temperatures are also not easy to set a priori. Stacked tempering (ST) requires additional training of a few models (our experiments were performed with 4 RBMs at most). In addition, training the deeper models is faster because: 1) the landscapes are smoother at deeper levels of the stack, and 2) the layers of the stack shrink and therefore these models are trained on data of smaller dimensionality. Table S3 in the Appendix now reports the training times. As can be seen from table S4, the cumulative training of the deeper RBMs takes a time comparable to the training time of the bottom RBM.\n\nFinally, we note that the total sampling time of stacked tempering can be decomposed as training time, which is a constant offset, plus sampling time, which is proportional to the number of generated samples. The stack needs to be trained once only.\n\n> This method is only applicable to RBMs.\n\nAlthough our formulation and numerical examples use RBMs, the method is more general. See answer below for examples.\n\n> Is it possible to extend this method for sampling from other energy-based models (e.g., deep Boltzmann machines)?\n\nWe believe several extensions are possible, but numerical experiments are out of scope for the present work. We speculate briefly on possible extensions.\n\n* A deep Boltzmann machine (DBM) is a particular case of an RBM (since it is bipartite, with even and odd layers on each partition), and therefore the method we have described applies in principle without modifications. It would be interesting to carry out numerical experiments to assess whether stacked tempering (ST) can help training DBMs, which are known to be computationally difficult in practice.\n* We have presented a simple formulation where the stack is trained layer by layer. Training can also be carried out \u201cin parallel\u201d for all RBMs in the stack, allowing gradients to flow from bottom layers to the top using a reverse KL divergence. This would allow exchanged samples to be used during training of the bottom layers.\n* More generally, energy-based models (such as RBM) map data configurations x to an energy E(x), eventually defining a probability over the data space P(x) = exp(-E(x))/Z. As in Refs. [1,2] E(x) can be parameterized by some deep neural network (NN), and P(x) can be sampled by local Langevin dynamics which can get stuck in rugged landscapes. Focusing on the activity h of an intermediate representation layer, we can write E(x) = F(h) = F(H(x)) as a composition where h = H(x) denotes the mapping from the data to the intermediate representation layer. We can train another generative NN for the inverse map x=g(h), by training an energy-based model with the energy E(g(h)) to fit the data h = H(x), with respect to g. The configurations h from the two NNs can then be exchanged according to Metropolis rule for the energy cost E(g(h1)) + F(h2) - E(g(h2)) - F(h1). In this manner the intermediate representations h are exploited to propose global moves in the data space. The stacked tempering method we have described can be regarded as the simplest implementation of this idea, where the mappings g(h), E(x), F(h), are naturally defined for the RBMs.\n\nReferences:\n* Song & Kingma (2021) [arXiv:2101.03288](https://arxiv.org/abs/2101.03288)\n* Du, Mordatch (2019) [NeurIPS](https://papers.nips.cc/paper_files/paper/2019/hash/378a063b8fdb1db941e34f4bde584c7d-Abstract.html)"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2423/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700086222819,
                "cdate": 1700086222819,
                "tmdate": 1700086222819,
                "mdate": 1700086222819,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dpXiZ3yoGX",
            "forum": "kXNJ48Hvw1",
            "replyto": "kXNJ48Hvw1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2423/Reviewer_4ET5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2423/Reviewer_4ET5"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new sampling scheme for RBMs that utilizes a hierarchy of RBMs of decreasing latent and visible dimensions to encourage learning a smoother version of the data distribution that captures more global aspects of the distribution in a manner akin to temperature annealing in parallel tempering. Their training approach involves sampling the latent variables at a given layer using alternating (between the latent and visible variables) Gibbs sampling (AGS) and then using the latent variable samples as training data for the next layer up in the hierarchy. Sampling involves using AGS to generate latent variables in a layer and then swapping the latent variables of one layer with the visible variables of the next one up using an acceptance probability. Their experiments on MNIST, a protein folding problem and the 2-D Ising model demonstrate how their approach can sample modes much quicker than AGS and parallel tempering. They also give a theoretical analysis in the overparameterized regime showing how different settings of the hyperparameters (regularization weight and ratio of hidden-to-visible variables) induces different representational regimes of the model. They also perform an analysis of the mixing time of their scheme indicating how it matches empirical data"
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The main strengths of the paper are the results which indicate their approach's superior sampling performance compared to other reasonable baselines of parallel tempering and AGS. \n- The explanation of their approach is clear to understand and is given succinctly.\n- The theoretical analysis gives good insight and interpretation of the results of their approach as well as the consequences of different settings of the hyperparameters. In addition, the fact that the relations established for the mixing times (as in Figure 5) match the empirical results adds confidence in the correctness of their analysis"
                },
                "weaknesses": {
                    "value": "- As their approach is intended to speed up sampling of RBMs a figure/table demonstrating how the real world sampling time is approved compared to the parallel tempering and AGS baselines would give the reader a better sense of how this method fares practically.\n- Adding figures similar to Fig 3(d-h) for the Ising model results in the main text would give the reader a better understanding of the results on this problem\n- A conceptual comparison to deep Boltzmann machines would be a great addition as it would make clear how their approach differs from sampling schemes for deep Boltzmann machines."
                },
                "questions": {
                    "value": "- No questions that need clarifying"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2423/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2423/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2423/Reviewer_4ET5"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2423/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700521677722,
            "cdate": 1700521677722,
            "tmdate": 1700521677722,
            "mdate": 1700521677722,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HQRHHB1Wg9",
                "forum": "kXNJ48Hvw1",
                "replyto": "dpXiZ3yoGX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2423/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2423/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their comments.\n\n> As their approach is intended to speed up sampling of RBMs a figure/table demonstrating how the real world sampling time is approved compared to the parallel tempering and AGS baselines would give the reader a better sense of how this method fares practically.\n\nWe have added table S4 in Supplementary materials during revision, with sampling times (in minutes) in our experiments for different algorithms and datasets. Overall we find that ST is faster than AGS or PT.\n\n> Adding figures similar to Fig 3(d-h) for the Ising model results in the main text would give the reader a better understanding of the results on this problem\n\nWe have added Figure S8 in supplementary showing example trajectories of the magnetization in the Ising model during sampling with the different algorithms. ST samples more easily configurations of alternate magnetization than AGS or PT.\n\n> A conceptual comparison to deep Boltzmann machines would be a great addition as it would make clear how their approach differs from sampling schemes for deep Boltzmann machines.\n\nWe agree that applications to deep Boltzmann machines (DBM) would be interesting and we would like to explore this in future work. A DBM can be regarded as a particular case of an RBM, in  which \u201cvisible\u201d and \u201chidden\u201d layers are made of the even and odd layers of the DBM. Therefore, stacked tempering (ST) should apply without modification. Whether ST can help training DBMs is an interesting open question, which we now briefly mention in Discussion."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2423/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700600480415,
                "cdate": 1700600480415,
                "tmdate": 1700600480415,
                "mdate": 1700600480415,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "C3pqmHeh7U",
                "forum": "kXNJ48Hvw1",
                "replyto": "dpXiZ3yoGX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2423/Reviewer_4ET5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2423/Reviewer_4ET5"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for addressing my questions/weaknesses. I'll keep my score the same"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2423/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631402430,
                "cdate": 1700631402430,
                "tmdate": 1700631402430,
                "mdate": 1700631402430,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]