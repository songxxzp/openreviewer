[
    {
        "title": "P$^2$OT: Progressive Partial Optimal Transport for Deep Imbalanced Clustering"
    },
    {
        "review": {
            "id": "43y1DekyeR",
            "forum": "hD3sGVqPsr",
            "replyto": "hD3sGVqPsr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3053/Reviewer_qtAk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3053/Reviewer_qtAk"
            ],
            "content": {
                "summary": {
                    "value": "This paper studied a more general clustering problem, deep imbalanced clustering. From the perspective of pseudo label generation, the authors propose a progressive partial optimal transport method to combat the imbalanced challenges in data. The technical way to incorporate the imbalanced distribution into the optimal-transport framework and align with the classical solver is appealing, and a range of experiments demonstrate the performance of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) The technical point is good, which leverages a virtual cluster to incorporate the spirit of sample selection in optimal transport is novel. The authors applied a mass increasing process in the constraint of unbalanced OT to progressively leverage more confident samples for representation learning and avoids the degeneration due to the skew distribution.\n\n2) The reformulation with entropy regularization and the weighted the KL divergence makes the classical Sinkhorn-knop algorithm appliable and stably optimize the target towards the progressive imbalance constraint. The authors carefully deal with the reformulation to make the algorithm can sufficiently incorporate the desire for sample selection.\n\n3) The authors conduct a range of experiments on the representative datasets like small dataset CIFAR100-LT, the mid-scale ImageNet-R, and large-scale iNaturalist. The experimental results and the visualization consistently support the author's claim, and the ablation study provided the insight on how the components work and how useful they are."
                },
                "weaknesses": {
                    "value": "Although the proposed method is overall novel, there are still some concerns that should be considered.\n1) The technical choice about Eq.(13) is unique. The other choice like introducing a hard equality constraint about the mass of the virtual cluster and applying the lagrange multiplier can be also possible. The main concern here is that the authors introduce two hyperparameters for weighted KL: one is for the target clusters, i.e., \\lambda, and the other is the large value for the virtual cluster.  How is the performance of directly constraining the equality about 1-\\rou with lagrange multiplier compared with Eq.(13).\n\n2) It is not clear why the authors have not compared with the clustering with self-labeling by Asano from the perspective of OT. For SPICE, the performance reported in the appendix is also not convincing. How is the performance comparison of SPICE and P^2OT on the balanced datasets. It will be provide more comprehensive comparison here on both imbalanced datasets and balanced datasets to show their pros and cons.\n\n3) In the perspective of representation learning, some related works should be included with the proper discussion, e.g., about the self-supervised long-tailed learning, like SDCLR [1], BCL [2] and re-weighted regularization, as they also target to representation learning of imbalanced data without label information. Some proper comparison will be better.\n\n[1] Jiang, Z., Chen, T., Mortazavi, B.J. and Wang, Z., 2021, July. Self-damaging contrastive learning. In International Conference on Machine Learning (pp. 4927-4939). PMLR.\n[2] Zhou, Z., Yao, J., Wang, Y.F., Han, B. and Zhang, Y., 2022, June. Contrastive learning with boosted memorization. In International Conference on Machine Learning (pp. 27367-27377). PMLR.\n[3] Liu, H., HaoChen, J.Z., Gaidon, A. and Ma, T., 2021. Self-supervised learning is more robust to dataset imbalance. In International Conference on Learning Representation, 2022."
                },
                "questions": {
                    "value": "1) The performance of P^2OT and SPICE on the balanced datasets.\n\n2) How is the performance comparison between the proposed deep imbalanced clustering and some self-supervised long-tailed learning methods in terms of the representation quality?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3053/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698731584948,
            "cdate": 1698731584948,
            "tmdate": 1699636250628,
            "mdate": 1699636250628,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lhKN8xBO9C",
                "forum": "hD3sGVqPsr",
                "replyto": "43y1DekyeR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3053/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3053/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer qtAk"
                    },
                    "comment": {
                        "value": "We appreciate the diligent assessment by reviewer qtAk, as well as the recognition of our technical contributions and our proposed methods. We hope the concerns raised could be addressed by the following response.\n\n## The technical choice about Eq.(13) is unique.\n\nIt is possible to apply the Lagrange multiplier to impose a hard equality constraint. But resulting OT problem can not be easily solved by fast scaling algorithm, making it less practical for large-scale application problems. By contrast, we introduce a virtual cluster and weighted KL constraint to transform Eq.5-6 into a form similar to Eq.1, which can be solved by the fast scaling algorithm. \n\nRegarding the additional hyperparameter, we assign $\\lambda_{:K}$  as 1 for all datasets without any tuning. And for $\\lambda_{K+1}$, we believe that setting it to a sufficiently large value is adequate. In practice, directly assigning the $\\frac{\\lambda_{K+1}}{\\lambda_{K+1}+\\epsilon}$ as 1 in Proposition 2 without any further adjustment proves effective. \n\n## Comparing with OT\n| Formulation | CIFAR100 | ImgNet-R | iNature500 |\n|:-----------:|:--------:|:--------:|:----------:|\n|             |    ACC   |    ACC   |     ACC    |\n|      OT     |   28.2   |   18.2   |    24.1    |\n|   P$^2$OT   |   38.2   |   25.9   |    32.2    |\n\nWe thank the reviewer qrAk for pointing out this problem. We have added the results of OT[1] to Table 2, and our method shows considerable improvement over naive OT [1].\n\nAs for the comparison of SPICE on balance datasets, please refer to General Response.\n\n[1] Yuki M. Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous clustering and representation learning. In International Conference on Learning Representations (ICLR), 2020.\n\n## Comparison with representation learning.\nPlease refer to General Response."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700419661215,
                "cdate": 1700419661215,
                "tmdate": 1700419661215,
                "mdate": 1700419661215,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OfoezbG6vW",
                "forum": "hD3sGVqPsr",
                "replyto": "lhKN8xBO9C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3053/Reviewer_qtAk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3053/Reviewer_qtAk"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. After reading the rebuttal, my major concerns have been addressed and will maintain the score. I hope the authors can take the advices to substantially refine the submission. \n\nBest,\n\nThe reviewer qtAK."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710907781,
                "cdate": 1700710907781,
                "tmdate": 1700710907781,
                "mdate": 1700710907781,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "b85IY5BvNi",
            "forum": "hD3sGVqPsr",
            "replyto": "hD3sGVqPsr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3053/Reviewer_xbkw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3053/Reviewer_xbkw"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the challenge of deep clustering under imbalanced data. The paper proposes a novel pseudo-labeling-based learning framework. This framework formulates pseudo-label generation as a progressive partial optimal transport problem, allowing it to generate imbalance-aware pseudo-labels and learn from high-confidence samples. The approach transforms the problem into an unbalanced optimal transport problem with augmented constraints, making it efficiently solvable. Experimental results on various datasets, including long-tailed CIFAR100, ImageNet-R, and iNaturalist2018 subsets, demonstrate the effectiveness of the method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper studies an interesting problem, deep clustering under imbalanced data. It proposed a progressive partial optimal transport algorithm to address this problem, and extensive experiments have been conducted to evaluate its effectiveness."
                },
                "weaknesses": {
                    "value": "1. Great computational cost. In Figure 5, it takes 1 second to estimate the pseudo-labels when K=40. It is impossible in practice with a larger number of clusters, eg, imagenet, and mini-batch training.\n2. Missing comparisons on balanced datasets. The true data distribution is unknown in real-world applications. This paper only investigates the settings of imbalanced data or tests on balanced data. It is not well aligned with most literature of deep clustering.\n3. This paper does not learn representations, which may be confusing and needs to be clarified. In addition, this paper uses a large pre-trained model and lacks a simple baseline with representation learning models. For example, BYOL or DNIO pre-trained on ImageNet can be used to extract the representations for subsequent K-means clustering. In such settings, we do not need to determine whether the data is imbalanced, as each sample belongs to a single class or a large number of clusters can be pre-defined.\n4. A good evaluation metric should be important for imbalanced deep clustering. Under the imbalanced setting, no samples may be assigned to tail classes during Hungarian matching. As we can see in Figure S7, the predictions of the proposed method are more uniform than baselines. There are more samples assigned to tail classes, though this is not true in training data. It confirms that a uniform clustering result is beneficial for evaluation. I suggest that kNN evaluation in representation learning can be adopted for imbalanced clustering, without the need of Hungarian matching. Due to the uniform constraints, more discussion should be paid to Huang et al., 2022.\n\nAlthough I have raised many questions, I think this paper is interesting, and I will keep/increase my score if the authors have addressed my concerns."
                },
                "questions": {
                    "value": "1. Change 'confidence sample selection' to 'confident sample selection'\n2. Which dataset is used for DINO pretraining? Is the backbone fixed during training?\n3. It is unclear about the use of historical predictions.\n4. What is visualized in Figure 2? We usually visualize the features before the classifier instead of class predictions. If the results are consistent for the features, we can conclude: more distinct clusters."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3053/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698739257657,
            "cdate": 1698739257657,
            "tmdate": 1699636250541,
            "mdate": 1699636250541,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "M6eLhv7d3A",
                "forum": "hD3sGVqPsr",
                "replyto": "b85IY5BvNi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3053/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3053/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xbkw"
                    },
                    "comment": {
                        "value": "We appreciate the thorough evaluation by reviewer xbkw and express our sincere gratitude for xbkw's interest in the problem and the method we proposed. We hope the following response could address the concerns raised.\n\n## Great computation cost\n\nThe unit of the horizontal axis is K (kilo), and the analysis is conducted on iNature1000. Thus, when there are 40000 points and 1000 clusters, it takes only 1 second, which is practical for mini-batch training in most cases.\n\n## Comparisons on Balanced Dataset\n\nPlease refer to the general response.\n\n## This paper does not learn representations.\nWe learn the representation. Specifically, we employ an ImageNet-1K unsupervised pre-trained ViT-B16 model and fine-tune the last block, which strikes a balance between complexity and flexibility. We have updated Appendix F to make it clear. \n\nWe thank the reviewer's suggestion. We note that K-means clustering is also affected by data imbalance, leading to inferior results and we provide detailed experimental analysis in the second General Response.\n\n## Evaluation Metric\n\nWe appreciate the importance of a good evaluation and recognize that it is an open question in imbalanced deep clustering. However, we disagree that a uniform clustering result is beneficial for evaluation. As evident from the results below, the naive OT, which enforces equality uniform distribution constraints, yields inferior results.\n\n| Formulation | CIFAR100 | ImgNet-R | iNature500 |\n|:-----------:|:--------:|:--------:|:----------:|\n|             |    ACC   |    ACC   |     ACC    |\n|      OT     |   28.2   |   18.2   |    24.1    |\n|   P$^2$OT   |   38.2   |   25.9   |    32.2    |\n\n\nIn addition, regarding Hungarian matching, it is true that on imbalanced training sets, tail classes may be less important during the matching process. However, when evaluating on balanced test sets (Tables S8-S10), all classes are equally important during Hungarian matching. Therefore, we can mitigate the bias of Hungarian matching by evaluating on balanced test sets, where we still observe significant improvements.\n\n\nWe appreciate reviewer xbKw's suggestion, but we believe that kNN is not a suitable evaluation metric for imbalanced clustering due to the following reasons:\n\n1. kNN evaluation requires label information from the training set, which is not available in deep clustering. \n2. kNN is usually an evaluation of representation, while deep clustering learns not only representation but also clustering head.\n   \nIn conclusion, we suggest evaluating on balanced test set to alleviate the bias of Hungarian matching and evaluate the generalization of different methods.\n\nHuang et al. achieve uniform constraints by maximizing the inter-cluster distance between prototypical representations to avoid degenerate solutions. Unlike [1], we achieve that by utilizing the KL constraint in the pseudo-label generation process. We have updated Appendix D to discuss more on [1] with respect to uniform constraints.\n\n\n[1] Zhizhong Huang, Jie Chen, Junping Zhang, and Hongming Shan. Learning representation for\nclustering via prototype scattering and positive sampling. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.\n\n\n## Questions\n1. We have corrected the typo.\n2. The DINO is pre-trained on ImageNet-1K, therefore we have not adopted the ImageNet-1K dataset as a training set. In the training, we only fine-tuned the last transformer block of ViT-B16.\n3. Please refer to **General Response Part 2**.\n4. Figure 2 is the visualization of the feature before the cluster head. We have updated the manuscript to make it clear."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700419463379,
                "cdate": 1700419463379,
                "tmdate": 1700419778094,
                "mdate": 1700419778094,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gQ3SvjkM8l",
                "forum": "hD3sGVqPsr",
                "replyto": "M6eLhv7d3A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3053/Reviewer_xbkw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3053/Reviewer_xbkw"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Thanks for your efforts in rebuttal. I have read the rebuttal and other reviews. There are no further comments."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700553990754,
                "cdate": 1700553990754,
                "tmdate": 1700553990754,
                "mdate": 1700553990754,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "a2v9PkDSg6",
            "forum": "hD3sGVqPsr",
            "replyto": "hD3sGVqPsr",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3053/Reviewer_H8zS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3053/Reviewer_H8zS"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to address deep clustering in an imbalanced scenario. In particular, the authors resort to partial optimal transport to gradually select imbalance-aware high-confidence samples based on pseudo-labels. The selected high-confidence samples are then considered as ground truth labelled data for supervised training. The proposed method has been evaluated in human-curated datasets and achieves superior results over baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe proposed algorithm is overall reasonable. \n2.\tSufficient empirical results are conducted. \n3.\tThe performance over SOTA clustering baselines are impressive."
                },
                "weaknesses": {
                    "value": "1. Some claims in this paper are confusing and need improvement. For instance, (i) \"the KL divergence-based uniform distribution constraint empowers our method to avoid degenerate solutions and generate imbalanced pseudo-labels\", \"demonstrating that KL constraint enables our P2OT to generate imbalanced pseudo labels.\" Why can the KL constraint generate imbalanced pseudo-labels, given that it is defined for a uniform distribution?\n2. I don't think Eq. 5-Eq. 6 are necessary. You can consider introducing Eq. 8 directly after Eq. 3 by introducing a virtual cluster. If I am wrong, please point it out.\n3. The technical contributions are overclaimed. It appears that the most important aspect of the proposed method lies in the KL constraints. If these KL constraints are added to other clustering baselines, the superiority of the proposed method will be marginal. Additionally, gradually increasing the number of high-confidence samples has been widely adopted in other clustering papers such as SPCIE. It is true that the proposed method can avoid manual selection, but it still introduces an additional hyperparameter in Eq. 7, i.e., $\\rho_0$. More importantly, it does not demonstrate the superiority over the baselines.\""
                },
                "questions": {
                    "value": "1. A very popular real imbalanced dataset is REUTERS-10K[1], which is more challenging than the constructed dataset. How does the proposed method perform on REUTERS-10K?  \n2. In Eq. 3, the KL constraint is defined for the uniform distribution. Why it can be called an unbalanced OT problem?  \n3. The proposed method is defined for the whole dataset. How can it be implemented for a mini-batch scenario? In the mini-batch scenario, samples from minority clusters may not exist.  \n4. Figure 2 shows that the embedding of P2OT is well-separated compared to that of other baselines. Can the authors explain why the embedding is much better than others, given that it is only slightly higher than SCAN in terms of NMI?  \n5. Figure 4 shows that the performance degenerates after 10 epochs. Can you explain the reason behind this?  \n\n[1] Xie, Junyuan, Ross Girshick, and Ali Farhadi. \"Unsupervised deep embedding for clustering analysis.\" International conference on machine learning. PMLR, 2016"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3053/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3053/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3053/Reviewer_H8zS"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3053/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698846243674,
            "cdate": 1698846243674,
            "tmdate": 1700739206226,
            "mdate": 1700739206226,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rU5AcXcNGb",
                "forum": "hD3sGVqPsr",
                "replyto": "a2v9PkDSg6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3053/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3053/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer H8zS"
                    },
                    "comment": {
                        "value": "We appreciate the valuable feedback and thoughtful comments from reviewer H8zS. We have diligently analyzed the suggestions and concerns raised, and we hope the following response could address them. However, we respectfully disagree with the assessment that \"The technical contributions are overclaimed.\".\n\n## Some claims ... are confusing ...\n\nWe appreciate that reviewer H8zS points out the confusing statements. We clarify that our method with $KL$ constraint can generate imbalanced pseudo-labels instead of KL can generate imbalanced pseudo-labels. It's worth noting that while the KL constraint is defined for a uniform distribution, it serves as a **relaxed constraint** compared to the typical equality constraint used in [1]. Consequently, the optimal Q in our P$^2$OT should take into account both the inherent imbalanced class distribution in P and a prior uniform distribution. This enables our method to: \n1. generate pseudo labels reflecting imbalanced characteristics akin to P\n2. avoid degenerate solutions where all samples are assigned to a single cluster.\n\nFor clarity, we have updated the confusion statement in the paper as follows:\n1. The sentence \"the KL divergence-based uniform distribution constraint empowers our method to avoid degenerate solutions and generate imbalanced pseudo-labels\" should be changed to: \"Notably, the $KL$ divergence-based uniform distribution constraint empowers our method to avoid degenerate solutions. And our approach with $KL$, which represents a relaxed constraint compared to an equality constraint, facilitates the generation of imbalanced pseudo-labels.\"\n2. The sentence \"demonstrating that KL constraint enables our P$^2$OT to generate imbalanced pseudo labels.\" should be changed to: \"demonstrating that our P2OT with $KL$ constraint can generate imbalanced pseudo labels.\"\n\n\n[1] Yuki M. Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labeling via simultaneous clustering and representation learning. In International Conference on Learning Representations (ICLR), 2020.\n\n## Eq.5 - Eq.6 are not necessary.\n\nWe respectfully disagree with Reviewer H8zS's assessment of this weakness for the following reasons:\n\n1. Eq.5-6 formulates the confident sample selection and imbalanced pseudo-label generation as a novel partial optimal transport problem, which is the central component of our method. \n\n2. Virtual clusters are commonly employed in addressing partial optimal transport (POT) problems. However, Eq.3-4 does not constitute a POT problem. Therefore, introducing Eq.8 directly after Eq.3-4 through the introduction of a virtual cluster makes it challenging to interpret and understand.\n\nIn conclusion, we believe that Eq.5-6 is necessary.\n\n[1] Luis A Caffarelli and Robert J McCann. Free boundaries in optimal transport and monge-ampere obstacle problems. Annals of Mathematics, pp. 673\u2013730, 2010.\n\n[2] Laetitia Chapel, Mokhtar Z Alaya, and Gilles Gasso. Partial optimal transport with applications on positive-unlabeled learning. Advances in Neural Information Processing Systems, 33:2903\u20132913, 2020."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700419012945,
                "cdate": 1700419012945,
                "tmdate": 1700419012945,
                "mdate": 1700419012945,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JLmhRJ3TeC",
                "forum": "hD3sGVqPsr",
                "replyto": "a2v9PkDSg6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3053/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3053/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## REUTERS-10K dataset\n\nAs shown in Table 1 of [1], the REUTERS-10k dataset contains only 10,000 points and 4 classes, which is significantly smaller than the ImageNet-R and iNature500 datasets used in our experiments. Additionally, it is a dataset intended for document clustering, whereas our manuscript focuses on image clustering.\n\n[1] Xie, Junyuan, Ross Girshick, and Ali Farhadi. \"Unsupervised deep embedding for clustering analysis.\" International conference on machine learning. PMLR, 2016\n\n## Why it can be called an unbalanced OT problem?\n\nWhen $F_1$ and $F_2$ are equality constraints, Eq. 1 is typically referred to as \"classical\" optimal transport. To generalize \"classical\" optimal transport, unbalanced OT [1,2,3] defines the problem on arbitrary positive measures and some divergence functions. In our manuscript, we refer to Eq. 3 as an unbalanced OT in order to distinguish it from \"classical\" optimal transport.\n\n\n[1] Lenaic Chizat, Gabriel Peyr\u00e9, Bernhard Schmitzer, and Fran\u00e7ois-Xavier Vialard. Scaling algorithms for unbalanced optimal transport problems. Mathematics of Computation, 87(314):2563\u20132609, 2018.\n\n[2] Matthias Liero, Alexander Mielke, and Giuseppe Savar \u0301e. Optimal entropy transport problems and a new Hellinger\u2013Kantorovich distance between positive measures. Inventiones Mathematicae, pages 1\u2013149, 2015.\n\n[3] S\u00e9journ\u00e9, Thibault, Gabriel Peyr\u00e9, and Fran\u00e7ois-Xavier Vialard. \"Unbalanced Optimal Transport, from theory to numerics.\" Handbook of Numerical Analysis 24 (2023): 407-471.\n\n\n## Mini-Batch\n\nPlease refer to General Response 3.\n\n\n\n## NMI explanation\n\nOur method does achieve better embedding but due to the calculation of NMI, we achieve modest improvement.\n\nBaselines tend to merge tail classes into head classes, whereas our approach tends to divide head classes into multiple clusters, resulting in a more even class distribution (Figure S7). The formula for the Normalized Mutual Information (NMI) is $\\frac{2I(Y;C)}{H(Y)+H(C)}$, where Y represents the ground truth and C denotes the cluster label. Despite our method achieving superior clustering performance, as indicated by a higher $I(Y;C)$, it also results in a relatively uniform distribution, thereby increasing $H(C)$. This leads to only a modest improvement or decrease in the NMI metric.\n\n\n\n\n## Explanation of Figure 4\nIn Figure 4, we show the weighted precision and recall based on our selection strategy, which generates a weight, $Q1_K$, for samples. As training progresses, we increase $\\rho$ using a sigmoid ramp-up strategy and select more samples, which inevitably includes more noisy samples, leading to a decrease in weighted precision and recall after 10 epochs. However, the precision and recall of the entire dataset are steadily increasing, illustrating our method learns meaningful clusters gradually. We have analyzed this phenomenon in the ablation study and mentioned that the current ramp-up function may not be optimal, leaving more advanced designs for future work."
                    },
                    "title": {
                        "value": "Response to Reviewer H8zS"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700419303448,
                "cdate": 1700419303448,
                "tmdate": 1700419321234,
                "mdate": 1700419321234,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MUPs44tbd3",
                "forum": "hD3sGVqPsr",
                "replyto": "a2v9PkDSg6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3053/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3053/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer H8zS"
                    },
                    "comment": {
                        "value": "Thank you so much for taking the time to review our submission. We appreciate your detailed feedback and appraisal of our work, which we have taken into careful consideration in our rebuttal response. As the rebuttal process is coming to an end, we would be grateful if you could acknowledge receipt of our responses and let us know if they address your concerns. We remain eager to engage in any further discussions."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722476137,
                "cdate": 1700722476137,
                "tmdate": 1700722476137,
                "mdate": 1700722476137,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "W7Gn6syUXY",
                "forum": "hD3sGVqPsr",
                "replyto": "JLmhRJ3TeC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3053/Reviewer_H8zS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3053/Reviewer_H8zS"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the responses. My concerns have been addressed. So I increase my score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3053/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700739120587,
                "cdate": 1700739120587,
                "tmdate": 1700739120587,
                "mdate": 1700739120587,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]