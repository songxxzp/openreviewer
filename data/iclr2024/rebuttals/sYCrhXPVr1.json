[
    {
        "title": "Video Generation Beyond a Single Clip"
    },
    {
        "review": {
            "id": "b5aN0MNFSs",
            "forum": "sYCrhXPVr1",
            "replyto": "sYCrhXPVr1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission466/Reviewer_cBdr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission466/Reviewer_cBdr"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed a multi-stage approach to generate long videos that encompass diverse content and multiple events, which utilizing additional guidance to steer the whole video generation process. These additional guidance includes object labels, box, and layout.\n\nSpecifically, this paper first predict all the keyframes jointly based on the input guidance. These keyframes represent the starting\nframes of each video clip. Then generating the entire video by predicting the intermediate frames between keyframes, using the video clip generation model.\n\nThe proposed method outperforms state-of-the-art video generation models by 9.5% on FVD."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The approach of initially predicting keyframes for different clips in a long video and then interpolating seems reasonable. Indeed, to some extent, it can address the issue of repetitive content in long videos.\n\n2. Constrain the interaction between layout and keyframe to preserve the object coherency in the long video using layout-keyframe attention mask.\n\n3. Achieved good performance on two public datasets."
                },
                "weaknesses": {
                    "value": "1. This work is meaningful and has demonstrated good performance on some mainstream datasets, marking a significant improvement compared to previous efforts. However, in the era of large models and datasets, video diffusion models have shown more promising performance with features such as high resolution and more reasonable motion [1][2][3]. If the authors could further expand their training dataset to enhance performance and generalization capabilities, it would make this work even more noteworthy. Of course, the reviewer understands the substantial effort required for such tasks and does not necessarily expect the authors to provide additional experiments or proofs.\n\n2. Furthermore, the reviewer recommends that the authors provide more visual demonstrations from other domains, not limited to cooking and driving scenarios. This would offer a more comprehensive showcase of long video generation applications across various scenes.\n\n3. Figure 4 presents a comparison of relevant quality degradation. An interesting observation is that using GT layouts appears significantly better than layouts predicted by the transformer. Layouts seem to play a crucial role in this method. The reviewer recommends that the authors provide visualizations of layouts, especially in bad cases. This can assist readers in understanding the performance of the layout component.\n\n[1] Wang, Yaohui, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang et al. \"LAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models.\" arXiv preprint arXiv:2309.15103 (2023).\n[2] He, Yingqing, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang et al. \"Animate-a-story: Storytelling with retrieval-augmented video generation.\" arXiv preprint arXiv:2307.06940 (2023).\n[3] Ho, Jonathan, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P. Kingma et al. \"Imagen video: High definition video generation with diffusion models.\" arXiv preprint arXiv:2210.02303 (2022)."
                },
                "questions": {
                    "value": "1. User study. The paper conducted the study with 40 videos and 11 participants. Perhaps the authors could provide a brief introduction to the origin and data distribution of these 40 videos, along with any relevant references if they align with previous works. Additionally, clarification on the participant demographics would be helpful, such as whether they are members of the author's team or hired participants. Providing this information would enhance the overall understanding of the study."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "none"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission466/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission466/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission466/Reviewer_cBdr"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission466/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697790808138,
            "cdate": 1697790808138,
            "tmdate": 1699635973079,
            "mdate": 1699635973079,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "wAbrDgiq6x",
            "forum": "sYCrhXPVr1",
            "replyto": "sYCrhXPVr1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission466/Reviewer_e4d3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission466/Reviewer_e4d3"
            ],
            "content": {
                "summary": {
                    "value": "This study addresses the challenge of generating longer videos than current video generation models can produce due to computational limitations. Instead of the common sliding window technique, which is limited to consistent content, the authors propose a multi-stage approach with additional guidance to produce diverse, eventful long videos. Their method, which enhances existing video generation models, has proven superior in real-world tests, outperforming competitors by 9.5% and being favored by users 80% of the time. The main contributions of this paper are: A new method to study the long video generation problem by guidances and a layout module for keyframes, which is essential to guide the generation of subsequent frames."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Approach: The paper introduces a new multi-stage method for generating long videos. The approach employs additional guidance to enable high-quality video generation within a limited time window. \n2. Presentation: The paper is well-organized and provides supplementary materials for readers to understand the proposed method and its components."
                },
                "weaknesses": {
                    "value": "1. It lacks a comprehensive explanation on maintaining temporal consistency, especially when generating intermediate frames under keyframes.\n2. The evaluation section is not robust, with limited results in the general video domain.\n3. There are concerns regarding the model's capacity for fine-grained generation and conditional control, with VQGAN's performance being questionable in these areas.\n4. The paper misses discussions or comparisons with relevant works like \"Towards Smooth Video Composition\" and \"AnimateDiff-Unlimited Context Length mode.\"\n5. In the visualization experiments, the variety of scene categories is limited, raising doubts about the model's adaptability to diverse or open scenes.\n\n\n[R1] Zhang, Qihang, et al. \"Towards Smooth Video Composition.\" ICLR 2023.\n\n[R2] Guo, Yuwei, et al. \"Animatediff: Animate your personalized text-to-image diffusion models without specific tuning.\" arXiv 2023."
                },
                "questions": {
                    "value": "See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Discrimination / bias / fairness concerns"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The generated content may contain some concept bias."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission466/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698293052120,
            "cdate": 1698293052120,
            "tmdate": 1699635973013,
            "mdate": 1699635973013,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "cKZz3k3R3R",
            "forum": "sYCrhXPVr1",
            "replyto": "sYCrhXPVr1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission466/Reviewer_V5GZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission466/Reviewer_V5GZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the challenge of generating long videos by proposing a multi-stage approach that incorporates additional guidance to steer the video generation process. The authors argue that existing methods, which utilize a sliding window technique, are restricted to homogeneous content and recurring events. The key contributions include the introduction of a holistic video modeling technique through key-frame generation and frame interpolation, enabling the generation of diverse and non-repetitive long videos. The method demonstrates superiority over existing approaches in both objective metrics, user study and qualitative results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. S1. Long video generation is a notorious task. It requires a lot of computational overhead and more elaborate model design. There is currently no universally recognized effective solution. The paper focuses on generating long videos with diverse content and non-repeating events, which importantly addresses a key limitation of existing video generation models.\n2. S2. The incorporation of additional auxiliary information, including labels and layouts, has proven to be an effective solution to generate long videos in this paper.\n3. S3. Quality: The proposed method outperforms state-of-the-art models in objective metrics, showcasing the high quality of generated videos."
                },
                "weaknesses": {
                    "value": "1. W1. Although a substantial number of quantitative and qualitative experiments are provided in this paper, they are all conducted on the 64-frame video. It is doubtful whether a 64-frame video is considered as the long video. The absence of experimental results for extended video scenarios casts doubt on the scalability of the proposed method. Relying solely on 64-frame videos undermines the comprehensive evaluation of the method's capabilities in long video generation. More results, such as $\\ge$ 128-frame videos, are needed.\n2. W2. While the integration of additional auxiliary conditions proves effective in long video generation, concerns arise regarding the generalizability of this approach. The method rely on the robust labels and layouts as auxiliary conditions may not be universally applicable. Not all videos can extract labels and layouts, like rivers and starry skies. Have the authors considered other types of auxiliary conditions?\n3. W3. Video diffusion models are a popular topic. It is suggested to discuss more about the advantages of the proposed method over recent diffusion-based video generation methods. Considering that the SOTA method with diffusion model can generate high-quality videos (100+ frames of 512x512 pixels), what will be the advantages of the proposed method?\n4. W4. This paper lacks clarity concerning the computational cost of the proposed method, particularly in terms of reasoning cost and efficiency. A more explicit delineation of these computational aspects is imperative for a comprehensive understanding of the method's practical implications and resource requirements.\n5. W5. Reference missing:\n- Zhang et al., \u201cTowards Smooth Video Composition\u201d, ICLR 2023\n- Yoo et al., \u201cTowards End-to-End Generative Modeling of Long Videos with Memory-Efficient Bidirectional Transformers\u201d, CVPR 2023"
                },
                "questions": {
                    "value": "Please refer to the weaknesses, especially W1, W2, and W3."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission466/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698564721027,
            "cdate": 1698564721027,
            "tmdate": 1699635972926,
            "mdate": 1699635972926,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "Stw2oXdQN9",
            "forum": "sYCrhXPVr1",
            "replyto": "sYCrhXPVr1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission466/Reviewer_ZyRs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission466/Reviewer_ZyRs"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a long video generation method by using the layout to guide the generation process. It breaks the generation into multiple stages, which first generate a sequence of layouts and use these layouts as conditions to generate keyframes of videos. Finally, it adopts frame interpolation to generate the intermediate frames. The authors evaluate their method on three datasets and achieve improved performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed method achieves improved performance on three datasets\n2. The authors provide a thorough analysis of related work."
                },
                "weaknesses": {
                    "value": "1. The idea of first generating keyframes and then interpolating temporally for long video generation has been proposed by many previous works, like \"Make-A-Video: Text-to-video Generation Without Text-Video Data\", \"CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers\" and \"Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models\". \n2. The authors propose to first generate the layout of video content, i.e. bounding boxes with class labels.  However these things are not readily accessible, and they require a substantial amount of manual annotation. Hence, it's not suitable for scaling up.\n3. Using simple domain-specific datasets for evaluation cannot adequately validate the proposed method's effectiveness. Even for these simple cases, the visualization result does not seem impressive and demonstrates severe artifacts."
                },
                "questions": {
                    "value": "How to generalize the proposed method to open-domain video generation given that the layout annotations of a lot of training video data are missing?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission466/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698740089503,
            "cdate": 1698740089503,
            "tmdate": 1699635972824,
            "mdate": 1699635972824,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]