[
    {
        "title": "PROSPECT: Learn MLPs Robust against Graph Adversarial Structure Attacks"
    },
    {
        "review": {
            "id": "ZpGXoiVPdP",
            "forum": "EZ4VwxpzCK",
            "replyto": "EZ4VwxpzCK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission634/Reviewer_Gu8Y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission634/Reviewer_Gu8Y"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an online GNN-MLP distillation framework, PROSPECT, that is able to handle heterophily and robust to graph structural attacks.\nThe online framework is enhanced via a specified learning rate scheduler.\nThe effectiveness of both the framework and scheduler is theoretically verified.\nThe robustness of PROSPECT is empirically validated via extensive experiments against structural attacks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. It is interesting to study how distillation methods perform under heterophily. \n2. Good theoretical guarantees for the proposed methods.\n3. Desirable robustness against structural attacks."
                },
                "weaknesses": {
                    "value": "1. **About Motivation.** The most concern of mine is that the improvement with respect to robustness and performance under heterophily results from the introduction of distillation but not the proposed PROSPECT. \nBased on the analysis, any GNN-MLP distillation framework seems to surpass a single GNN in the two fields.\nWhile the advantage of PROSPECT over previous distillation methods regarding clean accuracy is slightly discussed on page 4, how PROSPECT excels against attacks and heterophily is unclear. \n2. **Number of Hyperparams.** The whole framework, together with the scheduler, includes quite a few hyperparameters compared to the baselines. \nIt would be a concern if the hyperparameters varied a lot between different datasets. \nAlso, if the performance is highly affected by the hyperparameters.\n3. **Baseline Missing.** A robust defense model that also tackles heterophily is not included. It would be better to have a comparison to it. [1] \n4. **Presentation.** The current version is not friendly to those unfamiliar with distillation and graph adversarial attacks. It would be better if more details about distillation (like the online/offline settings) and a pseudo-code of PROSPECT were offered.\n\nOther concerns are discussed in Questions.\n \n[1] Deng, Chenhui, et al. \"GARNET: Reduced-Rank Topology Learning for Robust and Scalable Graph Neural Networks.\" Learning on Graphs Conference. PMLR, 2022."
                },
                "questions": {
                    "value": "1. **Training time.** While the framework shows the desirable inference efficiency, what about its training time? \n2. **About Figure 1.** It is good to see that the proposed QACA works through the ablation study in Figure 1. \nHowever, the fixed version seems noncompetitive against other defense baselines. \nIs the QACA scheduler the one that actually contributes to the performance?\nCan the previous distillation methods be enhanced by QACA as well?\n3.  **Concerns when faced with other attacks.** \nIt makes sense that MLP is robust against structural attacks and acts as good teachers to GNNs, but when faced with feature attacks or graph injection attacks, the introduction of distillation could lead to worse performance.\nAs the robustness is only tested against Metattack, it would be more convincing if PROSPECT is tested under more attack settings."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission634/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission634/Reviewer_Gu8Y",
                        "ICLR.cc/2024/Conference/Submission634/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission634/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697905181009,
            "cdate": 1697905181009,
            "tmdate": 1700704337088,
            "mdate": 1700704337088,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4Psc5IsAtQ",
                "forum": "EZ4VwxpzCK",
                "replyto": "ZpGXoiVPdP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission634/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission634/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reponse (Part 1 Weakness 1)"
                    },
                    "comment": {
                        "value": "* ***The benefits of GNN-to-MLP distillation.***\n\nPrior works, e.g., GLNN,first train a GNN teacher, then the GNN-to-MLP direction distillation is leveraged to transfer the knowledge from a pretrained GNN to a student MLP for fast inference. As shown by the clean accuracy results in Table 2, GLNN (i.e., GNN-to-MLP distillation)  cannot consistently improve the teacher SAGE (on heterophilic graphs) but succeeds in boosting the student MLP.  That is **GNN-to-MLP distillation is important to get a decent MLP w.r.t. clean accuracy.** The observation 2 in Appendix G.2.2 have revealed that GLNN often exceeds an alone SAGE teacher w.r.t. adversarial robustness, meaning that **GNN-to-MLP distillation can sometimes slightly mitigate the poisoning attacks.**\n\n* ***PROSPECT incorporates both GNN-to-MLP and MLP-to-GNN distillation.***\n\nOur PROSPECT simultaneously performs both GNN-to-MLP direction distillation and MLP-to-GNN direction distillation to jointly learn decent GNN and MLP. While the benefits of GNN-to-MLP distillation are shown by the above discussion, the benefits of MLP-to-GNN distillation is supported by Theorem 1. The goal of PROSPECT is to merge the knowledge from both MLP and GNN sides via these two distillation directions. Such knowledge fusion brings about desired effects, such as the robustness and adaptability to heterophily.\n\n* ***How PROSPECT excels GLNN and SAGE against (structure) attacks?*** Substantially by MLP-to-GNN distillation.\n\n  As stated by the paragraph between Remark1 and Remark2 (at p.5) and discussed by Observation 3 in Appendix G.2.2, the Prospect Condition is usually met for node $i$ under successful structure attacks that drastically degrade the SAGE performance but have no influence on the MLP due to its independence to graph structure. \n\n  One thing worth noting is that for the attacked node $i$, the MLP does NOT need to correctly classify it to be a good teacher because any successful structure attacks can always significantly decrease the SAGE classificaiton score on the ground-truth class $c$ of node $i$ such that even an MLP can output a higher score on class $c$. This makes the defense mechanism of PROSPECT more applicable.\n\n* ***How PROSPECT excels GLNN and SAGE against heterophily?*** By mutual distillation.\n\n  On one hand, as shown by Table 2 (at p.8), MLP can have decent performance on heterophilous graphs. On Texas and UAI, MLP even outperforms SAGE, meaning that MLP has some useful information to exploit. On the other hand, given their respective emphases on node attributes and topological structures, MLPs and GNNs are very likely to exhibit divergence in their correct prediction test sets V_{mlp} and  V_{gnn}.  So the union of these two sets (i.e., the knowledge fusion of MLP and GNN) V_{union} could lead to a larger correct prediction set than alone MLP or alone GNN. We have observed this phenomena across all clean and attacked (LLC) datasets used in the paper. Owing to the space limit, we only show the case on clean datasets in **Table A**. With the offline procedure, the SAGE teacher in GLNN can never acquire the knowledge from MLP. In contrast, the mutual disitllation mechanism of PROSPECT makes it possible to merge the knowledge from both GNN and MLP sides, and this is why PROSPECT excels alone MLP/SAGE and GLNN in clean (heterophily)  accuracy.\n\nAccording to the above analysis, the improvement of PROSPECT over GLNN (solely based on GNN-to-MLP distillation) and alone GNN results from the introduction of the neglected MLP-to-GNN distillation and the knowledge fusion achieved through mutual interaction.\n\n\n**Table A**: Test Acc. (%) on the correct test sets of MLP and GNN, and the union correct set, with seed=15\n|                       | cora  | citeseer | polblogs | acm   | cora_ml | texas | chameleon | uai   |\n| --------------------- | ----- | -------- | -------- | ----- | ------- | ----- | --------- | ----- |\n| $\\mathcal{V}_{gnn}$   | 83.40 | 73.76    | 95.71    | 91.65 | 84.43   | 68.71 | 54.50     | 63.24 |\n| $\\mathcal{V}_{mlp}$   | 62.78 | 66.23    | 52.04    | 85.91 | 68.42   | 68.03 | 40.23     | 65.04 |\n| $\\mathcal{V}_{union}$ | 87.42 | 80.04    | 97.44    | 95.99 | 89.01   | 78.23 | 64.60     | 71.96 |"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission634/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570851139,
                "cdate": 1700570851139,
                "tmdate": 1700621276307,
                "mdate": 1700621276307,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QGnO5SgOYy",
                "forum": "EZ4VwxpzCK",
                "replyto": "ZpGXoiVPdP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission634/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission634/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reponse (Part 2 Weakness 2-4, Question 1)"
                    },
                    "comment": {
                        "value": "**Weakness 2**: Number of Hyperparams.\n\n* ***The hyperparameters of PROSPECT:*** The hyperparameters introduced by PROSPECT include the loss weights for the logit distillation terms of the individual MLP and GNN optimization objectives, and the knowledge distillation temperatures of them. These are inevitable hyperparameters present in all mutual distillation architectures.\n\n* ***The hyperparameters of QACA:*** Although QACA contains hyperparameters including individual learning rates, weight decay values, minimum optimization cycle, and minimum learning rates of MLP and GNN, only the last three hyperparameters are actually introduced by the QACA scheduler. The other hyperparameters are originally present in the individual MLP and GNN models themselves.\n\n* ***Why these hyperparameters?*** **1)** The differences in these hyperparameters across datasets are primarily to accommodate the behaviour differences between the MLP and GNN on datasets with different characteristics (see **Table A** for a glance). How to design mutual distillation schemes between GNNs and MLPs that can self-adapt to different dataset characteristics is an important future research direction that would require a series of complete independent studies to elaborate (we are working on such a project). Adding this would make this already lengthy paper even more complex and diffuse in focus.\n\n**Weakness 3**: Baseline Missing.\n\nWe have supplemented many experiments, including the comprehensive comparison between GPRGNN [1], GARNET [2] and our PROSPECT under 4 other attacks in addition to Metattack. Please refer to the **Appended Experiments** section of joint response above for details.\n\n**Weakness 4**: Presentation.\n\n* ***More details about graph distillation***\n  We can only add an introduction about offline and online graph distillation models to the appendix due to the length limit of main text.\n* ***A pseudo-code***\n  By aggregating the losses of the MLP and GNN components to derive an overall loss term loss, then invoking loss.backward(), the PROSPECT training procedure resembles optimizing a solitary model, except that the MLP and GNN each have their own optimizers and QACA scheduler. If the reviewer feels necessary, we can add a pseudocode in the appendix.\n\n**Question 1**: Training time.\nThe MLP, GCN and SAGE can be formulated as $H=XW$, $H=AXW$ and $H=XW_1+AXW_2$  , respectively. And the basic operations to build them are  $Z=WX$ and $H=AZ$. Han et al. [3] have reported the forward and backward time of these two basic operations on serveral graphs. We extracte the data on Arxiv from the time comparison Tabel 1 in [3] into **Table B** below. According to the snippet, we can roughly estimate the training time of PROSPECT (that comprises one SAGE and one MLP), which shows that the additional training overhead incurred by PROSPECT over SAGE is almost negligible.\nMoreover, in the supplemented GR-BCD attack experiments on Arxiv (see the joint response), we report the training times of PROSPECT and GARNET. These results show that the training overhead of PROSPECT is almost the same as using GCN alone, and much lower than GARNET.\n\n**Table B**: Forward and backward times (ms). The first two rows are from Table 1 of [3], and the last two rows are the estimations based on the first rows.\n| Operation | Forward | Backward | Total   |\n| --------- | ------- | -------- | ------- |\n| $Z=WX$    | 0.32    | 1.09     | 1.42    |\n| $H=AZ$    | 1.09    | 1028.08  | 1029.17 |\n| SAGE      | 1.73    | 1030.26  | 1032.01 |\n| PROSPECT  | 2.05    | 1031.35  | 1033.43 |"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission634/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570961465,
                "cdate": 1700570961465,
                "tmdate": 1700570961465,
                "mdate": 1700570961465,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RJumH3BVe1",
                "forum": "EZ4VwxpzCK",
                "replyto": "ZpGXoiVPdP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission634/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission634/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reponse (Part 3 Question 2-3)"
                    },
                    "comment": {
                        "value": "**Question 2**: About Figure 1.\n\n* ***\"However, the fixed version seems noncompetitive against other defense baselines. \"***\n  As discussed in the **Contributions/The key to future mutual distillation** section of joint response, simply combining GNNs and MLPs for mutual distillation may easily lead to failure because of the model heterogeneity issue identified and (partially) solved by us.\n\n* ***\"Is the QACA scheduler the one that actually contributes to the performance?\"***\n  **1)** As discussed in **Weakness 1**, GNN-to-MLP and MLP-to-GNN distillation, i.e., the interaction between GNN and MLP contributes to the clean accuracy and robustness due to the accumulated knowledge post knowledge fusion (through mutual distillation). This gain has nothing to do with a specific optimization method. **2)** However, as pointed out in the **Contributions/The key to future mutual distillation** section of joint response, simply combining GNN and MLP can lead to failure and that's why we need a QACA optimization paradigm. **3)** In short, we think that the PROSPECT framework itself determines its capability, but this capability is locked. And QACA is just the key that opens the lock.*\n* ***\"Can the previous distillation methods be enhanced by QACA as well?\"***\n  **1)** QACA addresses underlying knowledge discrepancies within heterogeneous model distillation, hence offline frameworks, e.g.,GLNN, do not stand to benefit. Potentially, QACA mitigates conflicts even for general heterogeneous mutual distillation, but space constraints precluded detailed analysis beyond GNN-MLP. **2)** As pioneering GNN-MLP mutual distillation research, this paper validates QACA efficacy for this task. Intuitively, distilling between similar models unlikely exhibits severe conflicts, limiting QACA\u2019s boons for GNN-GNN. However, QACA could assist future GNN-GNN models incorporating knowledge conflicts.\n\n**Question 3**: Concerns when faced with other attacks.\n\n* ***Graph injection attack***\n  Graph injection attacker, e.g., $G^2A2C$ [4], first adds a virtual node $a$ for the target node $i$ and then connect it to the original graph. During the neighborhood aggregation, the feature of node $i$ will be interfered such that its score on the correct category $c$ will decrease significantly. Now the case fails back to the first question in **Weakness 1** and the point becomes that whether the MLP is affected by node $a$ such that the MLP have a smaller score on $c$ than that of the GNN and thus becomes unqualified to be a teacher. Since the injected nodes are not used to train the MLP (no neighbor aggregation), they do not impact the MLP. Thus PROSPECT can work like under the structure attack, e.g., Metattack, as shown by our submission. Our supplementary experiments (see Section **Appended Experiments/Graph injection attacks** in the joint response) with the SOTA injection attack $G^2A2C$ demonstrate this.\n* ***Node feature attack***\n  We discuss the node feature attack scenario in the\n  **Concerns about joint, injection and adaptive attacks** section of joint response.\n* ***\"It would be more convincing if PROSPECT is tested under more attack settings.\"***\n  To improve the persuasiveness regarding robustness, results across four additional attack methods have been included in the **Appended Experiments** section of the joint response.\n\n[1] E. Chien, et al., \u201cAdaptive Universal Generalized PageRank Graph Neural Network,\u201d ICLR'2021.\n\n[2] Deng, Chenhui, et al. \"GARNET: Reduced-Rank Topology Learning for Robust and Scalable Graph Neural Networks,\" LoG'2022.\n\n[3] X. Han, T. Zhao, Y. Liu X. Hu, and N. Shah, \u201cMLPInit: Embarrassingly Simple GNN Training Acceleration with MLP Initialization,\u201d ICLR'2023.\n\n[4] M. Ju, et al. \"Let Graph Be the Go Board: Gradient-Free Node Injection Attack for Graph Neural Networks via Reinforcement Learning,\" AAAI'2023."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission634/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700571085318,
                "cdate": 1700571085318,
                "tmdate": 1700571085318,
                "mdate": 1700571085318,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XwOPAM30lr",
                "forum": "EZ4VwxpzCK",
                "replyto": "RJumH3BVe1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission634/Reviewer_Gu8Y"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission634/Reviewer_Gu8Y"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for their efforts in clarification. Most of my concerns have been addressed. I would be glad to increase my score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission634/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700704315973,
                "cdate": 1700704315973,
                "tmdate": 1700704315973,
                "mdate": 1700704315973,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sht0dAa666",
            "forum": "EZ4VwxpzCK",
            "replyto": "EZ4VwxpzCK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission634/Reviewer_9crP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission634/Reviewer_9crP"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces PROSPECT, a graph distillation framework for learning robust GNNs/MLPs against graph adversarial attacks. Specifically, authors leverage two loss functions for GNN-to-MLP and MLP-to-GNN distillation. By alternately minimizing the loss functions, PROSPECT learns robust GNNs/MLPs that are resistant to graph attacks. Experimental results show that PROSPECT improves clean and adversarial accuracy over defense baselines on both homophilous and heterophilous graphs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Overall, the paper is well-written.\n- Authors have evaluated PROSPECT on both homophilous and heterophilous datasets."
                },
                "weaknesses": {
                    "value": "- Missing relevant defense models for evaluation. Authors mentioned that prior purification methods are computationally expensive and restricted to homophilous graphs. However, there are some recent studies (e.g., [1]) that have addressed these limitations. It would largely improve the paper to include comparisons against proper baselines.\n- Missing adaptive attack results. As shown by [2], most prior defense GNN methods can be easily broken by adaptive attacks, which are aware of the given defense method during attacking. Thus, it is very important to adaptively attack PROSPECT to demonstrate its true robustness.\n- The heterophilous datasets used in this work (e.g. Chameleon) are known to have some critical issues (e.g., train-test data leakage) [3]. Hence, the experimental results would be more compelling if authors could evaluate on the datasets from [3,4].\n- Missing detailed hyperparameter settings for baselines. Note that many defense methods (e.g., ProGNN) require decent hyperparameter tuning to achieve their best performance. It is unclear whether authors put in enough efforts on tuning those baselines.\n- Claim 3 is too strong. Given that PROSPECT is only integrated with GraphSAGE in this work, it is improper to claim PROSPECT can boost clean accuracy of GNNs, unless authors conduct more experiments with different GNNs integrated into PROSPECT.\n\n[1]: Deng et al., \u201cGARNET: Reduced-Rank Topology Learning for Robust and Scalable Graph Neural Networks\u201d, LoG'22. \\\n[2]: Mujkanovic et al., \u201cAre Defenses for Graph Neural Networks Robust?\u201d, NeurIPS'22. \\\n[3]: Platonov et al., \u201cA critical look at the evaluation of GNNs under heterophily: are we really making progress?\u201d, ICLR'23. \\\n[4]: Lim et al., \u201cLarge Scale Learning on Non-Homophilous Graphs: New Benchmarks and Strong Simple Methods\u201d, NeurIPS'21."
                },
                "questions": {
                    "value": "- It does not seem reasonable to adopt GCN as the surrogate model for MetaAttack on heterophilous datasets. Why didn't the authors choose heterophilous GNNs as the surrogate model?\n- Are baseline models trained with cosine annealing learning rate scheduler?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission634/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission634/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission634/Reviewer_9crP"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission634/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698603361659,
            "cdate": 1698603361659,
            "tmdate": 1699635991258,
            "mdate": 1699635991258,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3eoFyQodsz",
                "forum": "EZ4VwxpzCK",
                "replyto": "sht0dAa666",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission634/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission634/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Dear Reviewer 9crP,\n\nWe really appreciate your comments. We hope our point-to-point response can address your concerns.\n\n**Weakness 1**: \"Missing relevant defense models for evaluation.\"\n\nWe thank the reviewer for the reminder and add GPRGNN [1] GARNET [2] baselines. GARNET is indeed the master of current graph purification methods, but its scalability is limited by the (truncated) SVD decomposition and is weaker than PROSPECT, as shown by the supplementary experiments (see Section **Appended Experiments/Arxiv attacked by GR-BCD** of the joint reponse). \n\n**Weakness 2**: \"Missing adaptive attack results.\"\n\nWe explain in the **Concerns about joint, injection and adaptive attacks/Adaptive attacks** section of joint response why the adaptive attack test is missing and add the experiments with the black-box unit test datasets provided by Mujkanovic et al. [3].\n\n**Weakness 3**: \"The heterophilous datasets used in this work are known to have some critical issues.\"\n\nWe thank the reviewer for pointing out this critical issue. After seeing the comments, we tried to make attack datasets with the dataset proposed in [4], but due to time and computing resource constraints, we were unable to supplement the corresponding experimental results during the discussion period. Therefore, we hope you can refer more to the results on UAI, a safe heterophilic data set.\n\n**Weakness 4**: \"Missing detailed hyperparameter settings for baselines.\"\n\nInitially, we observed inconsistencies in current pre-attacked dataset creation paradigms. Specifically, certain works only utilize a sole train/validation/test split, risking less comprehensive evaluations. Hence, we strived to construct over 10 pre-attacked datasets under 4 attack budgets and 5 splits. During baseline hyperparameter tuning however, even abiding by original author guidelines, optimizing across the 5 splits proved challenging for some methods. We are benchmarking existing robust GNN techniques, tuning their hyperparameters accordingly, and will update performances if improvements emerge. Moreover, the diverse pre-processed attack datasets alongside all benchmark outcomes will be publicly disclosed in future endeavors.\n\n**Weakness 5**:  \"Claim 3 is too strong.\"\n\nThanks for pointing this out, we've narrowed the claims in Claim 3 (now renamed to Remark 3) to SAGE.\n\n**Question 1**:  \"Why didn't the authors choose heterophilous GNNs as the surrogate model?\"\n\nWe also agree with that heterophilous GNNs seems more reasonable, but for the following reasons, we have temporarily compromised. **1)** We found that GCN, as a surrogate for heterophilic data, can actually produce effective attacks, so this does not prevent the evaluation of the defense model. **2)** Attacking under the unified agent model, namely GCN, can provide a reference benchmark for exploring attack and defense on heterophilic graphs. **3)** the MetaAttack and Nettack implementations from current tools, e.g., DeepRobust [5], do not support flexible proxy model settings.\n\n**Question 2**:  \"Are baseline models trained with cosine annealing learning rate scheduler?\"\n\nThe baseline models are not trained with cosine annealing. \nThe performance improvement of PROSPECT mainly comes from the knowledge fusion of GNN and MLP sides. QACA works by solving the knowledge conflict problem in the fusion process but it never brings additional knowledge to one alone GNN or MLP. That is, training SAGE and MLP individually with cosine annealing does not bring additional knowledge and thus is not necessary.\n\n[1] E. Chien, et al., \u201cAdaptive Universal Generalized PageRank Graph Neural Network,\u201d ICLR'2021.\n\n[2] C. Deng, et al., \u201cGARNET: Reduced-Rank Topology Learning for Robust and Scalable Graph Neural Networks,\u201d LoG'2022.\n\n[3] F. Mujkanovic, et al., \u201cAre Defenses for Graph Neural Networks Robust?,\u201d NeurIPS'2023.\n\n[4] O. Platonov, et al., \u201cA critical look at the evaluation of GNNs under heterophily: Are we really making progress?,\u201d ICLR'2023.\n\n[5] Y. Li, et al., \"DeepRobust: A Platform for Adversarial Attacks and Defenses,\" AAAI' 2021."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission634/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570595559,
                "cdate": 1700570595559,
                "tmdate": 1700570595559,
                "mdate": 1700570595559,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QUw19F2xIE",
                "forum": "EZ4VwxpzCK",
                "replyto": "3eoFyQodsz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission634/Reviewer_9crP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission634/Reviewer_9crP"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response and additional clarifications. However, my major concern on the adaptive attack is still valid. Firstly, I hope authors could understand that adaptive attack is very important for assessing the model robustness, as demonstrated in [F. Mujkanovic, et al., NeurIPS'2023]. Thus, it is less convincing to claim the proposed model is robust when it's solely evaluated under the transfer attack setting. More importantly, I don't see any reason why adaptive attack is restricted to a single model. Isn't it true that gradient-based adaptive attacks (e.g., PGD and Meta-PGD used in [F. Mujkanovic, et al., NeurIPS'2023]) can be naturally adopted for the proposed model to maximize Equations 3a and 3b? Notably, it's defender's responsibility to design proper adaptive attacks on the proposed model for revealing its \"true\" robustness."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission634/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700618122431,
                "cdate": 1700618122431,
                "tmdate": 1700618122431,
                "mdate": 1700618122431,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3W8plOrPd5",
            "forum": "EZ4VwxpzCK",
            "replyto": "EZ4VwxpzCK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission634/Reviewer_GM8H"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission634/Reviewer_GM8H"
            ],
            "content": {
                "summary": {
                    "value": "To address 1) low adaptability to heterophily, 2) absent generalizability to early GNNs, and 3) inadequate inference scalability of current defense methods for GNNs, the authors introduce PROSPECT, a defense framework incorporating an online mutual distillation approach between a GNN and an MLP, which enhances the performance on node-level classification tasks for both poisoned and clean graphs. Additionally, the authors apply a quasi-alternating cosine annealing (QACA) learning rate scheduler to improve the optimization process. The proposed approach is supported by detailed theoretical analysis and extensive empirical experiments to validate its effectiveness in mitigating the identified issues in GNNs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Incorporating the mutual distillation method into the existing GD-MLP to impart the MLP\u2019s robustness to adversarial structure attack to GNNs is a concept of interest and significance in practice.\n\n- The theoretical assessment of MLP-to-GNN distillation and QACA learning rate schedular is well discussed and convincing to me.\n\n- Reducing inference time to the level of an MLP while improving the robustness to untargeted poisoning attacks and performance on clean graphs is both uplifting and commendable, with a series of empirical experiments conducted on both graphs of homophily and heterophily."
                },
                "weaknesses": {
                    "value": "While the application of mutual distillation to the GD-MLP is inspiring, it\u2019s worth noting that the novelty of the proposed approach appears somewhat limited, since there has been extensive research and discourse surrounding mutual distillation and distillation from GNNs to MLP. The absence of a specific design tailored to accommodate the unique characteristics of graph-structured networks might dilute the distinctiveness of this approach."
                },
                "questions": {
                    "value": "- The empirical examination of PROSPECT's robustness against a specific untargeted poisoning attack, Metattack, is commendable. However, it could potentially enhance its persuasiveness by extending the experiment to include targeted attack scenarios and additional untargeted attack methods, such as GraD.\n\n- Considering that PROSPECT is designed to be adaptable for various GNNs, it might be practical and insightful to explore its performance under more powerful GNN architectures, such as EvenNet and GPRGNN. This could offer insights into the performance limitations and strengths of the proposed framework."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission634/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission634/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission634/Reviewer_GM8H"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission634/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698805843753,
            "cdate": 1698805843753,
            "tmdate": 1699635991176,
            "mdate": 1699635991176,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oEX9qkoapF",
                "forum": "EZ4VwxpzCK",
                "replyto": "3W8plOrPd5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission634/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission634/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Dear Reviewer GM8H,\n\nWe really appreciate your positive comments. We hope the response can address your concerns and clarify our contribution.\n\n**Weakness 1**: Limited novelty and absence of design tailored to GNN.\n\nIn the joitn response at the top of the page, we reiterate and discuss the contribution and broader impact of our work in the **Contributions** section, hoping that the discussion can solve your concerns.\n\n**Question 1**: More attack scenarios.\n\nFollowing your suggestion, we have evaluated PROSPECT with 4 other kinds of attacks [1,2,3,4], including both untargeted and targeted ones. (The details of these new experiments are provided in Section **Appended Experiments** of the joint response.)\n\n**Question 2**: More backbones.\n\nWe absolutely agree with your opinion and plan to conduct relevant experiments. However, due to time constraints, we deeply regret that we are unable to complete the experiments and provide results during the discussion period.\n\n[1] D. Z\u00fcgner, A. Akbarnejad, and S. G\u00fcnnemann, \u201cAdversarial Attacks on Neural Networks for Graph Data,\u201d KDD'2018.\n\n[2] M. Ju, Y. Fan, C. Zhang, and Y. Ye, \u201cLet Graph Be the Go Board: Gradient-Free Node Injection Attack for Graph Neural Networks via Reinforcement Learning,\u201d AAAI'2023.\n\n[3] F. Mujkanovic, S. Geisler, S. G\u00fcnnemann, and A. Bojchevski, \u201cAre Defenses for Graph Neural Networks Robust?,\u201d NeurIPS'2023.\n\n[4] S. Geisler, et al., \u201cRobustness of Graph Neural Networks at Scale,\u201d NeurIPS'2021."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission634/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570377046,
                "cdate": 1700570377046,
                "tmdate": 1700570377046,
                "mdate": 1700570377046,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "waBehcI44N",
            "forum": "EZ4VwxpzCK",
            "replyto": "EZ4VwxpzCK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission634/Reviewer_JnCK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission634/Reviewer_JnCK"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a bidirectional distillation method, called Prospect, between a GNN and MLP to mitigate the vulnerability of GNNs w.r.t. structure perturbations. The authors propose a custom learning schedule based on cosine annealing with warm restarts and prove some properties of their method using cSBMs. The authors demonstrate the effectiveness of their approach on homophilous and heterophilous datasets using a transfer poisoning attack."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Theoretical motivation of architecture on cSBMs\n1. Prospect consistently outperforms the baselines in the empirical evaluation\n1. Prospect is demonstrated to defend against adversarial transfer attacks on homophilic and heterophilic datasets\n1. Prospect can handle scalable GNN architectures like GraphSAGE"
                },
                "weaknesses": {
                    "value": "1. The paper is full of overstating claims like \"Theorem 1 implies the adversarial robustness of Prospect\". While Theorem 1 might imply robustness on cSBMs, it is merely conjectured that this robustness extrapolates to real-world graphs.\n1. Poisoning defense heavily uses the restricted threat model to solely perturb the graph structure and not the node features. It is one thing to follow the many other works in this simplifying assumption that focuses on the distinct characteristics of the robustness of GNNs; however, exploiting the clean node features seems highly questionable. The authors should discuss this and evaluate w.r.t., e.g., a joint attack on the graph structure and node features.\n1. The empirical evaluation is insufficient: The authors solely evaluate using a non-adaptive transfer attack. As pointed out previously, it is vital to assess neural networks with adaptive attacks [C, D] to get a proper estimate of the model's robustness.\n1. The authors claim scalability. Thus, they should compare to other works using large graphs [A, B]\n1. Just because an MLP is robust w.r.t. structure perturbations does not imply it is useful. Moreover, there might be an interaction between the GNN and MLP. If the authors make a claim about evasion (e.g. second last line of page 3), they should also verify that empirically.\n1. The presentation of the Theorems in the main part could be improved. It is unclear how the reader should deduce Theorem 1 from Proposition 1&2. Perhaps it would be better to move the propositions to the appendix and instead add a proof sketch to the main part.\n\n[A] Adversarial attack on large scale graph, Li et al., IEEE Transactions on Knowledge and Data Engineering 2021. \n[B] Robustness of Graph Neural Networks at Scale, Geisler et al., NeurIPS 2021\n[C] On Adaptive Attacks to Adversarial Example Defenses, Carlini et al., NeurIPS 2020\n[D] Are Defenses for Graph Neural Networks Robust?, Mujkanovic et al., NeurIPS 2022"
                },
                "questions": {
                    "value": "1. Can the authors please provide a full list of assumptions required for Proposition 1 & 2 as well as Theorem 1? For improved readability, the assumptions could be stated more explicitly and organized in the main text.\n1. Could the theory be extended to further data-generating distributions like Barabasi\u2013Albert? [E] \n\n[E] Community recovery in a preferential attachment graph, Hajek and Sankagiri, IEEE Transactions on Information Theory 2019."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission634/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission634/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission634/Reviewer_JnCK"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission634/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827867457,
            "cdate": 1698827867457,
            "tmdate": 1699635991111,
            "mdate": 1699635991111,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7wZoVrePg2",
                "forum": "EZ4VwxpzCK",
                "replyto": "waBehcI44N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission634/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission634/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (Part 1 Weakness1-5)"
                    },
                    "comment": {
                        "value": "Dear Reviewer JnCK,\n\nWe really appreciate your comments. We hope our point-to-point response can address your concerns.\n\n**Weakness 1**: Overstating claims.\n\nWe apologize that there were overstated claims in our original manuscript. We have re-examined the paper, eliminating overstated claims. We understand the importance of avoiding overstatements in academic writing, and your feedback has helped us improve the manuscript in this regard. Please let us know if there are any remaining issues with overclaiming and we will be happy to address them.\n\n**Weakness 2**: \u201cThe authors should discuss this and evaluate w.r.t., e.g., a joint attack on the graph structure and node features.\u201d\n\nPlease refer to the discussion section **Concerns about joint, injection and adaptive attacks** in the joint response.\n\n**Weakness 3**: \u201cThe empirical evaluation is insufficient.\u201d\n\nWe explained in the **Concerns about joint, injection and adaptive attacks/Adaptive attacks** section of joint response why the test with an adaptive attack is not performed at the first place. Due to the adaption issue pointed out here and urgent discussion deadline, we can only conduct part of black box unit tests provided by [A]. The experiment results can found in Section **Appended Experiments** of the joint response.\n\n**Weakness 4**: \u201cThe authors claim scalability. Thus, they should compare to other works using large graphs.\u201d\n\nAlthough we have made every effort to supplement all experiments, time and computational constraints prevent us from conducting experiments on data beyond the pre-made large-scale perturbation datasets. We are so sorry that we only directly adopt the pre-attacked arxiv dataset from [C] and did not make pre-attacked datasets with the method from [B]. Nonetheless, we believe it is necessary to briefly introduce [B] in our paper.\n\n**Weakness 5.1**: \u201cJust because an MLP is robust w.r.t. structure perturbations does not imply it is useful. Moreover, there might be an interaction between the GNN and MLP.\u201d\n\nYou did capture the key point of PROSPECT - the naive interactive behavior resulted from simply combining GNN and MLP for mutual distillation will cause the mutual distillation between these two types of heterogeneous models to fail, as stressed out in the **Contributions/The key to future mutual distillation** section of joint response.\n\nHowever, it should be noted that the performance gain of PROSPECT mainly comes from the fusion of the respective knowledge of GNN and MLP (regardless of attack or clean scenarios), so its theoretical performance has nothing to do with a specific optimization method like QACA. \n\nWe think that the knowledge conflict caused by the model heterogeneity locks the performance of PROSPECT, and QACA is just the key to this lock.\n\n**Weakness 5.2**: \u201cIf the authors make a claim about evasion (e.g. second last line of page 3), they should also verify that empirically.\u201d\n\nWe apologize for misleading you with the imprecise statement on page 3. In fact, we want to say that PROSPECT-MLP trained by the PROSPECT framework does not require graph structure for inference, so it is completely robust to (structure) evasion attacks. Considering that both Prospect-MLP and Prospect-SAGE can be deployed for inference, we attribute this property to the PROSPECT framework.\n\nBy the way, the performance of PROSPECT-MLP under evasion structure attacks is actually the same as that in the clean accuracy table, i.e., Table 2 of our paper."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission634/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570100117,
                "cdate": 1700570100117,
                "tmdate": 1700570100117,
                "mdate": 1700570100117,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CEATBR3HHP",
                "forum": "EZ4VwxpzCK",
                "replyto": "waBehcI44N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission634/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission634/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (Part 2 Weakness6, Question 1-2 )"
                    },
                    "comment": {
                        "value": "**Weakness 6**:\"The presentation of the Theorems in the main part could be improved. It is unclear how the reader should deduce Theorem 1 from Proposition 1&2. Perhaps it would be better to move the propositions to the appendix and instead add a proof sketch to the main part.\"\n\nWe are very sorry that we did not add proof schetch due to page limit. However, we are happy to add a proof sketch summarizing the high-level approach before formally proving Theorem 1, if you think this would further enhance the presentation.\n\nWe give the main proof schetch of Theorem 1 and point out how Prop.1 and Prop.2 relate to the proof below.\n\n1. Prop. 1 gives the distribution of processed features  $\\mathbf{h}$ obtained by feeding the CSBM raw data to a SAGE layer.\n2. Prop. 2 gives the ideal optimal decision boundary of the processed featuress $\\mathbf{h}$.\n3. Based on the gradients of the cross-entropy and KLD losses w.r.t. the SAGE layer parameters, we can get different SAGE weights that output different processed features with and without MLP-to-GNN distillation, denoted by $\\mathbf{h}^{kd}$ and $\\mathbf{h}$, respectively. In this process, the conclusion of Prop. 1 and some other auxiliary propositons in the appendix are required.\n4. Per Prop.2, the optimal decision boundaries of $\\mathbf{h}^{kd}$ and $\\mathbf{h}$ can be derived. Then we comapre the (expected) distances of $\\mathbf{h}_i^{kd}$ and $\\mathbf{h}_i$ from their respective optimal decision boundaries (i.e., the mis-classified probabilities).\n5. Prove that $\\mathbf{h}_i^{kd}$ is more far away from the optimal decision boundary of  $\\mathbf{h}^{kd}$. And thus the distillation is helpful to SAGE.\n\nAs cSBM may be unfamiliar to some readers, keeping the propositions provides context about the data characteristics right before Theorem 1. So perhaps we should keep them before Theorem 1 in the main text. How do you think?\n\n**Question 1**: \"Can the authors please provide a full list of assumptions required for Proposition 1 & 2 as well as Theorem 1?\"\n\nProp. 1 is the probabilistic description of the data generated by the CSBM model and the features obtained after oen SAGE layer. Prop. 2 gives the best decision boundary for the obtained features. Neither of these contains any assumptions per se.\n\nRegarding Theorem 1, the assumption about data is that the raw graph data are generated by a CSBM or aCSBM model. Another assumption implicit in the proof is that the learning rate is not too large and the number of nodes in the graph is not too small. As mentioned in the example on page 23, this is easily satisfied by almost all popular (and real-life) graph datasets and learning rate settings.\n\n**Question 2**: \"Could the theory be extended to further data-generating distributions like Barabasi\u2013Albert?\"\n\nWe examined the Barabasi\u2013Albert data-generating distribution described in [D]. The biggest difference from the graph data generator used in our paper is probably that it does not contain node features, whereas Theorem 1 illustrates the benefits of MLP-to-GNN distillation by examining the distance between node features and the optimal decision boundary. Therefore, it cannot be directly generalized to the data generator you mentioned. Nevertheless, we guess that treating the category probability distribution of each node as its node feature may yield some interesting results with the Barabasi\u2013Albert data-generating distribution.\n\nBy the way, the key to generalizing Theorem 1 to other data-generating distributions is that you need to know and derive the distribution of node features after SAGE convolution under \"different matrices of strictly positive affinities for vertices of different labels\" [D].\n\n[A] F. Mujkanovic, et al., \u201cAre Defenses for Graph Neural Networks Robust?,\u201d NeurIPS'2023.\n\n[B] Li et al., \"Adversarial attack on large scale graph,\"  IEEE TKDE, 2021.\n\n[C] Geisler et al., \"Robustness of Graph Neural Networks at Scale,\" NeurIPS'2021.\n\n[D] Community recovery in a preferential attachment graph, Hajek and Sankagiri, IEEE TIT, 2019."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission634/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700570236435,
                "cdate": 1700570236435,
                "tmdate": 1700570236435,
                "mdate": 1700570236435,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9FHgpbbGD3",
                "forum": "EZ4VwxpzCK",
                "replyto": "CEATBR3HHP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission634/Reviewer_JnCK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission634/Reviewer_JnCK"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up"
                    },
                    "comment": {
                        "value": "I thank the authors for the exhaustive response and multitude of conducted experiments.\n\nI understand that the Prospect-MLP inherits perfect robustness w.r.t. evasion perturbations. However, if only deploying a Prospect-MLP it would be particularly important to understand its generalization capabilities in an *inductive* setting since the clean test nodes are known during training in a transductive setting w/o poisoning attack.\n\n> Therefore, current attack methods such as Nettack that can simultaneously attack structures and features will also spend the vast majority of the budget (probably over 90%) on graph topology perturbation, even under the joint attack setting.\n\nThis finding might be true, e.g., for a GCN. However, this is no proof that this is also the most effective attack strategy on Prospect. Moreover, a GCN does not explicitly incorporate the assumption of clean node features to improve its robustness.\n\nIt should be noted that applying an adaptive attack might not be simply applying an existing code base to a new defense. The authors should spend considerable effort on breaking their defense to increase confidence in its robustness.\n\nIn summary, even though I think that Prospect could be a valuable contribution to the field, due to the missing adaptive evaluation and missing evaluation on feature perturbations, I tend to maintain my score."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission634/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732778528,
                "cdate": 1700732778528,
                "tmdate": 1700732778528,
                "mdate": 1700732778528,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]