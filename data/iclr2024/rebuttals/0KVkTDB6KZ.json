[
    {
        "title": "EFFL: Egalitarian Fairness in Federated Learning for Mitigating Matthew Effect"
    },
    {
        "review": {
            "id": "QjXHBWr4ve",
            "forum": "0KVkTDB6KZ",
            "replyto": "0KVkTDB6KZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission807/Reviewer_YyVq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission807/Reviewer_YyVq"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates fairness in FL on both accuracy and decision bias. The authors propose a three-stage optimization method to optimize the proposed constrained multi-objective optimization problem. Numerical results show the benefits of the algorithm in terms of fairness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Considering both accuracy and decision bias is promising and important to FL research.\n2. The proposed multi-objective optimization problem is straightforward."
                },
                "weaknesses": {
                    "value": "1. The proposed algorithm, as described in Algorithm 1 (line 6), necessitates unbiased gradients in each communication round. However, conventional federated learning algorithms typically employ local SGD, rendering the gradients hard to compute.\n2. Some comments regarding related works appear to be unfair. For instance, in [1], a certain degree of performance inequality is permitted within an acceptable threshold, defined as the \"fair area.\"\n3. The motivation for dividing the optimization method into three stages is not well-explained. It remains unclear whether the algorithm's performance would change if the first two stages were eliminated.\n4. The experiments conducted on real datasets involve only 2 or 11 clients. It is advisable to include cross-device scenarios with hundreds of clients for a more comprehensive analysis.\n\n[1] Pan Z, Wang S, Li C, Wang H, Tang X, Zhao J. Fedmdfg: Federated learning with multi-gradient descent and fair guidance. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 9364\u20139371, 2023."
                },
                "questions": {
                    "value": "1. Could the authors elaborate on how to implement the proposed algorithm when employing local SGD, a common approach in most federated learning papers?\n\n2. Could the authors provide more detailed explanations and perform ablation studies to justify the design choices for the three stages of the algorithm?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission807/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698759343503,
            "cdate": 1698759343503,
            "tmdate": 1699636007978,
            "mdate": 1699636007978,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ScjzRyYTD2",
                "forum": "0KVkTDB6KZ",
                "replyto": "QjXHBWr4ve",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission807/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission807/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YyVq"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's constructive comments.\n\n1.  **To the Weakness 1**\n\n    We'd like to clarify that our primary focus lies in introducing the crucial concept of achieving egalitarian fairness within FL to mitigate the Matthew effect, emphasizing social ethics. Our proposed training mechanism can be operated within a parallel SGD framework, where clients provide gradients. Similar to existing fair FL work [1], the reason we conduct EFFL in this context is the gradient directions can help to balance the trade-off between the model performance and fairness requirements better. Achieving egalitarian fairness in local SGD-based FL, where clients only provide model parameters, could be an interesting direction for our future work.\n\n2.  **To the Weakness 2**\n\n    We'd like to clarify that [1] proposed an FL mechanism to achieve the fairness that they defined as \"the uniformity of performance across clients\", which is consistent with our description in related work \"To achieve equal accuracy across clients\". However, in a non-ideal scenario, absolute fairness is a nontrivial, due to the trade-off between multiple objectives. Therefore, their fairness solution is within an acceptable region, rather than being strictly fair.\n\n3.  **To the Weakness 3**\n\n    We would like to explain the motivation for three-stage algorithm:\n\n    - The staged solution facilitates a better balance of trade-offs: as we discussed in Sec.1, the main challenges in our work come from three types of trade-offs. Each stage is designed to solve the problem under one type of trade-off;\n\n    - By dividing the problem into three stages, we only need to focus on satisfying partial constraints in each stage, which is helpful for more effectively entering a more strictly constrained decision space in the following stage;\n\n    - The staged solution provides a higher convergence speed. In each stage, we only need to solve a subproblem, which can reduce the problem's complexity and improve the solution's speed.\n\n    We conduct an ablation study to examine the effect of each stage separately and present the results in the following. The results confirm that only the three-stage approach can obtain the desired hypothesis in the decision space defined by fairness constraints.\n\n     | Dataset | Stage 1 Acc. | Stage 1 Bias | Stage 2  Acc. | Stage 2 Bias | Stage 3  Acc. | Stage 3 Bias | Stage 1+Stage 2+Stage 3 Acc. | Stage 1+Stage 2+Stage 3 Bias |\n    |---------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|----------------------------------|----------------------------------|\n    | Synthetic | 0.6519\u00b10.0169 | 0.1033\u00b10.0477 | 0.5740\u00b10.0661 | 0.0972\u00b10.0740 | **0.6840**\u00b10.0983 | 0.2483\u00b10.0670 | 0.6327\u00b1**0.0087** | **0.0801**\u00b1**0.0359** |\n    | Adult | **0.7778**\u00b10.0530 | 0.0167\u00b10.0212 | 0.7611\u00b10.0450 | 0.0069\u00b10.0094 | 0.7383\u00b10.0306 | 0.0109\u00b10.0102 | 0.7685\u00b1**0.0281** | **0.0036**\u00b1**0.0009** |\n    | eICU | 0.6488\u00b10.0223 | 0.0289\u00b10.0263 | 0.6503\u00b10.0207 | 0.0356\u00b10.0234 | 0.6446\u00b10.0226 | 0.0309\u00b10.0209 | **0.6530**\u00b1**0.0195** | **0.0209**\u00b1**0.0201** |\n\n4.  **To the Weakness 4**\n\n    We'd like to elaborate on the scalability of EFFL under more clients as follows: Our EFFL training objective (Eq. (6)) explicitly and separately considers the decision bias and egalitarian fairness of each client. Therefore, our method can ensure fair performance when faced with a larger number of clients. As the number of clients increases, the decision space defined by decision bias and egalitarian fairness may become more constrained, which could result in a decrease in accuracy performance due to the trade-off between accuracy and fairness constraints. However, our method provides a Pareto optimal solution that balances the trade-offs.\n\n5.  **To the Question 1**\n\n    Same to response 1.\n\n6.  **To the Question 2**\n\n    Same to response 3."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700134462691,
                "cdate": 1700134462691,
                "tmdate": 1700134462691,
                "mdate": 1700134462691,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3jSQXCqCtC",
                "forum": "0KVkTDB6KZ",
                "replyto": "QjXHBWr4ve",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission807/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission807/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YyVq"
                    },
                    "comment": {
                        "value": "**To the Weakness 4**\nFollowing your suggestion, we have conducted additional experiments under the ACSPublicCoverage dataset [2], a SOTA  and complex tabular dataset currently available for fairness research. The dataset can be naturally partitioned into $51$ clients according to the State where the data was collected. The dataset was collected in the year 2022, and the results of the experiment are summarized as follows. Our method achieves superior performance in reducing decision bias and ensuring egalitarian fairness among clients. Ditto has the best accuracy but at the cost of high decision bias and significant performance disparity among clients, which may exacerbate the Matthew effect and be undesirable from a social welfare perspective.\n   \n   | Method | Local Acc. Avg. | Local Acc. Std. | Local Bias Avg. | Local Bias Std. |\n|--------|-----------------|-----------------|-----------------|-----------------|\n| FedAvg | 0.5778          | 0.0352          | 0.0260          | 0.0242          |\n| q-FFL  | 0.5888          | 0.0406          | 0.0236          | 0.0202          |\n| Ditto  | **0.6578**      | 0.0584          | 0.0307 \u2191        | 0.0328          |\n| FedMDFG| 0.5978          | 0.0453          | 0.0215          | 0.0187          |\n| FedAvg+FairBatch | 0.5972 | 0.0454          | 0.0214          | 0.0188          |\n| FedAvg+FairReg   | 0.5964 | 0.0453          | 0.0202          | 0.0194          |\n| FCFL   | 0.6085          | 0.0453          | 0.0215          | 0.0187          |\n| EFFL   | 0.6037          | **0.0284**      | **0.0147**      | **0.0128**      |\n\n[2] Ding, F., Hardt, M., Miller, J., & Schmidt, L. (2021). Retiring adult: New datasets for fair machine learning."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700449839419,
                "cdate": 1700449839419,
                "tmdate": 1700460585331,
                "mdate": 1700460585331,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "j80sboFt7r",
            "forum": "0KVkTDB6KZ",
            "replyto": "0KVkTDB6KZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission807/Reviewer_Raor"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission807/Reviewer_Raor"
            ],
            "content": {
                "summary": {
                    "value": "This paper examines the problem of fairness in a federated learning setting through the lens of MCMOO. The goal is to reduce global loss while satisfying several constraints: (1) Roughly equal loss/accuracy for each individual client; (2) Fairness at each individual client, i.e., low local unfairness; and (3) Local unfairness at each client is also roughly equal to global which reduces to \"equal\" unfairness at each client, i.e., equal local unfairness. The MCMOO problem is addressed by breaking it down into a three-stage optimization process where some of the stages are constrained optimizations. The approach to solving MCMOO has several interesting ideas that involve cleverly controlling the direction of the gradient such that certain other metrics do not increase.\nExperimental results are provided on a synthetic dataset, Adult (2 clients), and eICU (11 clients)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Some of these points have already been discussed in the Summary.\nFormulating the problem as an MCMOO and then attempting to solve it as a three-stage process is a good contribution. The approach to solving MCMOO has several interesting concepts that involve cleverly controlling the direction of the gradient such that certain other metrics do not increase.\nThey have also provided experimental results with several baselines."
                },
                "weaknesses": {
                    "value": "1. Such MCMOO problems are known to be quite difficult and unstable sometimes. Could the authors comment on the stability of their strategy? \n\n2. One weakness is that there is no theoretical guarantee on why this algorithm will converge, and under what conditions would it diverge. Is it possible to formalize the intuitions of Fig. 2 into a theorem? Or, at least discuss some scenarios where it would diverge.\nFor instance, there are also impossibility results on group fairness in federated learning in: https://arxiv.org/abs/2307.11333 \n\n3. The word Egalitarian is a bit confusing here. I would think Egalitarian would mean equal accuracy/loss across clients. But, in addition to that, there is also a group fairness criterion with local and global fairness constraints. Using the word \"group\" fairness at places would make it more clear.\n\n4. For what class of fairness metrics would the algorithm work? Please comment.\n\n5. Could the authors also elaborate on the communication cost when there is a maximum since to obtain the maximum also one needs all the values? How is communication complexity improved?\n\nAdditional Limitations:\nThere is no discussion on the privacy of this approach in a federated context."
                },
                "questions": {
                    "value": "Already included several questions along with the weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission807/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission807/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission807/Reviewer_Raor"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission807/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698802341849,
            "cdate": 1698802341849,
            "tmdate": 1699636007898,
            "mdate": 1699636007898,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wzFhwLSxGd",
                "forum": "0KVkTDB6KZ",
                "replyto": "j80sboFt7r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission807/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission807/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Raor"
                    },
                    "comment": {
                        "value": "We appreciate  the reviewer for acknowledging the contributions of our work and valuable comments.\n\n1.  **To the Weakness 1**\n\n    Given gradients from the clients have an error $\\tilde G (\\theta^t)=G (\\theta^t)+\\mathbf{e} ^t$, then $\\left \\| \\tilde{\\mathbf{\\alpha} } -\\mathbf{\\alpha} \\right \\| _2\\le \\mathcal{O}(max_t\\left \\| e^t \\right \\|_2 )$  in the convex optimization of Eq. (14) . Following that, the error of the model parameter $\\theta^{t+1}$ is bounded by:\n$\\left \\| \\tilde{ \\theta} ^{t+1}-\\theta^{t+1} \\right \\| _2 =\\eta\\left \\| \\tilde{\\alpha}  ^{T}{G}+\\tilde{\\alpha}  ^{T}e^t-{\\alpha}  ^{T}{G}\\right \\| _2\n  \\le \\eta \\left \\| G \\right \\| _2\\mathcal{O} \\left(\\max _{t}\\left\\|e^{t}\\right\\| _{2}\\right)+\\eta \\mathcal{O}\\left(\\max _{t}\\left\\|e^{t}\\right\\| _{2}\\right)\\left \\|e^t  \\right \\| _2+\\left \\|a  \\right \\|  _2\\left \\|e^t  \\right \\| _2$\n\n   Given that $\\eta$, $\\left \\| G \\right \\| _2$, and $\\left \\|a  \\right \\|  _2$ are bounded, $ \\left \\| \\tilde{ \\theta} ^{t+1}-\\theta^{t+1} \\right \\| _2\\le \\mathcal{O} \\left(\\max _{t}\\left\\|e^{t}\\right\\|\\_{2}\\right) $ is also bounded in our algorithm, the model's sensitivity to input errors satisfies $\\mathcal{O} \\left(\\max _ {t}\\left|e^{t}\\right| _{2}\\right)$-stability.\n\n2.  **To the Weakness 2**\n\n       **Proposition.** For $N$ optimization objectives $l_1(\\theta^t),...,l_N(\\theta^t)  $ and the  model parameter updating rule: $\\theta^{t+1}=\\theta^{t}+\\eta d$; if $d^T\\nabla_{\\theta}l_i\\le0$, there exists $\\eta_0$ such that for $\\forall \\eta \\in [0,\\eta_0]$, the objectives $ l_1(\\theta^t),...,l_N(\\theta^t) $ will not increase, and the iterations toward convergent.\n\n    **Proof.** Performing Taylor expansion at $\\theta^t$, we have $l_i(\\theta^{t+1})-l_i(\\theta^{t})=\\eta d^T\\nabla_{\\theta} l_i (\\theta^t)+R_1(\\theta^{t}+\\eta d)$, where $R_1$ is a higher-order infinitesimal of $\\eta $, denoted by $o(\\eta )$ ($ R_1(\\theta^{t}+\\eta d)=\\frac{\\nabla ^2_{\\theta}l(\\theta^t)}{2!}\\eta^2 d^2=o(\\eta)$). Therefore, we have $l _i(\\theta^{t+1})-l _i(\\theta^{t})=\\eta d^T\\nabla _{\\theta} l_i (\\theta^t)+o(\\eta)$, since $o(\\eta)$ approaches $0$ faster than $\\eta$, when $d^T\\nabla _{\\theta} l _i (\\theta^t)\\le 0$, there exists $\\eta_0>0$ such that for $\\forall \\eta \\in [0,\\eta_0]$, $l_i(\\theta^{t+1})-l_i(\\theta^{t})\\le 0$.\n\n    Based on the proposition, the parameters update in the proposed algorithm is towards a gradient descent direction $d$ that satisfies $d^T\\nabla_{\\theta}  g_k(h)\\le 0$, where $g_k$ is the different optimization objective at different stages. Thus, the proposed algorithm is convergent.\n\n3.  **To the Weakness 3**\n\n    We'd like to clarify the **egalitarian fairness** and **group fairness**. Group fairness focuses on the performance disparity among protected groups, while egalitarian fairness focuses on performance (**local accuracy** and **local decision bias**) disparity among clients. **Egalitarian fairness of the local decision bias** requires equal bias levels across all clients, different from local group fairness, targeting model bias within a **single** local client. Egalitarian fairness may resemble group fairness when a client exclusively presents one protected group. However, this is a rare case. Hence, we propose EFFL for broader scenarios.\n\n4.  **To the Weakness 4**\n\n    The main requirement for the fairness metrics is differentiable, e.g., we compute the TPSD gradients on its differentiable surrogates,\n\n    $TPSD=\\sqrt{\\frac{ {\\sum _{i=1}^{M}}\\left ( \\mathbb{E} _{(X=\\mathbf{x} \\mid S=i,Y=1)}\\left[h _{\\boldsymbol{\\theta}}(\\mathbf{x})\\right]-\\frac{\\sum _{i=1}^{M}\\mathbb{E} _{(X=\\mathbf{x} \\mid S=i,Y=1)}\\left[h _{\\boldsymbol{\\theta}}(\\mathbf{x})\\right]}{M} \\right ) ^2}{M} } $.\n\n    Other metrics, such as DP, EO, can be made differentiable in a similar way and applied to our algorithm. We choose TPSD/APSD because they are more general and can handle non-binary sensitive attributes. Moreover, our algorithm can be applied to non-binary target variables, e.g., by replacing the BCELoss with a multi-class loss function $loss=-\\sum_{i=0}^{C-1} y_{i} \\log \\left(p_{i}\\right)$, and replacing the decision bias metric with\n    $TPSD =max_{y\\in[|Y|]}\\sqrt{\\frac{\\sum_{i=1}^{M}\\left (\\operatorname{Pr}\\left (\\hat{Y}\\_k=y \\mid A_k=i, Y_k=y\\right )-\\mu\\right )^{2}}{M}} $.\n\n5.  **To the Weakness 5**\n\n    Similar to prior fair FL methods, our approach involves increased parameter communication compared to non-fair FL. Each client sends gradients of the local loss function and local decision bias, along with their evaluations, to the server. This roughly doubles communication compared to non-fair FL. However, server-to-client communication remains unchanged, limited to broadcasting model parameters. Despite the heightened communication, our method achieves Pareto optimal model performance under egalitarian fairness. The performance distribution variance, mean, maximum, etc., are computed and used internally within the server without extra communication costs."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700133368941,
                "cdate": 1700133368941,
                "tmdate": 1700136547106,
                "mdate": 1700136547106,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sMFMMciBNg",
            "forum": "0KVkTDB6KZ",
            "replyto": "0KVkTDB6KZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission807/Reviewer_hFqL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission807/Reviewer_hFqL"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies fairness in federated learning with. Specifically, it considers two types of fairness. The first one is the Matthew effect, which considers the performance across different clients; and the second one is the decision bias, which is a class of commonly studied fairness definitions like accuracy parity or equal opportunity across different demographic groups. The authors formulate the problem as a multi-constrained multi-objective optimization problem and propose a 3-stage solution to gradually search the optimal hypothesis in a more constrained hypothesis space. Experiments on both synthetic data and real-world data demonstrate the effectiveness of the proposed method over baseline methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "S1. Fair federated learning is a practical problem, and it is good to \n\nS2. The 3-stage solution is interesting.\n\nS3. Experimental results show the effectiveness on the tested datasets over baseline methods."
                },
                "weaknesses": {
                    "value": "W1. The paper needs stronger motivation to support the need of two fairness considerations. Right now it feels more like two fairness considerations are both important, so we will consider them simultaneously. Is it possible to provide some real-world examples or use cases?\n\nW2. Fig. 1 (a) is too hypothetical to support the claim that poor model could impair data generation capabilities and worsening the performance gap over time.\n\nW3. Is there any trade-off or correlation between the Matthew effect and APSD/TPSD? For example, if we only improve the Matthew effect, the accuracy of poorer model might increase, and it may further help reduce APSD/TPSD. Is it possible to show empirical analysis about it (e.g., ablation study)?\n\nW4. In Definition 1, $f_k(h)$ is constrained to be no larger than $\\epsilon_b$, and $\\{f_k(h) - {\\bar f}(h)\\}$ is also constrained to be less than $\\epsilon_{vb}$. Is it possible that enforcing these two set of constraints could hurt a fair local client? Consider a case where the global model is almost perfectly fair, $f_k(h) \\approx 0$, but violate $f_k(h) - {\\bar f}(h) \\leq \\epsilon_{vb}$. Then it might be possible to increase $f_k(h)$ (i.e., making it more biased) to enforce $f_k(h) - {\\bar f}(h) \\leq \\epsilon_{vb}$.\n\nW5. In several places, the authors mention that existing works cannot narrow the gap between worse-performing clients and better-performing clients. I don't understand why the claim is true. For example, minimax-based solution will always minimize the worst-performing clients so naturally it could reduce the disparity among different clients' performance. Is it possible to elaborate the reason more clearly?\n\nW6. It feels a bit conflicting that the authors say minimax cannot narrow the gap among clients' performance but still solve a minimax problem in stages 2 and 3.\n\nW7. Is there any analysis to show the convergence of the proposed method? The authors simply claim it can obtain the convergent solution.\n\n--- Post Rebuttal ---\nI appreciate authors' efforts to address my cocerns, and I have updated my score."
                },
                "questions": {
                    "value": "Please see weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission807/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission807/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission807/Reviewer_hFqL"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission807/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698988060505,
            "cdate": 1698988060505,
            "tmdate": 1700757721392,
            "mdate": 1700757721392,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eXTu5FFXQn",
                "forum": "0KVkTDB6KZ",
                "replyto": "sMFMMciBNg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission807/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission807/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hFqL"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's constructive comments. Our responses to the comments are listed as follows:\n\n1.  **To the W1**\n\n    We'd like to explain that the motivation for two fairness is that clients may contribute varying amounts due to inherent resource inequalities. Ignoring such inequities can result in the accumulation of resource inequities due to the Matthew effect. This issue is particularly noticeable when FL is applied in social welfare scenarios, e.g., collaborative training of a disease diagnosis model among hospitals. Hospitals with lower data resources will obtain poorer models. Lower accuracy and trustworthy will affect their subsequent diagnosis, leading to continuous resource inequality and the deterioration of social welfare.\n\n2.  **To the W2**\n\n    We thank the reviewer for the kind suggestion, and we will revise the figure better.\n\n3.  **To the W3**\n\n    As we explained in Sec. 1, there is a trade-off between the Matthew effect and APSD/TPSD. Specifically, it manifests in the following way:\n\n    - Equal accuracy distribution is achieved by improving the model's fit towards disadvantaged clients rather than advantaged clients. As reducing the model's fit on a dataset can mitigate decision bias [1], APSD/TPSD may decrease on advantaged clients but increase on disadvantaged clients.\n    - Equal decision bias distribution and minimizing APSD/TPSD have a more intuitive trade-off; for clients with inherently larger APSD/TPSD, APSD/TPSD decrease, while for clients with inherently smaller APSD/TPSD, APSD/TPSD increase.\n\n    Therefore, the trade-off we need to address is how to reduce the Matthew effect by egalitarian fairness while ensuring that APSD/TPSD increasing on certain clients still remains within an acceptable threshold.\n\n    We conduct an ablation experiment on Adult dataset to verify the trade-off between egalitarian fairness and APSD/TPSD.\n\n    | Considered Fairness         | Client1 Acc. | Client1 TPSD | Client2 Acc. | Client2 TPSD |\n    | --------------------------- | ------------ | ------------ | ------------ | ------------ |\n    | None fairness               | 0.7348       | 0.0393       | 0.8186       | 0.0262       |\n    | Egalitarian fairness        | 0.7502       | 0.0242       | 0.8071       | 0.0399 \u2191     |\n    | Egalitarian fairness + TPSD | 0.7403       | 0.0045       | 0.7967       | 0.0026       |\n\n    [1]Chouldechova, A., and Roth, A. (2020). A snapshot of the frontiers of fairness in machine learning.\n\n4.  **To the W4**\n\n    We'd like to clarify that it's acceptable to increase a client's $f_k$ from $0$ to non-zero, as long as it does not violate the decision bias threshold $\\epsilon_b$, in order to better achieve the egalitarian fairness objective and avoid the Matthew effect. Only pursuing the optimal performance of a single client $f_k\\rightarrow0$, may result in poor performance of the model on other clients, which is undesirable from the perspective of social welfare and ethics.\n\n5.  **To the W5**\n\n    The existing methods, e.g. min-max-based, only focus on improving the performance of the worst-performing client and may harm others; thus, we consider **egalitarian fairness**, which aims to achieve a more equal distribution of performance among all clients simultaneously.\n\n6.  **To the W6**\n\n    We avoid the drawback of the original min-max by introducing non-positive gradient constraints on non-worst clients. This makes the modified min-max equivalent to multi-objective optimization, improving the worst client without harm to other clients.\n\n7.  **To the W7**\n\n    We'd like to clarify the convergence of the proposed algorithm as follows:\n\n    **Proposition.** For $N$ optimization objectives $l_1(\\theta^t),...,l_N(\\theta^t)  $ and the  model parameter updating rule: $\\theta^{t+1}=\\theta^{t}+\\eta d$; if $d^T\\nabla_{\\theta}l_i\\le0$, there exists $\\eta_0$ such that for $\\forall \\eta \\in [0,\\eta_0]$, the objectives $ l_1(\\theta^t),...,l_N(\\theta^t) $ will not increase, and the iterations toward convergent.\n\n    **Proof.** Performing Taylor expansion at $\\theta^t$, we have $l_i(\\theta^{t+1})-l_i(\\theta^{t})=\\eta d^T\\nabla_{\\theta} l_i (\\theta^t)+R_1(\\theta^{t}+\\eta d)$, where $R _1$ is a higher-order infinitesimal of $\\eta $, denoted by $o(\\eta )$ ($ R_1(\\theta^{t}+\\eta d)=\\frac{\\nabla ^2 _{\\theta}l(\\theta^t)}{2!}\\eta^2 d^2=o(\\eta)$). Therefore, we have $l _i(\\theta^{t+1})-l _i(\\theta^{t})=\\eta d^T\\nabla  _{\\theta} l_i (\\theta^t)+o(\\eta)$, since $o(\\eta)$ approaches $0$ faster than $\\eta$, when $d^T\\nabla _{\\theta} l_i (\\theta^t)\\le 0$, there exists $\\eta_0>0$ such that for $\\forall \\eta \\in [0,\\eta_0]$, $l_i(\\theta^{t+1})-l_i(\\theta^{t})\\le 0$.\n\n    Based on the proposition, the parameters update in the proposed algorithm is towards a gradient descent direction $d$ that satisfies $d^T\\nabla_{\\theta}  g_k(h)\\le 0$, where $g_k$ is the different optimization objective at different stages. Thus, the proposed algorithm is convergent."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700132120568,
                "cdate": 1700132120568,
                "tmdate": 1700136272086,
                "mdate": 1700136272086,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2cT579j0zK",
                "forum": "0KVkTDB6KZ",
                "replyto": "eXTu5FFXQn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission807/Reviewer_hFqL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission807/Reviewer_hFqL"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "Thank you for the response. Most of my concerns are addressed, except for W2, because no revised version with updated figure is uploaded yet. For W2, it would be better to see an updated version of this manuscript. For other concerns, it would be good to incorporate such discussions in the revised version. I will keep my current score but am open to increase score if updated version is available."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716934200,
                "cdate": 1700716934200,
                "tmdate": 1700716934200,
                "mdate": 1700716934200,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PoJknKAN5s",
                "forum": "0KVkTDB6KZ",
                "replyto": "sMFMMciBNg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission807/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission807/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hFqL"
                    },
                    "comment": {
                        "value": "Thank you for your reply. We have submitted a revised version. Regarding W2, the figure was originally designed to illustrate how the Matthew effect influences the cycle of \u201cdata generation \u2192 model training \u2192 model deployment\u2192 data generation\u201d. However, to address W1, we have added an example of collaborative training of a diagnostic model  in the Sec. 1, which can help the readers better understand how the Matthew effect affects model training. Therefore, the original figure was no longer necessary, and we have removed it."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737262464,
                "cdate": 1700737262464,
                "tmdate": 1700739027440,
                "mdate": 1700739027440,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TeG4nwqPlm",
            "forum": "0KVkTDB6KZ",
            "replyto": "0KVkTDB6KZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission807/Reviewer_ta1u"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission807/Reviewer_ta1u"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes EFFL a client fairness approach that aims to mitigate the Mathew effect by producing a Pareto optimal model with equal decision bias and accuracy across the participating clients. EFFL problem is formulated as a MOOP with decision bias and fairness constraints. The authors provide a 3-step algorithm for solving this objective and perform experiments that showcase superior performance compared to various other baselines on a synthetic dataset and 2 real datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Addressing the combinatorial issue of achieving equitable performance across clients in federated learning holds significant importance.\n\n* The authors compared with multiple baselines and considered settings with adversarial attacks."
                },
                "weaknesses": {
                    "value": "* The proposed algorithm is complicated and lacks formal convergence guarantees, so it's hard to understand and confidence in the algorithm's behaviour and optimality of the produced global model. \n* It would be beneficial to have some guidance or a more systematic approach to determine the values of $\\epsilon_b$, $\\epsilon_{ub}$ and $\\epsilon_{ul}$.\n* The proposed problem and algorithms assume a binary target variable which is rather restrictive.\n* The algorithm requires full client participation and allows for a single local epoch, increasing the communication overhead (as also shown in Figure 3) and restricting the applicability of EFFL in large-scale FL applications. \n * The experiments were conducted on only 2 real datasets and considered very few clients (maximum 11 for the eICU dataset)."
                },
                "questions": {
                    "value": "* How to determine the values of $\\epsilon_b$ $\\epsilon_{ub}$ and $\\epsilon_{ul}$? There is some study on the effects of Appendix B.4.2, but I am unsure whether these results can be generalized given it was only examined for a single dataset that uses 2 clients.\n * Can the spaces required by the EFFL algorithm be infeasible? (e.g., the decision space $\\mathcal{H}_B\\cap\\mathcal{H}_E$ defined by the fairness and bias constraints)\n * Are there any assumptions on the smoothness and convexity of the hypothesis class and the local loss functions to get the final objective? \n* How well does this approach scale with a large number of clients? It would be interesting to see what EFFL's performance on the fe.g., on ACS Employment dataset, which naturally exhibits non-iid characteristics, being partitioned into 51 regions (that can act as separate clients).\n\n\n**Minor:**\n* what does the following sentence mean in the context of FL: \"Previous work overlooks the trade-offs in achieving equality from a social welfare perspective and local optimality from an individual beneficial perspective\"?\n* [1] i missing from related work \n\n[1]  Hu, S., Wu, Z. S., and Smith, V. (2022). Fair federated learning via bounded group loss."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission807/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission807/Reviewer_ta1u",
                        "ICLR.cc/2024/Conference/Submission807/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission807/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699561150503,
            "cdate": 1699561150503,
            "tmdate": 1700737812339,
            "mdate": 1700737812339,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4doKfe87ZK",
                "forum": "0KVkTDB6KZ",
                "replyto": "TeG4nwqPlm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission807/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission807/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ta1u"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's constructive comments. Our responses to the comments are listed as follows:\n\n1.  **To the Weakness 1**\n\n    We'd like to clarify the convergence of the proposed algorithm as follows:\n\n    **Proposition.** For $N$ optimization objectives $l_1(\\theta^t),...,l_N(\\theta^t)  $ and the  model parameter updating rule: $\\theta^{t+1}=\\theta^{t}+\\eta d$; if $d^T\\nabla_{\\theta}l_i\\le0$, there exists $\\eta_0$ such that for $\\forall \\eta \\in [0,\\eta_0]$, the objectives $ l_1(\\theta^t),...,l_N(\\theta^t) $ will not increase, and the iterations toward convergent.\n\n    **Proof.** Performing Taylor expansion at $\\theta^t$, we have $l_i(\\theta^{t+1})-l_i(\\theta^{t})=\\eta d^T\\nabla_{\\theta} l_i (\\theta^t)+R_1(\\theta^{t}+\\eta d)$, where $R_1$ is a higher-order infinitesimal of $\\eta $, denoted by $o(\\eta )$ ($ R_1(\\theta^{t}+\\eta d)=\\frac{\\nabla ^2_{\\theta}l(\\theta^t)}{2!}\\eta^2 d^2=o(\\eta)$). Therefore, we have $l_i(\\theta^{t+1})-l_i(\\theta^{t})=\\eta d^T\\nabla_{\\theta} l_i (\\theta^t)+o(\\eta)$, since $o(\\eta)$ approaches $0$ faster than $\\eta$, when $d^T\\nabla_{\\theta} l_i (\\theta^t)\\le 0$, there exists $\\eta_0>0$ such that for $\\forall \\eta \\in [0,\\eta_0]$, $l_i(\\theta^{t+1})-l_i(\\theta^{t})\\le 0$.\n\n    Based on the proposition, the parameters update in the proposed algorithm is towards a gradient descent direction $d$ that satisfies $d^T\\nabla_{\\theta}  g_k(h)\\le 0$, where $g_k$ is the different optimization objective at different stages. Thus, the proposed algorithm is convergent.\n\n2.  **To the Weakness 2**\n\n    $\\epsilon _b$,etc. are assigned based on the actual task requirements. We add the experiment under the eICU dataset (11 clients) as follows, to verify the scalability of the fairness budgets in controlling the fairness.\n    |  $\\epsilon_b$ | 0.01 | 0.02 | 0.05 | 0.1 |\n    | --- | --- | --- | --- | --- |\n    | Avg. Acc. | 0.627 | 0.653 | 0.654 | **0.659** |\n    | Avg. Bias | **0.013** | 0.020 | 0.030 | 0.041 |\n\n    | $\\epsilon_{vl}$ | 0.01      | 0.02  | 0.05  | 0.1   |\n    | --------------- | --------- | ----- | ----- | ----- |\n    | Std. Acc.       | **0.014** | 0.019 | 0.023 | 0.025 |\n\n    | $\\epsilon_{vb}$ | 0.01      | 0.02  | 0.05  | 0.1   |\n    | --------------- | --------- | ----- | ----- | ----- |\n    | Std. Bias       | **0.011** | 0.020 | 0.032 | 0.036 |\n\n3.  **To the Weakness 3**\n\n    The proposed algorithm can be applied to non-binary target variables, by replacing the BCELoss with a multi-class loss function, i.e., $loss=-\\sum_{i=0}^{C-1} y_{i} \\log \\left(p_{i}\\right)$, and replacing the decision bias metric with\n    $TPSD =max _{y\\in[|Y|]}\\sqrt{\\frac{\\sum _{i=1}^{M}\\left (\\operatorname{Pr}\\left (\\hat{Y} _k=y \\mid A _k=i, Y _k=y\\right )-\\mu\\right )^{2}}{M}}$.\n\n4.  **To the Weakness 4**\n\n    Similar to prior works in fair FL, our algorithm involves additional parameters communication, which approximately doubles the communication cost compared to non-fair FL. The benefit of the additional communication cost is the Pareto optimal model performance under the egalitarian fairness constraints.\n\n5.  **To the Weakness 5**\n\n    We'd like to clarify the scalability as the EFFL problem explicitly and separately considers the loss and fairness of each client. Therefore, our method is scalable when dealing with a larger number of clients. We appreciate your suggestion of ACS Employment dataset, and we'd like to incorporate it in future work.\n\n6.  **To the Question 1**\n\n    Same to response 2.\n\n7.  **To the Question 2**\n\n    We'd like to clarify that $\\mathcal{H} _B\\cap \\mathcal{H} _E\\neq \\emptyset $, e.g., $\\mathcal{H} _B\\cap \\mathcal{H} _E$ contains hypothesis $h\\in \\mathcal{H} _E$ with $\\bar f(h)\\le\\epsilon _b-\\epsilon _{vb}$. Since $h\\in \\mathcal{H} _E$, $h$ satisfies $| f _k \\left ( h  \\right ) - \\bar{f} \\left ( h \\right )   | _{k=1}^N\\le \\epsilon _{vb}$, and thus, it can be readily deduced that $h$ also satisfies $f _k \\left ( h \\right ) _{k=1}^N \\le \\epsilon _b$ and is also contained within $\\mathcal{H} _B$, which requires $f _k \\left ( h \\right ) _{k=1}^N \\le \\epsilon  _b$.\n\n8.  **To the Question 3**\n\n    We'd like to clarify  EFFL can be applied to both convex and non-convex local loss functions.\n\n9.  **To the Question 4**\n\n    Same to response 5.\n\n10. **To the Minor 1**\n\n    Individual beneficial perspective means the model provides maximized individual performance but may harm  others. Social welfare perspective means limiting performance in advantaged clients to improve disadvantaged ones, which opposes individual beneficial perspective.\n\n11. **To the Minor 2**\n\n    We'd like to add [1] to related work, which focuses on group-level fairness among protected groups. Our work focuses on client-level fairness, aiming at more uniform performance distribution among clients."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700131371048,
                "cdate": 1700131371048,
                "tmdate": 1700136243019,
                "mdate": 1700136243019,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NiPnvPnyL2",
                "forum": "0KVkTDB6KZ",
                "replyto": "TeG4nwqPlm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission807/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission807/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ta1u"
                    },
                    "comment": {
                        "value": "**To the Weakness 5**\nFollowing your suggestion, we conducted experiments under the ACSPublicCoverage dataset, where $51$ clients are divided by the State where their data originated.  The dataset was collected in the year 2022, and the results of the experiment are summarized as follows. Our method achieves superior performance in reducing decision bias and ensuring egalitarian fairness among clients. Ditto has the best accuracy but at the cost of high decision bias and significant performance disparity among clients, which may exacerbate the Matthew effect and be undesirable from a social welfare perspective.\n   \n   | Method | Local Acc. Avg. | Local Acc. Std. | Local Bias Avg. | Local Bias Std. |\n|--------|-----------------|-----------------|-----------------|-----------------|\n| FedAvg | 0.5778          | 0.0352          | 0.0260          | 0.0242          |\n| q-FFL  | 0.5888          | 0.0406          | 0.0236          | 0.0202          |\n| Ditto  | **0.6578**      | 0.0584          | 0.0307 \u2191       | 0.0328          |\n| FedMDFG| 0.5978          | 0.0453          | 0.0215          | 0.0187          |\n| FedAvg+FairBatch | 0.5972 | 0.0454          | 0.0214          | 0.0188          |\n| FedAvg+FairReg   | 0.5964 | 0.0453          | 0.0202          | 0.0194          |\n| FCFL   | 0.6085          | 0.0453          | 0.0215          | 0.0187          |\n| EFFL   | 0.6037          | **0.0284**      | **0.0147**      | **0.0128**      |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700449025388,
                "cdate": 1700449025388,
                "tmdate": 1700460614789,
                "mdate": 1700460614789,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BYqhOkDTaZ",
                "forum": "0KVkTDB6KZ",
                "replyto": "NiPnvPnyL2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission807/Reviewer_ta1u"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission807/Reviewer_ta1u"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their responses. I have revised my score accordingly."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737796230,
                "cdate": 1700737796230,
                "tmdate": 1700737796230,
                "mdate": 1700737796230,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]