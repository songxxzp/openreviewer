[
    {
        "title": "Debiasing Algorithm through Model Adaptation"
    },
    {
        "review": {
            "id": "I8sfRESaRu",
            "forum": "XIZEFyVGC9",
            "replyto": "XIZEFyVGC9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1695/Reviewer_FcGW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1695/Reviewer_FcGW"
            ],
            "content": {
                "summary": {
                    "value": "This work studies the the issues of biases and stereotypes in the large language models, and propose methods to (1) identify the specific components of LLMs that is responsible for these issues; and (2) resolve these issues with a linear projection on the feed-forward components of LLMs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The research question is interesting and critical to LLM research and application\n- The high-level ideas of the proposed techniques (on both identification and debiasing) make sense."
                },
                "weaknesses": {
                    "value": "I had a difficult time understanding this paper because of the following writing issues. \n\n- First of all, certain concepts are mentioned in the paper. However, they are either inconsistent or not explicitly defined. For example,\n    - Stereotypical keys vs. stereotyped keys\n    - Gender value vs. gendered value\n    - Also, what is grammatical gender?\n- About Equation 2, I have several questions\n    - What is z? Why do we need a $\\ell_2$ item on it?\n    - Missing a \u201c)\u201d somewhere in equation 2\n    - What is $P(o\u2019|X\u2019)$? The first two items in equations look similar to variational inference. In that case, $P(o\u2019|X\u2019)$ should be something similar to a prior distribution. However, I could not find its definition.\n- What is gender values metrics?  What is the relation between these V metrics and U?\n- Why $P$ in equation 5 is defined in that way, and how should we use it? I think it was explained in the paragraph right after equation 5, but I am not sure I understand it.\n- Figure 3 (b) seems to indicate that to reduce the bias, the cost is to get a much worse language model (based on the perplexity score), which is not exactly claimed in the paper.\n\nIn addition to my clarification question, one concern is about the technical novelty specifically the causal tracing idea is from prior work, and the linear project seems to be a straightforward idea in debiasing literature, so I am wondering what is the technical novelty of this work."
                },
                "questions": {
                    "value": "Please refer to the clarification questions mentioned in the previous section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1695/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698781773830,
            "cdate": 1698781773830,
            "tmdate": 1699636097919,
            "mdate": 1699636097919,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4YVG079yga",
                "forum": "XIZEFyVGC9",
                "replyto": "I8sfRESaRu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1695/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1695/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed comments and for recognizing the importance of debiasing in LLM research. We organize the rebuttal in three parts to comprehensively answer the raised concerns related to: 1) LM performance, 2) clarifications about concepts and equations in section 4;  3) the novelty of our approach.\n\n## Firstly, we want to address the concern about LM performance that might be an effect of misunderstanding our ablation setting:\n> Figure 3 (b) seems to indicate that to reduce the bias, the cost is to get a much worse language model.\n\n\nFigure 3 (b) illustrates ablation experiments with varying intervention layer numbers. The significant drop in perplexity occurs notably for 10 or 11 layers. It's crucial to note that, as stated in the beginning of section 4.4: \"We apply DAMA to MLPs in approximately one-third of the model\u2019s upper layers (in LLaMA 7B layers 21 - 29 out of 32),\u201d equating to 9 layers. The corresponding perplexity is denoted with a star on the figure, which is close to the score of the original model.\n\n\n## Secondly, we address your questions regarding concepts in Section 4. Moreover, we refined the section for better clarity and uploaded it to open review.\n> \"Certain concepts [..] are either inconsistent or not explicitly defined.\"\n\nWe clarified in section 4 that:\n- \u201cstereotypical keys\u201d and \u201cstereotyped keys\u201d are the same entity. We use just the former name.\n- \u201cgender values\u201d and. \u201cgendered values\u201d are the same entity. The latter name is now used exclusively.\n- \u201cGrammatical gender\u201d  is the quality manifested by the specific inflection of words. We now use a descriptive definition of grammatical gender to avoid confusion with gender scores.\n\n> Questions regarding Equation 2\n\n$z$ is the latent vector, specifically the output of the lth feed-forward layer, as denoted in the subscript  $FF_{out,l}=z$. We used it just as a loss argument to find the gendered value vector $v_o$\n We added $\\text{argmin}$ notation in Equation 2 to clarify that. The missing parenthesis was also fixed.\n\n> \"The first two items in equations look similar to variational inference.\"\n\nIn equation 2, we consider the model's output probability $P(o'|X')$  conditioned on a known prompt ( $X'$). The formula resembles variational inference, yet there is a crucial difference: $X'$ is a known prompt, not a random variable. As a result, a prior distribution cannot be defined here except for a trivial one.\n> Meaning of metrics $U$,$V$, and $P$:\n\nWe just want to clarify that $U$ and $V$ are matrices of concatenated stereotypical key vectors and gendered value vectors as described in the article (those are not metrics). We added further explanation regarding equation 5 in the revised paper.\n\n## Lastly, we\u2019d like to highlight the novelty of DAMA presented in the Discussion Section.\n\n> Novelty in the context of model editing methods\n\nPrevious model editing methods targeted highly specific information encoded in the model ( Mitchell et al., 2022; De Cao et al., 2021), while we introduced a method capable of editing general dataset artifacts, such as various manifestations of gender bias. In Table 2, we show that DAMA significantly outperforms MEMIT (Meng et al., 2022 ) in this aspect.\n\n>  Advancements in projection-based debaising.\n\nIn contrast to earlier projection-based debiasing methods that focused on learning and projecting latent representations for specific gender-related tasks (e.g., gender classification in bios Ravfogel et al. 2020), our method demonstrates that projection can be achieved without relying on auxiliary tasks. Notably, the projections in our approach are directly applied to the model component, avoiding alterations to the architecture or parameter size. Our theoretical and empirical evidence further establishes that applying debiasing projections on MLPs effectively reduces bias while preserving other encoded information, showcasing the generalizability of the method across tasks.\n\n\nWe hope that the response addresses most, if not all, of your concerns. We are open to further discussion, suggestions, or questions you may have."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1695/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700457869039,
                "cdate": 1700457869039,
                "tmdate": 1700457869039,
                "mdate": 1700457869039,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4tI6aY54xb",
            "forum": "XIZEFyVGC9",
            "replyto": "XIZEFyVGC9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1695/Reviewer_2cbg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1695/Reviewer_2cbg"
            ],
            "content": {
                "summary": {
                    "value": "The paper demonstrated unwanted gender bias still exists in the popular LLaMA family of models. To combat these prevalent biases, the paper located components of the model that caused such biases and edited these weights to mitigate the effect of gender biases on downstream tasks. The method, DAMA, is an improvement over the previous method both in terms of reduction rate and maintaining original performances."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Detailed and quantitative measurement of the effect of factual cues and stereotypical cues on model generations. Specifically in the result section, the authors measured the effect of factual cues and stereotypical cues based on layer number and token positions.\n2. The gender bias reduction method via model weight editing has sound theoretical backup.\n3. DAMA effectively reduces the bias rate without hurting too much of the downstream performances."
                },
                "weaknesses": {
                    "value": "1. Seems like there is still quite some room for improvements in the debiasing methods on all three evaluations proposed in the paper. For WinoBias and SteroSet gender, I interpret the results as being still pretty gender biased even after applying DAMA. And for Bias in LM, for larger models, the a_s and a_f are still far from complete removal. It would help greatly to provide other gender bias removal methods as baselines to better assess how well DAMA did.\n2. I think the study on the effects of downstream is great. However, its negative effect on reasoning tasks such as ARC can be concerning for a practical user. Is there any hypothesis on why it doesn't fare well with reasoning type of tasks?"
                },
                "questions": {
                    "value": "Styling:\n1. You are missing a right parenthesis on equation (2) for the KL divergence."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1695/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1695/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1695/Reviewer_2cbg"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1695/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699411251782,
            "cdate": 1699411251782,
            "tmdate": 1699636097816,
            "mdate": 1699636097816,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pZWs10fOBo",
                "forum": "XIZEFyVGC9",
                "replyto": "4tI6aY54xb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1695/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1695/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful review, particularly for acknowledging the significance of evaluating diverse gender bias metrics and downstream tasks. We also appreciate your positive evaluation of our method's theoretical background.\n\n\n> Seems like there is still quite some room for improvements\n\nWe agree that the method can be further improved for larger scales. In our opinion, the crucial strength of DAMA is its versatility in reducing bias for professions and tasks unseen in the adaptation. This contrasts with many past works that did not show consistent patterns across tasks (Delobelle et al. 2022, van der Wal et al. 2023). Admittedly,  bias annotation is oftentimes not consistent. Thus, some manifestation of it may always be present in the model when not specifically targeted.\n\n\n> Is there any hypothesis on why it doesn't fare well with the reasoning type of tasks? ARC\n\nPlease note that the difference between model instances in Table 2 is only barely statistically significant.\nWe compared the errors on the ARC-C task made by the original 65B model and the same model with DAMA and did not observe any clear patterns. On average, the original model selects the correct answer for 40% of DAMA\u2019s errors, while DAMA picks the correct answer for 33% of the original model\u2019s errors. Considering the probability of randomly choosing the correct answer is 33%, it suggests that the performance difference may be attributed to a random factor.\n\n> You are missing a right parenthesis on equation (2) for the KL divergence.\n\nThe equation was fixed in the updated version of the paper.\n\nThank you once more for your review. Should you have any additional comments or questions,  we'll be happy to address them."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1695/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700468632120,
                "cdate": 1700468632120,
                "tmdate": 1700468632120,
                "mdate": 1700468632120,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "j10oFcIo9u",
            "forum": "XIZEFyVGC9",
            "replyto": "XIZEFyVGC9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1695/Reviewer_S9AC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1695/Reviewer_S9AC"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates gender bias in the Llama model. First, an existing causal tracing method is adapted to work for measuring gender bias in different components of the model. Then, a method for modifying the model weights is described for reducing the gender bias. Evaluation is performed both on gender bias benchmarks and downstream task datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The methods are interesting. Evaluation is performed on a number of different datasets and using different measures.\n\nThe method for updating the model is also interesting. Particularly as it manages to not even increase the number of parameters in the model."
                },
                "weaknesses": {
                    "value": "A weakness is only focusing on Llama, as it is unclear how much these findings generalise to other models.\n\nThe causal tracing method is interesting. It is currently unclear how much this is based on previous work and what exactly is the novel contribution. Please clarify this.\n\nIt is unclear why a linear model is fitted across the two gender scores in order to investigate the extent of bias. If the two scores are correlated (which they likely are) then the coefficients of such a linear model might not give accurate indications of the different biases. Measuring correlation with the gender scores seems like a much more straightforward method.\n\nAgain, for the method of updating weights, please clarify the difference of the proposed method to previous work, including Rafvogel et al 2022.\n\nA major selling point of the method seems to be that the proposed DAMA still achieves good performance on downstream tasks. However, the baseline MEMIT actually seems to get better results on most of this metrics. This finding somewhat weakens this claim and should be addressed in the paper."
                },
                "questions": {
                    "value": "Please see above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1695/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699584676453,
            "cdate": 1699584676453,
            "tmdate": 1699636097733,
            "mdate": 1699636097733,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "i4OSzkB7kG",
                "forum": "XIZEFyVGC9",
                "replyto": "j10oFcIo9u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1695/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1695/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thoughtful comments and positive review of our work and for recognizing the importance of the proposed approach. We appreciate your acknowledgment of the need to evaluate the debiased model on a wide range of bias metrics and downstream tasks.\n\nIn this response, we want to highlight the novelty of DAMA and further explain our choices in the experimental setting.\n\n## Novelty of DAMA\n\n> Novelty in the context of model editing methods and comparison to MEMIT\n\nPrevious model editing methods targeted highly specific information encoded in the model ( Mitchell et al., 2022; De Cao et al., 2021), while we introduced a method capable of editing general dataset artifacts, such as various manifestations of gender bias.  Specifically, we show that the intervention performed on a set of professions generalizes well to unseen professions and even other manifestations of Gender Bias. This is beyond the capabilities of MEMIT (Meng et al. 2022), which targets only specific pieces of information that are sparsely encoded. Hence, MEMIT intervention shows marginally better performance on downstream tasks while still keeping bias in the model.\n\n\n>  Advancements in projection-based debaising.\n\nIn contrast to earlier projection-based debiasing methods that focused on learning and projecting latent representations for specific gender-related tasks (e.g., gender classification in bios Ravfogel et al. 2020), our method demonstrates that projection can be achieved without relying on auxiliary tasks. Notably, the projections in our approach are directly applied to the model component, avoiding alterations to the architecture or parameter size. Our theoretical and empirical evidence further establishes that applying debiasing projections on MLPs effectively reduces bias while preserving other encoded information, showcasing the generalizability of the method across tasks.\n\n## Further explanation for experimental setting choices:\n\n> It is unclear why a linear model is fitted across the two gender scores in order to investigate the extent of bias.\n\nThank you for this insightful observation. We have also checked the linear coefficients of the linear models fitted on one of the gender scores (factual and stereotypical). Notably, the patterns are closely aligned with those observed in the joint linear model, visible in the table below.  We opted to present the coefficients of a joint model in the paper because it has one intercept coefficient, which has a clear interpretation as the language model\u2019s preference for male pronoun when the subject is marginalized (as noted at the end of section 2.3).\n\n|          | Joint  |       | Separate |       |\n|----------|--------|-------|----------|-------|\n|          | $a_s$  | $a_f$ | $a_s$    | $a_f$ |\n| Original |  0.235 | 0.320 |    0.325 | 0.371 |\n| w MEMIT  |  0.209 | 0.282 |    0.288 | 0.327 |\n| w DAMA   | -0.005 | 0.037 |   -0.006 | 0.035 |\n\nMoreover, Pearson\u2019s correlation between factual and stereotypical scores is moderately low: $\\rho=0.258$ yet statistically significant.\n\n> only focusing on Llama, as it is unclear how much these findings generalize to other models.\n\nWe intend to explore the applicability of DAMA with other models and languages in future work. We chose LLaMA because of its strong performance and availability of weights across different scales.\n\nOnce again, thank you for your review. We will be happy to answer any further questions regarding our method."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1695/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700462599509,
                "cdate": 1700462599509,
                "tmdate": 1700462599509,
                "mdate": 1700462599509,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]