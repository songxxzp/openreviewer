[
    {
        "title": "Accurate Retraining-free Pruning for Pretrained Encoder-based Language Models"
    },
    {
        "review": {
            "id": "ceTvCiDT89",
            "forum": "s2NjWfaYdZ",
            "replyto": "s2NjWfaYdZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7722/Reviewer_nct7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7722/Reviewer_nct7"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a method to prune an encoder-only pre-trained transformer-based language model without retraining it. The method is based on the notion of a global importance score for each attention head or feed-forward neuron, considering both the prediction loss (KL divergence between pruned and unpruned models) and the representational loss (L2 distance between weights at each layer before and after pruning). It uses a layerwise pruning of attention heads and feed-forward neurons, starting with the bottom sublayer and working up. In each layer, three steps are performed: 1) Global importance computation: Compute the global importance score for each attention head or neuron. 2) Mask search: Search for a mask for each attention head or neuron, considering both prediction and representational knowledge. 3) Linear projection layer tuning: Tune the linear projection layers for the heads or neurons that are not pruned using a linear solver on a small amount of data. The proposed approach achieves an improvement between 8.5-58.0%p in F1 across tasks/models relative to a training-free approach in Kwon et al. (2022). Compared to the unpruned baseline, the proposed approach achieves an inference speedup of 2.65x, while the method of Kwon et al. gives at most a 1.87x speedup. It achieves comparable accuracy to retraining approaches (such as DynaBERT) with significantly less computation."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "*  Presents a new method for pruning a pre-trained encoder-only transformer language model without retraining.\n\n*  The proposed approach achieves F1 score improvements of 8.5% to 58% over existing training-free pruning approaches, while achieving similar performance to retraining approaches at a much lower computational cost.\n\n* Reports an ablation study that shows the relative importance of each component of the approach, revealing that layerwise pruning and weight tuning are critical."
                },
                "weaknesses": {
                    "value": "* Approach is restricted to encoder only models but many of the current LLM approaches are decoder-based.\n* Some aspects of the paper are unclear. See below in questions.\n\nUpdate after author rebuttal:\n* The authors have clarified most of my questions in their rebuttal."
                },
                "questions": {
                    "value": "* Sec 1 : What does the \"%p\" notation mean?\n* Sec 2.2: \"Once the mask variables are established after mask search, pruning of attention heads and neurons whose mask variables are zero does not affect the inference results.\" What does 'established' mean in this context? \n* Equation 7: The LHS K_{rep,l}(X_{\\tau,l},X_{\\rep,l};0)  should be a function of i. \n* Sec 3.3: \"We estimate the importance score not only in the target sublayer but also in the sublayers above the target sublayer\" Why is it necessary to estimate the importance score for sublayers above the target sublayer considering that only the heads/neurons in the target sublayer are pruned when a given layer is considered i.e. From the description/Figure 2, it seems like Algo 1 is run for each sub-layer.\n* The algorithm performs sublayer-wise pruning from the bottom to the top sub-layer. If we refer to this as a single-pass, is it ever necessary in practice to do a second pass to achieve the desired number of FLOPs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7722/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7722/Reviewer_nct7",
                        "ICLR.cc/2024/Conference/Submission7722/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7722/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698443527933,
            "cdate": 1698443527933,
            "tmdate": 1700543470107,
            "mdate": 1700543470107,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WcxnGlOi9k",
                "forum": "s2NjWfaYdZ",
                "replyto": "ceTvCiDT89",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7722/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7722/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> **[Q1] Approach is restricted to encoder-based models but many of the current LLM approaches are decoder-based.**\n\n**[A1]** Theoretically, Kprune is applicable to the decoder-based models since decoder-based Transformers also consist of multi-head attention (MHA) and feedforward network (FFN) sub-layers; we are able to measure the amount of knowledge in each attention head and neuron. \nIn this paper, we focus on encoder-based models to exhaustively demonstrate the effectiveness of Kprune since there are both state-of-the-art retraining-free and retraining-based algorithms for encoder-based models; we demonstrate the accuracy of Kprune by comparing it to retraining-free pruning algorithms and show the efficiency of Kprune by comparing it to retraining-based algorithms. On the other hand, it is impossible to compare Kprune to the retraining-based pruning algorithms using LLMs since retraining-based pruning algorithms require intractable time to prune LLMs.\nAs we mentioned in the Conclusion, extending Kprune to decoder-based models is an interesting future work.\n\n> **[Q2] Sec 1: What does the \"%p\" notation mean?**\n\n**[A2]** \u201c%p\u201d is a symbol of percent-point which is used for representing the difference of percentage values. The term percent-point is distinct from percent (%) though both terms signify shifts in percentage values. Percent (%) expresses the proportional change between two numbers, while the percent-point represents the absolute difference. For instance, a 10% increase from 50% results in 55%, whereas adding 10 percentage points (%p) to 50% yields 60%. We insert a footnote in Section 1 about %p for clarity.\n\n> **[Q3] Sec 2.2:  What does 'established' mean in this context?**\n\n**[A3]**  In this context, \u2018the mask variables are established\u2019 means that the values of the mask variables are determined. We aim to deliver that imposing a mask variable as zero is equal to the pruning of the corresponding attention head or neuron since the output of MHA or FFN is represented as the masked summation of attention heads or neurons, respectively. We substitute the word \u201cestablished\u201d with \u201cdetermined\u201d for clarity.\n\n> **[Q4] Equation 7: The LHS $K_{rep,l}(X_{\\mathcal{T},l},X_{\\mathcal{S},l};0)$ should be a function of $i$.**\n\n**[A4]** We use the notation $K_{rep,l}(X_{\\mathcal{T},l},X_{\\mathcal{S},l};0)$ as the simplified notation of $K_{rep,l}(X_{\\mathcal{T},l},X_{\\mathcal{S},l};m_{l,i}=0)$ which is a function of $i$. We substitute the simplified notation into $K_{rep,l}(X_{\\mathcal{T},l},X_{\\mathcal{S},l};m_{l,i}=0)$ to clearly show that $K_{rep,l}$ is a function of $i$ in Sections 3.2 and B.1.\n\n> **[Q5] Sec 3.3: \"We estimate the importance score not only in the target sublayer but also in the sublayers above the target sublayer\" Why is it necessary to estimate the importance score for sublayers above the target sublayer considering that only the heads/neurons in the target sublayer are pruned when a given layer is considered i.e. From the description/Figure 2, it seems like Algo 1 is run for each sub-layer.**\n\n**[A5]** Kprune measures the importance scores of the target sublayers and upper sublayers to determine the number of attention heads and neurons to prune in each sublayer, considering their global importance. If we do not measure the importance scores of masked units (attention heads or neurons) in upper sublayers, we cannot estimate how important the target sublayer is in the entire model. Therefore, we have to prune each sublayer equally, without considering the sublayer-wise importance, resulting in a significant accuracy drop. \nIn Figure 2, our target sublayer is the 2nd sublayer and we use the importance score of the 3rd and the 4th sublayers to determine the amount of neurons to prune (b). As illustrated in (c) we prune neurons only in the 2nd sublayer (target), not in the 3rd and the 4th sublayers even though we measure the importance of masked units in the 3rd and the 4th sublayers. Then, we move on to the next iteration to prune attention heads in the 3rd sublayer.\nWe revise the description of Figure 2 in Section 3.1 to clarify the reason why we estimate the importance scores in the upper sublayers.\n\n> **[Q6] The algorithm performs sublayer-wise pruning from the bottom to the top sub-layer. If we refer to this as a single pass, is it ever necessary in practice to do a second pass to achieve the desired number of FLOPs?**\n\n**[A6]** You don\u2019t need to manually tune hyperparameters or re-run Kprune to match the desired number of FLOPs. Kprune automatically finds the global pruning configurations that match the pre-defined desired pruning rate by adjusting the FLOPs budget after pruning each sublayer. We revise the last sentence in Section 3.4 to clearly deliver that Kprune does not require user intervention, such as hyperparameter tuning, to match the desired number of FLOPs."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7722/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700220755141,
                "cdate": 1700220755141,
                "tmdate": 1700220755141,
                "mdate": 1700220755141,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Nkd35m5apC",
                "forum": "s2NjWfaYdZ",
                "replyto": "WcxnGlOi9k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7722/Reviewer_nct7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7722/Reviewer_nct7"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the explanation"
                    },
                    "comment": {
                        "value": "Thanks for your explanations!"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7722/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700543391236,
                "cdate": 1700543391236,
                "tmdate": 1700543391236,
                "mdate": 1700543391236,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cvJqneQCQO",
            "forum": "s2NjWfaYdZ",
            "replyto": "s2NjWfaYdZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7722/Reviewer_sVyn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7722/Reviewer_sVyn"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces Kprune (Knowledge-preserving pruning), a novel retraining-free structured pruning algorithm designed for pretrained encoder-based language models. The key challenge addressed by this work is the accurate compression of such models without requiring retraining. Existing retraining-free algorithms suffer from significant accuracy degradation, particularly at high compression rates, due to their inability to handle pruning errors effectively.\n\nKprune employs an iterative pruning process that focuses on preserving the valuable knowledge contained within the pretrained model. This process includes three main steps: knowledge measurement, knowledge-preserving mask search, and knowledge-preserving weight-tuning. By implementing these steps, Kprune achieves remarkable results, with accuracy improvements of up to 58.02% higher F1 score compared to existing retraining-free pruning algorithms. These improvements are observed under a high compression rate of 80% when tested on the SQuAD benchmark, all without the need for any retraining."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Kprune presents a significant advancement in the field of pretrained language model compression by demonstrating the feasibility of high compression rates without compromising model accuracy. The approach's success lies in its ability to preserve the essential knowledge within the model, ultimately leading to impressive gains in performance compared to existing retraining-free pruning methods.\n2. The equations are clearly described."
                },
                "weaknesses": {
                    "value": "1. The writing of the introduction section seems unreasonable. I think some challenges and other content should be written in the introduction instead of the method.\n2. In the main experiment table (Table 1), there are not enough baselines for comparison."
                },
                "questions": {
                    "value": "Please refer to Weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7722/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698772479943,
            "cdate": 1698772479943,
            "tmdate": 1699636941377,
            "mdate": 1699636941377,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UqekMhevUS",
                "forum": "s2NjWfaYdZ",
                "replyto": "cvJqneQCQO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7722/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7722/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely appreciate the reviewer\u2019s careful feedback. We respond to your feedback below.\n\n> **[Q1] The writing of the introduction section seems unreasonable. I think some challenges and other content should be written in the introduction instead of the method.**\n\n**[A1]** Thank you for your constructive review. We have revised the Introduction section to reflect your comment. We add a paragraph about the need for retraining-free pruning algorithms and clarify the limitations of existing retraining-free algorithms.\n\n> **[Q2] In the main experiment table (Table 1), there are not enough baselines for comparison.**\n\n**[A2]** To the best of our knowledge, Kwon et al. (NeurIPS\u201922) and KCM (ICML\u201923) are the best-performing retraining-free pruning algorithms for encoder-based models, and Kprune is the first algorithm that outperforms both. In detail, Kwon et al. is the first retraining-free pruning algorithm for PLMs and Kwon et al. is the only competitor for KCM. There are a few retraining-free algorithms for (decoder-based) LLMs, such as SparseGPT[1] or Wanda[2], but, they employ an impractical unstructured pruning pattern; they require inefficient sparse tensor representations for memorization and customized hardware for acceleration.\n\n**[Reference]**\n\n[1] Frantar, Elias, and Dan Alistarh. \"SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot.\" (2023).\n\n[2] Sun, Mingjie, et al. \"A Simple and Effective Pruning Approach for Large Language Models.\" arXiv preprint arXiv:2306.11695 (2023)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7722/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700220169784,
                "cdate": 1700220169784,
                "tmdate": 1700220169784,
                "mdate": 1700220169784,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Lwp5fKnRzz",
                "forum": "s2NjWfaYdZ",
                "replyto": "UqekMhevUS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7722/Reviewer_sVyn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7722/Reviewer_sVyn"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Reviewer sVyn"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response. I have read the rebuttal and most of my concerns have been well addressed. Overall, I am towards acceptance and will maintain my score.\n\nBest, The reviewer sVyn"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7722/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736646783,
                "cdate": 1700736646783,
                "tmdate": 1700736646783,
                "mdate": 1700736646783,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "l77BRwvaxB",
            "forum": "s2NjWfaYdZ",
            "replyto": "s2NjWfaYdZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7722/Reviewer_9HrX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7722/Reviewer_9HrX"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents KPrune, a new retraining-free structured pruning method for compressing task-specific models while retaining their knowledge. Unlike prior techniques, KPrune considers both the overall loss impact and the effect on representation preservation when deciding which units to prune. It does this by analyzing the expected output of each head/neuron. As a result, KPrune requires minimal pruning time compared to retraining methods and outperforms previous retraining-free approaches. This work differs from Kwon et al. (2022) in two main ways: 1) it incorporates layer-wise representation loss when measuring unit importance, and 2) it recovers weights by tuning the output matrix with linear solvers. These innovations enhance pruning quality with minimal overhead versus retraining. In summary, KPrune efficiently compresses models while maintaining performance by carefully accounting for loss impact and representation preservation when pruning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well written and easy to follow!\n- KPrune is the first retraining-free pruning method to incorporate layer-wise representation preservation loss and KL loss on outputs, techniques commonly used in retraining-based pruning.\n- KPrune shows considerable performance improvements over previous retraining-free methods in high sparsity regimes (>80%). This demonstrates its ability to maintain model quality even with extreme compression rates."
                },
                "weaknesses": {
                    "value": "- While not a direct comparison, it would be interesting to compare KPrune's performance to stronger training-based methods like CoFiPruning. KPrune's  major advantage is requiring much less time for pruning, though training-based approaches may achieve better end results. \n- Since BERT models are relatively small in scale nowadays, reducing training compute is less impactful compared to large language models. It would be interesting to evaluate if KPrune can effectively scale up to larger and more powerful models where saving compute will be more significant."
                },
                "questions": {
                    "value": "I don't have further questions for the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7722/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698784765305,
            "cdate": 1698784765305,
            "tmdate": 1699636941248,
            "mdate": 1699636941248,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uPBDnTqMYr",
                "forum": "s2NjWfaYdZ",
                "replyto": "l77BRwvaxB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7722/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7722/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely appreciate the reviewer\u2019s careful feedback. We respond to your feedback below.\n\n> **[Q1] While not a direct comparison, it would be interesting to compare KPrune's performance to stronger training-based methods like CoFiPruning[1]. KPrune's major advantage is requiring much less time for pruning, though training-based approaches may achieve better end results.**\n\n**[A1]** Thank you for suggesting an interesting experimental setup. However, as we mention in the \u201cCompetitors\u201d paragraph in Section 4.1 of our paper, there is a reason for excluding CoFi as a competitor. According to Kwon et al.[2],  CoFi shows similar or worse accuracy than DynaBERT (Figure 6 in Kwon et al.) while requiring more than 2.7 times longer time for pruning than DynaBERT (Table 2 in Kwon et al.). Therefore, we chose DynaBERT and EBERT as our competitors and conducted our experiments accordingly.\n\n> **[Q2] Since BERT models are relatively small in scale nowadays, reducing training compute is less impactful compared to large language models. It would be interesting to evaluate if KPrune can effectively scale up to larger and more powerful models where saving compute will be more significant.**\n\nAs noted in the response [A2] for the reviewer **[akzo]**, the cost of Kprune for a model with $L$ sublayers is equivalent to performing $(L+1)/2$ times of forward and backward propagations on tiny sample data. If we follow the setting of SparseGPT[1], an \u201cunstructured\u201d pruning algorithm for large language models (LLMs), this involves using only about 128 sample instances. Therefore, if the hardware setting is adequate for running inference on massive LLMs, it is entirely feasible to execute Kprune within a tractable amount of time. Extending accurate and efficient Kprune to massive LLMs is our promising future work.\n\n**[Reference]**\n\n[1] Xia, Mengzhou, Zexuan Zhong, and Danqi Chen. \"Structured pruning learns compact and accurate models.\" Annual Meeting of the Association for Computational Linguistics (2022)\u201d: 1513-1528\n\n[2] Kwon, Woosuk, et al. \"A fast post-training pruning framework for transformers.\" Advances in Neural Information Processing Systems 35 (2022): 24101-24116."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7722/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700219717489,
                "cdate": 1700219717489,
                "tmdate": 1700725472420,
                "mdate": 1700725472420,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6U88k6iiZb",
                "forum": "s2NjWfaYdZ",
                "replyto": "uPBDnTqMYr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7722/Reviewer_9HrX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7722/Reviewer_9HrX"
                ],
                "content": {
                    "title": {
                        "value": "Inference latency of the pruned models"
                    },
                    "comment": {
                        "value": "Thanks for your response! I noticed that this paper, as well as Kwon et al., 2022, both use reduced FLOPs instead of inference latency as the metric to compare against different approach and it might be misleading. Reduced FLOPs and inference runtime could lead to very different results and interpretations of the final outcome of a pruning approach. Adding wall-clock analyses would be very helpful for understanding the effectiveness of the approach."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7722/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700489082571,
                "cdate": 1700489082571,
                "tmdate": 1700489082571,
                "mdate": 1700489082571,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CmxHL3xikf",
            "forum": "s2NjWfaYdZ",
            "replyto": "s2NjWfaYdZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7722/Reviewer_akzo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7722/Reviewer_akzo"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new algorithm called Kprune that can significantly improve the accuracy of pretrained language models while compressing them without the need for retraining. The authors explain that while pruning is a common technique for compressing deep neural networks, it is often difficult to apply to pretrained language models due to their complex architecture and the difficulty of preserving their useful knowledge during the pruning process. Kprune addresses these challenges by using an iterative pruning process that selectively removes neurons from the model based on their importance to the overall performance of the model. The authors evaluate Kprune on several benchmark datasets and compare its performance to existing retraining-free pruning algorithms."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper introduces a new algorithm, Kprune, that can significantly improve the accuracy of pretrained language models while compressing them without the need for retraining. \n- The authors provide a detailed explanation of the iterative pruning process used in Kprune and how it helps preserve the useful knowledge of the pretrained model. \n- The authors evaluate Kprune on several benchmark datasets and compare its performance to existing retraining-free pruning algorithms, providing evidence of its effectiveness. \n- The paper could have practical applications in the field of natural language processing, where the ability to compress pretrained language models without sacrificing accuracy is highly desirable."
                },
                "weaknesses": {
                    "value": "- The paper focuses specifically on encoder-based language models, so it may not be applicable to other types of language models. \n- The authors do not provide a detailed analysis of the computational resources required to implement Kprune, which could be a potential limitation for some applications. \n- The paper does not apply to large language models which are commonly used now, experiments on larger models are expected."
                },
                "questions": {
                    "value": "Can the proposed technique applied to LLMs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7722/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698959961511,
            "cdate": 1698959961511,
            "tmdate": 1699636941140,
            "mdate": 1699636941140,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "inVWuCHzXW",
                "forum": "s2NjWfaYdZ",
                "replyto": "CmxHL3xikf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7722/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7722/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely appreciate the reviewer for expressing your concerns. We respond to your concerns and questions below.\n\n> **[Q1] The paper focuses specifically on encoder-based language models, so it may not be applicable to other types of language models.**\n\n**[A1]** Theoretically, Kprune is applicable to the decoder-based models since decoder-based Transformers also consist of multi-head attention and feedforward network sub-layers; we are able to measure the amount of knowledge in each attention head and neuron. \nOn the other hand, we focus on encoder-based models to exhaustively demonstrate the effectiveness of Kprune since there are both state-of-the-art retraining-free and retraining-based algorithms for encoder-based models; we demonstrate the accuracy of Kprune by comparing it to retraining-free pruning algorithms and show the efficiency of Kprune by comparing it to retraining-based algorithms. On the other hand, it is impossible to compare Kprune to the retraining-based pruning algorithms using LLMs since retraining-based pruning algorithms require intractable time to prune LLMs. As we mentioned in the Conclusion, extending Kprune to decoder-based models is an interesting future work.\n\n> **[Q2] The authors do not provide a detailed analysis of the computational resources required to implement Kprune, which could be a potential limitation for some applications.**\n\n**[A2]** The computational cost of Kprune is composed of the computational costs of (a) Knowledge measurement, (b) Knowledge-preserving mask search (KPMS), and (c) Knowledge-preserving weight-tuning processes (KPWT). During (b) KPMS, the process involves inexpensive calculation of importance scores based on the measured amount of knowledge, and comparison of the scores to find the pruning masks; thus KPMS  takes a very short time. Moreover, (c) KPWT is also an inexpensive process, completing within a second for all sublayers when using the \u201clstsq\u201d solver provided in PyTorch. Therefore, the computational cost of Kprune mainly depends on the (a) knowledge measurement process, which involves repetitive forward and backward operations of sublayers.\n\nIf we assume that $M$ is the number of operations required for forward and backward propagations in a model with $L$ sublayers, then the operations required per sublayer are considered as $M/L$. The knowledge measurement process includes forward and backward operations for each sublayer from the bottom to the top. Each operation includes $L$, $L-1$, ..., and $1$ times of forward and backward operations of sublayers, and requires $(L+1)/2 * M$ operations in total, i.e. it is the same as the cost of running $(L+1)/2$ epochs on a sample dataset. Combined with the fact that Kprune utilizes a tiny sample dataset (about 0.64% of the MNLI dataset), the cost of Kprune is significantly low and Kprune is scalable to the larger language models.\nIn our paper, Kprune takes a few minutes on a single NVIDIA 1080Ti GPU for pruning BERT-base which is up to 422 times shorter time for pruning retraining-based algorithms.\n\n> **[Q3] The paper does not apply to large language models which are commonly used now, experiments on larger models are expected.\nCan the proposed technique applied to LLMs?**\n\n**[A3]** As noted in [A2], the cost of Kprune for a model with $L$ sublayers is equivalent to performing $(L+1)/2$ times of forward and backward propagations on a tiny sample dataset. If we follow the setting of SparseGPT[1], an \u201cunstructured\u201d pruning algorithm for large language models (LLMs), this involves using only about 128 sample instances. Therefore, if the hardware setting is adequate for running inference on massive LLMs, it is entirely feasible to execute Kprune within a tractable amount of time. Extending accurate and efficient Kprune to massive LLMs is our promising future work.\n\n**[Reference]**\n\n[1] Frantar, Elias, and Dan Alistarh. \"SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot.\" (2023)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7722/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700219400935,
                "cdate": 1700219400935,
                "tmdate": 1700725519051,
                "mdate": 1700725519051,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]