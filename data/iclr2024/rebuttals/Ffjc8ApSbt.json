[
    {
        "title": "Adaptive Causal Balancing for Collaborative Filtering"
    },
    {
        "review": {
            "id": "gzJ4RpWpJx",
            "forum": "Ffjc8ApSbt",
            "replyto": "Ffjc8ApSbt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9073/Reviewer_ptof"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9073/Reviewer_ptof"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a unified kernel-based method to balance functions on the reproducing kernel Hilbert space (RKHS) to address the bias in collaborative filtering due to the observational nature of the collected data. Although balancing (i.e., re-weighting the sample loss) is an important technique to address such selection bias, this paper characterizes the effect of balancing low-dimensional functions on the bias of inverse propensity score (IPS) and doubly robust (DR) methods. A novel adaptive causal balancing method that alternates between unbiased evaluation and model training is proposed. Extensive numerical experiments are conducted on real-world recommendation data sets to demonstrate the proposed method could effectively improve prediction performance compared with the existing benchmarks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The problem studied is important and relevant.\n2. The idea is interesting and novel.\n3. The evaluations are solid and convincing."
                },
                "weaknesses": {
                    "value": "1. The optimization formulation for balancing may be computationally intractable if the number of items and/or the number of users are large.\n\n2. It is unclear how to incorporate the proposed method into the modern deep learning (DL) based recommender system and to test its effectiveness in a field setting."
                },
                "questions": {
                    "value": "1. How to update the weights for the problems where the number of items and/or the number of users are large?\n\n2. Modern DL-based recommender system typically generates an embedding for each user and each item for subsequent tasks such as score prediction and recommendation. Is it possible to adjust this method as a means of fine-tuning the item and/or user embeddings?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9073/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9073/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9073/Reviewer_ptof"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9073/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698549132027,
            "cdate": 1698549132027,
            "tmdate": 1699637142630,
            "mdate": 1699637142630,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2uOAEEGjXY",
                "forum": "Ffjc8ApSbt",
                "replyto": "gzJ4RpWpJx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9073/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9073/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Please kindly find our concise and clear rebuttal below for addressing your current concerns [W1, W2, Q1, Q2]"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the reviewer\u2019s great efforts and insightful comments to improve our manuscript. In below, we address these concerns point by point and try our best to update the manuscript accordingly.\n\n> **[W1 & Q1] How to update the weights for the problems where the number of items and/or the number of users are large?**\n\n**Response:** We thank the reviewer for raising an interesting concern.\n\n- First, **the balancing weights $w_{u, i}$ are parameterized using a deep neural network model $g(x_{u, i}; \\phi_w)$**, which ensures the model scalability and the model parameters $\\phi_w$ could be much less than the number of user-item pairs when the number of items and/or the number of users is large. \n\n- Second, **we can write the primal optimization problem to the Lagrange dual problem**, in which the number of parameters is the same as the number of constraints $O(J)$, and is independent with the user numbers and item numbers.\n\n- Third, for the optimization problem, **we can uniformly sample a batch in $\\mathcal{D} = \\[u_1, u_2, ..., u_m\\] \\times \\[i_1, i_2, ..., i_n\\]$ to compute $\\tau^{(j)}$ for $j = 1, 2, ..., J$**, instead of using all $m \\cdot n$ user-item pairs that concerns the computational efficiency.\n\n> **[W2] How to incorporate the proposed method into the modern deep learning (DL) based recommender system?**\n\n**Response:** We thank the reviewer for the useful comments. **The proposed methods are model-agnostic**, which means the balancing weight model $g(x_{u, i}; \\phi_w)$ and the prediction model $f(x_{u, i}; \\theta)$ can adopt the modern deep neural networks. To show this, **we add the experiment on a large-scale industrial dataset Product** using **neural collaborative filtering (NCF) [1]** as the base model to empirically show the performance of the proposed methods in Appendix B, Table 2. The results are shown below.\n||AUC|NDCG@20|F1@20|\n|:--:|:--:|:--:|:--:|\n|NCF| 0.823\u00b10.001|0.575\u00b10.002|0.166\u00b10.002|\n|+CVIB|0.820\u00b10.002|0.544\u00b10.004|0.162\u00b10.002|\n||\n|+IPS|0.822\u00b10.003|0.579\u00b10.006|0.169\u00b10.003|\n|+SNIPS|0.833\u00b10.002|0.586\u00b10.002|0.178\u00b10.002|\n|+ASIPS|0.832\u00b10.002|0.583\u00b10.003|0.178\u00b10.002|\n|+IPS-V2|0.835\u00b10.001|0.588\u00b10.002|0.181\u00b10.001|\n|+CBIPS|0.832\u00b10.001|0.584\u00b10.003|0.178\u00b10.003|\n|+WKBIPS|0.833\u00b10.002| 0.588\u00b10.003|0.181\u00b10.003|\n|+AKBIPS|**0.836*\u00b10.001**| **0.592*\u00b10.002**|**0.183*\u00b10.002**|\n||\n|+DR |0.758\u00b10.004|0.526\u00b10.003|0.146\u00b10.003|\n|+DR-JL|0.832\u00b10.001|0.581\u00b10.003|0.178\u00b10.002|\n|+MRDR-JL|0.833\u00b10.001|0.585\u00b10.002|0.179\u00b10.001|\n|+DR-BIAS|0.834\u00b10.002|0.585\u00b10.003|0.178\u00b10.002| \n|+DR-MSE|0.834\u00b10.002|0.587\u00b10.002|0.180\u00b10.001|\n|+MR|0.837\u00b10.001|0.588\u00b10.002|0.181\u00b10.002|\n|+TDR-JL|0.834\u00b10.002|0.582\u00b10.002|0.179\u00b10.003|\n|+SDR|0.835\u00b10.002|0.587\u00b10.003|0.179\u00b10.002|\n|+DR-V2|0.837\u00b10.002|0.586\u00b10.004|0.182\u00b10.002|\n|+CBDR|0.834\u00b10.002 |0.586\u00b10.003|0.179\u00b10.003|\n|+WCBDR|0.837\u00b10.002| 0.588\u00b10.003|0.180\u00b10.002|\n|+ACBDR|**0.840*\u00b10.002**|**0.590*\u00b10.003**|**0.183\u00b10.002**| \n|| \n\nThe experiment results show that the proposed kernel balancing methods achieve overall performance improvement compared to the baseline methods.\n\n> **[Q2] Is it possible to adjust this method as a means of fine-tuning the item and/or user embeddings?**\n\n**Response:** **Yes, it is possible to fine-tune the embeddings using our proposed methods**. We illustrate this by using **click-through rate (CTR) prediction** and **post-click conversion rate (pCVR) prediction** in e-commerce recommendation as an example. From a debiasing formulation, the data formats and the quantity of interest in the \"offline rating prediction\" scenario and the \"e-commerce conversion rate prediction\" scenario are exactly the same [2-4]. We provide a comparison table as below.\n\n|Terminology|Offline ratings prediction|E-commerce pCVR prediction|Comments|\n|:--:|:--:|:--:|:--:|\n|$x_{u, i}$|user and item features|user and item features|all can be obtained in the raw data|\n|$o_{u, i}$|rating observe indicator\t|click indicator|$o_{u, i} = 1$ if observing, else $o_{u, i}=0$ if missing|\n|$r_{u, i}$|true rating, i.e., the true preference of user to item| post-click conversion indicator|only can be observed when $o_{u, i}=1$, otherwise missing|\n|$\\hat{p}_{u, i}$|propensity model|CTR model|for both we have $p_{u, i} = P(o_{u, i} = 1 \\mid x_{u, i})$|\n||\n\nTherefore, we can adopt the method proposed in this paper to adaptively choose $\\phi(x_{u, i})$ as a kernel function $K(x_{s, t}, x_{u, i})$ with some fixed $(s, t)$ and then fine-tune the CTR model $\\hat{p}(o = 1|x_{u, i})$. **One possible way to fine-tune the CTR model is minimizing the directed Kullback entropy divergence [5] defined by $h(w_i) = w_i log({w_i}/{q_i})$ between the new and the original CTR model predictions** as the **objective function** in the proposed entropy-based optimization problem. Moreover, we can further fine-tune the pCVR model based on the fine-tuned CTR model."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9073/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699019776,
                "cdate": 1700699019776,
                "tmdate": 1700712713583,
                "mdate": 1700712713583,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4j7PtY6LNM",
            "forum": "Ffjc8ApSbt",
            "replyto": "Ffjc8ApSbt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9073/Reviewer_4dU2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9073/Reviewer_4dU2"
            ],
            "content": {
                "summary": {
                    "value": "This work contributes to the field of debiased recommendation recommendation by proposing a\nuniversal kernel-based balancing method to balance functions. It also provides theoretical analysis and guidance of balancing property under finite samples. Experimental results on three real-world datasets verify the proposed balancing methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.This work discusses the limitations of existing methods including the Inverse Propensity Score (IPS) and Doubly Robust (DR) method for debiased recommendation, showing that the importance of balancing property under finite-dimensional function classes.\n\n2.The authors extend them to CBIPS and CBDR estimators and it is reasonable to proposed kernel balancing method for optimization.\n\n3.The work also provides theoretical analysis and proof of the proposed kernel balancing and proposes three causal balancing methods to effectively balance the kernel functions.\n\n4.Experimental results show the effectiveness of the adaptive kernel balancing method."
                },
                "weaknesses": {
                    "value": "1.It is advisable to include a discussion of the relationships or distinctions between this work and prior research in the Related Work section.\n\n2.In terms of the Adaptive Kernel Balancing method, I am confused why balancing the kernel functions with maximal $|\\alpha_{s,t}|$ can contribute the most to the $e_{u,i}$. It requires more explanation to improve the readability. Besides, the authors adopt this way to improve the efficiency but there is no corresponding experiment to validate this.\n\n3.Writting of this paper needs improving:\n- The background of debias recommendation is not enough. \n- The definition of $o_{u,i}$ is not clarified clearly.\n- Grammar needs carefully checking:\n  - the usage of \u201can\u201d such as \u201cMoreover, we propose an causal\u201d \n  - learn an appropriate propensity model that achieve lower estimation bias."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9073/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698555747409,
            "cdate": 1698555747409,
            "tmdate": 1699637142508,
            "mdate": 1699637142508,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ey5NLnd7yN",
                "forum": "Ffjc8ApSbt",
                "replyto": "4j7PtY6LNM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9073/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9073/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Please kindly find our concise and clear rebuttal below for addressing your current concerns"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the reviewer\u2019s great efforts and insightful comments to improve our manuscript. In below, we address these concerns point by point and try our best to update the manuscript accordingly.\n\n> **[W1] It is advisable to include a discussion of the relationships or distinctions between this work and prior research in the Related Work section.**\n\n**Response:** Thank you for your kind advice. As suggested by the reviewer, **we add a discussion of the relationships and distinctions between our work and prior research** in the Related Work section **for both \u201cDebiased Recommendation\u201d and \u201cCovariate Balancing in Causal Inference\u201d parts.**\n\n> **[W2.1] Why balancing the kernel functions with maximal $|\\alpha_{s, t}|$ can contribute the most to the $|e_{u, i}|$?**\n\n**Response:** Taking CBIPS as an example, in the last equation on Page 5, we use the universal property and the representer theorem to show the bias of CBIPS estimator is \n\n$$\\operatorname{Bias}^2(\\mathcal{L}_{\\mathrm{CBIPS}}(\\theta))$$\n\n $$= \\[\\frac{1}{|\\mathcal{D}|}\\sum_{(u,i) \\in \\mathcal{D}} (o_{u, i} w_{u,i}-1)(\\sum_{(s, t) \\in \\mathcal{D}}\\alpha_{s, t}K(x, x_{s, t})) \\]^2$$\n $$= \\[\\frac{1}{|\\mathcal{D}|}\\sum_{(s, t) \\in \\mathcal{D}}\\alpha_{s, t}\\sum_{(u,i) \\in \\mathcal{D}} (o_{u, i} w_{u,i}-1)(K(x, x_{s, t})) \\]^2$$\n $$= \\[\\frac{1}{|\\mathcal{D}|}\\sum_{(s, t) \\in \\mathcal{D}}\\alpha_{s, t}\\sum_{(u,i) \\in \\mathcal{D}} (o_{u, i} w_{u,i}K(x, x_{s, t}) - K(x, x_{s, t})) \\]^2.$$\nIf we balance $K(x, x_{s, t})$, we have $$\\frac{1}{|\\mathcal{D}|}\\sum_{  (u,i) \\in \\mathcal{D} }  o_{u, i}w_{u,i} K(x, x_{s, t})  =   \\frac{1}{|\\mathcal{D}|}\\sum_{ (u,i) \\in \\mathcal{D} }  K(x, x_{s, t}),$$ that is,\n$$\\frac{1}{|\\mathcal{D}|}\\sum_{(u,i) \\in \\mathcal{D}} (o_{u, i} w_{u,i}K(x, x_{s, t}) - K(x, x_{s, t})) = 0.$$\n\nTherefore, selecting the $K(x, x_{s, t})$ with maximal $|\\alpha_{s, t}|$ to balance can effectively and rapidly reduce $\\operatorname{Bias}(\\mathcal{L}_{\\mathrm{CBIPS}}(\\theta))$.\n\n> **[W2.2] No corresponding experiment to validate the efficiency?**\n\n**Response:** Thanks for pointing out this issue and we really apologize for the **typo** here. **We intend to say \"effectiveness\" instead of \"efficiency\".** Nevertheless, we still **add the experiments to analyze the trade-off between the effectiveness and the efficiency of CBDR and ACBDR** in Appendix B, Table 3. The results are shown below.\n\n| Number of balancing functions | Metrics | CBDR-Gaussian | ACBDR-Gaussian |\n|:--:|:--:|:--:|:--:|\n|$J=3$| AUC     | 0.681\u00b10.003   | 0.688\u00b10.003    |\n|| NDCG@5  | 0.650\u00b10.003   | 0.657\u00b10.003    |\n|| F1@5    | 0.323\u00b10.002   | 0.326\u00b10.002    |\n|| Time(s) | 389.28\u00b117.51  | 664.99\u00b112.37   |\n||\n|$J=5$| AUC     | 0.683\u00b10.002   | 0.694\u00b10.002    |\n|| NDCG@5  | 0.652\u00b10.003   | 0.664\u00b10.002    |\n|| F1@5    | 0.325\u00b10.002   | 0.332\u00b10.002    |\n|| Time(s) | 394.51\u00b116.61  | 678.13\u00b115.43   |\n||\n|$J=10$| AUC     | 0.682\u00b10.002   | 0.694\u00b10.002    |\n|| NDCG@5  | 0.650\u00b10.003   | 0.663\u00b10.002    |\n|| F1@5    | 0.324\u00b10.003   | 0.330\u00b10.003    |\n|| Time(s) | 399.91\u00b18.58   | 719.63\u00b123.79   |\n||\n|$J=20$                           | AUC     | 0.683\u00b10.002   | 0.695\u00b10.002    |\n|| NDCG@5  | 0.651\u00b10.002   | 0.664\u00b10.003    |\n|| F1@5    | 0.325\u00b10.002   | 0.332\u00b10.003    |\n|| Time(s) | 389.11\u00b122.39  | 727.29\u00b126.76   |\n||\n|$J=50$ | AUC     | 0.684\u00b10.002   | 0.695\u00b10.002    |\n|| NDCG@5  | 0.652\u00b10.003   | 0.664\u00b10.002    |\n|| F1@5    | 0.324\u00b10.003   | 0.333\u00b10.002    |\n|| Time(s) | 407.89\u00b111.67  | 722.43\u00b125.55   |\n||\n\nThe ACBDR method stably outperforms CBDR with varying number of balancing functions, which shows the **effectiveness** of the proposed adaptively balancing method. **For efficiency, ACBDR takes about \"doubled running time\" to converge than KBDR,** since it needs to sort all the coefficients of kernel functions, which is time-consuming when the number of user-item pairs is large. When $J > 5$, the performance of the AKBDR does not improve as the growth of $J$, but the running time increases.\n\n> **[W3] Writting of this paper needs improving.**\n\n**Response:** We thank the reviewer for pointing out this issue, and **we have added the formal definition of $o_{u, i}$ in Section 3.1, and carefully polished the manuscript in our revised version to avoid typos and improve readability.**\n\n***\n**We hope the above discussion will fully address your concerns about our work, and we would really appreciate it if you could be generous in raising your score.** We look forward to your insightful and constructive responses to further help us improve the quality of our work. Thank you!"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9073/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700697733198,
                "cdate": 1700697733198,
                "tmdate": 1700698139940,
                "mdate": 1700698139940,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vj5yJYwDz1",
            "forum": "Ffjc8ApSbt",
            "replyto": "Ffjc8ApSbt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9073/Reviewer_CJCu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9073/Reviewer_CJCu"
            ],
            "content": {
                "summary": {
                    "value": "This study explores the concept of incorporating a balance regularization loss to mitigate selection bias in Recommender Systems (RS). It centers on determining which function class requires balancing. The authors reframe the issue as an optimization problem, contending that the prediction error should result from a linear combination of balancing functions, supported by theoretical analysis. To achieve this, the authors suggest utilizing kernel functions as balancing functions and put forth three approaches for selecting these kernels. Empirical experiments illustrate the benefits of the proposed balancing techniques."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The study delves deeper into the challenge of addressing selection bias by examining the specific function that warrants balance. This research problem introduces a novel perspective.\n\n- The concept of employing a linear combination of kernel functions to attain both unbiasedness and universality is intriguing and offers inspiration."
                },
                "weaknesses": {
                    "value": "I think the idea in this paper is interesting and has some value. However, I have some significant concerns that prevent me from recommending this manuscript for acceptance.\n\n **1. Motivation**\n\n- W1: This paper lacks a sufficient motivation for the importance of carefully selecting the function class for balancing. The authors do discuss the computational cost associated with choosing numerous balancing functions in Section 3.2. However, the original paper [1] suggests that even selecting a single function for balancing can yield performance improvements. It is essential to elucidate why the choice of multiple balancing functions is necessary and why the specific type of function is critical. In my view, the motivation may be derived from Corollary 1 to some extent, which asserts that certain types of balancing functions introduce bias (correct me if I am wrong). Nonetheless, it would be valuable to provide a more insightful explanation in the introduction section. I would like to see an illustrative example, which could enhance comprehension of the motivation.\n\n**2. Clarity**\n\nThis paper contains several areas of ambiguity and insufficient support for its claims, which hinder the comprehension of its ideas. Some key issues include:\n\n- W2: The optimization problems in Sections 4.2 and 4.4 are perplexing. The rationale behind minimizing the sum of $g(\\cdot) \\log g(\\cdot)$ as opposed to a standard cross-entropy loss is unclear and confusing. The purpose of constraining the sum of $o_{u,i}g(\\cdot)$ to be one and the meaning of the third constraint require more detailed explanation and motivation. The authors should clarify the significance and reasoning behind each statement in the proposed optimization problem.\n\n- W3: Several theorems and corollaries, such as Theorem 1, Corollary 1, and Lemma 2, lack supporting proofs. In particular, the proof of Corollary 1, which appears directly relevant to the paper's motivation, is crucial.\n\n- W4: The paper suggests that \"balancing propensity can reduce the generalization bound\", but this claim is not clearly evident from Theorem 2, which seems to provide a bound without a comparative analysis.\n\n- W5: The fourth paragraph in Section 1 references \"the first question\" and \"the second question\", but it is unclear where these questions are presented in the paper.\n\n- W6: The paper does not elaborate on how the parameters within the kernel functions are learned. For instance, Gaussian kernel functions are known to have a parameter, $\\sigma$. The paper should explain the process for selecting suitable parameter values.\n\n- W7: Given that kernel functions are not frequently used for debiasing Recommender Systems, it is recommended that the authors provide more specific definitions and formulations regarding the kernel functions in Section 4.3 to enhance clarity.\n\n- W8: There are some typos in this paper. \n  - Section 1: \"it is the first paper provides ...\", \"Ours theoretical analysis shows...\". \n  - Section 4.4: \"which Random chooses...\".\n\n**3. Experiments**\n\n- W9: The paper does not specify which kernel function was chosen, whether Gaussian or exponential.\n\n- W10: I found that the performance on \"Coat\" you reported in Table 1 is significantly lower than that on [1]. It would be helpful to understand whether this variance is attributed to different experimental settings or other factors.\n\n- W11: The abbreviation \"CB\" for causal balancing is indeed similar to \"Covariate Balancing\" and may cause confusion. A more distinct abbreviation should be considered to prevent any potential misunderstandings. \n\nOverall, the motivation and clarity are my main concerns. I will consider changing my score if I receive a high-quality (concise and clear) rebuttal.\n\n[1] Haoxuan Li, Yanghao Xiao, Chunyuan Zheng, Peng Wu, and Peng Cui. Propensity matters: Measuring and enhancing balancing for recommendation. 2023d.\n\n\n---\n\n*After the rebuttal: the authors address the most of my concerns. Therefore, I am glad to raise my score from 3 to 6.*"
                },
                "questions": {
                    "value": "Please see the Weaknesses part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9073/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9073/Reviewer_CJCu",
                        "ICLR.cc/2024/Conference/Submission9073/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9073/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698682528432,
            "cdate": 1698682528432,
            "tmdate": 1700714653906,
            "mdate": 1700714653906,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tTYnj8gMj7",
                "forum": "Ffjc8ApSbt",
                "replyto": "vj5yJYwDz1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9073/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9073/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Please kindly find our concise and clear rebuttal below for addressing your current concerns [W1-W4]"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the reviewer\u2019s great efforts and insightful comments to improve our manuscript. In below, we address these concerns point by point and try our best to update the manuscript accordingly.\n\n**1. Motivation**\n\n> **[W1] This paper lacks a sufficient motivation for the importance of carefully selecting the function class for balancing.** The authors do discuss the computational cost associated with choosing numerous balancing functions in Section 3.2. However, the original paper [1] suggests that even selecting a single function for balancing can yield performance improvements. It is essential to elucidate why the choice of multiple balancing functions is necessary and why the specific type of function is critical. In my view, the motivation may be derived from Corollary 1 to some extent, which asserts that certain types of balancing functions introduce bias (correct me if I am wrong). Nonetheless, it would be valuable to provide a more insightful explanation in the introduction section.\n\n**Response:** We thank the reviewer for the careful and insightful thoughts, and **have revised the Introduction Section** in our revised manuscript.\n\n-\tFirst, we **agree** that \u201cthe original paper [1] suggests that even selecting a single function for balancing can yield performance improvements\u201d, and **our experimental results also prove this.**\n\n-\tHowever, on the one hand, **it is not realistic to balance all possible functions for a specific model** using only finite samples.\n\n-\tOn the other hand, **balancing only a single function is not sufficient for IPS and DR methods to achieve unbiased learning** (see Corollary 1 for the formal theoretical results).\n\n-\tThus, **it is necessary to discuss which functions should be more favored to be balanced for the IPS and DR estimators**, resulting in **smaller estimation biases of the ideal loss** and enhanced performance of unbiased learning.\n\n**2. Clarity**\n\n> **[W2] The optimization problems in Sections 4.2 and 4.4 are perplexing. The rationale behind minimizing the sum of $g(\\cdot)\\log g(\\cdot)$ as opposed to a standard cross-entropy loss is unclear and confusing. The purpose of constraining the sum of $o_{u, i}g(\\cdot)$ to be one and the meaning of the third constraint require more detailed explanation and motivation. The authors should clarify the significance and reasoning behind each statement in the proposed optimization problem.**\n\n**Response:** We thank the reviewer for pointing out this issue, and **have added detailed explanations regarding the optimization problems in Sections 4.2 and 4.4.** Both of the optimization problems consist of the following three features.\n\n-\tFirst, **the objective function** is the **empirical entropy of the balancing weights**, by the principle of maximum entropy, it **reaches the maximum value when the balancing weights are uniform**, thus **effectively avoiding high variance due to extremely small propensities.**\n\n-\tSecond, **the balancing constraints** are imposed to equalize the selected covariate functions between the observed and missing samples.\n\n-\tThird, **two normalization constraints are imposed**, which implies that the weights sum to the normalization constant of one, and the **nonnegativity of the balancing weights**, making the empirical entropy as the objective function well-defined.\n\n-\tFor the optimization problem in Section 4.4, we **further introduce a slack variable $\\xi_j$** for each balancing function and **a pre-specified threshold $C$**, since **achieving strict balancing on all balancing functions is usually infeasible** as the number of balancing functions increases.\n\n> **[W3] Several theorems and corollaries, such as Theorem 1, Corollary 1, and Lemma 2, lack supporting proofs. In particular, the proof of Corollary 1, which appears directly relevant to the paper's motivation, is crucial.**\n\n**Response:** We really apologize for the lack of supporting proofs, and **have added detailed proofs for Theorem 1, Corollary 1, and Lemma 2 in Appendix A.**\n\n> **[W4] The paper suggests that \"balancing propensity can reduce the generalization bound\", but this claim is not clearly evident from Theorem 2, which seems to provide a bound without a comparative analysis.**\n\n**Response:** We thank the reviewer for pointing out this issue, and **have added detailed discussions in Section 4.5,** explaining the reason why the derived generalization bound in RKHS is able to be greatly reduced when adopting the proposed adaptive KBDR learning approach as in Alg. 1.\n\n-\tFor the **first term in the generalization bound**, the prediction model minimizes the loss $\\mathcal{L}_{KBDR}(\\theta)$ during the model training phase.\n\n-\tFor the **second term in the generalization bound**, as shown in Theorem 1 and Corollary 1, the proposed adaptive kernel balancing method can automatically choose the balancing functions that most need to be balanced to reduce the bias of the KBDR estimator."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9073/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695639497,
                "cdate": 1700695639497,
                "tmdate": 1700695639497,
                "mdate": 1700695639497,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LJXYUreNz7",
                "forum": "Ffjc8ApSbt",
                "replyto": "zlpp5feXoO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9073/Reviewer_CJCu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9073/Reviewer_CJCu"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your responses"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThanks for the responses. The responses address the most of my concerns. While I still hope for an intuitive example or explanation illustrating why \"balancing only a single function is not sufficient for IPS and DR methods to achieve unbiased learning\", referencing Corollary 1 is enough for conveying the paper's motivation to readers.\n\nAdditionally, please remember to update the paper's title to reflect the shift from causal balancing (CB) to kernel balancing (KB).\n\nOverall, I decide to raise my score to 6."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9073/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714991193,
                "cdate": 1700714991193,
                "tmdate": 1700714991193,
                "mdate": 1700714991193,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7dqgGQjcej",
                "forum": "Ffjc8ApSbt",
                "replyto": "vj5yJYwDz1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9073/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9073/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the timely response and raising the score, below we offer an intuitive example"
                    },
                    "comment": {
                        "value": "> **[Intuitive Example] While I still hope for an intuitive example or explanation illustrating why \"balancing only a single function is not sufficient for IPS and DR methods to achieve unbiased learning\", referencing Corollary 1 is enough for conveying the paper's motivation to readers.**\n\n**Response:** Let $\\phi(X)$ be a vector of functions of $X$ to be balanced, such as $\\phi(X)=\\left(X, X^2\\right)$. It assign a balancing weight $w_{u, i}$ to each observed user-item pair $\\\\{(u, i): o_{u, i}=1\\\\}$ by\n$$\n\\begin{aligned}\n& \\min_{w_{u, i}: o_{u, i}=1} \\sum_{(u, i): o_{u, i}=1} w_{u, i} \\log w_{u, i} \\\\\n& \\text { subject to } w_{u, i} \\geq 0 \\\\\n& \\sum_{(u, i): o_{u, i}=1} w_{u, i}=1, \\sum_{(u, i): o_{u, i}=1} w_{u, i} \\phi\\left(X_{u, i}\\right)=\\tilde{\\phi}, \\quad \\text { (the balancing constraints)},\n\\end{aligned}\n$$\nwhere $\\tilde{\\phi}=\\frac{1}{|\\mathcal{D}|} \\sum_{(u, i)\\in\\mathcal{D}} \\phi\\left(X_{u, i}\\right)$.\n\n- **Intuitively, if $e_{u, i}$ is a linear function of $\\phi(X)=\\left(X, X^2\\right)$, that is, $e_{u, i}=\\phi\\left(X_{u, i}\\right)^T \\beta+\\epsilon_{u, i}, \\mathbb{E}\\left[\\epsilon_{u, i} \\mid X_{u, i}\\right]=0$. Then the IPS estimator with the balancing weights $w_{u, i}$ is unbiased**\n$$\n\\sum_{(u, i): o_{u, i}=1} \\hat w_{u, i} e_{u, i}=\\sum_{(u, i): o_{u, i}=1} \\hat w_{u, i} \\phi(X_{u, i})^T \\beta+\\sum_{(u, i): o_{u, i}=1} \\hat w_{u, i} \\epsilon_{u, i} \\approx \\frac{1}{|\\mathcal{D}|} \\sum_{(u, i)\\in\\mathcal{D}} \\phi(X_{u, i})^T \\beta+0 \\rightarrow \\frac{1}{|\\mathcal{D}|} \\sum_{(u, i)\\in\\mathcal{D}} e_{u, i} \\quad \\text{(ideal loss)},\n$$\nwhere the approximately equal holds via the **balancing constraints** in the optimization problem.\n\n- **Instead, if $e_{u, i}$ is not a linear function of $\\phi(X)=\\left(X, X^2\\right)$, then it is clear that the IPS estimator with balancing weights $w_{u, i}$ is not unbiased, and only balancing $\\phi(X)=\\left(X, X^2\\right)$ is not enough.**\n\n***\n\nPlease kindly refer to the intuitive example we provided further above. We are glad to know that your concerns have been effectively addressed. We are very grateful for your constructive comments and questions, which helped improve the clarity and quality of our paper. Thanks again!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9073/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717124758,
                "cdate": 1700717124758,
                "tmdate": 1700717154620,
                "mdate": 1700717154620,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]