[
    {
        "title": "Time-sensitive Weight Averaging for Practical Temporal Domain Generalization"
    },
    {
        "review": {
            "id": "xSWOlhfrDn",
            "forum": "CSm099mlOL",
            "replyto": "CSm099mlOL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6406/Reviewer_c3ap"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6406/Reviewer_c3ap"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies a specific case of domain adaptation over a sequence of tasks. Authors assume that distribution between consecutive domain change smoothly. Under this assumption, they propose Time-sensitive Weight Averaging, a learning approach based on weight averaging. The authors also present an experimental study of the proposed algorithms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well written and easy to follow.\n\nThe use of weight averages is one of the most common proposals to adapt to changes over time. The paper makes a good review of state-of-the-art methods that allow to understand the contributions of the proposed method.\n\nThe idea of using the time stamp is new and very interesting. In addition, the proposed methods provide theoretical guarantees.\n\nThe authors perform extensive numerical experiments"
                },
                "weaknesses": {
                    "value": "It will be great to know how much this assumption is agreed and violated on individual tasks on experiments. \n\nIt would be nice if the theoretical guarantees appeared in the paper instead of in the appendices.\n\nI think one improvement the authors can consider is to add some discussions about the computational complexity and running times.\n\nhaving explanations about methods used for comparison could  significantly increase the readability.\n\nThe authors' assumption about the distribution is the usual assumption made in supervised classification under concept drift (References below). I think the authors should mention that this is a common assumption (gradual drift assumption) in supervised classification under concept drift as well as describe how the methods that make this assumption adapt to changes in the distribution. \n\n\u017dliobait\u0117, I. (2010). Learning under concept drift: an overview. arXiv preprint arXiv:1010.4784.\n\nElwell, R., & Polikar, R. (2011). Incremental learning of concept drift in nonstationary environments. IEEE Transactions on Neural Networks, 22(10), 1517-1531."
                },
                "questions": {
                    "value": "How does the method perform if you use a dataset with tasks in which the distribution does not change? For example MNIST dataset. \nOr what happens if the distribution changes too fast?\nCould the method work well but the theoretical guarantees would not be true?\n\nCan the method be used on text datasets?The authors use the image datasets from the wild time library. Could the method be extended to the other three datasets?\n\nCan the method be used for any loss function?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6406/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698484086664,
            "cdate": 1698484086664,
            "tmdate": 1699636712777,
            "mdate": 1699636712777,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "P4ItvSBlIO",
                "forum": "CSm099mlOL",
                "replyto": "xSWOlhfrDn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6406/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6406/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer c3ap"
                    },
                    "comment": {
                        "value": "> It will be great to know how much this assumption is agreed and violated on individual tasks on experiments.\n\nFor synthetic datasets like Rotated MNIST, the assumption of smooth distribution shifts holds because distribution shifts on such datasets are controllable. For real-world datasets, the performance variation of the ERM baseline across different domains can serve as an intuitive indicator. If the changes in ERM performance are smooth, it suggests that distribution shifts are also smooth.\n\n> It would be nice if the theoretical guarantees appeared in the paper instead of in the appendices.\n\nDue to page limitations, we had to move the theoretical section to the appendix. We will consider incorporating some key parts into the main text in the revised version.\n\n> I think one improvement the authors can consider is to add some discussions about the computational complexity and running times.\n\nYes, we discussed efficiency in Table 11 of the paper. We further add more efficiency and comparisons in the following table.\n\n|      Dataset     |   CLEAR-10  |              |  CLEAR-100  |              |     FMoW    |              |\n|:----------------:|:-----------:|:------------:|:-----------:|:------------:|:-----------:|:------------:|\n|                  | Accuracy(%) | Cost (hours) | Accuracy(%) | Cost (hours) | Accuracy(%) | Cost (hours) |\n|       CIDA       |     87.3    |      6.8     |     69.2    |     14.7     |      \u00d7      |       \u00d7      |\n|     AdaGraph     |     75.5    |      3.4     |     50.2    |     10.3     |      \u00d7      |       \u00d7      |\n| GI |     82.2    |      3.5     |     54.3    |     14.6     |      \u00d7      |       \u00d7      |\n|       LSSAE      |      86     |     17.5     |     62.1    |     58.3     |     58.7    |     43.2     |\n|        DDA       |     86.3    |     15.6     |     41.2    |     42.3     |     49.2    |     33.5     |\n|        ERM       |     86.3    |      1.7     |     68.3    |      8.2     |     63.5    |      4.2     |\n|        TWA       |     88.5    |      1.9     |     72.1    |      8.4     |     69.5    |      5.3     |\n\n\n> Having explanations about methods used for comparison could significantly increase the readability.\n\nThank you for the suggestion; we will attempt to incorporate more detailed descriptions of the baselines in the revised version.\n\n> The authors' assumption about the distribution is the usual assumption made in supervised classification under concept drift (References below). I think the authors should mention that this is a common assumption (gradual drift assumption) in supervised classification under concept drift as well as describe how the methods that make this assumption adapt to changes in the distribution.\n\nThank you for the suggestion. We will consider including more discussions regarding concept drift.\n\n> How does the method perform if you use a dataset with tasks in which the distribution does not change? For example MNIST dataset. Or what happens if the distribution changes too fast? Could the method work well but the theoretical guarantees would not be true?\n\nWe will add the corresponding ablation study in the revised version. In theory, even if the distribution does not change, TWA can still improve performance. This is because weight averaging itself is a commonly used technique to enhance the generalization ability.\n\n> Can the method be used on text datasets? The authors use the image datasets from the wild time library. Could the method be extended to the other three datasets?\n\nYes, it can be used on text datasets. In the Section C.2 of appendix, we simply evaluated TWA on 2 text datasets, arXiv and HuffPost. The results show that TWA can also boost generalization ability on text datasets.\n\n> Can the method be used for any loss function?\n\nIn theory, TWA can be combined with various loss functions. However, it's unclear how different loss functions might impact the performance of TWA."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6406/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700287478783,
                "cdate": 1700287478783,
                "tmdate": 1700287520898,
                "mdate": 1700287520898,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "y36NHjke8U",
            "forum": "CSm099mlOL",
            "replyto": "CSm099mlOL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6406/Reviewer_Jcbo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6406/Reviewer_Jcbo"
            ],
            "content": {
                "summary": {
                    "value": "This article addresses the issue of enhancing the practicality of Temporal Domain Generalization (TDG). This paper comprehensively considers various aspects of TDG tasks, including methods, datasets, evaluation settings, comparisons with previous methods and more. \nIt makes the following contributions:\n- It analyzes the limitations of previous TDG approaches and summarizes three design principles for TDG methods: 1) Time-sensitive model, 2) Generic method, and 3) Realistic evaluation.\n- In the context of TDG tasks, the article improves the Weight Averaging method by automatically learning the optimal weight averaging strategy for different time intervals.\n- The authors enhance the TDG benchmark by incorporating more realistic datasets, namely CLEAR-10, CLEAR-100, Yearbook, and FMoW-Time, into the TDG setting.\n- Comprehensive evaluation results are provided, including comparison with other methods using both new and previous datasets, and results on both vision and language tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper provides valuable analysis, summary, and design principles for improving the practicality of Temporal Domain Generalization. The proposed three design principles are a meaningful step towards enhancing the practicality of TDG.\n- This paper proposed a novel TDG method, TWA, which is the first to apply weight averaging toTDG.  And TWA further makes the weight averaging learnable to adapt to the smooth distribution shift setting of TDG. \n- This paper is the first one to evaluate TDG methods on larger and more complex datasets, which contributes to the overall improvement of TDG's practicality in complex application scenarios.  Evaluation across new and common TDG benchmarks, along with efficiency assessments, allows us to obtain a comprehensive assessment of the proposed method.\n- Strong performance. Comprehensive experiments are conducted, outperforming other methods across various benchmarks and tasks with good efficiency. The proposed TWA is a \"simple yet effective\" method that can be easily applied and adapted to different tasks, models, and datasets."
                },
                "weaknesses": {
                    "value": "Like most other TDG works, TWA is also limited by some common problems within the TDG task:\n- TWA also relies on the \u201csmooth distribution shift\u201d assumption, which could limit its potential applications.\n- Evaluations are limited to relatively \u201csimple\u201d tasks, e.g. most TDG methods are evaluated with classification and regression tasks only. It\u2019s unclear whether these TDG methods could still work when generalized to more complex tasks, such as image segmentation or common object detection."
                },
                "questions": {
                    "value": "Overall, I have a positive impression of this paper. The limitations I listed are common for most existing TDG works, which do not hinder TWA from being a novel TDG method. And I got questions about the implementation details:\n\n- When you are applying TWA with different tasks, datasets or models, are you using a constant \u201cnumber of snapshots\u201d? If not, is there any strategy to select the best number of snapshots for different application settings? It would be helpful if there could be results of \u201cnumber of snapshots vs. task complexity\u201d."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6406/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698555649111,
            "cdate": 1698555649111,
            "tmdate": 1699636712562,
            "mdate": 1699636712562,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x1ISxN57bC",
                "forum": "CSm099mlOL",
                "replyto": "y36NHjke8U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6406/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6406/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Jcbo"
                    },
                    "comment": {
                        "value": "> TWA also relies on the \u201csmooth distribution shift\u201d assumption, which could limit its potential applications.\n\nYes, TWA also follows the \u201csmooth distribution shift\u201d assumption of TDG tasks.\n\n> Evaluations are limited to relatively \u201csimple\u201d tasks, e.g. most TDG methods are evaluated with classification and regression tasks only. It\u2019s unclear whether these TDG methods could still work when generalized to more complex tasks, such as image segmentation or common object detection.\n\nMoving from simple datasets and network structures to realistic datasets with larger networks is the first step in making TDG practical. We will explore the potential applications of TDG in various more complex tasks in future work.\n\n> When you are applying TWA with different tasks, datasets or models, are you using a constant \u201cnumber of snapshots\u201d? If not, is there any strategy to select the best number of snapshots for different application settings? It would be helpful if there could be results of \u201cnumber of snapshots vs. task complexity\u201d.\n\nFor different tasks, we may use different numbers of snapshots. We will add the specific numbers of snapshots to the experimental details. \n\nBased on our experience, it is better to use more snapshots when dealing with complex tasks. Conversely, selecting too many snapshots on simple tasks may harm performance.\n\nAs for the  \u201cnumber of snapshots vs. task complexity\u201d, we refer to Fig.3 (b) to show how performance changes with the number of snapshots on a relatively complex task. We further vary the number of snapshots on the Elec2 dataset, which is a relatively simple task, and get the following results.\n\n| Number of snapshots |      2     |      3     |      5     |\n|:-------------------:|:----------:|:----------:|:----------:|\n|     Accuracy (%)    | 86.5 \u00b1 1.1 | 84.8 \u00b1 2.0 | 83.2 \u00b1 1.7 |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6406/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700286935168,
                "cdate": 1700286935168,
                "tmdate": 1700286935168,
                "mdate": 1700286935168,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rPoQz7WVj2",
            "forum": "CSm099mlOL",
            "replyto": "CSm099mlOL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6406/Reviewer_Zoin"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6406/Reviewer_Zoin"
            ],
            "content": {
                "summary": {
                    "value": "Temporal Domain Generalization (TDG) aims to tackle temporal distribution shifts in models without future sample access. Traditional TDG methods, often oversimplified, were either time-sensitive or tried to estimate optimal model parameters for each temporal domain. To improve TDG's practical applicability, the authors introduce three main principles: a time-sensitive model, a generic method, and realistic evaluation. Following these principles, they present Time-sensitive Weight Averaging (TWA), which uses weight averaging of specialists for every temporal domain and a selector network trained on timestamps. TWA's effectiveness is confirmed through experiments on various benchmarks, showing up to a 4% improvement in accuracy over traditional methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors emphasize the importance of addressing Temporal Domain Generalization (TDG) in a more comprehensive manner. They introduce three key principles for TDG method design, which ensure that the approach is time-sensitive, generic, and evaluated under realistic conditions.\n2. The TWA approach is simple and effective. It leverages weight averaging for every temporal domain and is complemented by a selector network that estimates the best coefficients based on timestamp input.\n3. TWA has been tested extensively on multiple realistic benchmarks and demonstrated great performance."
                },
                "weaknesses": {
                    "value": "1. The motivation is a little weak in my opinion. Specifically, the authors criticize that existing methods rely too much on time-sensitive mechanisms, which may cause troubles for large generic models. However, given the task is **temporal** DG, it is quite intuitive to rely on time-sensitive information, and authors seem to agree with the strength of using the time-sensitive information. For most of the generic models like ChatGPT, the problem of DG may not be a big deal. Applying weight averaging to these foundation models is also a resource-consuming task.\n\n2. The technical design is a little trivial and odd to me. The motivation says existing methods rely too much on temporal information, we should use weight averaging instead. However, the proposed method still seems to leverage a selector network to model the trajectory of the distribution shift over time. In addition, the model is built upon SWAD (Cha et al., 2021) with some additional contributions to learning the averaging coefficient.\n\n3. Finally, the presentation of the manuscript still has a large room for improvement. For example, all the notations in Figure 1 are not introduced, which makes the visual presentation not self-contained. In addition, some sentences are hard to understand, e.g., \"They assumed that understanding say how a mobile phone\u2019s appearance has changed in the past may help predict future changes.\""
                },
                "questions": {
                    "value": "1. What is the definition of model snapshots? Is it the same thing as the model parameters?\n2. Following the previous question, if the model snapshot equals the model parameter, how can it be sampled? Is the model a Bayesian Neural Network?\n3. One major contribution that the authors claim is \"The method can be easily combined with various architectures and tasks, requiring as few architecture modifications as possible\". Therefore, it makes people expect there may be some experiments showing the proposed method is flexible with different architectures. In addition, runtime comparisons with other methods can also be demonstrated. However, Table 11 only contains the runtime comparison against itself. \n4. In the experiments, why the model is heavily compared with many DG methods, like ERM, MTL, GROUPDRO, etc? The major comparison between the proposed method and TDG methods is better to be placed in the main content instead of the appendix.\n5. The most up-to-date TDG method DRAIN is not compared in many datasets. Could the authors explain what exactly the \"highly task-specific design\" is in DRAIN (Bai et al., 2022)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6406/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698594689221,
            "cdate": 1698594689221,
            "tmdate": 1699636712359,
            "mdate": 1699636712359,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0nA0mCEQyy",
                "forum": "CSm099mlOL",
                "replyto": "rPoQz7WVj2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6406/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6406/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Zoin"
                    },
                    "comment": {
                        "value": "> The motivation is a little weak in my opinion. Specifically, the authors criticize that existing methods rely too much on time-sensitive mechanisms, which may cause troubles for large generic models. However, given the task is temporal DG, it is quite intuitive to rely on time-sensitive information, and authors seem to agree with the strength of using the time-sensitive information. For most of the generic models like ChatGPT, the problem of DG may not be a big deal. Applying weight averaging to these foundation models is also a resource-consuming task.\n\nWe need to clarify that we are not criticizing using time-sensitive mechanisms. In fact, we also believe that time-sensitive mechanisms are essential for TDG tasks. Our difference in motivation from previous works lies in that, previous works primarily concentrated on improving generalization performance through better utilization of temporal information, and we aim at incorporating more practical considerations into the design and evaluation of TDG. And we believe these 2 motivations are equally important.\n\nThe major issue we are addressing is that the time-sensitive mechanisms within many existing methods cannot generalize well to larger and more realistic application scenarios. As demonstrated in Table 1 of our global response, they either result in a severe performance drop or lead to a significant increase in training costs. These results also underscore the importance of considering TDG issues from a practical view.\n\nWhile large models trained on large datasets may directly achieve strong generalization, there are many other application scenarios that models and dataset are not \u201clarge\u201d enough to make DG/TDG less necessary. However, these scenarios could still be larger in scale than those studied in previous TDG approaches. Therefore, incorporating more practical considerations into TDG is a necessary step in bridging TDG with real-world applications.\n\n> The technical design is a little trivial and odd to me. The motivation says existing methods rely too much on temporal information, we should use weight averaging instead. However, the proposed method still seems to leverage a selector network to model the trajectory of the distribution shift over time. In addition, the model is built upon SWAD (Cha et al., 2021) with some additional contributions to learning the averaging coefficient.\n\nRather than opposing using temporal information, we are actually suggesting to improve the way to use temporal information from a practical perspective, making TDG methods more generic, easier to be applied to various application scenarios. And the weight averaging method exhibited the desired properties, so we chose to combine the weight averaging method with the temporal mechanism.\n\nIn the global response, we reiterated our novelty and contribution. When specifically comparing with SWAD, we drew inspiration from SWAD in the aspects of theoretical guarantees and overfit-aware sampling scheduling. And the other parts, including model training (e.g. normalization), model averaging approach (dense or not), and sampling strategy for TDG (late sampling) are totally different from SWAD. \n\n> Finally, the presentation of the manuscript still has a large room for improvement. For example, all the notations in Figure 1 are not introduced, which makes the visual presentation not self-contained. In addition, some sentences are hard to understand, e.g., \"They assumed that understanding say how a mobile phone\u2019s appearance has changed in the past may help predict future changes.\"\n\nThank you for pointing out these areas for improvement. We will refine these details in the revised version to ensure clearer expressions."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6406/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700286046030,
                "cdate": 1700286046030,
                "tmdate": 1700286046030,
                "mdate": 1700286046030,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OuOv3rPQsg",
            "forum": "CSm099mlOL",
            "replyto": "CSm099mlOL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6406/Reviewer_bw8U"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6406/Reviewer_bw8U"
            ],
            "content": {
                "summary": {
                    "value": "This paper extends weight averaging technique for domain generalization in non-stationary environments. Specifically, the authors employ the Time2Vec model to compute time-sensitive weights, facilitating the creation of an optimal model at each time step by leveraging existing model snapshots. Experimental findings across various real-world datasets consistently showcase the superior performance of the proposed approach compared to baseline methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper tackles an important issue of temporal domain generalization in machine learning under distribution shift.\n\n- The experiment covers more real-world dataset compared to existing works for temporal domain generalization.\n\n- Code is made available for the purpose of ensuring reproducibility."
                },
                "weaknesses": {
                    "value": "- The authors assert that they introduce a comprehensive benchmark, encompassing several realistic datasets and baselines for temporal domain generalization. However, the comparison with existing temporal domain generalization methods is conducted solely on the Portrait dataset. For other real datasets such as CLEAR, FMoW, arXiv, and HuffPost, the authors only provide comparisons with the weight-averaging method (SWAD [1]). It is recommended that the authors expand their analysis to include results from other methods, particularly those tailored for temporal domain generalization (e.g., GI [2], LSSAE [3], DDA [4], DRAIN [5], DPNET [6]), on these datasets to bolster the strength of their contribution. (NOTE: This is a significant concern. I am open to revising my evaluation if the authors can address and resolve this issue.)\n\n- The proposed method does not strictly adhere to the problem formulation outlined in Equation (1). Notably, while the original problem is cast as a bi-level optimization, the proposed algorithm utilizes a two-phase training approach to optimize the two terms sequentially. The authors should clarify the rationale behind their model design.\n\n- The two-stage training approach employed by the proposed method raises concerns about both time (for training the selector network) and space (for storing model snapshots) complexity.\n\n- The explanation of how the theoretical results influence the development of the proposed method is somewhat lacking in clarity. From my point of view, the current theoretical findings appear to be more apt for providing a theoretical assurance of TWA rather than serving as the primary guiding influence on the algorithm's design.\n\n- Could the authors offer more detailed insights to distinguish their work from SWAD? From my point of view, the technical contribution seems somewhat limited. It appears that the primary contribution lies in the utilization of Time2Vec to adapt the weight-averaging method to the temporal-shift scenario.\n\nReferences:\n\n[1] Cha, Junbum, et al. \"Swad: Domain generalization by seeking flat minima.\" Advances in Neural Information Processing Systems 34 (2021): 22405-22418.\n\n[2] Nasery, Anshul, et al. \"Training for the future: A simple gradient interpolation loss to generalize along time.\" Advances in Neural Information Processing Systems 34 (2021): 19198-19209.Nasery, Anshul, et al. \"Training for the future: A simple gradient interpolation loss to generalize along time.\" Advances in Neural Information Processing Systems 34 (2021): 19198-19209.\n\n[3] Qin, Tiexin, Shiqi Wang, and Haoliang Li. \"Generalizing to Evolving Domains with Latent Structure-Aware Sequential Autoencoder.\" International Conference on Machine Learning. PMLR, 2022.\n\n[4] Zeng, Qiuhao, et al. \"Foresee What You Will Learn: Data Augmentation for Domain Generalization in Non-Stationary Environments.\" arXiv preprint arXiv:2301.07845 (2023).\n\n[5] Bai, Guangji, Chen Ling, and Liang Zhao. \"Temporal Domain Generalization with Drift-Aware Dynamic Neural Networks.\" arXiv preprint arXiv:2205.10664 (2022).\n\n[6] Wang, William Wei, et al. \"Evolving Domain Generalization.\" arXiv preprint arXiv:2206.00047 (2022)."
                },
                "questions": {
                    "value": "Questions are given in the Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6406/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6406/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6406/Reviewer_bw8U"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6406/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839248512,
            "cdate": 1698839248512,
            "tmdate": 1699636712069,
            "mdate": 1699636712069,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pDzg7jXttN",
                "forum": "CSm099mlOL",
                "replyto": "OuOv3rPQsg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6406/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6406/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bw8U"
                    },
                    "comment": {
                        "value": "> The authors assert that they introduce a comprehensive benchmark, encompassing several realistic datasets and baselines for temporal domain generalization. However, the comparison with existing temporal domain generalization methods is conducted solely on the Portrait dataset. For other real datasets such as CLEAR, FMoW, arXiv, and HuffPost, the authors only provide comparisons with the weight-averaging method (SWAD [1]). It is recommended that the authors expand their analysis to include results from other methods, particularly those tailored for temporal domain generalization (e.g., GI [2], LSSAE [3], DDA [4], DRAIN [5], DPNET [6]), on these datasets to bolster the strength of their contribution. (NOTE: This is a significant concern. I am open to revising my evaluation if the authors can address and resolve this issue.)\n\nWe appreciate your suggestions. In the following, we are glad to provide more results of TDG baselines, the reason why some results are not available and our motivation to split the comparison into 2 parts. \n\n### &nbsp;&nbsp; Accuracy and efficiency evaluation on CLEAR-10 and CLEAR-100\n\nFirst, we evaluate various TDG methods on CLEAR-10 and CLEAR-100. The last domain is used as the target domain. These methods include CIDA, AdaGraph, GI, DDA, and LSSAE. The results are presented in the following Table. It can be observed that TWA exhibits a more pronounced advantage when compared to these TDG methods on CLEAR-10 and CLEAR-100.\n\n|      Dataset     |   CLEAR-10  |              |  CLEAR-100  |              |     FMoW    |              |\n|:----------------:|:-----------:|:------------:|:-----------:|:------------:|:-----------:|:------------:|\n|                  | Accuracy(%) | Cost (hours) | Accuracy(%) | Cost (hours) | Accuracy(%) | Cost (hours) |\n|       CIDA       |     87.3    |      6.8     |     69.2    |     14.7     |      \u00d7      |       \u00d7      |\n|     AdaGraph     |     75.5    |      3.4     |     50.2    |     10.3     |      \u00d7      |       \u00d7      |\n| GI |     82.2    |      3.5     |     54.3    |     14.6     |      \u00d7      |       \u00d7      |\n|       LSSAE      |      86     |     17.5     |     62.1    |     58.3     |     58.7    |     43.2     |\n|        DDA       |     86.3    |     15.6     |     41.2    |     42.3     |     49.2    |     33.5     |\n|        ERM       |     86.3    |      1.7     |     68.3    |      8.2     |     63.5    |      4.2     |\n|        TWA       |     88.5    |      1.9     |     72.1    |      8.4     |     69.5    |      5.3     |\n\n### &nbsp;&nbsp; Missing Comparison with GI and DRAIN\n\n**GI** is hard to be adapted to large datasets due to the gradient interpolation loss. Training a network with TReLU is feasible in terms of efficiency and accuracy. However, simply using GI loss leads to a significant performance drop. While finding adaptation with extensive experiments might alleviate this issue, the high cost of GI loss prevents us from this. On CLEAR-10, a single finetuning epoch with GI loss requires 23 GPU hours, and on CLEAR-100, each epoch even demands 153 GPU hours.\n\n**DRAIN** is missing as we cannot facilitate its proper training on new benchmarks. Methodologically, DRAIN is generic and can easily integrate with various model architectures. However, its implementation and empirical evaluation are primarily designed for small datasets, such as MNIST, and small models like MLP or 2-layer CNN. When we attempt to apply DRAIN to larger datasets, such as CLEAR, and larger CNN networks, such as ResNet, we find it hard to make the network learn properly, resulting in random guessing networks.\n\n### &nbsp;&nbsp; 2 parts of comparisons\n\nWhen many TDG methods struggle to generalize to the new settings, comparing with them only using new benchmarks and TDG methods may raise concerns about unfair comparisons. Perhaps better method adaptations could enhance the accuracy. But the significant cost increases are basically unavoidable, which also makes it hard to explore better adaptations. Therefore, we additionally selected some datasets commonly used by these TDG methods to enhance the fairness and comprehensiveness of the evaluation.\n\n### &nbsp;&nbsp; Other relevant clarifications.\n\n**CIDA and AdaGraph** are used as TDG baselines, as they are also selected as baselines by most other TDG methods. And TDG can be regarded as a special case of their original Continuous Domain Adaptation task.\n\n**ArXiv and HuffPost** are relatively less crucial, as they are simply used to test whether TWA can generalize to NLP tasks, instead of serving as the major benchmarks. And they also require using pretrained weights, making many TDG methods cannot be evaluated.\n\n**DPNET** seems to be not published yet."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6406/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700285492984,
                "cdate": 1700285492984,
                "tmdate": 1700285492984,
                "mdate": 1700285492984,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2EZ9l1xANs",
                "forum": "CSm099mlOL",
                "replyto": "OuOv3rPQsg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6406/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6406/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer bw8U"
                    },
                    "comment": {
                        "value": "> The proposed method does not strictly adhere to the problem formulation outlined in Equation (1). Notably, while the original problem is cast as a bi-level optimization, the proposed algorithm utilizes a two-phase training approach to optimize the two terms sequentially. The authors should clarify the rationale behind their model design.\n\nWe provide Eq.1 as a way to introduce the TDG formulationally generally. However, Eq. 3 represents the specific form of Eq. 1 that we used in our paper. \n\n> The two-stage training approach employed by the proposed method raises concerns about both time (for training the selector network) and space (for storing model snapshots) complexity.\n\nAccording to Table 11, we can observe that TWA does not significantly increase training time, only resulting in an increase of 2-20% of the total cost. As for the storage of snapshots, on one hand, the memory consumption of model weights is significantly smaller than that of feature maps. On the other hand, in practical applications, we can categorize a period of time into a domain, where all data within that period share the same domain index. This allows us to directly combine weights to obtain a single averaged model to save memory. For instance, store all snapshots and selectors on servers and use the single averaged model on memory-constrained devices.\n\n> The explanation of how the theoretical results influence the development of the proposed method is somewhat lacking in clarity. From my point of view, the current theoretical findings appear to be more apt for providing a theoretical assurance of TWA rather than serving as the primary guiding influence on the algorithm's design.\n\nYes, in the Section 3.3, we are stating \u201dwe have derived some **theoretical guarantees** on the ability of TWA to generalize from past to future data\u201d\n\n> Could the authors offer more detailed insights to distinguish their work from SWAD? From my point of view, the technical contribution seems somewhat limited. It appears that the primary contribution lies in the utilization of Time2Vec to adapt the weight-averaging method to the temporal-shift scenario.\n\nIn the global response, we reiterated our novelty and contribution. When specifically comparing with SWAD, we drew inspiration from SWAD in the aspects of theoretical guarantees and overfit-aware sampling scheduling. And the other parts, including model training (e.g. normalization), model averaging approach (dense or not), and sampling strategy for TDG (late sampling) are totally different from SWAD. We also want to argue that we are specifically targeting the TDG task, and excluding the TDG-specific algorithm designs and theoretical analysis from technical contributions is unreasonable and not objective."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6406/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700285720394,
                "cdate": 1700285720394,
                "tmdate": 1700285720394,
                "mdate": 1700285720394,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]