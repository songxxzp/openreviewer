[
    {
        "title": "Maximizing LLMs Potential: Enhancing Mongolian Chinese Machine Translation with RL Agents and Adversarial Multi Knowledge Distillation"
    },
    {
        "review": {
            "id": "U5tA4soOaW",
            "forum": "29pGC6IYaL",
            "replyto": "29pGC6IYaL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1959/Reviewer_Mjnh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1959/Reviewer_Mjnh"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an innovative approach that combines multi-source knowledge distillation and incorporates Reinforcement\nLearning (RL) to help models acquire and transfer knowledge from LLMs more effectively. RL plays a crucial role in this, making dynamic decisions to determine useful information for low-resource translation models and how to extract it efficiently. They introduce a new reward function to comprehensively guide knowledge distillation and experiments show that this approach harnesses the potential\nof LLMs, significantly improving translation quality in low-resource settings."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper notices that the LLMs' potential in low-resource NMT\n- This paper tries multi-source distillation with RL method, it has somewhat novelty"
                },
                "weaknesses": {
                    "value": "- This paper is hard to read. In other words, its presentation is very poor. It seems that the authors do not spend enough time to prepare such a paper. Specifically, \"RL\" in the title; what is the test sets and setting on Table 1, why the results of ChatGPT or GPT4 are not presented? What is the i of M_t^i? What is the M_y. What do you mean by \"follow 11\", above Equation 11?  What is \"eq.3\"\n- This paper cannot be viewed as a solid science paper, as I cannot find enough information to ensure their experiments are reasonable and convincing. What is your test set in Table 2, are they officially released test sets?  Why the student model's lr is lower than the teacher model? How do you train the LLMs, i.e., the teacher models? Do you mean that you will SFT them with millions or billions datas?\n- The experiments in this paper are not enough, and miss some important baselines."
                },
                "questions": {
                    "value": "1 See above\n2 Why not put Figure 1 stick with its description in the main content?\n3 Why so many duplicate contents in model selection?\n4 Why not cite ChatGPT or GPT4 related paper rather than GPT3 in the first line?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1959/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1959/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1959/Reviewer_Mjnh"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1959/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697853008041,
            "cdate": 1697853008041,
            "tmdate": 1699636127380,
            "mdate": 1699636127380,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "pAw147WVds",
            "forum": "29pGC6IYaL",
            "replyto": "29pGC6IYaL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1959/Reviewer_NGkR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1959/Reviewer_NGkR"
            ],
            "content": {
                "summary": {
                    "value": "This paper has introduced a novel framework for multi-source distillation in the context of machine translation, employing reinforcement learning. To be specific, this framework seamlessly integrates three distinct components within a cohesive distillation strategy. The student model initially acquires information from various language models, each trained with diverse datasets, and subsequently undergoes training with the adaptive decision-making capabilities of an RL agent to enhance its overall performance. Additionally, a novel reward function is introduced, meticulously accounting for the intricate and multifaceted decision-making processes. Finally, this paper presents empirical results in the context of low-resource Mongolian-to-Chinese translation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is easily comprehensible and straightforward.\n- The authors conduct an ablation study to systematically assess the contributions of each individual component."
                },
                "weaknesses": {
                    "value": "- The observed improvements are relatively marginal when compared to other baseline methods.\n- The paper lacks a comprehensive set of experiments to conclusively establish the superiority of the proposed approach over other existing methods. The performance enhancement is primarily demonstrated through improvements in BLEU scores in various translation directions. However, I'm hard to find a qualitative assessment of the method's effectiveness and why the proposed techniques are advantageous in enhancing translation models. Since the authors mentioned that the consideration of dynamic changes in the distillation process is a key contribution of the paper, it would be good to provide additional experiments to demonstrate this."
                },
                "questions": {
                    "value": "- The experimental results appear to exhibit a marginal improvement. \n    -  Are these results(Table 2) indicative of the average outcomes achieved through multiple runs, and has statistical significance been established regarding the variance in performance between the baseline and the proposed model?\n\n- Does the performance trend change depending on the student model size?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1959/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1959/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1959/Reviewer_NGkR"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1959/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698682701650,
            "cdate": 1698682701650,
            "tmdate": 1699636127300,
            "mdate": 1699636127300,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "hinWS94HQS",
            "forum": "29pGC6IYaL",
            "replyto": "29pGC6IYaL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1959/Reviewer_hubM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1959/Reviewer_hubM"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors enhance the performance of the  low-resource Mongolian-to-Chinese machine translation by integrating multi-source knowledge distillation and reinforcement learning in large language model. Specfically, the authors propose a new reward function designed to guide the knowledge distillation process and introduce adversarial noise to improve the effectiveness of the knowledge distillation process."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors proposed to use LLMs as the teachter model to distill the task-specific models, this is a promising direction for achieving a balance between performance and efficiency. \n\n2. Experimental results in multiple translation tasks show the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1. Why have you selected Llama as one of the teacher models? Llama has not been trained on the languages referenced in the paper.\n2. I want to know the performance of the reward models. In recent work, the performance of the reward model determines the final outcome.\n3. Training a model based on Reinforcement Learning (RL) is challenging. Is it possible to directly use rejection sampling for knowledge distillation?"
                },
                "questions": {
                    "value": "See Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1959/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1959/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1959/Reviewer_hubM"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1959/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698891739385,
            "cdate": 1698891739385,
            "tmdate": 1699636127229,
            "mdate": 1699636127229,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]