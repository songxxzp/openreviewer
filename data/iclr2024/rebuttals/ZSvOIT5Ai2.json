[
    {
        "title": "Interpretable Concept Discovery and Learning from Pretrained Vision-Language Models"
    },
    {
        "review": {
            "id": "8HN0QAOeJV",
            "forum": "ZSvOIT5Ai2",
            "replyto": "ZSvOIT5Ai2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5828/Reviewer_PAjz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5828/Reviewer_PAjz"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a concept discovery and learning (CDL) framework aiming to better align the semantic representation for a category label and its relevant attribute labels. It is inspired by an observation that in current visual-language models (VLM), there are inconsistencies between class labels and the attributes.\n\nTo mitigate such inconsistency, given a category label (e.g., a panda), CDL queries a large language model (LLM) about useful visual features that helps recognition of that category. This process results in a concept-class corelation matrix. A mutual information inspired method is then used to obtain the most informative concepts for a specific class. In the end, each class will correspond to a 0-1 vector indicating whether a concept is useful for that class. The obtained concept vectors are then used to finetune the last projection layer of CLIP. Through this process, the paper claims that CDL helps visual-language model learn a better correspondence between class labels and the attributes.\n\nExperimental studies on ImageNet, Food-101, CIFAR-100, and a few smaller datasets were conducted to show the efficacy of the proposed method. A human study on the interpretability of the CDL framework is also provided."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is technically and intuitively correct.\n- Extensive experiments on different image datasets and human study about interpretability are conducted."
                },
                "weaknesses": {
                    "value": "- Table 1 is used as a support of the paper's claim about the misalignment of concepts and classes in CLIP. However, there is not a similar table for CLIP+CDL using the same evaluation to show that it does improve the alignment between classes and concepts. Current experiments only show that VLM+CDL becomes a good concept detector but does not say that it effectively relates concepts with classes.\n- Mutual information-based concept selection is correct but not new. Moreover, I do not see the necessity of introducing MI here. The paper should better rationalise the choice of this criterion.\n    * How does using MI differ from using the normalised cooccurrence frequency between categories and classes? \n    * Is there any example showing the concepts selected for different classes?\n- Missing details about the similarity threshold ``$\\texttt{th}$'' (page 5). There is no further discussion on it except in Section 3.2. \n    * What is the typical value for it? \n    * Is the parameter fixed for all the datasets? \n    * Is the framework sensitive to this parameter? \n- Missing details about human study. \n    * There are no error bars in Figure 4, making it difficult to tell how well the human annotators agree and how significantly CDL improved over prior methods. Since each data point has been annotated by three human annotators (as described in the appendix), there should be at least an error bar for each result in the figure. \n    * Moreover, there is no information about the interface viewed by the annotators. For example, Are samples fairly displayed to the annotators, removing the possible bias caused by the order or the position of display? \n- Clarity: what is the \"CDL+CDL\" setting in Table 2?\n- Typo: \"directlyed\" in page 2."
                },
                "questions": {
                    "value": "Please see my points above. As a summary, I expect answers regarding:\n1. Does VLM trained with CDL really better models the relation between concepts and classes?\n2. MI for concept selection is good, but why bother using it here? Any ablation study about it?\n3. Missing details about model design, for example the similarity threshold $\\texttt{th}$.\n4. Missing details about human study, for example the error bars."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5828/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5828/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5828/Reviewer_PAjz"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5828/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698657170570,
            "cdate": 1698657170570,
            "tmdate": 1699636614981,
            "mdate": 1699636614981,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ze9KXpYCbU",
                "forum": "ZSvOIT5Ai2",
                "replyto": "8HN0QAOeJV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5828/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5828/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer PAjz for the constructive suggestions and insightful questions!\n\n#### Q1: The improvement of CDL on alignment between classes and concepts\n\nIn order to demonstrate the ability of our CDL model to align concepts with classes, we add the experiment to compare our CDL model with VDES on zero-shot classification in Table 1 (last row, \u201cCDL + Concept\u201d). The results show that our fine-tuned CLIP model can achieve competitive performance while using only class-agnostic concepts. Notably, when VDES is modified to rely only on its concepts, the performance drops significantly (the row \u201cCLIP + Concept\u201d). The results illustrate that our CDL method can learn correct concept and class association and provide interpretability on how VLMs utilize primitive concepts for recognition, and that CDL can indeed improve alignment between classes and concepts.\n\n#### Q2: The necessity of Mutual Information\n\nAs illustrated in Figure 1 (fifth column, \u201cLabo\u201d), we observe that previous methods include many non-visual concepts (e.g. \u201ckeep the panda warm\u201d) in their discovered concepts. We hypothesize that the selection of such concepts would make the concept-based representation less visually discriminative when the concept vocabulary size is fixed. Hence, we introduce Mutual Information to select visually discriminative concepts. \n\nThe Mutual Information between the image-concept similarity and the ground-truth containment of the image for the concept can measure how well the concept can be recognized by the VLM and can select **visually salient** concepts. If we use other methods like normalized co-occurrence frequency between categories and classes to select concepts, it still cannot distinguish visual and non-visual concepts. For example, the non-visual concepts like \u201clong life span\u201d and \u201ca very rare animal\u201d also usually co-occur with \u201cgiant panda\u201d, and we need to introduce the vision information to filter them.  \n\nTo better showcase the effectiveness of the Mutual Information based concept selection, we add some examples of the concepts selected and excluded by the method in Table 9 of the Appendix. From the examples we can see that our method can effectively select visually discriminative concepts and exclude non-visual ones.\n\n#### Q3: Details about the threshold\n\nWe fully agree that adding more details of the similarity threshold is necessary. The similarity threshold is 0.9 for our current method. Intuitively, a lower threshold will select fewer concepts and a higher threshold will include more similar concepts. We add the details of the similarity threshold selection in the experiment section and show the performance of different thresholds on the CUB dataset in Table 6 of the Appendix. The results show that the performance of the CDL model is reasonably robust against the choice of the threshold.\n\n#### Q4: Details about human study\n\nThank you for your suggestions on the presentation of the human study.We agree that more details about human study need to be included. \n- Error bars about human evaluation: we conduct the Students\u2019 T-test to evaluate the statistical significance of the human evaluation results. The results are shown in Table 8 of the Appendix. From the results we can observe that both our concept learning and concept discovery method significantly outperform the baseline methods regarding the intervention, factuality and groundability metrics. We also report the pairwise annotator agreement score of our human evaluation to validate the effectiveness of our human evaluation.\n- Potential biases in the annotation process: for the annotation, we randomly shuffled the order of instances to remove possible biases. We add the examples of human annotation interfaces in Figure 5 of the Appendix. Since the annotator receives one image each time, the position will not bring biases to the result. \n\n#### Q5: Clarifying the notations in the tables\n\n For the notations `A+B` in Table 2, `A` represents the CLIP model and `B` represents the concept discovery method. `CLIP+LaBo` represents the original CLIP model with LaBo concepts. `CLIP+CDL` represents the original CLIP model with our discovered concepts. `CDL+CDL` represents our fine-tuned CLIP model with our discovered concepts.\n\n#### Q6: Typo\nThank you for pointing out our typos. We have corrected them in the revised version of our paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694238198,
                "cdate": 1700694238198,
                "tmdate": 1700694238198,
                "mdate": 1700694238198,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aE2WerGy9p",
            "forum": "ZSvOIT5Ai2",
            "replyto": "ZSvOIT5Ai2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5828/Reviewer_8mWh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5828/Reviewer_8mWh"
            ],
            "content": {
                "summary": {
                    "value": "The study delves into Vision-Language Models (VLMs), specifically models like CLIP, and their proficiency in discerning and utilizing visual concepts such as colors and shapes for multi-modal recognition. Past research offers mixed views: some findings suggest VLMs might lack in interpretability, while others indicate that concept-based text prompts can enhance recognition and offer some degree of interpretability. This paper attributes these discrepancies to varied concept definitions and prompting methods among prior works. To address this, a novel framework is introduced to extract and learn interpretable, class-agnostic visual concepts from pretrained VLMs. These concepts are selected based on their visual distinctiveness, evaluated through mutual information between the images and the concepts. A self-supervised approach is then proposed to refine the VLM's recognition capabilities of these concepts. Results, supported by extensive quantitative and human evaluations, confirm that this approach not only bolsters interpretability but also enhances recognition accuracy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea is reasonable and contributes to the interpretability. Experimental results also support that the proposed approach outperforms the baselines."
                },
                "weaknesses": {
                    "value": "1. This paper found out that the classification accuracy drops significantly when the input prompt for the text encoder is without the class name. However, not using the class names may not be a large problem. The key concept of using CLIP is based on the contrastive learning between image-text pairs, which is powerful. As such, it is reasonable that the class names associated with the descriptions improvement the classification. Moreover, the model with class names can provide the correct description, e.g., the examples in Fig. 1.\n2. The method for alleviating the problem is to use different prompts, e.g., \u201cWhat are useful visual features for distinguishing a {category name} in a photo?,\u201d which is heuristic. Another alternatives should be considered and compared.\n3. The experiments should follow the similar setting with previous work. It is suggested to compare with VDES (and following works) and show the results with different backbones.\n4. The references are out-of-date. Only two papers published in 2023 are cited. Moreover, VDES is published in ICLR 2023, while the reference in the paper is still an arxiv paper."
                },
                "questions": {
                    "value": "It is suggested to provide a little bit more details about previous work (Menon & Vondrick, 2022) to improve the readability."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5828/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698843248581,
            "cdate": 1698843248581,
            "tmdate": 1699636614875,
            "mdate": 1699636614875,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eVGN9ocvOC",
                "forum": "ZSvOIT5Ai2",
                "replyto": "aE2WerGy9p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5828/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5828/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the helpful comments and suggestions from reviewer 8mWh!\n\n#### Q1: About using the class name in the concepts\n\nThis is a great point! We agree with the reviewer that class names are helpful for classification and can be used to improve empirical performance on zero-shot classification benchmarks. We would like to clarify that our main point is that the use of class names may bias the analysis on whether VLMs (e.g. CLIP) are able to learn and use concepts (e.g. black patches around the eyes). Specifically, the concepts retrieved by VDES may be correct for two reasons: First, CLIP captures the concept and uses it for zero-shot classification. Alternatively, CLIP uses the class name for zero-shot classification, while the concepts are only retrieved because GPT-3 indicates the concepts are correlated with the class names. \n\nTo better illustrate the point, we revise Figure 1 and show a scenario where VDES is applied on class names with randomly shuffled concepts (e.g. giant panda, which has black wings), and we can observe that the class names are correctly retrieved despite having completely irrelevant concepts. This observation indicates that the class names instead of the descriptive concepts make the decisive contribution to the recognition. We further validate this observation in Table 1 (fourth row, `CLIP + Name w/ Random Concept`), which shows that pairing the class names with randomly shuffled irrelevant concepts do not hurt the zero-shot classification performance.\n\n#### Q2: Other alternative prompts for concept discovery\n\nWe fully agree that the comparison between different prompts is important to show. We have conducted comparisons between different alternatives for the prompts for concept discovery. The results are in Table 5 of the Appendix. From the results we can observe that different prompts provide similar performance, which demonstrates the robustness of our observations.\n\n#### Q3: Comparison with VDES\n\nWe have added this suggested experiment in the revised Table 1 (last row). The row `CLIP + Name w/ Concept` represents the framework of VDES and the row `CDL + Concept` represents our fine-tuned CLIP with the discovered concepts. The results show that our fine-tuned CLIP model can achieve competitive performance while using only class-agnostic concepts. Notably, when VDES is modified to rely only on its concepts, the performance drops significantly (the row `CLIP + Concept`). The results illustrate that our CDL method can learn correct concept and class association and provide interpretability on how VLMs utilize primitive concepts for recognition.\n\n#### Q4: Out-of-date reference\n\nThank you for pointing it out. We have referred to more recently published works in the related work section and corrected the wrong references. Now we have 10 references published in 2023.\n\n#### Q5: Details about VDES\nThank you for your valuable advice on our presentation. We have added more details for VDES in Section 4.1 of our revision."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694073159,
                "cdate": 1700694073159,
                "tmdate": 1700694291519,
                "mdate": 1700694291519,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4EHsCxADai",
            "forum": "ZSvOIT5Ai2",
            "replyto": "ZSvOIT5Ai2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5828/Reviewer_JkAA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5828/Reviewer_JkAA"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a  framework to jointly discover and learn interpretable visual concepts from pretrained VLMs.  The authors claim that the discovered concepts are class-agnostic, and selected based on the visual discriminability measured by mutual information between images and concepts. \nBesides this, the authors propose a self-supervised framework to adapt the CLIP models to recognize the discovered concepts. Experiments on several datasets show these concepts are helpful for understanding CLIP models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "+ easy to follow and implement\n+ clear figures for readers to understand\n+ present experiments on multiple datasets."
                },
                "weaknesses": {
                    "value": "The technical novelty. The overall framework aims to decompose an existing class name into basic visual concepts and then compose them into a semantic verb for final supervision. This structure seems to be trivial and does not provide us with many insights. The ranking for concepts simply borrows the definition of mutual information as Eq.(3). The reviewers doubt the overall novelty of this framework and the contributions seem to be weak.\n\nThe experimental results somehow do not support the overall idea. From Tab.1 using concepts provides little performance improvements. In Tab.2, boosting the concepts from 1000 to 2000 provides only 0.1% improvements on the public imageNet dataset. The experimental results do not fully support the proposed contributions.\n\nThe overall presentations and organizations are not well exhibited. The paper is somehow not easy to follow the key idea and many insights behind the simple language concepts are not clearly explored. The reviewers suggest the authors further explore the semantic concepts or some hidden contextual information rather than this simple architecture."
                },
                "questions": {
                    "value": "Please refer to the weakness section above. Considering its overall quality, I tend to give a negative score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5828/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699609162181,
            "cdate": 1699609162181,
            "tmdate": 1699636614758,
            "mdate": 1699636614758,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GDW6jyspky",
                "forum": "ZSvOIT5Ai2",
                "replyto": "4EHsCxADai",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5828/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5828/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We would like to thank reviewer JkAA for their constructive feedback! We would like to clarify the significance of our contributions, and how the results support our contributions.\n\n#### Q1: On technical novelty and contributions.\n\nWe would like to highlight that our work is motivated by a line of research aiming to improve the interpretability and explainability of vision-language foundation models (VLMs). We aim to answer the fundamental research question: Do VLMs learn to represent images in terms of interpretable, more atomic visual concepts, such as colors and shapes? Knowing the answer to this question would not only help us better understand the potentials and limitations of the existing VLMs, but have important applications for compositional generalization (e.g. recognizing a purple banana despite only observing purple eggplants and yellow bananas during training), and performing test-time intervention (correcting the incorrect concepts and thus making more accurate final predictions).\n\n- Structure is trivial: We respectfully disagree. We adopt the same model structure as prior work, such as CBM (ICML 2020), CompMap (TMLR 2023), and LaBo (CVPR 2023). We assume that composite concepts (e.g. purple bananas) can be composed from primitive concepts (e.g. purple and bananas) using a linear model (i.e. the concept bottleneck classifier), which naturally allows us to inspect how the concepts are utilized to infer the composite concepts, when models are trained. We believe our assumption generalizes to real world scenarios.\n- Not many insights: Although we are not the first work that attempts to analyze if interpretable primitive concepts are captured by VLMs, we are the first one to reveal the limitations of previous analysis, namely the class-name bias. In the revised Figure 1, we show that existing concept-based methods, such as VDES (ICLR 2023), rely on class names as opposed to concepts to make decisions. When the class name is removed (third column from the left), the predicted class is wrong. When the class name is retained but the concepts are randomly shuffled (fourth column from the left), the predicted class remains correct but the retrieved concepts are all wrong. This observation is further validated in Table 1 (3rd and 4th rows). The class-name bias prevents us from properly understanding if the concepts are encoded by pre-trained VLMs, and we designed a framework of concept discovery and learning to address these issues.\n\nInspired by our observations above, we propose to discover class-agnostic (hence not class-name-biased) and visually discriminative concepts with the help of LLM and our proposed Mutual Information concept selection process. Compared to CBM and CompMap, our concepts are discovered as opposed to manually designed. Compared to LaBo, our concepts are chosen to be compact and visually discriminative, leading to better interpretability and higher recognition accuracy. Moreover, we propose a novel and efficient self-supervised concept learning framework to learn the association between concepts and classes, which achieves much better interpretability according to the Intervention, Factuality and Groundability metrics (see Section 5.3)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694331312,
                "cdate": 1700694331312,
                "tmdate": 1700694331312,
                "mdate": 1700694331312,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mqIGVNAmFm",
                "forum": "ZSvOIT5Ai2",
                "replyto": "4EHsCxADai",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5828/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5828/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "#### Q2: From Tab.1 using concepts provides little performance improvements\n\nWe would like to clarify that Table 1 aims to illustrate that the class names play the crucial role for the zero-shot classification accuracy across benchmarks. Using concepts (as performed by VDES) does provide moderate performance gains, but our point is to illustrate that the gain is not obtained via better understanding of concepts. When we remove the class names (the `CLIP + Concept` row) the performance drops substantially. When we randomly shuffle the concepts (the `CLIP + Name w/ Random Concept` row) the performance remains despite wrong concepts being utilized. We visualize such behaviors in Figure 1.\n\nHowever, as shown in the last row of the revised Table 1, after our proposed concept discovery and learning, the concept-only approach is able to achieve competitive performance on zero-shot classification (outperforms the `CLIP+Concept` row significantly), where the concepts are used for the \u201cright\u201d reasons (unlike the `CLIP + Name w/ Concept` row). Compared with the CLIP baseline (first row), our results demonstrate that the correct way of using concepts not only provides interpretability, but also improves the zero-shot classification accuracy, especially for ImageNet, CUB, and Flowers.\n \n#### Q3: In Tab.2, boosting the concepts from 1000 to 2000 provides only 0.1\\% improvements on the public imageNet dataset.\n\nThank you for pointing this out! This is exactly the expected behavior since we view learning a compact concept space that is effective for visual recognition as one of the advantages of our framework. We have demonstrated that even when we use a small concept space (equal to the number of classes in the target dataset), our approach still performs competitively.\n\n#### Q4: The overall presentations and organizations are not well exhibited.\n\nWe fully agree. We would like to incorporate our responses above and improve the overall presentation of our paper. Thank you so much for your feedback, and we look forward to your more detailed comments on the places we should improve!\n\nAlthough we believe our model architecture is well grounded and motivated, and our insights to be novel, we agree with the reviewer and plan to explore alternative architectures to properly understand the potential limitations of the concept bottleneck architecture."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694361179,
                "cdate": 1700694361179,
                "tmdate": 1700694417084,
                "mdate": 1700694417084,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]