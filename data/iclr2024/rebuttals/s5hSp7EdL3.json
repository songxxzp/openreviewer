[
    {
        "title": "The Human-AI Substitution game: active learning from a strategic labeler"
    },
    {
        "review": {
            "id": "Q7yBT9sFSr",
            "forum": "s5hSp7EdL3",
            "replyto": "s5hSp7EdL3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6724/Reviewer_k54k"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6724/Reviewer_k54k"
            ],
            "content": {
                "summary": {
                    "value": "This paper discuss the game between labeler and learning system, where the labeler's objective is to maximize profit, by following some strategy to choose abstaining labels and thus the learning is slowing down; meanwhile, the learning system's objective is thus finish learning as soon as possible, which puts higher requirements on the active query strategy, that is should be robust to abstaining. The paper theoretically analyzes the above observations and proves that absteining labels can indeed destroy the sample-efficient active learning system, base on which the authors further propose a near-optimal algorithm that is robust to absteining."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-  The problem this paper concerns is clearly defined, and all formalizations are clean.\n\n-  The paper is well-organized with clear writing.\n\n-  Modeling two parts of machine learning with respectively conflicting objectives by game and then it can be analyzed with some game-related techniques is novel."
                },
                "weaknesses": {
                    "value": "- The difference between an adversarial labeler and an abstaining labeler is not discussed.The difference between the concerning problem from active learning within an adversarial environment is not clear. Meanwhile, though it is mentioned that the labeler's strategy is identifiable, it is still unclear if it is the worst case.\n\n- The motivation of the problem is too detailed, such that it can hardly be extended to other applications. It is recommended that real-world experiments should be conducted to validate the problem or it seems too artificial.\n\n- The theoretical results are does not belong too much surprising information and has limited contribution to the algorithm design, since the algorithm design is mainly based on the labeler's strategy that is already pre-defined."
                },
                "questions": {
                    "value": "See weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6724/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6724/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6724/Reviewer_k54k"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6724/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698236020883,
            "cdate": 1698236020883,
            "tmdate": 1699636772726,
            "mdate": 1699636772726,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iqLtWKrlem",
                "forum": "s5hSp7EdL3",
                "replyto": "Q7yBT9sFSr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6724/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6724/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer k54k"
                    },
                    "comment": {
                        "value": "Thank you for your review, Reviewer k54k! We are confident that we can address your technical concerns, and summarize our responses below:\n\n```\nThe difference between an adversarial labeler and an abstaining labeler is not discussed.The difference between the concerning problem from active learning within an adversarial environment is not clear. Meanwhile, though it is mentioned that the labeler's strategy is identifiable, it is still unclear if it is the worst case.\n```\n\nTo clarify, in the learning game, we can *deduce* that the labeler will not be adversarial. Note that this is not by assumption, but rather by observation on which labeling strategies can be minimax optimal in the learning game.\n\nTo see this, if the labeler is adversarial, then this will lead to a payoff of zero. This is because adversarially generated data will not realize the learning outcome, and thus the learner will not pay the labeler (L67-70). Hence, while adversarial labeling is a valid strategy, it will not be minimax optimal in our learning game and will not be adopted by a labeler as an optimal labeling strategy. \n\nFinally, to your last point, the labeler\u2019s minimax strategy will always be identifiable. Only these strategies ensure the learner\u2019s learning outcome is met, and thus lead to a positive payoff in the learning game.\n\n```\nThe motivation of the problem is too detailed, such that it can hardly be extended to other applications. It is recommended that real-world experiments should be conducted to validate the problem or it seems too artificial.\n```\n\nWe believe that our setup is fairly general, involving only a simple twist of the standard active learning setting (L44). It serves to address an inherent tension in any supervised learning setup: if the labeler allows the learner to learn fast (e.g. through active learning), this in turn reduces the final profit of the labeler.\n\nMoreover, we believe our work is timely and well-motivated. Just in the recent months, we have seen (in the real-world) Hollywood screenwriters go on strike to negotiate a deal that prevents companies from training on their data and replacing them with AI [1]. Thus, we believe it is important to build our understanding of the replacement aspect of learning, which we set out to do in our paper.\n\n[1] WGA Negotiations \u2014 Status as of May 1, 2023\n\n\n```\nThe theoretical results are does not belong too much surprising information and has limited contribution to the algorithm design, since the algorithm design is mainly based on the labeler's strategy that is already pre-defined.\n```\n\nWe believe our learning algorithm is novel, since we prove that it is near-minimax optimal in the learning game. Contrary to your point, we do not pre-define the labeler\u2019s strategy. Our learning algorithm is proven to be near-optimal with respect to *any* labeling strategy in the learning game.\n\nWe hope this helped clarify most of the questions. We found your feedback very useful in revising the manuscript, thank you! Please let us know if there are any further questions."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6724/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699980978186,
                "cdate": 1699980978186,
                "tmdate": 1699980978186,
                "mdate": 1699980978186,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OGKBnPAFJ8",
            "forum": "s5hSp7EdL3",
            "replyto": "s5hSp7EdL3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6724/Reviewer_GuAP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6724/Reviewer_GuAP"
            ],
            "content": {
                "summary": {
                    "value": "The authors study an active learning problem where the labeler can choose to give the correct label or abstain from labeling (refuse to label) for each query, in order to prolong the learner's learning process, hence get more reward. This is motivated by real-world scenarios where AI training data come from human workers who will eventually be replaced by AI once the AI is fully trained. The authors formalize this interaction as a game, propose query complexity notations to measure how long the learning process can be prolonged, and provide an active learning algorithm with a small query complexity. Extensions to approximate learning, noisy labeling, arbitrary labeling, multiple tasks, are discussed."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) The problem is well-motivated.  Although the \"teacher wants to prolong learning process\" phenomenon is not new in other literature, it is an important, new problem for the active learning community.  This paper can potentially lead to many follow-up results. \n\n(2) The various definitions of the query complexity are interesting. Theoretical results are sound."
                },
                "weaknesses": {
                    "value": "W1: My major concern is that this paper has two really strong, restrictive assumptions. To illustrate this, let me describe the following scenario: Suppose there are two hypotheses $h_1$ and $h_2$ that give the same labels on all but one example $x^*$ (e.g., the hypotheses $h_1$ and $h_2$ in Table 1). Consider the case where the labeler correctly labels all examples except for $x^*$, and abstains from labeling $x^*$. In this case, the learner cannot tell whether the true hypothesis is $h_1$ or $h_2$. I can imagine that this scenario can easily happen in a real-world labeler-learner game where the labeler wants to prolong the learning process as long as possible. This is a key tension in the \"human-AI substitution\" game that motivates this paper. However, the authors simply disallow this scenario to happen by making two strong assumptions in their model:\n\n(1) **The learner is not allowed to query an example multiple times**. If the learner can query an example multiple times, then the labeler can first abstaining from labeling for a while and then eventually give a correct label. It is very natural in practice for a learner to query an example multiple times until he gets a label. But this is not allowed in the authors' model.\n\n(2) Under the assumption that the learner can query each example only once, the authors further assume **\"Guaranteeing Learning Outcome\"** (Section 1.1): the labeler must label in a way to guarantee that the learner can identify the hypothesis $h^*$ in the end. However, this requires the labeler to know the hypothesis space $\\mathcal H$ of the learner very well so that he will not abstain from labeling critical examples (e.g., the $x^*$ above). **This is a strong assumption -- how can an average human worker knows the hypothesis space of the sophisticated machine learner?**\n\nI might change my opinions if the authors can provide some strong motivations or real-world scenarios to support these two assumptions.\n\nW2: the presentation of this paper is not very good. See my Questions and Suggestions below."
                },
                "questions": {
                    "value": "**Questions:**\n\n(Q1) What is the purpose of Section 3.1 (in particular Algorithm 3 and Proposition 3.7) ? My rough understanding is that, Section 3.1 wants to show that finding the maximal E-VS bisection point $x$ (line 3) in Algorithm 2 can be done efficiently by Algorithm 3 (correct me if I am wrong). If this is the case, then what is the time-complexity of Algorithm 3? How does Proposition 3.7 shows that finding the maximal E-VS bisection point can be done efficiently?\n\n(Q2) What is the purpose of Section 3.9? The authors say that this section is to compare with EPI-CAL, but only give a formal result for EPI-CAL ($\\Omega(\\sqrt{m})$ samples in Proposition 3.9) and didn't give any formal result for the algorithm they proposed for comparison.\n\n(Q3) How is this paper's model and result related to the \"well documented\" phenomenon that \"teachers (masters) strategically slow down the training of their apprentices (Garicano & Rayo, 2017)\" ?\n\n\n**Suggestions:**\n\n(S1) The presentation of Section 5.1 and 5.2 is really confusing.\n\nSection 5.1 is about upper bound on the multi-task query complexity. Theorem 5.3 gives an upper bound under a regularity assumption and for the $c_{all}$ label cost. Proposition 5.1 is a counterexample where the upper bound doesn't hold without the regularity assumption, **but the authors didn't say which label cost this Proposition is for**. Proposition 5.2 is an example where the upper bound in Theorem 5.3 doesn't hold for the $c_{one}$ label function.\n\nSection 5.2 is about lower bound (for the $c_{one}$ cost). Theorem 5.6 gives a lower bound on the multi-task query complexity under two conditions for the $c_{one}$ cost, while Proposition 5.4 and 5.5 are counterexamples where the lower bound does not hold if the two conditions are not met.\n\n**I would suggest presenting the theorem first, and then give counterexamples.** And **can the authors give a lower bound result for $c_{all}$?**\n\n(S2) $c(T(x))$ in Definition 3.1 is undefined.  (It is mentioned later in the paper but not here.)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6724/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6724/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6724/Reviewer_GuAP"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6724/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698614824470,
            "cdate": 1698614824470,
            "tmdate": 1700283770787,
            "mdate": 1700283770787,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9ilpTrXDEx",
                "forum": "s5hSp7EdL3",
                "replyto": "OGKBnPAFJ8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6724/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6724/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer GuAP (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your detailed review, Reviewer GuAP! We are confident that we can address your technical concerns, and summarize our responses below:\n\n```\n(1) The learner is not allowed to query an example multiple times. If the learner can query an example multiple times, then the labeler can first abstaining from labeling for a while and then eventually give a correct label. It is very natural in practice for a learner to query an example multiple times until he gets a label.\n```\n\n**Deadlock:** We have considered the setting you propose, as a possible formulation. However, one issue is that some AL algorithms would deadlock. To see this, at time $t$, an algorithm would query the optimal point $x_t$. If the labeler declines to label *and* $x_t$ is not removed from the data pool, the algorithm would still query $x_t$ at $t+1$. This is because the state has not changed, $x_t$ would still be the best point to query, and eligible to be queried since it\u2019s in the data pool. Thus, for the setting you suggest, there would have to be an apt way to modify AL algorithms and ensure the process does not hang. By contrast, in our setting, a data point is removed from the pool at each time step, thus ensuring progress.\n\n**Labeler controlled queries:** If there is a way around the deadlock issue, we also thought that this setup may grant the labeler a lot of power. If the labeler can hold off on labeling a point indefinitely, the labeler can *dictate* the order the learner receives the labels. This is achieved by refusing to label any point but one (say $x\u2019_t$ at time $t$). Thus, the learner\u2019s choice in queries would not matter as labels can only be received in the order $x\u2019_1$, \u2026, $x\u2019_n$, as decided by the labeler. We thought that this setup may differ substantially from the standard AL setting, and our goal is to study the problem closest to the canonical AL setup (L43-44).\n\n```\nGuaranteeing Learning Outcome\" (Section 1.1): the labeler must label in a way to guarantee that the learner can identify the hypothesis in the end. However, this requires the labeler to know the hypothesis space of the learner very well so that he will not abstain from labeling critical examples (e.g., the above). This is a strong assumption -- how can an average human worker knows the hypothesis space of the sophisticated machine learner?...I might change my opinions if the authors can provide some strong motivations or real-world scenarios to support these two assumptions.\n```\n\n**Analyzing minimax strategies:** For most of the paper, in the learning game, we think of the labeler as a \u201c[resourceful] data labeling company\u201d (L199). This is because in game theory, one needs to consider players who play optimally in order to study the Nash Equilibria/minimax query-complexity.\n\n**Dealing with myopic labelers:** With that said, in subsection 4.1 and 4.3, we offer some remedies when facing a player that behaves sub-optimally, for instance due to lack of knowledge about $\\mathcal{H}$ as you suggest (assume also the learner does not communicate $\\mathcal{H}$ for some reason):\n\n1. If the learner knows that the labeler may behave myopically, one idea is to loosen the \u201clearning outcome\u201d to approximate identifiability. Thus, the labeler abstaining on *one* critical example is not fatal ($x^*$ in your example). In subsection 4.1, we show how to extend our learning algorithms to the PAC learning setting.\n2. In 4.3, we observe that the E-VS can be used to detect when an abstained point leads to non-identifiability. Thus, the learner can use this to send a *certified* \u201cwarning\u201d to the labeler: if this critical point (e.g. $x^*$) is abstained upon, it will provably lead to non-identifiability. In this way, the learner can use the E-VS representation to prevent an unaware labeler from prematurely halting the learning process. Indeed, non-identifiability is something *neither* party wants. The learner wants to learn. The labeler needs to realize the learning outcome, in order to be paid. \n\n**Worker Unions:** In the specific case of AI automation, we have seen in recent news that Hollywood screenwriters have collectively gone on strike, engaging in negotiations with companies through their union (\u201cWriters Guild of America'') [1]. Thus, in the AI automation setting, we are inclined to think that the labeler may in fact correspond to a union of workers, which may be more resourceful than a single worker.\n\n[1] WGA Negotiations \u2014 Status as of May 1, 2023\n\nFinally, as we write in the introduction (paragraph at L29), this paper studies a *general* conflict in incentives that can arise in *any* learner-labeler relationship. Active learning enables the learner to learn fast, but in doing so, leads to lower labeler profit. This calls into question a fundamental assumption in active learning: what is to say that the labeler wants the learner to learn fast? We note that the setting of the labeler as an \u201caverage human worker\u201d is only one instantiation of this setup (L34)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6724/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699980452910,
                "cdate": 1699980452910,
                "tmdate": 1699980452910,
                "mdate": 1699980452910,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Syc2iVOZit",
                "forum": "s5hSp7EdL3",
                "replyto": "9ilpTrXDEx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6724/Reviewer_GuAP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6724/Reviewer_GuAP"
                ],
                "content": {
                    "title": {
                        "value": "Happy with authors' response and raise score 5 -> 6"
                    },
                    "comment": {
                        "value": "I am happy with the authors' response and raised score from 5 to 6.\n\nI would appreciate it if the authors can add one or two sentences to motivate the assumption that the learner does not query a sample multiple times."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6724/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700283738778,
                "cdate": 1700283738778,
                "tmdate": 1700283738778,
                "mdate": 1700283738778,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "E7YT54DWMg",
            "forum": "s5hSp7EdL3",
            "replyto": "s5hSp7EdL3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6724/Reviewer_ciAE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6724/Reviewer_ciAE"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides a set of theoretical evidence that labelers can slow down learning, in an active learning setting. The labelers is compensated by the number of labels that provided, and the learner wants to minimize the cost of learning. The paper proposes a maximal bisection algorithm for this setting and proves its sample complexity is optimal up to logarithmic factors. The paper further studies the noisy labels and multi-task learning, advancing its assumptions realisticity and applicability."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is very well-motivated; the problem is of increasing interest for the society by more and more ML applications in the wild.\n2. Quality: the paper flows smoothly by starting with the minimal setting of single task learning without noise and generalizes to wider settings. It proves lower bounds for the sample complexity of the problem, and proposes algorithms matching that lower bound up to logarithmic factors.\n3. Significance: this paper is introducing a new interesting setting into active learning literature and proposes near-optimal algorithms for it."
                },
                "weaknesses": {
                    "value": "1. Clarity: the notation is convoluted; $V$ is used for both VS and a hypothesis space variable. For instance, in table 1, what is $V$? it seems introducing a specific notation for VS (e.g., $\\mathbf{V}$) or for the hypothesis space variable (e.g., $\\mathcal{V}$) could help.\n2. There is no experimental reported. It could help clarify many abstract concepts introduced in the paper using even synthetic data. Examples like Table 1 could help practitioners employ the algorithms introduced in the paper in their applications."
                },
                "questions": {
                    "value": "Suggestions: Please consider \n1. moving table 1 after Definition 2.3 since it uses that definition.\n2. separating the notations of $V$ (see 1. in the weaknesses section)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6724/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6724/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6724/Reviewer_ciAE"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6724/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698625246329,
            "cdate": 1698625246329,
            "tmdate": 1699636772428,
            "mdate": 1699636772428,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GtBYvXqNGD",
                "forum": "s5hSp7EdL3",
                "replyto": "E7YT54DWMg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6724/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6724/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer ciAE"
                    },
                    "comment": {
                        "value": "Thank you for your review and your helpful suggestions, Reviewer ciAE! Please see our responses to your questions below:\n\n```\nClarity: the notation is convoluted; V is used for both VS and a hypothesis space variable. For instance, in table 1, what is V? it seems introducing a specific notation for VS (e.g., V) or for the hypothesis space variable (e.g., V) could help.\n```\n\nThank you for this formatting suggestion! We will be sure to clarify the difference and adapt the presentation accordingly. To clarify, we meant to write $H[S]$ for the version space in Table 1.\n\nWe hope this helped clarify most of the questions. We found your feedback very useful in revising the manuscript, thank you! Please let us know if there are any further questions."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6724/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699979877800,
                "cdate": 1699979877800,
                "tmdate": 1699979877800,
                "mdate": 1699979877800,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ccNwmrli9c",
            "forum": "s5hSp7EdL3",
            "replyto": "s5hSp7EdL3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6724/Reviewer_rYHN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6724/Reviewer_rYHN"
            ],
            "content": {
                "summary": {
                    "value": "This paper consides strategic labelers for active learning task who may abstain to prolong the complexity of quering, such that they may be compensated with more labeling. The authors show that abstention could indeed enlarge query complexity, and present a novel minimax game as well as query complexity measure. A near-optimal algorithm based on E-VS bisection is designed to defend strategic labeling, and extensions to other active learning settings such as approximate learning, noisy labeling, multi-task learning are also discussed."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The strengths of this paper are its novelty of strategic learners for active learning tasks, as well as the theoretical soundness in the minmax formation and the effective version space bisection algorithm. This paper initiates the study of learning from a strategic labeler, which is original. The complexity of proposed E-VS method $O(\\log{|\\mathcal{X}|})$ clearly significantly improves existing VS algorithm of $\\Omega(\\{|\\mathcal{X}|})$."
                },
                "weaknesses": {
                    "value": "I think the biggest concern that might weaken this paper is its lack of empirical evidence. Despite the theoretical gaurantees, some of the settings are probabilistic (PAC or noisy observations for instance), making empirical evaluation of the proposed methods valuable. I understand there are so many great results in this paper and there is a page limit, but including some simple simulation studies could be beneficial."
                },
                "questions": {
                    "value": "I only have a minor question. This paper assumes \"the learner\u2019s strategy corresponds to some deterministic, querying algorithm\". Nevertheless, non-deterministic policies such as Thompson sampling is quite common in some active learning tasks such as Bayesian optimization. How does these non-deterministic policies fit into the framework of this paper?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6724/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6724/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6724/Reviewer_rYHN"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6724/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699163615063,
            "cdate": 1699163615063,
            "tmdate": 1699636772287,
            "mdate": 1699636772287,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lMJAmex4WY",
                "forum": "s5hSp7EdL3",
                "replyto": "ccNwmrli9c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6724/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6724/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer rYHN"
                    },
                    "comment": {
                        "value": "Thank you for your review, Reviewer rYHN! Please see our responses to your questions below:\n\n```\nI only have a minor question. This paper assumes \"the learner\u2019s strategy corresponds to some deterministic, querying algorithm\". Nevertheless, non-deterministic policies such as Thompson sampling is quite common in some active learning tasks such as Bayesian optimization. How does these non-deterministic policies fit into the framework of this paper?\n```\n\nTo clarify, Bayesian optimization/Thompson sampling is used for finding a maxima $x \\in X$ of some unknown $h$. There, the goal is to adaptively query in order to shrink the uncertainty over $h$, to find one of its maxima with low (simple) regret. \n\nOur setting is quite different in that we want to learn the unknown hypothesis $h$, instead of optimizing $h$. Moreover, our paper studies minimax learning strategies in the learning *game*, in face of a strategic labeler who may be playing an optimal labeling strategy. This game theoretic setup is another major difference that complicates the analysis and differs from the setup of Bayesian Optimization.\n\nWe hope this helped clarify most of the questions. We found your feedback very useful in revising the manuscript, thank you! Please let us know if there are any further questions."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6724/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699979755547,
                "cdate": 1699979755547,
                "tmdate": 1699979755547,
                "mdate": 1699979755547,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kMS5S45Ohi",
                "forum": "s5hSp7EdL3",
                "replyto": "lMJAmex4WY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6724/Reviewer_rYHN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6724/Reviewer_rYHN"
                ],
                "content": {
                    "title": {
                        "value": "Lack of empirical evidence"
                    },
                    "comment": {
                        "value": "Thank you for your timely responses. The clarification for the Bayesian optimization case was helpful. Nevertheless, I (and maybe Reviewers ciAE and k54k as well) am still willing to hear how you will respond to the lack of empirical evidence in your paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6724/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700166369497,
                "cdate": 1700166369497,
                "tmdate": 1700166369497,
                "mdate": 1700166369497,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OOE997z4lU",
                "forum": "s5hSp7EdL3",
                "replyto": "Psjx8Er5p4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6724/Reviewer_rYHN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6724/Reviewer_rYHN"
                ],
                "content": {
                    "comment": {
                        "value": "My comments and concerns are addressed well by the authors' response and additional experiments conducted during rebuttal phase. My score does not change, leaning towards acceptance."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6724/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700516838853,
                "cdate": 1700516838853,
                "tmdate": 1700516838853,
                "mdate": 1700516838853,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]