[
    {
        "title": "Entity-Centric Reinforcement Learning for Object Manipulation from Pixels"
    },
    {
        "review": {
            "id": "03BJl4zj2A",
            "forum": "uDxeSZ1wdI",
            "replyto": "uDxeSZ1wdI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3597/Reviewer_aSoT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3597/Reviewer_aSoT"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an object- or entity-centric RL algorithm for learning goal-conditioned manipulation. As object-centric representations, the authors use the Deep Latent Particles (DLP) method. The novelty is in the policy and Q-network, for which they propose an Entity interaction transformer (EIT), which is a transformer-based architecture to process the structured per-object latent representations. They test the method in an object manipulation task, with a robot manipulator and 2 static viewpoints provided as observations. They adopt a goal-conditioned RL setup, where the goal state is provided as target images, and introduce a Chamfer reward term to train the policy and Q function. The experiments show that their method can match the performance of another structured latent state method (SMORL), and outperform it when using image goals. Moreover, they demonstrate compositional generalization, where an agent trained on i.e. 3 colored cubes can generalize to a task with N colored cubes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper provides various novel contributions such as the transformer architecture for the Q and policy network, the Champfer reward to train policies conditioned on goal images, and demonstrates compositional generalization.\n\n- The experimental results show ablations for the various components, such as the Champfer reward, using object-centric structured latent state spaces and using multiple views."
                },
                "weaknesses": {
                    "value": "- The method seems very tied to the experimental setup of having a robot manipulator that needs to push objects to a particular location. Some of the proposed novelties such as the Champfer reward don't seem very applicable beyond this use case.\n\n- The experiments are limited to a single environment of colored cubes. It would be interesting to see whether the approach can scale to various objects (for example YCB objects), and more cluttered scenes.\n\n- As hinted by the authors SMORL is more sample efficient, as it learns to manipulate a single object and can then generalize to the others. This seems to be an essential feature / reason to go to object-centric approaches."
                },
                "questions": {
                    "value": "- An important rationale for object-centric representations for RL is that once you learn a policy on one object, you can apply it to other objects (i.e. explaining the sample efficiency gap with SMORL). Why did the authors choose to discard this feature in their architecture, and would there be options to combine the strengths of both?\n\nP.S: Fig 6 caption has a typo \"mathcing\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3597/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3597/Reviewer_aSoT",
                        "ICLR.cc/2024/Conference/Submission3597/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3597/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698393432813,
            "cdate": 1698393432813,
            "tmdate": 1700660546362,
            "mdate": 1700660546362,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cUFjFwyeIX",
                "forum": "uDxeSZ1wdI",
                "replyto": "03BJl4zj2A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3597/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3597/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for acknowledging the novelty and contribution of our work.\n\n**Experimental Setup**\n\n*Focus on Pushing Tasks* -  We have added experiments with a more complex task of orienting a T-shaped object. We believe that various other manipulation tasks could easily be handled by our policy, given a proper reward function during training.\n\n*Chamfer Reward* - Designing a reward function from images is a difficult task, and while we make some progress on it using the Chamfer reward (which can also be extended to the orientation task), we acknowledge that it is not yet solved.\n\n*Objects in the Environment* - The experiments we have added in the new revision may shed light on some of your concerns regarding the types of objects our method can deal with. The push-T task demonstrates manipulation of an object that is not a cube with more complex dynamics (see Appendix B.4 and \u201cPush-T\u201d tab on our website). In addition, we present zero-shot generalization results on variations of object properties, including cuboids of different dimensions (results are presented in Table 6 and discussed in Appendix B.2  in the new revision). While zero-shot generalization to objects with different properties is partial on some scenarios, the success rates are not negligible and therefore hint at potential for few-shot generalization. These results also imply that our method can handle training with different types of objects to begin with.\n\n*Cluttered Scenes* - The Group-Push experiment shows the ability of our agent to deal (zero-shot) with a large amount of objects densely occupying the table (see our website for demonstrations).\n\n**Sample Efficiency**\n\n*Comparison to SMORL* -  We would like to emphasize that SMORL\u2019s policy does not generalize to multiple cubes - it is trained to perform single-cube manipulation and does so also during inference. A scripted meta-policy is used to cycle between the objects in the environment and solve the sub-goals one by one during inference, disregarding the other objects while doing so. While SMORL is more sample efficient in the sense that it only requires learning single-object manipulation, it is fundamentally limited in performing complex multi-object tasks that require modeling interactions, as we demonstrate in our experiments (see Figure 4d, 4f and Table 1).\n\n*Best of Both Approaches* - We designed our method to learn simultaneous multi-object manipulation because it is applicable to a larger group of tasks than learning on a single object is, as we demonstrate in our experiments (see Figure 4d, 4f and Table 1). We present types of sub-goals that can\u2019t be achieved without taking all sub-goals into account (e.g. Ordered-Push).\n\nCompared to SMORL, there is indeed a tradeoff between sample efficiency and performance that originates from simplification of the learning problem. In some cases, which we claim to be closer to real-world scenarios, oversimplification significantly hurts performance.\n\nRegarding combining the strengths of both, this could be an interesting direction for future work. One way to do this would be to create a curriculum of an increasing number of objects similar to Li et al. [1]\n\n[1] Richard Li, Allan Jabri, Trevor Darrell, and Pulkit Agrawal. Towards practical multi-object manipulation using relational reinforcement learning. In 2020 ieee international conference on robotics and automation (icra), pp. 4051\u20134058. IEEE, 2020."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3597/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700417049912,
                "cdate": 1700417049912,
                "tmdate": 1700417049912,
                "mdate": 1700417049912,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lzMdOXoE6t",
                "forum": "uDxeSZ1wdI",
                "replyto": "cUFjFwyeIX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3597/Reviewer_aSoT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3597/Reviewer_aSoT"
                ],
                "content": {
                    "comment": {
                        "value": "I'd like to thank the authors for their responses, as well as for adding the extra experiments that touch upon the points I made. I also increased my score accordingly."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3597/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660671268,
                "cdate": 1700660671268,
                "tmdate": 1700660671268,
                "mdate": 1700660671268,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uIUsMlkULF",
            "forum": "uDxeSZ1wdI",
            "replyto": "uDxeSZ1wdI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3597/Reviewer_PeH5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3597/Reviewer_PeH5"
            ],
            "content": {
                "summary": {
                    "value": "Authors solve table-top goal conditioned tasks form pixels using particles encoding and a transformer based RL. This is an interesting improvement over previous SOTA works and its major weakness are: 1) entities are fixed cubes-with-specific-color, thus there is no possibility to generalize to other objects with different properties, 2) interaction between the objects has not properly been demonstrated and 3) related work could be improved."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-\tThe paper is clear, well written and the topic is very interesting for the community\n-\tThe viewpoint is a nice work around to solve depth ambiguities \n-\tThe conditional goal transformed is sound and nicely implemented for an actor-critic RL.\n-\tInputs are just pixels, thus making the problem very complex.\n-\tClever use of the Chamfer Distance with the particles.\n-\tExamples animations provided that shows the system working."
                },
                "weaknesses": {
                    "value": "**Related work**\n\nThe literature review is too focused on RL and could be improved.\n\nExample of missing SOTA object-centric perception:\nM. Traub et al. Learning what and where: Disentangling location and identity tracking without supervision,\u201d  International Conference on Learning Representations, 2023.\n\n\u201clearning to manipulate\u201d The citations related to manipulation are only for image segmentation, there is a scarce but very good literature on object-centric manipulation. The majority with full observability but some from pixels.\n\n- Works based on Interaction Networks, Propagation Networks and Graph networks. E.g., A. Sanchez-Gonzalez, N. Heess, J. T. Springenberg, J. Merel, M. Riedmiller, R. Hadsell, and P. Battaglia. Graph networks as learnable physics engines for inference and control.\n\n- Examples from pixels: \nvan Bergen & Lanillos (2022). Object-based active inference. Workshop on Causal Representation Learning @ UAI 2022.\nDriess et al. \"Learning multi-object dynamics with compositional neural radiance fields.\" Conference on Robot Learning. PMLR, 2023.\n\n- Finally, regarding the use of particles for robotic control I really think that this work is seminal: \nLevine, S. et al. (2016). End-to-end training of deep visuomotor policies. The Journal of Machine Learning Research, 17(1), 1334-1373.\n\nAs an aside comment, we can find similar table-top behaviours as the one presented here using LLMs, e.g., \u201cPalm-e: An embodied multimodal language model.\u201d\n\n**Methods:**\n\nThis sentence requires elaboration: \u201cObviously, we cannot expect compositional generalization on multi-object tasks which are different for every N.\u201d\n\nAssumption 1. Probably you are using a standard notation but please explain what is \\alpha* and v*.\n\nCould you explain the consequence of Theorem 2.\n\nWhy did you use off-policy algorithm TD3?\n\n\u00bfWhy do you need RL to train the DLP? It was mentioned that this module is pretrained, so no goal would be needed. Otherwise, you constraint the training for the defined goals that are set by the designer.\n\nGoal definition \u2013 Using the encoder. This is a common technique but prevents for proper generalization. How you would encode in this architecture non-predefined goals?, like move red objects to the left.\n\nParticles and only cubes. Using particles is very interesting, but evaluation with non-cube objects is not tested. This means that it could be that the experiments are assuming that the objects are point-mass entities. This would prevent generalization. In particular, the definition of cube-red as a single entity seems very restricted so you cannot perform behavioural operations on other shapes with different colours or other properties. \n\nAlso this rises the problem of permutation invariant, maintaining the identity of an object may is important in tasks that object permanence is needed for instance in dynamic-sequential tasks.\n\n**Experiments**\n\n\u00bfWhy adjacent goals require interactions? This can be solved reactively.\n\nI find very interesting the Ordered-Push. Should be the EIT trained for each task or it is trained on all tasks and the executed?\n\nI understand that you relegated the Chamfer distance to the Appendix, but it could be great that at least a written explanation is placed (or the equation) to understand how the rewards works.\n\nUsing this distance (and the L2)  as rewards why is RL needed, would it be enough to use a KL as objective function? Or are there other rewards used?\n\nWhat is state input? Full observability?\n\nThe agent is learning arm-object interaction thanks to the RL approach but it is not clear that the system is learning objects interaction.\nCompositional generalization. While I agree that training on N objects and then executing the task with less and more objects shows generalization capabilities. This does not necessarily endorses composition. \n\nCould you explain how the system changes when including more objects at the level of the  DLP and the EIT?\n\nBaselines: The text says: \u201cWe use DLP as the pre-trained OCR for this method for a fair comparison\u201d, but then SMORL is only compared in the results showed with \u201cstate\u201d access. Does this mean that this is without using pixels as input.\n\nIt is interesting that using RL also unstructured approach cannot handle the complexity. We obtained similar results using an ELBO loss. However, this makes the comparison too na\u00efve. As the comparison of your algorithm is against full observable (state) and unstructured.\n\n**Minor comments**\n\n- Please check open quotes, in latex you can use ``word\u201d\n- Self attention -> Self-attention"
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3597/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3597/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3597/Reviewer_PeH5"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3597/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698679803703,
            "cdate": 1698679803703,
            "tmdate": 1700492961684,
            "mdate": 1700492961684,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "L4l4dJLQCY",
                "forum": "uDxeSZ1wdI",
                "replyto": "uIUsMlkULF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3597/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3597/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for acknowledging the complexity of the tasks we deal with and the strengths of different aspects of our method. We also thank you for your questions and interest in our work.\n\n**Related Work**\n\nThank you for pointing out the various papers to us, widening the scope of our related work to both model-based planning and active inference methods. We have added them to the related work section of the new revision.\n\nRegarding Palm-e, it is an LLM-based planning algorithm that assumes access to a low-level manipulation policy. Here we use RL to learn a single policy that solves the entire task, both single-object manipulation and planning to achieve the overall goal while accounting for object-object interaction.\n\n**Method**\n\n*\u201dThis sentence requires elaboration: \u201cObviously, we cannot expect compositional generalization on multi-object tasks which are different for every N.\u201d\u201d*\n\nWe expect compositional generalization to an increasing number of objects where the objects are similar to the ones seen during training and the basic task remains the same with the increased number of objects, such as pushing each object to a desired goal location.\n\n*\u201dAssumption 1. Probably you are using a standard notation but please explain what is \\alpha* and v*.\u201d*\n\nWe reference the supplementary background for the attention mechanism (Appendix E) where $\\alpha$ represents the attention weights (produced from keys and queries) and $v$ represents the values. * denotes the optimal versions of those functions (per our assumption that the optimal Q function has an attention structure).\n\n*\u201dCould you explain the consequence of Theorem 2.\u201d*\n\nThe theorem bounds the approximation error of a learned Q-function vs. the optimal Q-function, when both have a self-attention structure. The Q-function is learned on environments with $M$ objects or less, and is epsilon-close to the optimal Q-function in this case (less than $M$ objects). The optimal Q-function is optimal for all $N$. We show that with no additional training, the approximation error increases linearly in the number of objects. This is a notion of compositional generalization defined through the Q-function, and we show that a Q-function with a self-attention structure obtains it.\n\nWhat this implies is that the Q-function used to optimize a policy on $M$ objects is not very far from the optimal Q-function for $M+k$ objects and thus the same policy should obtain similar returns even when deployed on environments with an increasing number of objects.\n\n*\u201dWhy did you use off-policy algorithm TD3?\u201d*\n\nTD3 is a SOTA off-policy model free deep RL algorithm based on DDPG which has been used in similar tasks in previous work, including the original HER paper. It is well suited for goal-conditioned RL because it allows dynamically relabeling goals in the replay buffer transitions sampled during training, thereby improving sample efficiency.\n\n*\u201d\u00bfWhy do you need RL to train the DLP? It was mentioned that this module is pretrained, so no goal would be needed. Otherwise, you constraint the training for the defined goals that are set by the designer.\u201d*\n\nWe do not need RL to train the DLP nor do we claim this. The DLP is pretrained on image data collected by a random policy. This is not related to the goals. Goals in our experimental setup are defined as images of the scene with a specific object configuration.\n\n*\u201dGoal definition \u2013 Using the encoder. This is a common technique but prevents for proper generalization. How you would encode in this architecture non-predefined goals?, like move red objects to the left.\u201d*\n\nWhat are you referring to by predefined goals? The goals in our setting are not predefined, they are provided as an image of the desired configuration by the user. This image is converted to the latent particle representation using the pre-trained DLP encoder, which is able to encode images it has not seen during training but that come from the same distribution. Moreover, it is able to generalize to images with a larger/fewer number of objects than it has seen during training. These are demonstrated by the success of our method as well as in the generalization capabilities in the Cube-Sort scenario.\n\nIf by \u201cmove red objects to the left\u201d you are referring to goal specification through natural language, this is very possible and integrates well with our proposed method, assuming access to a reward function. The goal entities will consist of the natural language tokens specifying the goal and their relation to the DLP-extracted image entities will be learned via the cross-attention block in the EIT. Reward design might be a challenge in this case but this is not exclusive to our method. We leave such an interesting investigation to future work."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3597/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700416228291,
                "cdate": 1700416228291,
                "tmdate": 1700416228291,
                "mdate": 1700416228291,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SNeU6e38Hn",
                "forum": "uDxeSZ1wdI",
                "replyto": "L4l4dJLQCY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3597/Reviewer_PeH5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3597/Reviewer_PeH5"
                ],
                "content": {
                    "title": {
                        "value": "Good achievement, new results"
                    },
                    "comment": {
                        "value": "After reading the reviewers comments and the responses, most of my concerns were addressed or at least explained. I believe it is a good contribution and the new results shows that there is room for improvement when changing the objects features. Comparison against only full-observable settings is not optimal but understandable. Thus, I would increase my score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3597/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492692563,
                "cdate": 1700492692563,
                "tmdate": 1700492692563,
                "mdate": 1700492692563,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Xw1rMoC9Ck",
            "forum": "uDxeSZ1wdI",
            "replyto": "uDxeSZ1wdI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3597/Reviewer_SBTK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3597/Reviewer_SBTK"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an object-centric RL model that can learn to manipulate many objects and shows generalization capabilities. The main contribution is the combination of Deep Latent Particles (DLP) as entity-centric perception pipeline and a transformer for policy and Q function. By defining a reward based on feature closeness and geometric distance there is no matching between goal-image and current image required. The only caveat is that the objects need to be filtered (the robot needs to be removed).  \nThe proposed method is pretty simple in comparison to prior work that considers entity-entity interactions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- scalability to many objects\n- a relatively simple method\n- no explicit matching is required\n- multiview\n- supplementary contains important comparisons w.r.t. the reward etc."
                },
                "weaknesses": {
                    "value": "- number of objects known. \n- missing related work and baselines:\n   - SRICS [1] is like SMOURL but dealing with object-object interactions \n   - DAFT-RL[3]: also tackles the interaction problem and baselines therein\n     DRAFT-RL is fairly recent, but it contains, IMO, relevant related work and further baselines, such as:\n     NCS [3], STOVE [4] etc.\n- supervision/filtering of entities such that only objects go into chamfer reward computation is hidden in the appendix \n- only empirical results on one type of environment: I am wondering how well it would generalize to more cluttered scenes, e.g. to a kitchen environment\n \nDetails:\n- Fig 5: too small font in the right subplot\n- Appendix A: Chamfer rewards:\n  The definition of $X_j$ and $Y_i$ after Eqn (1): what is the $i$ in the definition of $X_j$? Do I understand correctly, that it is all $x$ that have $y_j$ as their closest entity in $Y$? \n  Also afterward, when you write how to obtain standard Chamfer, the $sum_j$ is somehow missing for the second fraction. \n- I think some more information about the Generalized Density Aware Chamfer reward should go into the main text, and also that non-object particles are removed.\n- A paper that also addresses many-object manipulation with an object-centric representation is [5] (not from images)\n\n- citations/references are often published at conferences but listed as arXiv papers\n\n[1] https://proceedings.mlr.press/v164/zadaianchuk22a.html\n[2] https://arxiv.org/abs/2307.09205.pdf\n[3] https://openreview.net/forum?id=fGG6vHp3W9W\n[4] https://openreview.net/forum?id=B1e-kxSKDH\n[5] https://openreview.net/forum?id=NnuYZ1el24C"
                },
                "questions": {
                    "value": "- how important is it that the robot is mostly white on a white background? What happens if a larger part of the robot is seen in the images? I suggest discussing this in the limitations. Also, the need to filter non-object entities. Other works would also move the robot to a particular position in the scene if part of the goal. \n- what happens if the number of latent particles is higher than the number of entities?\n- How do you compare to the above-mentioned baselines?\n\n--- Post rebuttal update. My concerns were addressed. I changed my score from 5 to 8."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3597/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3597/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3597/Reviewer_SBTK"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3597/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698691796870,
            "cdate": 1698691796870,
            "tmdate": 1700741509498,
            "mdate": 1700741509498,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mAju541lYR",
                "forum": "uDxeSZ1wdI",
                "replyto": "Xw1rMoC9Ck",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3597/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3597/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for acknowledging the strengths of our work and simplicity of our method. We also thank you for your questions and  referring us to the various methods related to our work.\n\n**Related Work and Baselines**\n\nWe have added the various methods you referred us to in the related work section. Our method is model-free and arguably much simpler than the methods you mentioned. Additionally, we introduce multiview inputs which are seamlessly integrated in a single model, and show zero-shot generalization to a substantially larger number of objects than previous work.\n\nIn the following, we address each method in comparison to ours in detail.\n\n[SRICS](https://arxiv.org/abs/2109.04150)\n\nWe find this work an interesting extension of SMORL (by the same authors) for *state inputs* which takes into account object-object interaction. \n\n*Method Complexity*: compared to our method from state inputs, it requires training a separate network to consider relationships and interactions. This network is essentially a world-model that is used during both RL training to define rewards and during inference to decide on the ordering of sub-goals to solve using the RL policy. Our method incorporates all of the above aspects in a single transformer-based model, making it more robust, easy to train and requiring less hyper-parameters.\n\n*Assumptions*: An assumption of SRICS is that when trying to achieve a given sub-goal, the agent should minimize its effect on other sub-goals. This assumption is explicitly integrated in the method through the \u201cselectivity reward\u201d used to train the SAC agent. This can be a restrictive assumption that could negatively impact performance. Consider for example the task of moving objects located at one side of the table to the other. The SRICS agent will move the objects one by one, although the optimal behavior with respect to timestep efficiency (and a negative distance reward) would be to move multiple objects simultaneously. Our method does not make such assumptions.\n\n*Inputs*: While we find this work related to ours, it is not a relevant baseline because it is not designed for image inputs. The authors mention the extension of this method to image-based inputs as future work, implying that this would not be a trivial extension.\n\n[Linear Relation Networks (LRN)](https://arxiv.org/abs/2201.13388)\n\nThis method uses state inputs. It is similar to SMORL in the sense that it only considers relationships between objects and goals, therefore not accounting for object-object interaction. Due to the above, we believe SMORL is a better fit as a baseline which covers the essentials of the LRN method and improves over them (image-based, goal-conditioned via images).\n\n[CEE-US](https://arxiv.org/abs/2206.11403)\n\nThank you for pointing out this paper to us. Although it is not an RL method and uses state observations, it does deal with exploration and compositional generalization in the multi-object manipulation setting. We have added it to the related work section.\n\n[NCS](https://arxiv.org/abs/2303.11373)\n\nThis method presents a structured approach for solving object rearrangement tasks. They use a modification of SLATE as an object-centric representation of images. They then use it to construct a factored transition graph between clustered states where the nodes are states and the edges are actions. They use this graph to plan and execute actions on a high-level object rearrangement environment.\n\n*Method Complexity*: Generally, NCS is a complex method containing multiple manual (not learned) task specific substeps in each stage of the algorithm, increasing the number of hyper-parameters and making it less generally applicable. These include clustering states using K-Means, isolating which object moved in each transition based on the difference in state attributes, explicitly matching state and goal objects using the Hungarian algorithm (assumes a 1-to-1 match exists) and at each timestep, choosing the object with the largest distance from the goal as the next candidate to move, not taking into account goal constraints such as the presence of other objects in the goal position.\n\n*Assumptions*: This work makes several simplifying assumptions. They do not consider entity-entity interaction, do not handle occlusion (no agent in image), do not deal with low level control (assume high level actions). Our method is designed to address challenges that arise in the real world where these assumptions do not hold. We consider entity-entity interaction via the attention mechanism and handle occlusions by integrating multiple viewpoints in our EIT architecture. Additionally, our method is aimed at learning both low level and high level control, which prevents us from using NCS as a baseline in our experimental setup."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3597/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700415254308,
                "cdate": 1700415254308,
                "tmdate": 1700415254308,
                "mdate": 1700415254308,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pmViuasjSZ",
                "forum": "uDxeSZ1wdI",
                "replyto": "Xw1rMoC9Ck",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3597/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3597/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Related Work - Continued**\n\n[STOVE](https://arxiv.org/abs/1910.02425)\n\nSTOVE is an object-centric video prediction model which also demonstrates world-modeling capabilities for solving a sequential decision making task via planning (Monte Carlo Tree Search). They extend their video prediction model to dealing with control by conditioning the predictions on actions and predicting not only the next frame but also the reward to enable reward-maximizing planning. In their evaluation, they consider visually simple 2D environments and discrete action spaces.\nIt is not clear how this method would scale to complex 3D environments and continuous action spaces such as the ones we consider in our experimental setup. Additionally, expanding their model to multiview inputs is highly non-trivial.\n\n[DAFT-RL](https://arxiv.org/abs/2307.09205)\n\nThis paper proposes an object-centric model-based approach, where the state is not only factored into entities but also to their individual attributes such as physical and visual properties. It does so by learning multiple separate networks which include 3 graphs. One is in charge of modeling the relationships between object attributes, the agent\u2019s action and the reward. A second graph is in charge of modeling object-object interaction and the third is in charge of agent-object interaction and forward dynamics prediction.\n\n*Method Complexity*: The proposed method consists of several components and training stages. The stages are roughly separated into learning each of the graphs detailed above for each class of objects which is assumed to be known. Each network is learned by maximizing a distinct objective using different data. These do not include the policy network which is learned subsequently using the various model components. Compared to their method, our transformer-based model is trained to maximize a single objective and learns relationships between entity attributes as well as entity-entity interaction in an implicit manner in order to perform multi-object tasks.\n\n*Assumptions*: DAFT-RL makes several assumptions that explicitly affect their algorithm design choices. They assume a known set of object classes and that their attributes are both accessible from images and are disentangled in the latent image representation. The Push-T task we consider in our experiments is an example where this assumption does not hold, as both color and orientation are implicit in the latent visual features. In addition, the mass or friction coefficient of the objects for example, can\u2019t be inferred from images alone. They also assume each action directly affects a single object at a time. We do not make these assumptions. The attention mechanism in the EIT along with representing the action as an entity in the Q-function network enables modeling agent interaction with multiple objects simultaneously.\n\n**Related Work Conclusion**\n\nWe compare our method to what we see as the most relevant SOTA baselines: goal-conditioned model-free algorithms that are image-based. There are clear advantages to model-free over model-based methods (as is true for the other direction) and we see our work as a step forward in goal-conditioned object-centric model-free RL.\n\n**Environments**\n\nWe have demonstrated that our method solves core challenges in multi-object manipulation such as occlusions, interactions between objects, a continuous action space, and compositional generalization. Our new results also demonstrate success on more geometrically complex objects (Push-T). These challenges, which were largely ignored in previous studies, are important for real world scenes such as kitchens. There may be additional challenges, but we believe our results show a clear progress towards handling such scenes.\n\n**Number of Objects**\n\nWe wish to emphasize that the **number of objects is not known** and is not used explicitly in any part of the algorithm. The number of objects should generally affect the choice of the DLP model\u2019s \u201cnumber of particles\u201d hyper-parameter. In our experimental setup, we set \u201cnumber of particles\u201d=24, which is much larger than the amount of objects/entities we consider in any of our environments. We found that choosing a relatively high upper bound is beneficial for RL training as it reduces the chance of objects in the scene not being represented by at least a single particle, and allows representing large entities (such as the robot arm) with multiple particles. In addition, this allows for zero-shot generalization to a significantly larger number of objects, as we demonstrate in the Group-Push experiment."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3597/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700415426362,
                "cdate": 1700415426362,
                "tmdate": 1700415929812,
                "mdate": 1700415929812,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cbiAhPzF72",
                "forum": "uDxeSZ1wdI",
                "replyto": "gz2BI1u2qS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3597/Reviewer_SBTK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3597/Reviewer_SBTK"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your detailed answer. \nMost of my concerns are addressed. I like that you used 24 particles also for scenes with smaller number of objects. \n\nI have a different opinion on what is a limitation, or what should be stated there. I think it is important to specify which supervision and domain knowledge is required for your method to work well. Whether less assumptions would lead to suboptimal performance is a different question. \n\nCan you indicate whether:\n- The push-T task (very nice BTW) is going to be referenced in the main text\n- Fig 5 right will be updated"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3597/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737177426,
                "cdate": 1700737177426,
                "tmdate": 1700737177426,
                "mdate": 1700737177426,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cg9PacQUWx",
                "forum": "uDxeSZ1wdI",
                "replyto": "Xw1rMoC9Ck",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3597/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3597/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response - clarification"
                    },
                    "comment": {
                        "value": "Thank you for your response. To your suggestion, we address the supervision required for the Chamfer reward in our setting in the main text and in more detail in Appendix A.1. We have increased the font size in Figure 5, but we can further increase it if the reviewer still finds it necessary. We will add the Push-T results and discussion to the main text in the next revision."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3597/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740781992,
                "cdate": 1700740781992,
                "tmdate": 1700740811481,
                "mdate": 1700740811481,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "R4Gz6EMB1o",
                "forum": "uDxeSZ1wdI",
                "replyto": "cg9PacQUWx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3597/Reviewer_SBTK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3597/Reviewer_SBTK"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks. Great work. I will increase my score."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3597/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700741354231,
                "cdate": 1700741354231,
                "tmdate": 1700741354231,
                "mdate": 1700741354231,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YvN0Xb1MMU",
            "forum": "uDxeSZ1wdI",
            "replyto": "uDxeSZ1wdI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3597/Reviewer_sQ6n"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3597/Reviewer_sQ6n"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript introduces an innovative approach that seamlessly integrates an object-centric model with a transformer to master structured representations crucial for goal-conditioned reinforcement learning (RL), particularly in scenarios entailing multiple objects or entities. The employed object-centric model, denoted as DLP, equips the framework with the capability to capture a structured portrayal of the environments. Concurrently, the transformer component adeptly models the dynamics of the entities and their intricate physical interactions. The clarity of the conceptual foundation is commendable, and the results showcased, particularly in the challenging realm of image-based control, are robust and hold promise. Furthermore, the paper hints at potential advancements in the field of compositional generalization. Given these strengths, I am inclined to recommend this paper for acceptance, acknowledging its significant contributions and merits. However, there are some unclear points in the current version and it would be better if the authors could provide clarification on them."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- **[General idea]** Overall, the concept presented in the paper is elegantly simple and straightforward\u2014a notable strength, as this simplicity bodes well for better understanding and potential scalability of the framework. This is of particular importance, despite the approach essentially being a synthesis of OCR and transformer-based MBRL.\n\n- **[Presentation]** The clarity and coherence of the presentation, spanning both the main paper and the appendix, are commendable, facilitating easy comprehension for the reader. Nevertheless, I have enumerated several recommendations in the subsequent sections to further enhance the manuscript.\n\n- **[Experiments]** The experiments conducted using IsaacGym validate the method's efficacy, and the exploration of compositional generalization yields valuable insights. However, I have outlined several suggestions in the sections that follow, aimed at verifying some claims made in the algorithm's design."
                },
                "weaknesses": {
                    "value": "I list the weaknesses and questions together here.\n\n**[About the matching]** \n\nI concur with the authors regarding the permutation invariant block in the EIT, acknowledging its potential to obviate the need for matching post-OCR. However, the rationale behind the decision to forego a straightforward matching step subsequent to OCR is not entirely clear to me. Is this choice motivated by a desire for increased flexibility, or are there other factors at play? From my perspective, matching algorithms can serve as modular, plug-and-play components, exemplified by their seamless integration in slot attention mechanisms as outlined in [1]. I recommend a more thorough elucidation of this particular point in the rebuttal, as it would greatly enhance the clarity and comprehensiveness of the explanation.\n\n**[About the evaluation]**  \n\nIn order to rigorously assess the contribution of each individual component within the algorithm\u2019s design, I recommend broadening the scope of the ablation studies conducted. Specific areas to consider include: (1) experimenting with alternative OCR methodologies in lieu of DLP, to evaluate the framework\u2019s adaptability and performance consistency across varying OCR techniques; (2) a detailed evaluation of the impact that each component recognized by DLP has on the ultimate policy learning. This is particularly pertinent for elements that do not share a direct correlation with dynamics and rewards, such as background features. \n\n **[About the compositional generalization]**  \n\n-  Can the method generalize to the case where the novel objects (e.g., different shape but similar to the ones seen in the training, e.g., cuboid versus cube) exist during the inference phase? \n\n- Does the model possess capabilities for both extrapolation and interpolation with respect to the quantity of objects involved? To illustrate, consider a scenario wherein the model is trained on sets of 2, 4, 6, and 8 objects, and subsequently tested on sets of 3, 5, 7 (interpolation) as well as 1, 9, 10 (extrapolation). While I acknowledge the presence of some relevant results in Figure 5, a more systematic and thorough analysis of the model\u2019s extrapolation and interpolation capabilities would be beneficial. This approach would align with the high-level conceptualization of generalization discussed in [2].\n\n **[About the interaction]**  \n\n-  I would appreciate additional clarification from the authors regarding the nature of entity interactions within the model. From my perspective, these interactions can be broadly classified into two categories: (1) interactions that influence dynamics without impacting the reward, and (2) interactions that affect both dynamics and reward. While I understand that the transformer is capable of capturing both types of interactions, a more explicit discussion on how it accomplishes this, and the implications of these interactions on the model\u2019s performance, would be highly beneficial and contribute to a more thorough understanding of the model\u2019s capabilities.\n\n- I am interested in understanding how the density and frequency of interactions influence the performance of policy learning within the model. Could the authors possibly quantify and assess the model\u2019s precision in predicting interactions across varying levels of interaction density and frequency? One potential metric for this evaluation could be the accuracy of the predicted entity state in comparison to the ground truth state, especially if direct interaction capture proves challenging within the simulator. I hypothesize that a reduction in workspace size, given a constant number of objects, is likely to increase interaction occurrences. Focusing on this aspect would provide valuable insights into the model\u2019s robustness and adaptability under different operational conditions.\n\n **[About the presentation]**\n\nMinor: I would recommend transferring the contents of either Appendix A or E to the main paper. This adjustment not only enhances the overall presentation but also efficiently utilizes the remaining available space (currently less than 9 pages).\n\n\n\n\n\n*References*\n\n[1] Locatello, Francesco, et al. \"Object-centric learning with slot attention.\" Advances in Neural Information Processing Systems 33 (2020): 11525-11538.\n\n[2] Balestriero, Randall, Jerome Pesenti, and Yann LeCun. \"Learning in high dimension always amounts to extrapolation.\" arXiv preprint arXiv:2110.09485 (2021)."
                },
                "questions": {
                    "value": "I list the weaknesses and questions together in the above section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3597/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3597/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3597/Reviewer_sQ6n"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3597/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698781620255,
            "cdate": 1698781620255,
            "tmdate": 1699636314991,
            "mdate": 1699636314991,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cUrIBuQcSm",
                "forum": "uDxeSZ1wdI",
                "replyto": "YvN0Xb1MMU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3597/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3597/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the insightful and constructive feedback, we appreciate your acknowledgement of our work\u2019s contribution.\n\n**Matching**\n\nThe tasks we consider in our experiments require matching between entities in different views of the state as well as matching between state and goal entities. There are several reasons for our choice to take a learning approach for the matching rather than explicitly incorporating modular matching components in the algorithm. The overall rationale behind the decision is achieving flexibility, simplicity and robustness, as follows:\n\n- When working from image observations, a match for each entity does not always exist, for example in the case of occlusion in one of the viewpoints (happens frequently due to the robot arm). This case requires special handling in an explicit matching algorithm and there are not always a set of clear rules as to how to handle these discrepancies. This also requires incorporating additional hyper-parameters such as a threshold for what is considered a match in order to prevent matching with the next closest match even if it is not the same object (e.g matching a red cube to a purple cube by mistake). This increases complexity and hurts flexibility, as the hyper-parameters are not input dependent and may increase the chance of mismatches.\n\n\n- DLP often assigns multiple particles to a single object which creates a situation where there is not a one-to-one match. This makes it difficult to aggregate matching particles to a single entity of fixed size. Our learning approach deals with this automatically via the attention mechanism.\n\n- Generality: we designed our algorithm to have specific structure but try to keep it as general as possible. One direction for future work includes goal specification via natural language. In this case, the state tokens would be particles while the goal tokens would consist of natural language tokens. This is another case where the matching is not straightforward and a one to one match does not necessarily exist. \n\nIn addition, explicit matching is not always a simple task. Matching entities in consecutive observations within a single-view object-centric framework poses significant challenges, as demonstrated in prior works (https://arxiv.org/abs/2107.09240). While incorporating matching algorithms, such as the Hungarian algorithm, is feasible, it introduces computational complexities that hinder scalability to datasets involving more than 3 objects. In our multi-view setting, this challenge intensifies as we must match entities across views, contending with potential occlusions and the absence of a one-to-one correspondence. Furthermore, the extensive literature on multi-view image matching (https://openaccess.thecvf.com/content_ICCV_2017/papers/Maset_Practical_and_Efficient_ICCV_2017_paper.pdf, https://arxiv.org/pdf/2205.01694.pdf) highlights the inherent difficulty of this problem, independent of the additional complexities associated with training a generative object-centric model.\n\n**Evaluation**\n\n(1) We agree this would be an interesting ablation. We intend to experiment with the slot-based Slot-Attention ([1] Locatello et al.), which was shown to perform well in the investigation performed in OCRL ([2] Yoon et al.). \nThis ablation will require some time to find the right hyper-parameters for SLATE and integrate it with our code. We have started implementing this and will conclude the experiments after the rebuttal period.\n\nWe pre-trained Slot-Attention (SA) using the same data as DLP with 10 slots (the maximum number feasible for 128x128 resolution given our computational resources). Notably, the training process was considerably slower than training DLP on similar hardware. Visual results can be found in the following image:  https://i.postimg.cc/PtWm4r1q/slots-25.png. It's important to highlight that, in the SA representation for our data, the robot and the table are often contained in the same slot, and occasionally, multiple cubes are grouped in a single slot instead of being individually represented. We are currently training our model using object-centric representations from the pre-trained Slot-Attention model. However, we anticipate that the imperfect SA decomposition may impact final performance, and we will provide detailed results in the upcoming version.\n\nSlot-Attention image columns: (1) original image, (2) reconstruction, (3) mask, (4) refined mask, (5) ->(14) slots.\n\n\n\n(2) We would like to emphasize that we discard the background features when extracting the latent representation from images, as it is constant in all of our experiments. We can add experiments that ablate some DLP components. Note that our new results on Push-T show that the policy makes use of the \"feature\" component in DLP, as it is the only feature that (implicitly) contains orientation information."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3597/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700414370894,
                "cdate": 1700414370894,
                "tmdate": 1700414848450,
                "mdate": 1700414848450,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0tUoMnMkXh",
                "forum": "uDxeSZ1wdI",
                "replyto": "Kw24VmZeAb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3597/Reviewer_sQ6n"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3597/Reviewer_sQ6n"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed feedback. Most of my concerns have been addressed. It would be nice to see the full ablation in the final or updated version in the future (as the authors promised). Additionally, the updated related work as well as the author's feedback on reviewer SBTK makes the work more complete. \n\nI would like to clarify the question I raised regarding interaction, I mean that sometimes the interactions might directly affect the reward/target. Like the case in this paper: https://openreview.net/forum?id=dYjH8Nv81K. However, the case in the paper I pointed out is not that relevant to those environments you considered in this work. So after reading the rebuttal and the current revision, I do not think it is a question for now. \n\nGiven the detailed clarification and the technical contribution, I would keep the score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3597/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700445568959,
                "cdate": 1700445568959,
                "tmdate": 1700445568959,
                "mdate": 1700445568959,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]