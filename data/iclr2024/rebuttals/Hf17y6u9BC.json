[
    {
        "title": "Towards Best Practices of Activation Patching in Language Models: Metrics and Methods"
    },
    {
        "review": {
            "id": "CYZ9PZxGoG",
            "forum": "Hf17y6u9BC",
            "replyto": "Hf17y6u9BC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2241/Reviewer_PYwb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2241/Reviewer_PYwb"
            ],
            "content": {
                "summary": {
                    "value": "In this paper the authors study the effects of different activation patching techniques on the mechanistic model interpretability and make several recommendations. They study and compare the selection of the hyperparameters, evaluation metrics and input corruption techniques for activation patching based model interpretability. The paper studies two approaches, Gaussian Noise (GN) and Semantic Token Replacement (STR), for corrupting the inputs. Based on those studies the paper recommends using STR input corruption technique since it produces in-distribution samples as opposed to the GN approach which produces OOD samples. In terms of the evaluation metrics, it recommends logit difference since it is more granular and allows to detect model activations (components) that have negative impact on model performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The abstract and introduction are well and clearly written. The problem statement and contributions are easy to follow from those 2 sections.\n2) The paper performs thorough experimentation on sliding window techniques, localizing factual recall and circuit discovery."
                },
                "weaknesses": {
                    "value": "1) The work overall is very interesting but it feels a bit light on the novelty. Perhaps proposing additional novel methods for input corruption and improvements for the activation patching techniques can help to increase the novelty in this paper.\n2) The way the paper is written it might be a good fit for a workshop.\n3) Some terminology could be explained in the paper. E.g. Name Mover\nIt seems that the paper requires prior knowledge of another paper Wang et.al. Some concepts such as: 0.10 negative detection is not very clear.\n4) It might be good to describe clearly what 0.10 negative detection is under the `Negative detection of 0.10 under GN` section.  \n\n\n\nMinor comments:\n1) \u201cuse its own the method of generating\u201d -> \u201cuse its own method of generating\u201d ?"
                },
                "questions": {
                    "value": "1) What are some of the novel contributions of the paper ? \n2) How was the content of Table 1 computed ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2241/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2241/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2241/Reviewer_PYwb"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2241/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698730515877,
            "cdate": 1698730515877,
            "tmdate": 1700584033390,
            "mdate": 1700584033390,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TWDJeNqDxz",
                "forum": "Hf17y6u9BC",
                "replyto": "CYZ9PZxGoG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2241/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2241/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for reviewing our work! We address your concerns below.\n\n> The work overall is very interesting but it feels a bit light on the novelty. Perhaps proposing additional novel methods for input corruption and improvements for the activation patching techniques can help to increase the novelty in this paper.\n\n\nWe believe that our work is significant, since it gives the first rigorous and comprehensive evaluation of a foundational technique.  As we surveyed in the paper and [a rebuttal comment above](https://openreview.net/forum?id=Hf17y6u9BC&noteId=swlVw4U9Z4), a large number of recent papers in interpretability have all used activation patching (AP).\nYet, many works use somewhat different variants in terms of metric or implementation details. Prior to our work, there was no comprehensive study on the consistency of AP. Our paper fills this gap by (i) rigorously evaluating a wide range of techniques within AP and (ii) providing recommendations for its best practices. Hence, while the main contribution of our work may be unglamorous, we believe it is crucial as it provides the rigor for the field moving forward. \n\nIn terms of novelty, our investigation into GN corruption suggests that it may introduce OOD inputs to the model's internal components. We further argue that it may lead to unreliable interpretability claims. To the best of our knowledge, this insight is novel in the literature. It provides an importance evidence for the limitation of the technique. \n\n> The way the paper is written it might be a good fit for a workshop.\n\nOur hope was to write a paper that would be of value to practitioners in LM interpretability who use activation patching in their research, as well as of broad interest to the community. We would appreciate any advice on improving the presentation of our paper.\n\n> Some terminology could be explained in the paper. E.g. Name Mover It seems that the paper requires prior knowledge of another paper Wang et.al. Some concepts such as: 0.10 negative detection is not very clear.\n\n> It might be good to describe clearly what 0.10 negative detection is under the Negative detection of 0.10 under GN section.\n\nThanks for pointing this out! In the end of the first paragraph of Section 3.2, we added the definition that \u201cwe say a detection is negative if the patching effect of the component is negative (under a given metric).\u201d Intuitively, this means that patching suggests the component hurts model performance. We also added Appendix B, an overview of the attention heads in the IOI circuit of GPT-2 small, as found by Wang et al. We hope these clarify the setting of Table 1.\n\n> How was the content of Table 1 computed?\n\nAs detailed in Section 3.1, we say that a head is detected if its patching effect is 2 standard deviations (SD) away from the mean effect. We have also clarified this in the main text: Table 1 is computed by patching each individual head, calculating the patching effects, listing the heads that are detected (according to the definition above) and contrasting with the results from Wang et al."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2241/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699665384672,
                "cdate": 1699665384672,
                "tmdate": 1699667283956,
                "mdate": 1699667283956,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Vf9wFxANPs",
                "forum": "Hf17y6u9BC",
                "replyto": "TWDJeNqDxz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2241/Reviewer_PYwb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2241/Reviewer_PYwb"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "Thank you very much for addressing my comments. \n\n1. For Table 1. it would be good to explain what stands on the left and right sides of ' / '. At this point it's a bit hard to understand.\n2. In a general case, does the paper propose what's the best way of choosing Symmetric token replacement (STR). This could be a time consuming and manual task for many use cases, wouldn't it ?"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2241/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545278210,
                "cdate": 1700545278210,
                "tmdate": 1700545278210,
                "mdate": 1700545278210,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oIov7vQBDi",
                "forum": "Hf17y6u9BC",
                "replyto": "rtricxd38Z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2241/Reviewer_PYwb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2241/Reviewer_PYwb"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you very much for addressing my comments.\nI increased my score by one point."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2241/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700584076069,
                "cdate": 1700584076069,
                "tmdate": 1700584076069,
                "mdate": 1700584076069,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "G6BViLzIzc",
            "forum": "Hf17y6u9BC",
            "replyto": "Hf17y6u9BC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2241/Reviewer_bjM9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2241/Reviewer_bjM9"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors explore how the method of corruption and type of evaluation can cause conflicting take-aways from activation patching. Specifically, they look at two ways to add corruption to a token embedding: gaussian noise (adding noise to the embedding) or symmetric token replacement (replacing a token with a semantically related token). They find that gaussian noise can cause the input to be OOD, thus breaking the internal mechanism. When looking at logit difference vs. probability, they find that probability can overlook negative model components. Finally they look at sliding window patching, and find that it can inflate logit plots."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I like the motivation of this paper: I think its important to understand how hyperparameters can change the results of interpretability methods.\n\nThe authors go over several different types of hyperparameter (corruption method, evaluation method, sliding window) and convincingly show that these design choices can result in different interpretations. I liked the Name Mover analysis, which made it easier to understand how gaussian noise could be causing issues for activation patching. \n\nFinally, I appreciated Section 6, which gives recommendations on how activation patching should be performed."
                },
                "weaknesses": {
                    "value": "My biggest concern is relevance to the community: other than Meng et. al, are there other papers using gaussian noise? It's not clear to me that this is a wide-spread issue. \n\nMoreover, the recommended course of action (STR) can be more difficult to actually implement (as it requires having semantically similar substitutions). To be of most relevance to the community, it would be great if the authors suggested an approach that had the flexibility of gaussian noise without introducing as much bias.\n\nClarity: Table 1 and in particular the part about negative detection was a bit hard to parse. I would make the discussion around those results more clear."
                },
                "questions": {
                    "value": "How much does different substitutions for STR or different samples of noise (for gaussian noise) change the interpretation? Are the methods at least consistent within themselves?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2241/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2241/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2241/Reviewer_bjM9"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2241/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698795866440,
            "cdate": 1698795866440,
            "tmdate": 1700582678662,
            "mdate": 1700582678662,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "s9tWv4PCdY",
                "forum": "Hf17y6u9BC",
                "replyto": "G6BViLzIzc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2241/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2241/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for reading and appreciating our work! We address your questions and concerns below.\n> My biggest concern is relevance to the community: other than Meng et. al, are there other papers using gaussian noise? It's not clear to me that this is a wide-spread issue.\n\nWe remark that the follow-up work of [Meng et al (2023)](https://arxiv.org/abs/2210.07229) and [Hase et al (2023)](https://arxiv.org/abs/2301.04213) both use activation patching with Gaussian noise.  \n\nIn fact, [Meng et at (2022)](https://rome.baulab.info/), though fairly recent, is quite influential in the community, accumulating nearly 200 citations over the year. Thus, we believe more follow-ups of it may emerge in the future. \n\n> Moreover, the recommended course of action (STR) can be more difficult to actually implement.\n\nWe agree, but we also remark that this really depends on specific tasks. In the case of IOI, STR is not difficult to implement, as demonstrated by Wang et al. One can simply identify a collection of single-token names and use them for corruption. In the case of factual recall, it takes some effort, but in our work, we have managed to construct a dataset of 145 symmetric pairs of prompts, as detailed in Appendix C. \n\n> To be of most relevance to the community, it would be great if the authors suggested an approach that had the flexibility of gaussian noise without introducing as much bias.\n\nOne possibility is mean ablation and resample ablation. Here, mean ablation replaces the activation of a model component with the average activation (over a dataset), and\nresample activation averages the effects of different ablations; see [Chan et al 2022](https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing) for further discussions. \n\n We have added a note about this in the paper. We leave it as a future direction to compare ablations and activation patching for circuit analysis. \n\n> Clarity: Table 1 and in particular the part about negative detection was a bit hard to parse. I would make the discussion around those results more clear.\n\nThanks for pointing this out! In the end of the first paragraph of Section 3.2, we added the definition that \u201cwe say a detection is negative if the patching effect of the component is negative (under a given metric).\u201d Intuitively, this means that patching suggests the component hurts model performance. We also added Appendix B, an overview of the attention heads in the IOI circuit of GPT-2 small, as found by Wang et al. We hope these clarify the setting of Table 1.\n\n> How much does different substitutions for STR or different samples of noise (for gaussian noise) change the interpretation? Are the methods at least consistent within themselves?\n\nThis is a great question! In section 6, we investigate the effect of varying the corrupted token position in STR. For example, instead of corrupting S2 (as done in section 3), what happens if we corrupt S1 and IO? As shown in Appendix G, the experiments suggest this leads to different attention heads discoveries. Moreover, in Appendix I.3, we experiment with corrupting all S1, S2 and IO by three random names, in the same way as Wang et al. The results are again somewhat different. We believe, however, that all these discoveries are different perspectives on the same circuit and simply complement each other, since different corruptions just trace different information; see Section 6 for a conceptual argument of the point.\n\nFor GN, [the original work of Meng et al](https://rome.baulab.info/) attempt at different noise levels and noise distributions and find pretty consistent results. We reached the same conclusion in our experiments, and hence didn\u2019t delve into this issue again in the paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2241/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699665071086,
                "cdate": 1699665071086,
                "tmdate": 1699665071086,
                "mdate": 1699665071086,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NWxwc68pGS",
                "forum": "Hf17y6u9BC",
                "replyto": "s9tWv4PCdY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2241/Reviewer_bjM9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2241/Reviewer_bjM9"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your response. The authors have answered my questions. I have raised my score to an accept (8)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2241/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582698474,
                "cdate": 1700582698474,
                "tmdate": 1700582698474,
                "mdate": 1700582698474,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RlN8grsbnW",
            "forum": "Hf17y6u9BC",
            "replyto": "Hf17y6u9BC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2241/Reviewer_4A2t"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2241/Reviewer_4A2t"
            ],
            "content": {
                "summary": {
                    "value": "This paper delves into the realm of mechanistic interpretability, a burgeoning and promising domain within large language models. It primarily centers on activation patching, aiming to identify activations that hold a causal influence over the output. A notable aspect of this work is its pioneering stance in systematically studying the generation of corrupted prompts and the evaluation metrics for patching effects, which previously lacked standardization. Specifically, the paper scrutinizes two methodologies for generating corrupted prompts: 1) Gaussian Noising (GN) and 2) Symmetric Token Replacement (STR). Furthermore, it explores two evaluation metrics: 1) probability and 2) logit difference, alongside investigating the impact of sliding window patching."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The endeavor to understand the internal mechanisms of large language models through activation patching is pivotal. This paper stands out by empirically examining various methodologies, bridging the gap where variations across different papers have made it challenging to ascertain the more effective approach. By embarking on this comprehensive investigation, the paper makes a substantial contribution towards standardizing methods, which is invaluable to the mechanistic interpretability community."
                },
                "weaknesses": {
                    "value": "I am not very familiar with the details of the existing activation patching methods. Therefore, I am not sure whether the methods included in the paper are diverse and representative enough for the mechanistic interpretability community."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2241/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699045070434,
            "cdate": 1699045070434,
            "tmdate": 1699636157157,
            "mdate": 1699636157157,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "n5qLPnJW50",
                "forum": "Hf17y6u9BC",
                "replyto": "RlN8grsbnW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2241/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2241/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for evaluating and appreciating our work! \n\n> I am not very familiar with the details of the existing activation patching methods. Therefore, I am not sure whether the methods included in the paper are diverse and representative enough for the mechanistic interpretability community.\n\nFirst, we note that activation patching (AP), as a generic method, is foundational and widely used in mechanistic interpretability. In particular, the third paragraph of our introduction lists 5 prior works that apply the method. We also refer the reviewer to consider the related work section that gives more examples. In total, one can  identify over 20 papers using AP or its variants in the literature (which [we list in a separate comment below](https://openreview.net/forum?id=Hf17y6u9BC&noteId=swlVw4U9Z4)).\n\nSecond, the metrics and particular variants considered by our work cover a large fraction of existing techniques in AP. Specifically, this includes techniques used in [Meng et al., 2022](https://rome.baulab.info/), [Wang et al., 2023](https://arxiv.org/abs/2211.00593), [Hanna et al., 2023](https://arxiv.org/abs/2305.00586), [Heimersheim & Janiak, 2023](https://www.alignmentforum.org/posts/u6KXXmKFbXfWzoAXn/a-circuit-for-python-docstrings-in-a-4-layer-attention-only) and [Stolfo et al., 2023](https://arxiv.org/abs/2305.15054). All of them are studied in detail in our paper. As we surveyed in the related work section and the comment below, these same techniques are used in other works as well. In addition, we also investigate KL divergence as a metric, used in [Conmy et al., 2023](https://arxiv.org/abs/2304.14997). \n\nFinally, in section 6, we consider the effect of varying the corrupted token position, a rather neglected aspect in the literature. This again adds to the comprehensiveness of our study."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2241/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699664255545,
                "cdate": 1699664255545,
                "tmdate": 1699666968287,
                "mdate": 1699666968287,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]