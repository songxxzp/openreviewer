[
    {
        "title": "Structuring Representation Geometry with Rotationally Equivariant Contrastive Learning"
    },
    {
        "review": {
            "id": "NHP6ag6O5l",
            "forum": "lgaFMvZHSJ",
            "replyto": "lgaFMvZHSJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1028/Reviewer_avuo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1028/Reviewer_avuo"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a contrastive learning framework called CARE, which stands for Contrastive Augmentation-induced Rotational Equivariance. CARE extends the InfoNCE loss by including a term that enforces an equivariant constraint with respect to the image transformations employed during training, as opposed to the traditional contrastive learning that attempts to induce invariance.\n\nThe central concept behind CARE is to ensure that transformations in the input space correspond to local orthogonal transformations in the representation space, a property known as orthogonal equivariance. This is achieved by minimizing a loss term (L_equi) that encourages the angles between pairs of images and their transformed counterparts to remain ideally the same, thus promoting orthogonal equivariance. The authors show theoretically that satisfying their proposed loss (i.e., L_equi = 0) implies orthogonal equivariance. \n\nTo avoid a common issue where all data points collapse to the same location in the representation space, the authors introduce an additional term (L_uni) inspired by the work of Isola and Wang. L_uni encourages representations within a batch to be evenly distributed. However, the combination of L_equi and L_uni alone underperforms compared to traditional contrastive learning frameworks like SimCLR. To address this, the authors introduce a term (L_inv) traditionally used to induce invariance with respect to the applied transformations. The final loss function comprises these three terms: \\lambda * L_equi (minimize angle differences) + L_inv (minimize differences among positive pairs) + L_uni (maximize uniformity).\n\n\u2028The authors assess CARE on a protein point cloud dataset (Protein Data Bank), they also demonstrate that CARE can better induce orthogonal equivariance compared to SimCLR using metrics like Whaba's problem and Relative Rotational Equivariance applied to the embedding of a Resnet-18 architecture trained on CIFAR-10. They qualitatively compare CARE and SimCLR in an image retrieval task, and finally evaluate CARE against SimCLR, MoCo-V2, and BYOL in the linear probing image classification task on various datasets: CIFAR10, CIFAR100, STL10, and ImageNet100."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper tackles an important issue: that of training more generic representation. I appreciate the idea of mixing invariance and equivariance for this objective.\n\nI also find the idea of representing equivariance using angle preservation as a neat idea."
                },
                "weaknesses": {
                    "value": "I think this work has some interesting intuitions and hope that if it is not accepted the authors will continue to make it stronger. While I like the idea of mixing invariance and equivariance I think this work could be strengthened by a more thoughtful use of the contrastive framework and a more solid experimental evaluation. Let me elaborate on both these points. Since the experimental evaluation is, in my opinion, a more impactful weakness I start there, and follow with the use of the contrastive framework. \n\nThe experimental protocols should be improved in order to highlight the benefit of CARE. Results are not clearly showing why one should adopt CARE, some key experiments for this work are missing (transfer learning), and other experiments do not seem comparable with previously presented results.  Specifically,\n\n- The experiments that use the Wahba\u2019s Problem and the Relative Rotation Equivariance should contain some stronger baselines. Wouldn't the authors say that it is expected that CARE achieves better results in terms of equivariance compared to SimCLR (that does not try to achieve orthogonal equivariance)? SimCLR could be a lower bound but it is difficult to say if the results shown by CARE are \"good\" as no other strong comparison is provided. A stronger result would be to show that CARE is comparable or superior to other equivariant inducing algorithms. \n\n- The results on linear probing seems to report results on a not very common setting (training on CIFAR-10, 100, STL, ImageNet100, as opposed to training on ImageNet or ImageNetTiny and testing a linear probe on those datasets). For example see table Table 3 of BYOL, or Table 8 of SimCLR , or Table 3 (RRC column) of Duet. All these results were obtained training on ImageNet and testing on other datasets (including Cifar10 and Cifar100) and report much stronger results. I believe these results (i.e. Transfer Learning) are crucial for this work because they could support the claim that by using equivariance one can learn more generic and transferable features. Training and testing on the same dataset it is less interesting in my opinion. \n\n- Even for the few results that can be found about training SimCLR on CIFAR-10 (for example) the results reported seem below previously reported numbers. The original SimCLR paper reports results on CIFAR 10 in the Appendix and the accuracy is 94% (compared to 90.98%). My experience with contrastive frameworks is that once the pre-training pipeline is optimized and the training scales up (in terms of datasets, batch size, and epochs) the initial advantages one might observe by tweaking the model reduce until they (very often) disappear altogether. It would be great to show that this does not happen with equivariance.\n\n- On the protein point clouds experiments I find unclear to assess what is the desired (correct) trajectory. Perhaps this is due to my lack of experience with this protein task but the manuscript could explain more clearly what is the expected result (Figure 3). Also, it is known that SimCLR requires large batches (hence large datasets), I am not familiar with this dataset but what I wonder if its size makes it a suitable application for pre-training with SimCLR. Maybe a better test would be to pre-train on a larger dataset and fine tune on this dataset?\n\n- Similarly, the qualitative results on the image retrieval tasks are difficult to assess. I thought the results from SimCLR were better. This task should be explained more clearly and it would be even better to employ some quantitative evaluation (if possible). I also do not fully understand what is the role of the \u201cinput\u201d compared to that of the \u201cquery\u201d. \n\nIn terms of the contrastive learning framework, if I understand correctly all three terms of the proposed algorithm act on the same representation space (after projection head). This however introduces an ambiguity: is the objective to induce invariance or equivariance? While it is true that invariance satisfies equivariance it is also true that satisfying equivariance through invariance leads to the loss of information typical of the invariance-inducing algorithms (as the authors correctly mention in the paper). It is also why the common contrastive learning algorithms based on invariance impose this constraint after the projection head but use the representation before (where \"hopefully\" invariance is not achieved strongly). Therefore, while the equivariance idea is great and the use of angles is an interesting idea, its application seems to have \"just\" a regulation effect (especially given that the \\lambda value is so small). A more interesting (albeit I recognize this comment falls in the realm of speculations) use of the contrastive framework would be to impose equivariance on the embedding before the projection head (the embedding that are actually used for downstream tasks) and invariance after the projection head. By doing so, one can hope to take full advantage of the equivariant properties while still leveraging the constructive learning framework and the benefit of the invariance loss."
                },
                "questions": {
                    "value": "My question mostly revolve around the weaknesses listed above. Is there any misunderstanding in the list I provided above?\n\nIn addition\n- Would you agree that it make sense to compare Wahba\u2019s Problem and the Relative Rotation Equivariance with another equivariance inducing method (in addition to SimCLR which is not inducing equivariance)? And that without this comparison it's difficult to assess \"how good\" the results shown are.\n- Would you be able to provide transfer learning results? Would you agree that those are ideal to show that CARE is learning more generic features?\n- Could you resolve the issue on the CIFAR-10 classification results? Why SOTA report 94% accuracy while your version is at 90%?\n- Could you provide more clarity on the protein folding experiment? \n- Could you provide more clarity (and maybe quantitative metrics) on the image retrieval task?\n- One of the claim is that CARE can handle composition of transformations but it is unclear if there is any experiment that support this claim. I understand that this can be proven theoretically, however, L_equi is never zero and its contribution if further reduced by the \\lambda factor, hence in practice it remains to be seen if CARE can handle equivariance of composition of transformations. Or is it the case that the augmentations are random resize crop + rotation, hence the equivariance is achieved for the combination of these two?\n- Lastly, not a question but I hope you take on the suggestion of trying to impose the equivariance constraint on the embeddings used for downstream task rather than the one after the projection head."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1028/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1028/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1028/Reviewer_avuo"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1028/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698745667701,
            "cdate": 1698745667701,
            "tmdate": 1700495480999,
            "mdate": 1700495480999,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "01ZcUJHowN",
                "forum": "lgaFMvZHSJ",
                "replyto": "NHP6ag6O5l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1028/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1028/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Reviewer avuo"
                    },
                    "comment": {
                        "value": "We appreciate your critical and thorough feedback, which has been valuable in enhancing our work. We are glad you appreciated the technical novelty and analysis, and believe this could form the basis for a positive review. Below, we aim to answer their questions as concretely as possible. The changes have also been incorporated into the revised manuscript and are highlighted in magenta. \n\n---\n\n> Comparison of CARE with another equivariant baseline for the Wahba\u2019s problem\n\nThank you for this valuable suggestion. We have included the following additional experiments in our revised manuscript (highlighted in magenta) to address your comments about our experimental protocols as concretely as possible.\n\n\n- Wahba error for a model trained using EquiMOD on CIFAR10 in Figure 5. As shown in the figure, CARE achieves the lowest error on Wahba's problem, highlighting its ability to learn an _orthogonally equivariant_ representation.\n\n- EquiMod, an equivariant contrastive learning baseline, in our protein point cloud experiment (Figures 3 and 4, as shown in the updated manuscript). Figure 3 provides a qualitative comparison of trajectories, demonstrating that EquiMod does not exhibit a rotational structure and is qualitatively closer to SimCLR than CARE. Figure 4 presents quantitative results, showing that CARE outperforms EquiMod on the principal component prediction task.\n\n- Qualitative assessment of the representation learned by EquiMOD in Figure 21 on Flowers102. Both CARE and EquiMOD, being equivalent baselines, show sensitivity to color.  However, EquiMOD's representation exhibits nearest neighbors with significantly different shades (e.g., red and orange) compared to those learned by CARE, which are closer in color to the query images. Note that this experiment assesses CARE's ability to learn equivariance and not orthogonal equivariance. Thus, any equivariant baseline would exhibit this sensitivity to input transformations (color variations in this case).\n\n- Additionally, we examine the quality of features learned by training the Barlow Twins [1] invariant baseline with our equivariant loss $\\mathcal{L}\\_{\\text{equiv}}$. As shown below and in Table 1 of the revised manuscript, $\\text{CARE}\\_{\\text{Barlow Twins}}$ outperforms its invariant counterpart on all three datasets, CIFAR10, STL10 and CIFAR100. \n\nAlgorithm | CIFAR10 | CIFAR100 | STL10\n|----------|----------|----------|----------|\nBarlow Twins | 84.54 $\\pm$ 0.02 | 55.54 $\\pm$ 0.05 | 90.62 $\\pm$ 0.02\n$\\text{CARE}_{\\text{Barlow Twins}}$ | **85.65 $\\pm$ 0.05** | **56.76 $\\pm$ 0.02** | **90.92 $\\pm$ 0.01** |\n\n---\n\n\n\n> Transfer learning experiments \n\nWe acknowledge the reviewer's point regarding the common practice of evaluating linear probe performance for a model trained with ImageNet on other datasets like CIFAR10, CIFAR100, and STL10 when proposing invariant baselines. However, it's worth noting that existing equivariant research [1, 2] often conducts experiments involving pretraining and testing on the same datasets, with CIFAR10 and ImageNet being popular choices. In line with this convention, we test our approach on smaller benchmarks (CIFAR10, CIFAR100, STL10) and a larger dataset (ImageNet100) as a computationally efficient alternative to conducting experiments directly on ImageNet, which is beyond our resource limits.\n\n[1] Dangovski, Rumen, et al. \"Equivariant contrastive learning.\" arXiv preprint arXiv:2111.00899 (2021).\n\n[2] Devillers, Alexandre, and Mathieu Lefort. \"Equimod: An equivariance module to improve self-supervised learning.\" arXiv preprint arXiv:2211.01244 (2022).\n\n---\n\n\n> Desired correct trajectory for the protein experiment. Could the size of the dataset or batch be responsible for lower performance in this experiment for SimCLR. \n\n- **About the dataset and desired trajectory.** In this experiment, we use the Protein Data Bank (PDB)\u2014the single global repository of experimentally determined 3D structures of biological macromolecules and their complexes, containing approximately 130,000 samples. The core objective is to assess whether our method can effectively encode the SO(3) manifold, a highly challenging and critical aspect in drug discovery. Each input point cloud is transformed through action of the SO(3) group. Consequently, the desirect 2D trajectory is a circle corresponding to rotation along each of three orthogonal axes. This is consistent with the structure of the SO(3) manifold. FIgures 3, 17, 18, 19 and 20 illustrate additional trajectories observed through the embedding space, showing that CARE is adept at learning the SO(3) manifold. We had added additional clarification in the revised manuscript\n\n- **Failure of SimCLR.** The reason for low performance for SimCLR is its inability to encode any information about the input transformation i.e. rotation in 3D. As a consequence, it is unable to learn the orientation of a given protein. Note that the batch sizes and architectural choices were fixed for both CARE and SimCLR."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1028/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700275783684,
                "cdate": 1700275783684,
                "tmdate": 1700275895362,
                "mdate": 1700275895362,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8fXCRfMDoW",
                "forum": "lgaFMvZHSJ",
                "replyto": "NHP6ag6O5l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1028/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1028/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Number reported on some datasets such as CIFAR10 are below the previously reported numbers\n\nThe 94\\% top-1 accuracy reported in the appendix of the original SimCLR paper [1] corresponds to training the model with a batch size of 1024 and over 1000 epochs. Figure B.7 in this paper reports a performance of approximately 92\\% using a batch size of 256 and 400 as the number of pre training epochs. A popularly referenced paper for CIFAR10 baseline numbers [2], reports a top-1 accuracy of 91.1\\% for a batch size of 512 and 800 pre-training epochs. EquiMOD [3], another paper in the literature of equivariance in contrastive learning reports an accuracy of 90.98\\% with a batch size of 512 and 800 pre-training epochs. \n\nAs rightly highlighted in your review, the quality of representation learned by SimCLR and its performance on the downstream tasks are significantly influenced by the training batch size, choice of architecture, number of training epochs etc. Due to computational constraints, we experimented with a fixed batch size of 256 and 400 training epochs across experiments. Given our hyper-parameters, we believe our reported number of 90.98\\% are consistent with the reported numbers in the above works.\n\n[1] Chen, Ting, et al. \"A simple framework for contrastive learning of visual representations.\" International conference on machine learning. PMLR, 2020.\n\n[2] Chen, Xinlei, and Kaiming He. \"Exploring simple siamese representation learning.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.\n\n[3] Devillers, Alexandre, and Mathieu Lefort. \"Equimod: An equivariance module to improve self-supervised learning.\" arXiv preprint arXiv:2211.01244 (2022).\n\n\n---\n\n> Qualitative results on the image retrieval tasks are difficult to assess. What is the role of \u201cinput\u201d as opposed to \u201cquery\u201d\n\u200b\u200b\n\nIn Figure 9, the \u201cinput\u201d ($x$) corresponds to the original batch of images from the dataset, while \u201cquery\u201d ($\\tilde{x}$) refers to the transformed batch obtained using a randomly sampled augmentation $a$. The trained model receives $\\tilde{x} = a(x)$ as input, and the top 5 nearest neighbors of its embedding i.e. $f(a(x))$ are extracted based on the Euclidean distance. The images corresponding to these top 5 nearest neighbor embeddings are displayed for both CARE and SimCLR. \n\nIdeally, a representation equivariant to color variations in the input space must change the retrieved results (top 5 neighbors) in response to a change in query color. From Figure 9, CARE qualitatively performs better than SimCLR in the visual retrieval task, as its retrieved neighbors have colors similar to that of the query. In contrast, the retrievals for SimCLR remain relatively static.\n\n\n\n---\n\n\n> Use of equivariant loss before the projection head and invariance loss after the projection head \n\nIndeed, this is an excellent point. However, we tried this variant early in our testing, but found that it did not improve downstream performance. For instance we found that with ResNet18 on CIFAR10, we achieve the following kNN accuracies:\n\nAlgorithm | Top-1 kNN accuracy| \n|----------|----------|\nLoss before projection | 86.0% | \nLoss after projection | **86.7%** |\n\n\n---\n\n\n> On the consistency under compositions exhibited by CARE\n\nThank you for raising this excellent question. We have revised the manuscript to include an approximate equivariance bound (Section A.1 and B.1) when the equivariant loss is small but non zero.  We briefly describe the results below. \n\n- To generalize Corollary 3 in our submission, assume that the loss is bounded (not exactly zero), so $||AA^\\top - BB^\\top|| < \\epsilon^2$ in the 2-norm for some error $\\epsilon^2 > 0$. Then, using Theorem 1 in [1], we can demonstrate that $\\min_{R \\in O(d)} ||A - BR|| < o(\\epsilon)$, where $o(\\epsilon) \\to 0$ as $\\epsilon \\to 0$. As $\\epsilon$ (and consequently the loss) approaches zero, we recover our exact equivariance result. For $\\epsilon > 0$, we achieve an approximate equivariance up to $o(\\epsilon)$.\n\n- We further show that CARE enjoys consistency under composition of transformations even under low equivariant loss. Mathematically, $\\rho : \\mathcal A \\rightarrow O(d)$ given by $\\rho(a) = R_a$ satisfies\n$||\\rho(a' \\circ a) - \\rho(a')\\rho(a)|| \\leq o(\\epsilon^2)$ for almost all $a,a' \\in \\mathcal{A}$, where $o(\\epsilon) \\to 0$ as the error $\\epsilon \\to 0$.\n\nFurthermore, we would like to clarify that CARE is tested within the standard contrastive learning framework, where each input transformation is already a composition of random crop, random horizontal flip, color jitter, random grayscaling, and Gaussian blurring.\n\n[1] Arias-Castro, Ery, Adel Javanmard, and Bruno Pelletier. \"Perturbation bounds for procrustes, classical scaling, and trilateration, with applications to manifold learning.\" Journal of machine learning research 21 (2020)."
                    },
                    "title": {
                        "value": "Response to the Reviewer avuo (continued)"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1028/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700276013470,
                "cdate": 1700276013470,
                "tmdate": 1700276552591,
                "mdate": 1700276552591,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gRWWNaiwss",
                "forum": "lgaFMvZHSJ",
                "replyto": "8fXCRfMDoW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1028/Reviewer_avuo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1028/Reviewer_avuo"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to these authors for taking into account my comments and providing more clarity to some of their experiments. \nI would like to provide a few comments to these answers. \n\n> Comparison of CARE with another equivariant baseline for the Wahba\u2019s problem\n\nIt seems that EquiMod obtains even worse performance than SimCLR. Do the authors have any intuition about why this is the case?\n\n> Transfer learning experiments\n\nAdmittedly, it is true that in other publications authors performed similar experiments, however, I believe that this is an important experiment to show that the equivariance achieved in the embedding produced by CARE leads to more general representations. Without such experiment the contribution, in my opinion, remains theoretical rather than adding a practical extra value.\n\n> Desired correct trajectory for the protein experiment\n\nThank you for the clarification, now I understand the results better.\n\n> Number reported on some datasets such as CIFAR10 are below the previously reported numbers\n\nThank you for brining clarity to these differences. They are as I suspected related to not performing the \"correct and complete\" optimization, which, in my experience often leads to wrong conclusions. I appreciate that training with large batch sizes and epochs is a demanding task. In my experience, however, many small gains that one can appreciate in a lower scale regime completely disappear in large scale settings. I cannot say that CARE would also become equivalent (or worse) than SimCLR but neither could I assume it will not. Similarly to my above point, showing the CARE actually is better than SimCLR would have strengthened the empirical results. \n\n> Qualitative results on the image retrieval tasks are difficult to assess. What is the role of \u201cinput\u201d as opposed to \u201cquery\u201d \n\nThank for clarifying the difference between input and query. I still have one doubt. What is the desired outcome? Is an image that exhibit the same content as the input? Same color as query? Or both (content of input and color of query)? It seems to me by looking at Figure 5 and 21 that if the objective is to preserve content then both SimCLR and EquiMOD are better than CARE (3 images seems daisy in CARE, 19ish for SimCLR and 16ish in EquiMOD), if the objective is to preserve both then EquiMOD does a better job than CARE (3 images seems yellow daisy in CARE, 16ish in EquiMOD, only 2 in SimCLR). The only setting where CARE does better is to match the color (irrespective of the type of flower). \n\n> Use of equivariant loss before the projection head and invariance loss after the projection head\n\nThanks for performing this experiment. Interesting to see that after the project is slightly better\u2026 I wonder if this is an indication that the wrong loss is being optimized\u2026 My reasoning is that if the loss was \u201cperfect\u201d then the embedding immediately before the optimization should be ideal, if we need to \"backtrack\" and truncate the model then maybe the loss is less then idea (admittedly, this comment is true for all contrastive framework, not just for CARE).\n\n> On the consistency under compositions exhibited by CARE\n\nThanks for this analysis.\n\nAfter these clarifications I think this work is interesting from a theoretical perspective although I am skeptical about its practical use cases. Most of the experiments are still weak (although improved compared to the initial manuscript) since they do not compare with respect to the best state of art results, or settings. As mentioned above I expect these small gains to disappear once batch size and epochs are scaled up. The qualitative evaluation in my opinion is unfavorable to CARE. I do, however, recognize the merit of the proposal in terms of theoretical innovation, for this reason I will upgrade my recommendation."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1028/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700487825478,
                "cdate": 1700487825478,
                "tmdate": 1700487825478,
                "mdate": 1700487825478,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2f8jXee32d",
                "forum": "lgaFMvZHSJ",
                "replyto": "xqyUltQGp0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1028/Reviewer_avuo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1028/Reviewer_avuo"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to these authors for their additional thoughts."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1028/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662580703,
                "cdate": 1700662580703,
                "tmdate": 1700662580703,
                "mdate": 1700662580703,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UomgAA86ZQ",
            "forum": "lgaFMvZHSJ",
            "replyto": "lgaFMvZHSJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1028/Reviewer_MvBZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1028/Reviewer_MvBZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a self-supervised loss that demonstrably enforces a connection between latent embeddings of related complex augmentations through \"simple\" linear (rotational) transformations.\nThe loss is theoretically analyzed, showing that rotational equivariance emerges when scalar product in the embedding space are preserved under augmentations. Furthermore, the generalization of this fact to other bilinear forms and geometries is discussed.\nThe approach is assessed and demonstrated to enhance linear probing performance on image datasets. Additionally, it is qualitatively evaluated and shown to capture equivariance as intended by the suggested loss."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall the paper is well written and easy to follow.\n\nThe suggested loss simplifies existing equivariant techniques as it alleviates the need to learn the equivariance transformation, making it emerge solely as a result of an optimized loss term.\n\nThe method is theoretically analyzed. The analysis seems to be solid.  I appreciate the discussion about the possible generalizations to different geometries."
                },
                "weaknesses": {
                    "value": "It seems that the main weakness of the paper is in the evaluation section.\nFirst, I would expect to see both qualitative (figure 9)   and quantitative comparisons to methods that learn the equivariant transformation (such as Garrido et al.). \nSecondly, the evaluation metric suggested in the Wahba\u2019s Problem, seems to be another possible alternative loss to the suggested equivariance loss. Why shouldn\u2019t it be used to optimize directly?\nLastly, it would also be interesting to compare to explicit parametrization of R_a.\n\nAdditionally, I found Figure 8 and the paragraph discussing relative rotational equivariance to be somewhat unclear. It appears that they could benefit from a revision to enhance clarity."
                },
                "questions": {
                    "value": "No specific questions. I would appreciate a response with respect to the weakness stated above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1028/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811071620,
            "cdate": 1698811071620,
            "tmdate": 1699636029118,
            "mdate": 1699636029118,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Guk8ha6UOo",
                "forum": "lgaFMvZHSJ",
                "replyto": "UomgAA86ZQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1028/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1028/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Reviewer MvBZ"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's thorough and insightful review, along with their positive feedback regarding the strengths of our work, including our theoretical analysis and discussions on extending the approach to other groups and representation space geometries. In response to the review, we have highlighted the changes in magenta in the revised manuscript and address the questions raised below. \n\n---\n\n> Why not optimize the Wahba\u2019s Problem as an alternate loss to our equivariance loss. \n\nThis is indeed an excellent question.  While Wahba's error, denoted as $\\mathcal{W}\\_f$ = $\\min_{R \\in SO(d)} || RF - F_a ||_{\\text{Fro}}$, seems like a plausible loss function to enforce orthogonal equivariance, it faces practical challenges in real-world settings for two important reasons:\n\n- **Computational Inefficiency**: Solving the Wahba's problem involves obtaining a closed-form solution through the singular value decomposition of one of the two point clouds. Minimizing the Wahba's error in each iteration is computationally intensive, with a complexity of $O(m^2n, mn^2, n^3)$, where $m$ and $n$ are the dimensions of the involved matrices. In our context, the computational complexity is dominated by $O(bs * dim^2)$, where $bs$ is the batch size, and $dim$ is the dimensionality of the representation (e.g., 2048 for ResNet50). In contrast, our proposed loss achieves the same objective by operating on pairs, with the same complexity as standard invariant contrastive baselines.\n\n- **Noise and instability**: The closed-form solution to the Wahba's problem, obtained through singular value decomposition, can introduce instability and noise in calculations. These issues can accumulate over iterations, particularly when dealing with noisy data. In some cases, even minor data perturbations can lead to substantial changes in the decomposed components, especially in the smallest singular values and corresponding vectors. This noise becomes more pronounced when working with ill-conditioned matrices.\n\n\n---\n\n\n> Both qualitative and quantitative comparisons to equivariant methods\n\nThank you for this valuable suggestion. We have included the following additional experiments in our revised manuscript (in magenta)\n\n- EquiMod, an equivariant contrastive learning baseline, in our protein point cloud experiment (Figures 3 and 4, as shown in the updated manuscript). Figure 3 provides a qualitative comparison of trajectories, demonstrating that EquiMod does not exhibit a rotational structure and is qualitatively closer to SimCLR than CARE. Figure 4 presents quantitative results, showing that CARE outperforms EquiMod on the principal component prediction task.\n\n- Wahba error for a model trained using EquiMOD on CIFAR10 in Figure 5. As shown in the figure, CARE achieves the lowest error on Wahba's problem, highlighting its ability to learn an _orthogonally equivariant_ representation. \n\n- Qualitative assessment of the representation learned by EquiMOD in Figure 21 on Flowers102. Both CARE and EquiMOD, being equivalent baselines, show sensitivity to color.  However, EquiMOD's representation exhibits nearest neighbors with significantly different shades (e.g., red and orange) compared to those learned by CARE, which are closer in color to the query images. Note that this experiment assesses CARE's ability to learn equivariance and not orthogonal equivariance. Thus, any equivariant baseline would exhibit this sensitivity to input transformations (color variations in this case).\n\n- Additionally, we examine the quality of features learned by training Barlow Twins [1], an invariant baseline with our equivariant loss $\\mathcal{L}\\_{\\text{equiv}}$. As shown below and in Table 1 of the revised manuscript, $\\text{CARE}_{\\text{Barlow Twins}}$ outperforms its invariant counterpart on all three datasets, CIFAR10, STL10 and CIFAR100. \n\nAlgorithm | CIFAR10 | CIFAR100 | STL10\n|----------|----------|----------|----------|\nBarlow Twins | 84.54 $\\pm$ 0.02 | 55.54 $\\pm$ 0.05 | 90.62 $\\pm$ 0.02\n$\\text{CARE}_{\\text{Barlow Twins}}$ | **85.65 $\\pm$ 0.05** | **56.76 $\\pm$ 0.02** | **90.92 $\\pm$ 0.01** |\n\n[1] Zbontar, Jure, et al. \"Barlow twins: Self-supervised learning via redundancy reduction.\" International Conference on Machine Learning. PMLR, 2021.\n\n---\n\n\n> Comparison with explicit parametrization of $R_a$.\n\nWe agree that comparing with the direct parameterization of augmentations $a$ would be an interesting avenue of research. However, one of the primary objectives of our work was to design an equivariant approach that _did not_ rely on parameterizing augmentations $a$ and instead learns to be equivariant to them directly from data pairs. We view this as a possible disadvantage since it relies on having convenient features describing a particular augmentation sample. Although not for rotational equivariance, this approach has been explored in related works such as EquiMod, which we compare to in Table 1."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1028/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700275361696,
                "cdate": 1700275361696,
                "tmdate": 1700275361696,
                "mdate": 1700275361696,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "muDMidWDzq",
                "forum": "lgaFMvZHSJ",
                "replyto": "UomgAA86ZQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1028/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1028/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Reviewer MvBZ (continued)"
                    },
                    "comment": {
                        "value": "> Figure 8 and the paragraph discussing relative rotational equivariance\n\nWe apologize for the confusion regarding Figure 8, which has been updated to address the initial duplication issue. It illustrates three key metrics from left to right: the relative equivariance metric $\\gamma_f$, the rotational equivariance measure, and the invariance measure (as detailed in Section 4).\n\nHere are the two key observations from this plot:\n\n- The invariance measure, $\\mathcal{L}_{\\text{inv}}$, is comparable for both SimCLR and $\\text{CARE}\\_{\\text{SimCLR}}$ and is non-zero, implying approximate invariance.\n- The rotational equivariance measure is significantly lower for $\\text{CARE}\\_{\\text{SimCLR}}$ compared to SimCLR, indicating orthogonal equivariance in $\\text{CARE}\\_{\\text{SimCLR}}$.\n\nThese observations suggest that $\\text{CARE}_{\\text{SimCLR}}$ is not merely achieving lower equivariance error by collapsing into invariance, which would be a trivial solution of equivariance. We have included this clarification in the revised manuscript for better understanding."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1028/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700275435429,
                "cdate": 1700275435429,
                "tmdate": 1700276827488,
                "mdate": 1700276827488,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EZVAIjFWNB",
                "forum": "lgaFMvZHSJ",
                "replyto": "Guk8ha6UOo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1028/Reviewer_MvBZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1028/Reviewer_MvBZ"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "I want to express my gratitude to the authors for providing a thorough response.\n\nMost of my concerns were addressed. I do encourage the authors to consider for the next revision, conducting a comparison versus Wahba\u2019s Problem-based loss, to validate their claim about its instability."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1028/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688248173,
                "cdate": 1700688248173,
                "tmdate": 1700688248173,
                "mdate": 1700688248173,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "M4fcU8BKiP",
            "forum": "lgaFMvZHSJ",
            "replyto": "lgaFMvZHSJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1028/Reviewer_4N3v"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1028/Reviewer_4N3v"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel approach to self-supervised learning, adding geometric structure to the embedding space such that input transformations correspond to linear transformations in the embedding space. In the context of contrastive learning, the study presents an equivariance objective, which theoretically ensures that data augmentations in the input space align with rotations in the spherical embedding space. This method, named CARE, not only enhances performance in subsequent tasks but also captures essential data variations, like color, which standard methods overlook."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The proposed equivariant contrastive learning method that maps transformations of the input to local orthogonal transformations in the embedding space is new. The authors provide theoretical arguments and show empirical evidence for the desired structure of the embedding space. Both the method and its justification are novel and valuable.\n* The analysis of the structure of the learned representation space is solid and interesting. Fig. 9 is a particularly insightful and sheds light on the merits of the proposed method."
                },
                "weaknesses": {
                    "value": "* It would be beneficial to delve deeper into the influence of the choice of A\u2014the space of transformations experienced during training\u2014on the structure and caliber of the representations derived. An intriguing question to address is the method's capacity to generalize: Can the structures learned be effectively transferred to other classes or varied transformation parameter ranges? Exploring these nuances could further solidify the robustness and versatility of the method.\n* While observing the performance metrics, one notices that the performance gap between CARE and SimCLR on CIFAR10 and STL10 is <1%.  It raises the question of the actual significance and practical implications of this difference. For a more comprehensive understanding, it would be great if Table 1 could also report the variance alongside the mean.\n* The analysis is limited to ResNet networks. How well do the findings generalize to other architectures? How does the architecture affect the properties of the learned representations? How well does the proposed method generalize to other domains beyond the standard computer vision datasets?"
                },
                "questions": {
                    "value": "* In many practical applications input transformations may not preserve distances and angles. What does CARE learn in this case?\n* Fig. 8 is confusing: no legend, all three plots are the same (?)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1028/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1028/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1028/Reviewer_4N3v"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1028/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698830927494,
            "cdate": 1698830927494,
            "tmdate": 1699636029028,
            "mdate": 1699636029028,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "i8rhK0eMFx",
                "forum": "lgaFMvZHSJ",
                "replyto": "M4fcU8BKiP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1028/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1028/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Reviewer 4N3v"
                    },
                    "comment": {
                        "value": "We are grateful to the reviewer for the time they put in to review our work. We are glad to see that they recognize several strengths in our work, including theoretical guarantees, comprehensive empirical evaluation, and qualitative measures for assessing the structure of learned representation. Below, we share our thoughts on the questions asked. \n\n---\n\n > Can the structures learned be effectively transferred to other classes or varied transformation parameter ranges?\n\nThank you for your valuable suggestion. We would like to emphasize that the key aim of this paper is to learn a truly equivariant representation which maps complex input transformations to isometries in the embedding space. Assessing the generalization capabilities of the learned representation across a wide range of transformation parameters is indeed a very intriguing future direction and warrants thorough investigation.\n\n---\n\n\n> Reporting variance along with the mean due to small performance differences on CIFAR10 and STL10.\n\nThis is indeed a crucial point, and we have updated the manuscript to include both the average performance and standard deviations for clarity. As shown in the results summarized below, CARE outperforms its invariance counterparts by more than the corresponding standard deviation.  \n\nAlgorithm | CIFAR10 | CIFAR100 |STL10 | ImageNet100 |\n|----------|----------|----------|----------|----------|\nSimCLR | 90.98 $\\pm$ 0.10| 66.77 $\\pm$ 0.34 | 84.19 $\\pm$ 0.13| 72.79 $\\pm$ 0.08\n$\\text{CARE}_{\\text{SimCLR}}$ | **91.92 $\\pm$ 0.12** | **68.05 $\\pm$ 0.28** | **84.64 $\\pm$ 0.29** |**76.69 $\\pm$ 0.08**\n| | | | |\nMoCo-v2 | 91.95 $\\pm$ 0.05| 69.88 $\\pm$ 0.23| - | 73.50 $\\pm$ 0.19\n$\\text{CARE}_{\\text{MoCo-v2}}$| **92.19 $\\pm$ 0.01** | **70.56 $\\pm$ 0.15** | 88.97 $\\pm$ 0.48 | **74.30 $\\pm$ 0.07**\n\n---\n\n\n\n> On the generalization to other architectures and domains beyond computer vision. \n\nResNets are a popular and common choice of architecture for contrastive learning and are also used in our experiments.  To address the concern of the reviewer, we run additional baselines on the CIFAR10 dataset using both SimCLR and $\\text{CARE}_{\\text{SimCLR}}$ with DenseNet121. CARE outperforms the baseline on top-1 linear probe accuracy as shown below\n\nAlgorithm | Linear Probe Accuracy\n|----------|----------|\nSimCLR | 82.45\n$\\text{CARE}_{\\text{SimCLR}}$ | **83.56**\n\nFurther, in the original manuscript, we also demonstrate the effectiveness of our approach in learning the SO(3) manifold on 3D protein structures using a DeepSet architecture, which is significantly different from 2D vision models. Qualitative analysis to assess the learned structure in the representation is also conducted for the Flowers 102 dataset apart from the standard linear probe experiment on the vision datasets. \n\n---\n\n\n> In many practical applications input transformations may not preserve distances and angles. What does CARE learn in this case?\n\nIn our setup, we train a model to map complex input transformations like random cropping, color jitter, and random flipping to orthogonal transformations in the embedding space. Note that these input transformations typically do not preserve distances or angles for a general encoder. Even so, the underlying mechanism of CARE - to transform data into an embedding space where the complex input transformations become orthogonal transformations. This is provably possible for any transformation that belongs to a compact Lie group, so long as the embedding space dimension is large enough (this is also discussed after Corollary 1). \n\n---\n\n\n> Fig. 8 is confusing\n\nWe apologize for the confusion regarding Figure 8, which has been updated to address the initial duplication issue."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1028/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700275160705,
                "cdate": 1700275160705,
                "tmdate": 1700276406161,
                "mdate": 1700276406161,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SES6DJCfFF",
                "forum": "lgaFMvZHSJ",
                "replyto": "i8rhK0eMFx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1028/Reviewer_4N3v"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1028/Reviewer_4N3v"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for the effort they put into addressing my concerns."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1028/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691036189,
                "cdate": 1700691036189,
                "tmdate": 1700691036189,
                "mdate": 1700691036189,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bqfUkL0htu",
            "forum": "lgaFMvZHSJ",
            "replyto": "lgaFMvZHSJ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1028/Reviewer_Dtcq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1028/Reviewer_Dtcq"
            ],
            "content": {
                "summary": {
                    "value": "The authors consider learning an embedding $f$ for high-dimensional data into a\nstructured latent space using (unsupervised) contrastive-learning-type methods.\nThey propose a new loss function (in implementation, effectively a new\nregularizer) for learning this embedding, which builds on prior work on\nequivariant regularizers. The regularizer asks not for the embedding $f$ to be\ninvariant to augmentations $a(x)$ of the input $x$, as in previous contrastive\nlearning methods such as SimCLR, but that it maps those augmentations to simple\nlinear transformations of the input, say $f(a(x)) = T_a f(x)$; it does this\nindirectly via an equivariance regularizer that the authors prove yields the\naforementioned structure on $f$ when exactly minimized. The precise\nimplementation becomes using this regularizer on top of existing contrastive\nlearning losses, such as SimCLR or InfoNCE, to prevent trivial embeddings and to\nencourage (in some sense) $a \\mapsto T_a$ to be \"continuous\". Experiments\ndemonstrate on simple datasets that the method improves the embeddings with\nrespect to two equivariance metrics; it improves linear probe performance on\nImageNet-100 scale image classification over SimCLR/MoCo-v2 (in the best case,\nby a significant margin); and that its design choices are necessary for these\nimprovements, via ablations."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is very well written. Conceptual explanations are clear, theoretical\n  results are precisely phrased and explained (mercifully, the representation\n  theory is kept to an absolute minimum, which seems uncommon in this area), and\n  the general writing is engaging and compelling.\n\n- The experimental evaluation is solid: it proposes reasonable metrics to assess\n  learned equivariance, shows that CARE improves over baselines on these\n  metrics, demonstrates improved linear probe performance at reasonable scale,\n  and gives some useful ablations in the appendix.\n\n- The inclusion of a theoretical (mostly conceptual-type) basis for the method\n  is valuable, and completes a well-rounded presentation of the method and its\n  motivations."
                },
                "weaknesses": {
                    "value": "- The use of both an invariant and equivariant loss in the overall objective\n  function seems conceptually strange (although the ablations show it leads to\n  superior performance). I would like to understand what might be being learned\n  with this combination of losses -- my reading of the explanation in section 3\n  is that, among embeddings that *minimize* the equivariant regularizer (following\n  proposition 1), those that achieve a small invariant loss will prefer small,\n  rather than large, orthogonal transformations. However, it is not clear to me\n  why this setting should arise in experiments -- why not a situation where the\n  invariance loss is minimized, and the equivariant regularizer is only small?\n  I am also unsure how this connects with the discussion in the \"Relative\n  rotational equivariance.\" paragraph later.\n\n\n- It would be ideal if the authors could assert theoretically some degree of\n  approximate invariance given approximate minimization of their regularizer\n  (see a comment to this effect below). It seems important to precisely\n  understand what happens in this setting, given the fact that a mixture of\n  equivariant and invariant losses are required for strong practical\n  performance.\n\n\n### Minor issues\n\n- After equation (4): better to not reference a figure in the appendix without\n  adding something like \"In the appendix, we show ...\"\n\n- The claim after Proposition 1 that \"[c]onsequently, low [CARE] loss converts\n  'unstructured' augmentations in input space to have a structured geometric\n  interpretation as rotations in the embedding space\" does not seem to follow\n  from Proposition 1 or the preceding discussion (since the proposition requires\n  exact minimization of the loss, rather than just a low value). It is not clear\n  to me that the proof generalizes, because it relies on an external result from\n  invariant theory (which might be algebraic).\n\n- Bottom of page 5: principle -> principal"
                },
                "questions": {
                    "value": "- Is there an \"approximate\" version of the results from invariant theory that\n  the authors use to establish their theoretical results? If one digs into the\n  proof of the result from invariant theory (say, specialized to\n  $\\mathrm{SO}(n)$), what obstructions are there to having an \"approximate\"\n  version of the result?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1028/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1028/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1028/Reviewer_Dtcq"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1028/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699460653342,
            "cdate": 1699460653342,
            "tmdate": 1700681968197,
            "mdate": 1700681968197,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lQdlf9ReG4",
                "forum": "lgaFMvZHSJ",
                "replyto": "bqfUkL0htu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1028/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1028/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Reviewer Dtcq"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their thought-provoking review and encouraging words of appreciation for our work. We have endeavored to address your concerns as concretely as possible, with the changes in the revised manuscript highlighted in magenta. \n\n---\n\n\n > The use of both an invariant and equivariant loss in the overall objective function seems conceptually strange\n\n- As highlighted in our original submission, the core idea behind this conceptualization is based on our hypothesis that small perceptual changes in data, resulting from data augmentations, should correspond to small perturbations in embeddings. However, solely minimizing the equivariance and uniformity loss $\\mathcal{L}{\\text{equiv}} + \\mathcal{L}{\\text{unif}}$ does not guarantee this. To address this, we introduce an additional constraint by enforcing localized transformations in the representation space. A natural choice in contrastive learning is to minimize the invariance loss $\\mathcal{L}_{\\text{inv}}$ to push the embeddings of pairs of data closeby. In essence, our model aims to learn approximate equivariance, where augmentations in input space translate to _small_ and _orthogonal_ transformations in the representation space.\n\n- Relative rotational equivariance $\\gamma_f$ is the ratio of an orthogonally equivariant measure to the invariance loss. Specifically, since invariance $f(a(x)) = f(x)$ is a trivial solution of our equivariant loss, through $\\gamma_f$, we measure the degree to which $f(a(x)) = R_af(x)$ where $R_a \\neq I_d$.  A lower value of $\\gamma_f$ indicates a higher degree of orthogonal equivariance, signifying non-trivial equivariance.\n\n\n---\n\n\n>  Theoretical guarantees of CARE given approximate (and not exact) minimization of the regularize\n\nWe thank the reviewer for raising this excellent question. Indeed, we can provide an approximate equivariance bound when the equivariant loss is non zero. The changes are highlighted in Section A.1 and B.1 of the Appendix (in magenta) in the revised manuscript and are briefly described below. \n\n- To generalize Corollary 3 in our submission, assume that the loss is bounded (not exactly zero), so $||AA^\\top - BB^\\top|| < \\epsilon^2$ in the 2-norm for some error $\\epsilon^2 > 0$. Then, using Theorem 1 in [1], we can demonstrate that $\\min_{R \\in O(d)} ||A - BR|| < o(\\epsilon)$, where $o(\\epsilon) \\to 0$ as $\\epsilon \\to 0$. As $\\epsilon$ (and consequently the loss) approaches zero, we recover our exact equivariance result. For $\\epsilon > 0$, we achieve an approximate equivariance up to $o(\\epsilon)$.\n\n- We further show that CARE enjoys consistency under composition of transformations even under low equivariant loss. Mathematically, $\\rho : \\mathcal A \\rightarrow O(d)$ given by $\\rho(a) = R_a$ satisfies\n$||\\rho(a' \\circ a) - \\rho(a')\\rho(a)|| \\leq o(\\epsilon^2)$ for almost all $a,a' \\in \\mathcal{A}$, where $o(\\epsilon) \\to 0$ as the error $\\epsilon \\to 0$.\n\n[1] Arias-Castro, Ery, Adel Javanmard, and Bruno Pelletier. \"Perturbation bounds for procrustes, classical scaling, and trilateration, with applications to manifold learning.\" Journal of Machine Learning Research 21 (2020).\n\n---\n\n\n> Minor issues and typographical errors \n\nWe thank you for your attention to detail. We have made the following corrections in the revised manuscript.\n\n- *Referencing of figures in appendix*: Thank you for pointing this out. We have fixed this in the revised manuscript. \n- *Regarding low CARE loss guarantees*: We concur with the reviewer and have made the correction in the revised manuscript. \n- *Typographical error*: We are changed \u2018principle\u2019 to \u2018principal\u2019 at the bottom of page 5"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1028/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700275010137,
                "cdate": 1700275010137,
                "tmdate": 1700275010137,
                "mdate": 1700275010137,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YAOf1LzMVh",
                "forum": "lgaFMvZHSJ",
                "replyto": "lQdlf9ReG4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1028/Reviewer_Dtcq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1028/Reviewer_Dtcq"
                ],
                "content": {
                    "title": {
                        "value": "thanks"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThanks for your thorough response to my review, and for engaging with it. I am happy to see that perturbation bounds were available to generalize the 'exact' minimization result that was present in the submission. I will increase my score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1028/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681952649,
                "cdate": 1700681952649,
                "tmdate": 1700681952649,
                "mdate": 1700681952649,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]