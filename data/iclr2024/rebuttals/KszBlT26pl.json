[
    {
        "title": "FP-IRL: Fokker-Planck-based Inverse Reinforcement Learning --- A Physics-Constrained Approach to Markov Decision Processes"
    },
    {
        "review": {
            "id": "Bme64rULYi",
            "forum": "KszBlT26pl",
            "replyto": "KszBlT26pl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4529/Reviewer_72Xj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4529/Reviewer_72Xj"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed to combine physics based dynamics modelling with inverse reinforcement learning. Particularly they consider the class of systems governed by Fokker-Planck equations and derive a IRL method that is computationally efficient. The empirical experiments demonstrate the strength of method on a cancer cell dynamics problem."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper presents a computationally efficient IRL algorithm by leveraging the following properties in FP-dynamics: a. the potential of the system is the negated value function b. The steady state visitation of optimal policy in FP dynamics has a closed form solution in an analytical form of potential function. c. IRL is easy now as given steady state distribution \u2192 we can figure out optimal value function and given optimal value function we can extract the reward function.\n2. The authors propose to use Hermite cubic functions to induce structure in the potential functions/Q-function that maps state-actions to scalar.\n3. Two experiments - 1 on a 2D toy domain and other on a dataset for cancer-cell dynamics show that FP-IRL can recover reward functions and optimal policies well."
                },
                "weaknesses": {
                    "value": "1. I have strong concerns against the motivations in the paper:\n    1. \u201cMost IRL methods require the transition function to be known\u201d: This is repeatedly claimed in the paper and is considered as the motivation behind this paper. This is not true as a number of works propose sample-based estimators to solve IRL [1,2,3,4,5,6,7]. These methods should be discussed and compared against.\n    2. \u201cEmpirical treatment of dynamics using NN\u2019s is not generalizable\u201d: While this statement is true, the way this problem is addressed is not by FP but by using Hermite cubic functions since FP also requires estimating the potential function which has the same learning complexity under a expressible function class. How do other prior methods compare if you use the Hermite cubic function as limited hypothesis class.\n    3. \u201cThis means that the agent employs to reach states whose value function is high\u201d: This is not a true statement as some high value states can be unreachable under a given starting state distribution. A better explanation for the conjecture might be needed. \n2. Novelty: I believe the core strength of paper is in combining FP-dynamics with IRL, merging existing theoretical components. But this combination is not well investigated empirically, experiments are performed on low-dimensional single domain.\n3. Empirical evaluation: It is surprising that no baselines were compared against. There are a number of existing IRL baselines[1,2,3,4,5,6,7] that need to be compared in order to make the claims of the paper stand. The function class of Hermite cubic functions should be ablated in these comparisons as well.\n\n[1]: Maximum Entropy Inverse Reinforcement Learning (https://cdn.aaai.org/AAAI/2008/AAAI08-227.pdf)\n\n[2]: Algorithms for inverse reinforcement learning (https://ai.stanford.edu/~ang/papers/icml00-irl.pdf)\n\n[3]: Learning Robust Rewards with Adversarial Inverse Reinforcement Learning (https://arxiv.org/abs/1710.11248)\n\n[4]: IQ-Learn: Inverse soft-Q Learning for Imitation (https://arxiv.org/abs/2106.12142)\n\n[5]: Dual RL: Unification and New Methods for Reinforcement and Imitation Learning (https://arxiv.org/abs/2302.08560)\n\n[6]:OPIRL: Sample Efficient Off-Policy Inverse Reinforcement Learning via Distribution Matching (https://arxiv.org/abs/2109.04307)\n\n [7]: f-IRL: Inverse Reinforcement Learning via State Marginal Matching (https://arxiv.org/abs/2011.04709)"
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4529/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698721096166,
            "cdate": 1698721096166,
            "tmdate": 1699636430271,
            "mdate": 1699636430271,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "piDRuGIgdg",
                "forum": "KszBlT26pl",
                "replyto": "Bme64rULYi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4529/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4529/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 72Xj (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your reviews.\n\n### Reply to Weakness 1.1:\n- To reiterate our motivation as discussed in the 2nd and 3rd paragraph in Section 1 and last paragraph in Section 3.1:\n    > This is motivated by real-world examples (e.g. cancer cell metastasis), where we can only collect the trajectory data but dynamic knowledge is missing or imperfect, and therefore accessibility of sampling from transition is not available.\nMany systems (e.g., swarms, crowd behavior) have mechanistic foundations in physics, which if exploited can lead to a better understanding and more efficient learning of their incentive structures (i.e., reward function) and transition. With the above motivation, we propose a new method of physics-constrained IRL. This method simultaneously estimates the transition rooted in physics and reward functions using only data on trajectories, while also inferring physical principles that govern the system and using them to constrain the learning.\n- Our method is different than other IRL methods as they would require either the accessibility to sampling from the transition or the analytical form of the transition. Our method does not require both. To reiterate our problem definition: Our target problem setting is that both reward and transition are unknown and need to be inferred in the MDP (i.e., $\\mathcal{M} / \\{ T(\\cdot), R(\\cdot) \\}$).\nSpecifically, the transition is unknown in any of these forms:\n    - The analytical form of the transition function $T(s' | s, a)$ is unknown.\n    - It is not accessible for sampling the next state for a given state and action (i.e., $s' \\sim T(s' | s, a)$ cannot be realized). Please note that conventional IRL approaches often assume that sampling is available even though they claim their method does not require the transition (i.e., they do not require the analytical form of the transition but require the ability to sample from the transition.). This may lead to confusion for the audience.\n- We did not conduct an extensive comparison experimentally to other IRL methods because most existing IRL methods require the transition function to be known or **the ability to sample the next state from the transition function**. This is not consistent with our problem setting as these methods do not provide a systematic way to infer the transition or build a model for sampling. It's not directly comparable as the method we propose is designed to simultaneously estimate the transition and rewards under physics constraints.\n- Discussion on given papers: (1) None of these papers assume the transition is not accessible for sampling, and provide a method to infer the transition function as we did. (2) Therefore, they all require at least the sampling accessibility from the transition function in the reward inference (e.g., for collecting and comparing the trajectory of learned policy, or constructing the replay buffer for policy optimization using learned reward). A detailed discussion of each paper is provided below: \n    - Ref. [1]: In lines 2 and 5 of Algorithm 1, the transition is required for the estimation of expected state frequencies of learned policy. \n    - Ref. [2]: In the objective function Eq. (7) on page 4, the analytical form of transition (i.e., transition matrix) is required.\n    - Ref. [3]: In line 4 of Algorithm 1, accessibility to sampling from the transition is required for collecting trajectories and policy optimization (this paper uses TRPO).\n    - Ref. [4]: In Section 5.1, it requires the sampling from the transition for constructing the replay buffer. \n    % Moreover, this method uses IRL for imitation learning. Although it claims it does not require explicit transition knowledge.  \n    - Ref. [5]: It requires sampling from the transition either for online or offline settings. \n    % imitation learning, replay buffer\n    - Ref. [6]: In line 1 of Algorithm 1, it requires sampling from the transition to construct the replay buffer.\n    - Ref. [7]: In step 1 in the for-loop of Algorithm 1, it requires sampling from the transition to construct the replay buffer and collecting trajectories of learned policy."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700039724051,
                "cdate": 1700039724051,
                "tmdate": 1700039724051,
                "mdate": 1700039724051,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6waprVNi0d",
                "forum": "KszBlT26pl",
                "replyto": "Bme64rULYi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4529/Reviewer_72Xj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4529/Reviewer_72Xj"
                ],
                "content": {
                    "title": {
                        "value": "Unclear about role of transitions"
                    },
                    "comment": {
                        "value": "**Our method is different than other IRL methods as they would require either the accessibility to sampling from the transition or the analytical form of the transition. Our method does not require both.**\n\nThere are two IRL settings: Offline and Online. Online assuming sampling from the transition distribution by interacting with the simulator or an analytical form.\nOffline does not require either of those. It only requires a fixed dataset of transitions or a fixed dataset of trajectories. As far as I understand that is the setting this paper is operating under the offline setting. I would encourage the authors to provide a more clean and straightforward comparison of their algorithm vs offline IRL algorithms if this is not the case. Algorithm 1 shows that FP-IRL has access to trajectories.\n\nThere are a number of IRL algorithms that operate under the offline setting:   \n[1]: Chen, Mao, et al. \"Batch-Constraint Inverse Reinforcement Learning.\" PRICAI 2021: Trends in Artificial Intelligence: 18th Pacific Rim International Conference on Artificial Intelligence, PRICAI 2021, Hanoi, Vietnam, November 8\u201312, 2021,  Proceedings, Part III 18. Springer International Publishing, 2021.   \n[2]: (ICLR 23) Luo, Yicheng, et al. \"Optimal Transport for Offline Imitation Learning.\" arXiv preprint arXiv:2303.13971 (2023).    \n\n       \nRef. [4] has an offline variant in their paper as well. They also show how rewards can be extracted.     \nRef. [5] above assumes a fixed dataset of transitions for inverse RL as well.\n\n\nFinally, I believe it to be important that the work is properly situated in related prior literature and the discussion of why it is different should be a part of the paper."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700321442130,
                "cdate": 1700321442130,
                "tmdate": 1700322073491,
                "mdate": 1700322073491,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2zDgpso4Zp",
                "forum": "KszBlT26pl",
                "replyto": "nSweGVp1oD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4529/Reviewer_72Xj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4529/Reviewer_72Xj"
                ],
                "content": {
                    "comment": {
                        "value": "**The Hermite polynomial basis is only for the convenience of representation and by itself does not grant generalization. This problem indeed is addressed by the FP constraint on the MDP dynamics.**\n\nMy concern is that this hypothesis is not supported by evidence. For instance, one might be able to show this claim via Theoretical justification or empirically via ablations. What would happen if you replaced Hermite polynomial basis with neural networks? What would happen if you change FP to neural network dynamics?\n\nWithout these comparisons, it's hard to conclude why this method is superior or drives an improvement."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700321988142,
                "cdate": 1700321988142,
                "tmdate": 1700321988142,
                "mdate": 1700321988142,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KLTmnqBhMH",
            "forum": "KszBlT26pl",
            "replyto": "KszBlT26pl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4529/Reviewer_hceT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4529/Reviewer_hceT"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces an Inverse Reinforcement Learning (IRL) method that estimates the unknown reward function of a Markov decision process (MDP) without knowing the predefined transition function. Assuming the MDP follows an Ito dynamics, the method infers transitions and reward functions simultaneously from observed trajectories, leveraging mean-field theory through the Fokker-Planck (FP) equation. The authors postulate an isomorphism between time-discrete FP and MDP, which plays the central role in the algorithm design."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The strengths of this paper seem unclear as it hinges on a potentially problematic conjecture."
                },
                "weaknesses": {
                    "value": "1. This paper hinges on a conjecture, which might be problematic. This conjecture says that the $\\psi$ is equivalent to the $Q$ function in RL problem. However, this $\\psi$ function is related to the dynamics. In particular, $\\nabla \\psi$ is the gradient field. It seems unclear to me why the conjecture is true because the $Q$ function depends on the choice of reward function as well. \n\nIt is possible that the authors actually assume that the equivalence is between the (entropy-regularized) optimal policy and the (entropy-regularized) optimal Q function. In this case, it is still unclear to me why the conjecture should be true. It would be great if the authors could at least prove it in special cases such as linear-quadratic case. \n\n2. For discrete-time MDPs, there are existing works that study max-entropy IRL without knowing the transition function. In fact, not knowing the reward seems not a barrier -- we can just estimate the transition from the trajectory data. Thus, the claim that \"While most IRL approaches require the transition function to be prescribed or learned a-priori, we present a new IRL method targeting the class MDPs that follow the It^{o} dynamics without this requirement. \" seems ungrounded. Moreover, it seems unclear to me why Ito dynamics should be used here. \n\n3. Related work. This work is also related to the huge literature on solving inverse problems involving PDEs, for example, neural ODE and PINN."
                },
                "questions": {
                    "value": "1. Can you prove the conjecture on some toy cases?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4529/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698737112684,
            "cdate": 1698737112684,
            "tmdate": 1699636430191,
            "mdate": 1699636430191,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gnl2bz3rP6",
                "forum": "KszBlT26pl",
                "replyto": "KLTmnqBhMH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4529/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4529/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reviews.\n### Reply to Weakness 1 and Question 1:\n- We agree that there remains theoretical work to be carried out to provide principled guarantees for Conjecture 3.1. There also remains considerable scope for empirical evidence. We hope to work in this direction in future work. For now, we note that this work is motivated by problems of cancer cell dynamics for which our collaborators have generated the real-world example that we used. Cell migration dynamics are widely understood to be governed in the continuous limit by different versions of FP equations. This is the class of problems that this work addresses, by basing itself on Conjecture 3.1. Furthermore, many other real-world problems including Brownian dynamics, swarming and crowd behavior, pattern formation, and morphogenesis are described by FP equations in the continuous limit.  Understanding their incentive structure (i.e., reward function) holds great potential for understanding these complex systems. \nWe are not claiming that the conjecture holds uniformly for MDPs. That is likely not the case.\n### Reply to Weakness 2:\n- Our method is motivated by real-world problems (e.g. cancer cell problems) whose dynamic is known to be governed by physics, specifically that described by the Ito dynamics. A detailed reason for using Ito dynamics can be seen in our motivation as discussed in the 2nd and 3rd paragraph in Section 1 and last paragraph in Section 3.1:\n    > This is motivated by real-world examples (e.g. cancer cell metastasis), where we can only collect the trajectory data but dynamic knowledge is missing or imperfect, and therefore accessibility of sampling from transition is not available.\nMany systems (e.g., swarms, crowd behavior) have mechanistic foundations in physics, which if exploited can lead to a better understanding and more efficient learning of their incentive structures (i.e., reward function) and transition. With the above motivation, we propose a new method of physics-constrained IRL. This method simultaneously estimates the transition rooted in physics and reward functions using only data on trajectories, while also inferring physical principles that govern the system and using them to constrain the learning.\n- Our method is different than other IRL methods as they would require either the accessibility to sampling from the transition or the analytical form of the transition. Our method does not require both. To reiterate our problem definition: Our target problem setting is that both reward and transition are unknown and need to be inferred in the MDP (i.e., $\\mathcal{M} / \\{ T(\\cdot), R(\\cdot) \\}$).\nSpecifically, the transition is unknown in any of these forms:\n    - The analytical form of the transition function $T(s' | s, a)$ is unknown.\n    - It is not accessible for sampling the next state for a given state and action (i.e., $s' \\sim T(s' | s, a)$ cannot be realized). Please note that conventional IRL approaches often assume that sampling is available even though they claim their method does not require the transition (i.e., they do not require the analytical form of the transition but require the ability to sample from the transition.). This may lead to confusion for the audience.\n- > not knowing the reward seems not a barrier\" \n    - If the reviewer means \"not knowing the 'reward' seems not a barrier\": Yes, the goal of the IRL problem is to infer the reward function from the trajectory data.\n    - If the reviewer means \"not knowing the 'transition' seems not a barrier\": there are two ways to infer the transition in the IRL problem:\n       - Inferring the transition before the reward inference. This is what this reviewer means. As we discussed in Section 5 and in 2nd paragraph in Section 2: \n            > \"The empirically estimated transition may not inherit the underlying dynamics, and is often challenging to generalize to state and action regions away from training samples relying on data alone.\"\n\n            > \"The lack of interpretability in deep learning models (inferred before the reward inference) can translate to difficulty in scientific understanding of the system behavior.'\"\n        - Inferring the transition simultaneously with reward inference. This is what we try to do. \n            > \"By constraining with FP dynamics, we systematically reduce this ambiguity to identify a unique pair of transition and reward functions that comply with the FP dynamics.\" \n            \n            This is more beneficial and preferable when the system has a mechanistic foundation and we can exploit it into the inference to reduce the ill-posedness and add physical interpretability.\n### Reply to Weakness 3:\n- Please note that VSI is a published method [Wang et al. (2019; 2021)] that provides a way to infer FP PDE. Our major contribution in this work is FP-IRL algorithm.\n- PINNS or neural ODE also offers a way to infer the PDE, but they are out of the scope of this IRL work and not included in the related work."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700039618826,
                "cdate": 1700039618826,
                "tmdate": 1700039618826,
                "mdate": 1700039618826,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "a6rn42g8Xt",
                "forum": "KszBlT26pl",
                "replyto": "gnl2bz3rP6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4529/Reviewer_hceT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4529/Reviewer_hceT"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for addressing my concerns.\n\nI am still not convinced by the conjecture that $Q = - \\psi$ while $\\nabla \\psi $ is the vector field that drives the dynamics. This assumption is ungrounded because it directly links two seemingly terms that are not directly linked together. In particular, the transition, **together with the reward**, determines the $Q$ function. If the reward is not relevant to $-\\psi$, this assumption would definitely fail. It would be great if the authors could verify the conjecture, or find a sufficient condition for it and empirically verify. Because this assumption is fundamental to the proposed method, the method seems also flawed."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657397002,
                "cdate": 1700657397002,
                "tmdate": 1700657397002,
                "mdate": 1700657397002,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "F4gwNXCI1H",
            "forum": "KszBlT26pl",
            "replyto": "KszBlT26pl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4529/Reviewer_fsZH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4529/Reviewer_fsZH"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript presents a novel method for Inverse Reinforcement Learning (IRL) using a physics-based prior. In essence, the proposed method makes the assumption that underlying dynamics when following the expert policy (induced by the demonstrations) follow Fokker-Planck (FP) equation. With this the authors are able to learn jointly the dynamics and the policy in an elegant way, and perform IRL in systems where the dynamics are not known."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- Very cool idea. I really liked it. It is elegant, but although complicated math are involved, I can say that the method itself is simple.\n- Well-written paper; although the paper deals with non-trivial and non-popular (in ML) quantities, the authors have made a quite good job in effectively conveying their message.\n- Long and interesting discussion section\n- Detailed limitations of the method mentioned"
                },
                "weaknesses": {
                    "value": "- The evaluation/experiments is quite \"weak\". There are no baselines and no comparison to state of the art. It is important to know where the new method stands in terms of performance against other state of the art methods with no physics priors.\n- No timings are provided for the experiments. Although I tend to agree with the statement \"FP-IRL avoids such iterations altogether and instead induces a regression problem leveraging the FP physics that is also computationally more stable\", wall-time performance can have very different outcomes (e.g. training the model in the proposed method takes too long).\n- More details about the VSI method (in the main text) could help an ML-oriented reader"
                },
                "questions": {
                    "value": "- Can you provide some baselines of state of the art methods with no physics priors?\n- Can you provide timings of the proposed method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4529/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698847918896,
            "cdate": 1698847918896,
            "tmdate": 1699636430106,
            "mdate": 1699636430106,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "goz4JUDvov",
                "forum": "KszBlT26pl",
                "replyto": "F4gwNXCI1H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4529/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4529/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reviews.\n\n### Reply to Weakness 1 and Question 1:\n\n- Our target problem setting is that both reward and transition are unknown and need to be inferred in the MDP (i.e., $\\mathcal{M} / \\{ T(\\cdot), R(\\cdot) \\}$).\nSpecifically, the transition is unknown in any of these forms:\n    - The analytical form of the transition function $T(s' | s, a)$ is unknown.\n    - It is not accessible for sampling the next state for a given state and action (i.e., $s' \\sim T(s' | s, a)$ cannot be realized). Please note that conventional IRL approaches often assume that sampling is available even though they claim their method does not require the transition (i.e., they do not require the analytical form of the transition but require the ability to sample from the transition.). This may lead to confusion for the audience.\n- We did not conduct an extensive comparison experimentally to other IRL methods because most existing IRL methods require the transition function to be known or **the ability to sample the next state from the transition function**. This is not consistent with our problem setting as these methods do not provide a systematic way to infer the transition or build a model for sampling. It's not directly comparable as the method we propose is designed to simultaneously estimate the transition and rewards under physics constraints.\n\n### Reply to Weakness 2 and Question 2:\n- We regret that we did not measure the computation time of our method when we conducted the initial experiment on the cluster, but we reproduced the experiment using lower discretization resolution on the laptop with the Apple M1 Max chip and 32 GB memory: \n\nFor the synthetic toy example in Section 4.1 (Please note that original discretization resolution ranges from 5 to 17 in the convergence analysis):\n|     Number of Discretization |            8 |             9 |           10 |           11 |\n|------------------------------|-------------:|--------------:|-------------:|-------------:|\n|     FEA Time (sec)           |        77.05 |         128.6 |       253.56 |       344.04 |\n|     Regression Time (sec)    |         0.77 |          1.12 |         1.64 |         2.46 |\n|     Total VSI Time (sec)         |        77.82 |        129.72 |        255.20 |        346.50 |\n\nFor the modified Mountain Car problem in Appendix F.2 (Please note that original discretization resolution ranges from 10 to 30 in the convergence analysis):\n| Number of Discretization |   20  |   25  |   30   |\n|:------------------------:|:-----:|:-----:|:------:|\n| FEA Time (sec)           | 45.95 | 88.54 | 156.29 |\n| Regression Time (sec)    | 0.87  | 1.71  | 3.17   |\n| Total VSI Time (sec)         | 46.82 | 90.25 | 159.46 |\n\n### Reply to Weakness 3:\n- VSI details are provided in Appendix E and cited papers [Wang et al. (2019; 2021)] as indicated in the second sentence of Section 3.6. \n- Please note that VSI is a published method that provides a way to infer FP PDE. Our major contribution in this work, as discussed in Section 1, is the FP-IRL algorithm."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700039404970,
                "cdate": 1700039404970,
                "tmdate": 1700039404970,
                "mdate": 1700039404970,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "84cazH4gQN",
                "forum": "KszBlT26pl",
                "replyto": "goz4JUDvov",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4529/Reviewer_fsZH"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4529/Reviewer_fsZH"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the comments/replies.\n\n> We did not conduct an extensive comparison experimentally to other IRL methods because most existing IRL methods require the transition function to be known or the ability to sample the next state from the transition function\n\nThis should be stated better or more explicitly and in different places in the manuscript. So that it is super clear to the reader.\n\n> Reply to Weakness 3:\n\nI agree with the comments, but the ML audience is not very aware of it. Since VSI is needed to understand your method, it'd be easier for the audience to include this in the main text if possible.\n\nI have no other comments."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644888607,
                "cdate": 1700644888607,
                "tmdate": 1700644888607,
                "mdate": 1700644888607,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wU8UNOheEy",
            "forum": "KszBlT26pl",
            "replyto": "KszBlT26pl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4529/Reviewer_GdP4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4529/Reviewer_GdP4"
            ],
            "content": {
                "summary": {
                    "value": "The authors draw an interesting parallel between the Markov decision process (MDP) model commonly used in RL and the Fokker-Planck partial differential equation (FP-PDE) used in modeling physical & biological systems. More specifically, the authors hypothesize equivalence between the $Q$ function in the MDP model and the (negative) potential function $\\psi$ in the FP-PDE. This equivalence implies that methods that can be used to learn the potential are valid as IRL methods, using the inverse Bellman equation (IQ-Learn [1]). In this work, the potential is learned using variational system identification (VSI). VSI uses a parameterization scheme where the potential is a product of learnable parameters and features expressed using Hermite cubic functions.The learning occurs through minimization of the magnitude of the FP-PDE functional (since we want the FP-PDE function = 0 eventually). Overall, the established connection is interesting, but the use of VSI is limiting, which is reflected in the experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "It is known from [2] that the solution of FP-PDE has a Gibbs-Boltzmann density, and in many RL/IRL settings, a Boltzmann policy is assumed, therefore the connection between $\\psi$ and $Q$ is easy to grasp. This reveals an intuitive connection to physical phenomena, which is interesting."
                },
                "weaknesses": {
                    "value": "1. Not broadly applicable to more challenging problems, since discretization is required. As the state-action space increases, the approach would become infeasible. \n2. No comparison with other IRL methods was provided. For example, is there an advantage to using VSI for the cancer cell problem, over other simple IRL methods?"
                },
                "questions": {
                    "value": "1. Page 5: \"It imposes a physics constraint that the change in distribution should be small and approach zero over an infinitesimal time step\". In the limit $\\Delta t \\rightarrow 0$, the free energy term should become negligible in Equation 9, right? Why can we then ignore the squared Wasserstein distance term instead of the free energy term? \n2. Is VSI parameterization as good as a neural network parameterization, in terms of being able to learn the true function arbitrarily closely?\n3. Does Equation 21 have a unique minimizer? If not, how does VSI address the unidentifiability/ill-posedness issue in IRL, i.e. there are many possible rewards (and corresponding value functions) that yield the same policy?\n4. (Suggestion) Certain concepts could be introduced more better, through simple examples either in the main text or in the appendix. For example, intution behind FP-PDE - Equation 7, why Wiener processes are used, etc. \n\n**References**\n\n1.  IQ-learn: Inverse soft-Q learning for imitation, Garg et al. (2021)\n2.  Free energy and the fokker-planck equation, Jordan et al. (1997)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "_"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4529/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4529/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4529/Reviewer_GdP4"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4529/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698910462729,
            "cdate": 1698910462729,
            "tmdate": 1699636430000,
            "mdate": 1699636430000,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sigzAC43PZ",
                "forum": "KszBlT26pl",
                "replyto": "wU8UNOheEy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4529/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4529/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer GdP4 (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your reviews.\n\n### Reply to Weakness 1:\n- VSI can fundamentally handle high-dimensional state-action spaces. However, having its root in finite element methods, this method does suffer from the curse of dimensionality. More precisely, considering a $d$-dimensional state-action space, with a uniform grid of $n$-bins, we get a discrete state-action space with $n^d$ points. The number of operations in the numerical setup of VSI scales linearly with these points i.e. $\\sim \\mathcal{O}(n^d)$. Therefore, for a $k$-term model for the energy, we require $\\sim \\mathcal{O}(n^d k)$ operations that result in a linear regression problem with $k$-terms. These operations are vectorized and parallelizable whenever necessary.\n- Please note that VSI is a published method [Wang et al. (2019; 2021)] that provides a way to infer FP PDE. Our major contribution in this work, as discussed in Section 1, is the FP-IRL algorithm, which inherently does not have the requirement on dimensionality. Future work includes modularizing the FP PDE inference: using other inference methods for higher dimension problems.\n- As discussed in the **\"Applicability\"** part in Section 5, there are broad applications in scientific problems: \n    > \"With physics-constrained modeling, this allows the application of FP-IRL to problems where the transition is not available for sampling and has not been mathematically modeled, or discovered. Cancer cell migration, as well as the migration of other cell types, is known to be governed by physics, specifically that described by the FP equation (Bressloff, 2014). Therefore, there is interest in the fields of biology, biophysics, and physics more broadly, to have scientific machine learning methods that respect these physics. We achieve this by combining machine learning ideas (IRL) with physics principles (Minimum Energy Principle and FP dynamics). Although the proposed method may not apply directly to some RL problem domains, such as robotics, many other physics phenomena encompassing Brownian dynamics (Keilson and Storer, 1952), swarming (Correll and Hamann, 2015) and crowd behavior (Dogbe, 2010), pattern formation and morphogenesis (Garikipati, 2017) are also described by FP equations in the continuous limit, and this work would also be applicable to them.\"\n### Reply to Weakness 2:\n- Our target problem setting is that both reward and transition are unknown and need to be inferred in the MDP (i.e., $\\mathcal{M} / \\{ T(\\cdot), R(\\cdot) \\}$).\nSpecifically, the transition is unknown in any of these forms:\n    - The analytical form of the transition function $T(s' | s, a)$ is unknown.\n    - It is not accessible for sampling the next state for a given state and action (i.e., $s' \\sim T(s' | s, a)$ cannot be realized). Please note that conventional IRL approaches often assume that sampling is available even though they claim their method does not require the transition (i.e., they do not require the analytical form of the transition but require the ability to sample from the transition.). This may lead to confusion for the audience.\n- We did not conduct an extensive comparison experimentally to other IRL methods because most existing IRL methods require the transition function to be known or **the ability to sample the next state from the transition function**. This is not consistent with our problem setting as these methods do not provide a systematic way to infer the transition or build a model for sampling. It's not directly comparable as the method we propose is designed to simultaneously estimate the transition and rewards under physics constraints.\n- Our motivation also demonstrates the advantage of FP-IRL in the cancer cell problem as discussed in the 2nd and 3rd paragraph in Section 1 and last paragraph in Section 3.1:\n    > This is motivated by real-world examples (e.g. cancer cell metastasis), where we can only collect the trajectory data but dynamic knowledge is missing or imperfect, and therefore accessibility of sampling from transition is not available.\nMany systems (e.g., swarms, crowd behavior) have mechanistic foundations in physics, which if exploited can lead to a better understanding and more efficient learning of their incentive structures (i.e., reward function) and transition. With the above motivation, we propose a new method of physics-constrained IRL. This method simultaneously estimates the transition rooted in physics and reward functions using only data on trajectories, while also inferring physical principles that govern the system and using them to constrain the learning.\n- A general discussion of the advantages of FP-IRL over other methods is provided in the \"Significance\" part of Section 5.\n- Again, please note that VSI is a published method that provides a way to infer FP PDE. Our major contribution is the FP-IRL algorithm."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700039133316,
                "cdate": 1700039133316,
                "tmdate": 1700039133316,
                "mdate": 1700039133316,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "doiCGwGczZ",
                "forum": "KszBlT26pl",
                "replyto": "SdPb1nYRMB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4529/Reviewer_GdP4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4529/Reviewer_GdP4"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal response"
                    },
                    "comment": {
                        "value": "> VSI can fundamentally handle high-dimensional state-action spaces. \n\nFrom the description, it seems like VSI requires discretization. While VSI is only a part of FP-IRL algorithm, it's a major part of the algorithm since it is required to estimate $\\psi$. More experiments on challenging environments with high-dimensional state-action spaces would be needed to establish FP-IRL as a viable alternative to regular IRL methods. These environments do not have to be popular in the RL literature, but they should be more challenging w.r.t. state-action spaces.\n\n> Instead, IRL ill-posedness is reduced by the physics constraints (i.e., FP dynamic) from our conjecture as we proposed in the paper.\n\nI think this needs more formal justification.\n\nThanks for the clarification regarding the equation with $\\Delta t \\rightarrow 0$, and for other improvements in terms of clarity. I am inclined to keep my score at this point."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4529/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685500210,
                "cdate": 1700685500210,
                "tmdate": 1700685500210,
                "mdate": 1700685500210,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]