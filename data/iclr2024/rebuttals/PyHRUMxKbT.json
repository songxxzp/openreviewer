[
    {
        "title": "InfoNet: An Efficient Feed-Forward Neural Estimator for Mutual Information"
    },
    {
        "review": {
            "id": "Uw1t7Oo4rz",
            "forum": "PyHRUMxKbT",
            "replyto": "PyHRUMxKbT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4727/Reviewer_zWhA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4727/Reviewer_zWhA"
            ],
            "content": {
                "summary": {
                    "value": "This works present a new method for real-time mutual information (MI) estimate. The core of the method is a network that maps a population of empirical samples to the optimal function r(x,y) = p(x, y)/p(x)p(y). This optimal function is represented by a look-up table where the values of x, y are quantized into several bins (e.g. 100). To train this network, the authors use an idea similar to simulation-based inference, that they first extensively simulate data from various artificial distributions, then learn the relationship between the data population and the function r(x, y) behind. It is believed that after seeing sufficiently many artificial distributions, the network can cover the case in real world. Compared to traditional methods which learns r(x, y) from scratch, this method pre-compute r(x, y) via simulation, thereby is much faster at inference time. In addition to MI estimate, the idea can also be extended to compute Renyi's maximal correlation coefficient."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- **Significance**: the problem of scalable estimation of mutual information and statistical dependence is very fundamental in modern data science and machine learning, and by far there is still no method that can be both accurate (like neural network-based method) and efficient (like non-parameteric method). The method does a good job in trying to take the advantage of both worlds;\n- **Novelty**: the idea of this work, to my knowledge, is very novel within the context of MI and statistical dependence estimate. While many recent works have also focused on scalable computation of mutual information and statistical dependence [1, 2, 3, 4], this work presents an orthogonal possibility. The idea of learning to map the data population to the optimal neural estimator via massive simulation is so interesting, and the way to implement this idea via attention-based network is also technically sensible. In fact, the idea of simulation-based learning is not new, particularly in the field of statistics (see e.g. [5, 6]). However, the paper is the first one to apply this idea to statistical dependence modelling; \n- **Presentation**: the paper is also mostly well-written and easy to follow. I enjoy reading the paper very much.\n\nReferences:\n\n[1] The Randomized Dependence Coefficient, NeurIPS 2017\n\n[2] Sliced mutual information: A scalable measure of statistical dependence, NeurIPS 2021\n\n[3] Scalable Infomin Learning, NeurIPS 2022\n\n[4] Fairness-Aware Learning for Continuous Attributes and Treatments, ICML 2019\n\n[5] Simulation intelligence: Towards a new generation of scientific methods, arxiv 2112.03235\n\n[6] The frontier of simulation-based inference, PNAS 2020"
                },
                "weaknesses": {
                    "value": "- **Limitations** (major): while very interesting, the proposed method seems only work for low-dimensional data. This is due to (a) the limitation of simulation-based learning itself, where as the dimensionality grows it becomes more and more difficult to cover diverse cases of distributions; (b) the number of grids in the lookup table grows exponentially with the dimensionality of data, making the discrete representation proposed impractical. That being said, the idea can expect to work well in e.g. 1D/2D cases;\n- **Experiments** (major):  the experiments in this paper are done on synthetic dataset and one (relatively simple) real-world dataset, which is a bit disappointing compared to the interesting idea. In this regard, I think the current experiments do not really show the potential of the method developed. The synthetic task regarding maximal correlation is also not so intutitive/easy-to-understand to me compared to those done in existing works [1, 3, 4]. Why not test the power of the method in the presence of different statistical association patterns (see [1, 3, 4])?;\n- **Discussion on relationship to related works**: as mentioned in the strength block above, the paper presents a new method for estimating statistical dependence in a highly efficient way. It will be ideal to have a discussion between your method and these existing works, highlighting why your method is more preferable compared to these prior works. In addition, it *may* also be nice to mention the connection of your work and the field of simulation-based intelligence due to the inherent similarity between the underlying principles. This will connect your work with the broader community;\n- **Presentation** (minor): while the presentation is overall good, it was only after reading Section 3.1 and Algorithm 1 I can understand your method. In fact, Section 3.1 seems to be important description of the core training algorithm of your method. I think it would be easier to understand for readers if the authors make this part a separate section (e.g. training algorithm) rather than part of the experiments. There are also some typos/errors in the text (for example `MIN-100\u2019 in Table 1)."
                },
                "questions": {
                    "value": "- Is it really necessary to parameterize the network to be a lookup table ? Why can\u2019t we e.g. directly output the MI/Maximal correlation value for the input data population?\n- In the experiment, why study cases with known ground-truth maximal correlation? For example, you can generate data y = f(x) + eps with known f and calculate the maximal correlation with f."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4727/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4727/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4727/Reviewer_zWhA"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4727/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697815210250,
            "cdate": 1697815210250,
            "tmdate": 1699636454733,
            "mdate": 1699636454733,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NYJ4HUFylI",
                "forum": "PyHRUMxKbT",
                "replyto": "Uw1t7Oo4rz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4727/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4727/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer zWhA,\n\nThanks for your comprehensive review and the depth of your engagement with our manuscript, and the references you provide give us a lot of inspiration. Your acknowledgment of our method's significance and novelty within the field of mutual information is immensely encouraging. We are also grateful for your constructive critique, which has prompted an expansion of the method's scalability and experimental validation.\n\nIn response to your comments and suggestions, we have expanded our experimental scope and provided clarifications on the implementation details in our revised manuscript, which we believe can help reflect the potential and applicability of our proposed approach in a more accurate manner.\n\nWe hope that these clarifications can help finalize your assessment and the rating of our paper. Please also let us know if you have any further questions that we need to provide additional clarifications.\n\n***\n\n**W1: ``Limitations (major): while very interesting, the proposed method seems only work for low-dimensional data. This is due to (a) the limitation of simulation-based learning itself, where as the dimensionality grows it becomes more and more difficult to cover diverse cases of distributions; (b) the number of grids in the lookup table grows exponentially with the dimensionality of data, making the discrete representation proposed impractical. That being said, the idea can expect to work well in e.g. 1D/2D cases;''**\n\n\n\n**A**: Thank you for your insight into the difficulty of simulation/sampling in high-dimensional space, which may increase the size of the look-up table exponentially.\n\nIn the following, we would like to provide an alternative for high-dimensional MI estimation using the pre-trained InfoNet with the sliced technique provided in [1], which projects a high-dimensional signal to a low dimension and allows efficient MI estimation.\n\nWe have rigorously evaluated our InfoNet model on high-dimensional random variables compared with MINE, in terms of correlation order accuracy and independence test. \n\nSpecifically, we validated the pretrained InfoNet in two tasks:\n\nTask 1: given three jointly sampled random variables, X, Y, and Z from high-dimensional Gaussian, we want to know which one of Y and Z is more correlated with X, where the correct order, i.e., MI(X,Y)>=MI(X,Z) or MI(X,Y)<MI(X,Z)  is obtained by the ground-truth MI computed with the analytical solution of MI for Gaussian. The performance metric is the binary classification accuracy.\n\nTask 2: Independence test. Given a pair of random variables X, and Y, either independent or not, we apply InfoNet and MINE to check their independence since both metrics can claim independence between two random variables if the metrics are close to zero. We vary the epsilon as the threshold, and we plot the precision-recall curve to compare their performance using the area under the curve (AUC, the closer to 1, the better).\n\nResults:\n\nA. In terms of the correlation order test (Task 1), we get the following performance:\n\n| Dimensions | 2    | 3    | 4    | 5    | 6    | 7    | 8    | 9    | 10   |\n|------------|------|------|------|------|------|------|------|------|------|\n| MINE-5000  | 96.2 | 97   | 97   | 96.2 | 94.9 | 94.2 | 93.2 | 92.8 | 93   |\n| InfoNet | **97.7** | **96.4** | **96.2** | **97.9** | **97.4** | **98.1** | **98.2** | **97.3** | **98.3** |\n\nFrom the table, we can see that InfoNet performs better in terms of estimating the correlation orders between a triplet of random variables across multiple dimensions.\nMoreover, InfoNet runs at 0.05s per estimate, while MINE runs at 12.3s per estimate.\n\nB. Results of AUC on independence testing (Task 2)\n| Sequence length | 50    | 100    | 150    | 200    | 250    | 300    | 350    | 400 \n|------------|------|------|------|------|------|------|------|------|\n| MINE-1000 d=16 | 0.757 | 0.668 | 0.843 | 0.902 | 0.996 | 1. | 1 | 1 |\n| MINE-1000 d=128 | 0.45 | 0.633 | 0.520 | 0.607 | 0.886 | 0.949 | 0.848 | 0.954 |\n| InfoNet d=16  | 0.914 | 0.992   | 1.0   | 1.0 | 1.0 | 1.0 | 0.99 | 1.0 | \n| InfoNet d=128  | 0.694 | 0.603   | 0.819   | 0.832 | 0.954 | 0.950 | 0.981 | 0.992 |\n\nFrom the table, we can see that in both 16- and 128-dimensional space, InfoNet with the sliced technique can achieve better AUC for independence testing, which shows the potential of the proposed in high-dimensional applications even only trained on low-dimensional signals.\nDetails of this experiment can be seen in B.4.1 of the revised version.\n\n[1] Sliced mutual information: A scalable measure of statistical dependence, NeurIPS 2021"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4727/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700589279501,
                "cdate": 1700589279501,
                "tmdate": 1700589279501,
                "mdate": 1700589279501,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "d2om90SNlV",
                "forum": "PyHRUMxKbT",
                "replyto": "Uw1t7Oo4rz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4727/Reviewer_zWhA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4727/Reviewer_zWhA"
                ],
                "content": {
                    "title": {
                        "value": "Reply to authors"
                    },
                    "comment": {
                        "value": "I thank the authors for their detailed and effective rebuttal. I am largely satisfied with your response. I'll keep my already positive score at this stage.\n\nBelow are some further comments:\n\n- **On scaling to higher dimensions with slicing**. I am glad to see that the authors find the literature [2] helpful. Your new idea of utilizing this method to scale to high-dimensional case is indeed sensible and interesting, and the new experiments on higher-dimensional case and real-world data (e.g. CLIP) are helpful. However, as far as I am concerned, this new method might have two problems. First, it no more estimates the original mutual information, meaning that it may not be so useful in cases where the exact value of mutual information is of interests (for example, formally investigation of the trade-offs in data compression/privacy leakage). Second, the required number of slices may grow exponentially as the dimensionality increases [3]. That being said, I think your new method a very sensible extension, and all these above are only remarks rather than critiques.\n\n- **Discussion on flexible/scalable alternatives for measuring statistical dependence**. As mentioned in the review, there also exist other parametric techniques that allow scalable/fast computation of statistical dependence. For example, the statistical dependence metrics in [1, 3, 4] can all be computed analytically (similar to solving linear regression/eigen-decomposition), which are also fast to execute compared to neural network training but are still much more powerful than non-parametric/kernel-based methods. Importantly, these methods are also fully differentiable like your method, so they can be applied to representation learning (see e.g. [3]). When I read your paper, I had a sense that why not using these already proven approaches, which are tested on high-dimensional cases (see e.g. [3]) and cases with diverse association patterns (see e.g. [1, 4]). I think it would be highly helpful to include a short discussion between your method and these methods, highlighting that your idea (pre-training) is orthogonal to these methods, and is conceptually more efficient and accurate. I believe this will help readers better understand the merit of your method.\n\nOther comments: Algorithm 1 and Algorithm 2 seem duplicated?\n\n[1] The Randomized Dependence Coefficient, NeurIPS 2017\n\n[2] Sliced mutual information: A scalable measure of statistical dependence, NeurIPS 2021\n\n[3] Scalable Infomin Learning, NeurIPS 2022\n\n[4] Fairness-Aware Learning for Continuous Attributes and Treatments, ICML 2019"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4727/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653395111,
                "cdate": 1700653395111,
                "tmdate": 1700664629781,
                "mdate": 1700664629781,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hERNQBdyW7",
                "forum": "PyHRUMxKbT",
                "replyto": "Uw1t7Oo4rz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4727/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4727/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer zWhA,\n\n\nThanks a lot for your positive response. We are grateful for your insightful comments and appreciation of our new experiments. We will incorporate them into our final version and handle the discussion among the mentioned references carefully and with clarity. Also, thanks for pointing out the duplication of the algorithms. We intentionally keep them here so the change is trackable, but will definitely merge them in the final version.\n\nAlso, in case it may cause curiosity on the number of projections for the sliced technique we have used, we provide them for each dimension below:\n\n\n| Dim  |     8   |   16  |   128 |   512 |\n|---------|-----|-----|-----|-----|\n|#proj.  | 100 | 100 |  500 |   500 |\n\n\nThese numbers (of projections) are typical and can guarantee reasonable performance in our experiments. We also have an ablation on the performance improvement over different numbers of projections in Table 8. Moreover, due to the fact that our network can handle different projections in batch mode, the prediction efficiency is still maintained.\n\nThanks again for your valuable feedback\uff01"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4727/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666152995,
                "cdate": 1700666152995,
                "tmdate": 1700667362003,
                "mdate": 1700667362003,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xSYetdu8fw",
                "forum": "PyHRUMxKbT",
                "replyto": "Uw1t7Oo4rz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4727/Reviewer_zWhA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4727/Reviewer_zWhA"
                ],
                "content": {
                    "title": {
                        "value": "Reply to authors"
                    },
                    "comment": {
                        "value": "Great, please do include the new results regarding high-dimensional data/real-world data and the new discussion regarding other scalable/efficient alternatives in the final version. Also thanks for the new results regarding #proj, which are instructive. \n\nI will determine my final score after discussing with other reviewers and checking the final manuscript more carefully. As a general opinion, I think your work a quite fine contribution, and I will recommend acceptance."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4727/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673058343,
                "cdate": 1700673058343,
                "tmdate": 1700673133823,
                "mdate": 1700673133823,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Y6ClCXbOy4",
            "forum": "PyHRUMxKbT",
            "replyto": "PyHRUMxKbT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4727/Reviewer_9kNo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4727/Reviewer_9kNo"
            ],
            "content": {
                "summary": {
                    "value": "The paper explores mutual information estimation utilizing a feedforward neural network. Unlike most methods based on the Donsker-Varadhan formula, such as MINE, there is no need to train a critic function. The author also introduces a feedforward network designed to estimate maximal correlation. These methods forego the need for per-distribution optimization, enhancing their efficiency."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well-written with a clear motivation. \nThe idea of replacing the optimization procedure with a feedforward neural network is intriguing."
                },
                "weaknesses": {
                    "value": "1. The author fails to specify the dimension of the data $(x, y)$. If these are scalars, the task of mutual information estimation becomes considerably simpler. There exists an extensive body of literature on classical (non-neural network) methods addressing this, which warrants consideration.\n\n2. Mutual information estimation methods based on the Donsker-Varadhan formula, such as MINE, are primarily designed for estimating mutual information between complex real-world data (e.g., CLIP). Comparing these to simple scalar setups seems inequitable. Should the proposed method be effective with high-dimensional data, the author is advised to validate it through experimentation.\n\n3. Relying solely on Gaussian Mixture Models for training appears to be a limited approach, raising questions about its generalizability to other distributions. To validate training based solely on Gaussian Mixtures, the author should experiment with a variety of distributions. For further reference:\nCzyz et al., \"Beyond Normal: On the Evaluation of Mutual Information Estimators.\"\n\n4. The core concept is to approximate the critic function $\\theta$ which maximizes the Donsker-Varadhan formula using a neural network. The function that truly maximizes this is given by $\\log p(x,y)/p(x)p(y)$. It seems implausible that a single pretrained neural network could accomplish this for every joint distribution $p(x, y)$."
                },
                "questions": {
                    "value": "Please see Weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4727/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4727/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4727/Reviewer_9kNo"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4727/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698668076277,
            "cdate": 1698668076277,
            "tmdate": 1699636454620,
            "mdate": 1699636454620,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HWOX9SDxAm",
                "forum": "PyHRUMxKbT",
                "replyto": "Y6ClCXbOy4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4727/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4727/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 9kNo,\n\nThank you for your constructive comments and insightful feedback on our manuscript. We highly appreciate your recognition of the paper's clarity, and that the idea of using a feedforward neural network for mutual information estimation is intriguing.\nWe acknowledge your concerns regarding the dimensionality of the data, the comparison with classical methods, and the generalizability of our approach. We have addressed each of your points in our revised manuscript and believe that our additional experiments and clarifications will shed more light on the efficacy and applicability of our proposed method.\nWe hope that these clarifications can help finalize your assessment and the rating of our paper. Please also let us know if you have any further questions that we need to provide additional clarifications.\n\n***\n\n**W1: The author fails to specify the dimension of the data $(x,y)$. If these are scalars, the task of mutual information estimation becomes considerably simpler. There exists an extensive body of literature on classical (non-neural network) methods addressing this, which warrants consideration.**\n\n**A**: Thank you for highlighting the point of specifying the data dimensionality in mutual information estimation. \n\nOur work primarily focuses on scalar variables in classical mutual information (MI) estimation. However, it can be directly applied to estimating MI between high-dimensional random variables with the technique of sliced mutual information [1]. We have rigorously evaluated our InfoNet model on high-dimensional random variables, in terms of correlation order accuracy and independence test. \n\nSpecifically, we validated the proposed InfoNet in two tasks:\n\nTask 1: given three jointly sampled random variables, X, Y, and Z from high-dimensional Gaussian, we want to know which one of Y and Z is more correlated with X, where the correct order, i.e., MI(X,Y)>=MI(X,Z) or MI(X,Y)<MI(X,Z)  is obtained by the ground-truth mutual information (MI) computed with the analytical solution of MI for Gaussian. The performance metric is the binary classification accuracy.\n\nTask 2: Independence test. Given a pair of random variables X, and Y, either independent or not, we apply InfoNet and MINE to check their independence since both metrics can claim independence between two random variables if the metrics are close to zero. We vary the epsilon as the threshold, and we plot the precision-recall curve to compare their performance using the area under the curve (AUC, the closer to 1, the better).\n\n\nResults:\n\nA. In terms of the correlation order test (Task 1), we get the following performance:\n\n| Dimensions | 2    | 3    | 4    | 5    | 6    | 7    | 8    | 9    | 10   |\n|------------|------|------|------|------|------|------|------|------|------|\n| MINE-5000  | 96.2 | 97   | 97   | 96.2 | 94.9 | 94.2 | 93.2 | 92.8 | 93   |\n| InfoNet | **97.7** | **96.4** | **96.2** | **97.9** | **97.4** | **98.1** | **98.2** | **97.3** | **98.3** |\n\nFrom the table, we can see that InfoNet performs better in terms of estimating the correlation orders between a triplet of random variables across multiple dimensions.\nMoreover, InfoNet runs at 0.05s per estimate, while MINE runs at 12.3s per estimate.\n\nDetails of this experiment can be seen in B.3.1 of the revised version.\n\n\nB. In terms of the independence test, we have the AUCs of MINE-x (where x means the test-time gradient steps) and InfoNet obtained under different dimensions and sequence lengths (number for data points in each sequence):\n\n| Sequence length | 50    | 100    | 150    | 200    | 250    | 300    | 350    | 400 \n|------------|------|------|------|------|------|------|------|------|\n| MINE-1000 d=16 | 0.757 | 0.668 | 0.843 | 0.902 | 0.996 | 1.0 | 1.0 | 1.0 | 1.0 |\n| MINE-1000 d=128 | 0.45 | 0.633 | 0.520 | 0.607 | 0.886 | 0.949 | 0.848 | 0.954 | 0.877 |\n| InfoNet d=16  | 0.914 | 0.992   | 1.0   | 1.0 | 1.0 | 1.0 | 0.99 | 1.0 | \n| InfoNet d=128  | 0.694 | 0.603   | 0.819   | 0.832 | 0.954 | 0.950 | 0.981 | 0.992 |\n\nFrom the Table, we can observe that the AUCs obtained with InfoNet are consistently higher than the ones obtained with MINE, showing that InfoNet provides a better estimation than MINE of mutual information despite a much shorter running time.\n\nDetailed plots and results can be found in B.3.2 of the revised version.\n\nOur findings, detailed in B.3 in the revised paper, demonstrate that InfoNet, leveraging sliced mutual information, exhibits robust performance at low time complexity in comparison to existing methods for estimating mutual information in high dimension.\n\n[1] Goldfeld Z, Greenewald K. Sliced mutual information: A scalable measure of statistical dependence[J]. Advances in Neural Information Processing Systems, 2021, 34: 17567-17578."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4727/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588476143,
                "cdate": 1700588476143,
                "tmdate": 1700588476143,
                "mdate": 1700588476143,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8fBdUsh9Gi",
            "forum": "PyHRUMxKbT",
            "replyto": "PyHRUMxKbT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4727/Reviewer_FUUK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4727/Reviewer_FUUK"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors have proposed a neural network approach using the attention-based mechanism called InfoNet for mutual information estimation. Using the Donsker & Varadhan representation of the KL divergence, the parameter of the network can be optimized by maximizing an objective function (Equation 2). Furthermore, an evaluation of the efficacy of the proposed approach on the Hirschfeld-Gebelein-R\u00b4enyi (HGR) Maximum Correlation Coefficient has been considered."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Speeding up the previous similar work, MINE by Belghazietal., 2018 using highly parallelizable attention modules."
                },
                "weaknesses": {
                    "value": "Overall the paper is well written, and I don't have any specific concerns. I just wanna clarify two things:\n\n1- Is speeding up MINE using the attention architecture the only difference between InfoNet and MINE? Fundamentally, these two algorithms are similar and the same objective function using the same Donsker & Varadhan representation of the KL divergence has been used for the MI estimation. \n\n2 - While the authors have mentioned some motivations for evaluating the proposed approach for MMC, there are other dependence measures with computational efficiency that also enjoy the properties of MI. For instance, Energy Distance (ED) by G\u00e1bor  Sz\u00e9kely is one of them which is a dual form of Maximum Mean Discrepancy (MMD). In particular, ED  does not require any parametric assumption about the underlying probability distribution of the data, making it easy for many statistical inference problems. It is a good idea to compare the proposed MI estimator and MCC with ED in terms of the correlation of two random variables (similar to Figure 4 ) and computational speed given the fact that ED is quite fast to use in high dimensional data."
                },
                "questions": {
                    "value": "Please see my above comments."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4727/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4727/Reviewer_FUUK",
                        "ICLR.cc/2024/Conference/Submission4727/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4727/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698799653656,
            "cdate": 1698799653656,
            "tmdate": 1700740853703,
            "mdate": 1700740853703,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qb2fMEj3Uj",
                "forum": "PyHRUMxKbT",
                "replyto": "8fBdUsh9Gi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4727/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4727/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer FUUK,\n\nThank you for your insightful and constructive review of our manuscript. We greatly appreciate the time and effort you have devoted to evaluating our work, and your positive remarks on our proposed feed-forward neural network approach, InfoNet, using the attention-based mechanism for mutual information estimation, as well as the excellent presentation of the work.\n\nWe have taken a detailed look into your questions and comments, and have prepared comprehensive responses to each of your points, which we hope will address the issues effectively. We followed your suggestion to explore the comparison with other dependence measures like Energy Distance.\n\nWe hope that these clarifications can help finalize your assessment and the rating of our paper. Please also let us know if you have any further questions that we need to provide additional clarifications.\n\n***\n\n**W1 Is speeding up MINE using the attention architecture the only difference between InfoNet and MINE? Fundamentally, these two algorithms are similar and the same objective function using the same Donsker \\& Varadhan representation of the KL divergence has been used for the MI estimation.**\n\n**A**: Thanks for your comment. Speeding up MINE with the attention architecture is not the only difference. To elaborate, we first provide a brief summary of MINE and InfoNet. \n\n*MINE*: optimizes a neural network (NN) with the Donsker-Varadhan (DV) objective for a sequence \\{$x_t,y_t$\\} ($t=1,..., T$) sampled from a single distribution $p(x,y)$. When the optimization is done, the mutual information (MI) estimation of $p(x,y)$ is obtained. If one needs to estimate the MI of a different distribution $p\u2019(x,y)$ using MINE, the neural network will be optimized again using the DV objective, but the sequence sampled from $p\u2019(x,y)$. Since the optimal parameters of an NN ($\\theta^*$) are different for different distributions.\n\n*InfoNet*: optimizes a neural network (the proposed) also with the DV objective, but for many different distributions, i.e., sequences \\{$x_t^i,y_t^i$\\} ($t=1,..., T$,$i=1,..., N$) are sampled from N different distributions $p^i(x,y)$\u2019s. When the optimization is done, it can be used to estimate the MI of an arbitrary distribution via the generalization capability of the proposed InfoNet architecture, without re-training. Equivalently, InfoNet, with a sequence as input, can directly output the optimal set of parameters ($\\theta^*$) corresponding to the one obtained by performing a MINE optimization for a distribution (without finetuning InfoNet on this distribution).\n\nWith the above in mind, the key differences between MINE and InfoNet are listed below:\n\n1. During training, MINE optimizes the NN for a single distribution, but InfoNet optimizes the proposed NN for many distributions.\n\n2. During training, MINE treats the NN as a (scalar) discriminant whose optimum (defined by the DV formula and the sequence) gives the MI of the distribution represented by the input sequence. However, InfoNet acts as a hyper-network, and InfoNet outputs the optimal discriminant (NN in MINE) corresponding to an arbitrary distribution from the input sequence.\n\n3. During testing, when a new distribution arrives, MINE needs to optimize the NN for estimating its distribution with the sequence sampled from this new distribution. In contrast, InfoNet takes the sampled sequence from this new distribution, and directly outputs the optimal NN (represented as a look-up table) as well as the MI estimation without performing any network update."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4727/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587672797,
                "cdate": 1700587672797,
                "tmdate": 1700587672797,
                "mdate": 1700587672797,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "of5w0RlyEy",
                "forum": "PyHRUMxKbT",
                "replyto": "8fBdUsh9Gi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4727/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4727/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**W2: It is a good idea to compare the proposed MI estimator and MCC with ED in terms of the correlation of two random variables (similar to Figure 4 ) and computational speed given the fact that ED is quite fast to use in high dimensional data.**\n\n\n**A**: Thank you for bringing up the comparison with Energy Distance (ED). Indeed, ED is a noteworthy non-parametric measure known for its computational efficiency, especially in high-dimensional data scenarios, and it is a good measure in measuring the differences between two distributions. \n\nWe follow the suggestion and compare InfoNet (operated under 1000 random slices similar to [1]) with ED [2] in terms of computational efficiency and performance. Specifically, we consider two tasks. \n\nTask 1: given three jointly sampled random variables, X, Y, and Z from high-dimensional Gaussian, we want to know which one of Y and Z is more correlated with X, where the correct order, i.e., MI(X,Y)>=MI(X,Z) or MI(X,Y)<MI(X,Z)  is obtained by the ground-truth mutual information (MI) computed with the analytical solution of MI for Gaussian. The performance metric is the binary classification accuracy.\n\nTask 2: Independence test. Given a pair of random variables X, and Y, either independent or not, we apply InfoNet and ED to check their independence since both metrics can claim independence between two random variables. We vary the epsilon as the threshold, and we plot the precision-recall curve to compare their performance using the area under the curve (AUC, the closer to 1, the better).\n\nResults:\n\nA. Efficiency: Time compare:\n| Dimension    | 16 | 128 |\n|------------|----|----|\n| ED  | 0.035 | 0.224 |\n| Ours  | 0.050 | 0.058 |\n\nFrom the Table, we can see that the efficiency of InfoNet is not sensitive to the dimension of the random variable, and InfoNet is more efficient than ED at a higher dimension.\n\nB. In terms of the correlation order test (Task 1), we get the following performance:\n\n| Dimensions | 2    | 3    | 4    | 5    | 6    | 7    | 8    | 9    | 10   |\n|------------|------|------|------|------|------|------|------|------|------|\n| Energy Distance | 49.6 | 51.2 | 52.2 | 51.5 | 52.5 | 49.6 | 48.7 | 50.2 | 51.3 |\n| MINE-5000  | 96.2 | 97   | 97   | 96.2 | 94.9 | 94.2 | 93.2 | 92.8 | 93   |\n| InfoNet | **97.7** | **96.4** | **96.2** | **97.9** | **97.4** | **98.1** | **98.2** | **97.3** | **98.3** |\n\nFrom the table, we can see that InfoNet performs better in terms of estimating the correlation orders between a triplet of random variables across multiple dimensions.\n\nDetails of this experiment can be seen in B.3.1 of the revised version.\n\nC. In terms of the independence test, we have the AUCs of ED and InfoNet obtained under different dimensions and sequence lengths:\n\n| Sequence length | 50    | 100    | 150    | 200    | 250    | 300    | 350    | 400 |\n|------------|------|------|------|------|------|------|------|------|\n| Energy Distance d=16 | 0.733 | 0.964 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 |\n| Energy Distance d=128 | 0.167 | 0.1 | 0.265 | 0.416 | 0.473 | 0.768 | 0.879 | 0.981 |\n| InfoNet d=16  | 0.914 | 0.992   | 1.0   | 1.0 | 1.0 | 1.0 | 0.99 | 1.0 | \n| InfoNet d=128  | 0.694 | 0.603   | 0.819   | 0.832 | 0.954 | 0.950 | 0.981 | 0.992 |\n\nFrom the Table, we can observe that the AUCs obtained with InfoNet are consistently higher than the ones obtained with ED, showing that InfoNet provides a better metric than ED.\n\nDetailed plots and results can be found in B.3.2 of the revised version.\n\n\n[1] Goldfeld Z, Greenewald K. Sliced mutual information: A scalable measure of statistical dependence[J]. Advances in Neural Information Processing Systems, 2021, 34: 17567-17578.\n\n[2] Rizzo M L, Sz\u00e9kely G J. Energy distance[J]. wiley interdisciplinary reviews: Computational statistics, 2016, 8(1): 27-38."
                    },
                    "title": {
                        "value": "Replying to Official Comment by Authors"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4727/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588211226,
                "cdate": 1700588211226,
                "tmdate": 1700588955171,
                "mdate": 1700588955171,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yY8D7833od",
                "forum": "PyHRUMxKbT",
                "replyto": "8fBdUsh9Gi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4727/Reviewer_FUUK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4727/Reviewer_FUUK"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your effort to include more clarification about your approach in comparison with MINE, and adding more experiments about ED. I have read the comments and the revised version and increased my score from 6 to 8 (There is no 7 in the scores :))."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4727/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740783787,
                "cdate": 1700740783787,
                "tmdate": 1700740921382,
                "mdate": 1700740921382,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oPThVljCup",
            "forum": "PyHRUMxKbT",
            "replyto": "PyHRUMxKbT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4727/Reviewer_C35n"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4727/Reviewer_C35n"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the estimation of mutual information between two random variables, primarily focusing on sequences, by leveraging a neural network-based approach for efficiency. To be specific, the authors introduce the application of an attention mechanism for feed-forward predictions, trained with the MINE-based training objective. Experimental results demonstrate the effectiveness of the proposed approach with different families of distribution, as well as its promising results on real-world applications."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "### Originality\n\nThe paper builds on two existing works: MINE [Belghazi et al.] and attention mechanism [Jaegle et al.]. The former is used as a training objective, while the latter is used to parameterize the function in MINE. The combination of two existing ideas enables efficient computation for mutual information between two sequences.\n\n\n### Significance\n\nEstimating mutual information between two high-dimensional variables is important, yet challenging for many real-world tasks. The paper proposes one way to efficiently compute MI between two sequences."
                },
                "weaknesses": {
                    "value": "I am confused with some parts, might be due to writing or organization. Please see the following questions."
                },
                "questions": {
                    "value": "### Introduction\n\n- 'Specifically, we want to explore whether the estimation of mutual information can be performed by a feed-forward prediction of a neural network' -- What does a feed-forward prediction mean? For MINE, we still use NNs to parameterize a function and output a scalar via NNs. Is MINE a feed-forward prediction? Please elaborate it.\n\n- 'Moreover, each time the joint distribution changes (different sequences), a new optimization has to be performed, thus not efficient.' -- For Figure 1, which type of sequences are you considering? I don't understand 'a new optimization has to be performed'. Could you please elaborate more? Figure 1 lacks necessary contexts.\n\n- 'This way, we transform the optimization-based estimation into a feed-forward prediction, thus bypassing the time-consuming gradient computation and avoiding sub-optimality via large-scale training on a wide spectrum of distributions.' -- For MINE, we do need to update NNs' parameters. But InfoNet also needs gradient ascent. How to understand 'bypassing the time-consuming gradient computation'?\n\nAll in all, I think the confusion is due to the lack of enough explanations. Could you please elaborate the difference between MINE and InfoNet? The same training objective is used. Both can parameterize the function via NNs, even the same NN architecture.\n\n\n### Method\n\n- To estimate mutual information between two sequences in Eq.(2), InfoNet only considers $(x_{t}, y_{t})$ at each time step? Thus, it ignores mutual information between e.g., $x_{1}$ and $y_{2}$?\n\n- In terms of Figure 2, the difference between MINE and InfoNet is just a look-up table? I think same NN architectures could also be used in MINE?\n\n---\n**Update after rebuttal**: I raised the score: 5 --> 6."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4727/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4727/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4727/Reviewer_C35n"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4727/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698922626851,
            "cdate": 1698922626851,
            "tmdate": 1700660778886,
            "mdate": 1700660778886,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NPRva8Ep7z",
                "forum": "PyHRUMxKbT",
                "replyto": "oPThVljCup",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4727/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4727/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer C35n,\n\nThank you for your careful review and constructive questions. We also appreciate your acknowledgment of the importance of the problem we are tackling, and the efficiency of the InfoNet we are proposing.\n\nWe understand that there exists a misunderstanding, and the key differences between MINE and InfoNet need to be clarified. Accordingly, we have revised the paper with all the clarifications derived from these questions, which are answered in detail below. \n\nWe hope that these clarifications can help finalize your assessment and the rating of our paper. Please also let us know if you have any further questions that we need to provide additional clarifications.\n\n***\n\n**W1: I am confused with some parts, might be due to writing or organization. Please see the following questions.**\n\n**A**: Thanks for all your questions below. We understand that there is confusion on some parts due to the lack of enough explanations on the key differences between MINE and InfoNet. Next, will first elaborate on the key differences, and then provide clarifications for each of the questions.\n\n***\n\n**Q1: All in all, I think the confusion is due to the lack of enough explanations. Could you please elaborate on the difference between MINE and InfoNet? The same training objective is used. Both can parameterize the function via NNs, even the same NN architecture.**\n\n**A**: First, we provide a brief summary of MINE and InfoNet. \n\n*MINE*: optimizes a neural network (NN) with the Donsker-Varadhan (DV) objective for a sequence \\{$x_t,y_t$\\} ($t=1,..., T$) sampled from a single distribution $p(x,y)$. When the optimization is done, the mutual information (MI) estimation of $p(x,y)$ is obtained. If one needs to estimate the MI of a different distribution $p\u2019(x,y)$ using MINE, the neural network will be optimized again using the DV objective, but the sequence sampled from $p\u2019(x,y)$. Since the optimal parameters of an NN ($\\theta^*$) are different for different distributions.\n\n*InfoNet*: optimizes a neural network (the proposed) also with the DV objective, but for many different distributions, i.e., sequences \\{$x_t^i,y_t^i$\\} ($t=1,..., T$,$i=1,..., N$) are sampled from N different distributions $p^i(x,y)$\u2019s. When the optimization is done, it can be used to estimate the MI of an arbitrary distribution via the generalization capability of the proposed InfoNet architecture, without re-training. Equivalently, InfoNet, with a sequence as input, can directly output the optimal set of parameters ($\\theta^*$) corresponding to the one obtained by performing a MINE optimization for a distribution (without finetuning InfoNet on this distribution).\n\nWith the above in mind, the key differences between MINE and InfoNet are listed below:\n\n1. During training, MINE optimizes the NN for a single distribution, but InfoNet optimizes the proposed NN for many distributions.\n\n2. During training, MINE treats the NN as a (scalar) discriminant whose optimum (defined by the DV formula and the sequence) gives the MI of the distribution represented by the input sequence. However, InfoNet acts as a hyper-network, and InfoNet outputs the optimal discriminant (NN in MINE) corresponding to an arbitrary distribution represented by the input sequence.\n\n3. During testing, when a new distribution arrives, MINE needs to optimize the NN for estimating its MI with the sequence sampled from this new distribution. In contrast, InfoNet takes the sampled sequence from this new distribution, and directly outputs the optimal NN (represented as a look-up table) as well as the MI estimation without performing any network update.\n\n***\n\n**Q2: 'Specifically, we want to explore whether the estimation of mutual information can be performed by a feed-forward prediction of a neural network' -- What does a feed-forward prediction mean? For MINE, we still use NNs to parameterize a function and output a scalar via NNs. Is MINE a feed-forward prediction? Please elaborate it.**\n\n**A**: Thanks for the question. MINE optimizes the NN, which outputs a scalar for a data point in the sequence and accumulates the scalars to get the MI estimate of the distribution represented by the sequence. However, MINE has to perform another optimization for a different distribution (or sequence), with which, we state that MINE is not feed-forward. In other words, MINE is feed-forward for predicting the scalar for a data point, but not feed-forward for estimating the MI of a distribution.\n\nIn contrast, after massive training on a spectrum of distributions, when a new distribution is presented to InfoNet in the form of a sequence, InfoNet predicts the optimal discriminant (without changing its parameters), which then computes the scalars and the MI estimate of the new distribution. So, InfoNet is feed-forward for computing the MI of a distribution, no need to perform training for a new distribution. Thus InfoNet is much more efficient than MINE as no gradient ascent is needed with a test distribution."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4727/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700587199523,
                "cdate": 1700587199523,
                "tmdate": 1700587199523,
                "mdate": 1700587199523,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dnIna00BaF",
                "forum": "PyHRUMxKbT",
                "replyto": "NPRva8Ep7z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4727/Reviewer_C35n"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4727/Reviewer_C35n"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response!"
                    },
                    "comment": {
                        "value": "Thank you for the detailed response. I appreciate all the efforts to improve the paper and the new experiments!\n\nThe key differences between MINE and InfoNet are clear to me now. MINE is tailored for a specific data distribution. When a new data distribution comes in, it might fail without being re-trained on the new distribution. On the other hand, InfoNet looks like a universal mutual information neural estimator. Thus, it seems similar to what pre-trained models are doing.\n\nAdditionally, the DV objective is essentially similar to the training objective used for contrastive learning, where pre-training has gained popularity, even though these approaches operate in different domains.\n\nI raised the score: 5 --> 6."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4727/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660708540,
                "cdate": 1700660708540,
                "tmdate": 1700660708540,
                "mdate": 1700660708540,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]