[
    {
        "title": "Principled Federated Domain Adaptation: Gradient Projection and Auto-Weighting"
    },
    {
        "review": {
            "id": "HbKiipNiug",
            "forum": "6J3ehSUrMU",
            "replyto": "6J3ehSUrMU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2080/Reviewer_d97G"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2080/Reviewer_d97G"
            ],
            "content": {
                "summary": {
                    "value": "The submission studied domain adaptation under the federated setting. Two methods (FedDA (1) and FedGP(2)) to aggregate gradients were proposed based on the analysis of the delta error of an aggregation rule (Theorem 3.6). An auto-weighting rule (3) was proposed (FedDA_Auto and FedGP_Auto), too. Experiments showing the robustness (Figure 2) and target accuracies (Table 1) justified the proposed methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The submission identifies the factors that affect the performance of an aggregation rule and then proposes solutions based on the findings.\nThe target domain enjoys the robustness and performance increases brought by the solutions."
                },
                "weaknesses": {
                    "value": "Despite a comfortable reading experience and leading performance results, I would like to raise a concern about the problem formulation.\n\n(a) From the federated learning perspective, the server and the clients are learning together to achieve a better performance measured by the sum of ALL clients. Therefore, federatively speaking, paying the whole attention to ONE target client might not align with the original intention of studying federated learning."
                },
                "questions": {
                    "value": "(b) Given the federated learning nature, multiple target clients seem more practical. How would the proposed method scale with the number of target clients?\n\n(c) The current source clients are given and assumed to be well-trained. What are the potential and challenges to extending the proposed method to a scenario where every client learns and transfers simultaneously?\n\n(d) What if one negates the direction of the projections of negative source gradients (e.g., the projections of g_s3 and g_s4 in Figure 1)?\n\n(e) The behaviors of FedDA in Figure 2(a) and Figure 2(c) seem contracdict to each other. Is it trivial? Or may I have a clarification?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2080/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698024469569,
            "cdate": 1698024469569,
            "tmdate": 1699636140074,
            "mdate": 1699636140074,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LNbZ86EYGy",
                "forum": "6J3ehSUrMU",
                "replyto": "HbKiipNiug",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2080/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2080/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for reviewing our manuscript and we appreciate your feedback for refining the final version. We address your concern and question in the following.\n\n*\u201d\u2026, federatively speaking, paying the whole attention to ONE target client might not align with the original intention of studying federated learning\u2019\u2019* \n* Thank you for this comment. Our view is that the setting remains federated in the sense that there is a central server coordinating device toward a shared objective. However, we are most interested in the applied problem setting. An example that motivates our federated domain adaptation perspective is that: as a new client with limited local data (and with domain shift) shows up, the goal for the whole federated system is coming up with a model for the new client using federated data across all the clients. \n\n  In general, the settings considered in the paper are different from the original FL setting as the target client has a limited amount of data - which means if we do normal FL, it will gain fewer benefits from the federation.  As we show in the paper, personalized baselines do not work well in these settings. However, these settings still require federated collaboration. For the target client with limited data, our framework shows how other clients with sufficient data can collaborate and help it train a better model. Also, our work focuses on Domain Adaptation, which usually has one target domain to be adapted to. Further, we acknowledge the possibility of extending the current framework to multiple targets, which serves as a promising yet non-trivial future work.\n\n\n*\u201dGiven the federated learning nature, multiple target clients seem more practical. How would the proposed method scale with the number of target clients?\u201d*\n\n* Thanks for the great question! We can potentially have several copies of the global model for each target client in the system, where each target client learns their best-personalized model simultaneously from other source clients in the system. The direct extension of the method scales linearly with the number of target clients (in memory, central computation, and communication). Finding more efficient approaches for multi-target settings is intriguing, and left for future work.\n\n*\u201dWhat are the potential and challenges to extending the proposed method to a scenario where every client learns and transfers simultaneously?\u201d*\n\n* Generally speaking, we can separate the learning and transferring stages for this more challenging scenario: where clients are first trained using normal FL methods (e.g., FedAvg) without transferring. At some point when its model is comparatively well-trained, we can start to use it as source clients and perform transferring. However, we see the challenges are: (1) when we should stop the personalization and start to perform adaptation, (2) clients may have different best aggregation rules for transferring individually. However, one can potentially leverage our theoretical framework to decide which aggregation rule is the best to apply."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2080/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639432394,
                "cdate": 1700639432394,
                "tmdate": 1700640183310,
                "mdate": 1700640183310,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bBqM4qMgsx",
                "forum": "6J3ehSUrMU",
                "replyto": "HbKiipNiug",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2080/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2080/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "*\u201dWhat if one negates the direction of the projections of negative source gradients (e.g., the projections of g_s3 and g_s4 in Figure 1)?\u201d*\n\n* Thanks for the interesting idea! **We performed the following experiments and the results show that negating the direction does not impact the performance much.**\n\n   \n **On Fashion-mnist with label shifts**\n\n\n| Label shifts level         | 0.45  | 0.3   | 0.15  | 0.10  | 0.05  |\n|--------------|-------|-------|-------|-------|-------|\n| fedgp        | 82.11 | 83.45 | 85.74 | 88.67 | 91.79 |\n| fedgp_negate | 82.62 | 83.30 | 86.88 | 88.38 | 91.69 |\n\n  **On Fashion-mnist with noisy features**\n\n| Target noise level | 0     | 0.2   | 0.4   | 0.6   | 0.8   |\n|--------------------|-------|-------|-------|-------|-------|\n| fedgp              | 76.33 | 75.09 | 71.09 | 68.01 | 62.22 |\n| fedgp_negate       | 75.14 | 73.63 | 69.54 | 65.17 | 61.71 |\n\n  **On Cifar-10 with noisy features**\n\n| Target noise level | 0     | 0.2   | 0.4   | 0.6   | 0.8   |\n|--------------------|-------|-------|-------|-------|-------|\n| fedgp              | 64.14 | 65.92 | 65.39 | 63.67 | 61.29 |\n| fedgp_negate       | 65.38 | 66.18 | 64.84 | 63.93 | 61.35 |\n\n\n\n*\u201dThe behaviors of FedDA in Figure 2(a) and Figure 2(c) seem contradict to each other. Is it trivial? Or may I have a clarification?\u201d*\n\n* Thanks for the question and we appreciate the chance for clarification. The reason behind the distinct behaviors of FedDA in Figure 2(a) and 2(c) is due to the experiment settings. For 2(a), we have noisy features, so when the shifts are larger FedDA\u2019s performance becomes worse. In this setting, the variances on the target domain do not change much. However, for 2(c), the target variance decreases as the domain shift (eta -> 0) grows bigger (easier to predict with fewer classes), FedGP and auto methods will benefit from this with higher performance when the shifts are large. However, FedDA will have a very low performance with large shifts (eta closer to 0), which makes it have a different behavior than Figure 2(a)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2080/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639482047,
                "cdate": 1700639482047,
                "tmdate": 1700639560428,
                "mdate": 1700639560428,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XndbBX98ib",
                "forum": "6J3ehSUrMU",
                "replyto": "bBqM4qMgsx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2080/Reviewer_d97G"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2080/Reviewer_d97G"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the feedback"
                    },
                    "comment": {
                        "value": "Thank you for the materials provided. I will decide on my final score during the reviewer discussion. Due to the limited time, my preliminary understanding of combining the paper and the feedback materials is that the submission leverages the advantages of federated learning (FL) but avoids addressing the challenges of FL, such as community-wide optimization, multiclients, and asynchronous updates."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2080/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692267365,
                "cdate": 1700692267365,
                "tmdate": 1700692267365,
                "mdate": 1700692267365,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8nUEhld5sD",
            "forum": "6J3ehSUrMU",
            "replyto": "6J3ehSUrMU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2080/Reviewer_tJ7A"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2080/Reviewer_tJ7A"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes two algorithms to solve federated domain adaptation, a case in which there exists a distributional/domain shift between clients in federated learning. The authors tackle the problems of domain shift and data scarcity in their work. To solve these problems, they propose to design algorithms concerning the __server aggregation rule__, i.e., how the server merges the different gradients of the same model coming from the clients. The two proposed methods are called __FedDA__ which does a convex combination of clients' gradients (including the target), and __FedGP__, which extracts information from source clients' gradients based on the target client gradients."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "__Originality.__ The authors provide a novel theoretical framework for the analysis of Federated Learning under heterogeneity.\n\n__Quality.__ The paper is globally well-written and clear. Parts of the experimental section could be improved\n\n__Clarity.__ The novel theoretical framework is easy to follow. Assumptions, notation and definitions are clearly stated and the proposed algorithms are intuitive.\n\n__Significance.__ This paper tackles an extremly important problem in federated learning, i.e., how to deal with __client heterogeneity__. In this sense, besides being important for the niche of federated DA, it can also impact federated learning in general."
                },
                "weaknesses": {
                    "value": "__Major__\n\n__W1.__ The description of the real-world experiments in the main paper is not sufficient. While the authors do provide enough information in the appendix, the main paper does not fully describes the methodology the authors employed in consolidating the results of Table 1. For instance, how are the labeled data points chosen for the experiments? How does the performance change w.r.t. to the choice of data points (i.e. standard deviation of the accuracy on target domain)? These questions are answered in the appendix, but they should be clear in the main paper.\n\n__Minor.__ (note, this point __did not__ impacted negatively in my review).\n\n__W2.__ I would like to raise that, while this is a Federated DA paper, the authors assume access to a (small) set of labeled data in the target domain. This somewhat breaks the rules of _Unsupervised_ DA, and may bias performance towards methods that use target labeled data when comparing with UDA algorithms such as KD3A. This remains somewhat true even when supposing a small amount of labeled target data, depending on the degree of distributional shift. Nonetheless, __I do think the authors use labeled data in a clearly motivated and justified way__."
                },
                "questions": {
                    "value": "__Q1.__ Concerning __FedDA__ and __FedGP__ aggregation schemes, in order to have a convex combination, shouldn't $\\sum_{i=1}^{n}\\beta_{i}=1$? Is this constraint enforced? For instance, in Figure 9 (appendix), the sum of betas exceeds 1."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2080/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2080/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2080/Reviewer_tJ7A"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2080/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698598206369,
            "cdate": 1698598206369,
            "tmdate": 1699636139981,
            "mdate": 1699636139981,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DpohB6c9Td",
                "forum": "6J3ehSUrMU",
                "replyto": "8nUEhld5sD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2080/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2080/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for reviewing our manuscript and we appreciate your feedback for refining the final version. We address your concern and question in the following.\n\n*\u201dWhile the authors do provide enough information in the appendix, the main paper does not fully describes the methodology the authors employed in consolidating the results of Table 1. For instance, how are the labeled data points chosen for the experiments? How does the performance change w.r.t. to the choice of data points (i.e. standard deviation of the accuracy on target domain)? These questions are answered in the appendix, but they should be clear in the main paper.\u201d*\n\n* Thank you for pointing out the insufficient description of the real-world experiments. The labeled data are sampled uniformly at random, and the performance of the FedGP, FedDA_Auto and FedGP_Autp are robust w.r.t. different levels of target domain data scarcity. We have revised our manuscript and made sure the information was clear in the updated main paper.\n\n*\u201dI would like to raise that, while this is a Federated DA paper, the authors assume access to a (small) set of labeled data in the target domain. This somewhat breaks the rules of Unsupervised DA, and may bias performance towards methods that use target labeled data when comparing with UDA algorithms such as KD3A. This remains somewhat true even when supposing a small amount of labeled target data, depending on the degree of distributional shift. Nonetheless, I do think the authors use labeled data in a clearly motivated and justified way.\u201d*\n\n* Thanks for highlighting the difference between our setting and the UDA setting. We note that we included comparisons with UDA algorithms for the sake of completeness and to provide a broader perspective on the performance of our method. Indeed, there is a distinction between the setting of Federated DA and Unsupervised DA. \n\n*Concerning FedDA and FedGP aggregation schemes, in order to have a convex combination, shouldn't (sum of beta = 1)? Is this constraint enforced? For instance, in Figure 9 (appendix), the sum of betas exceeds 1.*\n\n* Thanks for the question and we appreciate the chance to clarify it. For each $\\beta_i$, it is the combination factor between one source domain $S_i$ and the target domain $T$. In other words, we use $\\beta_i$ and $(1-\\beta_i)$ to convexly combine the target domain model update and one source domain  $S_i$'s model update. For example, for the non-auto-weighting FedDA and FedGP, we set $\\beta_i = 0.5$ for all source domains. Therefore, the sum of beta may not be 1. Then, among different source domains, we use the convention of $w_i = \\frac{n_i}{n}$ ($n_i$ being the number of data points at source domain $S_i$, and $n$ being the total number of source data points) for weighting each (source, target) pair, which has a sum of 1."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2080/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640109441,
                "cdate": 1700640109441,
                "tmdate": 1700640109441,
                "mdate": 1700640109441,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zjrYF3HCeK",
            "forum": "6J3ehSUrMU",
            "replyto": "6J3ehSUrMU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2080/Reviewer_28v7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2080/Reviewer_28v7"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the optimal design of an aggregator in federated domain generalization (generalizing to a target client/domain in cross-silo). The FedDA method works by aggregating interpolations (i.e. weighted averages) of the target domain gradient and the source domain gradients. The FedGP method further projects the source gradient onto the \"positive direction\" of the domain gradient before interpolation (i.e. zeroing out conflicting directions). The interpolation factor for each domain is chosen such that the $L^\\pi$ distance between the source and target gradients is minimized w.r.t. some prior $\\pi$ on the parameters. The authors show that this error can be decomposed as a noise term of the target domain and a distance term between the source and the target domains. The optimal interpolation factor would then balance the source and target gradients based on those terms, which can be estimated during training in a scheme called auto_weight. Extensive experiments show the benefits of this approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper's analysis and experiments are well-detailed.\n- The analysis is interesting and covers many aspects of the design of an optimal aggregator in the federated domain generalization setting.\n- The method is intuitive and is easy to implement (save for the auto-weighting scheme).\n- The improvement seems to be significant in terms of generalizing to the target domain with respect to personalized federated learning algorithms."
                },
                "weaknesses": {
                    "value": "- It seems like it would be better to compare the algorithms presented in this paper to domain generalization algorithms, such as the ones shown in DomainBed's GitHub repo.\n- The methods presented make sense mostly in the cross-silo setting, as mentioned in the paper, which limits its applicability to general federated learning problems with a relatively larger number of clients that can benefit a lot from methods for generalizing to new clients.\n- It is mentioned multiple times that data scarcity is the setting of interest, in which FedDA and FedGP are supposed to perform more favorably. However, we do not see experiments showing the effect of data scarcity on the robustness of the performance of FedGP vs. FedAvg, for example.\n- Personalized federated learning algorithms are relevant for comparison, but I think that direct comparison of such algorithms with FedGP might put them at a disadvantage since they are not specifically designed for domain generalization. ColoredMNIST, VLCS, and TerraIncognita datasets are more concerned with shifts in p(x) or p(x|y), whereas personalized FL is more concerned with shifts in p(y) and p(y|x), i.e. personalizing the prediction rather than adapting to spurious correlations or invariant attributes. You should either choose federated datasets for comparison, or you should compare your algorithm to domain generalization algorithms, e.g. IRM and others. Or why not use a hospital dataset that fits the setting you described in the paper? For example, you can consider FLamby [2].\n- One work [1] from federated continual learning might be of interest (which even shares the same name FedGP). It is motivated from a similar intuition, which is to remove from the gradient its projection onto the negative direction of the reference gradient.\n- In algorithm 1, the auto_weight scheme requires intermediate gradients for each domain, which might require a lot of memory.\n\n[1] FedGP: Buffer-based Gradient Projection for Continual Federated Learning. Dai et al. 2023.\n[2] FLamby: Datasets and Benchmarks for Cross-Silo Federated Learning in Realistic Healthcare Settings. Terrail et al. 2022."
                },
                "questions": {
                    "value": "Can you train your algorithms and compare them on federated datasets that follows the setting of interest in the paper (cross-silo with data scarcity)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2080/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2080/Reviewer_28v7",
                        "ICLR.cc/2024/Conference/Submission2080/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2080/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698788976040,
            "cdate": 1698788976040,
            "tmdate": 1700641729548,
            "mdate": 1700641729548,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HrXnfJx5FI",
                "forum": "6J3ehSUrMU",
                "replyto": "zjrYF3HCeK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2080/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2080/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for reviewing our manuscript and we appreciate your feedback for refining the final version. We address your concerns and questions as follows. \n\n*\u201dIt seems like it would be better to compare the algorithms presented in this paper to domain generalization algorithms, such as the ones shown in DomainBed's GitHub repo.\u201d*\n\n* We thank the reviewer for this constructive suggestion to make our experimental analysis more comprehensive. **We have conducted the suggested experiment and the results have been detailed in the updated version of the paper.** As summarized below, the proposed method outperforms those reported in the DomainBed's GitHub repo for the Domain Generalization (DG) methods (we report the highest average DG performance vs. our methods). \n\n|         | Colored-MNIST |  VLCS    | PACS     | Office-Home | TerraIncognita | DomainNet |\n|---------|---------------|----------|----------|-------------|----------------|-----------|\n| Ours    | **84.1**     | **83.8** | **93.4** | **74.3**    | **74.6**       | **53.7**  |\n| Best DG |  67.7         | 79.9     | 87.2     | 68.5        | 54.4           | 41.8      |\n\n*\u201dThe methods presented make sense mostly in the cross-silo setting, as mentioned in the paper, which limits its applicability to general federated learning problems with a relatively larger number of clients that can benefit a lot from methods for generalizing to new clients.\u201c*\n\n* In our work, we focus on the cross-silo setting, but it is possible to extend our ideas to the cross-device setting. For example, we can potentially first perform clustering before applying our methods, to find a subset of clients to collaborate for better target performance. We believe our theoretical and empirical findings showcase good insights for extending current work to this different setting.\n\n*\u201dIt is mentioned multiple times that data scarcity is the setting of interest, in which FedDA and FedGP are supposed to perform more favorably. However, we do not see experiments showing the effect of data scarcity on the robustness of the performance of FedGP vs. FedAvg, for example.\u201d*\n\n* The experiment showing the effect of data scarcity on the robustness is included in Appendix C.8 where it shows the proposed methods are robust to data scarcity. In addition, we appreciate your suggestion of comparing with FedAvg as FedAvg serves as a baseline for the effect of data scarcity. **As shown below (detailed in appendix C.8 of the updated paper), we compare to FedAvg showing the effect of data scarcity. Our FedDA_auto consistently outperforms FedAvg with large margins.** \n\n|   noise level | Number of samples |     0.2    |        |     0.4    |        |     0.6    |        |\n|--------------:|-------------------|:----------:|:------:|:----------:|:------:|:----------:|:------:|\n|               |                   | FedDA_auto | FedAvg | FedDA_auto | FedAvg | FedDA_auto | FedAvg |\n| Fashion-MNIST |               100 |  **79.04** |  75.98 |  **72.21** |  59.36 |  **66.16** |  49.94 |\n|               |               200 |  **79.74** |  76.50 |  **74.30** |  60.28 |  **69.27** |  48.56 |\n|               |               500 |  **79.48** |  75.55 |  **75.21** |  58.74 |  **71.40** |  47.93 |\n|               |              1000 |  **80.23** |  76.12 |  **76.75** |  62.33 |  **73.16** |  50.81 |\n|    CIFAR-10   |                5% |  **63.04** |  24.54 |  **60.79** |  21.25 |  **60.02** |  19.42 |\n|               |               10% |  **65.72** |  22.86 |  **64.43** |  22.35 |  **62.25** |  18.60 |\n|               |               15% |  **66.57** |  23.20 |  **65.40** |  23.25 |  **63.36** |  17.88 |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2080/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640524935,
                "cdate": 1700640524935,
                "tmdate": 1700640524935,
                "mdate": 1700640524935,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vTnuXhDHdh",
                "forum": "6J3ehSUrMU",
                "replyto": "Dlc2P16CuK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2080/Reviewer_28v7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2080/Reviewer_28v7"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the great rebuttal and for running the extra experiments. The experiments show that your method is consistently better than some baselines. Thus, I have raised my score accordingly."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2080/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641614156,
                "cdate": 1700641614156,
                "tmdate": 1700641614156,
                "mdate": 1700641614156,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NPBZ3F3ko3",
            "forum": "6J3ehSUrMU",
            "replyto": "6J3ehSUrMU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2080/Reviewer_nVHX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2080/Reviewer_nVHX"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers two important issues in federated learning: small data at client sites and domain shift across clients. Simple, intuitive strategies such as gradient projection and auto-weighting for mitigating these issues are proposed. Several interesting theorems are proved regarding federated aggregation, gradient projectin. Results on three datasets are provided."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Nice treatment of federated learning in the presence of domain shift and small data. A good mixture of theoretical and experimental work."
                },
                "weaknesses": {
                    "value": "I loved the paper till I came to the experiments section. In this day and age, should we still be doing experiments with ColoredMNIST, VLCS, CIFAR-10 and TerraIncognita? ColoredMNIST, VLCS and TerraIngocnita are from 2019, 2013 and 2018 respectively! This raises the questions whether the proposed solutions will scale to larger and difficult datasets."
                },
                "questions": {
                    "value": "Try on DomainNet, Office Home and PACS datasets. Although these datasets are from 2019 and 2017, atleast they are more challenging datasets."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2080/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698794183074,
            "cdate": 1698794183074,
            "tmdate": 1699636139821,
            "mdate": 1699636139821,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4kgsZaGRfe",
                "forum": "6J3ehSUrMU",
                "replyto": "NPBZ3F3ko3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2080/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2080/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for reviewing our manuscript and we appreciate your feedback for refining the final version. We address your concerns as follows. \n\n*\u201dI loved the paper till I came to the experiments section. In this day and age, should we still be doing experiments with ColoredMNIST, VLCS, CIFAR-10 and TerraIncognita? ColoredMNIST, VLCS and TerraIngocnita are from 2019, 2013 and 2018 respectively! This raises the questions whether the proposed solutions will scale to larger and difficult datasets.\u201d*\n\n*  We thank the reviewer for this constructive suggestion to make our experimental analysis more comprehensive. **We conducted experiments on the PACS, Office-Home, and DomainNet datasets. The results are detailed in the updated version of our paper (appendix C.3); and are summarized as follows.** \n\n    * We see **the proposed methods outperform other Domain Generalization (DG) methods with significant margins** on PACS, Office-Home and DomainNet.\n    * **Despite its simplicity and efficiency, FedGP consistently performs well across most of the cases.**\n    * We found that **the auto-weighting versions of FedDA and FedGP generally outperform their fixed-weight counterparts.** An interesting exception is observed with FedDA on the OfficeHome dataset, where the fixed weight choice of $\\beta=0.5$ is surprisingly good. We observe that on Office-Home potentially the source-only baseline sometimes surpasses the Oracle performance on the target domain. Thus, we conjecture that the fixed weight $\\beta=0.5$ happens to be a good choice for FedDA on Office-Home, while the noisy target domain data interferes the auto-weighting mechanism. Nevertheless, the auto-weighted FedGP still shows improvement over its fixed-weight version.\n\n\n* **PACS (with 15% of target domain data)**\n\n| Domains    | A        | C        | P        | S        | Avg      |\n|------------|----------|----------|----------|----------|----------|\n| FedDA      |     92.6 |     89.1 |     97.4 |     89.2 | 92.0     |\n| FedGP      | **94.4** |     92.2 | **97.6** | **88.9** | 93.3     |\n| FedDA_Auto |     94.2 |     90.9 |     96.6 |     89.6 | 92.8     |\n| FedGP_Auto |     94.2 | **93.7** |     97.3 |     88.3 | **93.4** |\n| Best DG    |   87.8   |   81.8   |   97.4   |   82.1   |   87.2   |\n\n\n* **Office-Home (with 15% of target domain data)**\n\n| Domains     | A        | C        | P        | R        | Avg      |\n|-------------|----------|----------|----------|----------|----------|\n| Source Only | 50.9     | 66.1     | 74.5     | 76.2     | 66.9     |\n| FedDA       | **67.9** | **68.2** | **82.6** | **78.6** | **74.3** |\n| FedGP       |     63.8 |     65.7 |     81.0 |     74.4 | 71.2     |\n| FedDA_Auto  |     67.4 |     65.6 |     82.2 |     75.8 | 72.7     |\n| FedGP_Auto  |     66.1 |     64.5 |     82.1 |     74.9 | 71.9     |\n| Oracle      |     70.9 |     58.5 |     87.4 |     75.0 | 73.0     |\n| Best DG     |     64.5 |     54.8 |     76.6 |     78.1 | 68.5     |\n\n\n* **DomainNet (with 15% of target domain data)**\n\n|                   | clip     | info     | paint    | quick    | real     | sketch   | avg      |\n|-------------------|----------|----------|----------|----------|----------|----------|----------|\n| KD3A (ResNet -50) | 63.7     | 15.4     | 53.5     | 11.5     | 65.4     | 53.4     | 43.8     |\n| FedDA             | **67.1** | 26.7     | 56.1     | 33.8     | 67.1     | **55.7** | 51.1     |\n| FedGP             | 64.0     | 26.6     | **56.8** | **51.1** | **71.3** | 52.3     | **53.7** |\n| FedDA_auto        | 62.0     | **27.8** | 56.7     | 50.9     | 68.1     | 53.3     | 53.1     |\n| FedGP_auto        | 62.2     | 27.7     | **56.8** | 50.7     | 68.4     | 53.5     | 53.2     |\n|  Best DG          | 59.2     | 19.9     | 47.4     | 14.0     | 59.8     | 50.4     |  41.8    |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2080/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640857045,
                "cdate": 1700640857045,
                "tmdate": 1700641071393,
                "mdate": 1700641071393,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]