[
    {
        "title": "Causally Aligned Curriculum Learning"
    },
    {
        "review": {
            "id": "h8ybid6q86",
            "forum": "hp4yOjhwTs",
            "replyto": "hp4yOjhwTs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6200/Reviewer_1poU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6200/Reviewer_1poU"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a curriculum design method that produces causally aligned curriculums. Given a causal graph, a curriculum generator, and a target task, the proposed method creates a causally aligned curriculum using two main ideas. First, the FindMaxEdit algorithm, which constructs a set of causally aligned source tasks based on the modifying the editable variables of the target task. Importantly, optimal decision rules can be transported across causally aligned source tasks. Second, the authors propose ordering causally aligned source tasks with an expansion criterion on the optimal decision rules (i.e. tasks should be ordered such that the set of optimal decision rules expands with each task)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "i. The method is well-founded and rigorously specified. Further, the main idea of this paper - using knowledge of causal relationships to improve curriculum generation methods - is clearly useful to the community. \nii. The paper is clear, with all details necessary to reproduce the work present in the paper. \niii. The experimental results on Colored Sokoban and Button Maze are strong."
                },
                "weaknesses": {
                    "value": "Most of my concerns + comments are related to the motivation + presentation of the paper. \n\n- Motivation: Example 1's motivation for the paper is very contrived. The only reason a curriculum designer would design such a source task (where the box color is fixed to yellow) is if the curriculum designer was completely unaware of the causal dependencies of the reward. While this is possible in the case of causally-unaware automated curriculum generation methods, I find it a bit unlikely unless the crucial confounder variable U_i was either excluded from the state space (which would violate the usual Markovian reward assumption) or it is included in teh state space, but the agent cannot observe it (a partial observability assumption).  This leads to my questions: \n\t1)  Can the authors experimentally verify how often existing non-causal curriculum generation methods generate causally unaligned tasks? \n\t2) Can the authors comment on the expected benefit of their method in a domain with Markovian reward, or a fully observable domain w/no unobserved confounders? \n\n-  Acknowledging limitations and situating their work in presence of related work \n\t1) I saw that the related works section was relegated to the Appendix. There should at least be a pointer in the main paper to the related works. I found it helpful to read it, to understanding the positioning of this work, and think that it should be included in the main paper. The paper provides almost too many details about the method in the main paper, and at places is rigorous to the point of being pedantic. I think that the space would be better spent on situating the work properly. \n\t2) The inputs/outputs of the authors' method is not presented in plain language. As I understand it, the authors' method assumes access to the SCM of the task, and a curriculum generator such as GoalGAN. Such details should be made much clearer, perhaps through a summary methods figure or by adding a discussion of limitations. \n\n- Figures are not standalone: it is common practice to read a paper by skimming the figures + captions first, but I found that it was not possible to get the main idea of the figure, method and experimental results by doing so. Even after reading most of the accompanying text, I still found the figures + captions alone ambiguous. I had to find the specific part of the text referencing the figures -- sometimes needing to jump to other parts of the paper-- and read very carefully to understand what was happening. The authors should rewrite the figure captions and perhaps add a new \"methods\" figure summarizing the flow of their method. My specific comments on two figures are below, but all figures can be improved: \n\t1) Figure 5: Specifically, I was confused about why the curves labelled \"causal\" performed differently across the four columns? Also, the names of \"causal\" vs \"original\" were confusing. I think this is because the fact that the proposed method augments existing curriculum generation methods was presented only in a single sentence towards the end of the intro, and the knowledge was assumed in the rest of the paper. Seeing as this information is crucial to understand the paper, perhaps the authors can add this crucial piece \n\t2) Figure 3: It's confusing that figure (a) and (b) are identical except for the edit indicators. I needed to visually trace all arrows of both figures to verify this. The presence/absence of edit indicators should be explained in the caption, and the figure can be modified so that it is much quicker for the reader to notice that the only addition is the edit indicators (perhaps through the use of opacity / shading)."
                },
                "questions": {
                    "value": "See the weaknesses section for the most substantial questions and comments. \n\nMinor clarity comments are below: \n- Exogenous vs endogenous should be defined at some point in the preliminaries section on SCMs. \n- \"che\" \"de\" \"an\" meanings were not immediately clear to me. \n- In Def 1, the SCM M* is labelled with a *, yet this notation does not appear elsewhere in the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6200/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6200/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6200/Reviewer_1poU"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6200/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698090376924,
            "cdate": 1698090376924,
            "tmdate": 1699636675396,
            "mdate": 1699636675396,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tsXt5iwW3Z",
                "forum": "hp4yOjhwTs",
                "replyto": "h8ybid6q86",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6200/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6200/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer 1poU, thank you for your detailed suggestions and constructive feedback. Our work resides in a multidisciplinary field, and it is challenging to motivate the work in a way that is amenable to both sides, reinforcement learning and causal inference researchers. Your insighful comments will greatly assist in this endeavor. We will try incorporate your feedback and the subsequent discussion into our paper. \n\n>#### Weakness 1:\n>\u201cMotivation: Example 1's motivation for the paper is very contrived. The only reason a curriculum designer would design such a source task (where the box color is fixed to yellow) is if the curriculum designer was completely unaware of the causal dependencies of the reward. While this is possible in the case of causally-unaware automated curriculum generation methods, I find it a bit unlikely unless the crucial confounder variable U_i was either excluded from the state space (which would violate the usual Markovian reward assumption) or it is included in the state space, but the agent cannot observe it (a partial observability assumption). This leads to my questions:\n>1.  Can the authors experimentally verify how often existing non-causal curriculum generation methods generate causally unaligned tasks?\n>2.  Can the authors comment on the expected benefit of their method in a domain with Markovian reward or a fully observable domain w/o unobserved confounders?\u201d\n    \nRecall that in the definition of SCMs, each variable $V$\u2019s value is determined by a function of $f_V(\\operatorname{pa}(V), \\boldsymbol{U}_V)$ where $\\operatorname{pa}(V)$ are the values of parents of $V$ and $\\boldsymbol{U}_V$ are exogenous variables. The inherent randomness of the system is due to the existence of exogenous variables. When an exogenous variable affects more than one endogenous variable, it is called a confounder\\, which is unobservable to the agent and the curriculum designer. This holds for both automatic curriculum generation and manual curriculum designing. While it is a common practice to assume Markovian rewards or fully observable state space, as we stated in the introduction, our work investigates a more challenging scenario where the underlying environment contains unobserved confounders. For instance, in example 1 (the Colored Sokoban), one unobserved confounder is $U_i$, which is affecting both the box color $C_i$ and the reward $Y_i$. The randomness of both $C_i, Y_i$ are determined by $U_i$. \n  \n1.1 Please see the following chart for the percentage of misaligned source tasks generated by the baseline methods we tested in the both environments during training (300K environment steps). We can see that the existing non-causal curriculum generators are indeed generating a large portion of misaligned source tasks during the training even though they have the option to choose aligned source tasks. This indicates that the existing non-causal curriculum generators cannot effectively avoid misaligned source tasks and thus they are detrimental to the agent training when applied to environments with unobserved confounders.\n\n\n|Environments/Curriculum Generators |ALP-GMM|PLR|Currot|Goal-GAN\n|--|--|--|--|--|\n|**Colored Sokoban**|68.10%$\\pm$0.71%|69.10%$\\pm$2.77%|68.36%$\\pm$1.06%|66.41%$\\pm$1.84%|\n|**Button Maze**    |88.41%$\\pm$0.43%|87.33%$\\pm$3.77%|88.62%$\\pm$1.00%|90.30%$\\pm$0.27%|\n\n1.2 Even without confounders, if the state space is partially observable, the same situation as in the Colored Sokoban could happen. When the Markovian reward assumption holds, where the agent can observe all the parents of the reward variable, the reward cannot be confounded with any other variables. Under this stronger assumption, as our Theorem 1 indicates, all state variables are editable. On the other hand, we are dedicated to solving the curriculum generation problem in the presence of unobserved confounders. Thus, the setting of Markovian rewards actually falls into the traditional curriculum reinforcement learning problem studied in the literature, which our work is trying to relax."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6200/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534766266,
                "cdate": 1700534766266,
                "tmdate": 1700534766266,
                "mdate": 1700534766266,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KRVttPoVfa",
                "forum": "hp4yOjhwTs",
                "replyto": "MCSGZAz27W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6200/Reviewer_1poU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6200/Reviewer_1poU"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' replies! I appreciated them and found that the modifications improved my understandinig of the paper. Since my opinion that the paper should be accepted remains unchanged, I will not change my score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6200/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700609565711,
                "cdate": 1700609565711,
                "tmdate": 1700609565711,
                "mdate": 1700609565711,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LQS6NlrHVK",
            "forum": "hp4yOjhwTs",
            "replyto": "hp4yOjhwTs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6200/Reviewer_eG2G"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6200/Reviewer_eG2G"
            ],
            "content": {
                "summary": {
                    "value": "The paper tackled the problem of curriculum design in multi-task Reinforcement Learning. A challenge in the previous curriculum learning literature is that they all assume that the optimal decision rules are shared across different tasks. From a causal point of view, this expectation may not hold when the underlying environment contains unobserved confounders. To tackle this issue, the paper proposed a causal framework based on structural causal models and rigorously defined the notion of aligned tasks. The paper then proposed an algorithm that only generates aligned tasks. Simulation studies on Maze and Sokoban environments showed the advantage of the proposed algorithms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper finds a significant issue in the current literature of automatic curriculum learning and provides a clear framework to discuss the issue of non-alignment. Thus, the paper has great significance.\n\nThe paper is well-written and all the important messages are clearly delivered."
                },
                "weaknesses": {
                    "value": "1. The authors mentioned that curriculum learning could overcome the curse of dimensionality. However, all the examples discussed in this paper are tabular. There seems to be a mismatch. It would be interesting to discuss and experiment on an example with exponentially large state and action space.\n\n2. What is the computation-complexity of FindCausalCurriculum? Does it scale with the size of state and action space? If that is the case, it seems to be against the motivation of applying curriculum learning.\n\n3. Comments on experiments: In general, I believe the environment tested in this paper are rather naive.\n\nThe authors implement ALP-GMM by fixing the color. I don't think this is a good way to construct baseline. One can run ALP-GMM as a task sampler on a fixed set of tasks. ALP-GMM essentially assigns a probability for different tasks at the each round. If one task has the underlying U that is not aligned with the target task, ALP-GMM will adaptively reduce the weight for that task. \n\nI understand that fixing color matches the general story of the whole paper. However, I don't think this is a natural choice in examples where we have access to a simulator. It would be interesting to demonstrate a non-trivial example where C is a proxy of the underlying U and choosing C by running ALP-GMM could be harmful. \n\n4. An important way to improve the paper is to propose an algorithm for large (or continuous) state and action space. The current version seems to only work with tabular MDPs, which limits the complexity of environments the algorithm can be applied to."
                },
                "questions": {
                    "value": "1. I was a bit confused by the wordings in abstract. The authors first state that invariant\noptimal decision rules does not necessarily hold, then they propose condition characterizing causally aligned source tasks, i.e., the invariance of optimal decision rules holds. The flow does not seem right here.\n\n2. Need extra clarifications on Figure 1 and 2. What does it mean by fixing the color? When the different algorithms are trained, are they only allowed to choose tasks from misaligned source tasks? Should this example be too artificial? It would be interesting to see the results when ALP-GMM can generate tasks with unknown color, while the algorithm may reuse these tasks. In this way, it is possible for ALP-GMM to discover that certain tasks can be more helpful for learning the target task. This is in fact more aligned with the situations in the real-world."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6200/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698630944436,
            "cdate": 1698630944436,
            "tmdate": 1699636675282,
            "mdate": 1699636675282,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PED9vkYlaO",
                "forum": "hp4yOjhwTs",
                "replyto": "LQS6NlrHVK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6200/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6200/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer eG2G, thank you for your detailed review. We appreciate your acknowledgment of the significance of our work and have addressed each of your concerns in the following responses.\n\n>#### Weakness 1:\n>\u201cThe authors mentioned that curriculum learning could overcome the curse of dimensionality. However, all the examples discussed in this paper are tabular. There seems to be a mismatch. It would be interesting to discuss and experiment on an example with exponentially large state and action space.\u201d\n\nWe agree that the examples are in the grid world, but they are only for illustration purposes and don\u2019t indicate any limitations of our proposed method. For the ease of understanding, we delibrately choose such simple domains to better demonstrate our ideas and the implications of unobserved counfounders in the environment. One of the initial observations in these settings is that applying standard non-causal curriculum generators could be detrimental. Given that the environment is simple, we were able to explain the subtle interplay between curriculum generation, decision-making and unobserved confounding. \n\nHaving said, we note that our technical results (e.g., theorems, algorithms) are non-parametric, which means that they are immediately applicable to high-dimensional, continuous domains given merely qualitative causal knowledge about the domain. Once the proposed augmentation procedure is executed, we can leverage the computational power of existing curriculum generation methods to address those more complex domains. We also note that the observation spaces in our experiments are all in raw pixels instead of low-dimensional state vectors, which are high-dimensional. From the performance of agents trained directly in the target task (figure 5, horizontal green line), we can see that without causal curriculum learning, those agents cannot converge to the optimal in the target task at all, which suggests that our method helps with the learning. \n\nFinally, and based on the feedback provided, we further conducted an extra experiment with continuous actions and states to demonstrate the effectiveness of our methods in exponentially large state and action space empirically. Please see the common response to all the reviewers for the detailed results. \n\n>#### Weakness 2:\n>\u201cWhat is the computation-complexity of FindCausalCurriculum? Does it scale with the size of state and action space? If that is the case, it seems to be against the motivation of applying curriculum learning.\u201d\n\nThe worst-case time complexity of FindCausalCurriculum is $\\mathcal{O}(Hdn^2)$ where $H$ is the length of the horizon, $d$ is the number of actions, $n$ is the total number of nodes in the causal diagram. In an improved version of FindCausalCurriculum, which we show in the appendix (Algo 6 Causal Curriculum Learning), the worst-case time complexity is reduced to $\\mathcal{O}(Hn^2)$. Since there is no free lunch, given the performance boost and this relatively slow scaling overhead w.r.t the state/action size, applying causal curriculum learning is still preferable. More specifically, as shown in Figure 5, without curriculum learning, the agent trained directly in the target task cannot converge to the optimal at all. Even with curriculum learning, failing to utilize causal knowledge results in worse performance."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6200/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534440400,
                "cdate": 1700534440400,
                "tmdate": 1700534500813,
                "mdate": 1700534500813,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4cUSTdDVCO",
            "forum": "hp4yOjhwTs",
            "replyto": "hp4yOjhwTs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6200/Reviewer_StU4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6200/Reviewer_StU4"
            ],
            "content": {
                "summary": {
                    "value": "Curriculum reinforcement learning is to train the agent on a sequence of simpler, related tasks in order to gradually progress the learning towards a difficult, often sparse-reward target task. The hope is that if there are common optimal decisions among these simpler intermediate tasks and the target task, the agent will learn transferrable skills during the process and accelerate the learning process. However, this assumption may not hold if there are unobserved confounders in the environment. This paper delves into this problem from a causal perspective, defining conditions under which the optimal decisions remain consistent. It also introduces a method to create a curriculum that aligns causally. The method is tested in two grid-world environments with unseen confounders to validate the effectiveness of the proposed algorithms."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The structure and flow of the paper are easy to follow. Several simplified examples are provided to support the argument and illustration. To my knowledge, the perspective of confounders in curriculum reinforcement learning is novel."
                },
                "weaknesses": {
                    "value": "* Toyish experiments. The experiments conducted are limited to grid-world environments, which significantly narrows the scope of application. For the conclusions to be generalized, the experiments should ideally be extended to a more diverse set of environments.\n* Scalability. The paper does not adequately address how the proposed method scales to continuous environment variables. This is a significant concern as many practical applications in continuous or high-dimensional environment variable space. By not tackling this challenge, the authors leave a gap in understanding the full potential and limitations of their method.\n* Missing related work:\n    * Hu, Xing, et al. \"Causality-driven Hierarchical Structure Discovery for Reinforcement Learning.\" Advances in Neural Information Processing Systems 35 (2022): 20064-20076.\n    * Cho, Daesol, Seungjae Lee, and H. Jin Kim. \"Outcome-directed Reinforcement Learning by Uncertainty & Temporal Distance-Aware Curriculum Goal Generation.\" arXiv preprint arXiv:2301.11741 (2023).\n    * Huang, Peide, et al. \"Curriculum reinforcement learning using optimal transport via gradual domain adaptation.\" Advances in Neural Information Processing Systems 35 (2022): 10656-10670."
                },
                "questions": {
                    "value": "* In Algorithm 3, how is the actions sequence $\\{X_1, \\ldots, X_H\\}$ obtained in the first place? If we have this optimal action sequence before the learning starts, why do we need RL?\n* In Theorem 3, one of the conditions is that: For every $j=1, \\ldots, N-1$, actions $\\boldsymbol{X}^{(j)} \\subseteq \\boldsymbol{X}^{(j+1)}$. Does this mean that the transition must be deterministic and there only exists one unique optimal solution?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6200/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698775840017,
            "cdate": 1698775840017,
            "tmdate": 1699636675148,
            "mdate": 1699636675148,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JclUVwXyo5",
                "forum": "hp4yOjhwTs",
                "replyto": "4cUSTdDVCO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6200/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6200/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer StU4, thank you for your comments and feedback. We believe that a few misreadings of our work made some of the evaluations overly harsh and would like to ask you to reconsider the proposed paper in light of the clarifications provided below.\n\n> ####  Weakness 1&2:\n> \u201cToyish experiments. The experiments conducted are limited to grid-world environments, which significantly narrows the scope of application. For the conclusions to be generalized, the experiments should ideally be extended to a more diverse set of environments.\n> \u201cScalability. The paper does not adequately address how the proposed method scales to continuous environment variables. This is a significant concern as many practical applications in continuous or high-dimensional environment variable space. By not tackling this challenge, the authors leave a gap in understanding the full potential and limitations of their method.\u201d\n\nThis paper provides an augmentation procedure that allows existing curriculum generation algorithms to handle  environments with unobserved confounders. This augmentation procedure (Theorems 1-3) is non-parametric, relying on qualitative knowledge about the underlying environment. Once this augmentation procedure is executed, we can leverage the computational power of existing curriculum generation methods to address more complex domains. Consequently, our proposed method is generalizable to other, more diverse, high-dimensional/continuous environments.   \n\nAfter all, we are aware that scaling to high-dimensional domains is an important challenge. However, this paper investigates an orthogonal challenge that arises due to the existence of unobserved confounding bias, which we believe is also important in the design of curriculum generators. The proposed approach does not contradict but complements existing curriculum learning methods, by generalizing them to a more diverse set of environments where confounding bias may be present.  (Example 1 illustrates how current curriculum generators could fail when applied to environments with unobserved confounding, which motivated this work.) \n\nAlso, we would also like to note that our experiments use raw pixels as the observational space, which is high-dimensional, as stated in the experimental section and the conclusions. Still, following the reviewers\u2019 concerns, we included an additional experiment for continuous state/action variables to illustrate the effectiveness of the proposed method empirically. Further details can be found in the common response to all the reviewers.\n\n>#### Weakness 3:\n>\u201cMissing related work:\n>-   Hu, Xing, et al. \"Causality-driven Hierarchical Structure Discovery for Reinforcement Learning.\" Advances in Neural Information Processing Systems 35 (2022): 20064-20076. \n>-   Cho, Daesol, Seungjae Lee, and H. Jin Kim. \"Outcome-directed Reinforcement Learning by Uncertainty & Temporal Distance-Aware Curriculum Goal Generation.\" arXiv preprint arXiv:2301.11741 (2023).  \n>-   Huang, Peide, et al. \"Curriculum reinforcement learning using optimal transport via gradual domain adaptation.\" Advances in Neural Information Processing Systems 35 (2022): 10656-10670.\u201d\n    \nThank you for the suggested references; we have now incorporated them into the related work section. After reading them, we would like to contextualize their relevance to our work. In particular, Paper 1 focuses on causal discovery learning the underlying data generating mechanism directly. In the future, we may explore incorporating such methods to learn causal diagrams from data, rather than assuming them as an input. Papers 2 and 3, which are both curriculum generators like those we have tested, align with our theorem. They can be augmented to be robust to, and function in the presence of unobserved confounders. Overall, these references provide interesting perspectives that  complement our work, which develops the foundations for curriculum generation in more generalized settings where unobserved confounders are present."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6200/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534380630,
                "cdate": 1700534380630,
                "tmdate": 1700534380630,
                "mdate": 1700534380630,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bfEzo3VzGP",
            "forum": "hp4yOjhwTs",
            "replyto": "hp4yOjhwTs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6200/Reviewer_C4uc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6200/Reviewer_C4uc"
            ],
            "content": {
                "summary": {
                    "value": "The paper delves into the challenges of designing a curriculum of source tasks to tackle a complex target task in the presence of unobservable confounding variables within the environment. The authors leverage the structural causal model framework (Pearl, 2009) to define causally aligned source tasks for a given target task. They propose a causally aligned curriculum that incorporates qualitative causal knowledge of the target environment, and they validate their approach through experiments in two confounded environments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper addresses an important problem in curriculum reinforcement learning, highlighting the potential negative impact of inadequate task space design on target task performance. \n\nThe writing quality is commendable, even though the paper features heavy notation due to its nature. The repeated use of the Sokoban game aids comprehension and clarity regarding the paper's contributions and claims."
                },
                "weaknesses": {
                    "value": "The strategy for avoiding misaligned source tasks, as proposed in this work, relies on causal knowledge about the underlying data-generating mechanisms within the environment. This requirement limits the broader applicability of the strategy, making it dependent on domain-specific knowledge. The practicality of obtaining a causal diagram G for a general target task remains uncertain. \n\nIt remains unclear whether the proposed causal curriculum strategy can be extended to domains with continuous state and action spaces."
                },
                "questions": {
                    "value": "Re. the Colored Sokoban example:\n\nIn the baseline, the context/task space is restricted to tasks where the box color remains constant throughout the game. However, considering a more extensive task space that encompasses all possible combinations of box colors might lead the state-of-the-art curriculum strategies to automatically select relevant/aligned tasks during training. In contrast, the proposed causal curriculum limits the task space to initial agent and box positions, with the environment determining box colors based on intrinsic randomness. \n\nFormally, let us denote the initial positions of the agent and the box as $(a_0^x, a_0^y)$ and $(b_0^x, b_0^y)$, respectively. Furthermore, let $c_t$ represent the color of the box at time step $t$, and designate $H$ as the maximum number of steps allowable in the game. In the results presented, the baseline methodology confines the context/task space to instances of the form $[a_0^x, a_0^y, b_0^x, b_0^y, c_0 = c_1 = \\cdots = c_H]$. The extensive task space contains all possible task configurations denoted as $[a_0^x, a_0^y, b_0^x, b_0^y, c_0, c_1, \\dots, c_H]$. The proposed causal curriculum imposes a task space restriction, characterized by tasks of the form $[a_0^x, a_0^y, b_0^x, b_0^y]$, delegating the selection of $[c_1, c_2, \\dots, c_H]$ to the environment, predicated upon intrinsic randomness. \n\nPlease provide your insights on this aspect."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6200/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699140909406,
            "cdate": 1699140909406,
            "tmdate": 1699636675042,
            "mdate": 1699636675042,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8n3dW9buHf",
                "forum": "hp4yOjhwTs",
                "replyto": "bfEzo3VzGP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6200/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6200/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer C4uc, \nWe thank you for your thoughtful feedback. We will address your comments and questions in detail in the following.\n\n>#### Weaknesses 1\n> \u201cThe strategy for avoiding misaligned source tasks, as proposed in this work, relies on causal knowledge about the underlying data-generating mechanisms within the environment. This requirement limits the broader applicability of the strategy, making it dependent on domain-specific knowledge. The practicality of obtaining a causal diagram G for a general target task remains uncertain.\u201d\n\nWe appreciate the opportunity to clarifying this issue. First, we note that the knowledge our paper relies on, the causal diagram, is not fundamentally different from the assumption of MDP or POMDP. Formally, when assuming the underlying environment is an MDP or POMDP, a causal diagram is implicitly assumed. For example, if the environment is assumed to be MDP, at each time step there will be three nodes, state $S_t$, action $X_t$ and reward $Y_t$ and we will only have $S_t, X_t$ pointing to $S_{t+1}$. \nCurriculum learning methods already assume access to the domain-specific knowledge of the underlying system dynamics and abilities to manipulate them. Since a causal diagram is well defined from the underlying data-generating mechanisms, one could construct a causal diagram of the environment following the constructive procedure [1, Def. 27.10]. For example, in the Colored Sokoban example, as the curriculum designer, we know that the agent can observe the box color, change the box and itself\u2019s location, each object\u2019s current location, the grid world dynamics and that we might need to push a specific colored box to the location to get rewarded. Given such a level of information, a causal diagram of figure 3 is straightforward to construct. In other words, a causal diagram is not necessarily an additional assumption but an abstraction of the target task dynamics, and thus, a by-product of the assumptions of curriculum learning. \n\n> #### Weakness 2\n> \u201cIt remains unclear whether the proposed causal curriculum strategy can be extended to domains with continuous state and action spaces.\u201d\n\nThere might be some misreadings on our results and we take this as an opportunity to clarify this point. We first note that the results in the paper (Theorems 1-3) are non-parametric, in the sense that they do not rely on any parametric assumptions of the underlying variable domains and structural causal model. This means that these results are readily applicable to state variables and action variables over continuous domains. To illustrate this point empirically, we conducted extra experiments in a continuous domain to demonstrate the effectiveness of our methods. Please see the common response for the experiment result."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6200/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534187876,
                "cdate": 1700534187876,
                "tmdate": 1700534258187,
                "mdate": 1700534258187,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]