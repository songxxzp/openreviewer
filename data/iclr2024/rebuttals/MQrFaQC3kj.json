[
    {
        "title": "Dataset Fairness: Achievable Fairness On Your Data With Utility Guarantees"
    },
    {
        "review": {
            "id": "hky7wqHn6F",
            "forum": "MQrFaQC3kj",
            "replyto": "MQrFaQC3kj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5969/Reviewer_PBf8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5969/Reviewer_PBf8"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method to estimate the accuracy-fairness trade-off curve given the dataset and model class. It first uses an existing method, You-Only-Train-Once (YOTO), to get the trade-off curve efficiently without training multiple models. It then proposes a way to obtain the confidence intervals."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The accuracy-fairness curve is widely used in the algorithmic fairness community. How to set a range of achievable fairness violations is a practical problem. The method proposed in this paper provides us with an efficient solution for estimating the curve with two-sided confidence intervals."
                },
                "weaknesses": {
                    "value": "1. The method is highly based on an existing method, YOTO. Although it doesn't need to train multiple models, the cost of YOTO is not discussed in this paper.\n2. The accuracy-fairness curve is algorithm-agnostic. Figure 4 shows that the estimated curve has more errors when using some particular algorithmic fairness methods. However, people tend to use some algorithmic fairness methods to train the model. In that case, the practical use of the curve is concerned. \n3. The method requires an in-distribution calibration dataset and only applies to in-distribution tests."
                },
                "questions": {
                    "value": "1. What is the L_fair in equation (2)?\n2. What are the costs and limitations of YOTO?\n3. Can we extend the method to be algorithm-sensitive? For example, suppose we know what algorithmic fairness method to use, can we estimate the curve for that algorithm?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5969/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699223771578,
            "cdate": 1699223771578,
            "tmdate": 1699636637699,
            "mdate": 1699636637699,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HCqaYiNxyK",
                "forum": "MQrFaQC3kj",
                "replyto": "hky7wqHn6F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5969/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5969/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their thoughtful comments on our work. Below we respond to the questions raised. \n\n> The method is highly based on an existing method, YOTO. Although it doesn't need to train multiple models, the cost of YOTO is not discussed in this paper.\n\nWe describe the computational cost of the YOTO model in extensive detail in Appendix E.1.1. including the computational resources used to train each YOTO model, the time required to train each YOTO model for each dataset, the model architecture for each dataset as well as the training details for the YOTO model. At a high-level, training one YOTO model takes roughly the same amount of time as taken by each of the separately trained models in our experiments. More specifically, on Adult and COMPAS datasets, training one YOTO model on a Tesla-V100-SXM2-32GB GPU takes roughly 5 minutes, whereas on the CelebA dataset training one YOTO model takes roughly 1.5 hours and on the Jigsaw dataset training one YOTO model takes roughly 6 hours.\n\n> The accuracy-fairness curve is algorithm-agnostic. Figure 4 shows that the estimated curve has more errors when using some particular algorithmic fairness methods. However, people tend to use some algorithmic fairness methods to train the model. In that case, the practical use of the curve is concerned.\n\n We agree with the reviewer that the accuracy-fairness curve is algorithm-agnostic, and since our intervals are designed to align with the optimal trade-off curve, they should serve as reference for any model belonging to the model class $\\mathcal{H}$ regardless of the algorithmic fairness method used to train the model. Note that in practice, any methodology whose trade-off lies above our upper confidence interval is likely to have a suboptimal trade-off, suggesting that alternative approaches may offer improved trade-offs.\n\n> What is the L_fair in equation (2)?\n\nWe provided all of the smooth relaxations $L_{\\text{fair}}$ considered in our work in Appendix E (pages 23 and 25). In our experiments, we considered a number of different fairness relaxations $L_{\\text{fair}}$ as considered in previous fairness literature [Bendekgey & Sudderth, 2021]. For example, given a model $h_\\theta : \\mathcal{X} \\rightarrow \\mathbb{R}$, the relaxations for demographic parity considered in our baselines are of the form:\n $$L_{\\text{fair}}(h_\\theta)\u00a0=\u00a0|\\mathbb{E}[g(h_\\theta(X))\\mid A = 1]\u00a0-\u00a0\\mathbb{E}[g(h_\\theta(X))\\mid A = 0]|$$\nWhere the functions $g:\\mathbb{R}\\rightarrow \\mathbb{R}$ considered in our experiments include $g(x) = x$ (denoted as 'linear' in our results), $g(x)=\\log{\\sigma(x)}$ where $\\sigma(x):= 1/(1+\\exp{-x})$ (used for the 'separate' baseline as well as YOTO models) and $g(x)=\\log{\\sigma(x)}$  (denoted as 'logsig' in our results). We will also include these details in the main text in the updated version of our paper.\n\nWe hope our clarifications have addressed the reviewer's concerns, and kindly ask them to consider increasing their score.\n\n[Bendekgey & Sudderth, 2021] Harry Bendekgey and Erik B. Sudderth. Scalable and stable surrogates for flexible classifiers with fairness constraints."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5969/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700219939485,
                "cdate": 1700219939485,
                "tmdate": 1700219939485,
                "mdate": 1700219939485,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WYqIl0m6Tr",
            "forum": "MQrFaQC3kj",
            "replyto": "MQrFaQC3kj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5969/Reviewer_H5sg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5969/Reviewer_H5sg"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a methodology to estimate the optimal fairness-accuracy trade-off curve for a given dataset and model class. The key ideas of the proposed method are two steps::\n\n\n1. Use the You-Only-Train-Once (YOTO) framework to estimate the trade-off curve by training a single model. \n2. Use the YOTO result to construct confidence intervals around the estimated trade-off curve using a held-out calibration dataset. \n\n\nThe claimed contributions of this paper are:\n1. develop a method to calculate a range of allowed fairness violations for a given dataset and desired model accuracy. \n2. Construct confidence intervals that statistically guarantee the optimal fairness-accuracy tradeoff curve.\n3. Test the proposed method when sensitive attributes are scarce in the data.\n4. Test the proposed method on different types of data. The intervals contained the tradeoff curves from state-of-the-art fairness methods like regularization and adversarial learning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The confidence intervals of fairness seem needed in the fairness domain., however, it has problems.\n2. The paper is clearly written and easy to follow. The graphics effectively illustrate the key ideas."
                },
                "weaknesses": {
                    "value": "1. The statement \"For a given dataset, model class, and accuracy, the permissible range of fairness violation is x to y.\" in this paper is problematic. Accuracy and fairness have an inherent connection (they exhibit trade-offs) and will influence each other. So accuracy cannot be the condition for the range of fairness violation.\n2. How to evaluate whether the confidence interval is rational or correct? This does not seem to have ground truth for this and only visually evaluating figure 4 is not enough. There is a strong assumption that the curve learned with YOTO is the ground truth, this is not reasonable and even wrong. This strong but possibly wrong assumption is only mentioned in \"In other words, under the assumption of large enough model capacity, training the loss-conditional YOTO model performs as well as the separately trained models while only requiring a single model.\" (Page 4). I strongly recommend treating this seriously.\n3. The changeable fairness-accuracy trade-offs using one model may incur ethical issues, such as generating biased outcomes for certain groups of people. Based on this, I think this paper needs further ethical review.\n4. The $\\mathcal{L}_{fair}$ in Eq (2) is not presented at all. This paper should present the smooth relaxation of demographic parity.\n5. What is the meaning of \"Dataset Fairness\" in the title? It seems this title is not suitable for the proposed method.\n6. The experimental evaluation is not convincing to me. Since the Adult data is super imbalanced and the COMPAS data is small. I would suggest adding more experiments to really evaluate the proposed method, such as on the folktable dataset at https://github.com/socialfoundations/folktables\n\n\nThis paper does not meet the standards for acceptance to ICLR in its current form. For now, I would recommend rejection."
                },
                "questions": {
                    "value": "Please address my concerns in the Weakness part.\n\n----\n----**After rebuttal**---\n\nI thank the authors for their response and am sorry for the late response.  The authors' response addresses part of my concerns. But \n1. The new explanation that \"the lower confidence intervals presented in Proposition 3.4 depend on the gap between the YOTO achieved trade-off curve and the ground-truth trade-off curve\" is essentially making a strong assumption that the YOTO should be good enough, although not group truth. I do not think this point is reasonable and grounded. \n2. The evaluation based on such small tabular data is not convincing to me, without new results presented. \n\nBased on the above and my original comments, I would maintain my original score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Discrimination / bias / fairness concerns"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The changeable fairness-accuracy trade-offs using one model may incur ethical issues, such as generating biased outcomes for certain groups of people. Based on this, I think this paper needs further ethical review."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5969/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5969/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5969/Reviewer_H5sg"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5969/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699257696025,
            "cdate": 1699257696025,
            "tmdate": 1701065820453,
            "mdate": 1701065820453,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LnAGXSdLRB",
                "forum": "MQrFaQC3kj",
                "replyto": "WYqIl0m6Tr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5969/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5969/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors - Part I"
                    },
                    "comment": {
                        "value": "We thank the reviewer for providing comments on our work. Below we address the questions raised and try to clarify some of the misunderstandings:\n\n> The statement \"For a given dataset, model class, and accuracy, the permissible range of fairness violation is x to y.\" in this paper is problematic. Accuracy and fairness have an inherent connection (they exhibit trade-offs) and will influence each other. So accuracy cannot be the condition for the range of fairness violation.\n\nWe consider model fairness conditional on model accuracy **precisely because** of the accuracy-fairness trade-off. This is also in line with the fairness-constrained accuracy maximisation formulation provided in the fairness literature [Equation (3) in Agarwal et al, 2018]. Since accuracy and fairness can be at odds with each other and exhibit trade-offs, we should not consider the fairness violation value of a model in isolation.   As an example, if we do not constrain the model accuracy,  the minimum demographic parity for any given model class $\\mathcal{H}$ will be 0 and will be attained by a model which makes a constant prediction regardless of the input. In this case, even though the minimum attainable demographic parity is 0, the model accuracy will likely be very low. Therefore, the goal when obtaining the optimal accuracy-fairness trade-off is to minimise the model fairness violation while constraining the model accuracy. We will further clarify this in the updated version of the paper.\n\n> How to evaluate whether the confidence interval is rational or correct? This does not seem to have ground truth for this and only visually evaluating figure 4 is not enough.\n\nRecall that the ground truth trade-off curve is defined as:\n\n$\\tau^\\ast_{\\text{fair}}(\\psi) \\coloneqq \\min_{h\\in \\mathcal{H}} \\Phi_{\\text{fair}}(h)$ subject to $\\text{acc} \\geq \\psi$.\n\nFor real-world datasets, this ground truth trade-off curve is intractable. This is because, in general, the constrained optimisation problem above is non-trivial to solve exactly. Furthermore, we only have access to a finite dataset making it impossible to obtain the exact trade-off curve in general. As a result of this, previous works in fairness literature only consider empirical accuracy-fairness trade-offs [Agarwal et al., 2018, Bendekgey & Sudderth, 2021, Zafar et al., 2015; 2017; 2019]. In this work, we construct the confidence intervals in an attempt to account for the approximation errors, and since the ground truth trade-offs are unknown, we empirically verify that the state-of-the-art methodologies lead to empirical trade-offs which are largely consistent with the obtained confidence intervals.\n\nApart from this, we have conducted additional experiments with synthetic data setups where the ground truth is known. The experimental results included in the Appendix E.5 of the updated manuscript show that the empirical trade-off achieved by YOTO aligns very well with the ground truth trade-off curve. Additionally, the ground truth trade-off curve is covered by all the confidence intervals constructed.\n\n> There is a strong assumption that the curve learned with YOTO is the ground truth, this is not reasonable and even wrong. \n\nWe wish to clarify that our work **does not** make the assumption that the curve learned with YOTO is the ground truth, and every theoretical result presented in our paper remains valid regardless of whether the YOTO trade-off curve is optimal or not. More specifically, as we mention in Section 3.2.1, the upper confidence intervals on $\\tau^\\ast_{\\text{fair}}$ do not depend on how close the YOTO trade-off curve is to the ground truth optimum curve. In contrast, the lower confidence intervals presented in Proposition 3.4 depend on the gap between the YOTO achieved trade-off curve and the ground-truth trade-off curve (denoted by $\\Delta(h)$). While this gap, $\\Delta(h)$, is an unknown quantity in general, we propose sensitivity analysis techniques to posit plausible values for $\\Delta(h)$ in practice in Appendix C.\n\n> This strong but possibly wrong assumption is only mentioned in \"In other words, under the assumption of large enough model capacity, training the loss-conditional YOTO model performs as well as the separately trained models while only requiring a single model.\" (Page 4). I strongly recommend treating this seriously.\n\nThe statement above does not refer to the optimality of the YOTO trade-off. Instead, the statement provides an intuitive, informal explanation of (Dosovitskiy & Djolonga, 2020, Proposition 1) and implies that under large enough model capacity, the trade-off curves obtained using YOTO should align with the trade-off curves obtained by training models separately (which may or may not be the same as optimal ground-truth trade-off curve). We will further clarify this in the updated version of our paper, by possibly including the formal result provided in (Dosovitskiy & Djolonga, 2020, Proposition 1) for completeness."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5969/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700219138252,
                "cdate": 1700219138252,
                "tmdate": 1700219138252,
                "mdate": 1700219138252,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6mOPqeAeoK",
            "forum": "MQrFaQC3kj",
            "replyto": "MQrFaQC3kj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5969/Reviewer_SKWL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5969/Reviewer_SKWL"
            ],
            "content": {
                "summary": {
                    "value": "Fairness-accuracy trade-off widely exists in machine learning models and fundamentally depends on dataset characteristics. Such dataset-dependent property impedes chasing universal fairness requirement across datasets. To this end, this paper proposes a computationally efficient approach to approximate the trade-off curve with statistical guarantees via adopting YOTO framework. The empirical results provide the guidelines in accuracy-constrained fairness decisions for various data modalities."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe research problem on the fairness-accuracy trade-off is fundamental and important in machine learning fairness community.\n2.\tThis paper is overall well-written and easy to follow.\n3.\tDue the unknown Pareto frontier of trade-off, the investigation of trade-off with confidence interval with statistical guarantee makes much sense to me."
                },
                "weaknesses": {
                    "value": "1.\tMotivation. Can the authors elaborate on the motivation for using a universe fairness requirement across datasets? What are the advantages of doing this?  \n2.\tTechnique novelty. This paper introduces a computationally efficient method for estimating the trade-off. However, from my understanding, the efficiency part directly adopts YOTO framework and the confidence interval estimation only involves trivial bounds.  \n3.\tPareto frontier. The achievable trade-off by YOTO may not be consistent with the ground-truth Pareto optimum. It seems that this paper is over-claimed since the true Pareto trade-off investigation is not touched. Additionally, how do you use a universe fairness requirement across datasets? The approximated trade-off seems not be a good choice since the gap between achievable trade-off by YOTO and ground-truth Pareto optimum may also be dataset-dependent. \n4.\tExperiments. (a) The evaluation of the confidence interval is vague. It seems that the conservative estimation is never penalized by the current results, such as Figures 3 and 4. Which confidence interval estimation method is better? (b) In Section 3.1, the author mentioned $\\lambda$ in Eq. (2) offers litter control over the accuracy, which is counter-intuitive for such regularization. Can you provide experimental results to further support this statement? (c) Is it possible to create a synthetic dataset with a known ground-truth trade-off in the experiments? Otherwise, many conclusions can only hold for the achievable trade-off by YOTO. (d) There are confidence intervals for both accuracy and fairness. How can you plot these two intervals in Figures 3 and 4? \n5.\tFrom my understanding, the optimization for fairness with accuracy and the optimization for accuracy with fairness constraint have the same trade-off. I am curiosu why the authors select the former one and highlight the difference in the first paragraph of section 2.1."
                },
                "questions": {
                    "value": "Please see weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5969/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699263020468,
            "cdate": 1699263020468,
            "tmdate": 1699636637479,
            "mdate": 1699636637479,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9Iq0yUdRNH",
                "forum": "MQrFaQC3kj",
                "replyto": "6mOPqeAeoK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5969/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5969/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors - Part I"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for taking the time to review our work. There seem to be some misunderstandings regarding our methodology which we seek to clarify below. \n\n> Motivation. Can the authors elaborate on the motivation for using a universe fairness requirement across datasets? What are the advantages of doing this?\n\nWe would like to emphasise that, in this work, we argue **against** the use of a universal fairness requirement across datasets because the accuracy-fairness trade-off is highly dependent on the characteristics of the dataset.  Therefore, previous works such as [Agarwal et al., 2018 ] which use a uniform fairness requirement (such as requiring the Demographic Parity to be $\\leq 0.1$ regardless of the dataset) may have varying implications for the achievable model accuracy across the different datasets. Our proposed methodology instead allows practitioners to quantify the complete accuracy-fairness tradeoff curve which may be used to obtain the range of permissible fairness violations at any given level of model accuracy chosen at inference time and does not require imposing any restrictions on model fairness or accuracy apriori at training time. We will further clarify this distinction in the next version of our paper.\n\n> Technique novelty. This paper introduces a computationally efficient method for estimating the trade-off. However, from my understanding, the efficiency part directly adopts YOTO framework and the confidence interval estimation only involves trivial bounds.\n\nThere are a number of important technically novel aspects regarding both, the construction of confidence intervals and the application of the YOTO framework in our methodology, which we believe will be of interest to the ML fairness community:  \n1. Firstly, we would like to emphasise that our method of constructing the confidence intervals on the trade-off curve $\\tau^\\ast_{\\text{fair}}$ does not simply rely on trivial bounds. While standard methods like Hoeffding's inequality can be used directly to obtain confidence intervals on model accuracy, the same is not true for fairness violation metrics like Demographic Parity, Equalised Odds and Equalized Opportunity which involve multiple (conditional) expectation terms. While a naive application of union bounds can be used to construct confidence intervals on these fairness violation metrics, these bounds are known to be highly conservative and are often non-informative. Therefore, in order to construct informative confidence intervals on fairness violations, we propose a subsampling procedure in Appendix B.2 which provides a novel methodology of constructing valid confidence intervals on fairness violation metrics by subsampling calibration data. We will include a discussion of this approach in the main text of our paper.\n2.  Secondly, our confidence intervals not only incorporate finite sample uncertainty from finite calibration data, but also account for the possible sub-optimality in the fairness trade-offs achieved by $h$. Specifically, to incorporate the latter, we formally consider the gap between the optimal accuracy-fairness trade-off $\\tau^\\ast_{\\text{fair}}$ and the trade-off achieved by a model $h$, denoted by $\\Delta(h)$ in Figure 2 in the main text. Part of our methodology for obtaining the lower confidence interval involves quantifying this gap $\\Delta(h)$. While this gap $\\Delta(h)$ is intractable in general, we propose a sensitivity analysis methodology to posit plausible value for $\\Delta(h)$ in practice (in Appendix C). In contrast to the previous works in fairness literature [Agarwal et al., 2018, Bendekgey & Sudderth, 2021, Zafar et al., 2015; 2017; 2019], to the best of our knowledge, our work is the first to consider uncertainty arising from both, finite datasets used to evaluate accuracy and fairness metrics as well as the possible sub-optimality in the fairness trade-offs achieved by $h$. \n3. Thirdly, we also propose a novel methodology for constructing informative confidence intervals in the setting where the sensitive attributes $A$ are accessible only for a small subset of the calibration dataset. This methodology effectively combines data with actual and predicted sensitive attributes to derive tighter and more accurate CIs, even when the majority of $A$ values are absent.\n4. Finally, while YOTO has previously been applied in areas like multi-task learning [Lin et al. 2020], our work provides a novel application of the YOTO framework to fairness literature and offers practitioners the capability to, at inference time, specify desired accuracy levels and promptly receive corresponding admissible fairness violation ranges."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5969/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700217869029,
                "cdate": 1700217869029,
                "tmdate": 1700217869029,
                "mdate": 1700217869029,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]