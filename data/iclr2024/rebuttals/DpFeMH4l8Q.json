[
    {
        "title": "Group Preference Optimization: Few-Shot Alignment of Large Language Models"
    },
    {
        "review": {
            "id": "HVeIfxGfRa",
            "forum": "DpFeMH4l8Q",
            "replyto": "DpFeMH4l8Q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8317/Reviewer_UCgS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8317/Reviewer_UCgS"
            ],
            "content": {
                "summary": {
                    "value": "The study introduces Group Preference Optimization (GPO), an innovative alignment framework designed to tailor large language models (LLMs) to specific group preferences with minimal data. GPO optimizes the model with reduced data, surpassing current methods like in-context steering and fine-tuning. Experiments on OpinionQA exhibit that GPO effectively adjusted the model's preferences on multiple-choice tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1.\tThe paper commendably focuses on the concept of \"Group Preference\", introducing an efficient approach for aligning large language models to specific groups. \n\n2.\tEfficient fine-tuning of a few-shot learning scenario, blending both in-context and fine-tuning methods, stands out. This approach offers a practical solution in settings where extensive labeled data might not be available.\n\n3.\tThe empirical evaluations seem robust and thorough. Not only do the results show clear improvements, but the detailed analysis and discussion also provide insightful opinions."
                },
                "weaknesses": {
                    "value": "1.\tLack of clarity in the presentation of the method. For instance, it remains ambiguous which specific parameters are subject to training. Incorporating a detailed algorithm diagram or flowchart would greatly enhance the comprehensibility. Besides, more training details would be better.\n\n2.\tThe absence of evaluation results concerning the generalization capability of the GPO method. As GPO optimizes the parameters, it's crucial to show whether the model retains performance on general benchmarks after tuning."
                },
                "questions": {
                    "value": "1.\tAs mentioned, it is ambiguous which part parameters are tunable. Could the authors further clarify the training details?\n\n2.\tCould the authors provide examples of generation tasks (QA) to better showcase the model's preference? Demonstrating the model's performance on responses could provide a clearer insight into its practical applications."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8317/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8317/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8317/Reviewer_UCgS"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8317/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698501008720,
            "cdate": 1698501008720,
            "tmdate": 1699637034319,
            "mdate": 1699637034319,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jqexlkuBJe",
                "forum": "DpFeMH4l8Q",
                "replyto": "HVeIfxGfRa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8317/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8317/Authors"
                ],
                "content": {
                    "title": {
                        "value": "response to reviewer UCgS"
                    },
                    "comment": {
                        "value": "Thank you for your valuable comments and detailed insights! We address your questions below.\n\n\n> Q1: Lack of clarity in the presentation of the method. For instance, it remains ambiguous which specific parameters are subject to training. Incorporating a detailed algorithm diagram or flowchart would greatly enhance the comprehensibility. Besides, more training details would be better.\n\nA1: Thanks for pointing out the ambiguity. We added an algorithm box in Appendix A of the revision to illustrate how GPO is trained. GPO trains a transformer module for in-context predicting unseen group preferences based on the few-shot examples provided. And we do not train the base LLM weights. Moreover, the training details and hyperparameters are given in the appendix section G. We hope this clarifies your question. If you have more questions, please let us know.\n\n> Q2: The absence of evaluation results concerning the generalization capability of the GPO method. As GPO optimizes the parameters, it's crucial to show whether the model retains performance on general benchmarks after tuning.\n\nA2: GPO does not alter the base LLM's parameters. Instead, it augments the LLM with an additional module trained to predict group preferences based on the few-shot examples provided. This approach allows the LLM to retain its broad language capabilities while aligning its outputs to specific group preferences\u200b\u200b in a few-shot manner.\n\n> Q3: Could the authors provide examples of generation tasks (QA) to better showcase the model's preference? Demonstrating the model's performance on responses could provide a clearer insight into its practical applications.\n\nA3: We agree such datasets could provide valuable insights into GPO's capabilities. However,\n* We want to highlight that GPO is not inherently restricted to multiple-choice datasets. For open-ended long-form QA, GPO could predict raw preference scores and guide the LLM to align with group distributions, without requiring the scores to sum to 1. \n\n* To the best of our knowledge, we are unaware of suitable labeled long-form QA group preference datasets for real world settings which are diverse, nuanced, and seperated for diverse demographic groups that would allow benchmarking. \n\n* Collecting these dataset would be time consuming and is outside of this work\u2019s scope. Additionally, evaluating the long-form generations would require human judgement which is expensive in our academic research setting. For example, gathering diverse demographic groups labellers to evaluate generated responses and aggregate their reponses as group preference distributions would be impractical for academic research.\n* That said, we have made our best efforts to utilize the most comprehensive opinion datasets available for our current evaluations on multiple-choice QA. We chose OpinionQA and GlobalOpinionQA, which is of high community interest given their growing adoption, with 60+ GitHub stars recently for the former and 300+ monthly downloads of the latter. Additionally, their survey QA format enables efficient, accurate analysis without costly human evaluation.\n\nIf you have suggestions on such long-form QA datasets, please let us know. We would not hesitate to test GPO on them."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700290720979,
                "cdate": 1700290720979,
                "tmdate": 1700290720979,
                "mdate": 1700290720979,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eJAlZxQEe4",
                "forum": "DpFeMH4l8Q",
                "replyto": "jqexlkuBJe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8317/Reviewer_UCgS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8317/Reviewer_UCgS"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you to the authors for the response! While better benchmarks may not currently exist, I believe the current evaluation datasets are somewhat limited, which might affect the broader impact of your work. I will maintain the current score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700525702972,
                "cdate": 1700525702972,
                "tmdate": 1700525702972,
                "mdate": 1700525702972,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EQ0IDT2BTA",
            "forum": "DpFeMH4l8Q",
            "replyto": "DpFeMH4l8Q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8317/Reviewer_DqhU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8317/Reviewer_DqhU"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a method to alignment LLM to group preferences so that a LLM grounded on several shots of preference examples can better predict the group preference for the unseen questions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The problem this work proposes to solve is kind of new and unique.\n2. The baselines are quite comprehensive and the proposed method significantly outperforms all of them, which validates the superiority of this method."
                },
                "weaknesses": {
                    "value": "1. I am not sure about the significance and broadness of the problem this work tries to solve. Alignment of LLMs to group preferences sounds important, however, the evaluation datasets used by this work look quite specific and narrow and I don't think it is of interest to a broad range of research community. \n2. The proposed method look quite trivial and standard, which is a in-context fine-tuning method. There are some changes in the method details but those details are a bit hard to understand, which I will pose some questions on next."
                },
                "questions": {
                    "value": "1. In the second last paragraph of page 4, it is said that \"In particular, we discard the positional encodings commonly found in standard transformer architectures\". Wouldn't the removal of positional encodings significantly deteriorate the performance of those pre-trained and fine-tuned LLMs?\n2. Still in the second last paragraph of page 4, it is said that \"we employ a masking strategy where the context pairs can self-attend to each other\". Could you elaborate such a masking strategy?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8317/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698813063695,
            "cdate": 1698813063695,
            "tmdate": 1699637034161,
            "mdate": 1699637034161,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1WzjruGWtE",
                "forum": "DpFeMH4l8Q",
                "replyto": "EQ0IDT2BTA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8317/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8317/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer DqhU"
                    },
                    "comment": {
                        "value": "Thank you for your valuable comments and detailed insights! We address your questions below.\n\n> Q1:I am not sure about the significance and broadness of the problem this work tries to solve. Alignment of LLMs to group preferences sounds important, however, the evaluation datasets used by this work look quite specific and narrow and I don't think it is of interest to a broad range of research community.\n\nA1:Thanks for acknowledging the importance of steering LLMs to group preferences. We think it is an important task as current popular alignment methods such as prompting, in-context steering, and SFT do not work well for it. \n\nRegarding the evaluation datasets:\n* We have exerted our best effort to utilize the most comprehensive and relevant opinion datasets currently available for assessing LLM opinion distributions. Additionally, there are currently no long-form QA datasets with labeled ratings collected for diverse demographics groups. We chose OpinionQA and Anthropic's GlobalOpinionQA for their growing use and recognition in the field. For instance, OpinionQA has garnered over 60 stars on github in about half a year. Similarly, GlobalOpinionQA has seen over 300 downloads in the past month, indicating high community interest in LLM opinion distribution studies.\n\n* Our selection was also larged guided by the practical constraints of academic research, where datasets that allow for easy verification and no costly human evaluation are preferred. This motivation aligns with OpinionQA (as stated in [1] section 2.1) and GlobalOpinionQA\u2019s motivations too, which states that the use of survey QA data enables capturing diverse and nuanced public interest topics and also enables efficient and accurate evaluation.\n\n* Additionally, in contrast to prior work that treats human alignment as a single quantity such as safety or helpfulness, our work conceptualizes human alignment as an inherently subjective measure that depends on who it is evaluated against; to the best of our knowledge, **we are among the first to align LLMs to a group preference distribution rather than a single preference or group consensus.**\n\n[1] Whose Opinions Do Language Models Reflect?  https://arxiv.org/abs/2303.17548 \n\n> Q2: The proposed method look quite trivial and standard, which is a in-context fine-tuning method. There are some changes in the method details but those details are a bit hard to understand, which I will pose some questions on next.\n\nA2: \n* We would like to clarify that GPO is not an in-context fine-tuning method, it does not alter the base LLM's parameters. Instead, it augments the LLM with an additional transformer module trained to predict unseen group preferences based on the few-shot examples provided. Although GPO does not modifying the weights of the LLM itself, it can guide the LLM to produce sample outputs that are attuned to the desired group preferences. This allows the LLM to retain its broad language capabilities while aligning its outputs to specific group preferences\u200b\u200b in a few-shot manner. \n* In section 3 (Page 6), the in-context finetuning method is actually one of our baselines that performs worse than GPO. \n\n>Q3: ... it is said that \"In particular, we discard the positional encodings commonly found in standard transformer architectures\". Wouldn't the removal of positional encodings significantly deteriorate the performance of those pre-trained and fine-tuned LLMs?\n\nA3: We discard positional encoding for GPO but not the base LLM. We do this to achieve the property of context invariant, which means the prediction of preference for new questions should not depend on the ordering of the context questions. In the appendix Table 3 we submitted, we compared GPO with a standard autoregressive transformer that employs standard causal mask and positional encodings. And GPO\u2019s inherent inductive biases yield superior alignment performance compared to a traditional transformer. \n\n> Q4: in the second last paragraph of page 4, it is said that \"we employ a masking strategy where the context pairs can self-attend to each other\". Could you elaborate such a masking strategy?\n\nA4: Thank you for pointing out the potential confusion in our writing. We will provide more details on this in the revision. The masking strategy is as follows: \n\nOur input to the transformer is a sequence of context pairs followed by padded target pairs as in figure 2. The mask allows:\n1) The context pairs attend to each other, rather than using a causal mask that restricts each token to only attend to previous tokens. We use this because the order of the context pairs should not influence predictions for target points - it should be permutation invariant.\n2) The padded target pairs to only attend to the context pairs, and not to other targets. This restriction aligns with the conditional independence assumption in Equation 2, ensuring each target pair is predicted solely based on the context.\n\n\nIf you have other questions, please let us know."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700288554648,
                "cdate": 1700288554648,
                "tmdate": 1700288584491,
                "mdate": 1700288584491,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2ebVbWZRrM",
                "forum": "DpFeMH4l8Q",
                "replyto": "EQ0IDT2BTA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8317/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8317/Authors"
                ],
                "content": {
                    "title": {
                        "value": "reminder"
                    },
                    "comment": {
                        "value": "Thank you again for your insightful feedback on our paper! We've carefully considered your comments and revised our paper to address the questions you raised. As the discussion phase is drawing to a close, we hope you've had a chance to review our response. Considering the time constraint, are there any other questions or concerns you'd like us to address/clarify? Your support is greatly appreciated and we sincerely value your guidance in enhancing the quality of our work."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700503470706,
                "cdate": 1700503470706,
                "tmdate": 1700503470706,
                "mdate": 1700503470706,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vtAoe8SmTw",
                "forum": "DpFeMH4l8Q",
                "replyto": "EQ0IDT2BTA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8317/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8317/Authors"
                ],
                "content": {
                    "title": {
                        "value": "last day reminder"
                    },
                    "comment": {
                        "value": "Since we have reached the end of the rebuttal period, we would like to reiterate and summarize our response earlier. \n\n> **Q2**: The proposed method look quite trivial and standard, which is a in-context fine-tuning method. There are some changes in the method details but those details are a bit hard to understand, which I will pose some questions on next.\n\n* We **added an algorithm box in Appendix A** of the revision to illustrate how GPO is trained. GPO trains a transformer module for in-context predicting unseen group preferences based on the few-shot examples provided. And we do not train the base LLM weights. Moreover, the training details and hyperparameters are given in the appendix section G. \n\n\n* GPO is not an in-context fintuning method. In section 3 (Page 6), the in-context finetuning method is actually one of our baselines that performs worse than GPO. Another advantage is that GPO operates in the latent space and can incorporate more in-context examples, while \"in-context fine-tuning\" takes raw text tokens as input and is constrained by the context window size. By working in the latent space, GPO can leverage more few-shot examples to better adapt to the preferences of unseen groups at test time.\n\n* To our knowledge, GPO is among the first to do **in-context preference learning** for group alignment - we train a single model that can align to unseen groups at test time without any gradient updates: $p(y_{m+1:n} | x_{1:n}, y_{1:m})$ only with $m$ context pairs. This makes GPO efficient for few-shot adaptation to the nuanced distribution of preferences across groups which can be utilized to guide LLM as a group opinion simulator for downstream tasks.\n\n> **Q3**: ... it is said that \"In particular, we discard the positional encodings commonly found in standard transformer architectures\". Wouldn't the removal of positional encodings significantly deteriorate the performance of those pre-trained and fine-tuned LLMs?\n>\n> **Q4**: in the second last paragraph of page 4, it is said that \"we employ a masking strategy where the context pairs can self-attend to each other\". Could you elaborate such a masking strategy?\n\nRegarding the above concerns about positional encodings and masking strategy, we have **added a more detailed description in Appendix C** in the revision to formally describe the two properties integrated in GPO:\n\n1. **Context invariance**: GPO removes positional encodings, ensuring that predictions are not influenced by the order or permutation of context pairs. It employs a masking strategy other than the causal mask in traditional transformers, such that the context pairs can and only attend to all the other context pairs.\n\n2. **Target equivalence**: The masking strategy also ensures the targets attend to context points but not other targets, adhering to the conditional independence in Equation 2.\n\nWe hope this addresses your concerns. Given *today is the last day for the discussion period*, please let us know if you have more questions. Your support is greatly appreciated and we sincerely value your guidance in enhancing the quality of our work."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674267575,
                "cdate": 1700674267575,
                "tmdate": 1700674708890,
                "mdate": 1700674708890,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "piYrATxa6N",
            "forum": "DpFeMH4l8Q",
            "replyto": "DpFeMH4l8Q",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8317/Reviewer_dRt2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8317/Reviewer_dRt2"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a framework to tackle the preference prediction problem, that is given a question, predict a distribution over all possible answers provided by the format of multiple choice question. The authors propose to view the question as a semi-supervised prediction problem and use LLM to augment the input data x into a (x,r) pair. The final prediction is done by training a shallow Transformer model over the augmented."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper proposes a novel idea to augment the data. The final analysis shows that direct tuning on LLM would not also obtain the best qualities on few-shot datasets."
                },
                "weaknesses": {
                    "value": "While the paper presents a new infra on the problem of preference distribution, the alignment method does not look too much different than a normal semi-supervised framework and there are some caveats in the experiment design and baseline choice to fully justify the acceptance of this submission.\n\n* For the baseline, the authors seem to fail to include one direct method.\n  * A simple Transformer model that purely learns from the (q, y) pair and uses them in a semi-supervised fashion as a sequence of inputs. As is pointed out by the author, the Reward Model baseline is underperforming a lot of the other baselines, it would really make sense to add a comparable-sized baseline as in the author\u2019s proposed method to rule out the possibility that overfitting is the only cause of inferior baselines w/o really relying on LLM. \n* The PeftConfig configuration seems not consistent with the description of the Reward Model baseline in the paper. The authors argue to use a linear MLP head for the Reward Model baseline. However, in the code, the authors used a LoraConfig which should by default fine-tune every layer of LLM, and might be the major reason for the underperformed score of this baseline.\nAlso, it makes more sense to use cross-entropy for the loss function instead of MSE as the final output is a distribution.\n* The terminology of alignment seems a bit too abused and distracting in this setting. In this work, the authors only tried to learn a separate Transformer architecture that operates upon the output of LLM while the LLM itself does not enjoy any new abilities in its parameters based on the modifications.\n* It would also be an interesting point in this work to justify the reason for the output of (r) as input in this work (through ablation studies). My current understanding is that the sampled response would be used as an anchor point for the training of the proposed method, however, it would lead to a natural question: without r, will the model underperform a lot, meaning that LLM would also be the important ingredient? Also, in this case, it seems that the qualities of the response from the model would also seem to be quite important, and it would make more sense to replace it with random strings etc to further make the study more coherent."
                },
                "questions": {
                    "value": "It would be great to see the authors compare with the very basic baseline, perform ablation studies, and fix some config settings to make the paper more complete."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8317/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8317/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8317/Reviewer_dRt2"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8317/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699427694508,
            "cdate": 1699427694508,
            "tmdate": 1700635760431,
            "mdate": 1700635760431,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Lxjfu9FulA",
                "forum": "DpFeMH4l8Q",
                "replyto": "piYrATxa6N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8317/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8317/Authors"
                ],
                "content": {
                    "title": {
                        "value": "response to reviewer dRt2 (1/n)"
                    },
                    "comment": {
                        "value": "Thank you for the detailed and constructive reviews and suggestions for experiments, which have helped improve both the clarity and the rigor of our paper. We addressed your feedback below:\n> Q1. While the paper presents a new infra on the problem of preference distribution, the alignment method does not look too much different than a normal semi-supervised framework\n\nA1: We want to clarify that GPO is not a semi-supervised framework, but rather a few-shot learning framework that is trained on fully labeled preference dataset of (q, r, y), where q is a query question, r is the response, and y is the group\u2019s preference to this response given the q.\n\nGPO\u2019s transformer module learns to infer preference y for a viewpoint (concatenation of q and r), for unseen groups at test time, which is both more accurate and sample efficient than SFT or prompting methods. For better clarity, we have included a detailed algorithmic box for GPO\u2019s training pipeline in Appendix A of the revision paper.\n\n> Q2. For the baseline, the authors seem to fail to include one direct method. A simple Transformer model that purely learns from the (q, y) pair and uses them in a semi-supervised fashion as a sequence of inputs\n\nA2: Thank you for highlighting the potential oversight in our baselines. We'd like to address this in two parts based on our understanding of your suggestion:\n* Based on our problem setup, we think the q you referred to is actually x (the concatenation of q and r), because the preference y is associated with x rather than q alone. Then the concept of inputting the (x, y) pairs directly to a transformer model closely aligns with our 'In-Context Finetune' baseline. In this baseline, n context pairs (x,y) are indeed fed sequentially as inputs to the LLM to predict the preference y, and this method performs worse than GPO in our experiments. However, we are not using GPO nor the baseline in a semi-supervised fashion but rather train them on labeled (x, y) pairs.\n* Secondly, we have also included a baseline that aligns with your suggestion of a simple transformer model in appendix Table 3 we submitted. We compared GPO with an equally-sized standard autoregressive transformer that employs standard causal mask and positional encodings and also takes in (x,y) pairs as sequence inputs too. In contrast, GPO has a different masking strategy and removed positional encoding to adopt inductive biases that lead to two properties. One is context permutation invariance, where the prediction of queries does not depend on the ordering of the context pairs; and the other property is target equivariance, which requires that whenever we permute the target inputs, the predictions are permuted accordingly. These inductive biases are important for our problem setup. And GPO\u2019s inherent inductive biases yield superior alignment performance compared to a traditional transformer as shown in the table below. \n\nIf our interpretations do not fully address your baseline suggestion, please give us further clarification and we will provide additional baseline experiments.\n\n> Q3: The terminology of alignment seems a bit too abused and distracting in this setting. In this work, the authors only tried to learn a separate Transformer architecture that operates upon the output of LLM while the LLM itself does not enjoy any new abilities in its parameters based on the modifications.\n\nA3: We use \"alignment\" to mean aligning the output distributions of the generative model - a combination of LLM and GPO - to group preferences. Although GPO does not modify the weights of the LLM itself, it can guide the LLM to produce sample outputs that are attuned to the desired preferences. \n\nThis terminology is also in line with prior works for alignment such as [1] and [2], where alignment also refers to tuning sample outputs rather than altering LLM parameters. In these works, alignment is achieved using techniques like reward models and Best-of-N sampling over the pretrained LLM, without updating the base model itself. \nAdditionally, in contrast to prior work that treats human alignment as a single quantity such as safety or helpfulness, our work conceptualizes human alignment as an inherently subjective measure that depends on who it is evaluated against; to the best of our knowledge, we are among **the first to align LLMs to a group preference distribution rather than a single preference or group consensus.**\n\n[1] Let\u2019s verify step by step. https://arxiv.org/abs/2305.20050 \n\n[2] Scaling Laws for Reward Model Overoptimization  https://arxiv.org/pdf/2210.10760"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700286460131,
                "cdate": 1700286460131,
                "tmdate": 1700287868068,
                "mdate": 1700287868068,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KlZmRz5OKr",
                "forum": "DpFeMH4l8Q",
                "replyto": "piYrATxa6N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8317/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8317/Authors"
                ],
                "content": {
                    "title": {
                        "value": "response to reviewer dRt2 (2/n)"
                    },
                    "comment": {
                        "value": "> Q4:  \"the Reward Model baseline is underperforming a lot, it would really make sense to add a comparable-sized baseline as in the author\u2019s proposed method to rule out the possibility that overfitting is the only cause of inferior baselines w/o really relying on LLM. The PeftConfig configuration seems not consistent with the description of the Reward Model baseline...\"\n\nA4: Our reward models are initialized using the HuggingFace AutoModelForSequenceClassification which adds an additional linear layer to the LLM. When this model is converted to Peft model, the Peft library ensures that only the LoRA adapters and the additional linear layer are trained since our task_type is set to SEQ_CLS in our PeftConfig and we are using an AutoModelForSequenceClassification model. In the description of reward model baseline in section 3, when we state \u201cwe train a per-group reward model by adding a linear MLP head on a base LLM and train it on m context samples\u201d, \u201cit\u201d refers to all of the trainable parameters of the entire Peft model. We have updated the reward model baseline description to more clearly reflect the reward model training setup and apologize for the confusion.\n\nRelated works including InstructGPT[1] typically train a reward model by fine-tuning all parameters of a reward model(including the base LLM). Considering that we are training a very small subset of the base LLM\u2019s parameters with LoRA (not the full 7B), compared to prior works which train all 6B+ parameters for reward model baselines, it is unlikely that our reward models are overfitting majorly due to large model size. The number of trainable parameters for the reward linear layer, our original LoRA training and GPO\u2019s transformer module are shown in the table below. \n\n| Model    | Trainable parameter count |\n|---------------------|---------------------------|\n| GPO                 | 1188354                   |\n| Linear reward layer | 4096                      |\n| LLM + Linear layer, with LoRA           | 6295552                   |\n\nNevernethess, we conducted an experiment to rule out the overfitting possibility due to trainable parameters, and the results are shown below in the table, which shows no improvement for training only the top linear classification layer. We believe the reward model\u2019s inability still lies in the difficulty of learning a high-quality reward function from a small number of training examples for every group.\n|            Reward model setup              | OpinionQA         | GlobalOpinionQA  |\n|--------------------------|-------------------|------------------|\n| Train only top linear layer | 0.824 \u00b1 0.014      | 0.675 \u00b1 0.028    |\n| Orig setup: train both linear and LLM layers with LoRA | 0.831 \u00b1 0.019      | 0.683 \u00b1 0.033    |\n\n\n[1] InstructGPT: https://arxiv.org/pdf/2203.02155.pdf\n\n\n> Q5: Also, it makes more sense to use cross-entropy for the loss function instead of MSE as the final output is a distribution.\n\n\nA5: We chose to use MSE as the loss function because the reward model takes a single input viewpoint (x) and outputs a scalar target value (y) rather than a probability distribution. However, you raise a good point that cross-entropy loss may better match the probabilistic nature of our tasks. Given your suggestion, we experimented with CE loss on the two datasets as shown in the results table. Empirically, the two losses perform similarly, but CE has the advantage that the loss converges faster than MSE loss.\n|          | OpinionQA      | GlobalOpinionQA |\n|----------|----------------|-----------------|\n| CE loss  | 0.820 \u00b1 0.032  | 0.680 \u00b1 0.029   |\n| MSE loss | 0.831 \u00b1 0.019  | 0.683 \u00b1 0.033   |\n\n> Q6: ...justify the reason for the output of (r) as input in this work (through ablation studies) ... a natural question: without r, will the model underperform a lot, meaning that LLM would also be the important ingredient? Also it seems that the qualities of the response from the model would also seem to be quite important, and it would make more sense to replace it with random strings etc to further make the study more coherent.\n\nA6: We think there may be some confusion about the role of the response in our setup. To clarify, the response r is essential in our problem formulation. Our goal is to model preferences y over query-response pairs (q, r), where r provides the crucial context to ground each opinion. Without r, the score y would be assigned to the question q alone. Thus, r is critical for accurately reflecting the nuanced \"opinion\" we aim to infer group preferences over.\n\nHowever, we appreciate your suggestion of an interesting ablation and have conducted this experiment by giving a random string response to every query. As shown in the table below, the performance degraded substantially without the response. \n\n| GPO setup  | OpinionQA          |\n|---------------------------|--------------------|\n| Random string response    | 0.7909 \u00b1 0.009  |\n| Actual response           | 0.9201 \u00b1 0.0026    |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700287777118,
                "cdate": 1700287777118,
                "tmdate": 1700287777118,
                "mdate": 1700287777118,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rg0Wjb9nPT",
                "forum": "DpFeMH4l8Q",
                "replyto": "piYrATxa6N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8317/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8317/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reminder"
                    },
                    "comment": {
                        "value": "Thank you again for your insightful feedback on our paper. We've carefully considered your comments and conducted new experiments to address the questions you raised. We'd like to note that the discussion phase is drawing to a close, and we hope you've had a chance to review our response. Considering the time constraint, are there any other questions or concerns you'd like us to address? Your support is greatly appreciated and we sincerely value your guidance in enhancing the quality of our work."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700503112191,
                "cdate": 1700503112191,
                "tmdate": 1700503493158,
                "mdate": 1700503493158,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mvCxvnYeYQ",
                "forum": "DpFeMH4l8Q",
                "replyto": "rg0Wjb9nPT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8317/Reviewer_dRt2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8317/Reviewer_dRt2"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for the detailed response!\n\nI think the authors did a good job in answering my questions, which majorly arose from the unclear setup statement about the role of value r (a default value that is provided as part of the input).\n\nThe setup of the in-context learning becomes clearer after the Algorithm Box A.\n\nFor the explanation on y, could the authors explain a bit more about why it\u2019s a scaler? Based on Figure 1, I suppose the problem setup is to predict y given a pool of ((q, r), y) tuples where y is a distribution over the options from q. And authors also argue that the work is to \u201calign LLMs to a group preference distribution\u201d.\n\nI also want to ask for clarification one more time on the baseline of Transformer vs GPO. For these two methods, is in-context learning the major difference? i.e. for Transformer, only a ((q,r), y) pair is fed each time to the model to train.\n\nFor the novelty part, I appreciate the authors\u2019 response in stressing the major difference between their work and the existing ones on preference distribution and group consensus. I believe it could be better and clearer if the authors can explain more about the differences between their work vs existing literature in the machine learning jargon. That is, for the existing work of group consensus, did the existing works only show scalers, what is their predicting target and technique that the new method could differ most from?\n\nOne final suggestion is that the clarity of the draft can be clearer if the authors could move the Algorithm Box and the comparing baseline of Table 3 to the main body.\n\nI would be happy to raise my point to weakly accept if the authors could add some comments on my questions. \n\nMinor: Why the Meta train on 60% would have the highest qualities? In Table 3."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700546214034,
                "cdate": 1700546214034,
                "tmdate": 1700546214034,
                "mdate": 1700546214034,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qIuEHeqVRs",
                "forum": "DpFeMH4l8Q",
                "replyto": "EKZFrweUD6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8317/Reviewer_dRt2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8317/Reviewer_dRt2"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed explanation.\n\nI think the authors answered all my questions clearly, whereas the Transformer baseline may still seem a bit naive. I guess a better way would be to train for each category a separate model so that whenever the group identity is told, we could use the corresponding Transformer to make a prediction. In this way, the model will not mix up the preference data.\n\nOne more note here, which I think was originally confusing is about the scaler of y. The authors answered my question that y is used to pair with the response so that it's a scaler. Afterwards, a softmax score can be calculated. It still seems an unnatural setup: why not just compile every y and r together so that the model could directly predict the distribution all at once while resorting to making individual scores? This method can also result in a scaling issue as the number before the softmax has any associated meaning and can be scaled by the same constant.\n\nOverall, I would be still happy to increase my score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8317/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700635741168,
                "cdate": 1700635741168,
                "tmdate": 1700635741168,
                "mdate": 1700635741168,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]