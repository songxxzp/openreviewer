[
    {
        "title": "The Impact of Depth and Width on Transformer Language Model Generalization"
    },
    {
        "review": {
            "id": "NwgqOaTVEQ",
            "forum": "WIGsqpZpFT",
            "replyto": "WIGsqpZpFT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6715/Reviewer_mDPp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6715/Reviewer_mDPp"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the effect that LLMs\u2019 depth has on its performance on compositional genalization and language modeling tasks. To disentangle the effect of depth on performance from other factors, the authors fixed the total number of parameters of LLM by reducing the size of the transformer\u2019s feed-forward dimension (d_model) while increasing the layers of transformer. Experiments here showed that deeper LLMs result in better language modeling and compositional generalization up until when the LLM becomes too narrow when d_ff<d_model. The authors also conducted more experiments to show that the better compositional generalization by deeper LLMs is not simply due to better language modeling performance by using pretrained deeper LLMs that have similar perplexity than the shallower counterpart. Experiments are conducted on 3 model size classes, pretrained on C4 corpus and 4 compositional generalization tasks (COGS, COGS-vf, GeoQuery, English passivization)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+The paper\u2019s empirical findings contributes to the body of work that seek to better understand how to train LLMs most efficiently by choosing the best mix of model hyperparameters given a particular computational budget.\n\n+Experiments are designed well to disentangle possible confounders (language modeling performance etc).\n\n+The paper is generally well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "-The paper\u2019s core contributions centers around mostly confirming existing findings (e.g. Mueller et al. (2022) and Tay et al. (2021)) with empirical results that a bigger depth improves expressiveness of neural network or LLMs, limiting the impact of the work. Making it more obvious what is different from these prior work will help readers better appreciate the paper\u2019s contributions (e.g. in-depth analyses about why this occurs beyond empirical results on performance).  \n\n-The experiments focus only on compositional generalization and language modeling tasks while there is a plerotha of other tasks that can be used to evaluate LLMs\u2019 generalization capabilities."
                },
                "questions": {
                    "value": "Can compositional generalization and language modeling tasks along stand to evaluate the generalization of the LLMs (or mostly only compositional generalization)? It would be helpful to discuss the different types of generalization if the paper is claiming generalization as a whole beyond compositional generalization."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6715/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698614948926,
            "cdate": 1698614948926,
            "tmdate": 1699636771934,
            "mdate": 1699636771934,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KhiUbMpYRT",
                "forum": "WIGsqpZpFT",
                "replyto": "NwgqOaTVEQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6715/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6715/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their constructive feedback.\n\n**W. 1: Paper mostly confirms existing findings.**\n\nIn our view, prior empirical findings are suggestive but incomplete as the experiments were not controlled: depth was confounded with parameter count. The primary thing which distinguishes our work here from the findings of, e.g., Mueller et al (2022) or Tay et al. (2021) is our goal of controlling for parameter count; while these previous works note that depth can be helpful, this conclusion is fundamentally confounded by the fact that deeper models in their setups are also larger models. Our core contribution is to disentangle that confound and show what the impact of depth is, independent from size. Because, as we show, the usefulness of depth saturates extremely quickly, our results suggest that adding depth does not confer the strong advantage that conventional understanding might expect; rather, adding depth does very little for all but the shallowest models, and in fact, past a certain point increasing model depth can actually be harmful. To our knowledge, these takeaways are novel. Finally, we also show that this effect is present in models\u2019 compositional generalization abilities, and demonstrate that this effect scales with model size. \n\nWe have also since conducted an analysis on the compute efficiency of our models by depth. We find that compute efficiency\u2013as measured by latency\u2013is very linear in depth, indicating that models pay a constant price for each additional layer added. Combining this with our main results showing the diminishing returns on depth in terms of pure performance, our results additionally suggest that practitioners bound by resource constraints should prefer to make models much wider and shallower than convention would otherwise suggest. A revised version of the manuscript will include the specifics, and we will include a supplementary figure in our reviewer responses in the next couple of days. \n\n**W. 2: Adding other tasks.**\n\nWe agree, and are in the process of evaluating our models on the BigBench-Lite suite of tasks to get a broader picture of how depth impacts model performance. A revised version of the manuscript will include a breakdown of performance-by-depth on these tasks, similar to our section on compositional generalization.\n\n**Question: Generalization beyond compositionality**\n\nIn this particular study, we focused on compositional generalization; as we say in the second paragraph of the introduction, \u201cwe test the hypothesis that increasing a transformer\u2019s depth\u2014the number of layers it\nhas\u2014improves its out-of-distribution performance on tasks that require compositional generalization\u201d. We cite theoretical and empirical studies that motivate this hypothesis. We do not have specific hypotheses as to the effect of depth on other types of generalization, though we agree that it would be interesting to study any such effects empirically in future work. We do agree with you that our scope can be made clearer in the paper; in fact, to make it clearer that we\u2019re only making claims about compositional generalization, as opposed to other types of generalization, we have decided to add the word \u201ccompositional\u201d to the title of the paper. We will also go through the manuscript and ensure that the scope of our claims is appropriate."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6715/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700253820454,
                "cdate": 1700253820454,
                "tmdate": 1700253820454,
                "mdate": 1700253820454,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "n1lGujiEWu",
            "forum": "WIGsqpZpFT",
            "replyto": "WIGsqpZpFT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6715/Reviewer_1ZSK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6715/Reviewer_1ZSK"
            ],
            "content": {
                "summary": {
                    "value": "The paper provides a controlled study disentangling model depth from the width and total parameters. The results support the view that depth improves generalization in transformers, with diminishing returns past a shallow threshold. The paper makes a solid contribution to understanding model architecture choices for generalization. Overall, the paper makes a valuable contribution by investigating the impact of depth on the generalization ability of Transformer language models. However, addressing the following weaknesses would enhance the comprehensiveness and applicability of the research."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "By investigating the effect of depth on the model's generalization ability, the paper provides a valuable reference for improving and optimizing the design of language models."
                },
                "weaknesses": {
                    "value": "1. Since the paper mainly verifies the effect of the Transformer\u2019s \u201cdepth\u201d on combinatorial generalization, the \"depth and width\" in the title of the paper is misleading.\n2. While the paper primarily investigates the effect of depth, the impact of width on generalization is not extensively explored. It would be beneficial to analyze the trade-offs between depth and width and how they interact in terms of model performance and generalization.\n3. The paper does not thoroughly discuss the computational implications of increasing depth or width in Transformer models. Considering the computational cost associated with deeper models, it would be useful to analyze the trade-off between improved generalization and increased computational requirements."
                },
                "questions": {
                    "value": "1. Please double-check the reference format and standardize it.\n2. In the paper, you focus on the impact of depth on Transformer language model generalization, but the analysis of width is relatively limited. Can you provide further insights into the trade-offs between depth and width? How do these two factors interact in terms of model performance and generalization? It would be helpful to explore the joint effects of depth and width and their relative importance in achieving better generalization.\n3. You only conduct a single run for each experimental condition. Adding multiple runs would strengthen conclusions by quantifying uncertainty and ruling out run-specific fluctuations. Is it feasible to do multiple runs, even if a subset of conditions?\n4. Is there an optimal depth where returns diminish for all model sizes and domains? Or does optimal depth keep increasing with the model scale?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6715/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698636631804,
            "cdate": 1698636631804,
            "tmdate": 1699636771823,
            "mdate": 1699636771823,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TkcMXA6xSs",
                "forum": "WIGsqpZpFT",
                "replyto": "n1lGujiEWu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6715/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6715/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their constructive feedback!\n\n**W. 1 The title of the paper should focus on depth, not width.**\n\nThank you for this comment! On reflection, we tend to agree, and have decided to change the title as you propose.\n\n**W. 2: The effect of width is not extensively explored.**\n\nWe agree, and this is intentional: the goal of the paper is to test the hypothesis that deeper models have stronger compositional generalization abilities. To the extent that we do vary width, we do so only to enable fair comparisons across models of different depths (by keeping the total number of parameters matched). While there may be interesting effects of width (and other hyperparameters) on generalization, we do not have particular hypotheses as to what those would be, and as such such experiments are outside the scope of the paper. We hope that by changing the title we have made the scope of the paper clearer.\n\n**W. 3: Compute performance analysis.**\n\nWe fully agree that studying the compute impact of depth in addition to the performance impact is important. Since submitting the original manuscript, we have conducted an analysis of the compute cost of increasing depth as measured by latency; we plan to include this analysis in a future version of the manuscript very soon. We find that latency is roughly linear in depth, indicating that models incur a consistent compute-efficiency penalty as depth increases. This, combined with our main results which show that the performance improvement depth affords diminishes rapidly and quite early suggests that compute-constrained teams should likely prefer shallower models which are just deep enough to capture a substantial portion of the benefits depth affords, for two reasons. (1) In training, shallower models will either train in a shorter amount of time (for a fixed data budget) or can train on more data (for a fixed time budget) than deeper models of the same size; (2) In inference, shallower models have lower per-use latencies than deeper models of the same size do.\n\n**Q. 1: Standardize citation format.**\n\nThanks for pointing this out, we have gone through the bibliography and standardized our references. \n\n**Q. 2: The tradeoff between depth and width.**\n\nAs we mention in our response to Weakness 2, the main focus of our paper is on the effect of depth on generalization. To study it while keeping the comparison fair, however, we do need to trade depth off against width: for all models within a size class, we have to vary depth and width inversely with one another, which allows us to quantify how different depth-width combinations (for a fixed number of parameters) fare on our studied tasks. One particularly interesting finding we report concerning this tradeoff is the U-shaped relationship between depth and perplexity: when models become very deep their performance begins to degrade; as we mention in the paper, we hypothesize that this is due to the fact that they become too narrow.\n\n**Q. 3: Performing additional runs.**\n\nThanks for pointing this out - we fully agree, and have been working to replicate our analyses on multiple runs. We have updated our paper to replicate each result five times (that is, we now have 5 pretraining runs per condition for the language modeling results, and we have 5 fine-tuning runs based on those 5 pretraining checkpoints for the compositional generalization results). We have also added confidence intervals to both of these sets of results to quantify measurement uncertainty. In all cases, we find that our initial results hold up after repeated trials, strengthening our confidence in the conclusions.\n\n**Q. 4: Is there an optimal depth for all sizes & domains?**\n\nThanks for this very insightful question. Our analysis in Section 3.1, particularly the data shown in Fig. 3(b), suggests that the optimal performance\u2013as measured by absolute score for a fixed data budget\u2013consistently appears to be when the ratio of feed-forward dimension to embedding dimension is between 0.5 and 1.0. In our setup, this would mean that the optimal depth for a model should in fact increase with size, assuming that the embedding dimension is also scaled with parameter count, although this point may not line up with the depth of conventional existing models. For instance, among the model families we examine, the optimal 41M and 374M models are deeper than their conventional reference models. However, taking into account the reviewer\u2019s earlier point about compute cost, the \u201ccompute-optimal\u201d depth may actually be much shallower than convention suggests."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6715/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700253770817,
                "cdate": 1700253770817,
                "tmdate": 1700253770817,
                "mdate": 1700253770817,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xce1dtsydJ",
            "forum": "WIGsqpZpFT",
            "replyto": "WIGsqpZpFT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6715/Reviewer_yaYa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6715/Reviewer_yaYa"
            ],
            "content": {
                "summary": {
                    "value": "The paper empirically studied the impact of increasing depth and width on the model's out-of-distribution generalization performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper provides some interesting experiment results which might be useful for future research."
                },
                "weaknesses": {
                    "value": "1. The result is a bit too straightforward with only experiment results. More theoretical analysis on the difference between increasing depth and width on out-of-distribution generalization is required for a paper on venues such as ICLR.\n2. Why do the authors choose to focus on decoder-only models? What can be the difference between encoder-decoder models and decoder-only models on the impact from different depths and widths?"
                },
                "questions": {
                    "value": "Please see the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6715/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6715/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6715/Reviewer_yaYa"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6715/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698827344879,
            "cdate": 1698827344879,
            "tmdate": 1699636771669,
            "mdate": 1699636771669,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fOu5tH5b1e",
                "forum": "WIGsqpZpFT",
                "replyto": "xce1dtsydJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6715/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6715/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their helpful comments.\n\n**W1. The paper focuses on empirical experiments**\n\nWe agree that this paper is an empirical experimental study, though it is motivated by a theory (Merrill, Sabharwal & Smith, TACL, 2022), but we disagree that this is a reason to reject the paper; in our experience, ICLR regularly publishes papers that are primarily experimental, and in fact such papers may be more common than theory-heavy papers. In general, we believe that there is a place for both theory papers and empirical papers, and especially for empirical papers such as ours that test predictions made by a theory.\n\n**W2. The focus on decoder-only transformers**\n\nWe agree with the reviewers that in future work it would be worthwhile to investigate additional architectures, including encoder-decoder transformers or even LSTMs, but given the vast amount of computational resources required to pretrain language models, we had to limit our scope to a single architecture. We chose to focus on decoder-only transformers as at the time of writing they were the state-of-the-art and the de-facto standard for both proprietary (PaLM, GPT-3,4) and open-source (GPT-2, LlaMA) large language models."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6715/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700253696725,
                "cdate": 1700253696725,
                "tmdate": 1700253696725,
                "mdate": 1700253696725,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dkf2SOlWGa",
            "forum": "WIGsqpZpFT",
            "replyto": "WIGsqpZpFT",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6715/Reviewer_ZnHf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6715/Reviewer_ZnHf"
            ],
            "content": {
                "summary": {
                    "value": "This paper  studies how performance for \"compositional generalization\" in Transformers varies as a function of depth. Its main twist is to pay careful attention in keeping the number of parameters constant. Hence, when augmenting depth, it is reducing width accordingly. This is done for 3 different number of parameters.\nInspecting the results, my take-aways are the following: performance _does_ systematically get better with deeper models as long as they don\u2019t become so narrow so as to have a width that requires reducing input dimensionality. This said, depths=3-6 look largely enough for all practical purposes, and the main way to get better performance is just to increase the number of parameters, which matches usual knowledge."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper asks a clear question: how does performance vary as a function of depth vs width for a given and fixed number of parameters for transformer based architectures on LM.\nThe paper provides a very clear and rigorous treatment of this question, also providing relevant literature and areas of further investigations.\nI particularly like one of the final questions that is asked about \"alternative approaches to controlling for total size\". Universal transformers are quite an extreme way to go, with all layers sharing the same weights. Maybe you could find some alternative way, for instance by repeating blocks of layers instead of just one. Likewise, I wonder about hypernetworks. They could be used to fill out huge networks, but then constraining the number of parameters.\n\nAll in all, I think the paper may be interesting to some persons, at least as a reference on that precise question it is asking.\n\nI think the paper is just as good as it gets to answer this question for any person that could be interested in the topic. For this, my pick is it should be accepted."
                },
                "weaknesses": {
                    "value": "- Applicability of the study is arguable a bit weak and I would say that it mostly would serve as a reference for what is usually considered common knowledge without any rigorous treatment: \"for a given parameter budget, pick depth over width\".\n- It remains extremely clear from this paper that beyond very small depth (as soon as we get 3~ layers), performance doesn\u2019t really go up with depth alone: the way to go is just to add more parameters.\n- As a practitioner, I would be interested by the following question: what about if my budget is not really in terms of number of paremeters, but rather in compute power or memory? Do you see the same thing happening that one should pick depth?\n- p8: \"when studying the the impact\""
                },
                "questions": {
                    "value": "I am not sure about the questions I should ask, since the paper really looks pretty clear to me. I guess it\u2019s more about what\u2019s next. Personally I didn\u2019t find the 3rd and 4th limitations very illuminating, but liked the 2nd."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6715/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6715/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6715/Reviewer_ZnHf"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6715/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699041818189,
            "cdate": 1699041818189,
            "tmdate": 1699636771546,
            "mdate": 1699636771546,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GKIuZYJ18B",
                "forum": "WIGsqpZpFT",
                "replyto": "dkf2SOlWGa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6715/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6715/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their constructive feedback.\n\n**W1: The study would mostly serve as a reference for what is usually considered common knowledge**\n\nThe reviewer\u2019s summary of our results as a recommendation to, for a given parameter budget, pick depth over width, has some truth in it, but it does not fully convey the richness of results we report. First, we find that the marginal utility of added depth reduces significantly as models get deeper, such that a more accurate summary of our recommendation would need to be amended by the statement _\u201cfor a given parameter budget, picking a very deep model will not help much over picking a \u2018just-deep-enough\u2019 model\u201d_. But we also find that when the corresponding feed-forward block becomes too small, it can actually be harmful to increase depth. Finally, motivated by prior theoretical claims, we show empirically that depth dramatically affects models\u2019 compositional generalization abilities, and demonstrate that this effect scales with model size; we are aware of very little work that has investigated the sensitivity of compositional generalization to depth. Overall, then, we believe that our results significantly expand upon what anyone might take to be \u201ccommon knowledge\u201d in the field. Even if that were not the case, however, in our view there is significant value in publishing controlled scientific experiments that confirm a commonly but informally belief.\n\n**W2: It remains extremely clear from this paper that beyond very small depth (as soon as we get 3~ layers), performance doesn\u2019t really go up with depth alone**\n\nThe diminishing return to increasing depth is indeed one of the interesting findings of our paper (though the specific number of layers where we begin to observe diminishing returns varies by task and is usually greater than 3). But we are not sure why the reviewer mentions this finding as a weakness of the paper - this is an empirical scientific discovery, one we didn\u2019t expect to find! This discovery also has applied implications: standard pretrained transformers may be deeper, and therefore more computationally expensive at inference time, than they need to be to generalize well (see also the next point).\n\n**W3: I would be interested by the following question: what about if my budget is not really in terms of number of paremeters, but rather in compute power or memory?**\n\nGreat question, thank you for pointing this out. We had already begun investigating this after the submission deadline and we are able to provide an answer. A revised version of the manuscript will include an analysis of the compute performance by model depth. For the accelerators we trained on, we find a strongly-linear relationship between depth and latency when controlling for parameter count. Because latency increases linearly with depth while performance saturates quite early, compute-constrained practitioners can benefit from choosing architectures which are \u201cjust deep enough\u201d for two reasons: (1) since shallower models are computationally cheaper, they can be trained in less time for a fixed data budget, or trained on more data for a fixed time budget; (2) for the same reason, shallower models incur less per-inference compute cost. This reduces point-of-use latency.\n\nTaken together, these points actually point against what commonly-understood knowledge might otherwise suggest: _picking depth over width is not necessarily the best choice_. Rather, if one can ensure that a model is deep enough to capture most or all of the value of added depth (which happens quite early on), practitioners concerned with things like compute or latency tradeoffs should actually then choose to make models as wide as possible. \n\n**W4: p8: \"when studying the the impact\"**\n\nThank you for catching the typo. It has been fixed."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6715/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700253655680,
                "cdate": 1700253655680,
                "tmdate": 1700253655680,
                "mdate": 1700253655680,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]