[
    {
        "title": "Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective"
    },
    {
        "review": {
            "id": "A2QeG9L5VN",
            "forum": "mIEHIcHGOo",
            "replyto": "mIEHIcHGOo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8932/Reviewer_NWu9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8932/Reviewer_NWu9"
            ],
            "content": {
                "summary": {
                    "value": "Large Language Models (LLMs) inherently encapsulate a vast reservoir of knowledge within their parameters, acquired through pre-training on extensive textual data. The authors investigate the possibility of transferring parametric knowledge across Large Language Models (LLMs) of different sizes. They introduce a novel approach, examining knowledge transfer from a unique parametric angle. Employing a two-stage framework involving knowledge extraction and injection, they conduct thorough experiments on four varied benchmarks, confirming the natural transferability of model parameters. Additionally, through detailed analysis of the pivotal factors impacting parametric knowledge transfer, they provide valuable insights for future research in this field."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The transferability of parametric knowledge is an important and timely topic that this paper addresses. Filling this gap is crucial in understanding the broader applicability and potential democratization of cutting-edge machine learning capabilities.\n\nThe paper introduces a novel parametric knowledge transfer paradigm. It moves beyond traditional methods like online and offline distillation and presents a unique approach focused on selecting specific static parameters from teacher models and injecting them into student models.\n\nThe experiments are conducted across various benchmark categories and LLM sizes. The results consistently demonstrate improvements in student performance after transferring teacher model parameters, providing strong empirical support for the proposed method.\n\nThe paper provides in-depth analysis of crucial factors such as teacher scales, initialization strategies, number of seed samples, and the nature of extracted parameters. This enriches the understanding of parametric knowledge transfer and its underlying dynamics.\n\nThe paper is clear and concise in writing."
                },
                "weaknesses": {
                    "value": "How does your method compare with the existing distillation and pruning methods? It would have been great to see some other methods applied for comparison."
                },
                "questions": {
                    "value": "Given the rapid evolution of LLMs, do you foresee the long-term viability and adaptability of the proposed knowledge transfer approach?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8932/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697988251094,
            "cdate": 1697988251094,
            "tmdate": 1699637125059,
            "mdate": 1699637125059,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yRrKLD1mrb",
                "forum": "mIEHIcHGOo",
                "replyto": "A2QeG9L5VN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8932/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8932/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer NWu9 - Part 1"
                    },
                    "comment": {
                        "value": "We are grateful for your valuable review and thoughtful suggestions, and we try to answer your questions as follows.\n\n> **Q1**: How does your method compare with the existing distillation and pruning methods? It would have been great to see some other methods applied for comparison.\n\nFirstly, we would like to clarify that our study's focus on parametric knowledge transfer is fundamentally distinct from distillation and pruning methods, characterized by the following differences:\n\n* **Purpose and Focus**: Rather than proposing better distillation or pruning methods, our study is focused on exploring the transferability of implicit knowledge embedded in static parameters. While prior research has explored the detectability and editability of parametric knowledge, its transferability remains less explored. Our experiments provide empirical evidence in the knowledge transfer scenario, where student models demonstrate improved performance after receiving task-specific knowledge from the teacher model, as shown in Tables 1 and 2.\n\n* **Process and Efficiency**: Our approach differs from standard knowledge distillation, which typically requires fine-tuning or the direct involvement of the teacher model in student model training \u2014 a computationally intensive process. In contrast, our parametric knowledge transfer involves extracting task-specific parameters from the vanilla teacher model and integrating them into the student model. This method, requiring only 32 inferences from the teacher model, offers a significant efficiency advantage, especially in the context of LLMs.\n\nDespite these differences, we recognize the relevance of distillation methods in the knowledge transfer scenario and have conducted comparative experiments using LLaMA-1 models (13B to 7B and 30B to 7B). Here KD refers to vanilla knowledge distillation, while SeqKD is sequence-level knowledge distillation [1].\n\n| Models          | GSM    | MMLU   | Super NI | AlpacaFarm | Average |\n|-----------------|--------|--------|----------|------------|---------|\n| Vanilla 7B      | 4.70   | 32.10  | 5.55     | -          | -       |\n| Vanilla 13B     | 4.93   | 43.50  | 7.78     | -          | -       |\n| 7B-LoRA         | 17.26  | 43.43  | 40.49    | 9.07       | 27.56   |\n| 13B to 7B (KD)   | 17.69  | 43.57  | 42.08    | 9.32       | 28.17   |\n| 13B to 7B (SeqKD)| 17.86  | 43.33  | 41.91    | 9.36       | 28.12   |\n| 13B to 7B (Ours) | **18.73**  | 44.03  | 42.37    | 9.28       | 28.60   |\n| 30B to 7B (KD)   | 17.81  | 44.10  | 41.96    | 9.48       | 28.34   |\n| 30B to 7B (SeqKD)| 17.99  | 43.97  | 42.40    | **9.61**       | 28.49   |\n| 30B to 7B (Ours) | 18.63  | **45.20**  | **43.08**    | 9.40       | **29.08**  |\n\n* All models are fine-tuned with LoRA, using identical training data and hyperparameters.\n* In this paper, we attempt to explore the evidence that knowledge in static parameters can be transferred between different LLMs, and knowledge transfer is the scenario in which we find and provide empirical evidence. Our focus is not on proposing to find better methods in this scenario.\n\n**References**:\n\n[1] Kim et al.  Sequence-level knowledge distillation. EMNLP 2016."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8932/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700294303196,
                "cdate": 1700294303196,
                "tmdate": 1700294303196,
                "mdate": 1700294303196,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yTblXt2OnA",
                "forum": "mIEHIcHGOo",
                "replyto": "A2QeG9L5VN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8932/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8932/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer NWu9 - Part 2"
                    },
                    "comment": {
                        "value": "> **Q2**: Given the rapid evolution of LLMs, do you foresee the long-term viability and adaptability of the proposed knowledge transfer approach?\n\nAs the landscape of LLMs continues to rapidly evolve, impacting model structures, scales, and the encoding of implicit knowledge, we believe the long-term viability and adaptability of our proposed knowledge transfer approach for several key aspects:\n\n* **Efficiency and Practicality**: The process of transferring knowledge using sub-matrix extraction and the LoRA-based approach is efficient, requiring only a small number of inferences from the teacher model (32 in our case). As LLMs grow in size and complexity, this kind of efficiency becomes ever more crucial.\n\n* **Scalability with Model Sizes**: Our method has demonstrated effectiveness in transferring knowledge between models of varying sizes, as shown in our experiments with different scales of LLaMA models (7B, 13B, 30B, and the newly added 70B). Moreover, as the teacher model's scale increases, our approach shows improved performance in knowledge transfer, aligning well with the ongoing trend of scaling up LLMs.\n\n* **Architecture Flexibility**: Our approach, grounded in the concept of parametric knowledge transfer, is not tightly bound to any specific model architecture. The underlying principles are adaptable to various forms of LLMs. As we demonstrated in our experiments with LLaMA models, our approach can handle different scales, even with different numbers of layers and dimensions or architectural differences (newly added LLaMA-2 70B to 7B). This flexibility suggests the method's potential adaptability to future LLM architectures.\n\n* **Methodological Flexibility**: Centered around the idea of intrinsic dimension and utilizing techniques like LoRA, our approach can be adjusted or expanded to incorporate other methods that refine matrix intrinsic dimensions. This adaptability makes our framework suitable for integration with future developments in LLM methodologies.\n\n* **Potential for Further Research**: Our research presents empirical evidence of the transferability of implicit knowledge within model parameters across various LLMs. This opens up avenues for further research, especially as LLMs increase in size, making fine-tuning or updating model parameters more challenging. Exploring how to better utilize knowledge within static parameters may become increasingly practical and significant.\n\nIn conclusion, despite the rapid evolution of LLMs presenting new challenges, the core principles of our parametric knowledge transfer approach have the potential to remain applicable and adaptable in the long run."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8932/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700294419579,
                "cdate": 1700294419579,
                "tmdate": 1700294419579,
                "mdate": 1700294419579,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "P0BEb9UVSf",
                "forum": "mIEHIcHGOo",
                "replyto": "A2QeG9L5VN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8932/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8932/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kind Reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer NWu9,\n\nThank you again for your valuable feedback and comments!\n\nAs the discussion period is ending soon, we would greatly appreciate it if you could let us know whether you are satisfied with our response. We will be happy to address any remaining concerns.\n\nSincerely,\n\nPaper 8932 Authors"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8932/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700623660437,
                "cdate": 1700623660437,
                "tmdate": 1700623660437,
                "mdate": 1700623660437,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TzGtGLJMs2",
                "forum": "mIEHIcHGOo",
                "replyto": "P0BEb9UVSf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8932/Reviewer_NWu9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8932/Reviewer_NWu9"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "Dear Authors,\n\nThank you for your response and responding to the question.\n\nGood luck !!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8932/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630494144,
                "cdate": 1700630494144,
                "tmdate": 1700630494144,
                "mdate": 1700630494144,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PmZFO2RVtf",
            "forum": "mIEHIcHGOo",
            "replyto": "mIEHIcHGOo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8932/Reviewer_ccZi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8932/Reviewer_ccZi"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose to better understand knowledge transfer between LLMs by identifying/extracting/aligning knowledge-specific parameters between teacher/upstream and student/downstream LLMs.\nCompared to offlne/online distillation methods which transfer knowledge via synthetic datasets / teacher model outputs (offline) or use teacher activations as a loss/regularizer for the student (online), the proposed parametric knowledge transfer method seeks to identify and re-scale task-specific parameters in the teacher model and inject them into the student model via LoRA.\nThe authors use the gradient of parameters with respect to the loss across a number of sampled instances as a parameter set sensitivity measure, and extract parameters via 1) identifying the top LAYERS by task sensitivity; 2) extracting student matrix-size submatrices from the top teacher layer matrices to transfer; and 3) decompose the extracted parameters and fine-tune the student model with LoRA initialized from the decomposition.\nThe authors demonstrate that their method out-performs vanilla LoRA and fine-tuning of smaller student models across a variety of reasoning tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "S1 - The paper targets an important fundamental question about how knowledge is stored/represented across language models (e.g. are representations aligned / similarly structured across different pre-training or architectural regimes).\n\nS2 - The authors evaluate on a good variety of knowledge-rich tasks (reasoning, MMLU/professional QA, instruction tasks, and open dialog) to demonstrate significant improvements over vanilla LoRA and fine-tuning. It would be interesting to see a wider range of model sizes being compared or comparing models pre-trained on different corpora (e.g. LLaMA-1/2 cross-transfer, or something like GPT-J). \n\nS3 - There is a solid discussion of different factors that may affect parametric knowledge transfer here (layer selection, parameter location [ffn/self-attention], structure in transferred parameters, and number of seed samples)"
                },
                "weaknesses": {
                    "value": "W1 - The technical details (Section 3) are succintly written but could be written in a more clear and understandable way (e.g. summarize how standard LoRA is modified in knowledge injection). Certain details are explained in a more accessible manner in e.g. 4.3 but should be present in the main technical spec.\n\nW2 - The diagrams can be more clearly framed with larger font and clear colors. In particular, the legend for Figure 3 is very difficult to read.\n\nW3 - In 4.3, it seems that random submatrix initialization from the 30B model is copmarable to sensitivity-extracted submatrices from the 13B model. This merits further discussion about what the trade-offs are between just using random sub-matrices from larger models vs. the sensitivity method."
                },
                "questions": {
                    "value": "Q1 - Can the authors provide more detail about 3.2 - how exactly the submatrix (with highest cumulative sensitivity) is extracted from the larger-parameter teacher model layer to create a mapping matrix in the student model's parameter set? And why in particular extracting a submatrix (with arbitrary possible location in the larger 2D teacher layer matrix) works?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8932/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698069330396,
            "cdate": 1698069330396,
            "tmdate": 1699637124937,
            "mdate": 1699637124937,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WPiF86t0oW",
                "forum": "mIEHIcHGOo",
                "replyto": "PmZFO2RVtf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8932/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8932/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer ccZi"
                    },
                    "comment": {
                        "value": "Thanks for the detailed review and helpful comments. In response to your questions, our answers are as follows.\n\n> **Q1 & Q2**: The technical details could be written in a more clear and understandable way & the legend for Figure 3 is very difficult to read.\n\nThank you for your valuable feedback, especially regarding the clarity of our technical writing and the legend of Figure 3!\n\nIn response to your suggestions, we have revised the sections on sub-matrix extraction (Section 3.2) and knowledge injection using LoRA (Section 3.3) within the methodology part of our paper. Additionally, we have redesigned Figure 3 and updated its corresponding explanation in Section 4.3 of the analysis section. These modifications are highlighted in blue in our revised manuscript for your convenience.\n\nPlease let us know if there are still parts that you find challenging to comprehend. We are committed to enhancing our presentation to ensure it is accessible and clear to all readers.\n\n> **Q3**: In 4.3, it seems that random submatrix initialization from the 30B model is comparable to sensitivity-extracted submatrices from the 13B model. This merits further discussion about what the trade-offs are between just using random sub-matrices from larger models vs. the sensitivity method.\n\nFirstly, we would like to clarify random extraction methods depicted in Figures 3 and 5(c) differ: Figure 3 illustrates random submatrix extraction, whereas Figure 5(c) demonstrates random single-weight extraction. Our findings indicate that extracting submatrices yields significantly better results, underscoring the importance of preserving the structural integrity of the teacher model's parameters in effective parametric knowledge transfer.\n\nAddressing your question, the comparable performance between the 30B to 13B (Random) and the 13B to 7B (Sensitivity) is indeed an intriguing point. However, it's essential to consider the overhead perspective. Our method requires only 32 inferences from the teacher model, meaning scaling up from 13B to 30B does not impose a significant additional computational burden. Moreover, this scaling up consistently enhances performance in three out of the four tasks we studied, as shown in Table 1. Therefore, while using randomly extracted submatrices from larger models can achieve satisfactory performance within our framework, we still recommend employing the seed samples-based method for parameter extraction that we propose for better results.\n\n> **Q4**: Can the authors provide more detail about 3.2 - how exactly the submatrix (with highest cumulative sensitivity) is extracted from the larger-parameter teacher model layer to create a mapping matrix in the student model's parameter set? And why in particular extracting a submatrix (with arbitrary possible location in the larger 2D teacher layer matrix) works?\n\nFor the implementation of our method, we don't employ a mapping matrix. Instead, our process is as follows:\n\n1. We go through all sub-matrices in the teacher model's parameter matrix that align with the dimensions of the student model. During this, we identify and extract the sub-matrix with the highest total sensitivity scores.\n2. This chosen submatrix is then directly decomposed and used to initialize the LoRA module for the student model.\n\nWe apologize for any confusion and have revised the descriptions in Sections 3.2 and 3.2 in the updated manuscript to clarify this process.\n\nAs for why extracting sub-matrices proves more effective than selecting individual weights or rows and columns, our approach is inspired by research in structured pruning [1-2]. Studies in this area have shown that structurally pruning layers, heads, intermediate dimensions, or blocks in weight matrices is more effective than point-wise pruning. These findings suggest that task-specific knowledge is often more concentrated in complete structures within a network, rather than being dispersed across isolated weights. In our research on parametric knowledge transfer, we've observed similar patterns, affirming the efficacy of extracting and utilizing these integral sub-structures.\n\n**References**:\n\n[1] Lagunas et al. Block Pruning For Faster Transformers. EMNLP 2021.\n\n[2] Xia et al. Structured Pruning Learns Compact and Accurate Models. ACL 2022."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8932/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700294662994,
                "cdate": 1700294662994,
                "tmdate": 1700294662994,
                "mdate": 1700294662994,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wxfxNq71sl",
                "forum": "mIEHIcHGOo",
                "replyto": "PmZFO2RVtf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8932/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8932/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kind Reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer ccZi,\n\nThank you again for your valuable feedback and comments!\n\nAs the discussion period is ending soon, we would greatly appreciate it if you could let us know whether you are satisfied with our response. We will be happy to address any remaining concerns.\n\nSincerely,\n\nPaper 8932 Authors"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8932/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700623622496,
                "cdate": 1700623622496,
                "tmdate": 1700623622496,
                "mdate": 1700623622496,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5a3BHLf2CK",
                "forum": "mIEHIcHGOo",
                "replyto": "wxfxNq71sl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8932/Reviewer_ccZi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8932/Reviewer_ccZi"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response to my questions - it's helpful in clarifying some of the points around submatrix extraction/initialization. I'm still a bit confused by W3 however and feel like it should be explained in more detail in the paper with accompanying discussions around what ramifications this has in real-world applications. As such, I have not revised my score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8932/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673981962,
                "cdate": 1700673981962,
                "tmdate": 1700673981962,
                "mdate": 1700673981962,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jyUABcngvL",
            "forum": "mIEHIcHGOo",
            "replyto": "mIEHIcHGOo",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8932/Reviewer_Df53"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8932/Reviewer_Df53"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method to transfer knowledge from big to small models by sharing the parameter. To be specific, it first detects and extracts a submatrix of parameters in large models, and the submatrix has the same size as the small model. Then, it decomposes the matrix into the Lora module and adds this module with the parameters of a small model. On Five datasets, this method brings about 2 points improvement compared to the original small model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper proposes a novel method that transfers knowledge from large models to small models by sharing parameters."
                },
                "weaknesses": {
                    "value": "1. The experiments are limited. This paper only tests on a few datasets for LLMs, does not discuss which amount of parameters is the best for transferring, and does not compare with other distillation methods, and does not compare with other methods about intrinsic dimension.\n\n2. The application of this method is limited. The paper does not experiment on different models. Can the knowledge transfer across architectures with this method?  This paper also assumes that the small model is already fine pre-trained. Can we transfer to the random-initialized model?"
                },
                "questions": {
                    "value": "1. I would like to see results from llama2 70b to 7b.\n\n2. In Formula 3, you use Taylor expansion to approximate the change of masking some parameters. But mask parameters are not trivial. Why can 1-order Taylor expansion approximate it?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8932/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699346133810,
            "cdate": 1699346133810,
            "tmdate": 1699637124835,
            "mdate": 1699637124835,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nhO57eTveN",
                "forum": "mIEHIcHGOo",
                "replyto": "jyUABcngvL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8932/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8932/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer Df53 - Part 1"
                    },
                    "comment": {
                        "value": "We appreciate your thorough review as well as constructive feedback, and we try to answer your questions as follows.\n\n> **Q1**: I would like to see results from llama2 70b to 7b.\n\nWe add the following experiment based on your suggestion, where for the GSM and MMLU datasets we present the 0-shot results and the metric used for Super NI is ROUGE-L.\n\n| Models      | GSM    | MMLU   | Super NI | AlpacaFarm | Average |\n|-------------|--------|--------|----------|------------|---------|\n| Vanilla 7B  | 3.34   | 41.70  | 4.68     | -          | -       |\n| Vanilla 13B | 6.52   | 52.10  | 4.84     | -          | -       |\n| 7B-LoRA     | 23.38  | 47.77  | 41.25    | 20.50      | 33.23   |\n| &nbsp;&nbsp;&nbsp;&nbsp;+13B Param.  | 25.30  | 49.37  | 42.98    | **24.64**      | 35.57   |\n| &nbsp;&nbsp;&nbsp;&nbsp;+70B Param. | **26.16**  | **49.60**  | **43.65**    | 24.27      | **35.92**   |\n\n* Due to the differences in attention architectures between LLaMA-2 70B and 7B models (with the 70B employing grouped-query attention and the 7B using multi-head attention), we restrict the parametric knowledge transfer from 70B to 7B to the FFN and embedding layers, which account for 0.36% trainable parameters. In contrast, in the transfer from the 13B to 7B model, we also include the attention module, increasing the percentage of trainable parameters to 0.61%.\n* Nevertheless, the transfer from the 70B to the 7B model demonstrates greater performance gains than transferring from 13B to 7B. This implies that our parametric knowledge transfer approach becomes increasingly effective as the teacher model scales up, even in the presence of architectural differences beyond the number of layers and dimensions.\n\n> **Q2**: In Formula 3, you use Taylor expansion to approximate the change of masking some parameters. But mask parameters are not trivial. Why can 1-order Taylor expansion approximate it?\n\nThe choice to neglect the first-order Taylor's remainder, which could be computed for greater accuracy using the Lagrange form, is based on practical considerations. Firstly, incorporating this remainder entails substantial computational efforts. Secondly, activation function such as ReLU typically leads to a smaller second-order term [1]. Therefore, this approximation method has been extensively employed in previous research and has consistently demonstrated effective results [2-4].\n\nIn addition, to further verify the efficacy of our sensitivity-based approach, we have included visualizations in Appendix A.1 of the revised version (Figure 6). They illustrate how parametric knowledge is distributed across different layers in four distinct task categories, which helps showcase the ability of our approach to capture task-specific knowledge.\n\n> **Q3**: Discussion about which amount of parameters is the best for transferring.\n\nOur analysis of the origin of parameters (Section 4.3, Figure 5(b)) involves a discussion of parameter amounts. For example, transferring a combination of the embedding layer, FFN, and attention modules, which constitute 0.608% of the model's parameters, yields the best results. In contrast, transferring a single module, like the FFN alone which accounts for 0.343%  trainable parameters, leads to relatively poor performance.\n\nTo further investigate the effect of the amount of parameters, we extend the experiments at LLaMA-1 13B to 7B and 30B to 7B by introducing comparisons with different LoRA r (intrinsic rank). The results are the average scores on the four datasets.\n\n| Transfer Module | LoRA r | Param. | 13B to 7B | 30B to 7B |\n|-----------------|--------|--------|----------|----------|\n| Embedding       | 128    | 0.137% | 27.33    | 27.52    |\n| Attention       | 16     | 0.248% | 27.53    | 27.92    |\n| FFN             | 16     | 0.343% | 27.87    | 28.23    |\n| All             | 4      | 0.152% | 27.93    | 28.37    |\n| All             | 8      | 0.304% | 28.17    | 28.48    |\n| All             | 16     | 0.608% | 28.22    | **28.63**    |\n| All             | 32     | 1.216% | 28.19    | 28.54    |\n| All             | 64     | 2.432% | **28.27**    | 28.60    |\n\n* For the transfer involving only the embedding layer, we set LoRA r at 128, ensuring that the number of trainable parameters remains comparable.\n* We observe that increasing LoRA r beyond 16 does not significantly enhance the results when transferring all modules. Consequently, we maintain LoRA r at 16 for the experiments presented in this paper.\n* Our analysis indicates that the origin of the parameters has a more pronounced impact on the effectiveness of parametric knowledge transfer than the amount of parameters transferred."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8932/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700295157924,
                "cdate": 1700295157924,
                "tmdate": 1700295157924,
                "mdate": 1700295157924,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SbBuNdEEqD",
                "forum": "mIEHIcHGOo",
                "replyto": "jyUABcngvL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8932/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8932/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer Df53 - Part 2"
                    },
                    "comment": {
                        "value": "> **Q4**: Comparison with distillation methods.\n\nTo address your query, it's important to first emphasize that our study's focus on parametric knowledge transfer is fundamentally distinct from traditional distillation methods, characterized by the following differences:\n\n* **Purpose and Focus**: Rather than proposing better distillation methods, our study is focused on exploring the transferability of implicit knowledge embedded in static parameters. While prior research has explored the detectability and editability of parametric knowledge, its transferability remains less explored. Our experiments provide empirical evidence in the knowledge transfer scenario, where student models demonstrate improved performance after receiving task-specific knowledge from the teacher model, as shown in Tables 1 and 2.\n\n* **Process and Efficiency**: Our approach differs from standard knowledge distillation, which typically requires fine-tuning or the direct involvement of the teacher model in student model training \u2014 a computationally intensive process. In contrast, our parametric knowledge transfer involves extracting task-specific parameters from the vanilla teacher model and integrating them into the student model. This method, requiring only 32 inferences from the teacher model, offers a significant efficiency advantage, especially in the context of LLMs.\n\nDespite these differences, we recognize the relevance of distillation methods in the knowledge transfer scenario and have conducted comparative experiments using LLaMA-1 models (13B to 7B and 30B to 7B). Here KD refers to vanilla knowledge distillation, while SeqKD is sequence-level knowledge distillation [5].\n\n| Models          | GSM    | MMLU   | Super NI | AlpacaFarm | Average |\n|-----------------|--------|--------|----------|------------|---------|\n| Vanilla 7B      | 4.70   | 32.10  | 5.55     | -          | -       |\n| Vanilla 13B     | 4.93   | 43.50  | 7.78     | -          | -       |\n| 7B-LoRA         | 17.26  | 43.43  | 40.49    | 9.07       | 27.56   |\n| 13B to 7B (KD)   | 17.69  | 43.57  | 42.08    | 9.32       | 28.17   |\n| 13B to 7B (SeqKD)| 17.86  | 43.33  | 41.91    | 9.36       | 28.12   |\n| 13B to 7B (Ours) | **18.73**  | 44.03  | 42.37    | 9.28       | 28.60   |\n| 30B to 7B (KD)   | 17.81  | 44.10  | 41.96    | 9.48       | 28.34   |\n| 30B to 7B (SeqKD)| 17.99  | 43.97  | 42.40    | **9.61**       | 28.49   |\n| 30B to 7B (Ours) | 18.63  | **45.20**  | **43.08**    | 9.40       | **29.08**  |\n\n* All models are fine-tuned with LoRA, using identical training data and hyperparameters.\n* In this paper, we attempt to explore the evidence that knowledge in static parameters can be transferred between different LLMs, and knowledge transfer is the scenario in which we find and provide empirical evidence. Our focus is not on proposing to find better methods in this scenario.\n\n> **Q5**:  Discussion about other methods for intrinsic dimension.\n\nRegarding the intrinsic dimension aspect, our focus isn't to develop or compare various methods in this area. Instead, we propose using the concept of intrinsic dimension as a bridge for transferring parametric knowledge. We instantiate our framework with LoRA, a standard representative in this domain. The process involves decomposing the teacher model's parameter matrix, which then serves as the initialization for the student model's LoRA module, aiding in the knowledge injection.\n\nNotably, the proposed workflow is designed to be adaptable, allowing any method that refines the intrinsic dimension of matrices to be directly incorporated into our existing framework.\n\n> **Q6**: About \u201cthis paper only tests on a few datasets for LLMs\u201d.\n\nWe would like to highlight the diversity and comprehensiveness of the 4 datasets we selected. 3 of these are collections of multiple tasks or domains: MMLU includes questions from 57 varied domains, Super NI covers 1,616 different NLP tasks, and AlpacaFarm is a compilation of open-ended conversation sources from five different origins. Given their diversity and extensive use in evaluating LLMs, we believe that the consistent improvements demonstrated by our method across these varied categories of benchmarks are a strong indication of its effectiveness."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8932/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700295353141,
                "cdate": 1700295353141,
                "tmdate": 1700295353141,
                "mdate": 1700295353141,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vORYzwWYkL",
                "forum": "mIEHIcHGOo",
                "replyto": "jyUABcngvL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8932/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8932/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer Df53 - Part 3"
                    },
                    "comment": {
                        "value": "> **Q7**: The paper does not experiment on different models. Can the knowledge transfer across architectures with this method?\n\nWe answer this question from the following two perspectives:\n\n* **Previous Research on Parametric Knowledge**: Earlier studies have primarily focused on detecting and editing parametric knowledge within a single pre-trained language model. When it comes to merging models, the common practice has been to use two identical models. As a result, the potential for transferring parametric knowledge between models with different layers, dimensions, or overall architectures remains largely unexplored.\n\n* **Our Experimental Models**: In our experiments, we use LLaMA-1 and LLaMA-2 models, which differ in their pre-training. The LLaMA models come in various scales (7B, 13B, 30B, 70B), each with a different number of layers and dimensions. In addition, the LLaMA-2 70B model has a unique attention module. Despite these significant differences, our experiments successfully demonstrate the transferability of parametric knowledge across various models. Moreover, our supplementary experiments transferring knowledge from LLaMA-2 70B to 7B (see Q1) confirm that our method remains effective even when attention architectures differ. Given that most Transformer variants share the same architecture in terms of FFNs and embedding layers, our method appears viable for transferring parametric knowledge across these models.\n\n> **Q8**: This paper also assumes that the small model is already fine pre-trained. Can we transfer to the random-initialized model?\n\nRecent research on knowledge transfer in the context of PLMs and LLMs typically assumes that student models are fine pre-trained. This is primarily due to two factors:\n\n1. From a computational perspective, the cost of pre-training from scratch is prohibitive.\n2. For practical applications, the majority of task-specific models are built upon and fine-tuned using existing PLMs or LLMs, leading to a research focus on the fine-tuning stage.\n\nHowever, we strongly agree that the study on the random-initialized models has promising applications, and we discuss this direction of research in our related work (Section 3.2), termed knowledge inheritance. This concept involves larger models receiving parameters from smaller models to accelerate the pre-training process. This research direction contrasts with the focus of our paper, as it emphasizes a different phase of training (pre-training rather than fine-tuning) and a reverse direction of knowledge flow (from smaller to larger models, as opposed to our large-to-small transfer).\n\nThank you once again for your valuable feedback and comments! They have been crucial in enhancing the comprehensiveness of our experiments and enriching our study with deeper insights and more empirical explorations in the field of parametric knowledge transfer. If there are any further questions or aspects you feel remain unaddressed, we are more than willing to provide additional information and clarifications as needed.\n\n**References**:\n\n[1] Molchanov et al. Pruning Convolutional Neural Networks for Resource Efficient Inference. ICLR 2017.\n\n[2] Ding et al. Global Sparse Momentum SGD for Pruning Very Deep Neural Networks. NeurIPS 2019.\n\n[3] Lubana et al. A Gradient Flow Framework For Analyzing Network Pruning. ICLR 2021.\n\n[4] Liang et al. No Parameters Left Behind: Sensitivity Guided Adaptive Learning Rate for Training Large Transformer Models. ICLR 2022.\n\n[5] Kim et al.  Sequence-level knowledge distillation. EMNLP 2016."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8932/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700295546948,
                "cdate": 1700295546948,
                "tmdate": 1700295546948,
                "mdate": 1700295546948,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ioO0ySbtWb",
                "forum": "mIEHIcHGOo",
                "replyto": "jyUABcngvL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8932/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8932/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Kind Reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer Df53,\n\nThank you again for your valuable feedback and comments!\n\nAs the discussion period is ending soon, we would greatly appreciate it if you could let us know whether you are satisfied with our response. We will be happy to address any remaining concerns.\n\nSincerely,\n\nPaper 8932 Authors"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8932/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700623552395,
                "cdate": 1700623552395,
                "tmdate": 1700623577220,
                "mdate": 1700623577220,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]