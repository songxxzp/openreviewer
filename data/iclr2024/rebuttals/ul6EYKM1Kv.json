[
    {
        "title": "Cognition-Supervised Learning: Contrasting EEG Signals and Visual Stimuli For Saliency Detection"
    },
    {
        "review": {
            "id": "XneLuSzCmW",
            "forum": "ul6EYKM1Kv",
            "replyto": "ul6EYKM1Kv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1665/Reviewer_h8j5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1665/Reviewer_h8j5"
            ],
            "content": {
                "summary": {
                    "value": "This paper uses contrastive learning for decoding EEG signals. The visual stimuli are annotations, the brain responses are the data, and both are matched using a contrastive loss instead of a classical supervised loss (e.g. least-squares). The authors empirically show that the learned representations cluster around annotation features (e.g. \"young\" or \"old\")."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper applies contrastive learning to EEG signals and demonstrates evidence that the learned representations cluster around interesting features, including age group."
                },
                "weaknesses": {
                    "value": "The writing style is at times over-emphatic compared to the actual contributions. For example, the abstract sets up the contribution as something completely novel and major (\"remains paramount\", \"practically impossible\", \"novel paradigm\", \"the first of its kind\"). However, in my understanding, this paper applies the existing framework of supervised contrastive learning [1] to EEG signals. Many relevant works are not cited: for example, consider [2] which also uses contrastive learning to learn representations from EEG signals and also demonstrates that the representations cluster following age group and other features. \n\nMapping the brain response to the stimulus is called \"decoding\" in the neuroscience literature [3]: it would be worth using this terminology. \n\nSome citations could be added to support strong statements that begin a paragraph, e.g. \"as human is known to respond strongly to facial stimuli\" (missing citation) or \"leverages a fundamental observation that the human brain response to differences in perception\" (missing citation). \n\nSome technical terms (e.g. \"visual saliency\" and \"semantic saliency\" in the abstract, \"epochs\" in the preprocessing paragraph) are used without being defined. Other terms (\"target and non-target epochs\") are also vague and undefined. \n\n\n[1] Khosla et al. Supervised Contrastive Learning. NeurIPS 2020. \n\n[2] Banville et al. Uncovering the structure of clinical EEG signals with self-supervised learning. Journal of Neural Engineering, 2021. \n\n[3] King et al. Encoding and Decoding Neuronal Dynamics: Methodological Framework to Uncover the Algorithms of Cognition. Preprint, 2017."
                },
                "questions": {
                    "value": "It might seem that the evaluation procedure is biased by design. For example, the \"clusterability\" of the learned representations around annotation features (e.g. young and old) is used to evaluate the learning procedure. Yet it is known that a contrastive loss performs clustering by design [1], so that \"positive\" pairs (here, a stimulus and corresponding brain response) are given \"close\" embeddings, and \"negative\" pairs (here, a stimulus and random brain response) are given \"far\" embeddings. So Table 3 and Figure 3 are entirely expected. Could the authors comment on that? \n\nSome details are missing in the data preprocessing. For example, when \"including a band-pass filter\", could the authors specify the frequency-range in the main text?\n\n[1] Wang et al. Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere. ICML, 2020."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1665/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697722607959,
            "cdate": 1697722607959,
            "tmdate": 1699636094581,
            "mdate": 1699636094581,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "M6UDDDs0F0",
                "forum": "ul6EYKM1Kv",
                "replyto": "XneLuSzCmW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1665/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1665/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "1. Regarding innovativeness, while we are not the first to apply self-supervision methods to EEG signals, our work is pioneering in applying these methods contrastively with visual stimuli. Prior research, such as sleep staging [1], has focused on using EEG signals alone for self-supervision. Our method's novelty lies in simultaneously utilizing both stimuli information and EEG signals, which are temporally aligned, a common practice in lab-collected EEG data and feasible in real scenarios with multiple sensors. We posit that using stimuli alongside EEG signals facilitates learning and identifying EEG structures, particularly in response to semantically salient stimuli, rather than deciphering these structures from scratch.\nOur approach differs from existing frameworks like SimCLR (unsupervised) and SupCon (supervised) that focus on a single modality (e.g., EEG signals) and typically require large datasets and batch sizes (4096 for SimCLR, 2048 for SupCon) for effective training. In contrast, EEG data collection is challenging, and our dataset comprises approximately 4400 data pairs for each task post-artifact removal, which is considerably smaller.\nWe acknowledge that our initial manuscript may have overstated our contributions. We have revised the abstract, introduction, and conclusion to more accurately reflect our work's novelty and scope.\n\n[1] Jiang, Xue, et al. \"Self-supervised contrastive learning for EEG-based sleep staging.\" 2021 International Joint Conference on Neural Networks (IJCNN). IEEE, 2021.\n\n[2] Chen, Ting, et al. \"A simple framework for contrastive learning of visual representations.\" International conference on machine learning. PMLR, 2020.\n\n[3] Khosla, Prannay, et al. \"Supervised contrastive learning.\" Advances in neural information processing systems 33 (2020): 18661-18673.\n\n2. We appreciate the suggestion to use the term 'decoding'. We consciously chose not to use 'decoding' in our paper as our method focuses on learning an embedding from EEG signals that maps to only task-relevant parts of stimuli, which slightly diverges from traditional 'decoding' that maps EEG directly to stimuli. In our view, 'decoding' more accurately describes models that translate EEG signals into explicit labels, such as 'target' or 'non-target,' or identify specific stimuli, like a digit observed by a participant. Our approach, however, is a generalization of this task. It does not create a one-to-one mapping between EEG and stimuli but instead generates a versatile embedding applicable to various downstream tasks not limited to clustering, such as the conditional image generation discussed in Section 4.4 of our paper.\nTo clarify our position, we have added a brief discussion on this distinction in the related work section of our revised manuscript."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1665/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700572576258,
                "cdate": 1700572576258,
                "tmdate": 1700572576258,
                "mdate": 1700572576258,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iWzUGlvJNn",
                "forum": "ul6EYKM1Kv",
                "replyto": "XneLuSzCmW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1665/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1665/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "3. Regarding the query on model performance and perceived evaluation bias, we contend that our results are not 'entirely expected' or trivial. The inherent noise in EEG signals makes even binary classification a significant challenge, even so in supervised BCI settings. In such supervised scenarios with complex visual stimuli, accuracies typically hover around 0.7 or lower in real-world applications, as seen in studies [1], [2], and [3]. As shown in Table 2 of our paper, the supervised EEGNet achieves a mean accuracy of only 0.699 across all tasks. Our model achieves comparable performance without relying on any external labels.\n+\nConsidering the potential for signal distortion due to artifacts like eye blinks, the correlation of EEG signals to stimuli is not guaranteed, underscoring the difficulty of brain-computer interfacing tasks. To better illustrate this point, we have included additional discussions and relevant references in the related work section of our revised manuscript. Additionally, we have added tSNE plots in the Appendix A.5 of the revised manuscript to visually represent the embedding model's effectiveness before and after training.\n\n[1] Bagchi, Subhranil, and Deepti R. Bathula. \"EEG-ConvTransformer for single-trial EEG-based visual stimulus classification.\" Pattern Recognition 129 (2022): 108757.\n\n[2] Lawhern, Vernon J., et al. \"EEGNet: a compact convolutional neural network for EEG-based brain\u2013computer interfaces.\" Journal of neural engineering 15.5 (2018): 056013.\n\n[3] Ahmed, Hamad, et al. \"Object classification from randomized EEG trials.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n\n\n\n\n4. On the band passing details, it is 0.2 - 35 Hz, as briefly explained in Appendix A.1 data preprocessing section. This information is now added to the main text in section 3.1 in the revised manuscript. More details: the raw eeg data are processed with MNE (version 1.5.1), it is first reampled to 500Hz using mne.io.raw.resample(500), then filtered using mne.io.raw.filter(0.2, 35). These details will be added to Appendix A.1."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1665/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700572609548,
                "cdate": 1700572609548,
                "tmdate": 1700572626772,
                "mdate": 1700572626772,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nSITBnYGxr",
                "forum": "ul6EYKM1Kv",
                "replyto": "iWzUGlvJNn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1665/Reviewer_h8j5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1665/Reviewer_h8j5"
                ],
                "content": {
                    "title": {
                        "value": "Acknowledging author response"
                    },
                    "comment": {
                        "value": "I thank the authors for their response which clarifies some of my concerns. \n\n**Expected results?** The contrastive training used by the authors --- by design --- pulls together EEG embeddings that are paired with similar stimuli, and pushes EEG embeddings that are paired with differing stimuli. In other words, the training clusters EEG embeddings based on the stimulus which acts as a label. So evaluating the embeddings by checking how well they predict (features of) the label that was used to cluster them seems biased at first glance. I would note that this concern seems to have been independently brought up by another reviewer (VsJo). Now, I understand the authors' point that their results are actually not so obvious. First, because the noise level in EEG data may be better handled by their contrastive training than by a fully supervised procedure where the EEG is used to predict the age / sex / hair color. If this is a claim the authors make, it should be clearer and substantiated. Second, because the EEG embeddings cluster based on one measure of similarity (scalar product) between stimuli used for training, but that does not necessarily imply that the EEG embeddings have to cluster based on another measure of similarity (features such as age / sex / hair color) between stimuli used for evaluation. This should be discussed much more clearly in order to guide the reader in understanding in what ways the authors' results could be construed as obvious but are maybe not so obvious. \n\n**Is it really self-supervised learning?**: the authors use the stimulus as a label during training, and then predict (features of) the stimulus during evaluation. It does raise the question whether this is actually supervised (not self-supervised) learning, which is a valid question brought up by Reviewer yZyL. I understand that there is a subtle point here, which is that that the \"entire\" stimulus is used for training, while evaluation consists in predicting a \"latent class\" of the stimulus (e.g. age / sex / hair color). So it is not exactly as if the same labels are used for training and evaluation. This should be further clarified in the main text as it is a point that was also brought up by another reviewer (yZyL).\n\nOverall, the authors' empirical results are interesting but more clarification is needed in the main text."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1665/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648662201,
                "cdate": 1700648662201,
                "tmdate": 1700648662201,
                "mdate": 1700648662201,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KMlQ0o6VgA",
            "forum": "ul6EYKM1Kv",
            "replyto": "ul6EYKM1Kv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1665/Reviewer_VsJo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1665/Reviewer_VsJo"
            ],
            "content": {
                "summary": {
                    "value": "The paper propose an interesting idea called \"cognition-supervised learning\". The method uses EEG signals as direct supervisory information. The approach uses EEG to contrastively train models to detect visual saliency without the need for manual annotations. The paper then applies the learned representations for several downstream tasks (classification, clustering, and image generation) showing competitive performance compared to models trained with manually labeled datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper provides a promising new direction for research where brain signals such as EEG can be utilized directly for supervising deep learning models. The paper is well-written."
                },
                "weaknesses": {
                    "value": "I ask the authors to respond to the following weaknesses/questions:\n\n- There has been prior works on image reconstruction from EEG, e.g. \n\"NeuroGAN: image reconstruction from EEG signals via an attention-based GAN\"\n\"EEG2IMAGE: Image Reconstruction from EEG Brain Signals\"\n\"Photorealistic Reconstruction of Visual Texture From EEG Signals\"\n\"Visual Saliency and Image Reconstruction from EEG Signals via an Effective Geometric Deep Network-Based Generative Adversarial Network\", and others. \nGiven that the proposed paper seems to perform the opposite direction of these works, they could be discussed. Moreover, simple reverse versions of these can be developed to use as baselines.\n\n- It would be valuable to see the impact of the CLIP loss by exploring and comparing other ones as well.\n\n- There's a small error in Fig 1, \"Find-tuning\" --> Fine-tuning  \n\n- While the RQs are very interesting, I wonder whether the outcome is completely expected. Given that we can map EEG to certain classes/actions/objects (e.g., affect classes, objects, directions, etc.), doesn't the outcome become expected? In other words, the method is mapping two high-dimensional data points onto a common lower dimensional feature space, where the locality of the image embeddings is used as the supervisory signal. Can the authors please (a) provide a discussion on this, (b) show the common embedding space (using tSNE, UMAP, etc) to extract some more insights about the reason behind why the method works. In general, I found the \"analysis\" part of the paper a bit weak. Further experiments to analyze the method, its components, and embedding space, are recommended."
                },
                "questions": {
                    "value": "Please see my comments under weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1665/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698730682206,
            "cdate": 1698730682206,
            "tmdate": 1699636094503,
            "mdate": 1699636094503,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dodzqGtOev",
                "forum": "ul6EYKM1Kv",
                "replyto": "KMlQ0o6VgA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1665/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1665/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Regarding prior works on EEG-based image reconstruction, our introduction section discusses a series of earlier studies suffering from confounded EEG data due to specific experimental block designs. This includes the EEG-GAN approach [1], [2], Thoughtviz [3], Brain2image [4], EEG-ChannelNet[5], and subsequent research such as EEG2IMAGE [6], DM-RE2I [7], NeuroGAN [8], and others [9]. Many of these studies use the same datasets; for instance, NeuroGAN uses the same dataset from Thoughtviz, and [9] employs the EEG-ImageNet dataset from [5].\n\nHowever, subsequent analyses [10], [11], [12] have identified a critical flaw in these approaches: the block design in data collection introduces temporal correlations between the stimulus class and the experiment's duration due to the same presentation order of stimulus class. Replication attempts have suggested that models were learning to condition on this temporal correlation rather than stimulus-related brain activity.\n\nOn the other hand, there are studies that utilize EEG signals from multiple trials to overcome the low signal-to-noise ratio inherent in EEG, like [13], which employs 24 repetitions of each stimulus and averages the visual evoked potentials for texture reconstruction. While these approaches have shown promising results, single-trial EEG-based high-resolution image reconstruction remains largely unexplored, to the best of our knowledge. \n\nWe have appended this discussion to the related work section of our revised manuscript. \n\n[1] Palazzo, Simone, et al. \"Generative adversarial networks conditioned by brain signals.\" Proceedings of the IEEE international conference on computer vision. 2017.\n\n[2] Spampinato, Concetto, et al. \"Deep learning human mind for automated visual classification.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.\n\n[3] Tirupattur, Praveen, et al. \"Thoughtviz: Visualizing human thoughts using generative adversarial network.\" Proceedings of the 26th ACM international conference on Multimedia. 2018.\n\n[4] Kavasidis, Isaak, et al. \"Brain2image: Converting brain signals into images.\" Proceedings of the 25th ACM international conference on Multimedia. 2017.\n\n[5] Palazzo, Simone, et al. \"Decoding brain representations by multimodal learning of neural activity and visual features.\" IEEE Transactions on Pattern Analysis and Machine Intelligence 43.11 (2020): 3833-3849.\n\n[6] Singh, Prajwal, et al. \"EEG2IMAGE: Image reconstruction from EEG brain signals.\" ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023.\n\n[7] Zeng, Hong, et al. \"DM-RE2I: A framework based on diffusion model for the reconstruction from EEG to image.\" Biomedical Signal Processing and Control 86 (2023): 105125.\n\n[8] Mishra, Rahul, et al. \"NeuroGAN: image reconstruction from EEG signals via an attention-based GAN.\" Neural Computing and Applications 35.12 (2023): 9181-9192.\n\n[9] Khaleghi, Nastaran, et al. \"Visual saliency and image reconstruction from EEG signals via an effective geometric deep network-based generative adversarial network.\" Electronics 11.21 (2022): 3637.\n\n[10] Li, Ren, et al. \"The perils and pitfalls of block design for EEG classification experiments.\" IEEE Transactions on Pattern Analysis and Machine Intelligence 43.1 (2020): 316-333.\n\n[11] Ahmed, Hamad, et al. \"Object classification from randomized EEG trials.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n\n[12] Ahmed, Hamad, et al. \"Confounds in the data\u2014Comments on \u201cDecoding brain representations by multimodal learning of neural activity and visual features\u201d.\" IEEE transactions on pattern analysis and machine intelligence 44.12 (2021): 9217-9220.\n\n[13] Wakita, Suguru, Taiki Orima, and Isamu Motoyoshi. \"Photorealistic reconstruction of visual texture from EEG signals.\" Frontiers in Computational Neuroscience 15 (2021): 754587."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1665/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700572484183,
                "cdate": 1700572484183,
                "tmdate": 1700572484183,
                "mdate": 1700572484183,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oGlFT4MXR2",
                "forum": "ul6EYKM1Kv",
                "replyto": "KMlQ0o6VgA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1665/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1665/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We think that our results are not entirely expected. Indeed, our embedding model maps high-dimensional EEG data to a lower-dimensional embedding, preserving relevant structures with the aid of paired stimuli data. However, this mapping is not trivial for two key reasons.\n\nFirstly, the inherent noise in single-trial EEG signals makes even supervised binary classification a challenging task, often resulting in accuracies around 0.7. For instance, accuracies is 0.78 in [1] for classifying human faces against objects, around 0.7 in [2] for within-subject and around 0.4 for cross-subject,  0.708 in [3] and below 0.6 for text stimuli in [4]. This variability underscores that mapping EEG signals to a common feature space is not straightforward.\n\nSecondly, while we use the locality of image embeddings to reduce the dimensions of EEG signals, these image embeddings alone are insufficient to discern semantic saliency. Section 4.1 and Table 1 demonstrate that clusters based on image embeddings alone do not align with the semantic saliency of our tasks. However, when clustering the EEG embeddings in the common feature space, there is a notable alignment with the target semantic feature. Furthermore, a PCA on these learned embeddings reveals that the most significant component correlates with the task's targeted semantic feature. We acknowledge the value of a deeper exploration into the learned EEG embeddings and their properties beyond mere correlation with task semantics. To this end, we have added new tSNE plots of both raw EEG signals and the learned embeddings in Appendix A.4 of our revised manuscript. \n\n[1] Bagchi, Subhranil, and Deepti R. Bathula. \"EEG-ConvTransformer for single-trial EEG-based visual stimulus classification.\" Pattern Recognition 129 (2022): 108757.\n\n[2] Lawhern, Vernon J., et al. \"EEGNet: a compact convolutional neural network for EEG-based brain\u2013computer interfaces.\" Journal of neural engineering 15.5 (2018): 056013.\n\n[3] Ahmed, Hamad, et al. \"Object classification from randomized EEG trials.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n\n[4] Eugster, Manuel JA, et al. \"Predicting term-relevance from brain signals.\" Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval. 2014.\n\n* Thank you for highlighting the typo in Figure 1. We have now corrected it in the revised manuscript."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1665/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700572523369,
                "cdate": 1700572523369,
                "tmdate": 1700572523369,
                "mdate": 1700572523369,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NQmXiTg7WI",
            "forum": "ul6EYKM1Kv",
            "replyto": "ul6EYKM1Kv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1665/Reviewer_4tkM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1665/Reviewer_4tkM"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents cognition-supervised learning using EEG data to contrastively train models for visual saliency without manual annotations, which generates competitive performance on subsequent tasks and opens the way for future research on human cognitive system-guided supervision. for computer vision and machine learning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper introduces a original and innovative concept, \"cognition-supervised learning,\" which leverages human brain signals (EEG data) as direct supervisory signals for training machine learning models.\n\nThe paper explores the concept of cognition-supervised learning and provides a well-structured experimental framework to validate its effectiveness.\n\nThe paper is well written and clearly communicates its key ideas, methodology, and results. The introduction clearly states the context and motivation for the research, and the research questions are clearly described.\n\nThe contribution with an open EEG dataset in this paper is a great contribution, as it not only encourages transparency and reproducibility, but also promotes collaboration and facilitates further exploration of cognition-supervised learning, encouraging growth and progress of the research field.\n\nThe paper provides a detailed description of the dataset used, along with a detailed explanation of the processing of the data involved in the study. This strengthens the methodological quality of the research.\n\nThe paper maintain a level of quality in its experiments. The use of well-established techniques like unsupervised clustering and linear classifiers ensures the reliability of the evaluation. In addition, the incorporation of a qualitative evaluation using generative adversarial networks adds a layer of quality.\n\nTo sum up, the use of EEG data for supervision in machine learning is a novel and interesting direction that could have positive implications for the field."
                },
                "weaknesses": {
                    "value": "The paper discusses results in the context of facial images, but it is not clear how well the proposed approach generalizes to other types of images or domains. It would be important to conduct experiments with a larger range of datasets to demonstrate the robustness of the cognition-supervised learning approach.\n\nThe paper mentions that it opens avenues for human-in-the-loop systems, but it would be valuable to provide insights into potential future research directions and how this work could be extended and applied in various domains (e.g. NLP)\n\nIn section 3.1 the authors mention that \"The images were manually screened to ensure realism and the absence of visual artifacts\", but I think a little more detail should be given regarding this.\n\nAt the beginning of section 3.2, it would be desirable for authors to include relevant references to support these claims, thus facilitating a deeper exploration of their claims in the article.\n\nIn section 4.2, it would be good for the authors to compare the results with more control models (e.g. EEGNet Fusion, MI-EEGNet, etc.) in order to have stronger results. Furthermore, the difference between EEGNet and contrastive embedding (Mean column) is very small (0.699 \u00b1 0.037 vs 0.704 \u00b1 0.046), which even falls within the standard deviation.\n\nAs mentioned in the limitations, the present work only applies to two classes; it would be of great interest to expand it to more classes, since many problems are of this nature."
                },
                "questions": {
                    "value": "Do the authors believe that the present work could be expanded to more classes and not just two?\n\nDo the authors plan to expand the work with other types of images or in another domain (e.g. NLP)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1665/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698887254883,
            "cdate": 1698887254883,
            "tmdate": 1699636094398,
            "mdate": 1699636094398,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nJ0M8JiaMb",
                "forum": "ul6EYKM1Kv",
                "replyto": "NQmXiTg7WI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1665/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1665/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "1. Regarding the generalizability of our approach and the use of facial images: We chose facial images due to their homogeneity in features such as color, shape, and geometry, which minimizes confounding factors in EEG experiments. For instance, using an image with a red sports car might evoke brain responses related to the color red or interest in cars, rather than the intended semantic features. To ensure the validity of our dataset and avoid such biases, we needed stimuli that varied semantically while being invariant to other confounding attributes.\n+\nArtificial facial images were specifically selected to further reduce confounding effects. Recognizing a celebrity could trigger brain responses associated with such recognition effect rather than the semantic saliency of facial features, which was our experimental focus. \n+\nAs for image selection and processing, all 70,000 images were manually screened by several researchers. The criterion was to exclude images with visual artifacts, such as distorted faces or other clear signs of an artificial image, to prevent brain responses from being influenced by artifact recognition rather than semantic saliency. The set of 70,000 images were the starting point for sampling the stimuli, which was done in the grouping phase. We have extended appendix section A.1  of our revised manuscript to further clarify this.\n\n2. Regarding the first question on the expansion of our work beyond binary tasks: Yes, we believe that our method for learning embeddings is indeed applicable to non-binary tasks. Our model is designed to learn a versatile representation of EEG responses to a variety of stimuli, not restricted to binary classes. However, expanding our approach to non-binary tasks would require careful consideration of experimental design, from stimuli selection to data preprocessing, to ensure a strong and meaningful correlation between EEG signals and the stimuli.\n\n3. Regarding the second question on the potential application of our work with different types of stimuli: We see significant potential for applying our approach to stimuli beyond artificially generated facial images, primarily because it obviates the need for manual annotation of these stimuli. For various groups of images, one could use image encoders instead of latent codes as the representation of the stimuli, where the encoder can be trained on large image datasets. Similarly, it is feasible to employ stimuli in other modalities, such as text, audio, and video, leveraging existing large models that learn embeddings for these formats. \n\n4. Regarding the results in Section 4.2 and Table 2, and the relatively modest performance difference between EEGNet and our method, we have expanded the interpretation in the results section to highlight a key distinction: our model is trained without labels. In contrast, EEGNet and its variants, such as EEGNet Fusion and MI-EEGNet, are supervised models trained with ground truth labels. In our linear evaluation, even though a supervised linear classifier is trained on the frozen embeddings from our model, its performance is naturally expected to be somewhat inferior to models trained entirely under supervised conditions. This is consistent with findings in other studies, such as simCLR [1], where the ImageNet Top-1 accuracy of a fully supervised ResNet-50 is above 75%, compared to 69.3% for a linear classifier trained on top of simCLR embeddings, despite using the same architecture.\n\n[1] Chen, Ting, et al. \"A simple framework for contrastive learning of visual representations.\" International conference on machine learning. PMLR, 2020."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1665/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700572432270,
                "cdate": 1700572432270,
                "tmdate": 1700572432270,
                "mdate": 1700572432270,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "y0J2JnKOjC",
            "forum": "ul6EYKM1Kv",
            "replyto": "ul6EYKM1Kv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1665/Reviewer_yZyL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1665/Reviewer_yZyL"
            ],
            "content": {
                "summary": {
                    "value": "The proposed approach for self-supervised machine learning, which involves using human brain signals as direct supervisory signals, raises questions. The authors suggest a paradigm of cognition-supervised learning, achieved through leveraging EEG data to receive solid binary labels. However, using BCI-style responses for such self-supervised learning could be more ethically questionable and logically messy, as it depends on human involvement and may not have practical applications. Moreover, the study assumes involuntary or automatic responses from humans, which further raises ethical concerns."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "It is quite challenging to identify any strengths in the manuscript. The claims of self-supervision using \"hard P300\" labels provided by humans who were forced to label pictures in visually tiring conditions (all BCI P300-style experiments are actually exhausting) are not well-explained and validated. It is difficult to identify the novelty of the model, and the final results are not impressive."
                },
                "weaknesses": {
                    "value": "It is not clear how the concept adheres to the self-supervision philosophy, as efficient labels like those of P300 are actually human-generated labels. Self-supervised learning is typically considered a form of unsupervised learning where the model generates its own labels or annotations from the input data, without the need for external human-labeled data. This involves creating a pretext or auxiliary task that does not require human annotations. The model is trained to predict some aspect of the input data based on other parts of the same data. However, P300 responses use human-generated labels in an ethically questionable setup."
                },
                "questions": {
                    "value": "The standard 10-20 EEG electrode placement system consists of 21 electrodes. However, in this case, 32 electrodes were used. I am curious about the placement of these additional electrodes on the head and the reason for using 32 instead of the usual single Pz electrode. Moreover, I am concerned about the ethical implications of violating the Declaration of Helsinki by misusing human subjects."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Potentially harmful insights, methodologies and applications",
                        "Yes, Responsible research practice (e.g., human subjects, data release)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The concept of using individuals as \"involuntary labelers\" and potentially compromising privacy by manipulating difficult-to-control automatic reactions is ethically controversial. This questionable technology, which employs human faces, could infringe upon the privacy of subjects' sexual preferences and other personal information. The study's logic is also flawed, as it does not appear to involve self-supervised training, rather, it utilizes a convoluted approach to supervised training which misleads and violates the EEG experiment participants' ethical rights."
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1665/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699521229149,
            "cdate": 1699521229149,
            "tmdate": 1699636094331,
            "mdate": 1699636094331,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OsQJjY4TQ2",
                "forum": "ul6EYKM1Kv",
                "replyto": "y0J2JnKOjC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1665/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1665/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Regarding the definition of self-supervision, our model is trained without explicit labels; it uses only EEG data and images during training. Labels are employed solely for performance evaluation. Similar EEG-based self-supervised methods have been applied to sleep stage classification [1], emotion recognition [2] and pathology screening [3]. Our approach is similar to prior contrastive learning methods SimCLR [4] and supervised contrastive learning SupCon [5], where a contrastive loss is used in the self-supervised setting. However, our approach differs from these existing frameworks that focus on a single modality (e.g., EEG signals) and typically require large datasets and batch sizes (4096 for SimCLR, 2048 for SupCon) for effective training. In contrast, EEG data collection is challenging, and our dataset comprises approximately 4400 data pairs for each task post-artifact removal, which is considerably smaller.\n\nWhile EEG signals are indeed human-generated inputs, with participants actively engaged in the task, the mixed stimuli images which include both target and non-target semantics, make the task of decoding EEG signals to a binary label particularly challenging, more so than in typical supervised settings where labels are provided. In such supervised scenarios with complex visual stimuli, especially in true single-trial binary classification tasks where repetition of stimuli classes is not allowed, the typical accuracy often hovers around 0.7, and can be even lower in real-world applications, for example 0.78 in [6] for classifying human faces against objects, around 0.7 in [7] for within-subject and around 0.4 for cross-subject,  0.708 in [8] and below 0.6 for text stimuli in [9]. Our model achieves comparable performance without relying on any external labels.\nTo enhance the clarity, we added the above discussion and references  to the introduction and related work section of our revised manuscript.\n\n[1] Jiang, Xue, et al. \"Self-supervised contrastive learning for EEG-based sleep staging.\" 2021 International Joint Conference on Neural Networks (IJCNN). IEEE, 2021.\n\n[2] Mohsenvand, Mostafa Neo, Mohammad Rasool Izadi, and Pattie Maes. \"Contrastive representation learning for electroencephalogram classification.\" Machine Learning for Health. PMLR, 2020.\n\n[3] Banville, Hubert, et al. \"Uncovering the structure of clinical EEG signals with self-supervised learning.\" Journal of Neural Engineering 18.4 (2021): 046020.\n\n[4] Chen, Ting, et al. \"A simple framework for contrastive learning of visual representations.\" International conference on machine learning. PMLR, 2020.\n\n[5] Khosla, Prannay, et al. \"Supervised contrastive learning.\" Advances in neural information processing systems 33 (2020): 18661-18673.\n\n[6] Bagchi, Subhranil, and Deepti R. Bathula. \"EEG-ConvTransformer for single-trial EEG-based visual stimulus classification.\" Pattern Recognition 129 (2022): 108757.\n\n[7] Lawhern, Vernon J., et al. \"EEGNet: a compact convolutional neural network for EEG-based brain\u2013computer interfaces.\" Journal of neural engineering 15.5 (2018): 056013.\n\n[8] Ahmed, Hamad, et al. \"Object classification from randomized EEG trials.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.\n\n[9] Eugster, Manuel JA, et al. \"Predicting term-relevance from brain signals.\" Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval. 2014."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1665/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700572035258,
                "cdate": 1700572035258,
                "tmdate": 1700572035258,
                "mdate": 1700572035258,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "J3Zz0Xh8WW",
                "forum": "ul6EYKM1Kv",
                "replyto": "y0J2JnKOjC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1665/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1665/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Regarding the number of electrodes, their placement, and montage: The 10-20 electrode placement system, introduced in 1958, served as the standard relative head surface positioning method for many years. However, with the advent of multichannel EEG hardware systems, there has been a shift towards higher EEG electrode density. This shift led to the development and adoption of the 10-10 and later the 10-5 systems as new standards by the American Clinical Neurophysiology Society [Ref:soc1] and the International Federation of Clinical Neurophysiology [Ref:fed1]. Modern EEG equipment typically use 32, 64, or upto 128 channels. We utilize the standard 32 electrode set and placement. \nSpecifically, we used 32 equidistant electrodes situated at FP1, FP2, F7, F3, Fz, F4, F8, FC5, FC1, FC2, FC6, T7, C3, Cz, C4, T8, TP9, CP5, CP1, CP2, CP6, TP10, P7, P3, Pz, P4, P8, PO9, O1, O2, PO10, Iz, within the 10% system, as explained in the appendix A.1 of our revised manuscript. We also added a plot of the electrodes placement in the Appendix section A.1.\n\n[Ref:soc1] American Electroencephalographic Society. Guideline Thirteen: guidelines for standard electrode position nomenclature. J.Clin.Neurophysiol 1994;11:111\u2013113.\n\n[Ref:fed2] Nuwer M R ,Comi G ,Emerson R,et.al. IFCN standards for digital recording clinical EEG.Electroencephalogr Clin Neurophysiol 1998;106:259\u2013261.\n\nRegarding ethical concerns, we have worked with the highest ethical standards set for neuroscientific research and research involving human participants. Our experiments are accepted by the ethical review board of BLINDED FOR REVIEW with a full ethical evaluation of the experiments, intention of use and the handling and management of data. Our experiments fully comply with general ethical guidelines of the world medical association (declaration of Helsinki). The data acquisition experiments have been run by experienced neuroscience researchers. We have detailed these ethical considerations extensively in our paper in Ethics Statement. Additionally, our approach to decoding human brain responses for research is in line with practices in the field, as evidenced by related studies published in journals like Nature Neuroscience [nat1, nat2, nat3, nat4]. We have not employed any form of 'involuntary labeling' in our research, and we are puzzled by the reviewer\u2019s interpretation in this regard. We respectfully request a careful re-examination of the ethical discussions already present in our manuscript and expect a fair assessment of our work in the ICLR review process.\n\n[nat1] D\u00e9fossez, Alexandre, et al. \"Decoding speech perception from non-invasive brain recordings.\" Nature Machine Intelligence (2023): 1-11.\n[nat2] Tang, Jerry, et al. \"Semantic reconstruction of continuous language from non-invasive brain recordings.\" Nature Neuroscience (2023): 1-9.\n[nat3] Wolff, Michael J., et al. \"Dynamic hidden states underlying working-memory-guided behavior.\" Nature neuroscience 20.6 (2017): 864-871.\n[nat4] Van Ede, Freek, et al. \"Concurrent visual and motor selection during visual working memory guided action.\" Nature Neuroscience 22.3 (2019): 477-483.\n\n\nIn summary, we are sorry to state this, but this review is not fair and the reviewer does not seem to be knowledgeable about the EEG specifics  nor have carefully read the ethical protocol that we have described in the paper, or is not familiar with this type of research. We hope that the reviewer and the area chair can reconsider this review and reflect it according to our responses."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1665/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700572079381,
                "cdate": 1700572079381,
                "tmdate": 1700572079381,
                "mdate": 1700572079381,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fvwCsVJ2Fp",
                "forum": "ul6EYKM1Kv",
                "replyto": "y0J2JnKOjC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1665/Reviewer_yZyL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1665/Reviewer_yZyL"
                ],
                "content": {
                    "title": {
                        "value": "No change in the reviewer decision"
                    },
                    "comment": {
                        "value": "The decision of the reviewer remained unchanged, even after the authors provided feedback. Although the authors responded in detail, they were unable to provide sufficient justification for their use of self-supervision. The use of the human brain as an external \"labeler\" by the authors raised ethical concerns that were not adequately addressed, despite obtaining a successful local ethical review. The reviewer refrained from commenting on the authors' evaluations of the EEG experience. Additionally, the reviewer carefully considered the remaining reviews with feedback, but they did not change the original decision."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1665/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638587522,
                "cdate": 1700638587522,
                "tmdate": 1700638944355,
                "mdate": 1700638944355,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kjqSDzkV5G",
                "forum": "ul6EYKM1Kv",
                "replyto": "PMKmkXuRNH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1665/Reviewer_yZyL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1665/Reviewer_yZyL"
                ],
                "content": {
                    "title": {
                        "value": "Further clarification"
                    },
                    "comment": {
                        "value": "Thank you very much for getting back to me so quickly from the senior co-author of the submission. The reviewer agrees that the area chair could provide helpful comments. It is possible to counteract a single negative review in such cases.\n\nTo clarify, the reviewer does not question the ethics of the particular experiment reported in the submission. The classical P300 experiment, which involved mentally discriminating targets from non-targets, was probably completed according to worldwide ethical standards. The issue lies with the broader moral implications of such experiments and applications, where humans \"could be used\" as more or less cooperating entities, potentially in a setup similar to the one in the movie THE MATRIX. As ICLR is a highly influential conference, such publications could cause more harm to the field despite already published papers on similar subjects. Unfortunately, the humble reviewer is not in a position to comment on them. If the authors could persuade the reviewers that such technology could bring more good than harm, it would convince the reviewer yZyL to reconsider the evaluation. The choice of P300, probably the best in the BCI/EEG field, was brilliant. However, the solid ERP somehow killed the concept of self-supervision.\n\nThis issue of \"self-supervision\" has also caused concern in the reviewer's h8j5 recent comment. Therefore, this is another point where the area chair could help decide."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1665/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700650605217,
                "cdate": 1700650605217,
                "tmdate": 1700650605217,
                "mdate": 1700650605217,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]