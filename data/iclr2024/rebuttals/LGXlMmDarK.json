[
    {
        "title": "On the Stochasticity in Graph Neural Networks"
    },
    {
        "review": {
            "id": "HBJG1P69JE",
            "forum": "LGXlMmDarK",
            "replyto": "LGXlMmDarK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3832/Reviewer_7ZWa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3832/Reviewer_7ZWa"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript studies the role of structural stochasticity (where edge strength can be perturbed, but no edge is added in the original adjacency matrix) in Graph neural networks (GNNs). Specifically, the authors claim that the limited expressiveness, oversmoothing, and over-squashing problems can be remedied with the adoption of the structural stochasticity. Based upon this observation, the author suggests a design principle of the structure of the GNNs with the stochasticity."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The problem is of sufficient interest since limited expressiveness, oversmoothing, and over-squashing problems on GNNs are significant issues that need to be resolved, and empirically, engineers have tried to put randomness to resolve these issues. Hence, the objective is of importance to analyze the role of the structural stochasticity and quantify how much this helps in improving performance."
                },
                "weaknesses": {
                    "value": "1. There are rooms for improvement on the statements of Theorems 3 & 4: The statements do not directly tell us that oversmoothing and oversquashing can be alleviated. Both contain inequalities between the one with stochasticity and the original one. If the difference is marginal, as the number of layers $K$ increases, this would not affect the improvement. For example, if the values of the two are $1/2$ and $1/3$, then $(1/2)^K \\approxeq (1/3)^K$ even for the moderate $K$.\n2. Technical contributions seem to be rather limited: The proof that the authors relied on is the convexity of the activation functions (or that of the first derivative of the activations), which can be limited in practice. For example, sigmoid and tangent hyperbolic functions are not convex.\n3. The stochasticity assumption on the case where edge strength can be perturbed, but no edge is added in the original adjacency matrix, is bit limited.\n4. It would have been nice if the assumptions that the authors made is presented in a clearer manner (e.g., by explicitly put \"Assumption\" separately from the normal texts or Theorem/Lemma statement).\n5. Minor typo: p-5 stps -> steps"
                },
                "questions": {
                    "value": "1. p-2 $A[:,v, v] \\sim q(Z)$: Does this mean that for each feature (i.e., for each column) $A[v, v]$ only becomes zero? or the entire row $A[v, :]$ or column $A[: v]$ become zero?\n2. p-5: Does $K$ indicate the number of layers?\n3. p-5 in the proof of Theorem 2: For $\\ell = r+1$, is $r \\in \\{r_1, r_2\\}$?\n4. p-5 in the proof of Theorem 2: Does $\\mathcal{I}(\\cdot)$ indicate the indicator function?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3832/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3832/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3832/Reviewer_7ZWa"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3832/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698798284505,
            "cdate": 1698798284505,
            "tmdate": 1699636341179,
            "mdate": 1699636341179,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Bzd9vLD323",
                "forum": "LGXlMmDarK",
                "replyto": "HBJG1P69JE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3832/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3832/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Jensen's Gap; No convexity assumption; Improved assumption presentation."
                    },
                    "comment": {
                        "value": "Thank you so much, Reviewer `7ZWa`, for your constructive feedback! In the first stage of our manuscript revision, focusing on the theoretical presentation, we have incorporated your feedback to significantly overhaul the theoretical framework, which, we hope, has addressed your concerns on the novelty of our theoretical contribution.\n\n>The statements do not directly tell us that oversmoothing and oversquashing can be alleviated. Both contain inequalities between the one with stochasticity and the original one. If the difference is marginal, as the number of layers\u00a0\u00a0increases, this would not affect the improvement.\n\nTo address your concern, we have derived the Jensen's gap analytically for Theorem 3, which is related to the variance of the edge distribution---the higher variance correspond to slower decay in Dirichlet energy, which we also illustrate in Figure 2. With a sufficient structural noise, the difference is considerable and amplified as the number of layer increases. On the other hand, the Jensen's gap for Theorem 4 can also be related to the moments of the edge distribution, albeit the derivation is less intuitive.\n\n>Technical contributions seem to be rather limited: The proof that the authors relied on is the convexity of the activation functions\n\nWe have rewritten Theorem 3 and 4 to cleanly separate the roles of activation functions---they are not necessary conditions, as no activation or identity activation also suffices. On the other hand, concave activation functions like `sigmoid` and `tanh` might indeed break these theorems, so we included \"convex activation functions\" as one of the design principles in Section 4 (Architecture).\n\n>The stochasticity assumption on the case where edge strength can be perturbed, but no edge is added in the original adjacency matrix, is bit limited\n\nAs stated in the Limitation section, the scope of this paper only entails perturbation of edge weights, with no added edge, as that would require more assumptions on the nature of the graph which is not always available, as Zhang et al. 2018 has shown. \n\n>It would have been nice if the assumptions that the authors made is presented in a clearer manner\n\nWe have reworked the presentation of the theorems so that the assumptions are more clearly stated at the beginning of the theorems.\n\n>1. p-2\u00a0: $A[:, u, v]$ Does this mean that for each feature (i.e., for each column)\u00a0\u00a0$A[v,v]$ only becomes zero? or the entire row\u00a0\u00a0$A[v,:]$ or column\u00a0$A[:, v]$\u00a0become zero?\n\nThe later. We have rewritten the confusing paragraph.\n\n>2. p-5: Does\u00a0\u00a0$K$ indicate the number of layers?\n\nYes! We have clarified.\n\n> 3. p-5 in the proof of Theorem 2: For\u00a0$l=r+1$, is\u00a0$r\\in r_1, r_2$?\n\nYes! We have now clarified.\n\n> 4. p-5 in the proof of Theorem 2: Does\u00a0\u00a0indicate the indicator function?\n\nYes! We have made this clearer in the text now.\n\nReference:\nYingxueZhang,SoumyasundarPal,MarkCoates,andDenizU \u0308stebay.Bayesiangraphconvolutional neural networks for semi-supervised classification, 2018."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3832/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700201773467,
                "cdate": 1700201773467,
                "tmdate": 1700235015348,
                "mdate": 1700235015348,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ASpMa8ay7i",
                "forum": "LGXlMmDarK",
                "replyto": "HBJG1P69JE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3832/Reviewer_7ZWa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3832/Reviewer_7ZWa"
                ],
                "content": {
                    "title": {
                        "value": "Thank you very much for the detailed rebuttal"
                    },
                    "comment": {
                        "value": "Thank you very much for the detailed rebuttal. I have read the authors' rebuttals and the other reviews. Although the reviewer thinks that the implications of the results are interesting, it would be nice to see more concrete quantifications of Jensen's gap and how the proposed design of model will impact on the performance and addressed issues in a more concrete manner.\nTherefore, I will maintain the score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3832/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700693086909,
                "cdate": 1700693086909,
                "tmdate": 1700693100182,
                "mdate": 1700693100182,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gmH97Cl2qZ",
            "forum": "LGXlMmDarK",
            "replyto": "LGXlMmDarK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3832/Reviewer_cENw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3832/Reviewer_cENw"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors study how using stochasticity can improve GNNs. The model used in the paper is called Bayesian Rewiring of Node Networks (BRONX) which allows to quantify structural uncertainty using variational inference. In Theorem 2, they show how stochasticity can allow to go beyond WL test, thus improving GNNs power. In Theorem 3, how the randomness prevents oversmoothing, and Theorem 4 shows how it prevent oversquashing. The paper is concluded with experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- motivated problem\n- various experiments"
                },
                "weaknesses": {
                    "value": "- the paper is 'slow,' the contributions start from page 4\n- the theoretical result while being nice are not enough to address GNN issues"
                },
                "questions": {
                    "value": "I believe this is a nice paper but unfortunately the theoretical contributions of the paper are not enough for this venue.\n\n\n--------------------------------------------\nAfter the rebuttal: I appreciate the authors for their response and revision, as they made a lot of changes to improve the quality of the paper. As they partially addressed my questions/comments, I decided to slightly increase my score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3832/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3832/Reviewer_cENw",
                        "ICLR.cc/2024/Conference/Submission3832/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3832/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698902252013,
            "cdate": 1698902252013,
            "tmdate": 1700793154511,
            "mdate": 1700793154511,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4D2tghKiaT",
                "forum": "LGXlMmDarK",
                "replyto": "gmH97Cl2qZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3832/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3832/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Overhauled flow; enriched theoretical framework."
                    },
                    "comment": {
                        "value": "Many thanks again, Reviewer `cENw`, for your constructive feedback. According to your suggestion we have significantly improved the theoretical framework of our paper, as well as its presentation.\n\n>- the paper is 'slow,' the contributions start from page 4\n\nWe have overhauled the structure of the paper, with compressed introduction and preliminary sections and a new theoretical presentation framework, where we interweave the problems statements with theoretical evidence. Please let us know if you believe that this has improved the flow and readability of the paper.\n\n>- the theoretical result while being nice are not enough to address GNN issues\n\nWe have enriched our theoretical framework with new insights, including the derivation of Jensen's gap for Theorem 3 and the separation of convexity requirement for activation functions. We believe that these new insights justifies the empirical performance of stochasticity-incorporating GNNs and provides new guidelines for stochastic GNN design. We would be grateful if you would elaborate on why you think the previous results are not enough so we can further improve the manuscript to address your concerns."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3832/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700202326736,
                "cdate": 1700202326736,
                "tmdate": 1700202326736,
                "mdate": 1700202326736,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zFHRi7X8Q4",
            "forum": "LGXlMmDarK",
            "replyto": "LGXlMmDarK",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3832/Reviewer_NERp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3832/Reviewer_NERp"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on the structural stochasticity in graph neural networks (GNNs) and its benefits in addressing issues like limited expressiveness, oversmoothing, and over-squashing. It provides a theoretical framework justifying how structural uncertainty alleviates over-smoothing, over-squashing, and limited expressiveness in GNNs. It also discovers that stochastic GNNs can exceed the discriminative power of the Weisfeiler-Lehman tests. It introduces a method called BRONX, which quantifies the structural uncertainty in GNNs through variational inference. BRONX showcases competitive performance with other stochastic GNN models like GRAND and DropEdge with real-world experiments, highlighting the importance of edge uncertainty quantification and a principledly constructed amortized scheme for edge uncertainty."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Originality: The paper introduces the concept of structural stochasticity in graph neural networks (GNNs) and demonstrates its benefits in addressing limitations of traditional GNNs. It also presents the BRONX framework, which quantifies structural uncertainty in GNNs through variational inference, providing a novel approach to address these limitations.\n\n2. Significance: The paper's findings have significant implications for the field of GNNs. By injecting stochasticity into GNN structures, the paper shows that simple GNNs can outperform the Weisfeiler-Lehman test. The introduction of BRONX as a method to quantify structural uncertainty in GNNs also has practical implications for improving GNN performance.\n\n3. Clarity: The paper presents its findings and contributions in a clear and concise manner. It explains the theoretical arguments, the design of BRONX, and the real-world experiments in a way that is easy to understand."
                },
                "weaknesses": {
                    "value": "1. The paper does not thoroughly discuss the potential limitations or drawbacks of the BRONX framework. It would be valuable to address any potential challenges or trade-offs that researchers might encounter when using BRONX in different contexts.\n\n2. The paper lacks a comprehensive comparison with existing stochastic GNN models, limiting the understanding of how BRONX performs in relation to other approaches.\n\n3. The paper does not provide a comprehensive comparison of the proposed method, BRONX, with existing methods for quantifying structural uncertainty in GNNs. Without such a comparison, it is difficult to assess the effectiveness and superiority of BRONX in practical scenarios. \n\n4. This paper only considers a simpler case where edge strength are perturbed, but not cover the increase and decrease of the number of the edges."
                },
                "questions": {
                    "value": "1. Could you provide a more comprehensive comparison of BRONX with existing stochastic GNN models to better understand its performance and advantages?\n\n2. Could you explain what \"$\\mathcal {I}$\" means in equation (20)?\n\n3. Could you discuss any potential limitations or drawbacks of the BRONX framework? Addressing these challenges would enhance the understanding and applicability of the approach.\n\n4. It would be beneficial if you could discuss the scalability of the BRONX framework and its performance on larger datasets. This would provide a better understanding of its practical utility.\n\n5. It would be helpful if you could provide more detailed explanations and examples of the practical implementation of the BRONX framework to assist readers in applying it in real-world scenarios.\n\n6. There may be some minor errors in the Equation (29), which can be corrected."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3832/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698929926474,
            "cdate": 1698929926474,
            "tmdate": 1699636340981,
            "mdate": 1699636340981,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UWs5qMJJyS",
                "forum": "LGXlMmDarK",
                "replyto": "zFHRi7X8Q4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3832/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3832/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Emphasized theoretical and architectural limitations; Added Comparison and Complexity sections;"
                    },
                    "comment": {
                        "value": "Thank you, Reviewer `NERp`, for your thorough and detailed review. \nWe have incorporated your advice and significantly improved the manuscript.\n\n>The paper does not thoroughly discuss the potential limitations or drawbacks of the BRONX framework.\n\nWe have fleshed out the **Limitation** section, discussing the *theoretical* and *architectural* limitations of our framework.\nIn particularly, we have reiterated that we are only concerned with edge weight perturbation in this paper, with no new edges being added.\n\n>The paper lacks a comprehensive comparison with existing stochastic GNN models\n\n>The paper does not provide a comprehensive comparison of the proposed method, BRONX, with existing methods for quantifying structural uncertainty in GNNs.\n\nThanks for pointing this out. We have now emphasized in the paper that, BRONX treats the edges *individually* and assigns different distributions to different edges, which, to the best of our knowledge, is entirely novel and allows transductive learning.\nAll previous methods samples the prior distribution for all edges, making transductive learning impossible.\n\n>1. Could you explain what \"$\\mathcal{I}$\" means in equation (20)?\n\nIt is the indicator function. We have now clarified this. Thanks again for raising this.\n\n> 2. Could you discuss any potential limitations or drawbacks of the BRONX framework? Addressing these challenges would enhance the understanding and applicability of the approach.\n\nWe have reworked the limitations sections to reflect the limitations and future directions.\n\n>3. It would be beneficial if you could discuss the scalability of the BRONX framework and its performance on larger datasets. This would provide a better understanding of its practical utility.\n\nWe have now included a complexity analysis---the runtime and storage complexity scales linearly with regard to the number of edges, making it feasible for large systems.\n\n>4. It would be helpful if you could provide more detailed explanations and examples of the practical implementation of the BRONX framework to assist readers in applying it in real-world scenarios.\n\nWe have now expanded the Details section in the appendix. The python package is also distributed open-source.\n\n>5. There may be some minor errors in the Equation (29), which can be corrected.\n\nThank you! We have now corrected the errors!"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3832/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690342277,
                "cdate": 1700690342277,
                "tmdate": 1700690342277,
                "mdate": 1700690342277,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]