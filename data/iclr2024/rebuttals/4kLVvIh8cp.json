[
    {
        "title": "Pessimistic Nonlinear Least-Squares Value Iteration for Offline Reinforcement Learning"
    },
    {
        "review": {
            "id": "9dAZvKIrYg",
            "forum": "4kLVvIh8cp",
            "replyto": "4kLVvIh8cp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8813/Reviewer_38or"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8813/Reviewer_38or"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers offline RL with non-linear function approximation. The authors propose a new pessimism-based algorithm, Pessimistic Nonlinear Least-Square Value Iteration, to solve this problem. The proposed algorithm is oracle-efficient, and achieves instance-dependent regret bound characterized by the newly-proposed $D^2$ divergence."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper proposes an algorithm for offline RL with non-linear function approximation that has instance-dependent regret guarantees, which is new in literature.\n- The author extend techniques used for linear MDPs (including variance-weighted ridge regression and reference-advantage decomposition) to nonlinear function classes.\n- The paper is generally well-written and clear."
                },
                "weaknesses": {
                    "value": "- This paper poses a very strong converage assumption (Assumption 3.5), which may not be realistic in practice. In contrast, most pessimism-based offline RL papers that I'm aware of adopt weaker partial coverage assumptions. I wonder how valuable it is to prove an instance-dependent bound when we have to impose a uniform coverage condition.\n- The lower bound cited by the paper only works for linear cases. The authors do not provide a matching lower bound to showcase the optimality of the proposed algorithm with general function approximations.\n- The authors do not provide any numerical experiments in the paper. In my opinion, adding numerical experiments can better showcase the benefits of the proposed algorithm compared to other offline RL algorithms with general function approximation."
                },
                "questions": {
                    "value": "- Which part of the proof requires uniform data coverage while partial data coverage does not work?\n- This paper considers general function approximation, yet I'm unclear what types of function classes & instance structures will satisfy the conditions listed in the paper. Can you give some concrete examples beyond linear MDPs?\n- Can you provide a lower bound for general function approximations?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8813/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698700341832,
            "cdate": 1698700341832,
            "tmdate": 1699637108768,
            "mdate": 1699637108768,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BAQKbZpyuI",
                "forum": "4kLVvIh8cp",
                "replyto": "9dAZvKIrYg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8813/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 38or"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback. We address your questions point-by-point.\n\n**Q1**: How valuable is it to prove an instance-dependent bound when we have to impose a uniform coverage assumption? Which part of the proof requires uniform data coverage while partial data coverage does not work?\n\n**A1**: In this work, we aim to achieve an optimal dependency on the complexity of the function class. For this purpose, we utilize the technique of reference-advantage decomposition to overcome the challenge posed by the additional error from uniform convergence over the function class $\\mathcal F_h$. However, the uniform coverage assumption is essential in the reference-advantage decomposition when we prove the advantage part is dominated by the reference part. In detail, in the proof of Lemma F.1 in Appendix G.1, we need the uniform coverage condition to bound the last inequality of Eq. (G.2). Note that Lemma F.1 provides an upper bound on the weighted $D^2$-divergence, and plays a pivotal role in proving Lemma F.2 through reference-advantage decomposition. The uniform coverage assumption is also required in existing works on the optimal complexity for offline RL with linear function approximation [1][2]. How to relax the uniform coverage assumption to partial coverage assumption while still achieving the optimal statistical efficiency remains an open problem and we will study it in the future.\n\n----\n**Q2**: Can you provide a lower bound for general function approximations?\n\n**A2**: That\u2019s a good question. Currently we don\u2019t know how to prove a lower bound for general function approximation in our framework. In most studies of RL with general function approximation, they do not provide a lower bound dependent on the complexity of the function class. As far as we know, the only exception is DEC [3], which is a framework for online RL and their application to offline RL remains open. How to obtain a lower bound for general function approximations is an interesting future direction, especially for offline RL.\n\n----\n**Q3**: The authors do not provide any numerical experiments in the paper. In my opinion, adding numerical experiments can better showcase the benefits of the proposed algorithm compared to other offline RL algorithms with general function approximation.\n\n**A3**: The main focus and contribution of our work is on the theoretical side of offline RL. We plan to implement the proposed offline RL algorithm using general function approximation and compare it with existing empirically strong baselines on offline RL benchmarks in our future work.\n\n----\n**Q4**: More concrete examples beyond linear MDPs.\n\n**A4**: Besides the linear MDP, the differentiable function class studied in [4] is also an example covered by our general function class. We have added a comment in Remark 3.7 in the revision. \n\n----\n[1] Xiong et al. 2023 Nearly minimax optimal offline reinforcement learning with linear function approximation: Single-agent MDP and markov game, ICLR.\n\n[2] Yin et a;. 2022a Near-optimal Offline Reinforcement Learning with Linear Representation: Leveraging Variance Information with Pessimism. ICLR\n\n[3] Foster et al. 2021  The statistical complexity of interactive decision making.\n\n[4] Yin et al. 2022,  Offline reinforcement learning with differentiable function approximation is provably efficient. ICLR"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471524276,
                "cdate": 1700471524276,
                "tmdate": 1700471524276,
                "mdate": 1700471524276,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gvpvvdTbGq",
                "forum": "4kLVvIh8cp",
                "replyto": "9dAZvKIrYg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8813/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow up with Reviewer 38or"
                    },
                    "comment": {
                        "value": "Dear Reviewer 38or,\n\nWe greatly appreciate your valuable feedback and are reaching out to see whether there have been any remaining questions that you have. If our response and revision have addressed your questions and concerns, we kindly request you to re-evaluate our work. Thank you!"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700619810794,
                "cdate": 1700619810794,
                "tmdate": 1700619810794,
                "mdate": 1700619810794,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nYCgyeHwxP",
                "forum": "4kLVvIh8cp",
                "replyto": "9dAZvKIrYg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8813/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Gentle Reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer 38or\n\nAs the deadline for the author-reviewer discussion is fast approaching, we would like to kindly inquire whether our response and revision have adequately addressed your questions and concerns. We really appreciate your feedback."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703115125,
                "cdate": 1700703115125,
                "tmdate": 1700703115125,
                "mdate": 1700703115125,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hXEPIE1fb7",
            "forum": "4kLVvIh8cp",
            "replyto": "4kLVvIh8cp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8813/Reviewer_Fsc5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8813/Reviewer_Fsc5"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an offline RL algorithm called Pessimistic Nonlinear Least-Square Value Iteration (PNLSVI) for general function approximation. It introduces a type of D$^2$-divergence to quantify the uncertainty of the offline dataset, and proves an instance-dependent regret bound that has a tight dependence on the function class complexity (its covering number)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ This work fills a gap of designing efficient offline RL algorithms with general function approximation.\n+ It extends the concept of D$^2$-divergence in online RL to offline RL.\n+ It generalizes reference-advantage decomposition to general function approximation."
                },
                "weaknesses": {
                    "value": "- The work is overall incremental. The key techniques are largely known, either from online RL or offline RL. I can see that there are technical barriers in directly extending them to the problem of offline RL with general function approximation, but such extensions are mostly not too difficult.\n- Improving the regret by a square-root of $d$ is quite standard for reference-advantage decomposition. This is more about the previous paper not doing the best job than developing truly novel method/analysis."
                },
                "questions": {
                    "value": "- How to interpret Thm 5.1? \n- When is Thm 5.1 better than instance-independent regret bounds?\n- Your coverage assumption is weaker, but does it really matter in practice?\n- You assume that dataset is produced by a single BP. In reality, this may not always be true. What is the impact of different BPs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8813/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698804863706,
            "cdate": 1698804863706,
            "tmdate": 1699637108647,
            "mdate": 1699637108647,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4DjfW1nDtm",
                "forum": "4kLVvIh8cp",
                "replyto": "hXEPIE1fb7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8813/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Fsc5"
                    },
                    "comment": {
                        "value": "Thank you for your positive feedback. We address your questions as follows.\n\n**Q1**: The work is overall incremental. The key techniques are largely known, either from online RL or offline RL. I can see that there are technical barriers in directly extending them to the problem of offline RL with general function approximation, but such extensions are mostly not too difficult.\n\n**A1**: We agree with the reviewer that the extension of RL with linear function approximation to general function approximation is in general not too difficult given so many recent developments. However, as acknowledged by the reviewer, there are still numerous technical barriers that require effort, and our work is dedicated to addressing these challenges. \n\nIn pursuit of optimal statistical complexity in offline RL, we delve into leveraging variance information within the general function approximation setting\u2014a previously unexplored domain. We believe our contribution in this regard is quite significant.\n\n----\n**Q2**: Improving the regret by a square-root of $d$ is quite standard for reference-advantage decomposition. This is more about the previous paper not doing the best job than developing truly novel method/analysis.\n\n\n**A2**: We agree with the reviewer that improving the regret by $\\sqrt{d}$ is anticipated when doing reference-advantage decomposition. However, such a technique has never been explored in offline RL with general function approximation, and we are the first to extend this decomposition technique to this setting. \n\n----\n**Q3**: How to interpret Thm 5.1?\n\n**A3**: Thm 5.1 establishes an upper bound for the suboptimality of our policy $\\hat \\pi$. It is an instance-dependent result because it depends on the expected uncertainty, which is characterized by the weighted $D^2$-divergence along the trajectory. Both the trajectory and the weight function are based on the optimal policy and the optimal value function, respectively. Moreover, our result has an optimal dependency on the complexity of the function class when it is specialized to the linear case, characterized by the cardinality of the function class $\\tilde O(\\sqrt{\\log \\mathcal N})$.\n\n----\n**Q4**: When is Thm 5.1 better than instance-independent regret bounds?\n\n**A4**: The suboptimality bound in Theorem 5.1 is always no worse than the corresponding instance-independent regret bounds under the same setting. In the worst case, $D_{ \\mathcal F_h}$ $(z_h,\\mathcal D_h, \\[\\mathbb V_h V^*_{h+1}\\](\\cdot,\\cdot)) $ in Theorem 5.1 can be bounded by $\\tilde O(\\frac{H}{\\sqrt{K\\kappa}})$ due to Lemma F.1, which reduces to the instance-independent regret bound. \n\n----\n**Q5**: Your coverage assumption is weaker, but does it really matter in practice?\n\n**A5**: Our goal in comparing various coverage assumptions is to provide a comprehensive review of the assumptions prevalent in the literature. Additionally, we want to ensure that our coverage assumption is no more stringent than that of related works, ensuring a fair comparison of the efficacy of our method against others. It is worth noting that, in practice, verifying the coverage assumption may be challenging. This is precisely why we opt for the weaker coverage assumption\u2014to maximize its practical satisfaction to the greatest extent.\n\n----\n**Q6**: You assume that dataset is produced by a single BP. In reality, this may not always be true. What is the impact of different BPs?\n\n\n**A6**: The key part of our proof is Assumption 3.5, where we make assumptions on the distribution of the dataset. For simplicity, we consider the single behavior policy situation, which satisfies Assumption 3.5. But it doesn't really matter as long as we have an assumption similar to Assumption 3.5."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700471185379,
                "cdate": 1700471185379,
                "tmdate": 1700471185379,
                "mdate": 1700471185379,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2c94kXFNiw",
                "forum": "4kLVvIh8cp",
                "replyto": "4DjfW1nDtm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8813/Reviewer_Fsc5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8813/Reviewer_Fsc5"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarification. I'll keep my score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660236995,
                "cdate": 1700660236995,
                "tmdate": 1700660236995,
                "mdate": 1700660236995,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gDkpfOnz29",
            "forum": "4kLVvIh8cp",
            "replyto": "4kLVvIh8cp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8813/Reviewer_KfKk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8813/Reviewer_KfKk"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies offline reinforcement learning with non-linear function approximation. It proposes an oracle-efficient algorithm, Pessimistic Nonlinear Least-Square Value Iteration (PNLSVI), for offline RL with non-linear function approximation. The algorithmic design comprises three innovative components: (1) a variance-based weighted regression scheme that can be applied to a wide range of function classes, (2) a subroutine for variance estimation, and (3) a planning phase that utilizes a pessimistic value iteration approach. The algorithm enjoys a regret bound that has a tight dependency on the function class complexity and achieves minimax optimal instance-dependent regret when specialized to linear function approximation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper proposes a pessimism-based algorithm Pessimistic Nonlinear Least-Square Value Iteration (PNLSVI) designed for nonlinear function approximation, which strictly generalizes the existing pessimism-based algorithms for both linear and differentiable function approximation (Xiong et al., 2023; Yin et al., 2022b). The algorithm is oracle-efficient, i.e., it is computationally efficient when there exists an efficient regression oracle and bonus oracle for the function class (e.g., generalized linear function class). In addition, this paper introduces a new type of D2-divergence to quantify the uncertainty of an offline dataset, which naturally extends the role of the elliptical norm seen in the linear setting and the D2-divergence."
                },
                "weaknesses": {
                    "value": "1. Even though there is an Appendix C explaining the computational aspect of computing the bonus, it seems only address the first bullet point in 4.3. How to computationally efficiently obtain the condition for the second bullet point in 4.3?\n\n2. This paper claims it generalizes over the differentiable parametric models in [Yin et al. 22b], would the main theorem 5.1 improves the results obtained in  [Yin et al. 22b]? \n\n3. This paper seems to be closely related to [Alekh et al. 23], however is not enough discussion about. May I consider this paper as an offline version of [Alekh et al. 23]? If not, what are differences?"
                },
                "questions": {
                    "value": "Please answer the questions above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8813/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817461447,
            "cdate": 1698817461447,
            "tmdate": 1699637108515,
            "mdate": 1699637108515,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0PtTiJPIKN",
                "forum": "4kLVvIh8cp",
                "replyto": "gDkpfOnz29",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8813/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer KfKk"
                    },
                    "comment": {
                        "value": "Thank you for your positive feedback. We address your questions point-by-point.\n\n**Q1**: How to computationally efficiently obtain the condition for the second bullet point in 4.3?\n\n**A1**: In Appendix C, we actually discuss how to efficiently compute the optimization problem $\\max\\{ |f_1(z_h)-f_2(z_h)|, f_1,f_2 \\in \\mathcal F_h: \\sum_{k \\in [K] } \\frac{(f_1(z_h^k)- f_2(z_h^k))^2}{(\\hat \\sigma_h(s_h^k,a_h^k))^2} \\leq (\\beta_h)^2\\}$.  With the definition of the D^2 divergence, for any $f_1,f_2 \\in \\mathcal F_h $ satisfying $\\sum_{k \\in [K]}\\frac{1}{(\\hat\\sigma_h(z_h^k))^2}(f_1(z_h^k)-f_2(z_h^k))^2 \\le (\\beta_h^2)$, we have\n    $|f_1(z_h)-f_2(z_h)| \\le D_{\\mathcal F_h}(z;\\mathcal D_h; \\hat\\sigma_h^2) \\sqrt{(\\beta_h)^2 + \\lambda}$.\nTherefore, the second bullet is automatically satisfied. We have provided a more detailed explanation in Appendix C in the revision.\n\n----\n**Q2**: This paper claims it generalizes over the differentiable parametric models in [1]. Would the main theorem 5.1 improve the results obtained in [1]?\n\n**A2**: Yes, our Theorem 5.1 improves the results obtained in [1]. The main improvement is the dependency on the complexity of the function class. Consider the covering number (or the cardinality for finite function class) for the differentiable parametric model studied in [1], which is covered by our framework. It has been proved in [1] that the covering number is $\\tilde O(d)$, and their result has a linear dependency on $d$. Using our Theorem 5.1, we get a $\\tilde O (\\sqrt{d})$ dependency on $d$, which has a $\\sqrt {d}$ improvement compared with the results obtained in [1].\n\n----\n\n**Q3**: This paper seems to be closely related to [Alekh et al. 23], however is not enough discussion about. May I consider this paper as an offline version of [Alekh et al. 23]? If not, what are differences?\n\n**A3**: Alekh et al. 23 [2] is an inspiring work in the realm of online RL with general function approximation. We have drawn considerable insights from their research, particularly in the definition of $D^2$-divergence and the selection of the bonus function, which we have discussed. However, there is a key distinction beyond the apparent difference between the settings of online and offline RLs. Unlike the approach in [2], which involves a very intricate policy derived from the optimistic Q-value function or, under specific conditions, the overly optimistic Q-value function, our algorithm employs a simple greedy policy. This eliminates the need for an overly pessimistic Q-value function (the counterpart of the overly optimistic Q-value function as we\u2019re doing offline RL) in addition to the pessimistic Q-value function. Our approach simplifies the algorithm significantly, distinguishing it from a direct offline version of the online algorithm in [2].\n\n----\n[1] Yin et al. 2022,  Offline reinforcement learning with differentiable function approximation is provably efficient. ICLR\n\n[2] Agarwal et al. 2023, $VOQL$: Towards optimal regret in model-free rl with nonlinear function approximation. COLT"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470691345,
                "cdate": 1700470691345,
                "tmdate": 1700470691345,
                "mdate": 1700470691345,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "57QlmYvLUo",
            "forum": "4kLVvIh8cp",
            "replyto": "4kLVvIh8cp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8813/Reviewer_FxDp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8813/Reviewer_FxDp"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the problem of offline reinforcement learning in finite-horizon MDPs with general function approximation, from a theoretical perspective.\nThe main assumptions are:\n- Bellman completeness: completeness of the of the value-function hypothesis class under first and second order Bellman optimality operators, possibly with misspecification\n- Uniform coverage of the hypothesis class by the offline dataset, in a novel form suited to general nonlinear value function approximation.\n- Access to a computational oracle (nonlinear least-squares regression)\n\nThe algorithm is UCBVI-like and combines pessimism variance-weighted least squares regression, where in turn the variance is estimated pessimistically from a fold of the dataset. It is oracle-efficient.\n\nThe main theoretical result is an upper bound on the simple regret that scales with the square root of the logarithm of the cardinality (or covering number) of the hypothesis class, and with inverse data coverage as measured by a D^2 divergence. The latter makes the bound instance-dependent. In this way, an instance-dependent regret bound is achieved in a more general setting and with weaker assumptions w.r.t. previous works."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well written and clear. Theoretical claims are supported by detailed and well commented proofs, the main technical tools are highlighted and explained, and the assumptions are clear.\nThe summary of the state of the art is particularly complete and detailed. Both the strengths and the limitations of the work are properly highlighted and discussed."
                },
                "weaknesses": {
                    "value": "I just have some minor remarks:\n1. In section 3 I think you went a bit too far with the abuse of notation when defining Bellman operators. Is f a function of the state or the state and action? How can you define the relationship between the Q and the V function of a fixed policy by Bellman's *optimality* operator?\n2. Assumption 3.2 is followed/complemented by other assumptions that are just given in-line. I suggest to state them as separate assumptions to improve clarity.\n3. You define $\\mathcal{N}$ as the cardinality of the hypothesis class but refer to it as the \"covering number\". The two are not the same, and the only covering argument that I could find was in Remark 5.3. for the linear case. \n\nTypos:\n- page 4 \"closed to the optimal value function\"\n- page 6: \"construct this variance estimator with...\""
                },
                "questions": {
                    "value": "Could you give an intuitive reason for why you also need an *under*estimation of the variance? Also, does it make sense to call it \"pessimistic\" in this case? A smaller variance estimate has the effect of inflating the value estimate since you are using inverse variance weights, so it would seem more optimistic than pessimistic."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8813/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699195013667,
            "cdate": 1699195013667,
            "tmdate": 1699637108313,
            "mdate": 1699637108313,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vJFKgN7zy8",
                "forum": "4kLVvIh8cp",
                "replyto": "57QlmYvLUo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8813/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8813/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer FxDp"
                    },
                    "comment": {
                        "value": "Thank you for your strong support. We will address your questions as follows.\n\n**Q1**: In section 3, I think you went a bit too far with the abuse of notation when defining Bellman operators. Is $f$ a function of the state or the state and action? How can you define the relationship between the Q and the V function of a fixed policy by Bellman's optimality operator?\n\n**A1**: Thank you for your suggestion. We have defined $f(s) = \\max_{a}f(s,a)$ and added a footnote to clarify the two slightly abused notations $f(s)$ and $f(s,a)$. \n\nFor the Bellman operator, we have two separate equations. One is the Bellman equation for value functions of a fixed policy. The other is the Bellman optimality equation for optimal value functions.\n\n----\n**Q2**: Assumption 3.2 is followed/complemented by other assumptions that are just given in-line. I suggest to state them as separate assumptions to improve clarity.\n\n**A2**: Thanks for your suggestion. We have revised it to be two separate assumptions (i.e., Assumptions 3.2 and 3.3): realizability and completeness.\n\n----\n**Q3**: You define $\\mathcal{N}$ as the cardinality of the hypothesis class but refer to it as the \"covering number\". The two are not the same, and the only covering argument that I could find was in Remark 5.3. for the linear case.\n\n**A3**: Sorry for the confusion. In this paper, for simplicity, we assume the function class is finite, and we simply use $\\mathcal N$ to denote the cardinality of the hypothesis class. For the infinite function class, we can use the covering number to replace the cardinality. Note that the covering number will be reduced to the cardinality when the function class is finite. We have added a clarification to avoid any misunderstanding.\n\n----\n**Q4**:  Intuitive reason for why you also need an underestimation of the variance. A smaller variance estimate has the effect of inflating the value estimate since you are using inverse variance weights, so it would seem more optimistic than pessimistic.\n\n**A4**: In fact, we only need an accurate enough estimation of the variance. In Eq. (6.1), we prove that the variance estimator is very close to the true variance (same order for large enough $K$). Therefore, we can use it as the weight (more precisely, inverse weight) for weighted regression. We have removed \u201cpessimistic\u201d from \u201cpessimistic variance estimator\u201d to avoid any confusion.\n\n----\n**Q5**: Typos: page 4 \"closed to the optimal value function\"\npage 6: \"construct this variance estimator with...\"\n\n\n**A5**: Thanks for pointing out our typos. We have fixed them."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8813/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470313298,
                "cdate": 1700470313298,
                "tmdate": 1700470313298,
                "mdate": 1700470313298,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]