[
    {
        "title": "Scalable Long Range Propagation on Continuous-Time Dynamic Graphs"
    },
    {
        "review": {
            "id": "s02HCaQvxo",
            "forum": "l6eA8Srlqd",
            "replyto": "l6eA8Srlqd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5881/Reviewer_Bq6a"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5881/Reviewer_Bq6a"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents Continuous-Time Graph Anti-Symmetric Network as a new graph neural network for Continuous-Time Dynamic Graphs. The authors build upon ideas presented in the literature that oversmoothing can be avoided if the eigenvalues of the dynamics are chosen carefully -- thereby facilitating long-range information transfer."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well written, and clearly articulates an important problem for GNNs as well as a potential solution.\nThe proposed solution is not entirely new, but appears to be well implemented."
                },
                "weaknesses": {
                    "value": "- While the benefits of having a non-dissipative dynamics is discussed, the disadvantages are not discussed very prominently.\nIf all modes are essentially undampended there is also no filtering of noise -- the tradeoffs here should be discussed. Even better would be an experiment that considers the robustness of these ideas when faced with (adversial or even just random) noise.\n- Relating to the previous point, the authors actually have added a dissipative term (-yI) when discretizing their dynamics via an Euler discretization -- I think they should discuss the importance of this term.\n- Proposition 1 is essentially standard linear algebra; calling this a proposition and adding a formal proof seems to be somewhat disproportionate to what this does -- I suggest the author can simply discuss this.\n- The temporal aspects of the data are almost never used; the authors simply use a \"temporal\" interpretation -- this raises some questions:\na) why not compare with other (non-temporal) architectures as baselines that can deal with this kind of data?\nb) what benefits does the architecture really bring to temporal graphs? It appears to me that the contribution provided here is in some sense orthogonal to the fact that temporal graphs are considered as data?!\nPlease discuss these aspects further."
                },
                "questions": {
                    "value": "See above. \n\nIn particular, I would encourage the authors to more clearly articulate the trade-offs that come with imaginary eigenvalues (e.g., while no energy is dissipated but oscillations can arise) and how they address this trade-off.\nMoreover, it seems that the temporal aspects of the data is actually not really central to the exposition at hand -- in what parts does this play a strong role? This should be discussed more."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5881/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5881/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5881/Reviewer_Bq6a"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5881/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698658437413,
            "cdate": 1698658437413,
            "tmdate": 1700398324844,
            "mdate": 1700398324844,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8RIEngAslG",
                "forum": "l6eA8Srlqd",
                "replyto": "s02HCaQvxo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5881/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5881/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer Bq6a (part 1/3)"
                    },
                    "comment": {
                        "value": "Our first acknowledgement goes to the reviewer for the insightful feedback. In the following, we attend to each of the raised concerns one by one.\n\n___\"While the benefits of having a non-dissipative dynamics is discussed, the disadvantages are not discussed very prominently. If all modes are essentially undampended there is also no filtering of noise -- the tradeoffs here should be discussed. Even better would be an experiment that considers the robustness of these ideas when faced with (adversial or even just random) noise.\"___\n\nWe thank the reviewer for the comment, we have clarified this aspect in Appendix B and C. \nWe agree with the reviewer that a fully non-dissipative dynamic may not be ideal for every circumstance. For such a reason, it is possible to modulate CTAN's non-dissipative behavior, either by adjusting the $\\gamma$ value (as elaborated in Appendix C) or by introducing a dissipative $\\psi$ function (as discussed in Appendix B). \n\nThe reviewer makes a good point in suggesting testing the model under noise. Nevertheless, we believe that in our setup, where we train a model in a self-supervised way with backpropagation, the model should learn to only propagate the meaningful information while discarding the noise. To the reviewer\u2019s comment, not all nodes are undampened, only those that contribute to good predictions during training. This in turn means the model learns to compute robust representations of nodes that only incorporate the meaningful information. While we can not investigate this in depth in the limited rebuttal time, we would point out that the experiments in Section 5.1.1 specifically test the model\u2019s proficiency in propagating long-range information under random noise injection. In the experiment, all the intermediate information found in the linear graph consists of uniformly sampled feature values, and the model needs to be able to ignore it to correctly predict what was seen on the first node. The experiment shows that CTAN can filter out noise and learn to propagate only the meaningful information, thus demonstrating robustness to the injection of noise while propagating event information. \n\nTesting whether the model can recover meaningful node representations after seeing adversarial inputs is an interesting future direction, but we will not be able to address it in this paper.\n\n___\"Relating to the previous point, the authors actually have added a dissipative term $(-\\gamma \\mathbf{I})$ when discretizing their dynamics via an Euler discretization -- I think they should discuss the importance of this term.\"___\nWe have clarified the importance of the term $(-\\gamma \\mathbf{I})$ in the Euler\u2019s forward discretization in Appendix C. This term plays a crucial role in ensuring the stability of Euler\u2019s discretization. As outlined in [1], the Euler\u2019s forward method is considered stable when $(1+\\epsilon\\lambda(\\mathbf{J}(t)))$ lies within the unit circle in the complex plane for all eigenvalues of the system. However, since the eigenvalues of the Jacobian matrix are exclusively imaginary, it follows that $|1+\\epsilon\\lambda(\\mathbf{J}(t))| >1$, which makes the Eq. 3 unstable when solved using forward Euler\u2019s method. Under this circumstance, we opted to introduce a dissipative term to facilitate the stability of the discretization method.\nWe note that higher values of $\\gamma$ reduce the fully non-dissipative propagation behavior of CTAN, thereby enhancing its applicability to a broader range of situations where a fully non-dissipative dynamic is not essential.\n\n[1] Computer Methods for Ordinary Differential Equations and Differential-Algebraic Equations, Ascher et al., 1998\n\n___\"Proposition 1 is essentially standard linear algebra; calling this a proposition and adding a formal proof seems to be somewhat disproportionate to what this does -- I suggest the author can simply discuss this.\"___\n\nWe appreciate the reviewer\u2019s suggestion regarding Proposition 1. Our primary intention was to provide a clear and thorough explanation of the proposition, and we believe that the added proof contributes to achieving this goal. Nevertheless, we are open to removing the formal proof if the reviewer believes that a simpler discussion will improve the quality of our work."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5881/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700351603483,
                "cdate": 1700351603483,
                "tmdate": 1700351804174,
                "mdate": 1700351804174,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "imI90gsXvQ",
                "forum": "l6eA8Srlqd",
                "replyto": "s02HCaQvxo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5881/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5881/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer Bq6a (part 3/3)"
                    },
                    "comment": {
                        "value": "___\"Q1: I would encourage the authors to more clearly articulate the trade-offs that come with imaginary eigenvalues (e.g., while no energy is dissipated but oscillations can arise) and how they address this trade-off. Moreover, it seems that the temporal aspects of the data is actually not really central to the exposition at hand -- in what parts does this play a strong role? This should be discussed more.\"___\n\nWe have expanded on the tradeoff between dissipative and non-dissipative behavior in Appendix B and C. As reiterated earlier, we emphasize the significance of time in our framework. Eq. 2 is explicitly defined with respect to time, and while irregular timestamps are addressed through the decomposition of Eq. 2 into multiple sub-problems, the causality of events must be respected during event propagation. Therefore, temporality is a critical aspect of our framework. Propagating multiple events in parallel would lead to catastrophic performance, as each sub-problem heavily relies on previous events. In Appendix B, we provide a mathematical analysis comprising a newly added proposition that clarifies the property of non-dissipativity over both time and space of the proposed CTAN model.\n\n\n---\n[1] - Kumar et al., Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks, 2019\n\n[2] - Xu et al., Inductive Representation Learning on Temproal Graphs, 2020\n\n[3] - Rossi et al., Temporal Graph Networks for Deep Learning on Dynamic Graphs, 2020"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5881/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700351848161,
                "cdate": 1700351848161,
                "tmdate": 1700352252434,
                "mdate": 1700352252434,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SUS7dXA1Ea",
                "forum": "l6eA8Srlqd",
                "replyto": "imI90gsXvQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5881/Reviewer_Bq6a"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5881/Reviewer_Bq6a"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their detailed responses."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5881/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700398308162,
                "cdate": 1700398308162,
                "tmdate": 1700398308162,
                "mdate": 1700398308162,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lBAyrIeWBH",
            "forum": "l6eA8Srlqd",
            "replyto": "l6eA8Srlqd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5881/Reviewer_udjB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5881/Reviewer_udjB"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new deep learning framework called Continuous-Time Graph Anti-Symmetric Network (CTAN) to address the problem of long-range propagation in continuous-time dynamic graphs (C-TDGs). The authors propose a theoretically motivated ODE-based framework to enable effective long-range propagation in dynamic graph learning. The paper also experimentally evaluates the proposed solutions on both synthesized and real-world networks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1. Modeling long-range dependencies is an important problem in dynamic graph learning.\n\nS2. The paper provides a theoretical analysis of how the anti-symmetric weight matrices ensure stability and non-dissipativeness of the ODE for long-range propagation.\n\nS3. The paper is generally well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "W1. The idea of using ordinary differential equations (ODEs) to model dynamic graphs is not novel.\n\nW2. The authors design the sequence classification on temporal path graphs to validate the algorithm. However, I have reservations about the rationale of basing the prediction of the initial node's features solely on the last node in the sequence. Such a design indeed increases the difficulty of the task, as it requires the model to effectively propagate long-distance dependencies. However, from a practical standpoint, it may not be quite reasonable, as it completely ignores the intermediate information. Moreover, a binary classification task with only two types of features is too simple.\n\nW3. More baselines should be included in the experiments. For example, [1-3].\n\n[1] Y. Wang, Y.-Y. Chang, Y. Liu, J. Leskovec, and P. Li. Inductive representation learning in temporal networks via causal anonymous walks. International Conference on Learning Representations, 2021.\n\n[2] W. Cong, S. Zhang, J. Kang, B. Yuan, H. Wu, X. Zhou, H. Tong, and M. Mahdavi. Do we really need complicated model architectures for temporal networks? arXiv preprint arXiv:2302.11636, 2023.\n\n[3] Y. Luo and P. Li. Neighborhood-aware scalable temporal network representation learning. In Learning on Graphs Conference, pages 1\u20131. PMLR, 2022."
                },
                "questions": {
                    "value": "See W1-W3 for details.\n\nMinor Comment\n\nThe authors claim to present a scalable method for propagating long-range dependencies in the title and introduction, but do not follow through with this assertion in the subsequent discussion. This might give the reader the impression that the author has not fully delivered on the promise of the title in the text. I would suggest the authors elucidate the method's scalability in the methodological discussion section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5881/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5881/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5881/Reviewer_udjB"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5881/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698911344403,
            "cdate": 1698911344403,
            "tmdate": 1699636623375,
            "mdate": 1699636623375,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SMAadbJwjF",
                "forum": "l6eA8Srlqd",
                "replyto": "lBAyrIeWBH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5881/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5881/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer udjB (part 1/2)"
                    },
                    "comment": {
                        "value": "We start by thanking the reviewer for their valuable comments.\n\n___\"W1. The idea of using ordinary differential equations (ODEs) to model dynamic graphs is not novel.\"___\n\nWe agree with the reviewer that ODE-based methods exist to model dynamic graphs, however, our CTAN framework results novel with respect to state-of-the-art methods in the C-TDG domain because of its modeling of long-range dependencies. Compared to previous work, CTAN can benefit from theoretical properties which guarantee non-dissipativeness both over the time and spatial dimension. For the former, we added a mathematical analysis comprising a newly added proposition that clarifies the property of non-dissipativity over both time and space of the proposed CTAN (see Appendix B). CTAN leverages non-dissipative ODE to propagate event information in C-TDGs by means of truncated non-dissipative propagation, which is a strategy never used in literature; it allows scalable information propagation radius; it results in a lightweight architecture that does not require the composition of multiple modules to model temporal and spatial information, thereby enabling scalable computation; and it is a general framework that allows leveraging the aggregation function that is more adequate for the specific task. Moreover, our framework focuses on long-range information propagation in C-TDGs, which is an open problem in this domain. While it is true that using ODE in dynamic graphs is not novel, we believe the C-TDG settings together with CTAN  properties and the above considerations make the method\u2019s novelty noteworthy.\n\n___\"W2. The authors design the sequence classification on temporal path graphs to validate the algorithm. However, I have reservations about the rationale of basing the prediction of the initial node's features solely on the last node in the sequence. Such a design indeed increases the difficulty of the task, as it requires the model to effectively propagate long-distance dependencies. However, from a practical standpoint, it may not be quite reasonable, as it completely ignores the intermediate information. Moreover, a binary classification task with only two types of features is too simple.\"___\n\nThe primary goal of the sequence classification task is to demonstrate models\u2019 capability to propagate information without dissipating it. A compelling approach to achieve this is by transferring information seen on an initial node along a linear graph, and to be able to use that information at a later point along such a graph. The reviewer is correct in pointing out that depending on the task at hand, information in C-TDGs might not always need to propagate this way, depending on how much high-order information is present in the graph and how much of it is noise. Nevertheless, we _specifically designed_ the Sequence Prediction task (Section 5.1.1) to test whether models exhibit smoothing or dissipative behavior when presented with noisy intermediate information. Models that fail at such task are incapable of propagating information far away, and therefore can not model high-order information, which CTAN is able to do. \n\nWhile we acknowledge the reviewer's perspective on the simplicity of a binary classification task, our empirical findings indicate that the examined literature methods exhibit notable inadequacies in effectively addressing this task: at linear graph size=7, all convolutional-based methods (DyRep, TGAT, TGN) can not predict correctly already (accuracy<60% on a balanced dataset). This corroborates the non-dissipative information propagation claims of CTAN which instead can propagate on longer sequences  with good accuracy.\n\n___\"W3. More baselines should be included in the experiments. For example, [1-3].\"___\n\nAs mentioned above, in response to the reviewer's valuable suggestion, we incorporated additional baselines into our experiments, and the corresponding results are now presented in Tables 9, specifically covering methods [1] and [2] pointed out in the review. We believe that this will ease the comparison with other methods providing a more comprehensive view of our proposed method's performance in comparison to the newly introduced baselines."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5881/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700351167948,
                "cdate": 1700351167948,
                "tmdate": 1700351167948,
                "mdate": 1700351167948,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ta3p7mG1m4",
                "forum": "l6eA8Srlqd",
                "replyto": "lBAyrIeWBH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5881/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5881/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer udjB (part 2/2)"
                    },
                    "comment": {
                        "value": "___\"Q1: The authors claim to present a scalable method for propagating long-range dependencies in the title and introduction, but do not follow through with this assertion in the subsequent discussion. This might give the reader the impression that the author has not fully delivered on the promise of the title in the text. I would suggest the authors elucidate the method's scalability in the methodological discussion section.\"___\n\nWe thank the reviewer for your insightful feedback. We appreciate the observation regarding the perceived discrepancy between the claim of presenting a scalable method for propagating long-range dependencies in the title and introduction and the subsequent discussion. We concede that the term might not have been used in the most intuitive way. In a number of related works (such as [2] and [DyGF]) the term \u2018scalable\u2019 refers to the methods __resources and computational complexity__, while our primary intent was to point out how the __range__ of information propagation can be scaled up by increasing the number of CTAN convolution layers, i.e. the non-dissipative event propagation can be scaled to reach more nodes by increasing the number of layers. To elucidate this property, we add an investigation in Appendix H showing how a larger number of Graph Convolution Layers improves the performance on the Sequence Classification task of Section 5.1.1, effectively _scaling up_ the range of propagation of the information present on the first node of the linear graph. Moreover, we emphasized the assertion of scalable non-dissipative propagation in the revised paper. We hope these enhancements address your concerns and contribute to a clearer understanding of our work.\n\n[DyGF] - Yu, et al., Towards Better Dynamic Graph Learning: New Architecture and Unified Library, NeurIPS 2023"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5881/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700351187036,
                "cdate": 1700351187036,
                "tmdate": 1700352102596,
                "mdate": 1700352102596,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HEsiPTUSKs",
                "forum": "l6eA8Srlqd",
                "replyto": "lBAyrIeWBH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5881/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5881/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comment to Reviewer udjB"
                    },
                    "comment": {
                        "value": "We have not heard from the reviewer yet. If further inquiries or clarifications are needed, please do not hesitate to ask. We hope that our comprehensive responses and inclusion of additional details and experiments have contributed positively to the manuscript. If so, we kindly ask the reviewer to consider adjusting the score accordingly."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5881/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582815177,
                "cdate": 1700582815177,
                "tmdate": 1700582955231,
                "mdate": 1700582955231,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "m8J3Y3rXMt",
            "forum": "l6eA8Srlqd",
            "replyto": "l6eA8Srlqd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5881/Reviewer_eCU8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5881/Reviewer_eCU8"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a deep graph network called CTAN on continuous-time dynamic graphs. The CTAN model is designed within the ordinary differential equations framework that enables efficient propagation of long-range dependencies. The paper shows that the CTAN model can robustly perform stable and non-dissipative information propagation over dynamic evolving graphs. The number of ODE discretization steps allows scaling the propagation range. The paper further presents empirical results to demonstrate the effectiveness of the CTAN model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "S1. The paper is well-motivated by the need for scalable GNN models capable of capturing long-range dependencies in continuous-time dynamic graphs.\n\nS2. The paper provides theoretical proof of the effectiveness of the proposed model.\n\nS3. The paper empirically validates the proposed model on many graph benchmarks. The experimental results illustrate the superiority of the proposed model. \n\nS4. The paper is generally well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "W1. The benchmark datasets are all of moderate size. Validating the model's performance on larger, dynamic datasets would be beneficial.\n\nW2. None of the C-TDG benchmarks include negative instances. The authors introduce negative sampling to the benchmark datasets by randomly sampling non-occurring links in the graph. However, the distributions of negative sampling can differ significantly from uniform distributions in real-world applications. Demonstrating the performance of the proposed model in handling negative instances across various distributions would be advantageous. \n\nW3. Some important baselines are omitted in the experiments. For example: \n- CAW: Inductive representation learning in temporal networks via causal anonymous walks. ICLR 2021\n- NAT: Neighborhood-aware scalable temporal network representation learning. LoG 2022."
                },
                "questions": {
                    "value": "Please refer to the Weaknesses part for details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5881/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5881/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5881/Reviewer_eCU8"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5881/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699104667852,
            "cdate": 1699104667852,
            "tmdate": 1699636623265,
            "mdate": 1699636623265,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cmvEzMazBi",
                "forum": "l6eA8Srlqd",
                "replyto": "m8J3Y3rXMt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5881/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5881/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "To begin, we thank the reviewer for providing feedback that allow us to improve our work.\n\n___\"W1. The benchmark datasets are all of moderate size. Validating the model's performance on larger, dynamic datasets would be beneficial.\"___\n\nWe would point out that at the time of experiments we could not find larger C-TDG benchmarks in the literature. At the time of submission, we would have hesitated to categorize the datasets used in Section 5.2 as moderate scale: to the best of our knowledge, these tasks were among the largest ones available (as substantiated by [1]).\n\nFollowing up on the reviewer\u2019s point, as of today, we are now aware of a large-scale benchmark [2] to appear at NeurIPS 23, thus we have introduced these new tasks to strengthen our work (see results in Appendix G, Table 9). We observe that given the time constraints involved in conducting all the experiments and the limited timeframe of the rebuttal phase, we regret that we are unable to execute a comprehensive and equitable model selection process, comparable to the procedures applied to other methods from the literature listed in the table (this model selection process would require weeks of GPU time, see wall time for epochs in Appendix G). Despite this limitation, it is noteworthy that our method demonstrates good performance on these new benchmark datasets.\n\n___\"W2. None of the C-TDG benchmarks include negative instances. The authors introduce negative sampling to the benchmark datasets by randomly sampling non-occurring links in the graph. However, the distributions of negative sampling can differ significantly from uniform distributions in real-world applications. Demonstrating the performance of the proposed model in handling negative instances across various distributions would be advantageous.\"___\n\nWe agree with the reviewer's insight regarding the advantages of sampling negative instances across various distributions. To address this, we extended the evaluation of CTAN to include the TGB benchmark [1]. TGB stands out not only as a large-scale benchmark for C-TDGs, but also as it presents particularly challenging tasks in which validation and test splits include the sampling of hundreds of negative edges for each positive edge from various distributions (both random and historical negatives are used). This contributes to a more generalized and realistic evaluation, contrasting with the random negative sampling approach used in our experiments of Section 5.2. The outcomes of these evaluations on the TGB benchmark are presented in Table 9, aiming to highlight CTAN's effectiveness in handling negative instances across diverse distributions.\n\n___\"W3. Some important baselines are omitted in the experiments. For example: CAW: Inductive representation learning in temporal networks via causal anonymous walks. ICLR 2021 NAT: Neighborhood-aware scalable temporal network representation learning. LoG 2022.\"___\n\nAs the reviewer suggested, we were able to include CAWN as part of the baselines in Table 9, where we perform evaluation on TGB. Due to the limited time in the rebuttal phase, we were not able to evaluate NAT in the paper, but we highlight how Table 9 now contains even more recent methods such as DyGFormer[3] and GraphMixer[4] (from 2023); we leave further comparisons for future work.\n\n---\n[1] Towards Better Evaluation for Dynamic Link Prediction, Poursafaei et al., 2022\n\n[2] - Huang at al., Temporal graph benchmark for machine learning on temporal graphs, NeurIPS 2023\n\n[3] - Yu, et al., Towards Better Dynamic Graph Learning: New Architecture and Unified Library, NeurIPS 2023\n\n[4] - Cong et al., Do We Really Need Complicated Model Architectures For Temporal Networks? ICLR 2023"
                    },
                    "title": {
                        "value": "Reply to Reviewer eCU8"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5881/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700350707166,
                "cdate": 1700350707166,
                "tmdate": 1700352052023,
                "mdate": 1700352052023,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gTSG5FP5BY",
                "forum": "l6eA8Srlqd",
                "replyto": "cmvEzMazBi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5881/Reviewer_eCU8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5881/Reviewer_eCU8"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors' rebuttal"
                    },
                    "comment": {
                        "value": "I appreciate the authors' detailed responses. I recommend that the authors incorporate the supplementary experiments provided in their rebuttal into any future versions of this manuscript (even in the appendix considering the page limits)."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5881/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700445002244,
                "cdate": 1700445002244,
                "tmdate": 1700445002244,
                "mdate": 1700445002244,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xML1v3O7oM",
            "forum": "l6eA8Srlqd",
            "replyto": "l6eA8Srlqd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5881/Reviewer_tkro"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5881/Reviewer_tkro"
            ],
            "content": {
                "summary": {
                    "value": "This work proposed a graph learning framework for continuous time dynamic graph learning, which utilize ANTI-SYMMETRIC DGN[1] to help CTDG method capture long-range information, by better model the evolution of the node memory. A special designed long range memorize task shows the effectiveness of the proposed method.\n\n[1] https://arxiv.org/abs/2210.09789"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. This paper porosed CTAN, which is a new deep graph network for learning C-TDGs based on ODEs. \n2. This paper presented novel benchmark datasets specifically designed to assess the ability of DGNs to propagate information over long spatio-temporal distances within C-TDGs"
                },
                "weaknesses": {
                    "value": "1.\tThe model is an extension from existing model A-DGN to continuous dynamic graph, though the paper claims that it is the first ODE-based architecture suitable for C-TDGs, most of the original ideas are same as the previous work, and the paper fails to contribute more on the method, which makes the novelty of the paper limited.\n2.\tThe paper's baseline models are \"too old and uncompetitive\", for example, there are several sequence based CTDG methods, for example, Graphmixer[1] and DyGFormer[2], that can capture long range dependency, the paper should consider compare with more up-to-date baselines to show the effectiveness.\n3.\tThe experiment in Table 2 can not support the claim that the model can capture long-term information well, for example, the performance of LastFM with Edgebank increases by the sequence length growth, and the proposed method fails to outperform edgebank.\n4.\tThe test setting on benchmark dataset is different to most existing methods, the paper only contains transductive setting, and neglects the inductive setting, the negative sample strategie is also different, makes me hard to compare the performance of proposed method with existing methods.\n\n[1] https://arxiv.org/abs/2302.11636\n\n[2] https://arxiv.org/abs/2303.13047"
                },
                "questions": {
                    "value": "1.\tWhat is the difference between the proposed method and the existing method A-DGN? Does dynamic graph learning task contain more strength of using such method?\n2.\tAs a memory based model, how can CTAN treat with inductive setting(cold start problem)?\n\n3. Can ODE-based encoder outperform other sequence learning encoder, when comparing with sequence-based CTDG methods like DyGFormer?\n4.\tSince there are three negative sample strategie mentioned in Edgebank, why does the method only compare with baseline models on random negative sample?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5881/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699331432999,
            "cdate": 1699331432999,
            "tmdate": 1699636623173,
            "mdate": 1699636623173,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YJgwRPIYSJ",
                "forum": "l6eA8Srlqd",
                "replyto": "xML1v3O7oM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5881/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5881/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer tkro (part 1/4)"
                    },
                    "comment": {
                        "value": "First, we would like to thank the reviewer for the feedback. Below, we address your comments one by one.\n\n___\"The model is an extension from existing model A-DGN to continuous dynamic graph, though the paper claims that it is the first ODE-based architecture suitable for C-TDGs, most of the original ideas are same as the previous work, and the paper fails to contribute more on the method, which makes the novelty of the paper limited.\"___\n\nKindly allow us to provide a more detailed explanation of the contributions and novelty in our work, emphasizing that it goes beyond being a mere extension of A-DGN.\n\nWhat relates CTAN to A-DGN is that A-DGN is an ODE-based model achieving non-dissipative propagation through static graphs, i.e., in the time-unaware spatial domain. With the goal of achieving non-dissipative propagation through C-TDGs, we identified and overcame challenges whose solutions we consider novel and which categorically separate CTAN from A-DGN.\n\nFirst of all, we note that time-aware nodes and edges combined with possibly irregularly sampled repetitive edges between the same pair of nodes natively render A-DGN (as well as other methods designed for static graphs) inapplicable to C-TDGs. While principally one could encode time as a node/edge feature in the static cases, attempts to do so have been shown to underperform temporally-aware models [5, 6]. Less trivially, non-dissipative propagation in C-TDGs cannot be achieved through mere non-dissipative propagation through space. On the contrary, non-dissipative propagation of information through time is a property unique to DGNs designed for C-TDG, necessary for their overall non-dissipativeness. To support our claims and better highlight our contributions, we added a mathematical analysis comprising a newly added proposition that clarifies the property of non-dissipativity over both time and space of the proposed CTAN model in Appendix B of our revised manuscript.\n\nFurthermore, our CTAN framework results also novel with respect to state-of-the-art methods in the C-TDG domain since it proposes the use of an ODE to propagate event information in C-TDGs by means of truncated non-dissipative propagation, which is a strategy never used in literature, allowing scalability in the information propagation radius. Moreover, CTAN results in a lightweight architecture obviating the need for complex module compositions to model temporal and spatial information, and offers a versatile framework adaptable to diverse aggregation functions tailored for specific tasks.\n\n\n___\"The paper's baseline models are \"too old and uncompetitive\", for example, there are several sequence based CTDG methods, for example, Graphmixer[1] and DyGFormer[2], that can capture long range dependency, the paper should consider compare with more up-to-date baselines to show the effectiveness.\"___\n\nWe would point out that DyGFormer has been accepted at NeurIPS 2023 (happening in Dec 23) and, unfortunately, was not available at the time of submission of our work.\n\nWhile both DyGFormer [1] and GraphMixer [2] may have increased capability to capture long-range dependencies, this is only applicable to __time__-only dependencies, and not spatial ones. DyGFormer models long-range time dependencies on node representations by fetching up to 4096 previous interactions (i.e., neighbors) for a node (\u201csequence length\u201d in the paper [1]). Similarly, GraphMixer considers up to 2000 previous interactions ($T$ in the paper). Both methods only rely on __first-hop__ neighbors information and do not consider spatial propagation of higher-order node information, which is in fact mentioned as a limitation of DyGFormer [1, Appendix A1].\n\nComparably, CTAN is still a graph _convolution_-based model, capable of propagating information in a non-dissipative way not only over time but also over the spatial dimension of the graph, scaling the range of propagation with the number of convolutions. This property enables propagating information to neighbors beyond the first-hop ones, which in turns allows solving tasks such as those in Section 5.1.1 and 5.1.2 in the paper. To provide further empirical evidence of CTAN\u2019s superior ability to propagate such information, we are doing our best to add GraphMixer as a baseline to the new long-range tasks proposed in Section 5.\n\nAs the reviewer suggested, for a sounder comparison with the new and more competitive methods, we added experiments on CTAN results on the Temporal Graph Benchmark [3], where (i) more recent baselines (GraphMixer and DyGFormer) (ii) larger datasets and (iii) different sampling strategies are available;  these results are in Table 9 in Appendix G. Overall, we found that CTAN is competitive with these two methods even at inferior # of parameters: we outperform GraphMixer on 2/3 datasets, and we outperform DyGFormer on 1/3 datasets. We added a brief discussion with some insights on such comparison."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5881/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700349397798,
                "cdate": 1700349397798,
                "tmdate": 1700352164940,
                "mdate": 1700352164940,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "m7idmJe6XD",
                "forum": "l6eA8Srlqd",
                "replyto": "xML1v3O7oM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5881/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5881/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer tkro (part 2/4)"
                    },
                    "comment": {
                        "value": "___\"The experiment in Table 2 can not support the claim that the model can capture long-term information well, for example, the performance of LastFM with Edgebank increases by the sequence length growth, and the proposed method fails to outperform edgebank.\"___\n\nWe appreciate your comment regarding any potential ambiguity in the presentation of experiments in Table 2, and we have clarified this aspect in the revised paper. In particular, we employed LastFM, MOOC, Reddit, and Wikipedia to demonstrate how well the model fares against baselines in real-world scenarios under an even playground of computational budget and sampled neighbors, without specifically focusing on long-range propagation. This comprehensive evaluation showcases the model's capabilities in diverse settings. The goal of the experiments in Section 5.2 was to show that when presented with the same amount of information and similar compute budget, CTAN can beat baselines thanks to its non-dissipative propagation properties. \n\nTo the reviewer\u2019s point, the claim that CTAN can capture long-range information is supported by the theoretical analysis we have provided throughout the paper and the experimental results achieved in particular in Section 5.1.1 and 5.1.2, which are long-range tasks by design. We can explain the fact that the performance of EdgeBank on LastFM is so exceptional with the observation that a memorization-based method such as EdgeBank is able to access the entire temporal adjacency matrix at the time of inference. In LastFM specifically, node historical neighbors are particularly dense and informative: a quick investigation shows that the median node degree before testing is 903 (1152 mean, 1722 std). This means that EdgeBank stores and uses 903 points of information per node, in comparison, CTAN and the other baselines in Section 5.2 use 5 neighbors.  As nodes tend to have larger degrees, sampling larger neighborhoods is fundamental to access and therefore retain information. This is not a limitation of CTAN but a limitation of the neighborhood sampling which is used in the majority of methods in the literature [JODIE, CTDNE, TGAT, TGN, GRAPHMIXER, TCL, CAWN]."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5881/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700350159967,
                "cdate": 1700350159967,
                "tmdate": 1700350474907,
                "mdate": 1700350474907,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1O8DqrK66I",
                "forum": "l6eA8Srlqd",
                "replyto": "xML1v3O7oM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5881/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5881/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer tkro (part 3/4)"
                    },
                    "comment": {
                        "value": "___\"The test setting on benchmark dataset is different to most existing methods, the paper only contains transductive setting, and neglects the inductive setting, the negative sample strategie is also different, makes me hard to compare the performance of proposed method with existing methods.\"___\n\nIn Section 5.2 we employed transductive setting and random negative sampling as it is done in JODIE [4], TGAT [5], TGN [6], EdgeBank [7] and GraphMixer [2]. We chose not to employ an inductive setting in those experiments, as it is not easily applicable to this type of graphs. Specifically:\n1) There is no clear consensus in the literature regarding the definition of inductive settings, making it difficult to identify the nodes considered for assessing this experimental setup (e.g. [6] differs from [5]).\n2) Some definitions of inductive settings may lead to situations where the number of sampled inductive nodes are not statistically relevant for evaluation.\n3) Other interpretations of inductive settings disrupt the true dynamics of the graph, i.e., in [6], certain nodes (and their associated edges) are removed from the training set with the purpose of isolating an inductive set of nodes.\n\nWe also observe, thanks to the analysis performed in [7], that among all the considered datasets in our paper there is mix of inductive and transductive edges, which can be measured with the  surprise index from [7], measuring the proportion of unseen edges at test time, which we report in this table:\n\n|                | T-PathGraph | T-PascalVOC10 | T-PascalVOC30 | Wikipedia | Reddit | LastFM | MOOC | tgbl-wiki-v2 | tgbl-review-v2 | tgbl-coin-v2 |\n|----------------|-------------|---------------|---------------|-----------|--------|--------|------|--------------|----------------|--------------|\n| Surprise index | 1.0         | 1.0           | 1.0           | 0.42      | 0.18   | 0.35   | 0.79 | 0.108        | 0.987          | 0.120        |\n\nComparing the method\u2019s performance to the surprise index in this table, one can see that our method can cope reasonably well even in fully inductive tasks, such as those in Section 5.1, i.e, T-PathGraph, T-PascalVOC10 T-PascalVOC30 where it generally ranks first among other baselines. We added these considerations to the paper in Appendix D where we present the datasets properties.\n\nAs mentioned above, to ease the comparison with other methods under a unified and larger setting, we also evaluate CTAN on the larger negative sample strategies included in TGB [3], and we report the results in Table 9 (Appendix G). In these experiments/datasets, the train split uses random negative sampling as has been done in the past, but during validation and test splits, for a single positive edge, _hundreds of negative edges_ are sampled from the space of possible edges. These larger negative sets form an even more general task than separating _historical_ and _inductive_ negative sampling proposed as in [7]. To conform with the TGB evaluation, these experiments measure Mean Reciprocal Rank rather than AuC, the former being a more fine-grained metric. We hope the results in Table 9 can clarify the ability of CTAN in handling negative instances across various distributions. \n\n___\"Q1: What is the difference between the proposed method and the existing method A-DGN? Does dynamic graph learning task contain more strength of using such method?\"___\n\nAs mentioned earlier, A-DNG presents an architecture designed for non-dissipative information propagation within static graphs, limiting its suitability only to the spatial domain. Conversely, the C-TDG setting necessitates the capacity to capture the spatio-temporal evolution of the graph and perform non-dissipative propagation across both space and time. Given that A-DGN does not account for graph evolution and is exclusively defined for spatial information propagation, it results as inadequate for effectively learning C-TDGs. On the contrary, our framework is capable to model spatio-temporal evolution of the graph, while achieving non-dissipative propagation of information in both space and time."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5881/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700350271251,
                "cdate": 1700350271251,
                "tmdate": 1700350406048,
                "mdate": 1700350406048,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0BN1UhQtOJ",
                "forum": "l6eA8Srlqd",
                "replyto": "xML1v3O7oM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5881/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5881/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer tkro (part 4/4)"
                    },
                    "comment": {
                        "value": "___\"Q2: As a memory based model, how can CTAN treat with inductive setting(cold start problem)?\"___\n\nWe thank the reviewer for the comment, we have clarified the considerations between inductive vs transductive settings in the response above and in the revised paper (appendix D). In practice, our method deals with inductive nodes similarly to other approaches such as TGN, JODIE, TGAT, and EdgeBank. Node embeddings initially only capture information related to the node\u2019s input state, which in absence of node features is a null vector. As links appear and connect an un-initialized node, its embedding will assimilate information about the evolution of the C-TDG. This propagation is backed by theoretical non-dissipative properties, which enable it to capture more informative information. In practice, the experiments of Section 5.1 confirm that even when working with completely unseen (inductive) nodes for which there is no information, CTAN can beat state-of-the-art methods\n\n___\"Q3: Can ODE-based encoder outperform other sequence learning encoder, when comparing with sequence-based CTDG methods like DyGFormer?\"___\n\nWe thank the reviewer for the question. Our experiments show that CTAN can outperform sequence learning encoders, such as JODIE, DyRep, and TGN, but we were not able to compare with DyGFormer [1] as the method is yet to appear in Neurips 2023, and was not available at the time of experiments. Keeping in mind the reviewer\u2019s suggestions, to ease the comparison with the most recent methods, we were able to evaluate CTAN on tasks from TGB [3] (see Table 9 in Appendix G for the results). Even in this scenario, CTAN shows good performance with respect to other methods (even at inferior # of parameters and restricted model selection process), proving the effectiveness of ODE-based encoders, and beating DyGFormer on one benchmark. Given the limited time in this rebuttal phase and the recency of the suggested DyGFormer method, we were not able to address the reviewer\u2019s question in depth. We discussed above why DyGFormer won\u2019t be able to propagate information along the spatial axis, but the reviewer makes a good point noting that DyGFormer performs well in propagating long-range time information.\n\n___\"Q4: Since there are three negative sample strategie mentioned in Edgebank, why does the method only compare with baseline models on random negative sample?\"___\n\nAs previously mentioned, in order to consider various negative sampling strategies as pointed out by the reviewer, we assess CTAN using the extensive negative edge sets provided in TGB [3]. In these experiments, the validation and test splits entail the sampling of hundreds of negative edges for a single positive edge. These expanded negative sets present a more generalized and realistic task compared to the random negative sampling employed in our experiments in Section 5.2. We report the results on such benchmarks in Table 9 with the aim of elucidating CTAN's proficiency in handling negative instances across diverse distributions.\n\n----\n[1] - Yu, et al., Towards Better Dynamic Graph Learning: New Architecture and Unified Library, NeurIPS 2023\n\n[2] - Cong et al., Do We Really Need Complicated Model Architectures For Temporal Networks? ICLR 2023\n\n[3] - Huang at al., Temporal graph benchmark for machine learning on temporal graphs, NeurIPS 2023\n\n[4] - Kumar et al., Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks, 2019\n\n[5] - Xu et al., Inductive Representation Learning on Temproal Graphs, 2020\n\n[6] - Rossi et al., Temporal Graph Networks for Deep Learning on Dynamic Graphs, 2020\n\n[7] - Poursafaei et al., Towards Better Evaluation for Dynamic Link Prediction, 2022"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5881/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700350391595,
                "cdate": 1700350391595,
                "tmdate": 1700350461618,
                "mdate": 1700350461618,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gve8E3AS3m",
                "forum": "l6eA8Srlqd",
                "replyto": "xML1v3O7oM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5881/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5881/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Comment to Reviewer tkro"
                    },
                    "comment": {
                        "value": "We have not heard from the reviewer yet. If further inquiries or clarifications are needed, please do not hesitate to ask. We hope that our comprehensive responses and inclusion of additional details and experiments have contributed positively to the manuscript. If so, we kindly ask the reviewer to consider adjusting the score accordingly.\n\nLastly, we would like to highlight that a supplementary evaluation of CTAN has been added in Appendix F, demonstrating that the performance of our method on the LastFM task is constrained solely by the range of hyperparameter values considered. This finding aligns with our earlier response. Notably, a 3.5% improvement in CTAN performance was achieved by merely increasing the sampler size."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5881/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582737693,
                "cdate": 1700582737693,
                "tmdate": 1700582917024,
                "mdate": 1700582917024,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]