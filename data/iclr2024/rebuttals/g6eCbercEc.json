[
    {
        "title": "InfoCon: Concept Discovery with Generative and Discriminative Informativeness"
    },
    {
        "review": {
            "id": "aX8VwZddno",
            "forum": "g6eCbercEc",
            "replyto": "g6eCbercEc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4476/Reviewer_v8jc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4476/Reviewer_v8jc"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method for extracting different manipulation concepts (i.e. grasp, align, insert) from expert trajectories of robot manipulation tasks. In essence, the algorithm unsupervisedly learns relevant subgoals to accomplish a task, and in addition, also provides a gradient signal for actions to achieve these (sub)goals. This is done by training two components: predicting the next subgoal given the current state, and training a compatibility function that indicates how \"compatible\" a state is with the desired subgoal, using contrastive learning. The gradient of the compatibility function can then be used to select actions, i.e. which action increases the goal compatibility given the current state. The method is evaluated on 4 tasks of the ManiSkill2 benchmark, and some qualitative examples are provided of the discovered subgoals."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Interesting ideas and approach to go from expert trajectories to subgoals, and in addition obtain policies to accomplish those subgoals."
                },
                "weaknesses": {
                    "value": "- The experimental results don't provide standard deviations, which makes it difficult to assess if there is any significant improvement compared to the presented baselines."
                },
                "questions": {
                    "value": "- What are the standard deviations on the results in Table 1. To what extent is InfoCon actually significantly better than the other baselines?\n\n- For some tasks (i.e. P&P Cube) the GT key states underperform the other approaches. Any insight on why the ground truth key states are insufficient to efficiently execute the task, and which are the \"extra\" subgoals identified by InfoCon that might explain this gap?\n\n- The model is trained on only 500 trajectories. To what extent is this overfitting to the train set, and does this explain the large gap between seen and unseen scenarios? Would this gap be closed by just adding more trajectories?\n\n- The VQ-VAE is pretrained on the trajectories without any task-related signal. Hence, the learned subsequences are merely clustered by visual appearance, rather than semantic relevance of being a valid \"subgoal\" for a task?  Wouldn't it make sense to also adjust the codes in the codebook based on e.g. how good one can predict a particular goal and/or how well-behaved a compatibility function is?\n\n- The policy is conditioned on the current state and the gradient of the compatibility function. Wouldn't it make sense to also condition the policy on the goal state, i.e. similar to e.g. https://arxiv.org/abs/2211.13350"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4476/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4476/Reviewer_v8jc",
                        "ICLR.cc/2024/Conference/Submission4476/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4476/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698396003261,
            "cdate": 1698396003261,
            "tmdate": 1700659997860,
            "mdate": 1700659997860,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dP5kBzIU6y",
                "forum": "g6eCbercEc",
                "replyto": "aX8VwZddno",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4476/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4476/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer v8jc,\n\nThank you for your comprehensive review and the valuable insights provided on our paper. We are grateful for your recognition of the innovative aspects of our approach in extracting manipulation concepts from expert trajectories. \n\nWe have provided detailed responses to each of your questions. We believe these responses will clarify the points you raised and provide a deeper understanding of our methodology and findings. Especially, on providing deviations, more analysis of the usefulness of the subgoals, ablation with more training demos, and discussion on the implementation details.\n\nWe hope that these clarifications can help finalize your assessment and the rating of our paper. Please also let us know if you have any further questions that we need to provide additional clarifications.\n\n***\n\n**W1, Q1: What are the standard deviations on the results in Table 1. To what extent is InfoCon actually significantly better than the other baselines?**\n\n**A**: Thanks for the question. We provide the statistics of the policy performance when the initial state of each evaluation sample is under perturbations, in order to simulate the noise happening in the real world. As can be seen from the following, the performance is relatively stable compared with our original results.\n\n| InfoCon | P&P Cube | Stack Cube | Turn Faucet | Peg Insertion |\n|:-|:-|:-|:-|:-|\n| Seen Env. | 96.3 \u00b1 0.1 | 63.2 \u00b1 0.4 | 53.5 \u00b1 0.3 | 65.0 \u00b1 1.4 |\n| Unseen Env. | 80.0 \u00b1 1.0 | 46.0 \u00b1 1.0 | 52.0 \u00b1 1.0 (sf.) 17.6 \u00b1 0.6 (uf.) | 17.4 \u00b1 0.4 |\n\n***\n\n**Q2: For some tasks (i.e. P&P Cube) the GT key states underperform the other approaches. Any insight on why the ground truth key states are insufficient to efficiently execute the task, and which are the \"extra\" subgoals identified by InfoCon that might explain this gap?**\n\n**A**: Thanks for the insightful comment. We have examined the discovered concepts in Fig. 8 in Sec. E of the updated paper. Please check the details in Sec. E for the experiment with the task of Peg-Insertion.\n\nIn contrast to only annotating a few key states (e.g., three, by human-defined rules), InfoCon can actually discover more fine-grained yet semantically meaningful key states (in Peg-Insertion, in total 8 concepts are discovered), which are shown below:\n\n* 1. The gripper is positioned above the peg (discovered concept \\#7).\n* 2. The gripper is aligned with the peg and ready to grasp (discovered concept \\#5).\n* 3. The peg is grasped (discovered concept \\#0).\n* 4. The peg is grasped and lifted (discovered concept \\#1).\n* 5. The peg is aligned with the hole distantly (discovered concept \\#4).\n* 6. The peg is aligned with the hole closely (discovered concept \\#6).\n* 7. The peg is inserted half-way into the hole (discovered concept \\#8).\n* 8. The peg is fully inserted (discovered concept \\#2).\n\nAs observed, InfoCon discovered extra key states, which are also helpful. For example, the extra key state: \u201cThe gripper is aligned with the peg and ready to grasp\u201d (item \\#2 above). This is a helpful key state that promotes a more stable process to successfully perform the whole peg insertion task (positioning the two handles of the gripper on two sides of the peg can have more guarantee on the next grasp process). This could be the main reason that the discovered concepts can perform better for policy training.\n\n***\n\n**Q3: The model is trained on only 500 trajectories. To what extent is this overfitting to the train set, and does this explain the large gap between seen and unseen scenarios? Would this gap be closed by just adding more trajectories?**\n\nBelow is an experiment of increasing training data of [CoTPC + InfoCon Key states] on task Peg Insertion from 500 to 800. It seems that the performance on unseen environments has improved. But the improvement on seen environments is relatively small. This indicates that the over-fitting gap can be narrowed by using more training data.\n\n|| 500 traj. | 800 traj. |\n|:-:|:-:|:-:|\n| Seen Env. | 63.6 | 64.3 |\n| Unseen Env. | 17.8 | 27.0 |\n\nSince all the experiment results we used from CoTPC are trained on 500 trajectories, we keep this number in our experiments for fairness."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4476/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578298137,
                "cdate": 1700578298137,
                "tmdate": 1700578298137,
                "mdate": 1700578298137,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vPFihVBSZ8",
                "forum": "g6eCbercEc",
                "replyto": "1Pmu8291G1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4476/Reviewer_v8jc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4476/Reviewer_v8jc"
                ],
                "content": {
                    "comment": {
                        "value": "I'd like to thank the authors for addressing my questions, and also for providing an extra experiment to improve the unseen case.\n\nI have increased my score accordingly."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4476/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660096491,
                "cdate": 1700660096491,
                "tmdate": 1700660096491,
                "mdate": 1700660096491,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "q06iAHfduQ",
            "forum": "g6eCbercEc",
            "replyto": "g6eCbercEc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4476/Reviewer_WSFG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4476/Reviewer_WSFG"
            ],
            "content": {
                "summary": {
                    "value": "This work, InfoCon, uses self-supervised learning method to discover the manipulation concepts in robotic tasks. The concepts are verified with semantic meaning in terms of human linguistics while saving much manual annotation efforts. This can be used as auxiliary task to support the encoding in the policy optimization. Experiements demonstrate that the policy trained based on these learned concepts can achieve the state-of-the-art results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. InfoCon can be self-supervised given state-action trajectory without human annotation, guided by network architecture VQ-VAE and informativeness objectives. Surprisingly, the self-supervised key states even performs better than the human GT in the COTPC for policy generation.\n\n2. The robot with InfoCon can discover abstract concepts themselves other than struggling with the grounding of concepts that are manually defined.\n\n3, The concepts of generative goal and discriminative goal are novel and beneficial to the trajectory encoding, which serves as the auxiliary task for self-supervision.\n\n4. Strong results in simulation comparing to extensive baselines."
                },
                "weaknesses": {
                    "value": "1, The proposed approach and concept of self-supervised manipulation concepts and key states seems closely related to the COTPC, as COTPC needs the key states. However, in the method description, there is no mention of COTPC. It may be possible to achieve co-optimization between policy generation and self-supervised manipulation concept. Moreover, can the proposed approach be general beneficial to policy optimization beside COTPC?\n\n2. The experience portion is a bit weak where only few tasks are evaluated. For the baseline, it should also include COTPC + other manipulation concept discovery for fair comparison.\n\n3. Lack of real robotic experiments. It is hard to judge if the proposed self-supervised approach works for the real-world complex tasks and videos."
                },
                "questions": {
                    "value": "1. As generative models, there are many VAE variants. Can you explain why you choose VQ-VAE architecture or include ablation study?\n\n2. As mentioned in the paper: the manipulation concept, key state, and state are random variables depending on the trajectory. Does the initialization of these variables affect the experimental results? Or other prototype network approaches help?\n\n3. How about the generalization capability of InfoCon? In the paper, the training and testing tasks are same: P&P Cube, Stack Cube, Turn Faucet and Peg Insertion."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4476/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698746979488,
            "cdate": 1698746979488,
            "tmdate": 1699636423398,
            "mdate": 1699636423398,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lxhUsDT5kx",
                "forum": "g6eCbercEc",
                "replyto": "q06iAHfduQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4476/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4476/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer WSFG,\n\nThank you for your constructive and insightful feedback on our manuscript. We appreciate your recognition of the novelty and effectiveness of InfoCon, particularly the self-supervised characteristics, its ability to discover manipulation concepts with semantic meaning, and its contribution to enhancing policy learning.\n\nBelow, we have provided clarification and extra experimental results to address the comments and questions. Especially on the discussion of the co-optimization, the benefits of the proposed to other policy learning schemes, and some ablations for further understanding of the proposed.\n\nWe hope that these clarifications can help finalize your assessment and the rating of our paper. Please also let us know if you have any further questions that we need to provide additional clarifications.\n\n***\n\n**W1.1: The proposed approach and concept of self-supervised manipulation concepts and key states seems closely related to the COTPC, as COTPC needs the key states. However, in the method description, there is no mention of COTPC.**\n\n**A**: Thanks for the comment. We put the details about CoTPC for training concept-guided policies in the training details section, and we will move this part to the method section in our final version.\n\n***\n\n**W1.2: It may be possible to achieve co-optimization between policy generation and self-supervised manipulation concept.**\n\n**A**: Thanks for the suggestion. We would like to explore this idea in future research. Currently, we do not perform joint training as the policy training would impose another layer of dynamics that could make it difficult to analyze the efficiency of the proposed generative and discriminative goal losses.\n\nWhile our experiments highlight InfoCon's strength in identifying key states, integrating this with policy generation is an exciting and unexplored area. We see a valuable opportunity for future work in developing a method that not only discovers key states using InfoCon but also generates a policy similar to CoTPC.\n\n***\n\n**W1.3: Moreover, can the proposed approach be general beneficial to policy optimization beside COTPC?**\n\n**A**: In this paper, we mainly employ CoTPC for the evaluation of the usefulness of the discovered key states, as currently, there are not many methods that focus on making use of key states in their decision-making policies with a tailored design. Here are some methods that are relevant to our knowledge:\n\n* CoTPC.\n* MaskDP[1]. However, it has a weakness: appending the end state into the input sequence is needed. The agent must know the exact key states based on the initial state, which makes it constrained to achieving a very specific goal and is not applicable to our problem setting, where the end goal state is not provided.\n* From the work of AWE [2], we found that DT performs best, so we present extra experiments on Decision Transformer [2] (DT), where we adapt DT to let it leverage key state prediction during the policy learning (our adaptation is based on the implementation in [3]).\n\nWe trained the adapted DT and tested on Peg Insertion and saw that policies based on InfoCon also have better performance compared with DT without key states or using ground truth (GT) key states. \n\nResults are in the table below.\nPeg Insertion | DT | DT+GT | DT+InfoCon |\n|:-:|:-:|:-:|:-:|\nSeen Env. | 5.6 | 34.6 | 52.0 |\nUnseen Env. | 2.0 | 6.3 | 7.8 |\n\nGiven this evidence, we believe that InfoCon is beneficial to the usage of key states in different strategies or policies.\n\n[1]. Liu, F., Liu, H., Grover, A., & Abbeel, P. (2022). Masked autoencoding for scalable and generalizable decision making. Advances in Neural Information Processing Systems, 35, 12608-12618.\n\n[2]. Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., ... & Mordatch, I. (2021). Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34, 15084-15097.\n\n[3]. Shi, L. X., Sharma, A., Zhao, T. Z., & Finn, C. (2023). Waypoint-based imitation learning for robotic manipulation. arXiv preprint arXiv:2307.14326.\n\n***\n\n**W2: it should also include COTPC + other manipulation concept discovery for fair comparison.**\n\n**A**: Thanks for the comment. We have included a few CoPTC+other manipulation concepts baselines (in total 4) in Table 1 of Section 3 (Experiments) in the manuscript (Last State, AWE, LLM + CLIP, GT Key States). We will carry out more experiments on this aspect in the final version.\n\n***\n\n**W3: Real-world examples**\n\n**A**: Thanks for the comment. Due to the time limit, we have run the proposed InfoCon on human pose sequences estimated from real-world videos to check its efficiency in discovering human motion concepts. A qualitative result is presented in Fig. 9. With this experiment, we show the potential of the proposed InfoCon in discovering key states from real-world data, but we are interested in training InfoCon on large-scale data when time permits."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4476/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700577889990,
                "cdate": 1700577889990,
                "tmdate": 1700577889990,
                "mdate": 1700577889990,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "il4ah2IFyK",
            "forum": "g6eCbercEc",
            "replyto": "g6eCbercEc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4476/Reviewer_mBdA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4476/Reviewer_mBdA"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose using vector quantization to discretize robot manipulation trajectories into sets of discrete sub-trajectory encodings that maximize proposed metrics on discriminative and generative informativeness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well written and the motivations and technical details are clear.  There are detailed evaluations and the proposed method is compared to multiple SOTA approaches.  The authors also include an ablation study and highlight a comparison of of human interpretability in addition to policy performance."
                },
                "weaknesses": {
                    "value": "How do the authors feel about the interpretability of manipulation concepts for human robot interaction?  Learned concepts may not be as understandable in an interaction.  I am curious about an opinion on being able to map the learned concept back to a semantic concept or how that can be integrated into the objective.  I know human intuition is covered in Table 2, and the authors state there is a weak correlation with policy performance, but there may be cases where the goal is to optimize for both.\n\nIn section 2, \"partition each trajectory into semantically meaningful segments\" should this be clarified on what \"semantically meaningful\" means?  This goes with the earlier stated motivation of moving away from human semantic discretization into self-discovered discretization that optimizes discriminative and generative informativeness.\n\nFor eq 1, how does the observed state sequence factor into the generative goal?"
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4476/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698870021540,
            "cdate": 1698870021540,
            "tmdate": 1699636423283,
            "mdate": 1699636423283,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ivM14jOZ86",
                "forum": "g6eCbercEc",
                "replyto": "il4ah2IFyK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4476/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4476/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer mBdA,\n\nThank you for your detailed review and valuable insights on our manuscript. We appreciate your recognition of the motivation of our method, your positive comments on the experimental performance, the clarity and thoroughness of the presentation, as well as the quality of evaluations. \n\nIn response to your concerns, particularly regarding the interpretability of manipulation concepts for human-robot interaction and the clarification of \"semantically meaningful\" segments, we offer detailed explanations and additional experiments.\n\nWe hope that these clarifications can help finalize your assessment and the rating of our paper. Please also let us know if you have any further questions that we need to provide additional clarifications.\n\n***\n\n**W1: How do the authors feel about the interpretability of manipulation concepts for human robot interaction? Learned concepts may not be as understandable in an interaction. \u2026 I am curious about an opinion on being able to map the learned concept back to a semantic concept or how that can be integrated into the objective.**\n\n**A**: Thanks for the insightful comment. Before the discussion on human-robot interaction, we would like to clear the ground by a clarification of the human intuition experiment performed in the paper. \n\nClarification: InfoCon aims at discovering concepts that can explain the \u201csub-goal\u201d in manipulation processes, instead of fitting into the preference or intuition of humans. As we can see, the key states related to humans\u2019 semantic concepts (GT key states, LLM+CLIP) do not help CoTPC policies better than key states identified by InfoCon. This indicates that InfoCon can also discover some key states which cannot be described easily using natural language, but still help us train better policies.\n\nBy saying the above, we like to emphasize that the human intuition experiment is performed with the GT concepts provided by CoTPC, which only assigns a few states with key state descriptions that can be described using simple words, like, align, grasp, insert. However, it does not mean that the key states discovered by InfoCon can not be described by human language to establish interpretability.\n\nNow get back to the question \u201cabout the interpretability of manipulation concepts for human robot interaction.\u201d\n\nWe fully agree that manipulation concepts need to be meaningful enough to be assigned with robust explanations. These will help with the interpretability and transparency of the learned policies. Moreover, we want to point out that the key concepts discovered by InfoCon are understandable. In Appendix E of the updated paper, we perform an experiment to assign discovered concepts with semantically meaningful descriptions.\nFor the Peg-Insertion task, the discovered concepts are:\n\n* 1. The gripper is positioned above the peg (discovered concept \\#7).\n* 2. The gripper is aligned with the peg and ready to grasp (discovered concept \\#5).\n* 3. The peg is grasped (discovered concept \\#0).\n* 4. The peg is grasped and lifted (discovered concept \\#1).\n* 5. The peg is aligned with the hole distantly (discovered concept \\#4).\n* 6. The peg is aligned with the hole closely (discovered concept \\#6).\n* 7. The peg is inserted half-way into the hole (discovered concept \\#8).\n* 8. The peg is fully inserted (discovered concept \\#2).\n\nFrom these experiments, we can see that the concepts are understandable. It is just that some concepts are more fine-grained and do not appear in the GT provided by CoTPC. These findings suggest that there exist fine-grained and semantically meaningful manipulation concepts that may require more words to describe but are critical for the efficiency of the manipulation policies. This also shows the effectiveness of the proposed self-supervised discovery metrics.\n\nPlease also refer to Fig.8 in Appendix E of the updated manuscript for more details.\n\nRegarding \u201can opinion on being able to map the learned concept back to a semantic concept or how that can be integrated into the objective.\u201d\n\nIn Appendix E of the updated paper, we performed an experiment to assign discovered key concepts with semantically meaningful descriptions. Since InfoCon can automatically discover those concepts from trajectories, now we have labeled all trajectories from Peg-Insertion using the provided description. We would like to use this as a baseline to map the learned concepts back to a semantic concept in the future. \n\nFurther, we would also like to prompt Large Language Models to propose semantic concepts and then optimize both the semantic compatibility generated by foundation models as well as the InfoCon criteria proposed to ensure manipulation efficiency and maximize human understanding of the discovered concepts.\n\n***"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4476/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700577658513,
                "cdate": 1700577658513,
                "tmdate": 1700577658513,
                "mdate": 1700577658513,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oyX0i8YW0a",
            "forum": "g6eCbercEc",
            "replyto": "g6eCbercEc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4476/Reviewer_1PDE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4476/Reviewer_1PDE"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript proposed InfoCon, a framework that can discover concepts in manipulation tasks automatically based on information reuseness. Specifically, the authors designed an analysis system that outputting discretized concepts from an offline manipulation dataset using a few different losses: generative goal loss, discriminative goal loss. Also, the side product from the architecture is the derivative of the discriminative loss, which represents the action to perform in order to complete the task."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The manuscript is nicely written. I can follow most of the parts.\n2. The evaluation covers a medium size of tasks and dataset, which is not perfect (not large-scale) but I believe is enough for showcasing in robot learning area.\n3. The results look impressive that beat previous method marginally, achieving either the first or second best result in the whole table."
                },
                "weaknesses": {
                    "value": "See question."
                },
                "questions": {
                    "value": "1. I am a little bit confused about the motivation of discriminative goal loss. How is it trained and why it is useful for extracting information from the input states?\n2. I am not fully confident about the fairness of the comparison. It seems to me that methods including LLM+CLIP are zero-shot learning process that does not require the training data to be seen. Am I correct or it's actually not the case? If so, I would suggest to separating them in the comparison. However, I still think the proposed method has technical contributions that is worth to present.\n3. Is there any real-world example to indicate the effectiveness?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4476/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4476/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4476/Reviewer_1PDE"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4476/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698971185939,
            "cdate": 1698971185939,
            "tmdate": 1699636423210,
            "mdate": 1699636423210,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7tvot0ATdA",
                "forum": "g6eCbercEc",
                "replyto": "oyX0i8YW0a",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4476/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4476/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 1PDE,\n\nThank you for your comprehensive review and the positive remarks on our manuscript. We highly appreciate your recognition of our efforts in proposing an automatic concept discovery framework and analysis system along with it.\n\nIn response to your insightful questions, we have addressed each of your concerns in detail in the following. We hope that these clarifications can help finalize your assessment and the rating of our paper. Please also let us know if you have any further questions that we need to provide additional clarifications.\n\n***\n\n**Q1.1: I am a little bit confused about the motivation of discriminative goal loss.** \n\n**A**: The discriminative goal is motivated by the observation that given a (physical) state (e.g., a hand holding a cup), one can assign a compatibility score to this state with varying goals. For example, if the goal is \u201cgetting water to drink,\u201d then the compatibility score of the state with the goal would be high; otherwise, if the goal is \u201cperforming yoga,\u201d the score would be low. In other words, a concept as a discriminative goal shall be able to help distinguish the state that is within the process of achieving this goal or not.\n\nThe above motivates us to formulate the concept as a discriminative goal, in the sense that the concept can be transformed into a discriminator conditioned on the concept.\n\nMoreover, still with the example of a state \u201ca hand holding a cup\u201d and another \u201ca hand holding a cup placed under the faucet,\u201d the discriminator instantiated by the concept \u201cgetting water to drink\u201d shall assign a higher score of the latter than the former one. This phenomenon illustrates that a concept in its discriminative form shall inform the change in the state (action) via its change in the score (gradient). This gives us the second term regarding the discriminative goal.\n\n***\n\n**Q1.2: How is it trained and why it is useful for extracting information from the input states?**\n\n**A**: How it is trained:\n\nEach manipulation concept is related to a vector. We decode this vector into a compatibility (scalar) function representing the discriminative goal of this concept via a hyper-network, which takes in the vector and outputs the weights of the concept-conditioned discriminator. \n\nThe concept-conditioned discriminator is then trained with two loss terms regarding the concept as a discriminative goal. The concept-conditioned discriminator function shall satisfy the two aspects described in Q1.1: grading states and providing states transition information using its gradient field. In other words, it should assign high scores to the states that are in the process of achieving this goal, and enable good prediction of the action with its gradient. These two losses are instantiated in the form of cross-entropy and L-2 reconstruction.\n\nWhy it is useful:\n\nWith the above loss terms, the VQVAE structure employed in our pipeline will get training signals to adjust both its encoder and the codebooks and learn to discover meaningful concepts (in the form of codes in the codebook). \n\nThe learning dynamic consists of two aspects. First, the encoder and the codebook will adjust to assign a state to a discriminator that assigns the highest score to it, which encourages clustering according to the discriminator. On the other hand, the discriminator shall encourage meaningful clusterings, otherwise, its gradients will not be informative of the action. For example, if the discriminator assigns all states to a single concept, then it is uniformly 1 over the state space, and its gradient is minimally informative of the action (state change). These correlations ensure that the whole training process is stable and the concept learned is non-trivial, promoting the usefulness of the learned concepts.\n\n***"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4476/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700577274045,
                "cdate": 1700577274045,
                "tmdate": 1700577274045,
                "mdate": 1700577274045,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]