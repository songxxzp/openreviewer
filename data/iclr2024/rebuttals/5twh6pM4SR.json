[
    {
        "title": "Automating Continual Learning"
    },
    {
        "review": {
            "id": "eFmLV1tMkT",
            "forum": "5twh6pM4SR",
            "replyto": "5twh6pM4SR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9084/Reviewer_StyD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9084/Reviewer_StyD"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to formulate continual learning as a sequence learning problem and applies self-referential weight matrices (SRWM), which can be considered a sequence model, as the key mechanism for continual learning.\nSRWM is a linear layer that produces self-modification as an auxiliary output."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I agree with the general direction of this paper that formulates continual learning as a sequence learning problem.\nThis idea of formulating a learning process as sequence learning has been used in the meta-learning literature, especially for few-shot settings, but has not been utilized in the continual learning domain.\nI believe this direction requires further investigation."
                },
                "weaknesses": {
                    "value": "### Missing Related Works in Meta-Continual Learning\n\nAccording to my understanding, this work should be classified as a meta-continual learning (MCL) approach, which is also referred to as *learning to continually learn*.\nIt is a direct extension of meta-learning that replaces each learning episode with a continual learning episode, which also aligns with the authors' description in section 2.2.\nThere are several important prior works in this domain [1, 2, 3] that were not mentioned in the paper.\nThey should also be compared as baselines.\n\n### Confusing Description About the Experimental Settings\n\nSince MCL is a branch of meta-learning that aims to optimize a learning algorithm, it is crucial to separate meta-training and meta-test sets.\nAlso, there should be no overlap in the constituent tasks between them.\nOtherwise, the model can achieve a high score simply by memorizing the tasks in the meta-training set without learning new knowledge during the meta-test phase.\n\nIn the paper, I could not find any description of how meta-training and meta-test sets are constructed.\nI suspected that the authors would have followed the conventional meta-splits for Omniglot and Mini-ImageNet datasets, but I suggest the authors refine the overall terminology to be consistent with the existing meta-learning or MCL literature.\n\n### Weak Experimental Results\n\nThe proposed method is tested only on two-task and three-task CL scenarios, which is an unreasonably tiny scale compared to previous works on MCL [1, 2, 3].\nI do not think such a small number of tasks can be considered meaningful.\n\n### Reproducibility\n\nIt seems hard to reproduce the experimental results solely from the provided text.\nTo verify and reproduce experimental results, I believe that including code with the submission should be the standard practice.\n\n---\n- [1] Javed, Khurram and Martha White. \u201cMeta-Learning Representations for Continual Learning.\u201d NeurIPS (2019).\n- [2] Beaulieu, Shawn L. E. et al. \u201cLearning to Continually Learn.\u201d ECAI (2020).\n- [3] Banayeeanzade, Mohammadamin et al. \u201cGenerative vs. Discriminative: Rethinking The Meta-Continual Learning.\u201d NeurIPS (2021)."
                },
                "questions": {
                    "value": "- Experiments with much longer training sequences, as in [1, 2, 3], seem necessary.\n- Since each convolution filter is just a linear layer applied to a local patch, shouldn't it be possible to construct a CNN version of ACL?\n- I have doubts about the representational capability of SRWM since the complex learning dynamics in non-stationary streams depend solely on the initial parameters. Is it really sufficient to manipulate the initial parameters? Can SRWM really handle long sequences?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9084/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698481349277,
            "cdate": 1698481349277,
            "tmdate": 1699637144101,
            "mdate": 1699637144101,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GVOQ6JJjMg",
                "forum": "5twh6pM4SR",
                "replyto": "eFmLV1tMkT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9084/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9084/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer StyD (part 1/2)"
                    },
                    "comment": {
                        "value": "**NB: Please find our response to other questions in \"Common Responses to All Reviewers\" above. Thank you.**\n\nWe thank the reviewer for valuable time reviewing our work and for valuable comments.\n\n> There are several important prior works in this domain [1, 2, 3] that were not mentioned in the paper. They should also be compared as baselines.\n\nThe reviewer is right: these citations were missing. We actually had realized this in the meanwhile, so we had already added these MAML-based/like approaches in our internally updated paper.\nFollowing the reviewer\u2019s suggestion, the updated paper includes a comparison to [1] on Split-MNIST (Table 3; the experimental details can be found in Appendix A.6). Please note that according to the readme file in the official repository of [1] https://github.com/khurramjaved96/mrcl, *\u201c... it is possible to get the same results as ANML (S. Beaulieu 2020) without using any neuromodulation layers\u201d* We concluded that comparing to [2] is unnecessary here; we focus on [1] as our meta-continual learning baseline.\n\nThat said, regarding:\n\n> According to my understanding, this work should be classified as a meta-continual learning (MCL) approach, which is also referred to as learning to continually learn.\n\nWhile our method can definitely be categorized as a meta-learning approach for continual learning, it should not be categorized as \u201clearning to continually learn\u201d in the sense of [2], since ACL learns a learning algorithm while MAML based methods [1, 2] only learn *representations* for CL (or representation learning networks) and still relies on the standard learning algorithm (typically Adam). These are fundamentally different methods even if the title of [2] is very broad, and gives the impression of generality; that is not the case. In particular, their meta-testing requires tuning the learning rate\u2014and in addition, in our experiments/Table 3/Appendix A.6, we also observed that they also require tuning meta-test *training iterations* to perform well on an unseen domain, (Split-)MNIST.\nIn contrast, our learned learning algorithm, once trained, does not require any tuning (learning rate is self-regulated by the trained SRWM; sigma(beta) in Eq.3) This distinction is important. We clarify this in the updated \u201cprior work\u201d paragraph (Page 8, Sec 5.).\n\n\n> Since MCL is a branch of meta-learning that aims to optimize a learning algorithm, it is crucial to separate meta-training and meta-test sets. Also, there should be no overlap in the constituent tasks between them. Otherwise, the model can achieve a high score simply by memorizing the tasks in the meta-training set without learning new knowledge during the meta-test phase.\n\nYes, of course, we are aware of this. We used the standard few-shot learning sequence construction process to ensure this (see further comment in the next point). Otherwise, it is anyway impossible to obtain a model that can learn unseen MNIST or CIFAR-10 during meta-testing, while only meta-trained on Omniglot and Mini-ImageNet (see Table 2).\n\n> In the paper, I could not find any description of how meta-training and meta-test sets are constructed. I suspected that the authors would have followed the conventional meta-splits for Omniglot and Mini-ImageNet datasets, \n\nYes, absolutely. In the Appendix A.1., we had mentioned that we used `torchmeta` for that, but we agree this was not clear in the main text (relating to the next point).\n\n> but I suggest the authors refine the overall terminology to be consistent with the existing meta-learning or MCL literature.\n\nThe reviewer is right; our terminology was confusing (which likely caused Reviewer jN2L\u2019s confusion). Following this suggestion, we now use \u201cmeta-training train/test\u201d and \u201cmeta-test train/test\u201d terminology in the updated paper. Thank you for pointing this out.\n\n> The proposed method is tested only on two-task and three-task CL scenarios, \n\nThis is not accurate; while we had only used 2-task and 3-task objective functions, we evaluated our models on 4-task CL (now called Table 4). Now with Split-MNIST, we also test our method for 5-task settings."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9084/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533399806,
                "cdate": 1700533399806,
                "tmdate": 1700533828365,
                "mdate": 1700533828365,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "t5lq7ZYL9b",
                "forum": "5twh6pM4SR",
                "replyto": "9VUohVZbKE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9084/Reviewer_StyD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9084/Reviewer_StyD"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response.\nMany of my concerns are addressed, especially regarding writing and providing code.\nHowever, several issues still remain.\n\n> \"Meta-learning approach for continual learning\" vs. \"learning to continually learn\"\n\nI think these two are basically the same.\nAs meta-learning is often referred to as \"learning to learn,\" meta-continual learning (MCL) can be called \"learning to continually learn.\"\nIn meta-learning, each learning episode is an offline learning episode, while in MCL, it is a CL episode.\nLearning initializations is just one approach to MCL, not its definition (the title of [2] is indeed excessively general, and I think it is not a proper title).\nEven in the meta-learning literature, some methods are based on MAML, while others are not.\n\n\n> We\u2019d like to note that focusing only on the number of tasks is a restricted view on the problem of CL and to evaluate significance of CL methods.\n\nThis is correct.\nLikewise, focusing solely on task diversity is also a restricted view.\nTherefore, I expected an evaluation with a much larger number of tasks.\nIf not hundreds as in [1, 2, 3], I anticipated at least dozens of tasks.\nEven if the proposed method does not work well in such settings, I think it can provide valuable insights into the proposed approach.\n\n> [1, 2] focus on the number of tasks but ignore the importance of task diversity (e.g., they only meta-train on Omniglot and meta-test on Omniglot).\n\nMeta-training and meta-test sets sharing the same task distribution is one of the key assumptions in meta-learning [4] and MCL.\nIf we meta-train on Omniglot, we generally should not expect the learning algorithm to work well on Mini-ImageNet.\nIt is the same as training a classifier on MNIST and testing it on CIFAR-10 in a standard offline learning setting.\nGeneralizing to such out-of-distribution (meta-)test data is an orthogonal research direction.\n\n\n[4] T. Hospedales, A. Antoniou, P. Micaelli and A. Storkey, \"Meta-Learning in Neural Networks: A Survey\" in IEEE Transactions on Pattern Analysis & Machine Intelligence, vol. 44, no. 09, pp. 5149-5169, 2022."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9084/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700617039277,
                "cdate": 1700617039277,
                "tmdate": 1700617039277,
                "mdate": 1700617039277,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qkHTOYpJtj",
                "forum": "5twh6pM4SR",
                "replyto": "AA7xmYkxRL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9084/Reviewer_StyD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9084/Reviewer_StyD"
                ],
                "content": {
                    "comment": {
                        "value": "I apologize for not being entirely clear in the last comment since I was in a bit of a rush.\nMore details are provided in the following.\n\nTo summarize, my main concern is that the experiments in this work still do not provide meaningful comparisons with the baselines.\n\nThe primary goal of any learning algorithm (not just meta-learning approaches) is to perform well on what it learned in the training phase.\nAnything other than that, such as out-of-distribution (OOD) generalization, is secondary.\nI'm not saying that OOD generalization is useless; of course, it would be nice to have a good performance on OOD data, and there have been numerous works on OOD generalization.\nWhat I want to emphasize is that a learning algorithm should have a good performance on in-distribution (ID) data first.\n\nAssume two learning algorithms, A and B.\nEach algorithm produces a model trained on the same training set.\nThere are two types of test sets; one is an ID test set (sampled from the same distribution as the training set), and the other is an OOD test set.\nIf A scores better with the ID test set and B scores better with the OOD test set, which algorithm is better?\nGenerally, the answer would be A.\nIf the ID score is lower, the OOD score is mostly due to a lucky OOD test set; it can perform much worse in other OOD test sets.\nMoreover, if an algorithm performs better on an ID test set, it is relatively easy to improve the training set by increasing the amount and scope of the data.\n\nThe same principle applies to meta-learning and MCL at the meta-level: you should prioritize evaluation with an ID meta-test set (sharing the same task distribution as the meta-training set).\nEven if the proposed method works better on a few OOD meta-test sets, it cannot be considered superior if it performs worse on ID meta-test sets.\nFor this reason, all the previous works on MCL [1, 2, 3] primarily use ID meta-test sets.\n\nWhile [1] is added as a baseline in the updated draft, only the scores of an OOD meta-test set, i.e., Split-MNIST, are reported after meta-training on Omniglot + Mini-ImageNet.\nI think the authors should mainly report the ID scores.\nThe OOD scores are also not impressive.\nIt seems unreasonable to claim successful OOD generalization with merely 74.6% accuracy in MNIST.\nThis score may seem strong compared to other CL baselines, but other MCL approaches achieve far better accuracy in much more challenging scenarios if an ID meta-test set is used [1, 2, 3].\n\nLastly, I think [3] should also be compared as a baseline.\nIt seems reasonable to skip [2] since [2] is an extension of [1] and does not seem to perform better than [1].\nHowever, [3] is a completely different approach and is reported to perform significantly better than [1].\nIt may show stronger performance even in OOD meta-test sets."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9084/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666081767,
                "cdate": 1700666081767,
                "tmdate": 1700666081767,
                "mdate": 1700666081767,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aJCF8GgBn5",
                "forum": "5twh6pM4SR",
                "replyto": "eFmLV1tMkT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9084/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9084/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for these additional comments.\nWe really appreciate the reviewer's engagement.\n\nWe understood the reviewer's point on the ID evaluation. That said, regarding:\n\n> It seems unreasonable to claim successful OOD generalization with merely 74.6% accuracy in MNIST. This score may seem strong compared to other CL baselines, but other MCL approaches achieve far better accuracy in much more challenging scenarios if an ID meta-test set is used [1, 2, 3].\n\nWe can not agree with this argument/reasoning.\nOur high-level goal/claim is to replace hand-crafted CL algorithms by a \"better\" learned CL algorithm.\nOn Split-MNIST (a standard CL benchmark suggested by the two other reviewers), our learned algorithm effectively outperforms other hand-crafted CL baselines: \"74.6% accuracy\" has to be compared to the performance of the baselines which is around 20-25%, strongly supporting our original claim.\n\nWe can not directly compare these numbers to other numbers achieved on other datasets which have nothing to do with this benchmark.\nWe emphasize that we ran this Split-MNIST experiment not because it's our \"lucky OOD test set\" (in Table 2 and 4, we also evaluate our system on CIFAR-10 and F-MNIST) but because it is a standard/universal CL benchmark that the CL community is generally interested in (as suggested by the two other reviewers).\n\nNow we can add MCL baselines to this comparison; we already reported [1] (implicitly covering [2]) as requested by the reviewer; we could also add [3] or any other existing MCL methods, but that would not fundamentally diminish our contributions much since none of [1,2,3] really learns learning algorithms. As we explained in A.6., [1] is sensitive to meta-test hyperparameters tuning (learning rate, and number of iterations) which is still a characteristic of handcrafted algorithms---something we want to avoid to \"automate CL.\" [3] indeed does not require tuning meta-test hyper-parameters, but it is based on a very different paradigm (of generative classifiers). These MCL methods are relevant (we agree) but they are not solutions to our original goal of replacing hand-crafted CL algorithms for neural nets. Our method is conceptually unique/new in this regard (also directly relevant to the now-popular in-context learning) and its effectiveness is clearly shown on a well-known standard benchmark; while this may still be experimentally limited, follow-up works may further improve this approach and apply it to other datasets... We'll respectfully leave the reviewers and AC to ultimately judge.\n\nOnce again, we thank the reviewer for his/her engagement during this rebuttal, and for very useful comments."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9084/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677951475,
                "cdate": 1700677951475,
                "tmdate": 1700679691085,
                "mdate": 1700679691085,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pjcJSthwnY",
            "forum": "5twh6pM4SR",
            "replyto": "5twh6pM4SR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9084/Reviewer_jN2L"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9084/Reviewer_jN2L"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new way to think about, and potentially solve, the continual learning problem (in particular, the supervised task incremental learning variation of CL). \nThis new approach views CL as a sequence learning problem. Each sub-sequence consists of input/target examples corresponding to one task to be learned. These sub-sequences can then form longer sequences, for multiple tasks, by concatenating multiple sub-sequences.\nOnce formulated as such a sequence-learning task, a gradient descent search for CL learning algorithms can look for the desired CL behavior by constructing loss functions that avoid catastrophic forgetting and aim to achieve goals such as forward transfer."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "On the positive side, the approach of viewing the CL problem in the context of sequence learning seems interesting -- and it gives a fresh perspective to an area (CL) that is becoming increasingly incremental."
                },
                "weaknesses": {
                    "value": "I have some major concerns about whether this method is actually doing CL (versus multitask learning). \n\nAnother major concern is whether ACL can be used in a practical context in which many tasks will be learned over time (as opposed to just a handful).  \n\nPlease see comments below."
                },
                "questions": {
                    "value": "On Page 6, the paper states: \u201cUnless otherwise indicated, we concatenate 15 examples for each class for each task in the context during both training and evaluation (resulting in sequences of length 75 for each task).\u201d \nHaving a temporally structured input during evaluation is not a valid approach in the context of CL (although I am aware that some meta-learning papers unfortunately do that -- but that does not mean that their approach can be accepted without question because it has been previously published). Such temporally structured inputs makes discrimination between classes of different tasks trivial. For example, if you give someone 75 Omniglot examples and 75 Imagenet examples and ask them to classify an input x during testing, I can easily determine whether x is from Omniglot or Imagenet without learning (just by computing some statistics of pixel values). Letters would, of course, look different than natural images. Then, predictions become much easier.\n\nOn Page 6, the paper states: \u201cThe order of appearance of two tasks within training sequences is alternated for every batch.\u201d This sounds like both datasets are available at the same time. If that is true, what the paper is actually doing multitask learning, not CL.\n\nLooking at the loss function (Equation 4), the first term requires access to old model weights W_A (linear growth in memory requirement as they see more tasks), the second term is okay in terms of CL, but the third term requires access to a previous test dataset, which violates CL. It may be that these are some form of \u201creplay\u201d examples, but the paper does not mention that.\n\nI see that the method is significantly different from other continual learning methods, still I would expect the authors to benchmark against some existing methods. After all, the claim is that instead of hand-crafting CL algorithms, we can learn how to sequentially learn. Does ACL perform better than handcrafted tricks? The method is computation intensive, and it does not seem easily scalable to more tasks. So, I would at least want to see the paper outperform some existing methods in the two-task scenario to argue that learning how to continual learn is a promising direction to pursue.\n\nIf you examine Equation 5 on the last page, you'll notice that in order to learn a third task, they need to add three terms to the loss function. In continual learning, a five-task setting is considered small. To learn SplitMNIST, for example, they would actually need 1 + 2 + 3 + 4 + 5 (15) terms in the loss function. As a result, their method becomes quadratically more expensive in terms of computation (i.e., for Task n, you require backpropagation through (n)(n-1)/2 terms). This is clearly not practical."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9084/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9084/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9084/Reviewer_jN2L"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9084/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698775216202,
            "cdate": 1698775216202,
            "tmdate": 1699637143982,
            "mdate": 1699637143982,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "I8MPBlkQP4",
                "forum": "5twh6pM4SR",
                "replyto": "pjcJSthwnY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9084/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9084/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jN2L (part 1/2)"
                    },
                    "comment": {
                        "value": "**NB: Please find our response to other questions in \"Common Responses to All Reviewers\" above. Thank you.**\n\n\nWe thank the reviewer for valuable time reviewing our work and for encouraging comments on the originality of this work (*\u201ca fresh perspective to an area (CL) that is becoming increasingly incremental\u201d*).\n\nThe reviewer has both some critical misunderstandings of certain aspects of our method, AND really important/relevant comments at the same time. We\u2019d like to resolve/respond to them all here.\n\n> I have some major concerns about whether this method is actually doing CL (versus multitask learning).\n\n[NB: We assumed that by \u201cmultitask learning\u201d the reviewer means \u201cjoint training\u201d.]\n\nHere the reviewer has some critical misunderstandings about our method. As we\u2019ll explain, our framework is a proper CL framework. We are not affected by ANY of the problems/concerns raised by the reviewer regarding whether our method respects the framework of CL. \nWe believe what causes some of these confusions is the meta-learning procedure, and our terminologies were not very helpful in that regard. To improve this, we introduce meta-training meta-testing terminology (as suggested by Reviewer StyD).\nPlease let us try to resolve this confusion one-by-one as follows.\n\n> Having a temporally structured input during evaluation is not a valid approach in the context of CL (although I am aware that some meta-learning papers unfortunately do that -- but that does not mean that their approach can be accepted without question because it has been previously published). Such temporally structured inputs makes discrimination between classes of different tasks trivial. For example, if you give someone 75 Omniglot examples and 75 Imagenet examples and ask them to classify an input x during testing, I can easily determine whether x is from Omniglot or Imagenet without learning (just by computing some statistics of pixel values). Letters would, of course, look different than natural images. Then, predictions become much easier.\n\n[This confusion is independent of meta-learning] \nWe first would like to clarify that all our main CL settings are \u201cdomain-incremental\u201d (except the \u201cclass-incremental\u201d setting in our new Split-MNIST experiments). This means that if our CL task consists of 2 tasks with both of them being 5-way classification, the output dimension of the model is also 5 (not 10), which is shared for both tasks. In this setting, the temporal structure can not help the model for classification: (taking the reviewer\u2019s example) an input image has to be classified to be one of the 5 labels among {0, 1, 2, 3, 4} regardless of whether the image comes from Omniglot or Mini-ImageNet. A model may easily recognize that an input image is from Omniglot, that\u2019s likely the case, but that would not facilitate the task; it still has to do the prediction among the 5 labels."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9084/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700533102766,
                "cdate": 1700533102766,
                "tmdate": 1700540375405,
                "mdate": 1700540375405,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QkZoYpZ8SI",
                "forum": "5twh6pM4SR",
                "replyto": "ggvwXAA0WR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9084/Reviewer_jN2L"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9084/Reviewer_jN2L"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "First, let me thank the authors for their detailed response to all reviews. \n\nUnfortunately, these responses did not address my concerns on whether the paper actually solves the Continual Learning problem -- at least in any practical manifestation of that problem. The following sentence, copied from the authors' response, shows how confusing the proposed approach is, for instance in terms of whether it uses training data during testing (but there are also other similarly confusing points): \n\n\"During meta-testing, our model reads a sequence of input/label example pairs (called \u201cmeta-test training\u201d examples) sampled from the training set of MNIST, then (similarly) a sequence of CIFAR-10 training examples; our model ends up in a weight state W_{MNIST, CIFAR-10} at the end of the entire sequence. No prediction has been done so far. Now using the resulting weights W_{MNIST, CIFAR-10}, we make predictions on examples (called \u201cmeta-test test\u201d examples) from the test set of MNIST and CIFAR-10 to evaluate the model\u2019s capability to predict both the first task, MNIST, and the second one, CIFAR-10.\"\n\nThe fact that three independent reviewers had very similar concerns about the proposed approach means something in my opinion. I think that it would be beneficial for the paper if the authors largely re-write the paper, at least the problem formulation, the description of the method, and the setup of the experiments, also following the more common terminology and assumptions in the CL and metalearning literature."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9084/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545667394,
                "cdate": 1700545667394,
                "tmdate": 1700545667394,
                "mdate": 1700545667394,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vJ3pxjdjKN",
                "forum": "5twh6pM4SR",
                "replyto": "pjcJSthwnY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9084/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9084/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your prompt response.\n\n> Unfortunately, these responses did not address my concerns on whether the paper actually solves the Continual Learning problem\u00a0-- at least in any practical manifestation of that problem.\n\nWe are sorry to read this. Please let us ask a few questions that might help.\n\n> The following sentence, copied from the authors' response, shows how confusing the proposed approach is, for instance in terms of whether it uses training data during testing (but there are also other similarly confusing points):\n\"During meta-testing, our model reads a sequence of input/label example pairs (called \u201cmeta-test training\u201d examples) sampled from the training set of MNIST, then (similarly) a sequence of CIFAR-10 training examples; our model ends up in a weight state W_{MNIST, CIFAR-10} at the end of the entire sequence. No prediction has been done so far. Now using the resulting weights W_{MNIST, CIFAR-10}, we make predictions on examples (called \u201cmeta-test test\u201d examples) from the test set of MNIST and CIFAR-10 to evaluate the model\u2019s capability to predict both the first task, MNIST, and the second one, CIFAR-10.\"\n\nCould you please explain what exactly the reviewer finds \"confusing\" here?\nWe detailed every and each step of the process precisely because the reviewer did not seem to be familiar with few-shot/meta-learning.\nWe can also describe standard continual learning with a hand-crafted learning algorithm (let's say, Adam) exactly in the same style:\n\n\"\"\"\n\nLet\u2019s say our goal is to learn two tasks sequentially: MNIST, then CIFAR-10 using the Adam\u00a0optimizer.\nWe'll train a network with weights W, initialized as W_0.\nWe sample input/label example pairs from the training set of MNIST.\nWe do a few iterations of Adam on these examples, starting from weight W_0; after this, the weight\u00a0becomes W_{MNIST}.\nNow we sample input/label example pairs from the training set of CIFAR-10.\nWe do a few iterations of Adam on these examples, starting from weight W_{MNIST}; after this, the weight becomes W_{MNIST, CIFAR-10}.\nThis is the end of training; the resulting weight is\u00a0W_{MNIST, CIFAR-10}.\u00a0\nNow we make predictions on examples from the test set of\u00a0MNIST and CIFAR-10 to evaluate the model\u2019s (with weight W_{MNIST, CIFAR-10}) capability to predict both the first task, MNIST, and the second one, CIFAR-10.\n\n\"\"\"\n\nIs this confusing?\nThe only difference between the conventional\u00a0CL and ours is that the learning algorithm used for CL is\u00a0hand-crafted (e.g, Adam) or learned (forward pass of SRWM).\nWe emphasize\u00a0that in general, we are not supposed to go into this level of detail to describe continual learning though.\u00a0\n\n> The fact that three independent reviewers had very similar concerns about the proposed approach means something in my opinion\n\nThis is not true. Reviewer WhrD wrote \"The paper is well written and properly structured. The figures are of high quality and help in quickly grasping the main ideas.\"\nWe also believe that the figure is almost self-explanatory of the method but we also understand that this perception may depend on the readers' familiality with meta-learning/sequence-processing.\n\n>\u00a0I think that it would be beneficial for the paper if the authors largely re-write the paper, at least the problem formulation, the description of the method, and the setup of the experiments, also following the more common terminology and assumptions in the CL and metalearning literature.\n\nThe terminology on metalearning has been already pointed out by Reviewer\u00a0StyD, and we have immediately adopted his/her suggestion as Reviewer StyD was specific about his/her suggestion.\nWe'll only be able to re-write the paper if the reviewer can be more specific: what\u00a0terminology and assumptions in the CL literature is the reviewer referring to?"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9084/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700584923828,
                "cdate": 1700584923828,
                "tmdate": 1700588662131,
                "mdate": 1700588662131,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sklCXZtEH8",
                "forum": "5twh6pM4SR",
                "replyto": "Y48xoCUrad",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9084/Reviewer_jN2L"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9084/Reviewer_jN2L"
                ],
                "content": {
                    "title": {
                        "value": "My final remark"
                    },
                    "comment": {
                        "value": "Dear Authors, in your latest remarks it seems that you are doubting my understanding/knowledge of this area. Without disclosing my identity, I have published at top-tier conferences in the area of continual learning. And given that your paper claims to make an important contribution in CL, I have to evaluate it based on how that community defines that problem and evaluates proposed solutions. \n\nBy the way, I think the rebuttal process for a conference (as opposed to a journal) is supposed to play a very different role: short clarifications, fixing minor issues, adding missing references, etc -- not trying to rewrite large portions of the paper, asking the reviewers to evaluate the paper again in 3-4 days. \n\nSo, despite the long argumentation by the authors in the rebuttal, my opinion remains:\na) this paper does not solve a practically useful formulation of the CL problem\nb) even if we consider it as a solution to some hypothetical problem, it would not scale to a large number of tasks."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9084/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633917369,
                "cdate": 1700633917369,
                "tmdate": 1700633917369,
                "mdate": 1700633917369,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2keBhRkTei",
            "forum": "5twh6pM4SR",
            "replyto": "5twh6pM4SR",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9084/Reviewer_WhrD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9084/Reviewer_WhrD"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a continual learning method based on self-referential weight matrices. By posing the continual learning problem as a meta-learning task, it is possible to formulate the standard continual learning desiderata (low forgetting, high forward and backward transfers) simply as terms of the meta-learning objective. Authors show that their approach is promising through experiments on MNIST, Omniglot, and Mini-ImageNet."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Originality is the main strength of the proposed approach. To my knowledge, the application of SRWM to continual learning is a novel idea. Automating the discovery of continual learning algorithms by including the desired requirements as loss terms of the meta-learner is an exciting approach and it would be great to explore it in a bit more details. The paper is well written and properly structured. The figures are of high quality and help in quickly grasping the main ideas."
                },
                "weaknesses": {
                    "value": "The main weakness of the paper is the experimental evaluation. Despite presenting their approach as a continual learning method, the authors don't use any of the standard benchmarks (e.g. Split MNIST, Split Mini-ImageNet), nor do they compare to any previous work (regularization, replay, or parameter isolation methods). The meta-learning formulation is also a major limitation, as the number of loss terms grows rapidly with the number of tasks and it is not clear whether the method is practical for e.g. 10 tasks (which is still a small number compared to the requirements of real-world lifelong learning). It would also be great to include a figure that illustrates the architecture of your model in more detail."
                },
                "questions": {
                    "value": "How do the data requirements of your method grow with the number of tasks?\n\nHow is the training sequence constructed? Do you use a single sequence? If not, doesn't it mean you're effectively performing joint training?\n\nCould you elaborate what do you mean by \"certain real-world data may naturally give rise to an ACL-like objective\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9084/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9084/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9084/Reviewer_WhrD"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9084/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698836408232,
            "cdate": 1698836408232,
            "tmdate": 1700578185884,
            "mdate": 1700578185884,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "93sB5OCCG9",
                "forum": "5twh6pM4SR",
                "replyto": "2keBhRkTei",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9084/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9084/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WhrD"
                    },
                    "comment": {
                        "value": "We thank the reviewer for valuable time reviewing our work and for very positive comments on the originality and clarity of this work.\n\n> Authors show that their approach is promising through experiments on MNIST, Omniglot, and Mini-ImageNet.\n\nJust to clarify as this is not fully accurate: the set of datasets we used is Omniglot, Mini-ImageNet, FC100 for (meta-)training and MNIST, Fashion-MNIST, and CIFAR-10 for (meta-)testing.\n\n> How do the data requirements of your method grow with the number of tasks?\n\nIn terms of data requirements to increase the number of tasks, what we need is essentially to increase the number of classes. In principle, we would only need a handful of examples for each class (as we anyway want to train our models as sample-efficient few shot learners). For example, Omniglot has 1632 classes with only 20 examples each.\n\n> How is the training sequence constructed? \n\nWe follow the standard procedure used in few-shot learning with sequence processing networks; that is, given a dataset with $C$ classes, for each sequence, we sample $N$ random but distinct classes out of $C$ ($N < C$). The resulting $N$ classes are re-labelled such that each class is assigned to one out of $N$ distinct random label index which is unique to the sequence. \n\n> Do you use a single sequence? If not, doesn't it mean you're effectively performing joint training?\n\nWe use multiple sequences in a batch, but no, it does not result in any form of joint training (please also refer to our response to Reviewer jN2L where we clarify more misunderstandings).  This is because the training sequences are constructed such that class-to-label mapping is unique to each sequence (that is, each sequence is a unique learning problem). While this is actually classic in few-shot learning settings, since other reviewers also find this obscure, we\u2019ll be happy to add a more elaborated description in the final version.\n\n> Could you elaborate what do you mean by \"certain real-world data may naturally give rise to an ACL-like objective\"?\n\nYes, what we mean is the following. If we consider a language model trained on a very long training sequence (as is the case now with large models); it is not implausible that such a sequence naturally consists of (let\u2019s say three) text chunks (A, B, A\u2019) where A\u2019 is \u201crelated\u201d to A (each letter is a chuck/paragraph of text). In this sequence (processed from left to right), learning to predict part A\u2019 after having observed (A, B) encourages remembering the contents of part A. This is very similar to the (artificial) construction of our ACL loss, but obtained without explicit intention of continual learning objective.\n\nWe hope the most crucial concern of the reviewer is resolved through the Split-MNIST results.\nRegarding other limitations, we consider them as interesting research questions to be investigated in follow-up works; we believe the current/updated version of the paper demonstrates that the proposed method is an interesting new approach for continual learning. If the reviewer agrees with this, we\u2019d appreciate it a lot if you can consider increasing the score. Thank you very much.\n\n**PS: Please find our response to other questions in \"Common Responses to All Reviewers\" above. Thank you.**"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9084/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700532932098,
                "cdate": 1700532932098,
                "tmdate": 1700540338687,
                "mdate": 1700540338687,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Df3OdVvCu4",
                "forum": "5twh6pM4SR",
                "replyto": "93sB5OCCG9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9084/Reviewer_WhrD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9084/Reviewer_WhrD"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "Thanks for addressing my concerns and running the Split-MNIST experiment. I have raised my score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9084/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578132602,
                "cdate": 1700578132602,
                "tmdate": 1700578132602,
                "mdate": 1700578132602,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]