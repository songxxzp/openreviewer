[
    {
        "title": "The Power of Linear Combinations: Learning with Random Convolutions"
    },
    {
        "review": {
            "id": "yFVfYZMoUD",
            "forum": "YSTaRLVP2G",
            "replyto": "YSTaRLVP2G",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission90/Reviewer_gGqG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission90/Reviewer_gGqG"
            ],
            "content": {
                "summary": {
                    "value": "This paper questions the significance of learned convolution filters, while following the convolutional paradigm with according spatial inductive bias. Contemporary CNN architectures can achieve high test accuracies without updating the randomly initialized filters. It further shows that romdom filters mitigates overfitting and enhances overall performance and robustness. Learning gains increase proportionally with kernel size."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- CNNs can be trained to high validation accuracies on computer vision tasks without ever updating weights of randomly intialized spatial convolutions.\n- training with random filters can outperform the accuracy and robustness of fully learnable convolutions due to implicit regularization in the weight space. \n- The paper is well written."
                },
                "weaknesses": {
                    "value": "- The idea is trivial and well-known. \n- the experiments are done on toy models and datasets, which are not convincing."
                },
                "questions": {
                    "value": "- Experiments on state-of-the-art models"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission90/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698618758737,
            "cdate": 1698618758737,
            "tmdate": 1699635934106,
            "mdate": 1699635934106,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lGtCpTTn0i",
                "forum": "YSTaRLVP2G",
                "replyto": "yFVfYZMoUD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission90/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission90/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "> W1: The idea is trivial and well-known.\n\nWith all due respect, we remind you of the reviewer's instructions for raising weaknesses: *\u201cBe specific, avoid generic remarks. [...] if you believe the contribution lacks novelty, provide references and an explanation as evidence;\u201d* \n\n\nIf you believe that our results are well-known, please provide references and explanations as evidence. As a reminder here are our key statements from the conclusion section:\n\n1. Random frozen CNNs can outperform fully trainable baselines in clean and robust accuracy.\n2. In modern off-the-shelf networks, learned spatial convolution filters only marginally improve the performance compared to frozen random models in small kernel regimes. The reason for this is an implicit computation of linear combinations between pointwise and spatial filters.\n3. Adding learnable pointwise convolutions immediately after spatial convolutions can decrease performance and robustness.\n4. Only, under increasing kernel size, learned convolution filters begin to increase in importance but only due to a different spatial distribution of weights that cannot be reflected by currently practiced i.i.d. initializations.\n\nNeedless to say, we disagree that these statements are well-known or trivial.\n\n> W2: the experiments are done on toy models and datasets, which are not convincing.\n> Q1: Experiments on state-of-the-art models\n\nIt is not true that we only experiment with toy models and datasets. We ablate the effects on various ResNets scaling linear combinations, width, and depth on CIFAR-10. Following that, we show that the results transfer to multiple other datasets including ImageNet (Sec. 5.2) which is arguably the benchmark in vision, as well as other tasks such as Object Detection and Segmentation in Appendix A.1 containing real-life problems from Kaggle Challenges. Fig. 1 also shows the behavior of popular benchmark CNNs: ResNe(X)t-50 and Wide-ResNet-50. In all of these experiments, we see the same observations - how can the results be unconvincing? \n\nPlease let us know if we have addressed your concerns. If you have any other questions/concerns please feel free to ask. We would be grateful if you would reconsider your score in light of our updates. Thank you again for your time and consideration."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission90/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699880014510,
                "cdate": 1699880014510,
                "tmdate": 1699880014510,
                "mdate": 1699880014510,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qujN0wErgl",
                "forum": "YSTaRLVP2G",
                "replyto": "lGtCpTTn0i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission90/Reviewer_gGqG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission90/Reviewer_gGqG"
                ],
                "content": {
                    "comment": {
                        "value": "The idea is trivial because:\n1. It is well known that separable conv (depthwise + pointwise) is better than naive 3x3 (ref: MobileNet). \n2. The implicit computation of linear combination is quite obvious, by basic linear algebra.\n\nThe experiments are toy.\nTo give you a non-trivial example, on ImageNet, you need to do an experiment with 90%+ baseline accuracy."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission90/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699995179216,
                "cdate": 1699995179216,
                "tmdate": 1699995179216,
                "mdate": 1699995179216,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5efWHPdG0l",
            "forum": "YSTaRLVP2G",
            "replyto": "YSTaRLVP2G",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission90/Reviewer_zzwD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission90/Reviewer_zzwD"
            ],
            "content": {
                "summary": {
                    "value": "The authors investigate whether it is necessary to learn spatial convolutional filter weights in CNNs at all, to achieve good performance on data-specific tasks. Their approach is to fix the spatial convolutional kernel at initialization, only allowing learning linear combinations of these random spatial kernels during training. Authors argue that this approach has a regularising effect, as it prevents overfitting of the spatial kernels to specific spatial patterns occurring in the training data, also showing that learnable spatial convolutions in combination with a high rate of linear combinations actually decreases validation accuracy. Authors show that the benefits of learning linear combinations of random spatial kernels are mostly present at small kernel sizes, as for larger kernel sizes performance of the standard learnable spatial kernel and random spatial kernel starts to diverge. Authors conclude by giving a number of possible research extensions of their work, e.g. modifying the standard random i.i.d. initialization to reflect spatial distributions found in trained spatial kernels could be a very interesting next step."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Authors are very thorough in positioning their work and contrasting it with previous methods. I appreciate the thorough literature review, as it contributes to a clear context for the current work. The main research question that the authors investigate; the possibility of training a CNN without modifying the original spatial convolution weights, is well addressed theoretically and empirically, at least in the setting of image classification. The authors perform a number of relevant ablations over the hyperparameters of their method (i.e. kernel size, expansion rate) and provide extensive analysis of their results. Furthermore, where the method underperforms (larger kernel sizes), the authors give a qualitative analysis through visualisation of the distribution of learned kernels and provide pointers for possible ways of addressing this problem."
                },
                "weaknesses": {
                    "value": "- My main objection with respect to this manuscript is the clarity of practical impact of this approach. It seems like in most settings, the proposed approach slightly underperforms traditional CNN architectures. For larger expansion rates, the model may outperform the baseline, but to me the trade-off in computational complexity is unclear. Although the author\u2019s findings are interesting in their own right - it is certainly somewhat suprising that a factorization this drastic is still able to perform this impressively - I think the manuscript would greatly benefit from more clear indication of where potential benefits of frozen spatial convolution weights may lie, as the current version of the manuscript doesn\u2019t sufficiently address this in my opinion. \n- Second, the theoretical analysis of their approach seems somewhat limited. Although the authors list as one of their contributions the theoretical explanation for the impressive performance of their factorization, I'm not sure where in the paper this happens. On the one hand the authors say a large enough basis of random spatial filters provides enough expressivity to represent any other fully learnable spatial kernel (lemma 4.1), on the other hand they indicate that their approach has a clear regularizing effect, improving robustness. Are these not two contrasting observations?\n- Third, the experiments conducted by the authors in the main body of the paper are quite limited in scope, in that they are all classification problems. Given that the authors themselves indicate that linear combinations of random weights have a hard time forming sharp spatial patterns, I think more attention should also be given to e.g. fine-grained segmentation (I\u2019ve seen the results in the appendix but the authors might consider moving them to the main body and providing a more detailed experimental setup)."
                },
                "questions": {
                    "value": "- For the experiments in 5.1, how do the modified models and their baseline counterparts compare in terms of computational requirements? I.e. as you increase the expansion factor it increases computational complexity right? How do the different expansion factors and the original model compare, for example in terms of flops?\n- In 5.3 the role of kernel size, you indicate that you investigate the spatial distribution for kernels for different kernel sizes. I assume the visualisations of the kernels shown for the frozen model are obtained after training? You indicate that there is a very clear spatial pattern in trained non-frozen kernels that isn\u2019t present in the frozen model, and you indicate this as the main reason for trailing performance. This seems like a reasonable hypothesis (although somewhat in contrast to Fig 2), but would this mean your model is unable to perform more fine-grained tasks such as segmentation? Why not?\n- Could you clarify what you see as the main practical impact of this approach? (see above)\n- Could you clarify what you list as theoretical analysis for the performance of your approach? (see above)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission90/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698674344456,
            "cdate": 1698674344456,
            "tmdate": 1699635933964,
            "mdate": 1699635933964,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "X8RxLrjpVf",
                "forum": "YSTaRLVP2G",
                "replyto": "5efWHPdG0l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission90/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission90/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal 1/2"
                    },
                    "comment": {
                        "value": "> W1:  [...]  practical impact of this approach [...] \n> Q3: Could you clarify what you see as the main practical impact of this approach? (see above)\n\nOur practical relevance is showing, that not learning the spatial convolutions obtains similar or even better accuracy/robustness in large networks. This comes at the benefit of a significantly reduced parameter count, and thus lower VRAM, faster training etc. \n\n*Are such large networks realistic?* Generally, we are all aware of the trend for larger models, but even today adversarial robustness SOTA (see https://robustbench.github.io/ under CIFAR-10,  $\\ell_\\infty$) uses WideResNets-70x16 [1] (16 times wider than a normal ResNet)! We assume that we would obtain cheaper and better training with freezing. Sadly we cannot verify this, due to the necessary GPU VRAM to run experiments.\n\nJust to clarify: generally, we do not promote training a frozen LCified network over a non-LC baseline. That would be indeed useless (although in some cases it is an easy trick to improve performance - e.g., see Tab. 4 in the Appendix). The LC-ResNets are just proxies for linear combinations in real architectures.\n\nAdditionally, we deliver \u201cfood for thought\u201d: i) pointwise convolutions after learnable spatial convolutions can impair performance - does this limit current architectures (e.g., ResNet-50)? ii) currently practiced initializations are not ideal when scaling kernel size (as commonly done) - spatial considerations are one thing to consider; iii) any studies of convolution filters must consider the presence of linear combinations.\n\nDoes this clarify our intent? Would the reviewer suggest that we update our conclusion with this text?\n\n>W2.1: Second, the theoretical analysis of their approach seems somewhat limited. Although the authors list as one of their contributions the theoretical explanation for the impressive performance of their factorization, I'm not sure where in the paper this happens. \nQ4: Could you clarify what you list as theoretical analysis for the performance of your approach? (see above)\n\nWe refer to the theory at the beginning of Sec. 4, where we show that a pointwise convolution following a spatial convolution is equal to a single convolution with a linear combination of the filters. Then it follows, that in such a scenario, freezing spatial filters to random inits and learning LCs is as expressive as learning them (if you have sufficiently many). Since random LC networks can learn approximations of fully learnable filters they perform similarly. \nWe believe that this theory sufficiently explains why some networks can train to near-same accuracy with random spatial filters. Does this clarify the theoretical analysis?\n\n> W2.2: On the one hand the authors say a large enough basis of random spatial filters provides enough expressivity to represent any other fully learnable spatial kernel (lemma 4.1), on the other hand they indicate that their approach has a clear regularizing effect, improving robustness. Are these not two contrasting observations?\n\nActually, they are not! The lemma only holds if you have sufficiently many kernels. In practice, you will have a limited number and only approximate the learnable filters (e.g., see Fig. 8 in the Appendix) - keep in mind, that the 1x1 operates on the $k\\times k\\times c_{in}$ filter and not kernel level and must create all $c_{out}$ filters ($k\\times k\\times c_{in}$) from the random filters. \n\n> W3: Third, the experiments conducted by the authors in the main body of the paper are quite limited in scope, in that they are all classification problems. Given that the authors themselves indicate that linear combinations of random weights have a hard time forming sharp spatial patterns, I think more attention should also be given to e.g. fine-grained segmentation (I\u2019ve seen the results in the appendix but the authors might consider moving them to the main body and providing a more detailed experimental setup).\nQ2.3: \u2026 but would this mean your model is unable to perform more fine-grained tasks such as segmentation? Why not?\n\nDue to space limits, we decided to put these results in the appendix (as the trend is similar). Including these results would mean that we have to remove or shorten some other section. Do you have any suggestions on what we should replace?\n\nRegarding the sharp spatial patterns: This only affects larger kernels. Common models such as U-Net with 3x3 are not affected. Besides, \u201csharp\u201d transitions in masks are generated by the high-frequency information on skip-connections, not the up-convolutions [1]. In fact, the results in the appendix are obtained on a U-Net. \n\n[1] Olaf Ronneberger, Philipp Fischer, Thomas Brox, \u201cU-Net: Convolutional Networks for Biomedical Image Segmentation\u201d, MICCAI, 2015."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission90/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699881330703,
                "cdate": 1699881330703,
                "tmdate": 1699881330703,
                "mdate": 1699881330703,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mkeFg65LSQ",
                "forum": "YSTaRLVP2G",
                "replyto": "5efWHPdG0l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission90/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission90/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal 2/2"
                    },
                    "comment": {
                        "value": "> Q1: how do the modified models and their baseline counterparts compare in terms of computational requirements?\n\nWe deliberately did not report computational requirements, because we do not propose to use the LC-ResNets instead of off-the-shelf architectures (see above). \n\nOur message is freezing spatial filters reduces training time and does not affect accuracy (or even decreases) it in large networks. Here is an example of common networks. We report the number of trainable parameters (as a proxy for VRAM requirements) and latency (required time for one forward and backward pass). Please note that the inference time will depend on the framework, driver, hardware etc. FLOPs remain unaffected.\n\n| Model     \t| Frozen \t| Trainable Parameters \t| Latency |\n|-----------|--------|----------------------|---------|\n| RN-50     \t| no     \t| 25.6 M               \t| 141 ms  \t|\n|           \t| yes    \t| 14.2 M               \t| 131 ms  \t|\n| WRN-50x2  \t| no     \t| 68.9 M               \t| 211 ms  \t|\n|           \t| yes    \t| 23.6 M               \t| 185 ms  \t|\n| WRN-101x2 \t| no     \t| 162.9 M              \t| 359 ms  \t|\n|           \t| yes    \t| 41.5 M               \t| 311 ms  \t|\n\n> Q2.1:  I assume the visualisations of the kernels shown for the frozen model are obtained after training? \n\nYes, all visualized weights are shown after training.\n\n> Q2.2: You indicate that there is a very clear spatial pattern in trained non-frozen kernels that isn\u2019t present in the frozen model, and you indicate this as the main reason for trailing performance. This seems like a reasonable hypothesis (although somewhat in contrast to Fig 2), \u2026\n\nFig. 2 shows a smooth resulting kernel - for an example of a \u201csharp\u201d filter please see Fig. 8 in the Appendix. However, if you had enough filters/LCs, you could reproduce the sharp filters as such there is no contrast to Fig. 2.\n\nPlease let us know if we have addressed all of your concerns. If you have any other questions/concerns please feel free to ask. We would be grateful if you would reconsider your score in light of our updates. Thank you again for your time and consideration."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission90/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699881415489,
                "cdate": 1699881415489,
                "tmdate": 1699881415489,
                "mdate": 1699881415489,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ER6CCtCoIu",
            "forum": "YSTaRLVP2G",
            "replyto": "YSTaRLVP2G",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission90/Reviewer_6E1w"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission90/Reviewer_6E1w"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose to investigate the impact of learned spatial filters v.s. random spatial filters in a deep Convolutional Neural Network (CNN) that uses 1x1 filters, the hypothesis being that the linear combinations (of the spatial filters) learned by 1x1 filters, are powerful enough themselves with random spatial filters alone to learn visual representations and attain good generalization on tasks such as image classification. The authors empirically investigate this hypothesis using existing CNN architectures with appropriate 1x1 filters, and furthermore by adding 1x1 filters to ensure that spatial filters are followed by a linear combination, to create \"LC\" blocks. The authors demonstrate that by drastically increasing the width of these LC blocks (i.e. both the spatial filters and 1x1), using only uniform randomly sampled spatial filters with learned 1x1 linear combinations, CNNs can match the generalization performance of using learned spatial filters on ResNet-based models with ImageNet, CIFAR-100, CIRAR-10, SVHN and Fashion-MNIST. The authors do also show however, a degradation in robustness performance when not using learned spatial filters that warrants further study."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "* The paper is well-written overall, with a good background on both the recent developments in convolutional kernel sizes, and usage of 1x1 convolutions.\n* The experimental setup is overall quite good overall (with some exceptions listed below), with the appropriate models and datasets used to demonstrate convincing empirical evidence on the author's hypothesis.\n* The robustness results in the paper are perhaps the most novel/insightful part of the paper, and would benefit from more analysis/further study."
                },
                "weaknesses": {
                    "value": "* The hypothesis and results are not surprising at all given how many linear combinations the authors learn in replacement of learned spatial filters. We already know that we can learn to reconstruct anything in a space by learning a linear combination of orthogonal basis vectors. While the basis vectors in this case are not designed to be orthogonal, they are sampled randomly sampled from a high-dimensional space (e.g. the kernel space of e.g. 3x3xC, where C is often very large). Randomly sampled vectors in high dimensional spaces are extremely likely to be orthogonal. Furthermore, we don't need to reconstruct perfectly any possible vector in filter space \u2013 the filters learned by CNNs likely represent a low-dimensional manifold within filter space, and approximating these filters rather than exact reconstruction is sufficient. \n* Given the fact the hypothesis/results should not be surprising, this paper cannot just be considered an analysis paper pointing out a novel/surprising result to the research community, which is in itself a perfectly valid thing to do. The paper should be motivated outside of the fact that they hypothesis is true - i.e. is it useful to train a CNN in this manner over learning spatial filters? It's obvious that, as explained by the authors in the paper, using frozen LC models that match learned LC generalization performance are much more expensive in terms of compute and perhaps more importantly working memory (i.e. GPU VRAM), so this appears to not be the case at all.\n* Many of the empirical results compare models with drastically different numbers of trainable parameters, while using the same training hyperparameters. However, we know that models with different numbers of trainable parameters usually need different hyperparameters and training setups. For example, models with more trainable parameters typically need longer to converge and require different learning rates, etc. Given this, it's not completely convincing that the model's generalization being compared between frozen lc and learned lc is fair.\n* No explicit comparison of the compute and VRAM requirements for training models that are compared, in many cases there is quite a drastic difference it would seem. This is especially stark in section 5.1 where models with exponential increasing compute/param are compared with a fixed baseline model, i.e. ResNet-20-16.\n* While I like the robustness analysis and think it warrants further attention, this is yet another reason why even when frozen LC models match generalization, they are poorly motivated."
                },
                "questions": {
                    "value": "* If you believe this is not explained simply as learning from an orthogonal basis, as explained above, how many of the randomly sampled spatial filters are not orthogonal? You could perform an analysis to understand the representational ability of the random basis used.\n* If you do believe this explanation, what does your paper contribute that is not simply empirical validation of this understanding? i.e. things that are specific to the DNN context, and other findings that are not trivially a result of being able to learn from an orthogonal basis.\n* Why is it useful to train a frozen LC model v.s. a learned spatial model that achieve the same generalization? What is the tradeoff in compute and working memory (i.e. GPU VRAM) utilization for achieving this with frozen LC models v.s. a baseline learned spatial model for the same generalization performance? \n* Did you do any hyperparameter turning for frozen LC v.s. learnable LC models you compared to ensure using the same hyperparameters/training setup is appropriate?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission90/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698787705550,
            "cdate": 1698787705550,
            "tmdate": 1699635933893,
            "mdate": 1699635933893,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gereNx3NiA",
                "forum": "YSTaRLVP2G",
                "replyto": "ER6CCtCoIu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission90/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission90/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal 1/2"
                    },
                    "comment": {
                        "value": "> W1: The hypothesis and results are not surprising at all given how many linear combinations the authors learn in replacement of learned spatial filters. [...]\n> Q1: If you believe this is not explained simply as learning from an orthogonal basis, as explained above, how many of the randomly sampled spatial filters are not orthogonal? [...]\n\nThe central assumption of the reviewer\u2019s rejection rating regarding the orthogonality does not hold - neither in theory nor in practice! Theoretically (by the law of large numbers), the reviewer's orthogonality assumption only holds for very high dimensional vectors ( $C \\rightarrow \\infty$ ), but the actual dimensionality of C used in our evaluation (and realistic applications) is far from that. We have $ C \\leq 64$ in our experiments.   \nTo verify this, we have followed your suggestion and computed this on the largest layer in a ResNet-LC20x32 under both, uniform and normal inits. This model breaks even with the baseline - as such it should contain a significant ratio of orthogonality. Yet, even when we conservatively define orthogonality with cosine similarity < 0.2, **0% of the filter pairs are orthogonal** in both cases.\n\n> W2: [...] is it useful to train a CNN in this manner over learning spatial filters? It's obvious that, as explained by the authors in the paper, using frozen LC models that match learned LC generalization performance are much more expensive [....]\n> Q2: what does your paper contribute that is not simply empirical validation of this understanding?\n> Q3.1: Why is it useful to train a frozen LC model v.s. a learned spatial model that achieve the same generalization?\n\nJust to clarify: Generally, we do not promote training a frozen LCified network over a non-LC baseline. That would be indeed useless (although in some cases it is an easy trick to improve performance - e.g., see Tab. 4 in the Appendix). The ResNets-LC are just proxies for linear combinations in real architectures.\n\nInstead, we ask: why would you train all parameters, if not learning the spatial convolutions obtains similar or even better accuracy at a significantly reduced parameter count and lower VRAM, faster training etc.? Our results show that, eventually, the utility of learned convolutions saturates and beyond that deteriorates performance - if networks are sufficiently large. \n\n\n*Are such networks realistic?* Yes. SOTA research on adversarial robustness (see https://robustbench.github.io/ under CIFAR-10,  $\\ell_\\infty$) already uses WideResNets-70x16 [2] (16 times wider than a normal ResNet)! We assume that we would obtain cheaper and better training with freezing. Sadly we cannot verify this, due to the necessary GPU VRAM to run experiments. And, generally, we are all aware of the trend for larger models.\n\n\nDoes this clarify our intent? Would the reviewer suggest that we update our conclusion with this text? \n\n> W3: Many of the empirical results compare models with drastically different numbers of trainable parameters, while using the same training hyperparameters. [...]\n> Q4: Did you do any hyperparameter turning for frozen LC v.s. learnable LC models you compared to ensure using the same hyperparameters/training setup is appropriate?\n\nWe have opted for an extra-long schedule relative to the small datasets (200 Epochs) to ensure appropriate convergence. We have ensured that the training accuracy of learnable LC-ResNets at any expansion is near 100%, i.e. they have converged. Further, earlier experiments with shorter schedules (75 Epochs under cosine decay) showed similar trends. Thus we are confident that we appropriately train the models. Tuning is mostly trial-and-error, of course, it may be possible to skew the results in either way - we are confident that using these parameters for all experiments allows for a fairer comparison."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission90/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699881782408,
                "cdate": 1699881782408,
                "tmdate": 1699881782408,
                "mdate": 1699881782408,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KoItexKGa7",
                "forum": "YSTaRLVP2G",
                "replyto": "ER6CCtCoIu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission90/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission90/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal 2/2"
                    },
                    "comment": {
                        "value": "> W4: No explicit comparison of the compute and VRAM requirements for training models that are compared, in many cases there is quite a drastic difference it would seem. This is especially stark in section 5.1 where models with exponential increasing compute/param are compared with a fixed baseline model, i.e. ResNet-20-16.\n> Q3.2: What is the tradeoff in compute and working memory (i.e. GPU VRAM) utilization for achieving this with frozen LC models v.s. a baseline learned spatial model for the same generalization performance?\n\nWe deliberately did not report computational requirements, because we do not propose to use the ResNet-LCs instead of off-the-shelf architectures (see above). \n\nOur message is freezing spatial filters reduces training time and does not affect (or even decreases) accuracy in large networks. Here is an example of common networks. We report the number of trainable parameters (as a proxy for VRAM requirements) and latency (required time for one forward and backward pass). Please note that the inference time will depend on the framework, driver, hardware etc.\n\n| Model     \t| Frozen \t| Trainable Parameters \t| Latency |\n|-----------|--------|----------------------|---------|\n| RN-50     \t| no     \t| 25.6 M               \t| 141 ms  \t|\n|           \t| yes    \t| 14.2 M               \t| 131 ms  \t|\n| WRN-50x2  \t| no     \t| 68.9 M               \t| 211 ms  \t|\n|           \t| yes    \t| 23.6 M               \t| 185 ms  \t|\n| WRN-101x2 \t| no     \t| 162.9 M              \t| 359 ms  \t|\n|           \t| yes    \t| 41.5 M               \t| 311 ms  \t|\n\n> W5; While I like the robustness analysis and think it warrants further attention, this is yet another reason why even when frozen LC models match generalization, they are poorly motivated.\n\nWe aim to explore the robustness aspect in future work. We have also observed that by using label smoothing we can further amplify the effect. One idea that we can give in this rebuttal is that due to the linear combinations, filters tend to be smoother, and at least for the initial layer this has been linked to improved robustness [1].\n\n[1] Haohan Wang, Xindi Wu, Zeyi Huang, Eric P. Xing, \u201cHigh-frequency Component Helps Explain the Generalization of Convolutional Neural Networks\u201d, CVPR, 2020.\n\n[2] ShengYun Peng, Weilin Xu, Cory Cornelius, Matthew Hull, Kevin Li, Rahul Duggal, Mansi Phute, Jason Martin, Duen Horng Chau, \u201cRobust Principles: Architectural Design Principles for Adversarially Robust CNNs\u201d, BMVC, 2023.\n\n**Please let us know if we have addressed your concerns. If you have any other questions/concerns please feel free to ask. We would be grateful if you would reconsider your score in light of our updates. Thank you again for your time and consideration.**"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission90/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699881897198,
                "cdate": 1699881897198,
                "tmdate": 1699881897198,
                "mdate": 1699881897198,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FBInrN8Ym2",
                "forum": "YSTaRLVP2G",
                "replyto": "ER6CCtCoIu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission90/Reviewer_6E1w"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission90/Reviewer_6E1w"
                ],
                "content": {
                    "comment": {
                        "value": "I'd like to thank the authors for their rebuttal, and apologize for my late participation in the rebuttal period - this was due to exceptional circumstances.\n\n> The central assumption of the reviewer\u2019s rejection rating regarding the orthogonality does not hold - neither in theory nor in practice! Theoretically (by the law of large numbers), the reviewer's orthogonality assumption only holds for very high dimensional vectors (  ), but the actual dimensionality of C used in our evaluation (and realistic applications) is far from that. We have  in our experiments.\nTo verify this, we have followed your suggestion and computed this on the largest layer in a ResNet-LC20x32 under both, uniform and normal inits. This model breaks even with the baseline - as such it should contain a significant ratio of orthogonality. Yet, even when we conservatively define orthogonality with cosine similarity < 0.2, 0% of the filter pairs are orthogonal in both cases.\n\nThis has little to do with the law of large numbers. It's just simply a fact that sampling random vectors from high-dimensional spaces is unlikely to result in vectors that are collinear, and yes even 3x3x64 is still a relatively high-dimensional space where I believe that will hold. If all your basis vectors (i.e. spatial filters) are co-linear as you are claiming above, you could never represent anything very different with any of your LC models's filters... so that makes very little sense! I believe it sounds like you have not computed the orthogonality of the spatial filters correctly (note this is the random spatial filters only you should be looking at).\n\n> Just to clarify: Generally, we do not promote training a frozen LCified network over a non-LC baseline. That would be indeed useless (although in some cases it is an easy trick to improve performance - e.g., see Tab. 4 in the Appendix). The ResNets-LC are just proxies for linear combinations in real architectures.\n> Instead, we ask: why would you train all parameters, if not learning the spatial convolutions obtains similar or even better accuracy at a significantly reduced parameter count and lower VRAM, faster training etc.?\n> ...\n>Are such networks realistic? Yes. SOTA research on adversarial robustness (see https://robustbench.github.io/ under CIFAR-10, ) already uses WideResNets-70x16 [2] (16 times wider than a normal ResNet)! We assume that we would obtain cheaper and better training with freezing. Sadly we cannot verify this, due to the necessary GPU VRAM to run experiments. And, generally, we are all aware of the trend for larger models.\n>\n> Does this clarify our intent? Would the reviewer suggest that we update our conclusion with this text?\n\nUnfortunately that doesn't clarify anything for me, in many ways it confuses me further. If your question is \"why would you train all parameters..\", and simultaneously you are claiming \"we do not promote training a frozen LCified network over a non-LC baseline. That would be indeed useless\", then hasn't that answered the question you claim to pose? Also since VRAM is the most bottlenecked resource for DNN training on GPU, any tradeoffs you show for \"efficiency\" in training LC models at the sacrifice of VRAM is very poorly motivated indeed.\n\n> Our results show that, eventually, the utility of learned convolutions saturates and beyond that deteriorates performance - if networks are sufficiently large.\n\nThis is indeed the most interesting bit of your results I'll grant you if it holds up, but I'm not convinced that the results do hold up given the other issues I highlighted on using the same hyper-parameters across models of very different numbers of learnable parameters.\n\n> We have opted for an extra-long schedule relative to the small datasets (200 Epochs) to ensure appropriate convergence. We have ensured that the training accuracy of learnable LC-ResNets at any expansion is near 100%, i.e. they have converged. Further, earlier experiments with shorter schedules (75 Epochs under cosine decay) showed similar trends. Thus we are confident that we appropriately train the models. Tuning is mostly trial-and-error, of course, it may be possible to skew the results in either way - we are confident that using these parameters for all experiments allows for a fairer comparison.\n\nAn extra-long training schedule does not make up for potentially poor choices of hyper-parameters in training models with very different numbers of trainable parameters. While I appreciate this is a difficult thing to do, giving us some idea of how you've found the hyper parameters for different models (or trying some tuning to verify they are reasonable) is necessary when you are comparing two training runs of very different models.\n\nUnfortunately this rebuttal has only made me question some of the analysis and results further. I apologize for being so late to the review process, but I hope the authors can further clarify."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission90/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682678037,
                "cdate": 1700682678037,
                "tmdate": 1700682692564,
                "mdate": 1700682692564,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eDZcaghGjc",
            "forum": "YSTaRLVP2G",
            "replyto": "YSTaRLVP2G",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission90/Reviewer_SmiK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission90/Reviewer_SmiK"
            ],
            "content": {
                "summary": {
                    "value": "This paper analyzes the power of using random convolutions in different neural architectures. The authors find that linear combinations suffice to effectively recombine random conv filters into expressive network operations (by means of 1x1 convs). The authors also emphasize on the fact that the difference between learnable and random kernels becomes larger as the size of the kernels is increased."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The presentation of the paper is very good. All the findings are provided in a digestible and organized way, making the paper an interesting, intriguing and enjoyable read. The study is carried out in a systematic way, shedding light into the modus operandi of CNNs. \n\nAltogether I believe this is a very strong submission with a little flaw in its evaluation."
                },
                "weaknesses": {
                    "value": "The only weakness I see is that the experiments in Section 5.2 are not conclusive. I would encourage the authors to try and improve this part as it weakens the analysis of the paper \u2013which is very strong up to this point."
                },
                "questions": {
                    "value": "Do you have any insights wrt modus operandi of long convolutional models and how it differs from that of conventional (small kernel) architectures?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission90/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission90/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission90/Reviewer_SmiK"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission90/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819466843,
            "cdate": 1698819466843,
            "tmdate": 1699635933823,
            "mdate": 1699635933823,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WIPdicGG6Y",
                "forum": "YSTaRLVP2G",
                "replyto": "eDZcaghGjc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission90/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission90/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "> S1: Altogether I believe this is a very strong submission with a little flaw in its evaluation.\n\nThank you for stating that our submission is very strong! Could you please clarify why you think the evaluation has a flaw? We would be grateful if you could tell us what we need to improve for you to increase your score.\n\n> W1: The only weakness I see is that the experiments in Section 5.2 are not conclusive. I would encourage the authors to try and improve this part as it weakens the analysis of the paper \u2013which is very strong up to this point.\n\nSec. 5.2 (\u201cScaling to other problems\u201d) shows that our results are not limited to CIFAR-10 but scale to other datasets and tasks. We think it is a very important section - otherwise, readers may be concerned that the findings don\u2019t scale (e.g., see Reviewer gGqG). Could it be that you meant to some other section?\n\n> Q1: Do you have any insights wrt modus operandi of long convolutional models and how it differs from that of conventional (small kernel) architectures?\n\nLong convolutions are more popular in NLP tasks, where it seems reasonable to assume that there is a correlation between distance in the context and relevance, i.e., far tokens are less relevant than closer ones. As such, long convolution kernels often show a decay of magnitude with depth (e.g., see [1]), which is very similar to the large kernel findings on vision we have in Sec. 5.2. Thus we would expect similar implications: I) current init methods won\u2019t allow to us to reconstruct these filters only via LCs from random filters; II) we need better init methods for large/long convolution weights.\n\n[1] Y. Li, T. Cai, Y. Zhang, D. Chen, and D. Dey, \u201cWhat makes convolutional models great on long sequence modeling?\u201d, arXiv preprint arXiv:2210.09298, 2022."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission90/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699879879232,
                "cdate": 1699879879232,
                "tmdate": 1699879879232,
                "mdate": 1699879879232,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wCsMqRIFhU",
                "forum": "YSTaRLVP2G",
                "replyto": "WIPdicGG6Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission90/Reviewer_SmiK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission90/Reviewer_SmiK"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your answers.\n\nWrt W1, I was referring to the final of Sec. 5.2. Here you stated that on ImageNet, \"we were not able to outperform the baseline, but we attribute this to the granularity of the tested expansion rates and the fluctuations in the measurements of a single run\", which gives the feeling that experiments on ImageNet were inconclusive. Could you comment on that? \n\nBest,\n\nThe reviewer"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission90/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639707981,
                "cdate": 1700639707981,
                "tmdate": 1700639707981,
                "mdate": 1700639707981,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]