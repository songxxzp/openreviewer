[
    {
        "title": "MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback"
    },
    {
        "review": {
            "id": "BRLa6JZwBx",
            "forum": "jp3gWrMuIZ",
            "replyto": "jp3gWrMuIZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2811/Reviewer_Nxjw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2811/Reviewer_Nxjw"
            ],
            "content": {
                "summary": {
                    "value": "This paper evaluates LLMs in multi-turn interaction with the help of tools and language feedback. The authors introduces the MINT benchmark composing of code generation, decision making, and reasoning tasks, and compared 20 open- and closed-source LLMs. Results show that LLMs benefit from multiple rounds of tool-use and language feedback, and LLMs trained with supervised instruction-finetuning (SIFT) and RLHF may hurt multi-turn performance. Moreover, better single-turn performance does not generalize to better multi-turn performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper proposes an interesting MINT benchmark to evaluate multi-turn interactions using code execution and language feedbacks. The modification of using a unified subset of examples across a bundle of tasks would be useful for the community in general.\n2. Results showing that supervised instruction-finetuning and RLHF may hurt model performance of multi-turn interactions with tools and language feedback, and worse performance on multi-turn compared to single-turn, may suggest that multi-turn interaction data is required. This provides valuable suggestions to alignment research."
                },
                "weaknesses": {
                    "value": "1. There is no comparison to simple baselines, such as self-critic (which is essentially a special case of multi-turn interactions defined in this paper). Other baselines include only providing binary feedback, or sample k times and ask a LLM to select the final answer. Without such baselines, it is not clear how much language feedback the model actually incorporates in. Furthermore, I generally like the idea t hat using a LLM to provide feedback can improve a model's performance in general, but can the authors provide any benefit (such as efficiency) comparing using a large LLM as a feedback provider, compared to using a LLM to generate the results directly (either using self-critic or not)?\n2. The proposed MINT dataset is relatively biased towards math problems that specifically require code execution and can benefit from explicit human feedback (e.g., correct or incorrect).  It seems that the only tool used is the python execution tool. Compared to other papers leveraging tools (toolformer, ReAct), the claim of utilizing tools through turns would improve model performance is not very convincing, especially for tasks that may not require code execution (e.g., HotpotQA). Moreover, I agree that selecting a subset of datasets would hinder fair comparisons, but sampling for instances, only 43 examples for HotpotQA (which is arguably the only multihop-qa dataset in MINT) may not be statistically large enough to represent the performance on the dataset. More importantly, MINT is limited to multi-turn interactions using the specific code tools and language feedback used in this paper, not the more natural and systematic tasks that require multi-turn interactions.\n3. Accordingly, results suggesting that for example, \"better single-turn performance does not guarantee better multi-turn performance\" and \"RLHG may hurt multi-turn capabilities\" are limited to the setup that those worse performance would largely be due to \"less viable of incorporating feedback\" rather than necessarily indicating that they are indeed worse on multi-turn interactions with agents in general. Therefore, the claims are not well justified."
                },
                "questions": {
                    "value": "1. Where did you compare to the \"Lazy User-LLM Interaction\"? Or is this only for k=1?\n2. The proposed method is very similar to the Tree-of-thought paper (Yao et al., 2023) where the authors used LMs as a feedback/reward provider.\n\nYao et al., 2023. Deliberate Problem Solving with Large Language Models."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2811/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698723550268,
            "cdate": 1698723550268,
            "tmdate": 1699636223840,
            "mdate": 1699636223840,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JxHaMRzNbq",
                "forum": "jp3gWrMuIZ",
                "replyto": "BRLa6JZwBx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2811/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2811/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their detailed responses, and we are glad that the reviewer find our paper important and easy to follow.\n\n> \u2026 no comparison to simple baselines, such as self-critic ... \n\nWe evaluate self-critic in section 3.4 when LLMs provide feedback for themself. We find that even GPT-4 suffers from performance degradation, suggesting the limited performance gain from self-critic.\n\n> \u2026 can the authors provide any benefit (such as efficiency) comparing using a large LLM as a feedback provider, compared to using a LLM to generate the results directly\u2026?\n\nWe believe such a comparison is irrelevant to our research questions. MINT never intends to use a large model to *improve* the performance of the smaller model, but to **evaluate** the latter's capabilities.\n\nOne of MINT\u2019s goals is to **quantify a model\u2019s ability to leverage language feedback**, which, in the real-world setting, is provided by users in their multi-turn interaction with LLM. The decision to employ GPT-4 (a larger/stronger LLM) as a feedback provider was driven by the need to establish a reproducible way to simulate users' language feedback for the evaluation of LLMs.\n\n> It seems that the only tool used is the python execution tool\u2026\n\nPlease refer to the general response.\n\n> \u2026 dataset \u2026 biased towards math problems that specifically require code execution and can benefit from explicit human feedback \u2026 not the more natural and systematic tasks \u2026 only 43 examples for HotpotQA \u2026 may not be statistically large enough to represent the performance\u2026\n\nWe argue that MINT is *not* biased toward math (~25% of instances are math). It covers a wide range of tasks, including math, code generation, decision-making, and multihop QA, that reflect the representative and commonly occurring tasks that can benefit from tool & feedback (our goal of evaluation).\n\nRegarding the size of our dataset, we had to trade-off between the number of examples and the cost of evaluation. Our original dataset contains 29k entries; while we could have included all these data points for evaluation with sufficient funding, it would have been prohibitively expensive (for GPT-4 to provide feedback) for the academic community to use and hindered the broader research and dissemination.\n\nIn preliminary experiments, we found that the models' performance trend across the selected data points is consistent and representative of that on a larger selection. Our decision on downsampling largely reduces the cost of MINT without negatively impacting the conclusions one can draw with it. We believe that a sample of 500 entries that requires interaction is an appropriate compromise, as seen in other datasets in the field (e.g., TheoremQA, AlfWorld). \n\nWe will also release the script we used to down-sample these datasets so users may choose to up-sample the HotpotQA subset when deemed necessary.\n\n> results \u2026 are limited to the setup that those worse performance would largely be due to \"less viable of incorporating feedback\" rather than necessarily indicating that they are indeed worse on multi-turn interactions with agents in general...\n\nWe would like to clarify that these findings hold regardless of whether the GPT-4 simulated feedback is included, as discussed at the end of the introduction. Particularly, in Section 3.2, we find supporting evidence that even when **no GPT-4 feedback is provided at all, the claims the reviewer mentioned still hold**.\n\nIf the reviewer is referring to \u201cfeedback\u201d from the Python interpreter (i.e., observation), we believe that the capabilities of improving from *observations* in multi-turn interaction and incorporating natural language feedback are highly correlated. We are happy to hear the reviewers' thoughts on how to disentangle them and include experiments to verify them accordingly.\n\n> Where did you compare to the \"Lazy User-LLM Interaction\"? Or is this only for k=1?\n\nAs explained in Section 2.1, \u201cLazy User-LLM Interaction\u201d defines the behavior of users in interaction (i.e., provides *at most one response* when the solution is wrong). This alone corresponds to the setting of k=2 (2 turns of LLM response): LLM proposes answer (at most twice) for each response with no tools used, which deviates from our goal of evaluating tool and feedback incorporation capabilities. Therefore, \u201cLazy User-LLM Interaction\u201d is just a description of the user behavior when no GPT-4 simulated feedback is involved. We will make this clearer in the next revision.\n\n> The proposed method is very similar to the Tree-of-thought paper (Yao et al., 2023) where the authors used LMs as a feedback/reward provider\n\nWe emphasize that our focus **differs significantly** from that of Tree-of-thought (ToT). Despite some similarity in methodology, MINT focuses on **evaluating** any LLM under the setting of multi-turn interactions with tools and feedback, which diverge substantially from the objectives of the ToT that shows prompting LLM in ToT improves problem-solving performance."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2811/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700369962092,
                "cdate": 1700369962092,
                "tmdate": 1700370230794,
                "mdate": 1700370230794,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ff3jSW1stR",
                "forum": "jp3gWrMuIZ",
                "replyto": "JxHaMRzNbq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2811/Reviewer_Nxjw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2811/Reviewer_Nxjw"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the responses.\n\nI would like to clarify that I understand that the goal of the paper is to evaluate LLMs and there is a computational constraint. However, I am still not convinced by the claim of \"interaction with tools\" where python interpreter is the only tool evaluated. Furthermore, regarding the size of data, if sampling cost is a limit (to verify the statistic significance), you may be able to show the performance on a larger sample set using closed models. If the claim is to evaluate on different types of tasks, number of examples per category should be adequate."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2811/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700629170501,
                "cdate": 1700629170501,
                "tmdate": 1700629170501,
                "mdate": 1700629170501,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "reF1GqgtZB",
            "forum": "jp3gWrMuIZ",
            "replyto": "jp3gWrMuIZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2811/Reviewer_hgTu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2811/Reviewer_hgTu"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents MINT, a benchmark that evaluates LLMs\u2019 multi-turn interaction capabilities, in particular when LLMs are using external tools and provided with natural language feedback from users (simulated by GPT-4). The experiments use a subset of existing evaluation datasets on reasoning, coding, and decision-making tasks, resulting in 586 instances, which require multi-turn interaction (judged by GPT-3.5). \nThe authors compare 20 open- and close-sourced LLMs on these instances and present 7 findings (listing them in order to easily refer to them in the other parts of the review):  \n\n1. All models benefit from tool interaction and natural language feedback\n2. Better single-turn performance does not lead to better multi-turn performance\n3. There is a notable gap between open- and closed-source LLMs in multi-turn interaction performance\n4. Models trained with supervised instruction fine-tuning (SIFT) and reinforcement learning from human feedback (RLHF) perform worse in multi-turn settings \n\nFrom the text:\n\n5. LLMs\u2019 feedback-providing capability could be orthogonal to task-solving ability\n6. MINT\u2019s evaluation reveals undesired artifacts in ShareGPT data\n7. GPT-4 simulated language feedback is as helpful as human-written feedback based on human evaluation and task performance."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The work studies a **timely, important topic** of LLMs\u2019 evaluation in interactive settings, striving to bridge the gap between traditional static/single-turn benchmarks in NLP and \u201creal-world use cases\u201d (as noted in abstract). The use of LLMs as a proxy for human feedback reduces cost significantly and increases scalability.\n* The paper is overall **well-written** and the design of the experiments seem **sound**. In addition, I find the **details of the experiments** are properly documented either in the main paper or appendix.\n* Even if the exact numbers in the findings are likely to become less relevant in the future due to the fast-changing landscape of LLMs, the **high-level findings and takeaways are likely to be still relevant**, especially about single-turn vs. multi-turn performance between models (e.g., #2 and #4)."
                },
                "weaknesses": {
                    "value": "* Although the paper presents many interesting findings, it wasn\u2019t clear to me which ones are **new findings vs. confirmation of existing findings**. For instance, #1 has been shown by ToolLLM (Qin et al., 2023), Tool Augmented Language Models (TALM) (Parisi et al., 2022), Chain of Hindsight (Liu et al., 2023), etc.;  and #2, #3, and #4 have been (fully or partially) shown by Human-AI Language-based Interaction Evaluation (HALIE) (Lee et al., 2023); and #7 has been studied by Liu et al. (2023), Chiang & Lee (2023), Gao et al (2023), Shen et al (2023), and Duan et al. (2023). Situating the findings in the existing literature will give this work more validity and help readers understand the contributions of the paper.\n* For that reason, it is a bit unclear to me what the **main contribution of this paper** is at the moment. In comparison to prior work, the authors note that \u201cDifferent from prior work, MINT covers a range of diverse tasks and is designed to measure the multi-turn interaction capabilities of LLMs with both tools and user feedback that are more aligned with real-world applications.\u201d As some of the prior work covers a range of diverse tasks/measures multi-turn interaction capabilities of LLMs/simulates human feedback with LLMs, the main difference of this work is (presumably) accounting for **the use of tools and user feedback** on top of all of these elements. In this case, I wonder if it'll help to scope the description of the tasks more tightly (e.g., tasks that can benefit from tool use, as opposed to open-ended generation) and provide a bit more justification for why it\u2019s important to look at these elements altogether (as there are prior work looking at the tool use and NL feedback) and what we expect to see (do we expect to see different results when these elements are added? do we actually observe any interesting interplay between these elements or is it merely a simple performance boost across all models?)."
                },
                "questions": {
                    "value": "* Most of the questions that can change my opinion are described in the weaknesses. I'm generally excited about the direction of the paper, and with clarification and contextualization of this work\u2019s contributions and findings with respect to prior/concurrent work, I\u2019d be happy to reconsider and adjust my ratings. Here, I list some questions/comments I had while reading the paper.\n* Maybe another bit that\u2019s quite important to convey nuanced findings in the paper is the comparison of GPT-4-generated feedback to human-written feedback. Although the surface-level similarity and helpfulness are similar, my guess is that GPT-4 generated feedback lacks the variability that we\u2019d normally observe in human-written feedback, which can easily affect these models\u2019 performance (e.g., Lee et al., 2023). The limitation section in the appendix mentions coverage of GPT-4 generated feedback, which is relevant but different from this point.\n* Similarly, although I generally agree that this is a step towards including more real-world-like elements compared to evaluating LLMs without tool use and natural language feedback, I wouldn\u2019t say that the current setup is strictly \u201cmore aligned with real-world applications\u201d than prior work as there is a clear trade-off due to simulating human-written feedback with LLMs and potential drawbacks with the simulation (e.g., Koo et al. (2023), Rajani et al. (2023)).\n* In finding #3, could the size of a model be the main confounder? Is the finding only applicable to multi-turn settings or also in single-turn settings?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2811/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2811/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2811/Reviewer_hgTu"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2811/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698785806796,
            "cdate": 1698785806796,
            "tmdate": 1700694945054,
            "mdate": 1700694945054,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oEIWbxaKzW",
                "forum": "jp3gWrMuIZ",
                "replyto": "reF1GqgtZB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2811/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2811/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their detailed and thoughtful responses, and we are glad that the reviewer found our paper important, sound, and easy to follow.\n\n> \u2026it wasn\u2019t clear to me which ones are new findings vs. confirmation of existing findings\u2026\u2026Situating the findings in the existing literature will give this work more validity and help readers understand the contributions of the paper\u2026  As some of the prior work covers a range of diverse tasks/measures multi-turn interaction capabilities of LLMs/simulates human feedback with LLMs, the main difference of this work is (presumably) accounting for the use of tools and user feedback on top of all of these elements\u2026 why it\u2019s important to look at these elements altogether (as there are prior work looking at the tool use and NL feedback) and what we expect to see?\u2026 I wouldn\u2019t say that the current setup is strictly \u201cmore aligned with real-world applications\u201d than prior work as there is a clear trade-off due to simulating human-written feedback with LLMs and potential drawbacks with the simulation (e.g., Koo et al. (2023), Rajani et al. (2023)).\n\nWe acknowledge the contributions and findings from existing literature on tool-augmented language models (e.g., ToolLLM) and human-AI interactions (e.g., HALIE). Different from existing work that independently contributed to the understanding of a particular element (e.g., tools, multi-turn, human feedback), to the best of our knowledge, MINT is the first to put everything together.\n\nMINT, by unifying the evaluation dataset for both the tool and language feedback aspect, allows us to examine the interplay between these two elements, i.e., the trade-off between tool performance and the model\u2019s ability to leverage language feedback, which is common in real-world applications (e.g., chatGPT with plugins). We actually can observe such a trade-off in MINT\u2019s evaluation of the Lemur model [1]: we find that their incorporation of reasoning (e.g., OpenOrca) task-specific single-turn data in SIFT improved the model\u2019s reasoning performance (tool-only) without language feedback. However, training on such single-turn data sacrifices their ability to leverage language feedback (e.g., only +0.9 improvement when language feedback is given in Table 3). We will include more discussion on relevant literature and better situate our findings to make it clearer.\n\n[1] Lemur: Harmonizing Natural Language and Code for Language Agents\n\n> \u2026 Although the surface-level similarity and helpfulness are similar, my guess is that GPT-4 generated feedback lacks the variability that we\u2019d normally observe in human-written feedback, which can easily affect these models\u2019 performance (e.g., Lee et al., 2023)...\n\nWe acknowledge that although it has been more and more widely accepted to use GPT-4 to provide feedback, this approach has the limitations that the reviewer points out (i.e., they might not capture the variability that might present in real human feedback). Possible solutions include explicitly prompting GPT-4 to play various personas, which would complicate the evaluation and out of the scope of MINT but is nonetheless worth exploring. We conjecture that even though our setting (with less variety) does not perfectly reflect the models' performance with real human feedback, it highly correlates with it and provides a good starting point for future research that studies the difference between real-world human feedback and GPT-4 simulated feedback. We will include more discussion and clarifications about this in our next revision.\n\n> In finding #3, could the size of a model be the main confounder? Is the finding only applicable to multi-turn settings or also to single-turn settings?\n\nThis is a great question! As discussed in section 3.2, size does play a role in the multi-turn performance: multi-turn tool-augment interaction performance of LLM with the same training data improves with scale. However, since commercial models did not release their actual model size, it is hard to say whether their good performance in multi-turn interaction comes from model scaling versus better data. We believe that finding #3 mainly applies to multi-turn settings since we did not comprehensively test LLM\u2019s single-turn performance with well-established benchmarks (e.g., MMLU, GSM8K)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2811/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700367924716,
                "cdate": 1700367924716,
                "tmdate": 1700367924716,
                "mdate": 1700367924716,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aYcNteMwie",
                "forum": "jp3gWrMuIZ",
                "replyto": "oEIWbxaKzW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2811/Reviewer_hgTu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2811/Reviewer_hgTu"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response! My questions have been all answered by the authors."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2811/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700693819148,
                "cdate": 1700693819148,
                "tmdate": 1700693819148,
                "mdate": 1700693819148,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Vy2Vm0DOuv",
            "forum": "jp3gWrMuIZ",
            "replyto": "jp3gWrMuIZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2811/Reviewer_vnHy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2811/Reviewer_vnHy"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates the capabilities and limitations of large language models (LLMs) when solving tasks with multi-turn interactions by using tools and leveraging natural language feedback. The main contributions of the paper can be summarized as follows:\n(1) The authors propose a structured evaluation framework where LLMs can leverage tools via executing Python code and receive users' natural language feedback;\n(2) The authors have conducted abundant experiments, analyzing 20 open- and closed-source LLMs over 8 datasets.\n(3) From the experiments, the authors have made some interesting observations, like RLHF may hurt multi-turn interaction performances in problem solving."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The authors introduce MINT, a novel benchmark and framework, that evaluates LLMs' capabilities to solve tasks with multi-turn interactions with tool usage and natural language feedback.\n- The authors conduct abundant experiments, covering 20 open- and closed-source LLMs and 8 datasets from different tasks.\n- The authors have made some interesting statements, including (1) better single-turn performance does not guarantee multi-term performance; (2) RLHF may hurt the multi-turn capabilities. Corresponding experimental results and analysis are included to support the claims.\n- The paper is well-written and is easy-to-follow. The visualizations are also clear and can help readers better understand the framework and model performance comparisons."
                },
                "weaknesses": {
                    "value": "Overall, this is a very thorough and comprehensive benchmark paper. I have only several concerns: \n- The authors claim that the SIFT can benefit models' capabilities of tool-augmented task-solving in multi-turn interaction (section 3.2), while the authors also claim that the SIFT can hurt models' ability to leverage feedback (Section 3.3). Why do these two claims seem to contradict each other? \n- I am also very curious about the trade-off between tool-use capabilities and abilities to leverage human feedback. SIFT on general domain multi-turn interaction data (like ShareGPT, in section 3.2) can bring performance gain in problem-solving. I think this might be because of the multi-turn feature that caters to the final application. I also think if the model can SIFT on some task-specific data (if exists), the performance can also improve. I am very curious about which is more important for SIFT."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2811/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698789658319,
            "cdate": 1698789658319,
            "tmdate": 1699636223700,
            "mdate": 1699636223700,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZlWDoG7Pi7",
                "forum": "jp3gWrMuIZ",
                "replyto": "Vy2Vm0DOuv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2811/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2811/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their detailed and thoughtful responses, and we are glad that the reviewer found our paper novel, important, and easy to follow.\n\n> The authors claim that the SIFT can benefit models' capabilities of tool-augmented task-solving in multi-turn interaction (section 3.2), while the authors also claim that the SIFT can hurt models' ability to leverage feedback (Section 3.3). Why do these two claims seem to contradict each other?\n\nWe thank the reviewer for pointing out this confusion! In summary, we find that SIFT trained on **multi-turn** conversations (e.g., Vicuna, Lemur was trained on ShareGPT) have improved tool-use (Section 3.2) and leveraging feedback capabilities (Section 3.3); While for other models (e.g., LLaMA, CodeLLaMA), SIFT can hurt their capabilities. We will make this part clearer in the revision.\n\n> I am also very curious about the trade-off between tool-use capabilities and abilities to leverage human feedback. SIFT on general domain multi-turn interaction data (like ShareGPT, in section 3.2) can bring performance gain in problem-solving. I think this might be because of the multi-turn feature that caters to the final application. I also think if the model can SIFT on some task-specific data (if exists), the performance can also improve. I am very curious about which is more important for SIFT.\n\nWe agree that it is an interesting and important trade-off regarding the use of general-domain multi-turn conversation (e.g., ShareGPT) versus task-specific data in improving models with SIFT. In MINT\u2019s evaluation of the Lemur model [1], we find that SIFT on task-specific data (e.g., OpenOrca for reasoning) significantly improved the model\u2019s ability to perform reasoning *without* language feedback; However, the task-specific dataset OpenOrca only contains single-turn data, which we hypothesize could make the trained model more \u201cstubborn,\u201d i.e., sacrificing their ability to leverage language feedback (e.g., only +0.9 improvement in Table 3 when given language feedback).\n\nIf our ultimate goal is to improve the model\u2019s single-turn performance (single question-answer pair), we think training on task-specific data might be sufficient. However, If the ultimate goal is to build a helpful interactive assistant that is not only good at providing good answers in the first turn, but can also collaborate with users in multi-turn settings to correct its previous answers (e.g., an almost perfect piece code that only needs a small tweak to work), balancing the model\u2019s ability of direct problem-solving AND leveraging language feedback in a collaborative setting would be essential. We believe a more rigorous study could be done as a follow-up to further analyze each SIFT component\u2019s effect on the model\u2019s different capabilities.\n\n[1] Lemur: Harmonizing Natural Language and Code for Language Agents"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2811/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700367238597,
                "cdate": 1700367238597,
                "tmdate": 1700367238597,
                "mdate": 1700367238597,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UvHMA2P3hi",
                "forum": "jp3gWrMuIZ",
                "replyto": "ZlWDoG7Pi7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2811/Reviewer_vnHy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2811/Reviewer_vnHy"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Reviewer vnHy"
                    },
                    "comment": {
                        "value": "Thank you for your efforts during the rebuttal stage. My concerns have been resolved. I will maintain my positive score and vote for acceptance."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2811/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700450504381,
                "cdate": 1700450504381,
                "tmdate": 1700450504381,
                "mdate": 1700450504381,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9CEgNnu0D2",
            "forum": "jp3gWrMuIZ",
            "replyto": "jp3gWrMuIZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2811/Reviewer_VNEC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2811/Reviewer_VNEC"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduced MINT, a benchmark for evaluating LLMs when they can interact with external tools and natural language feedback. In the benchmark environments, \"tools\" are considered as functions that can be called by executing the LLM-generated Python code, and \"natural language feedback\" is simulated using GPT-4. The benchmark covers tasks such as math reasoning and code generation, where only instances that require multiple turns of interaction are retained. In experiments, a comprehensive set of open/closed-source LLMs are evaluated on MINT."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. LLM evaluation in an interactive environment is an important and novel topic.\n2. Experiments on a comprehensive set of LLMs under different settings (base vs. SIFT vs. RLHF, open-source vs. closed-source) were conducted, resulting in several interesting observations (e.g., RLHF may hurt LLM-tool interaction while SIFT helps).\n3. The paper is easy to follow."
                },
                "weaknesses": {
                    "value": "My major concerns lie in the natural language feedback simulation setup:\n1. The example in Figure 1 seems to assume a \"collaborative work\" interaction between the LLM and the user, as opposed to the more common case where a user seeks assistance from the LLM; in the latter case, it is not practical to assume such detailed feedback from the user because the user does not know how to solve the problem (otherwise they have no need to seek help from the LLM). In some sense, the current LLM-user interaction has made the user a \"teacher\" who instructs the LLM to correct its mistakes.\n\n    In fact, user feedback has been widely studied in code generation/semantic parsing tasks (references below, which are missing in related work discussion). Prior work has particularly focused on collecting or simulating natural language feedback that is close to \"what a prospective, help-seeking user in practice could give\". While MINT's feedback interaction is still valuable (as it assesses how well an LLM can incorporate the feedback, no matter if it is realistic or not), I'd like to note that its kind of feedback may not reflect the realistic case, and this should be clarified in the paper. \n\n    References:\n    - Ziyu Yao, Yu Su, Huan Sun, and Wen-tau Yih. 2019. Model-based Interactive Semantic Parsing: A Unified Framework and A Text-to-SQL Case Study. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5447\u20135458, Hong Kong, China. Association for Computational Linguistics.\n    - Ahmed Elgohary, Saghar Hosseini, and Ahmed Hassan Awadallah. 2020. Speak to your Parser: Interactive Text-to-SQL with Natural Language Feedback. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2065\u20132077, Online. Association for Computational Linguistics.\n    - Hao Yan, Saurabh Srivastava, Yintao Tai, Sida I. Wang, Wen-tau Yih, and Ziyu Yao. 2023. Learning to Simulate Natural Language Feedback for Interactive Semantic Parsing. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3149\u20133170, Toronto, Canada. Association for Computational Linguistics.\n\n2. The human evaluation of GPT-4 generated feedback does not look at the discrepancy mentioned above either.\n\n3. As indicated in footnote 4, evaluating one LLM using MINT costs around 100 USD, due to the use of GPT-4 for feedback simulation. This cost can be an issue for iteratively improving (and then evaluating) an LLM. \n\nOther than the feedback simulation, another comment on the tool interaction:\n\n4. The current tool use has been limited to Python function calls. It offers better reproducibility but also loses the benchmark scope."
                },
                "questions": {
                    "value": "1. Can the authors respond to my comment on the feedback simulation discrepancy?\n2. Can the authors share thoughts on reducing the benchmark cost?\n3. Can MINT be integrated with AgentBench (Liu et al., 2023) or InterCode (Yang et al., 2023), so that a larger task scope could be facilitated with both tool and user interaction?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2811/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820133356,
            "cdate": 1698820133356,
            "tmdate": 1699636223599,
            "mdate": 1699636223599,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZVsXJdNYnt",
                "forum": "jp3gWrMuIZ",
                "replyto": "9CEgNnu0D2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2811/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2811/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their detailed responses, and we are glad that the reviewer found our paper novel, important, and easy to follow.\n\n>... Figure 1 \u2026 assume a \"collaborative work\" interaction between the LLM and the user, as opposed to the more common case where a user seeks assistance from the LLM ... not practical to assume such detailed feedback from the user because the user does not know how to solve the problem... While MINT's feedback interaction is still valuable \u2026its kind of feedback may not reflect the realistic case, and this should be clarified in the paper\u2026 The human evaluation of GPT-4 generated feedback does not look at the discrepancy \u2026\n\nWe thank the reviewer for suggesting relevant work, which we will include in our next revision. For ease of evaluation, in MINT, we disentangle a user\u2019s \u201cassistance-seeking\u201d and \u201ccollaborative\u201d feedback-providing behavior.\n\nThe \u201c**assistance-seeking**\u201d (not depicted in figure 1) corresponds to (1) the initial user-provided instruction to LLM and (2) templated user feedback LLM received (e.g., Your answer is wrong) when proposing a solution that users disagree with (Lazy User-LLM Interaction section on Page 3, Table 2). As a side note, we think this is similar to real-world user interaction with LLM: even when the user does not know how to solve the problem, it is still possible for the user to provide feedback. For example, I might not know how to write code to solve a particular problem from scratch, so I ask LLM to help, but I might be able to understand and judge the LLM-produced code (and its intermediate code execution result) to see if it fits my expectations and can response with minimal feedback (e.g., \"this is not what i want\" - similar to the templated feedback). MINT focuses on evaluating LLM on these types of \u201chard to solve directly, but easy to verify\u201d problems (Table 1).\n\nFor the latter \u201c**feedback-providing**\u201d behavior (a more collaborative setting), we use GPT-4 to simulate feedback to mimic the \u201ccollaborative\u201d feedback aspect of the user, which is the dotted box shown in Figure 1 with results in Table 3. As described in Section 2.1, we separately evaluated these two as \u201c*LLM-Tool interaction with Lazy User-LLM Interaction*\u201d and \u201c*Informative User-LLM interaction with Language Feedback*.\u201d We will use the suggested wording to explicitly distinguish and clarifies these type of interactions in the revision.\n\nMINT leverages GPT-4 (a stronger / larger model) to simulate human language feedback, which is reproducible and costs 14x less (footnote 4) compared to hiring real humans. We do agree that GPT-4 simulated feedback can be different compared to real-world human feedback, which is evidenced in our human evaluation in Table 5, where we found most human feedback is comparatively less helpful than GPT-4 feedback. Still, such GPT-4 simulated feedback can serve as an initial step to help us gauge LLM\u2019s ability to incorporate feedback (i.e., can they incorporate feedback that\u2019s already very detailed and helpful?), which can provide future research that studies the difference between real-world human feedback and GPT-4 simulated feedback a good starting point. We will include clarifications in our next revision to make this more clear.\n\n> As indicated in footnote 4, evaluating one LLM using MINT costs around 100 USD, due to the use of GPT-4... can be an issue for iteratively improving (and then evaluating) an LLM... thoughts on reducing the benchmark cost?\n\nMINT, similar to the widely used MT-Bench [1],  chose to use GPT-4 due to its effectiveness and reproducibility. We are aware that such a cost might become a roadblock for iterative improvement, and therefore, we made significant efforts to keep MINT\u2019s evaluation dataset compact and representative to reduce the GPT-4 feedback cost down to ~100 USD (98% less compared to the original size of the datasets). Furthermore, Human evaluation costs on a similar quantity of examples are likely to cost 14x more (footnote 4), which already makes GPT-4 feedback simulation more cost-effective.\n\nIn our design, we deliberately made the feedback provider replaceable (Section 3.4), which allows potential future cost-saving by replacing GPT-4 with a cheaper model that specializes in providing feedback (e.g., [2]).\n\n[1] Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\n\n[2] Shepherd: A Critic for Language Model Generation\n\n> The current tool use has been limited to Python function calls. It offers better reproducibility but also loses the benchmark scope.\n\nPlease refer to the general response.\n\n> Can MINT be integrated with AgentBench (Liu et al., 2023) or InterCode (Yang et al., 2023), so that a larger task scope could be facilitated with both tool and user interaction?\n\nYes! We do plan to continuously integrate additional tasks (e.g., AgentBench, InterCode) into MINT to further expand its scope in the future (e.g., a MINT 2.0 version) as MINT\u2019s evaluation framework is general and task-agnostic."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2811/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700366685521,
                "cdate": 1700366685521,
                "tmdate": 1700366685521,
                "mdate": 1700366685521,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ySyb744dFY",
                "forum": "jp3gWrMuIZ",
                "replyto": "ZVsXJdNYnt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2811/Reviewer_VNEC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2811/Reviewer_VNEC"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response!"
                    },
                    "comment": {
                        "value": "Thanks to the authors for answering my questions!\n\n> For example, I might not know how to write code to solve a particular problem from scratch, so I ask LLM to help, but I might be able to understand and judge the LLM-produced code (and its intermediate code execution result) to see if it fits my expectations and can response with minimal feedback (e.g., \"this is not what i want\" - similar to the templated feedback). MINT focuses on evaluating LLM on these types of \u201chard to solve directly, but easy to verify\u201d problems (Table 1).\n\nI think the authors' response is a bit misleading. Clearly the feedback shown in Fig 1 has went beyond \"this is not what i want\". The demonstrated feedback, e.g., \"You should have used ... Then you can express...\", is procedural and detailed, which may have assumed that the user knows the exact step-by-step answer. So again it looks more like a user teaching LLMs. If a user knows the problem with so many details, why don't they directly edit the code, which could even give a better correction guarantee than natural language feedback? A fundamental question here is, who will be the prospective user under this framework's formulation? People who cannot solve math problems completely but actually knows the procedures, can read code but cannot directly edit code? \n\nI understand that this work is mainly to evaluate the feedback incorporation of LLMs, which is very exciting to me and I'm still holding a positive opinion on it. However, I hope the authors could have a more in-depth discussion on the envisioned user-LLM interaction scenario and how it connects to real-world applications/user groups (or not), and acknowledge the effort of prior work in bridging these important gaps. A good example is the work of Elgohary et al., which introduced natural language explanation of SQL code such that the interface can benefit users without SQL programming expertise (i.e., who cannot read SQL code nor edit it). \n\nRegarding the tool use, I agree that it is often converted to code gen. Thanks for the reminder of F.1."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2811/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665300896,
                "cdate": 1700665300896,
                "tmdate": 1700665300896,
                "mdate": 1700665300896,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]