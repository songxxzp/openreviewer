[
    {
        "title": "DGTAT: DECOUPLED GRAPH TRIPLE ATTENTION NETWORKS"
    },
    {
        "review": {
            "id": "h6LBopr3Im",
            "forum": "bgKGwLYmAy",
            "replyto": "bgKGwLYmAy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission878/Reviewer_xSa6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission878/Reviewer_xSa6"
            ],
            "content": {
                "summary": {
                    "value": "This paper analyzes the factors influencing the performance of graph learning models. The authors proposes a model named DGTAT, which is based on MPNN and a sampling strategy. The proposed method decouples local and global interactions, separates learnable positional, attribute, and structural encodings, and computes triple attention. This design allows DGTAT to capture long-range dependencies akin to Transformers while preserving the inductive bias of the graph topology."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea of decoupling local and global interactions, as well as separating learnable positional, attribute, and structural encodings, is interesting;\n\n2. The proposed method combines several techniques, i.e. laplacian, random walk, positional encoding, GNN, which is solid;"
                },
                "weaknesses": {
                    "value": "1. The major concern is the experimental result of the proposed method. According to Table 3 and Table 4, the proposed method does not show significant improvement over baseline methods. Most of the numbers are quite close to the previous SOTA results. It is unclear to see from the results that the proposed techniques in this paper works.\n\n2. There is no experimental result showing that how the proposed components in this paper performs. It is necessary to do ablation study to see how the performance changes with/without a particular module.\n\n3. The presentation of this paper could be improved. The authors introduces a lot of notations, and sometimes it is hard to find the actual meaning of a particular notation. Figure 1 and figure 2 is also confusing and hard to understand."
                },
                "questions": {
                    "value": "see weaknesses above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission878/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698788174788,
            "cdate": 1698788174788,
            "tmdate": 1699636014411,
            "mdate": 1699636014411,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cI4C1JFkj7",
                "forum": "bgKGwLYmAy",
                "replyto": "h6LBopr3Im",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission878/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission878/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply (1/3)"
                    },
                    "comment": {
                        "value": "We appreciate your effort very much in commenting on our paper and endorsement of our idea and methods. We will explain the weaknesses that you put forward and answer your questions.\n>###  W1. The major concern is the experimental result of the proposed method. According to Table 3 and Table 4, the proposed method does not show significant improvement over baseline methods. Most of the numbers are quite close to the previous SOTA results. It is unclear to see from the results that the proposed techniques in this paper works.\n\nDGTAT shows a steady improvement over the SOTA baseline on the datasets listed in our paper, both in terms of maximum and average accuracy (Accuracy improvements ranged from 0.14\\% to 1.98\\%, with an average improvement of 1\\%). Compared to the GT-based SOTA baseline, our improvement is sufficiently significant that it is greater than or equal to the magnitude of the improvement in SOTA on these datasets by previously published GT-related works (such as GRIT, GraphGPS, etc), especially on large-scale graphs that require long-range dependency (compare with the SOTA baseline: NAGphormer).\n\nThe clean decoupled framework we propose and the results obtained from our ablation experiments, in which each decoupled component contributes, are more noteworthy. This can inspire a deeper exploration of the decoupling idea, searching for a coding that is more in line with the design idea, as well as exploring the theoretical relationship between the graph structure information and the location information.\n\nIn response to *\"It is unclear to see from the results that the proposed techniques in this paper works.\"*, we have included more ablation experiments in Appendix A to demonstrate that it is our decoupled framework and our proposed method that allows DGTAT to achieve significantly higher performance than GT baselines, as shown in our response to the next question."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission878/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700044352223,
                "cdate": 1700044352223,
                "tmdate": 1700044352223,
                "mdate": 1700044352223,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tvsCFNXlF0",
                "forum": "bgKGwLYmAy",
                "replyto": "h6LBopr3Im",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission878/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission878/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A kindly Reminder Message: 2days left in discussion phase"
                    },
                    "comment": {
                        "value": "Dear Reviewer xSa6:\n\nI hope this email finds you well. As the deadline for revision and discussion is approaching, we are sincerely looking forward to your feedback. \n\nWe understand you may have a busy schedule, but we believe that we have addressed all your concerns and refined our paper to ensure that our work meets the standards.\n\nIf you still have further concerns or feel unclear after reading our responses, please do not hesitate to reach out to us. We would be happy to have a further discussion with you. If you are satisfied with our responses so far, we sincerely hope you could consider your score. \n\nThank you in advance!\n\nBest wishes, Authors"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission878/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700559621391,
                "cdate": 1700559621391,
                "tmdate": 1700559637045,
                "mdate": 1700559637045,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mrnqXVy3RG",
            "forum": "bgKGwLYmAy",
            "replyto": "bgKGwLYmAy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission878/Reviewer_Zxgv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission878/Reviewer_Zxgv"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to decouple (position, structure, attribute) and (global interaction local interaction) in graph transformers with a novel model DGTAT. DGTAT is consistently effective on both Homophilic and heterophilic graphs at different scales."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1 The proposed method is consistently effective on both Homophilic and heterophilic graphs at different scales.\n\nS2 The proposed method avoids over-smoothing and has better expressivity."
                },
                "weaknesses": {
                    "value": "W1 The contribution of the paper is marginal. The performance improvement is most likely due to the incorporation of existing techniques like MPNN+VN, LapPE, JaccardPE, SE. \n\nW2 Why we need decoupling in graphs is not well discussed. There are only vague claims on page 14 stating that \"compared to the GT with SE/PE, through the decoupling of PE, SE and AE, DGTAT can distinguish some graphs with more sensitive position, structure, and attribute information that coupled PS/SE cannot learn\" (this is an assumption, not an explanation/analysis). Figure 5 also cannot support this claim well.\n\nW3. Only node classification experiments are performed. In contrast, standard GTs are evaluated by graph classification."
                },
                "questions": {
                    "value": "Explain W2 please."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission878/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission878/Reviewer_Zxgv",
                        "ICLR.cc/2024/Conference/Submission878/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission878/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698791838683,
            "cdate": 1698791838683,
            "tmdate": 1700185905848,
            "mdate": 1700185905848,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iXsvVcUoiq",
                "forum": "bgKGwLYmAy",
                "replyto": "mrnqXVy3RG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission878/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission878/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply (1/3)"
                    },
                    "comment": {
                        "value": "We appreciate your effort very much in commenting on our paper. Firstly we would like to clarify certain misunderstandings in W1. Then we acknowledge your concerns in W2 and have added sufficient ablation experiments in our refined work to resolve the questions of W2. At last we conducted a preliminary experiment to alley your concern in W3.\n\n>### W1. The contribution of the paper is marginal. The performance improvement is most likely due to the incorporation of existing techniques like MPNN+VN, LapPE, JaccardPE, SE.\n\nAs you said, some of the strategies we used such as MPNN+VN, LapPE, RRWP, and RRSE do already exist. They were proposed by some previous works with good results. \n\nHowever, the previous works did not delve deeper into exactly where each encoding contributes, e.g., acting on local structural information/global structural information/local position information/global position information, and how they can be incorporated.\n\nMoreover, it turns out that simply stacking and incorporating these strategies and encodings does not improve performance further due to the coupling of information (We give an analysis in the Introduction, and in our revised paper we have added ablation experiments that support this claim, as described in the response to W2). So, it is important to find a way to incorporate these techniques to improve the performance of graph learners.\n\n**Our idea and work solve this problem, and are evaluated as interesting and solid by other reviewers.** We analyzed the specific domains in which these techniques play a role. **For the first time in the GT domain, we propose the idea of decoupling structure, position and attribute information, presenting a clean decoupled framework.** \n\nBy populating our framework with already existing encodings that contain decoupled information, combined with our sampling strategy and triple-attention method, we effectively combine these encodings to make them work decoupledly and achieve SOTA performance. So, indeed, what we have done matters, and getting these technologies to incorporate well is where we excel."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission878/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700044207728,
                "cdate": 1700044207728,
                "tmdate": 1700044207728,
                "mdate": 1700044207728,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6iDEsInBF7",
                "forum": "bgKGwLYmAy",
                "replyto": "mrnqXVy3RG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission878/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission878/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply (3/3)"
                    },
                    "comment": {
                        "value": ">### W3. Only node classification experiments are performed. In contrast, standard GTs are evaluated by graph classification.\n\nThe node classification task is one of the most classical and mainstream tasks in the graph domain, and one of our motivations is to capture long-range dependencies, which requires the evaluation of medium-scale and large-scale graphs, which are generally employed in the node classification task. In addition, the node classification task can better reflect the generality of DGTAT for both heterophilic and homophilic graphs. \n\nMost of the baseline papers of GT have been evaluated on the node classification task, and some work also evaluated only the node classification task (such as NodeFormer, NAGPHORMER, and AERO-GNN). Similarly, we only consider the graph classification task and focus on the node classification task.\n\nIt is important to note that this does not mean that DGTAT does not perform well in the graph classification task. To allay your concerns, we conducted a preliminary experiment on a graph classification dataset. By using virtual nodes with a pooling readout strategy, we extended DGTAT to graph classification tasks with SOTA results in the ZINC dataset, shown as follows, and more graph classification tasks will be our follow-up work.\n|                | MAE \u2193         |\n|----------------|---------------|\n| GCN            | 0.367 \u00b1 0.011 |\n| GAT            | 0.384 \u00b1 0.007 |\n| SAN            | 0.139 \u00b1 0.006 |\n| GT             | 0.598 \u00b1 0.049 |\n| Graphprmer     | 0.122 \u00b1 0.006 |\n| K-Subgraph SAT | 0.094 \u00b1 0.008 |\n| GraphGPS       | 0.070 \u00b1 0.004 |\n| DGTAT          | 0.059 \u00b1 0.004 |\n\nMany thanks again for your devotion and comments!  All the updates and modifications are now added to our revised paper. And we summed up all the comments on this paper from every reviewer including yours and our responses to them in Appendix F in our revised paper. \n\nWe appreciate it very much if you could read our rebuttals and updates. We always remain available to answer any other questions and suggestions on our paper and improve it. We are looking forward to your reply and new reviewing grade, which matters to us."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission878/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700044291614,
                "cdate": 1700044291614,
                "tmdate": 1700135992893,
                "mdate": 1700135992893,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DMO8k0XP8x",
                "forum": "bgKGwLYmAy",
                "replyto": "6iDEsInBF7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission878/Reviewer_Zxgv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission878/Reviewer_Zxgv"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "I sincerely appreciate the comprehensive response and additional results provided. \nThe authors have successfully convinced me these two aspects: (1) the essentiality of the framework's design and (2) the efficacy of DGTAT in graph classification tasks. Based on these insights, I have revised my evaluation accordingly."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission878/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700185883871,
                "cdate": 1700185883871,
                "tmdate": 1700185883871,
                "mdate": 1700185883871,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KU0nYC87Vf",
            "forum": "bgKGwLYmAy",
            "replyto": "bgKGwLYmAy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission878/Reviewer_2B2X"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission878/Reviewer_2B2X"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel graph neural network architecture called DGTAT that decouples the propagation of positional, structural, and attribute information to improve model expressiveness and interpretability. The key ideas are: 1) Use dedicated encodings to represent positional and structural information separately from attributes; 2) Compute triple attention based on positional, structural, and attribute information; 3) Sample relevant nodes based on positional/structural attention for capturing long-range dependencies. Experiments show SOTA results on node classification tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Clearly motivates the need for decoupling different types of information in GNNs, both theoretically and empirically.\n- Proposes a clean design framework to achieve decoupling of positional, structural, and attribute information.\n- Achieves SOTA results across multiple datasets, especially on heterophilic graphs.\n- Ablation study illustrates the contribution of each decoupled component."
                },
                "weaknesses": {
                    "value": "- The sampling strategy based on positional/structural attention is heuristic and may not optimally capture long-range dependencies.\n- Increased model complexity due to decoupled computations and triple attention.\n- Lacks analysis of computational efficiency compared to baselines."
                },
                "questions": {
                    "value": "- How is the sampling distribution optimized during training? Is there a learnable component?\n- What is the empirical complexity of DGTAT compared to GT and MPNN baselines?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission878/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission878/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission878/Reviewer_2B2X"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission878/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698852526415,
            "cdate": 1698852526415,
            "tmdate": 1699636014206,
            "mdate": 1699636014206,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "E2g8qlHxDC",
                "forum": "bgKGwLYmAy",
                "replyto": "KU0nYC87Vf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission878/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission878/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply (1/2)"
                    },
                    "comment": {
                        "value": "We appreciate your effort very much in commenting on our paper and endorsement of our innovations. We will explain the weaknesses that you put forward and answer your questions.\n>###  W1. The sampling strategy based on positional/structural attention is heuristic and may not optimally capture long-range dependencies.\n\nOur motivation for proposing the sampling strategy is to improve the long-range dependency capturing capability of the model. Through the global attention computation of positional and structural encoding, we sample nodes with similar positional receptive field and structural topology without distance constraints, and connect them for communication to capture long-range dependencies. \n\nWhile the sampling strategy is heuristic, each training layer samples K optimal long-range-dependency nodes for communication. For different datasets, different number of layers and hyperparameters K are selected to ensure that every important long-range dependency is captured. We remain open to exploring potential refinements and extensions in future research.\n\nAdditionally, the introduction of virtual nodes improves the ability to capture long-range dependencies from attribute information. Therefore, DGTAT has a good ability to optimally capture long-range dependencies in all three aspects: attribute information, location information, and structural information. \n\nFrom an experimental point of view, large-scale graphs require more of a model's ability to capture long-range dependencies, and thus a model's performance on large-scale graphs is an important metric for evaluating a model's ability to capture long-range dependencies. Our experimental results exemplify that DGTAT achieves significant accuracy improvement relative to SOTA baselines. \n\nAdditionally, we have added an ablation experiment for the sampling strategy. We test DGTAT without the sampling strategy as a comparison (other computations are consistent with standard DGTAT). The experimental results as follow show a significant decrease in accuracy for all datasets tested, especially for large-scale graphs, further illustrating the ability of our sampling strategy to capture long-range dependencies.\n\n|                        | Pubmed  | Citeseer | Cora  | Texas  | Cornell   | Actor | AMiner-CS | Amazon2M |\n|------------------------|---------|----------|-------|--------|-----------|-------|-----------|----------|\n| DGTAT without sampling | 79.88   | 73.1     | 83.97 | 79.15  | 78.24     | 35.77 | 54.97     | 76.51    |\n| DGTAT                  | 80.71   | 73.4     | 85.12 | 84.39  | 82.14     | 36.43 | 56.38     | 78.49    |\n| Gain                   | +0.83   | +0.3     | +1.15 | +5.24  | +3.9      | +0.66 | +1.41     | +1.98    |\n\n>### Q1. How is the sampling distribution optimized during training? Is there a learnable component?\n\nAs shown in the paper in Figure 1, we have a crucial learnable parameter that dynamically weighs the importance of structural and positional attention to optimize our sampling strategy. It allows us to learn to measure whether long-range dependencies between different nodes in the dataset are more correlated with positional information or more correlated with structural information, so that our model is well adapted to different datasets. \n\nFurthermore, the learnability of the structural encoding and positional encoding is also relevant for optimizing sampling. As these encodings are learned and updated at each layer, they influence attention computation, subsequently shaping the sampling results and ultimately contributing to an optimized sampling strategy.\n\n>### W2. Increased model complexity due to decoupled computations and triple attention.\n\nThe source of Transformer's complexity is mainly the global computation of attention, which has a quadratic complexity in terms of the number of nodes. In fact, although we compute triple attention, our complexity is even lower than that of standard GT, mainly because we do not compute global attention for attribute encoding.\n\nDuring the sampling phase, we only compute global attention for positional encoding and structural encoding, whereas standard GTs computes global attention using node attribute information. The length of the positional encodings and structural encodings (usually no more than 12) is much smaller than the length of the attribute encoding (the initial feature is larger than 1K and the hidden dimension is usually at least 64).\n\nDuring the attribute information propagation and update phase of DGTAT, nodes only communicate with neighboring nodes, sampled nodes and virtual nodes, for which the complexity of the attribute encoding is linear.\n\nTherefore, the maximum complexity of DGTAT is smaller than standard GT. In future work, we can further reduce the complexity by optimizing with the linear GT method, such as using Performer, etc."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission878/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700043885778,
                "cdate": 1700043885778,
                "tmdate": 1700043885778,
                "mdate": 1700043885778,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QsNjIsvTN0",
                "forum": "bgKGwLYmAy",
                "replyto": "KU0nYC87Vf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission878/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission878/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply (2/2)"
                    },
                    "comment": {
                        "value": ">### W3. Lacks analysis of computational efficiency compared to baselines & Q2. What is the empirical complexity of DGTAT compared to GT and MPNN baselines?\n\nAppendix E  of the original paper contains a complexity analysis for DGTAT as follows\uff1a\n\n***Time complexity***\n\n*The time complexity of DGTAT mainly depends on 3 modules. The complexity of the global attention module for structural and positional encodings is $O(n^{2}k)$ and we can use a linear kernel attention method to reduce it to $O(nk)$. The  complexity of attribute attention and message passing is $O((|\\varepsilon|+nk)d)$. The computation of RRWP and RRSE is $O(nk|\\varepsilon|)$, where $n$ denotes the number of nodes, $d$ denotes the feature dimension, $|\\varepsilon|$ denotes the number of edges and $k$ denotes the hyperparameter (the length of the PE/SE and the number of sampling nodes each layer).*\n\n***Space complexity***\n\n*The space complexity is based on the number of model parameters and the outputs of each layer. The first part is mainly on the structural and positional attention module $O(dk)$. The second part is on the edge updating methods $O((d+d_{e})d^{\\prime})$. And there are some MLP or FFN layers cost $O(dk)$ or $O(dd^{\\prime})$. Thus, the total space complexity in one layer is $O(dk + (d+d_{e})d^{\\prime})$, where $d^{\\prime}$ denotes the hidden dimension and $d_{e}$ denotes the dimension of edge feature.*\n\nIn order to evaluate the complexity of DGTAT more concretely and intuitively, we have added an table in the appendix to compare the complexity of DGTAT with various GT-based and MPNN, as shown below\uff1a\n\n|            | Complexity         | Notes                                                                          |\n|------------|--------------------|--------------------------------------------------------------------------------|\n| GCN-II     | $O(nd)$              |                                                                                |\n| GAT        | $O(nd)$              |                                                                                |\n| GT         | $O(n^{2}d)$          |                                                                                |\n| Graphormer | $O(n^{2}d)$          |                                                                                |\n| GraphGPS   | $O(nd) or O(n^{2}d)$ | Linear if using Performer, quadratic if using full Transformer                 |\n| NAGphormer | $O(n(K+1)^{2}d)$     | K is the number of hops                                                        |\n| DGTAT      | $O(n^{2}K+nd)$       | K is the length of PE/SE and the number of sampled nodes each layer and $K<<d$ |\n\n\nMany thanks again for your devotion and comments! More details are included on the Appendix F of our revised paper. All the updates and modifications are now added to our revised paper. We summed up all the comments on this paper from every reviewer including yours and our responses to them on the Appendix F. Other than the modifications we made on the basis of your questions, we carried out new ablation experiments that gave us interesting results. \n\nWe appreciate it very much if you could read our updates. We always remain available to answer any other questions and suggestions on our paper and improve it. We are looking forward to your reply and new reviewing grade."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission878/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700043938321,
                "cdate": 1700043938321,
                "tmdate": 1700208189100,
                "mdate": 1700208189100,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kyR7XNym2I",
                "forum": "bgKGwLYmAy",
                "replyto": "KU0nYC87Vf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission878/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission878/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A kindly Reminder Message: 2days left in discussion phase"
                    },
                    "comment": {
                        "value": "Dear Reviewer 2B2X:\n\nThanks again for your great effort in reviewing our paper! Since the deadline for revision and discussion is approaching, we sincerely look forward to your feedback. \n\nWe understand you may have a busy schedule, but we believe that we have addressed all your concerns and refined our paper to ensure that our work meets higher standards.\n\nIf you still have further concerns or feel unclear after reading our responses, please kindly let us know and we are willing to have a further discussion with you about all technical details and concerns. If you are satisfied with our responses so far, we sincerely hope you could consider your score. \n\nThanks very much!\n\nBest regards, Authors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission878/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700559552008,
                "cdate": 1700559552008,
                "tmdate": 1700559560931,
                "mdate": 1700559560931,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]