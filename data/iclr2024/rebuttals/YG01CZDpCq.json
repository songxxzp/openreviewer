[
    {
        "title": "Don't Paint Everyone with the Same Brush: Adaptive Prompt Prototype Learning for Vision-Language Models"
    },
    {
        "review": {
            "id": "fs96jGhmcg",
            "forum": "YG01CZDpCq",
            "replyto": "YG01CZDpCq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1016/Reviewer_cng2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1016/Reviewer_cng2"
            ],
            "content": {
                "summary": {
                    "value": "this paper addresses the significant visual variance  problem when apapting VLMs to downstream tasks. The authors incorporate multiple prompts as class prototypes, use attentin matrix to weigh the prototypes, and design a prototype decrrelation loss to surpass co-occurence of multiple confident prototypes. Experiments show that the proposed method outperforms existing methods significantly."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. the whole method is carefully designed for multiple class prototypes, like adaptive attention, closest prototype, prototype decorrelation. \n2. the improvement is siginficant. \n3. experiments are well designed with the design of the methods. the adaptive attention visualization, understanding prototpyes by image retrieval and convincing. the analysis of failure cases gives helps me better understand the paper.\n4. the Discussion and Comparison to Context Optimization Methods are inspiring."
                },
                "weaknesses": {
                    "value": "1. As stated in the paper, Prototype learning traces its roots to classical models such as K-Nearest Neighbors (Peterson, 2009) and Learning Vector Quantization. Though some new aspects (adaptive attention, decorrelation, etc) are introduced in this paper, the technical novely seems stil limited. \n2. The paper addresses the adaptive attention of prototypes. This does work but is also somewhat a straightforward point. The paper does not tackle the adaptive attention of words inside a prototype. The importance is verified in the failure case analysis in the experiments."
                },
                "questions": {
                    "value": "what's the learnable part of prompt prototypes in Figure 2?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1016/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698761764471,
            "cdate": 1698761764471,
            "tmdate": 1699636027586,
            "mdate": 1699636027586,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "grYj2SPirx",
                "forum": "YG01CZDpCq",
                "replyto": "fs96jGhmcg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1016/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1016/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer cng2"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the positive and constructive comments. Please find our response to each comment below:\n\n>**W1**. As stated in the paper, Prototype learning traces its roots to classical models such as K-Nearest Neighbors (Peterson, 2009) and Learning Vector Quantization. Though some new aspects (adaptive attention, decorrelation, etc) are introduced in this paper, the technical novelty seems still limited.\n\n**Re W1**. Thank you for your comment regarding the technical novelty of our work in the context of prototype learning. We acknowledge the foundational roots of prototype learning in classical models, but we believe our approach brings significant advancements to this domain.\n\nA key innovation in our work is the integration of Vision-Language Models (VLMs) into prototype learning. This represents a considerable shift from traditional methods that predominantly utilize visual data. By leveraging textual descriptions through VLMs, our model is able to generate prototypes that capture a more diverse and comprehensive representation of each class. This is particularly crucial in scenarios with limited visual data, such as few-shot learning, where traditional prototype models may exhibit biases towards certain styles or attributes due to sample scarcity. Furthermore, the introduction of adaptive attention and decorrelation in our approach addresses key limitations in existing prototype learning methods. Adaptive attention allows our model to dynamically focus on the most relevant aspects of each prototype, enhancing the accuracy and robustness of classification. Prototype decorrelation, on the other hand, ensures diversity among the prototypes, preventing overlap and redundancy that often plague traditional prototype-based methods.\n\nThese advancements are not merely incremental; they represent a significant leap in the field of prototype learning. Our method overcomes challenges inherent in few-shot scenarios and extends the capabilities of prototype models to handle a wider variety of classes more effectively.\n\nIn summary, the integration of VLMs for prototype generation, coupled with adaptive attention and decorrelation, constitute the core technical novelties of our work. These contributions significantly enhance the versatility and efficacy of prototype learning, marking a notable advancement in the field. We believe that these points, as elaborated in our paper, underscore the novelty and significance of our approach in pushing the boundaries of prototype learning.\n\n\n>**W2**. The paper addresses the adaptive attention of prototypes. This does work but is also somewhat a straightforward point. The paper does not tackle the adaptive attention of words inside a prototype. The importance is verified in the failure case analysis in the experiments.\n\n**Re W2**. Thank you for your insightful suggestion regarding the exploration of word-level adaptive attention within prototypes. While this is indeed an intriguing aspect, our current approach using CLIP presents certain limitations in this regard. Specifically, the visual-language interaction in CLIP occurs at the embedding space level, which restricts our ability to dissect and analyze the semantic meanings of individual words within the prompts.\n\nFurthermore, in our approach, we do not constrain the length of the prompts to preserve their generative nature. This variability in prompt length poses additional challenges in consistently analyzing word-level attention across different prototypes. Consequently, our research primarily focused on adaptive attention at the prototype level. This decision was guided by the objective to enhance the overall classification performance and robustness by dynamically focusing on the most relevant prototypes for each class.\n\nHowever, we recognize the potential value of investigating word-level attention and its impact on model performance. In future research, we aim to explore methodologies that could allow for more granular attention analysis, perhaps by employing different models or techniques that can facilitate word-level semantic grounding in the context of visual-language tasks. We believe that such explorations could uncover new insights into the intricate interplay between textual and visual elements in prototype-based learning, further advancing the field.\n\n>**Q1**.  What's the learnable part of prompt prototypes in Figure 2?\n\n**Re Q1**. Thank you for your query regarding the learnable part of prompt prototypes in our model, as depicted in Figure 2. The learnable aspect primarily consists of textual features. For instance, in the ViT-B/16 configuration, these textual features have a dimension of 512. The dimension of the entire learnable weights for prompt prototypes is calculated as the product of the class number, the number of prototypes per class, and 512. These textual features are updated and refined during the training process, dynamically adjusted to better capture the nuances of each class."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1016/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700202442316,
                "cdate": 1700202442316,
                "tmdate": 1700202442316,
                "mdate": 1700202442316,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ka2qUsqN0T",
                "forum": "YG01CZDpCq",
                "replyto": "grYj2SPirx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1016/Reviewer_cng2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1016/Reviewer_cng2"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' addressing of my questions. The responses addressed some of my concerns. Thank you again and good luck."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1016/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644643044,
                "cdate": 1700644643044,
                "tmdate": 1700644643044,
                "mdate": 1700644643044,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2dlqBxJRq7",
            "forum": "YG01CZDpCq",
            "replyto": "YG01CZDpCq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1016/Reviewer_uJ9U"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1016/Reviewer_uJ9U"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the prompt learning of visual-language models.  Different from previous prompt learning methods such as CoOp, this paper goes further to explore how to assign different prompts for different classes for better performance.  To achieve this goal, this paper proposes to construct the various prompts with LLMs as class prototypes and learns an attention module to reweight these class prototypes. This paper follows the setting of CoCoOp and MaPLe to evaluate the methods, and compare the methods with baseline methods including CoOp, CoCoOp, and MaPLe. The proposed method achieves more than 2% improvement on average."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) This paper proposes to leverage multiple prompts to enhance the recognition ability. For different classes, the prompts are allowed to be different. Its idea makes sense since the \"classes\" are the abstract of the observation, in which different classes may have different focuses. \n2) The proposed method takes each prompt as a point and tries to find a prototype (with an attention model) for given classes. This method is easy but effective. \n3) The proposed method achieves good performance on the base-2-new setting."
                },
                "weaknesses": {
                    "value": "The main concern is about the presentation, which does not effectively verify the methods and demonstrate the superiority. I summarize some detailed suggestions below. \n1) The experiments follow the base-to-new setting in CoCoOp. However, the base-to-new setting is more about generalization ability. Besides, the performance of the base-to-new setting is very sensitive to the hyperparamers, especially for epochs. It is because the performance of this setting requires a balance between alignment and generalization, which can be achieved by reducing the epochs.  When tuning the training epochs of CoOp, it will also achieve good performance. It is suggested to use the few-shot learning setting in CLIP and CoOp, which is more fair and supportive to demonstrate the effectiveness of the proposed methods. \n2) The main idea of this paper is to explore how to assign multiple prompts to one class. PLOT also shares similar targets to leverage multiple prompts (ProDA is similar too). Thus, it is much better to employ these methods as the main baselines for comparison, instead of CoCoOp which targets generalization. It is suggested to compare with PLOT and ProDA in the few-shot setting.  It is better to add a discussion about the difference between the proposed method and them. \n3) What are your prompts for GPT-3 to generate prototypes?  Is the model robust for different generations?\n4) There are a series of methods for the class-wise LLM-generated prompts, such as [1-2]. It is suggested to add some discussions and comparisons with these methods. \n [1] Menon, Sachit, and Carl Vondrick. \"Visual Classification via Description from Large Language Models.\" ICLR 2023.\n [2] Pratt S, Covert I, Liu R, et al. What does a platypus look like? generating customized prompts for zero-shot image classification. ICCV 2023."
                },
                "questions": {
                    "value": "Please refer to the weaknesses part.  The main concern is about the unsuitable experimental comparison and fewer discussions. \nI will modify the final score after the discussion with the authors and other reviewers."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1016/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1016/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1016/Reviewer_uJ9U"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1016/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698787765752,
            "cdate": 1698787765752,
            "tmdate": 1700689909630,
            "mdate": 1700689909630,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tBdqztq8hx",
                "forum": "YG01CZDpCq",
                "replyto": "2dlqBxJRq7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1016/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1016/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer uJ9U"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments and answer them, as numbered, below:\n\n>**W1**. The experiments follow the base-to-new setting in CoCoOp. However, the base-to-new setting is more about generalization ability. Besides, the performance of the base-to-new setting is very sensitive to the hyperparamers, especially for epochs. It is because the performance of this setting requires a balance between alignment and generalization, which can be achieved by reducing the epochs. When tuning the training epochs of CoOp, it will also achieve good performance. It is suggested to use the few-shot learning setting in CLIP and CoOp, which is more fair and supportive to demonstrate the effectiveness of the proposed methods.\n\n**Re W1**. Thank you for your valuable suggestion regarding the incorporation of few-shot learning settings. We have indeed conducted experiments in this setting and reported the performance in our response to W2.\n\nHowever, our primary focus on the base-to-new setting stems from our objective to critically evaluate the generalization ability of our methods, particularly in scenarios involving new classes. This setting is essential for understanding how well a model can adapt to novel categories, a key aspect of our research. While methods CoCoOp and MaPLe claim improvements in generalization on new classes, our findings indicate that with descriptive prompts, CLIP can already demonstrate superior performance on new classes than these context optimization methods. We provide a detailed discussion on this validation in our paper.\n\nFurthermore, our analysis specifically addresses the issue of overfitting towards base classes and explores the potential of fine-tuning prototypes for both base and new classes with multiple prompts. This approach is aimed at enhancing generalization in new classes.\n\nRegarding the impact of training epochs on CoOp, our experiments reveal significant insights. As shown in the tables for datasets like DTD, Flowers, and UCF, reducing the number of training epochs does not necessarily improve performance on new classes. In fact, we observe a consistent drop in new class performance with CoOp, highlighting a potential limitation of this method in achieving a balanced performance.\n\nThese findings are crucial as they demonstrate the comparative advantages of our approach, particularly in maintaining a balance between alignment and generalization across different class settings. We believe that these insights add significant value to the field and underscore the effectiveness of our methods.\n\n**DTD**\n| Methods | CLIP |CoOp200|CoOp150|CoOp100|CoOp50 | Co-CoOp| APPLe |\n|:--------|:----:|:-----:|:-----:|:-----:|:-----:|:------:|:-----:|\n|  Base   | 53.24| 79.44 | 79.25 | 78.82 | 79.98 | 77.01  | 82.41 |\n|  New    | 59.90| 41.18 | 36.15 | 35.39 | 36.67 | 56.00  | 69.57 |\n|  H      | 56.37| 54.24 | 49.65 | 48.85 | 50.28 | 64.85  | 75.45 |\n\n**Flowers**\n| Methods | CLIP |CoOp200|CoOp150|CoOp100|CoOp50 | Co-CoOp| APPLe |\n|:--------|:----:|:-----:|:-----:|:-----:|:-----:|:-----:|:------:|\n|  Base   | 72.08| 97.60 | 97.53 | 97.60 | 97.47 | 94.87 | 96.58  |\n|  New    | 77.80| 59.67 | 58.59 | 60.17 | 61.72 | 71.75 | 78.58  |\n|  H      | 74.83| 74.06 | 73.20 | 74.44 | 75.58 | 81.71 | 86.66  |\n\n**UCF**\n| Methods | CLIP |CoOp200|CoOp150|CoOp100|CoOp50 | Co-CoOp| APPLe |\n|:--------|:----:|:-----:|:-----:|:-----:|:-----:|:-----:|:------:|\n|  Base   | 72.08| 84.69 | 84.38 | 84.07 | 84.80 | 82.33 | 86.56  |\n|  New    | 77.80| 56.05 | 50.82 | 55.22 | 58.36 | 73.45 | 81.99  |\n|  H      | 74.83| 67.46 | 63.43 | 66.66 | 69.14 | 77.64 | 84.21  |"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1016/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700201845864,
                "cdate": 1700201845864,
                "tmdate": 1700201845864,
                "mdate": 1700201845864,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ebxY5GiyMY",
                "forum": "YG01CZDpCq",
                "replyto": "tBdqztq8hx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1016/Reviewer_uJ9U"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1016/Reviewer_uJ9U"
                ],
                "content": {
                    "title": {
                        "value": "Additional Comments and Queries"
                    },
                    "comment": {
                        "value": "I appreciate the author's feedback and the additional experiments provided. However, I have a few more points to discuss regarding the author's response:\n\n**New-Class Generalizability Claim**: The response indicates that the paper focuses on new-class generalizability. However, upon reviewing the revised version, I double-checked the paper but did not observe any unique developments targeting this aspect, akin to how CoCoOp applies context for generalizability. If the base-to-new setting is indeed the primary objective, I suggest a thorough rewrite of the paper, particularly the introduction and abstract, to align the stated objectives with the implementation and contributions.\n\n**Experiment Details**: I appreciate the new experiments. However, the new experimental results provided seem somewhat sparse, only presenting data for 200, 150, 100, and 50 settings. Maybe a more dense evaluation would help.  In the implementation of CoCoOp and Maple,  10/5  epochs seem to be used respectively. \n\n**Comparisons with ProDA and PLot**: Thanks for the comparisons made with ProDA and PLot. However, I would like more information about their backbones. Are they the same as that of APPLE? Also, what prompts were used in these settings, vision or text? This information wasn't available in the revised paper but is crucial for a complete understanding of the experimental setup.\n\n**GPT4 vs GPT3 Performance**: The experiments involving GPT4 are interesting, particularly noting that its performance is lower than GPT3's. Could you elaborate on the potential reasons for this? Is it attributable to randomness or some other factors?\n\nFor now, my concerns still remain."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1016/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700383617818,
                "cdate": 1700383617818,
                "tmdate": 1700383617818,
                "mdate": 1700383617818,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "punz0x6MZO",
                "forum": "YG01CZDpCq",
                "replyto": "woPC0IvouX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1016/Reviewer_uJ9U"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1016/Reviewer_uJ9U"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response!"
                    },
                    "comment": {
                        "value": "All my concerns are well addressed.  I truly believe this version is much better then original one with more complete evaluations and insightful discussions.\n\nI have raise my score accordingly! \n\nHere are two more suggestions.\n1) Could you include a more complete evaluation on the base-2-new setting with epoches into appendix? I think it would be a chance to justify the base-2-new setting.\n2) The current revised version is good, but more that 9 pages (it is not allowed).  My suggestion is to move the 1) Discussion and Comparison to Context Optimization Methods, 2) Visualization, and 3) one of Cross-Dataset Transfer/Domain Generalization into appendix."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1016/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689857244,
                "cdate": 1700689857244,
                "tmdate": 1700689857244,
                "mdate": 1700689857244,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "G9iA19jebd",
            "forum": "YG01CZDpCq",
            "replyto": "YG01CZDpCq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1016/Reviewer_fKhC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1016/Reviewer_fKhC"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed an Adaptive Prompt Prototype Learning (APPLe) method for VLMs. The author has designed an adaptive attention mechanism to alleviate the noise and flaws within the prompts. The experimental results show that the method proposed by the author has consistent performance improvement on all 11 datasets and all tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. In the experimental results table, absolute performance improvements have been added to make the experimental results more intuitive.\n\n2. The article has a complete system and clear organization, from problem introduction, formula reasoning, and image explanation to experimental results, making it easier for readers to read.\n\n3. The method proposed by the author has better advantages compared to some counterpart methods."
                },
                "weaknesses": {
                    "value": "1. As an important contribution, the Attention weighting and L_dec only gain limited performance improvements, which degrades the contribution to the community. The overall compared methods are also very limited. \n\n2. There is some confusion in the layout of tables and images.\n\n3. Although using multiple prompts as category prototypes can help capture visual differences, in practice, not every visual sample closely matches each prototype. \n\n4. The article mentions the introduction of prototype decorrelation loss to suppress the co-occurrence of multiple confident prototypes. However, specific details on how the loss was designed and worked were not mentioned. This may affect the performance of the model in tasks with complex category distributions or a large number of categories.\n\n5. It is not clear how to initialize these prototypes and how to obtain the base and novel class prompts."
                },
                "questions": {
                    "value": "See Above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1016/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698900330612,
            "cdate": 1698900330612,
            "tmdate": 1699636027428,
            "mdate": 1699636027428,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IOMUeovnwi",
                "forum": "YG01CZDpCq",
                "replyto": "G9iA19jebd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1016/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1016/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer fKhC"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments and answer them, as numbered, below:\n\n>**W1**. As an important contribution, the Attention weighting and L_dec only gain limited performance improvements, which degrades the contribution to the community. The overall compared methods are also very limited.\n\n**Re W1**. We appreciate your insights on the performance impact of L_dec and the breadth of our comparisons. It's important to note that L_dec improves the Harmonic Mean (HM) performance by 0.31% on ImageNet, which is significant given the dataset's size. We've also tested L_dec's contribution on other datasets, observing larger impacts without tuning its coefficient. This underscores the broader applicability and effectiveness of L_dec across various datasets.\n\n|dataset| L_dec | Base  |  New  |   HM  |  \n|:------|:------|:-----:|:-----:|:-----:|\n| DTD   |  yes  | 82.41 | 69.57 | 75.45 |\n|       |  no   | 79.05 | 68.12 | 73.18 |\n|AirCraft| yes  | 44.66 | 43.13 | 43.88 |\n|       |  no   | 46.70 | 38.87 | 42.43 |\n|SUN397 |  yes  | 82.44 | 79.04 | 80.70 |\n|       |  no   | 81.87 | 79.02 | 80.42 | \n|Cars   |  yes  | 80.23 | 75.12 | 77.59 |\n|       |  no   |79.11  | 74.84 | 76.91  | \n\n>**W2**.There is some confusion in the layout of tables and images.\n\n**Re W2**. Thank you for pointing out the layout concerns. We understand that clarity in presentation is crucial for comprehensibility. We will thoroughly review our document to improve the placement and clarity of tables and images, ensuring they align intuitively with the corresponding text sections. We would also appreciate specific instances from the reviewer where the confusion is more pronounced, to better address these areas.\n\n>**W3**. Although using multiple prompts as category prototypes can help capture visual differences, in practice, not every visual sample closely matches each prototype.\n\n**Re W3**. Thank you for the comment. We recognize that not every visual sample will closely match each of our multiple prototypes. However, this is an integral aspect of our design. The use of multiple prompts as category prototypes is intended to capture the broad spectrum of visual variations within a class.\n\nIn our method, the matching mechanism is designed to be flexible and robust. It assesses the affinity of a visual sample to the range of prototypes, rather than seeking a one-to-one precise match. This approach ensures that our model can generalize well across diverse samples within a class, even when direct matches to specific prototypes are not feasible.\n\nMoreover, we have implemented strategies to further refine this matching process. These include the decorrelation loss, max/mean logits balancing that assesses the degree of similarity between samples and prototypes, ensuring that each sample is associated with the most representative prototypes for its class."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1016/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700201640214,
                "cdate": 1700201640214,
                "tmdate": 1700201640214,
                "mdate": 1700201640214,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FtO53hP8rx",
                "forum": "YG01CZDpCq",
                "replyto": "G9iA19jebd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1016/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1016/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up questions of Reviewer fKhC before the discussion phase ends"
                    },
                    "comment": {
                        "value": "Dear Reviewer fKhC,\n\nWe would like to express our sincere gratitude for your encouraging remarks about our work, particularly for highlighting aspects such as its intuitiveness, completeness, clear organization, and notable advantages. Your kind words are genuinely appreciated and give us a sense of accomplishment.\n\nIn response to the weaknesses you identified in your review, we have provided detailed responses and hope that these have adequately addressed your concerns regarding our paper.\n\nAs the discussion phase is about to close in the next 12 hours, we are keenly awaiting any further feedback you may have regarding our responses. We remain prepared to answer any additional questions or provide clarifications as needed, ensuring a thorough and timely exchange of information.\n\nOnce again, thank you for your invaluable input and the time you have dedicated to reviewing our work. Your insights are greatly valued.\n\nBest regards, \nThe Authors"
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1016/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698906739,
                "cdate": 1700698906739,
                "tmdate": 1700698906739,
                "mdate": 1700698906739,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "s4NruKvrOU",
            "forum": "YG01CZDpCq",
            "replyto": "YG01CZDpCq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1016/Reviewer_tgxP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1016/Reviewer_tgxP"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors study how to better prompt VLM, specifically CLIP model to tackle image classification tasks. The authors notice that using a single text prompt for each class is insufficient to capture the diversity of visual representations within that class. To address this, the authors introduce Adaptive Prompt Prototype Learning (APPLe), a technique that provides multiple text prompts for each class. Additionally, to mitigate the impact of noise in the textual prompts, the authors develop an adaptive attention mechanism capable of disregarding ineffective prompts. The implementation of these strategies results in performance that surpasses that of the current state-of-the-art methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors demonstrate robust performance across both training-free and training-based methods, consistently outperforming strong baselines on nearly all datasets for both 'Base' and 'New' sets.\n- Notably, the training-free methods implemented by the authors are capable of surpassing some training-based method.\n- The authors present comprehensive analyses and comparisons with baseline methods, contributing valuable insights to the field."
                },
                "weaknesses": {
                    "value": "- The authors keep claiming that CLIP only uses one prompt, but in CLIP paper section 3.1.4, they discuss how they use 80 prompts to improve the performance without sacrificing test time speed (unlike APPLe which is slower with more prompts). The authors should definitely compare their method to CLIP with 80 prompts as a baseline.\n- The presentation can be improved:\n    - It needs to be clarified how training-free works. I think the authors should more explicitly describe it. My understanding is that training-free = 50 prototypes only (the second row in Table 4). Correct me if I am wrong.\n    - The description of the training process is also vague. Section 4 omits details on how prototype features are fine-tuned. It seems to me that the text encoder and the prompts are only used to initialize the prototypes. Correct me if I am wrong."
                },
                "questions": {
                    "value": "- The authors primarily experimented with one CLIP model. It is unclear if this method can work with different CLIP variants, open-sourced CLIP replication or other VLM models. I'm curious if changing the model architecture, training data, or VLM format would yield different results.\n    - While the method appears to be general, I'm concerned about it potentially \"overfitting\" to a specific model and dataset.\n- How does the training process for cross-dataset transfer work? When training on the source data (e.g., ImageNet), the model learns prototype features for ImageNet classes and adaptive attention weights for them. How does this transfer to target datasets where prototypes and attention weights remain untouched during fine-tuning?\n- Could you clarify the importance of the quality of prompts used in the experiments? What would happen if we used GPT-4 to generate the prompts? How does the quality of the input prompt to the GPT model impact the final performance?\n- Although the authors claim that fine-tuning the prompt textual features does not lead to overfitting issues, there are no ablations on the performance of training with frozen prototype features to demonstrate whether fine-tuning the prototype is necessary.\n- In Equation 7, the authors selected a method that can balance between all prototypes and the closest prototypes. Are there other balancing methods, such as using the Boltzmann operator or logsumexp, that could be considered?\n- While the authors aim for diverse prompts, it might be interesting to fix the prototype and only train the attention weights and forcing the attention weights to be a multinomial distribution with low entropy. This would be essentially learning to select the best prototype. It would be interesting to see if GPT-3 can produce a better single prompt than the hand-designed prompts used in CLIP.\n- Have the authors attempted to use the embedding ensembling method used in CLIP?\n\n\nMinors:\n-  In Equation 7, stating the \"average cosine similarity\" is not entirely accurate because the cosine similarities are weighted by the attention weights.\n-  While the trend in Figure 5 is clear, it could be valuable to include settings with 0/1 and 1/0 to further illustrate the findings.\n\nJustification:\nIn terms of performance, this paper demonstrates strength, and I commend the authors for their straightforward yet valuable concepts. Nonetheless, there are various intriguing aspects that remain unaddressed, leaving certain concerns. Additionally, the authors have made claims about the CLIP paper that may not be accurate."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1016/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699090607268,
            "cdate": 1699090607268,
            "tmdate": 1699636027359,
            "mdate": 1699636027359,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2LJ7Fgf9Cp",
                "forum": "YG01CZDpCq",
                "replyto": "s4NruKvrOU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1016/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1016/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial Response to Reviewer tgxP"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable comments and answer them, as numbered, below:\n\n> **W1**. The authors keep claiming that CLIP only uses one prompt, but in CLIP paper section 3.1.4, they discuss how they use 80 prompts to improve the performance without sacrificing test time speed (unlike APPLe which is slower with more prompts). The authors should definitely compare their method to CLIP with 80 prompts as a baseline.\n\n**Re W1**. We thank the reviewer for pointing out the usage of 80 prompts in the CLIP model. It is true that CLIP's performance on ImageNet benefits from these prompts. However, our work focuses on the limitations of these generic prompts, particularly for fine-grained datasets. These prompts, as limited in the official CLIP repository[1], are less effective for such datasets due to their lack of specificity. For example, prompts like  'a plastic {}' or 'a {} in a video game' do not capture fine-grained distinctions well. It is worth noting that the CLIP performance reported in our paper uses the customized prompts as indicated in the CLIP paper, e.g., \u201cA photo of a {label}, a type of pet.\u201d for OxfortPets.\n\nWe acknowledge that these context prompts can indeed improve the performance. We have evaluated the base-to-new setting on ImageNet in the table below (CLIP-80-emb). As for the embedding ensembling method that is raised in question 7. We have tested the performance between embedding ensembling and logits ensembling methods. It can be seen that without fine-tuning, the performance results between the two methods are relatively similar. However, if we want to further fine-tune the textual features, embedding ensembling method tends to overfit to base classes. This phenomenon has been discussed in Figure 3 (impact of prototype number). \n\nAs for the test time speed, logits ensembling indeed causes more computation overhead, but only for dot product between the visual and textual features. Because the textual features of the 80 generic prompts also need to be inferred through the language transformer 80 times. The dot product between visual and textual features needs relatively less computational resources. \n\n[1] https://github.com/openai/CLIP/blob/main/notebooks/Prompt_Engineering_for_ImageNet.ipynb\n\n| Methods| CLIP  |CLIP-80-emb| CLIP-80-logits | APPLe*-50-emb | APPLe*-50-logits  |  APPLe-50-emb | APPLe-50-logits  | \n|:-------|:-----:|:------:|:-----:|:-----:|:-----:|:-----:|:-----:|\n| Base   | 72.43 | 73.56  | 73.54 | 74.62 | 74.62 | 74.16 | 78.17 |\n| New    | 68.14 | 69.97  | 69.99 | 71.79 | 71.94 | 71.93 | 72.12 |\n| HM     | 70.22 | 71.72  | 71.72 | 73.17 | 73.26 | 73.02 | 75.02 |\n\n\n>**W2**.  The presentation can be improved:\nIt needs to be clarified how training-free works. I think the authors should more explicitly describe it. My understanding is that training-free = 50 prototypes only (the second row in Table 4). Correct me if I am wrong.\nThe description of the training process is also vague. Section 4 omits details on how prototype features are fine-tuned. It seems to me that the text encoder and the prompts are only used to initialize the prototypes. Correct me if I am wrong.\n\n**Re W2**. Thank you for your valuable feedback regarding the clarity of our training-free approach and the overall training process. We agree that these aspects require more explicit descriptions and have revised our paper accordingly.\n\nThe training-free version indeed utilizes only 50 prototypes. These prototypes are initialized by the CLIP text encoder using specifically designed prompts that correspond to the categories or classes in our dataset. This initialization is critical as it provides a foundation for our zero-shot classification approach. This training-free method contrasts with our trained approach, where after the initial prototype initialization, we further fine-tune the prototype features. The fine-tuning process involves adjusting these features to better align with the specific nuances of our dataset, thereby improving the classification performance.\n\nTo address the reviewer\u2019s concern about the vagueness in our training description, we have expanded Section 4 to include a detailed account of this process. This includes how the prototypes are initialized, the role of the text encoder and prompts in this initialization, and the subsequent fine-tuning steps, if applicable. Specifically, we explain how the text encoder and prompts serve as a starting point, and how image samples from our dataset are used to refine the prototypes for improved accuracy and applicability."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1016/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700200312402,
                "cdate": 1700200312402,
                "tmdate": 1700200312402,
                "mdate": 1700200312402,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WkWKy5iwOM",
                "forum": "YG01CZDpCq",
                "replyto": "s4NruKvrOU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1016/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1016/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up questions before the discussion phase ends"
                    },
                    "comment": {
                        "value": "Dear Reviewer tgxP,\n\nFirst and foremost, we would like to extend our gratitude for the positive feedback you have provided on our work. Your kind words are both encouraging and appreciated.\n\nIn response to the points of weakness you highlighted, we have previously submitted detailed, point-by-point responses. We hope that these have satisfactorily addressed the concerns about our paper.\n\nAs the discussion phase is approaching its conclusion in the next 12 hours, we eagerly anticipate any additional feedback you might have on our responses. Should there be any further questions or clarifications needed, we are ready and willing to provide additional information in a timely manner.\n\nThank you once again for your valuable input and the time you have invested in reviewing our work.\n\nBest regards,\nThe Authors"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1016/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698285655,
                "cdate": 1700698285655,
                "tmdate": 1700698285655,
                "mdate": 1700698285655,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8hDvrv1twP",
                "forum": "YG01CZDpCq",
                "replyto": "nVE2cwxQes",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1016/Reviewer_tgxP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1016/Reviewer_tgxP"
                ],
                "content": {
                    "title": {
                        "value": "Re"
                    },
                    "comment": {
                        "value": "Thank you for your responses.\n\nI have a follow-up on Q6.\nThe result is interesting and it also shows that each of the authors' prompt are potentially more detailed and the prompts are more orthogonal to each other.\nI am wondering if there is still time to see how the \"diversity\" of the prompts affect the results, and could that be the reason why GPT-3 is better than GPT-4: instruction-tuned model may be less diverse compared to untuned foundation models."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1016/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700721388595,
                "cdate": 1700721388595,
                "tmdate": 1700721388595,
                "mdate": 1700721388595,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]