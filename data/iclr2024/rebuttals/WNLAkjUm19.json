[
    {
        "title": "On the Role of Discrete Tokenization in Visual Representation Learning"
    },
    {
        "review": {
            "id": "f9sCcjQ1NS",
            "forum": "WNLAkjUm19",
            "replyto": "WNLAkjUm19",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8697/Reviewer_d6S4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8697/Reviewer_d6S4"
            ],
            "content": {
                "summary": {
                    "value": "Masked image modeling methods (MIM) have started to employ discrete tokenization as a reconstruction target instead of raw pixel values. Yet, the role of tokenization and how it affects downstream performance is not well studied. This paper studies the impact of tokenizer design on downstream performance. The paper finds that that tokenizer that are more aligned with the downstream task labels (ie, more discriminative with respect to downstream classes) result in better performance, and design a metric to measure this similarity. The paper also introduces a new tokenizer based on clustering features to provide discretized reconstruction targets. Methods trained with the new tokenization achieve a better performance that existing approaches. The paper also includes some interesting analysis for the new tokenizer.\n\n**Update (11/20):** I have raised my rating from 6 to 8 in respond to reading the other reviews and the author's response to all reviews. I think the paper presents an interesting study of the impact of discretization and proposes an interesting approach to representation learning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The framing of the equivalency structure induced by the tokenization was quite interesting. \n\n- The paper poses an interesting question regarding the impact of tokenization and provides several interesting observations and analysis to answer it. \n\n- The proposed metric is intersting and seems to have predictive power. Although, as noted in the weaknesses, it would be great to report it for all experiments. \n\n- The proposed tokenizer is fairly simple, especially for K-MIM pixel which achieves a significant performance gain without requiring any other models."
                },
                "weaknesses": {
                    "value": "- I found the name \"Mixture Tokenizer\" a bit confusing since it almost seems adversarial in nature, and wasn't sure if this naming is common in another sub-field. It seems to me that this would be an adversarial/worst-case tokenizer; one that implicitly learns a class structure that is \"orthogonal\" to the desired one (eg, one that learns backgrounds instead of foreground objects in a perfectly balanced datasets). If so, I would suggest naming it something that denotes this quality better simply a \"mixture tokenizier.\" \n\n- The use of a pre-trained SSL models to create the tokenization seems a bit odd. I would argue that if you use the features of model X to generate the labels/tokens to train model Y, then model Y is effectively being supervised by model X. While this is okay since both models have similar training requirements, one would expect model Y to outperform model X for this strategy to be pragmatic. Yet, K-MIM DINO achieves a much lower linear probe accuracy than DINO. Furthermore, the efficiency argument made in Sec 5.2 would need to take into account the time taken to train DINO for the K-MIM DINO results (the argument for K-MIM PIXEL holds and is a very nice finding). \n\n- The introduction goes directly into specific methods that might not be known to the readers. Providing some smoother transition (eg, explaining what those methosd are doing first) could improve the readability of the introduction.\n\n- The TCAS metric is only shown in Table 2, I think it would be nice to include it as a column in Tables 3 and 4 to see how well it explains variation in performance. Particularly, I am curious how much it changes with the choice of K in table 4, and whether that would explain some of the patterns there."
                },
                "questions": {
                    "value": "- Could you explain why the bounds in equation 7 do not depend on $l$? Is it because each point is equivalent to an equal number of each class? \n\n- He et al (CVPR 2022) reported a fine-tuning performance of 83.6, yet Table 3 reports 82.9. Could you please comment on this discrepancy? \n\n- Table 3 notes that K-MIM DINO achieves a linear probe accuracy of 67.4, which is significantly lower than 78.2 reported by Caron et al (ICCV 2021), while outperforming them on fine-tuning (83.8 vs. 82.8). I was curious why you think the model underperforms this much despite being given being trained using the equivalency structure learned by DINO. \n\n- The results reported in Table 4 are quite interesting as they indicate that performance deteriorates quickly for larger token books, while a larger number of tokens seems to benefit DINO. Could you please comment on this result? I would be curious if TCAS could shed some light on this. \n\n- I think the finding that discretized features (HOG/DINO) provide better targets than raw pixels. DINO is an interesting target as it has been shown to be good local descriptor. I am curious if other features would be useful as well; eg, imagenet-supervised VIT features or MAE features. The first should be more aligned with the downstream task structure, while the latter is based on a model that seems to perform worse. I am curious if you had tried this or what your thoughts are on other features as targets for clustering."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8697/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8697/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8697/Reviewer_d6S4"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8697/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698717982323,
            "cdate": 1698717982323,
            "tmdate": 1700525917156,
            "mdate": 1700525917156,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qluB8hWP7K",
                "forum": "WNLAkjUm19",
                "replyto": "f9sCcjQ1NS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8697/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8697/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer d6S4 (1/2)"
                    },
                    "comment": {
                        "value": "We thank Reviewer d6S4 for the comments and the appreciation on our work. We address your concerns in the following points:\n\n---\n\nQ1. The name \"Mixture Tokenizer\" is a bit confusing.  It seems to me that this would be an adversarial/worst-case tokenizer; one that implicitly learns a class structure that is \"orthogonal\" to the desired one (eg, one that learns backgrounds instead of foreground objects in a perfectly balanced datasets). If so, I would suggest naming it something that denotes this quality better simply a \"mixture tokenizier.\"\n\nA1. Thanks for your thoughtful suggestion! We rename \"Mixture Tokenizer\" with \"Cross-class Tokenizer\", which reflects the essence of the tokenizer, emphasizing that each equivalence class comprises elements from multiple classes.\n\n---\n\nQ2. The use of a pre-trained SSL models to create the tokenization seems a bit odd. K-MIM DINO achieves a much lower linear probe accuracy than DINO. Furthermore, the efficiency argument made in Sec 5.2 would need to take into account the time taken to train DINO for the K-MIM DINO results.\n\nA2. This setting mainly follows previous tokenization-based MIM works that adopt DINO as a supervision for MIM (like MaskFeat [1] and dBoT [2]). Nevertheless, as you pointed out, this setting is indeed a bit odd since DINO is already good enough, and it also introduces heavy pretraining cost when taken into account (in practice we just load public checkpoints).\n\nTo resolve this issue, we further propose K-MIM PIXEL, which directly performs tokenization from raw pixels and thus have little pretraining cost. From Table 3, we can observe that K-MIM PIXEL significantly outperform those without pretrained models (e.g., raw pixels, HOG), and it can even outperform DINO supervision on finetuning (e.g., 86.4 vs 84.7 on ImageNet-100). Thus, we believe that K-MIM PIXEL is more practical solution for tokenization-based MIM training.\n\n[1] Wei et al. Masked Feature Prediction for Self-Supervised Visual Pre-Training, CVPR 2022.\n\n[2] Liu et al. Exploring Target Representations for Masked Autoencoders, Arxiv 2209.03917.\n\n---\n\nQ3. The introduction goes directly into specific methods that might not be known to the readers. Providing some smoother transition (eg, explaining what those methosd are doing first) could improve the readability of the introduction.\n\nA3. Thanks for your thoughtful suggestion! We add more explanations into the introduction part in the revision.\n\n---\n\nQ4. The TCAS metric is only shown in Table 2, I think it would be nice to include it as a column in Tables 3 and 4 to see how well it explains variation in performance. Particularly, I am curious how much it changes with the choice of K in table 4, and whether that would explain some of the patterns there.\n\nA4. Thanks for your suggestions! Since Table 3 also contains several methods without using discrete tokenization, we do not include TCAS in Table 3. Following your suggestions, we organize the TCAS score and performance of all the tokenizers in Appendix E in the revision. It can be indicated from Figure 4 that there is a strong linear correlation between TCAS and Linear Probing Accuracy in general. This indicates that TCAS does offer a guidance on assessing the performance of a tokenizer. \n\n---\n\nQ5. Could you explain why the bounds in equation 7 do not depend on $l$? Is it because each point is equivalent to an equal number of each class?\n\nA5. No. It is because each class exhibit identical ratios of elements from the three sets $P_1/P_2$, $P_2/P_1$ and $P_1\\cap P_2$. In fact, we can prove that if two equivalence classes $S_i$ and $S_j$ exhibit identical ratios of elements from the three sets $P_1/P_2$, $P_2/P_1$ and $P_1\\cap P_2$, i.e. $n_{i,1}/n_{j,1}=n_{i,2}/n_{j,2}=n_{i,3}/n_{j,3}$, then the bound remains unchanged if we merge $S_i$ and $S_j$ into a new equivalence class. Since the proportions of elements from any of the three sets remain consistent across equivalence classes resulting from cross-class tokenization, the $l$ equivalence classes can merge into one without affecting the bound. Consequently, this explains why the bounds in Equation (7) exhibit no dependency on $l$.\n\n---"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8697/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492216637,
                "cdate": 1700492216637,
                "tmdate": 1700492216637,
                "mdate": 1700492216637,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "X0ghhPuOPQ",
                "forum": "WNLAkjUm19",
                "replyto": "f9sCcjQ1NS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8697/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8697/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer d6S4 (2/2)"
                    },
                    "comment": {
                        "value": "Q6. [3] reported a fine-tuning performance of 83.6, yet Table 3 reports 82.9. Could you please comment on this discrepancy?\n\nA6. In [3], the fine-tuning performance of 83.6 is based on the checkpoints of 1600 epochs pretraining, where our setting is 200 epochs pretraining. Our 200-epoch result is consistent with the result in [4].\n\n[3] He et al. Masked Autoencoders Are Scalable Vision Learners, CVPR 2022.\n\n[4] Zhange et al. How Mask Matters: Towards Theoretical Understandings of Masked Autoencoders, NeurIPS 2022.\n\n---\n\nQ7. Table 3 notes that K-MIM DINO achieves a linear probe accuracy of 67.4, which is significantly lower than 78.2 reported by Caron et al (ICCV 2021), while outperforming them on fine-tuning (83.8 vs. 82.8). I was curious why you think the model underperforms this much despite being given being trained using the equivalency structure learned by DINO.\n\nA7. Indeed, MIM methods will underperform contrastive methods in the linear probing accuracy and outperform them in the finetuning accuracy. There has been a previous work studying this phenomenon [5]. The following statements are quoted from [5]:\n\n>Contrastive learning methods outperform MIM in linear probing and small model regimes Contrastive learning will capture the shapes of the images, which will  help recognize objects and distinguish images. In contrast, MIM preserves texture and the diversity of representation, which may not correlate as strongly with objects or content when compared to the emphasis on shapes.\n\nTherefore, the phenomenon that K-MIM DINO will underperform DINO in the linear probing accuracy is due to the different training dynamics (MIM v.s. Contrastive Learning). \n\n[5] Park et al. What Do Self-Supervised Vision Transformers Learn? ICLR 2023.\n\n---\n\nQ8. The results reported in Table 4 are quite interesting as they indicate that performance deteriorates quickly for larger token books, while a larger number of tokens seems to benefit DINO. Could you please comment on this result? I would be curious if TCAS could shed some light on this.\n\nA8. Indeed, the results reported in Table 4 indicate that the optimal clustering number for K-MIM DINO will be larger than K-MIM PIXEL. arises from the distinct separability of Kmeans performing in the pixel space and K-means performing in the DINO feature space [6]. To be specific, the dataset cannot be clustered well into real classes using K-means in the pixel space. Therefore, when increasing the number of clusters in K-means, it leads to the model overfitting noise or local features in the data, resulting in low-quality clusters and larger labeling error. Hence, the optimal clustering number for K-MIM is smaller. In the contrary, each class can be seperated well in the DINO feature space [7]. Therefore, a larger clustering number $K$ is optimal for the K-MIM DINO.\n\n[6] Caron et al. Deep Clustering for Unsupervised Learning of Visual Features\n\n[7] Caron et al. Emerging Properties in Self-Supervised Vision Transformers, ICLR 2023.\n\n---\n\nQ9. I am curious if other features would be useful as well; eg, imagenet-supervised VIT features or MAE features. The first should be more aligned with the downstream task structure, while the latter is based on a model that seems to perform worse.\n\nA9. As you suggest, we use DeiT features and MAE features in our K-MIM methods, where DEiT is an supervised ViT method trained on ImageNet. We pretrain DeiT and MAE models using ViT-S on ImageNet-100 for 800 epochs. The evaluation setting is consistent with that in our paper. Since DeiT is more aligned with the downstream task structure and MAE performs worse, we expect K-MIM DeiT will have a smaller TCAS socre and achieve better results than K-MIM DINO. Conversely, we anticipate that K-MIM MAE will have a larger TCAS score and underperform K-MIM DINO. The results are shown below:\n\n|Method|TCAS score|Linear Probing Acc.|\n|---|---|---|\n|K-MIM DINO|0.15|59.7|84.7|\n|K-MIM DeiT|0.09|64.8|83.1|\n|K-MIM MAE|0.34|52.3|83.2|\n\nAs the table indicates, the TCAS score aligns well with the linear probing accuracy. K-MIM DeiT with the smallest TCAS score achieves the best linear probing accuracy and K-MIM MAE with the largest TCAS score underperforms K-MIM DINO. This conclusion ensures the reliability of our TCAS metric. For more detailed information, please refer to Appendix E, where we organize the TCAS scores and performances of various tokenizers.\n\n---\n\nThanks for your comments and hope our answers could address your concerns. Please let us know if you have additional questions."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8697/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492324412,
                "cdate": 1700492324412,
                "tmdate": 1700492585416,
                "mdate": 1700492585416,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MazPvsWMgQ",
                "forum": "WNLAkjUm19",
                "replyto": "X0ghhPuOPQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8697/Reviewer_d6S4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8697/Reviewer_d6S4"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer Response"
                    },
                    "comment": {
                        "value": "Thank you for the clarification. I appreciate your thoroughness and willingness to include additional results. I think the paper is strong with the additional results and hope the other reviewers will agree as well. After reading the rebuttal and the other reviews, I will be raising my score as my concerns have been addressed. \n\nOne last thing question that I had for the authors. The current paper presents a very interesting study on the impact of tokenization. One alternative is to use an online tokenizer as done by iBOT and scaled up by DINOv2. This seems to results in a very strong performance. Meanwhile, K-MIM with DeIT outperforms that other MIM methods, but still seems to be short of iBOT's performance despite being trained with the equivalency structure imposed by the ground-truth labels that has a very low TCAS score. I was curious what you thought about this, specifically:\n- is comparing K-MIM DeiT with iBOT numbers a fair comparison to K-MIM DeiT or is there something that iBOT is leveraging that could balance the numbers?\n- if an online tokenizer does indeed outperform a discrete tokenization using a groundtruth trained model's features, what should we infer from that? is this an issue due to the loss, the discretization, or something else? \n\nThanks again for the clarification and I would appreciate your thoughts on the questions above."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8697/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700525786856,
                "cdate": 1700525786856,
                "tmdate": 1700525786856,
                "mdate": 1700525786856,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qnd9r84nFD",
                "forum": "WNLAkjUm19",
                "replyto": "hFw8L5y7DI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8697/Reviewer_d6S4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8697/Reviewer_d6S4"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarification. It seems the main explanation is the additional image-level contrastive loss. One final suggestion is to add this to the discussion or to even consider an iBOT baselined without the CLS token. Table 9 in iBOT [1] suggests that it does very poorly, although, it is unclear if this is due to them still using the CLS of the model which would receive no training in the final layer, or if this was accounted for somehow. Regardless, this is just a suggestion and I already recommend the paper for acceptance as mentioned in my previous comment. Thanks again for the engagement. \n\n[1] https://arxiv.org/pdf/2111.07832.pdf"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8697/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678435524,
                "cdate": 1700678435524,
                "tmdate": 1700678435524,
                "mdate": 1700678435524,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2YzNAPF6Nk",
            "forum": "WNLAkjUm19",
            "replyto": "WNLAkjUm19",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8697/Reviewer_f8wZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8697/Reviewer_f8wZ"
            ],
            "content": {
                "summary": {
                    "value": "This authors of this paper present theoretical analysis on the problem of masked image modeling (MIM), and study the impact of discrete tokenization in the MIM pipeline."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors present an intriguing tokenization approach using graph representation, and the paper is both technically robust and clearly articulated"
                },
                "weaknesses": {
                    "value": "The theoretical analysis is only considered for two classes. I wonder if this can be extended into multiple classes."
                },
                "questions": {
                    "value": "Other the generalization to multi-classes. I have another question regarding the downstream error bound, it would be helpful if you can talk more in detail about the derivation perhaps in the appendix."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8697/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8697/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8697/Reviewer_f8wZ"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8697/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698782629657,
            "cdate": 1698782629657,
            "tmdate": 1699637090078,
            "mdate": 1699637090078,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xG4MPJbgC0",
                "forum": "WNLAkjUm19",
                "replyto": "2YzNAPF6Nk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8697/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8697/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer f8wZ"
                    },
                    "comment": {
                        "value": "We thank Reviewer f8wZ for the comments and the appreciation on our work. We address your concerns in the following points:\n\n---\n\nQ1. The theoretical analysis is only considered for two classes. I wonder if this can be extended into multiple classes.\n\nA1. Sure. We add some discussions on the case of multiple classes in Appendix C. Similarly, we consider the three tokenization settings in this case and discuss how the tokenization influences the intra-class and inter-class connectivity, as well as the downstream error bound. In this situation, we still find that class-wise tokenization will have larger intra-class connectivity and cross-class tokenization will have larger inter-class connectivity. Therefore, similar to the two-class case in the paper, class-wise tokenization will enjoy a lower downstream error bound and cross-class tokenization will have a larger downstream error bound.\n\n---\n\nQ2. I have another question regarding the downstream error bound, it would be helpful if you can talk more in detail about the derivation perhaps in the appendix.\n\nA2. We provide more detailed derivation process regarding the calculation of downstream error bound in Appendix B.\n\n---\n\nThanks for your comments and hope our answers could address your concerns. Please let us know if you have additional questions."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8697/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492042444,
                "cdate": 1700492042444,
                "tmdate": 1700492609303,
                "mdate": 1700492609303,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Wl0aa4RyNt",
            "forum": "WNLAkjUm19",
            "replyto": "WNLAkjUm19",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8697/Reviewer_ss62"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8697/Reviewer_ss62"
            ],
            "content": {
                "summary": {
                    "value": "The paper analyzes the effect of discrete tokens as targets of masked image model (MIM) training. Using a graph-based view for MIM [A], the paper shows that discrete tokens change the graph of MIM. Also, it provides a theorem that discrete tokens similar to image class are the best for MIM targets. Based on the theorem, a metric to measure better tokenizer for MIM is proposed, named Token-Class Alignment Similarity (TCAS). Last, it is shown that simple K-means, which have a better TCAS score than other baselines, could improve the performance of MIM.\n\n[A] How Mask Matters: Towards Theoretical Understandings of Masked Autoencoders, NeurIPS 2022"
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Theoretical analysis on a discrete tokenization method looks novel and interesting.\n- A metric for tokenization (TCAS) would help a lot of researchers to investigate MIM."
                },
                "weaknesses": {
                    "value": "- I think using discrete tokenization is not a mainstream of MIM. Representative methods, such as MAE, MaskFeat, and data2vec, demonstrate impressive performance without the discrete tokens. Thus, the contribution of the paper is hard to cover diverse variants of MIM.\n\n- According to theorem 1, image classification training could be the best way to downstream error bound. But, in practice, MIM works better than classification training in a lot of cases. Thus, I doubt the general applicability of this theorem and the metric (TCAS) on diverse MIM tasks.\n\n- Experimental results are limited to a small dataset (ImageNet-100) and short training (200 epochs on IN-1k). I think it is not enough to validate the effect of K-MIM. Reported numbers on TCAS are also limited."
                },
                "questions": {
                    "value": "- Using DINO as a target representation is similar to [B]. Is there any relation between the distillation target and the TCAS metric? I think the paper would be better to cite [B] and add a discussion on it.\n\n[B] Exploring Target Representations for Masked Autoencoders, arxiv\n\n- According to theorem 1, it looks like an image classifier would be the best tokenizer for MIM. What is the TCAS score and MIM performance when using an image classifier, such as DeiT, as a tokenizer?\n\n- Section 3 is explained with tokenization for a group-of-tokens, i.e. $x_2 \\in R^{n \\times s}$. But, in Section 4, it seems the tokenization is conducted for a single token. Is it possible to generalize a theorem from the group-of-tokens case to the single-token scenario?\n\n- Discrete tokenizers like dVAE and VQGAN employ ConvNet or ViT, utilizing the entire image to create tokens. These tokens are interrelated, and a token from one location can incorporate patches from others. However, it looks like the paper handles these tokens as individual local information, which is not correct. Is there any explanation for this?\n\n- Although K-MIM has better TCAS than other tokenizers, it is hard to say that K-MIM is specially designed for TCAS. Is there \bany discrete tokenization method optimized for the TCAS metric?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8697/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8697/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8697/Reviewer_ss62"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8697/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698818178010,
            "cdate": 1698818178010,
            "tmdate": 1700628014314,
            "mdate": 1700628014314,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ouBKBqB7OI",
                "forum": "WNLAkjUm19",
                "replyto": "Wl0aa4RyNt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8697/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8697/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer ss62 (1/3)"
                    },
                    "comment": {
                        "value": "We thank Reviewer ss62 for careful reading and detailed comments. We address your concerns in the following points:\n\n---\n\nQ1. I think using discrete tokenization is not a mainstream of MIM. Representative methods, such as MAE, MaskFeat, and data2vec, demonstrate impressive performance without the discrete tokens.\n\nA1. Although discrete tokenization is not a default choice in MIM, it has been receiving wider and wider attention recently. In fact, one of the first MIM papers BeiT [1] uses tokenization, and so does iBoT [2]. Compared to those relying on raw inputs, the tokenization-based MIMs generally achieve much higher linear probing accuracy, which is known to benefit zero-shot generalization. \n\nVery recently, there is a phenomenological work [3] showing that tokenization is the key to visual generation. With the help of a good tokenizer, they attain the **best generation quality on ImageNet** simply with language model, surpassing previous sophiscated diffusion models. Similarly, visual tokens are also widely adopted by recent developments of visual foundation models. For example, equipped with visual tokenizers, MAGE [4] achieves **SOTA  generation ability within MIM methods**, and BEiT-3 [5] demonstrates **SOTA transfer performance on both vision-only and vision-language tasks**.\n\nTherefore, the importance of tokenization is increasingly recognized in visual tasks. And our investigation from a theoretical perspective provides timely insights into the mechanism behind it. As the first theoretical work in this direction, we believe that it could help inspire more researchers to investigate the use of tokenization for visual representation learning.\n\n[1] Bao et al. BEiT: BERT Pre-Training of Image Transformers, Arxiv 2106.08254.\n\n[2] Zhou et al. Image BERT Pre-training with Online Tokenizer, ICLR 2022.\n\n[3] Yu et al. Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation, Arxiv 2310.05737.\n\n[4] Li et al. MAGE: MAsked Generative Encoder to Unify Representation Learning and Image Synthesis, CVPR 2023.\n\n[5] Wang et al. BEiT Pretraining for All Vision and Vision-Language Tasks, CVPR 2023.\n\n\n---\n\nQ2. According to Theorem 1, image classification training could be the best way to downstream error bound. But, in practice, MIM works better than classification training in a lot of cases. Thus, I doubt the general applicability of this theorem and the metric (TCAS) on diverse MIM tasks.\n\nA2. We note that in existing literature, MIM has many downstream tasks, e.g., classification, detection, segmentation, etc. Among classification, there are two settings: linear probing (frozen features) and finetuning (tuning all features). Theorem 1 is established for the linear probing setting, which is the common evaluation setting in SSL theory, e.g., [6,7]. And indeed, the empirical results align well with our theory, since **MAE pretrained features** (no finetuning) underperform supervised learning by a large margin in this setting, as shown below.\n\n|Method|Epochs|Accuracy (%)|\n|---|---|---|\n|Supervised|300|77.9|\n|MAE|200 pretrain + 100 linear probing|55.4|\n\n**Difficulty in theoretical analysis.** Meanwhile, as you pointed out, MIM features often perform better on other settings when finetuning is allowed. However, we are not aware of any existing theory on these tasks. Finetuning is hard for theoretical analysis since the MAE pretraining weights only serve as the initialization, and SGD training on NNs is highly non-contex. \n\n**Correlation between tasks.** Nevertheless, we note that the downstream performance of MIM on other datasets is often correlated well with its linear classification performance [8]. Therefore, although Theorem 1 is based on linear classification, we believe the theorem still provides valuable guidelines for improving its performance on other MIM tasks.\n\n[6] Zhang et al. How Mask Matters: Towards Theoretical Understandings of Masked Autoencoders, NeurIPS 2022.\n\n[7] Cao et al. How to understand masked autoencoders, Arxiv 2202.03670.\n\n[8] Li et al. Benchmarking Detection Transfer Learning with Vision Transformers, Arxiv 2111.11429."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8697/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491595978,
                "cdate": 1700491595978,
                "tmdate": 1700544662117,
                "mdate": 1700544662117,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hlJEx3Y801",
                "forum": "WNLAkjUm19",
                "replyto": "Wl0aa4RyNt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8697/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8697/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer ss62 (2/3)"
                    },
                    "comment": {
                        "value": "Q3. Experimental results are limited to a small dataset (ImageNet-100) and short training (200 epochs on IN-1k). I think it is not enough to validate the effect of K-MIM. Reported numbers on TCAS are also limited.\n\nA3. Thanks for your suggestions! We have added additional experiments involving 800 epochs of pretraining on ImageNet-1k using ViT-B as the backbone. The comparisons with baseline methods are presented in the table below:\n\n|Method|Epochs|Linear Acc.|Finetuning Acc.|\n|---|---|---|---|\n|MAE|1600|68.0|83.6|\n|BEiT|800|N/A|83.2|\n|K-MIM PIXEL|800|69.4|83.6|\n\nNotably, with only half the number of pretraining epochs, K-MIM PIXEL reaches the same finetuning accuracy with MAE and even ourperforms MAE in the linear probing accuracy. The results indicate that K-MIM PIXEL outperforms the baseline results in the classification tasks, ensuring the better learning ability of K-MIM. We also add this experiments in Appendix D in the revision.\n\n\n---\n\nQ4. Using DINO as a target representation is similar to [9]. Is there any relation between the distillation target and the TCAS metric? I think the paper would be better to cite [9] and add a discussion on it.\n\nA4. Our proposed method, MaskFeat, and dBot [9] can all leverage representations from a pretrained model like DINO as the prediction target, with variations in their processing methods. The distinctions are outlined in the following table:\n\n|Method|Process Method|\n|---|---|\n|MaskFeat|Directly use the representations|\n|dBot|Initially employs the representations, updating them per certain epochs|\n|K-MIM|Use the K-means results of the representations|\n\nSince the prediction target of dBot remains continuous, and its training is conducted in a multi-stage manner, our TCAS metric is designed specifically for discrete targets. As such, we will leave the analysis of distillation target of dBot to future works. We have cited [9] and incorporated the above discussion into Section 2 in the revision.\n\n[9] Liu et al. Exploring Target Representations for Masked Autoencoders, Arxiv 2209.03917.\n\n---\n\nQ5. According to theorem 1, it looks like an image classifier would be the best tokenizer for MIM. What is the TCAS score and MIM performance when using an image classifier, such as DeiT, as a tokenizer?\n\nA5. We list the comparison of different tokenizers on ImageNet-100 using ViT-S with 200 epochs training in the table below for better illustration. The clustering number is set to $K=100$.\n\n|Method|TCAS score|Linear Probing Acc.|\n|---|---|---|\n|K-MIM PIXEL|0.52|50.1|\n|K-MIM DINO|0.15|59.7|84.7|\n|K-MIM MAE|0.34|52.3|83.2|\n|K-MIM DeiT|0.09|64.8|83.1|\n\nThe results indicate that employing DeiT as a tokenizer obtains the lowest TCAS score and outperforms other tokenizers in the linear probing accuracy, which aligns well with Theorem 1. For more detailed information, please refer to Appendix E, where we organize the TCAS scores and performances of various tokenizers.\n\n---"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8697/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491686677,
                "cdate": 1700491686677,
                "tmdate": 1700491958033,
                "mdate": 1700491958033,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ntJdsZPstH",
                "forum": "WNLAkjUm19",
                "replyto": "Wl0aa4RyNt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8697/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8697/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer ss62 (3/3)"
                    },
                    "comment": {
                        "value": "---\n\nQ6. Section 3 is explained with tokenization for a group-of-tokens, i.e. $x_2\\in R^{n\\times s}$. But, in Section 4, it seems the tokenization is conducted for a single token. Is it possible to generalize a theorem from the group-of-tokens case to the single-token scenario?\n\nA6. Indeed, in Section 3, our theory deals with general tokenizers that involve interactions among patches. However, in practice, it leads to a **exponential large space**, since each patch representation can be unique. However, when designing a practical metric in Sec 4, it is intractable for us to deal with. So we consider a bag-of-word model [10] for patch representations during the analysis in Sec 4, making it computationally tractable. Since patch representations are often local, this simplification (as a 1-gram model) also makes sense. In practice, we find the derived TCAS metric has good alignment with downstream performance, indicating that it could provide insights for the general cases. We have added clear explanations on these relationship in the beginning of Section 4.1.\n\n[10] Zellig Harris. Distributional Structure, doi:10.1080/00437956.1954.11659520\n\n---\n\nQ7. Discrete tokenizers like dVAE and VQGAN employ ConvNet or ViT, utilizing the entire image to create tokens. These tokens are interrelated, and a token from one location can incorporate patches from others. However, it looks like the paper handles these tokens as individual local information, which is not correct. Is there any explanation for this?\n\nA7. Please see A6. \n\n\n---\n\nQ8. Although K-MIM has better TCAS than other tokenizers, it is hard to say that K-MIM is specially designed for TCAS. Is there any discrete tokenization method optimized for the TCAS metric?\n\nA8. The guiding principle for TCAS is to assess the dissimilarity between the discrete token class and the true label. **Hence, the optimal choice for the TCAS metric is simply utilizing the true label as the tokenization.** In this context, our aim in the paper is to explore how to optimize the TCAS metric **when label information is unavailable**. Employing clustering methods such as K-means is likely to group samples with the same label together. Therefore, K-MIM is designed to optimize TCAS **under the condition of lacking label information.**\n\n---\n\nThanks for your comments and hope our answers could address your concerns. Please let us know if you have additional questions."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8697/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491886314,
                "cdate": 1700491886314,
                "tmdate": 1700491886314,
                "mdate": 1700491886314,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qluFAgKNo6",
                "forum": "WNLAkjUm19",
                "replyto": "Wl0aa4RyNt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8697/Reviewer_ss62"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8697/Reviewer_ss62"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response.\n\nYour response has addressed all of my questions.\nI'm satisfied with the revision.\n\nI think short responses can not solve the major weaknesses.\n- I knew that some papers use discrete tokens. But, still, discrete tokenization isn't an essential part of MIM, the paper can only contribute a specific range of MIM.\n- According to A2, A3, and A8, the contribution is focused on linear probing rather than fine-tuning of MIM. Because I think fine-tuning performance is the most surprising factor of MIM, it is a major weakness of the paper.\n\nBut, I think the paper has a limited yet original contribution, and I slightly lean toward acceptance. \nI adjusted my rating to marginal acceptance."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8697/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700627122139,
                "cdate": 1700627122139,
                "tmdate": 1700630589953,
                "mdate": 1700630589953,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xztkjxSEqQ",
            "forum": "WNLAkjUm19",
            "replyto": "WNLAkjUm19",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8697/Reviewer_yDgo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8697/Reviewer_yDgo"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the role of tokenization in Masked image modeling (MIM). It starts by discussing how discrete tokenization affects generalization performance on downstream tasks. For measuring the proficiency of different tokenization roles, this work designs the token-class alignment similarity (TCAS) metric. Based on the TCAS, they propose a MIM framework (K-MIM) with a cluster-based discrete tokenization scheme. Extensive experiments are conducted to validate the effectiveness of the proposed tokenization strategy."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The main problem, \"What is the role of tokenization in MIM? How does it affect downstream performance?\", is interesting and necessary.\n- The proposed token-class alignment similarity (TCAS) is a cheap but effective metric to measure the performance of tokenization roles.\n- This paper theoretically and empirically demonstrates the superiority of class-wise tokenization.\n- The paper is well-organized and easy to follow."
                },
                "weaknesses": {
                    "value": "- It would be better to show more ablation results about clustering numbers in Table 4."
                },
                "questions": {
                    "value": "- Could you please show more ablation results about clustering numbers (e.g., K = 10, 25, 200) in Table 4?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8697/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8697/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8697/Reviewer_yDgo"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8697/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826610022,
            "cdate": 1698826610022,
            "tmdate": 1699637089847,
            "mdate": 1699637089847,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9MSYpNizDt",
                "forum": "WNLAkjUm19",
                "replyto": "xztkjxSEqQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8697/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8697/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer yDgo"
                    },
                    "comment": {
                        "value": "We thank Reviewer yDgo for the appreciation on our work. We address your concern in the following point:\n\n---\n\nQ1. Could you please show more ablation results about clustering numbers (e.g., K = 10, 25, 200) in Table 4?\n\nA1. Sure. We conduct K-MIM PIXEL experiments on ImageNet-100 using ViT-Small, with clustering numbers $K=10, 25, 200$. The results (along with $K=50,100,1000$) are shown below.\n\n|Clustering Numbers $K$| LP Acc | FT Acc|\n|---|---|---|\n|10|42.7|81.9|\n|25|48.3|84.6|\n|50|52.7|86.4|\n|100|50.1|83.7|\n|200|50.8|84.8|\n|1000|50.6|84.4|\n\nThe results indicate that the performance peaks at $K=50$ and gradually declines as $K$ decreases from this optimal value. The diminished performance for small $K$ is due to the large inter-class connectivity and large labeling errors, where we have discussed in Section 3 that these factors lead to a larger error bound and consequently result in poorer performance.\n\n\n\n---\n\nThanks for your comments and hope our answers could address your concerns. Please let us know if you have additional questions."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8697/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491343632,
                "cdate": 1700491343632,
                "tmdate": 1700491343632,
                "mdate": 1700491343632,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aPq3TsSozg",
                "forum": "WNLAkjUm19",
                "replyto": "9MSYpNizDt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8697/Reviewer_yDgo"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8697/Reviewer_yDgo"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer Response"
                    },
                    "comment": {
                        "value": "Thanks for the response, I have decided to maintain my original score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8697/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631543401,
                "cdate": 1700631543401,
                "tmdate": 1700631543401,
                "mdate": 1700631543401,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]