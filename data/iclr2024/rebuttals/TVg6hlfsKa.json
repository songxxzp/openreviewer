[
    {
        "title": "Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition"
    },
    {
        "review": {
            "id": "m2QE36Fkd9",
            "forum": "TVg6hlfsKa",
            "replyto": "TVg6hlfsKa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1824/Reviewer_Gibf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1824/Reviewer_Gibf"
            ],
            "content": {
                "summary": {
                    "value": "In this paper the authors propose a global-local adaptation method to seamlessly adapt the pre-trained DINOv2 model to produce both global and local features for the visual place recognition task. The proposed feature representation can focus on discriminative landmarks and eliminate dynamic interference. The output local features are used in local matching for re-ranking to further boost performance. This method outperforms other state-of-the-art methods on multiple datasets with high computational efficiency."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThis paper is very well written and clearly presented.\n2.\tRecapitulation of related work is good.\n3.\tThe authors design a hybrid adaptation method to seamlessly adapt pre-trained foundation model to the VPR task. The method is novel and interesting, and the authors did not over-complicate it.\n4.\tThe experimental results are really good."
                },
                "weaknesses": {
                    "value": "1.\tPitts250k is also a common VPR dataset. The proposed approach has shown excellent results on multiple benchmark datasets. Providing the results on the Pitts250k dataset might make the experiment more complete.\n2.\tRe-ranking top-100 candidates seems a common setting for two-stage VPR methods. However some works also show the performance with different numbers of re-ranking candidates [1], which can help other researchers choose the optimal number of candidates when using this method. I think it is also necessary to show the performance of different numbers of candidates.\n\n[1] Zhu, Sijie, et al. \"R2former: Unified retrieval and reranking transformer for place recognition.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
                },
                "questions": {
                    "value": "Please see the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1824/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698619066207,
            "cdate": 1698619066207,
            "tmdate": 1699636111857,
            "mdate": 1699636111857,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Lb0HVDk08F",
                "forum": "TVg6hlfsKa",
                "replyto": "m2QE36Fkd9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1824/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1824/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Gibf"
                    },
                    "comment": {
                        "value": "Thanks for your encouraging words and constructive comments.\n\nThe results of our SelaVPR on Pitts250k are shown in the table below. MixVPR achieves the previous best results on Pitts250k, and SelaVPR outperforms MixVPR.\n\n|  Method  |   | Pitts250k  |  |  \n| :---: | :---: | :---: | :---: | \n|   | R@1 | R@5 | R@10 |\n| MixVPR | 94.1 | 97.9 | 98.7 |\n| SelaVPR | 95.7 | 98.8 | 99.2 |\n\nThe results of re-ranking different numbers of candidates are shown in the table below. Performance roughly reaches saturation when re-ranking top-100 candidates, which is our recommended setting for optimal recognition performance. Re-ranking top-50 (or even top-20) candidates is also a good option for faster two-stage retrieval.\n\n|  Method  |   | Tokyo24/7  |  |  |   | Pitts30k  |  |  |   | MSLS-val  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|   | R@1 | R@5 | R@10 |  | R@1 | R@5 | R@10 |  | R@1 | R@5 | R@10 |\n| re-ranking top-20  | 93.0 | 96.2 | 97.1 |  | 92.7 | 96.6 | 97.5 |  | 90.7 | 96.4 | 97.0 |\n| re-ranking top-50  | 93.7 | 96.5 | 96.8 |  | 92.8 | 96.8 | 97.6 |  | 90.8 | 96.4 | 97.0 |\n| re-ranking top-100  | 94.0 | 96.8 | 97.5 |  | 92.8 | 96.8 | 97.7 |  | 90.8 | 96.4 | 97.2 |\n| re-ranking top-200  | 94.0 | 96.8 | 97.5 |  | 92.8 | 96.9 | 97.8 |  | 90.7 | 96.5 | 97.3 |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1824/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494966564,
                "cdate": 1700494966564,
                "tmdate": 1700494966564,
                "mdate": 1700494966564,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sVeQqVGpqA",
            "forum": "TVg6hlfsKa",
            "replyto": "TVg6hlfsKa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1824/Reviewer_mSaw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1824/Reviewer_mSaw"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a hybrid global and local adaptation method to adapt pre-trained foundation models to two-stage visual place recognition. The global adaptation is achieved by adding parallel and serial adapters in each ViT block. The local adaptation is implemented by adding up-sampling layers after ViT backbone to produce dense local features. A novel mutual nearest neighbor local feature loss is proposed to train the local adaptation module. This architecture achieves fast two-stage place retrieval and outperforms several SOTA methods. It is ranked first on the MSLS challenge leaderboard."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well-organized and presents a good overview of the related work.\nThe approach is simple and easy to follow. \nThe experimental datasets are sufficient (conducted on 6 VPR benchmark datasets) and the results are excellent (outperform previous SOTA methods by a large margin).\nThis method can bridge the gap between the tasks of model pre-training and VPR using only a small amount of training data and training time. The two-stage retrieval runtime on Pitts30k is less than 0.1s (about 3% of the TransVPR method). This makes contributions to use pre-trained foundation models for real-world large-scale VPR applications."
                },
                "weaknesses": {
                    "value": "This method achieves significantly better performance than other methods on several VPR datasets, and the authors qualitatively demonstrate some challenging examples. However, the motivation of the proposed method is not demonstrated well.  In particular, the gap of the tasks of model pre-training and VPR is not very clear to me. In addition, this paper does not show failure cases, which can inform future research in VPR."
                },
                "questions": {
                    "value": "1. Will re-ranking more candidate places achieve better performance or hurt the results?\n2. This work finetunes the models on the MSLS dataset and further finetunes them on Pitts30k to test on Pitts30k and Tokyo24/7, which is the same as R2Former. However, the R2Former work provides the result on Pitts30k of the model that only trained on MSLS, which can prove the model's transferability to the domain gap. Can the proposed SelaVPR still outperform R2Former on Pitts30k using only MSLS for training?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1824/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698626078820,
            "cdate": 1698626078820,
            "tmdate": 1699636111785,
            "mdate": 1699636111785,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "irYf5rVgti",
                "forum": "TVg6hlfsKa",
                "replyto": "sVeQqVGpqA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1824/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1824/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mSaw"
                    },
                    "comment": {
                        "value": "Thanks for your positive recommendation and insightful comments.\n\n**[Q1:]** Will re-ranking more candidate places achieve better performance or hurt the results?\n\n**[A1:]** Re-ranking more candidates neither improves nor hurts performance. The results of re-ranking different numbers of candidates are shown in the table below. Performance roughly reaches saturation when re-ranking top-100 candidates, which is our recommended setting for optimal recognition performance.\n\n|  Method  |   | Tokyo24/7  |  |  |   | Pitts30k  |  |  |   | MSLS-val  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|    | R@1 | R@5 | R@10 |  | R@1 | R@5 | R@10 |  | R@1 | R@5 | R@10 |\n| re-ranking top-20  | 93.0 | 96.2 | 97.1 |  | 92.7 | 96.6 | 97.5 |  | 90.7 | 96.4 | 97.0 |\n| re-ranking top-50  | 93.7 | 96.5 | 96.8 |  | 92.8 | 96.8 | 97.6 |  | 90.8 | 96.4 | 97.0 |\n| re-ranking top-100  | 94.0 | 96.8 | 97.5 |  | 92.8 | 96.8 | 97.7 |  | 90.8 | 96.4 | 97.2 |\n| re-ranking top-200  | 94.0 | 96.8 | 97.5 |  | 92.8 | 96.9 | 97.8 |  | 90.7 | 96.5 | 97.3 |\n\n\n**[Q2:]** Can the proposed SelaVPR still outperform R$^2$Former on Pitts30k using only MSLS for training?\n\n**[A2:]** The results of R$^2$Former and our SelaVPR on Pitts30k using only MSLS for training are shown in the table below. Our SelaVPR also outperforms R$^2$Former. The results of SelaVPR trained only on MSLS are even better than those of SelaVPR only trained on Pitts30k (similar to the case with R$^2$Former), showing that our method has good transferability as well.\n\n|  Method  |   | Pitts30k  |  |  \n| :---: | :---: | :---: | :---: | \n|    | R@1 | R@5 | R@10 |\n| R$^2$Former | 88.4 | 94.2 | 95.7 |\n| SelaVPR | 92.2 | 96.5 | 97.6 |\n\nFor weaknesses: The gap between the model pre-training and VPR tasks primarily refers to the difference in training objectives and data, which further results in the pre-trained models focusing on different objects from the specially trained VPR models. A robust VPR model should focus on the static discriminative backgrounds (e.g., buildings and trees) and ignore the dynamic foreground objects (e.g., pedestrians and vehicles), but a pre-trained model cannot do this, as vividly shown in the attention map visualizations in our paper.\nThe failure cases mainly occur in some natural scenes that lack discriminative landmarks, which is difficult to address for existing VPR methods. It\u2019s the focus of our future work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1824/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494155259,
                "cdate": 1700494155259,
                "tmdate": 1700494155259,
                "mdate": 1700494155259,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "I1dLlCfPBc",
            "forum": "TVg6hlfsKa",
            "replyto": "TVg6hlfsKa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1824/Reviewer_S5xw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1824/Reviewer_S5xw"
            ],
            "content": {
                "summary": {
                    "value": "1.\tVisual place recognition (VPR) is a fundamental task for applications in robot localization and augmented reality. This paper aims to bridge the gap between the tasks of pre-training and VPR, thus fully unleashing the capability of pre-trained models for VPR.\n2.\tThe authors introduce a hybrid adaptation method to get both global features for retrieving candidate places and dense local features for re-ranking. \n3.\tMeanwhile, a novel local feature loss is designed to guide the production of proper local features for local matching without geometric verification in re-ranking."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1.\tThis work is novel and solid. The proposed SelaVPR is a well-designed method and achieves very fast two-stage retrieval.\n2.\tThis paper makes two technically strong contributions: closing the gap between the pre-training and VPR tasks, and outputting proper dense local features for VPR task using DINO v2. The extensive ablation experiments and visualization results show that the proposed method well adapts the pre-trained model to the VPR task. The produced dense local features also perform well in local matching re-ranking.\n3.\tThis method achieves better performance than the SOTA methods with less training data and training time."
                },
                "weaknesses": {
                    "value": "1.\tThis work adapts the pre-trained model to the VPR task. The global and local features produced by this hybrid adaptation seem to be useful for more visual tasks. Expanding the use of this method can make the contribution of this paper more obvious.\n2.\tThe clarity of the paper could be further improved."
                },
                "questions": {
                    "value": "1.\tWhy is L2 distance used to measure global feature similarity, but dot product used to calculate local feature similarity?\n2.\tIs it feasible to re-rank top-k candidate images directly using coarse patch tokens from ViT? and how is the performance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1824/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698744613965,
            "cdate": 1698744613965,
            "tmdate": 1699636111706,
            "mdate": 1699636111706,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6kqfFZGptD",
                "forum": "TVg6hlfsKa",
                "replyto": "I1dLlCfPBc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1824/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1824/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer S5xw"
                    },
                    "comment": {
                        "value": "Thanks for your positive recommendation and valuable suggestions.\n\n**[Q1:]** Why is L2 distance used to measure global feature similarity, but dot product used to calculate local feature similarity?\n\n**[A1:]** To make the first stage (global retrieval) compatible with some one-stage VPR methods and the benchmark [1], the L2 distance is used to compute global feature similarity. For the convenience of finding mutual matches (i.e. fast computing), local feature similarity is based on the inner product. Since local features have been L2 normalized, the inner product is equal to cosine similarity.\n\n**[Q2:]** Is it feasible to re-rank top-k candidate images directly using coarse patch tokens from ViT? and how is the performance?\n\n**[A2:]** It\u2019s feasible in terms of program implementation. However, Re-ranking candidates with the coarse patch tokens from ViT does not make sense in two-stage VPR. Although it provides an improvement (but not as good as ours) on Pitts30k compared to global retrieval, it performs significantly worse than global retrieval on MSLS-val, where VPR methods are more susceptible to perceptual aliasing. That is, re-ranking with coarse patch features will bring negative effects and damage the results of global retrieval (i.e. the first stage retrieval). \n\n|  Method  |   | Pitts30k  |  |  |   | MSLS-val  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|    | R@1 | R@5 | R@10 |  | R@1 | R@5 | R@10 |\n| global adaptation for global retrieval  | 87.3 | 94.6 | 96.6 |  | 87.4 | 95.9 | 96.9 |\n| coarse patch tokens for re-ranking  | 89.8 | 95.4 | 96.9 |  | 82.8 | 92.0 | 94.9 |\n| dense local features for re-ranking  | 91.4 | 96.5 | 97.8 |  | 90.8 | 96.4 | 97.2 |\n\nFor the weaknesses, we will improve the clarity in the revised paper and try to use the hybrid adaptation method for other visual tasks in future work. Thanks again for your suggestions.\n\nReference\n\n[1] Gabriele Berton, Riccardo Mereu, Gabriele Trivigno, Carlo Masone, Gabriela Csurka, Torsten Sattler, and Barbara Caputo. Deep visual geo-localization benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5396\u20135407, 2022."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1824/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700492487392,
                "cdate": 1700492487392,
                "tmdate": 1700492487392,
                "mdate": 1700492487392,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "oEDBgEKMBw",
            "forum": "TVg6hlfsKa",
            "replyto": "TVg6hlfsKa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1824/Reviewer_LtSJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1824/Reviewer_LtSJ"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a method to adapt foundation models to the task of visual place recognition, arguing that the object-centric focus of the training of foundation models does not align with the background/static-object attention needed in the VPR task. Thus, they propose, instead of fine-tuning the visual transformers, to extend the transform block with two mechanisms (added MLP) that operate as adapter for the global feature computation. Another local adaptation is done for reranking, similar to geometric verification of retrieved images."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ concept is straightforward: the idea behind the adaptation, and the motivation why it is needed to adapt from foundation models is clearly presented.\n+ experiments on relevant datasets: results are very good, improving on existing approaches\n+ the paper is easy-to-read"
                },
                "weaknesses": {
                    "value": "_very limited or non-existing insights_\nthe paper is built solely around overcoming the results of existing methods, while insights and evidence-based contributions are not provided. After reading the paper I am left with a big question: \"why this method works and what do I learn that can perhaps use to design methods in different applications?\". Would the adaptation work also for CNNs pre-trained on ImageNet (as they share the same object-centric bias)? \nI would expect (at ICLR) a thorough analysis of reasons why the performance are much higher, what the implications of doing adaptation are, and what are the real scientific contributions behind this work (not just that the method gets better results than sota methods).\n\n_design choice weakly explained_\nno motivations or justification of why the adapters are designed in a certain way, and what the difference w.r.t. existing approaches for adaptation of foundation models are. What is the hypothesis behind this kind of design, and what explanations can be given (with experimental evidence) about their working principle?\n\n_data-efficiency not elaborated upon_\nas data-efficiency is a key argument about using foundation models, the authors indeed mention it but do not provide substantiable experimental evidence about how it benefits their approach. \n\n_parameter difference not well-analyzed_\nThe adaptation mechanisms proposed are still requiring the fine-tuning of +50M parameters, which is much more than other methods train. Summed up with the +300M parameters of the foundation backbones, these models account for much more capacity than whatever method used previously. The authors do not provide any discussion about this point, or experiments with adaptation of other (smaller) models.\n\nA missing reference:\nLeyva-Vallina et al., Data-Efficient Large Scale Place Recognition With Graded Similarity Supervision; CVPR 2023"
                },
                "questions": {
                    "value": "- How would the adapters work with other (smaller) models?\n- What are the reasons, and interpretaions (with evidences) of why the proposed adapters work?\n- How the adapter parameter space influences the improve of performance, and how does it relate with 'smaller' backbones?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1824/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698940936568,
            "cdate": 1698940936568,
            "tmdate": 1699636111604,
            "mdate": 1699636111604,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9Uz3I6IFkr",
                "forum": "TVg6hlfsKa",
                "replyto": "oEDBgEKMBw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1824/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1824/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LtSJ (1/2)"
                    },
                    "comment": {
                        "value": "Thanks for your comments and suggestions. We hope the following clarifications help to address your concerns.\n\n**[Q1:]** How would the adapters work with other (smaller) models?\n\n**[A1:]** The adapters can work with smaller models in the way described in the paper and achieve good performance. But it only makes sense if this model is pre-trained on a large amount of data and produces well-generalized representations. Otherwise, we can directly fully fine-tune the model. Here we provide the results (only fine-tuning the model on Pitts30k and testing on Pitts30k) of our adaptation method using a distilled version of DINOv2 (based on ViT-Base) as backbone, denoted as DINOv2-B. The DINOv2-L (based on ViT-Large) is the backbone used in our paper. Their performances are close, demonstrating that our adaptation method can be used with smaller models.\n\n| Backbone  | R@1 | R@5 | R@10 |\n| :---: | :---: | :---: | :---: |\n| DINOv2-L   | 91.4 | 96.5 | 97.8 |\n| DINOv2-B   | 90.5 | 96.1 |97.1 |\n\n**[Q2:]**  What are the reasons why the proposed adapters work?\n\n**[A2:]**  Although visual foundation models can provide powerful feature representations, the objects that need to be focused on may be different for various downstream tasks. Freezing the foundation model can retain the powerful representation capabilities, while inserting the lightweight trainable adapters so that the entire model can be adapted to the VPR task after fine-tuning on VPR datasets, that is, the trained adapter has learned VPR-related knowledge. Our visualization results well illustrate that the adapted model can focus on objects that are helpful for place recognition and ignore dynamic foreground interference, which is the key reason for its good performance. Meanwhile, our hybrid adaptation method combines global adaptation and local adaptation, which not only yields a robust global representation, but also enables the model to output dense local features for spatial matching re-ranking, thereby further boosting performance.\n\n**[Q3:]** How the adapter parameter space influences the improve of performance, and how does it relate with 'smaller' backbones.\n\n**[A3:]** The number of parameters of the adapter in our global adaptation is adjustable. Appropriately increasing the parameters of the adapter can slightly improve performance. However, in most cases, we can actually use a very lightweight adapter to obtain good performance. In the Appendix.C of our paper, we have shown the results of using only one adapter in each transformer block or reducing the bottleneck ratio to 0.25, and the performance is still good. Here, we provide the results (on Pitts30k) of using only the parallel adapter and reducing the bottleneck ratio from 0.5 (in the paper) to 0.05, as shown in the table below (SelaVPR** uses the more lightweight adapter. SelaVPR represents the original setup in paper). The SelaVPR** has only 5.19M trainable parameters (less than most methods), and the results are still excellent. For smaller backbones, fewer transformer blocks and lower token dimensions will naturally lead to fewer parameters of the adapter. Overall, the original paper focused more on recognition performance without excessively limiting the number of parameters. In fact, our method can achieve good performance using a significantly smaller number of parameters.\n\n| Method  | Tunable Param (M)  | R@1 | R@5 | R@10 |\n| :--- | :---: | :---: | :---: | :---: |\n| ResNet50-GeM |  7  |  82.3 |  91.9 |  94.6 | \n| SelaVPR  |  53 |  91.4 |  96.5 |  97.8 |\n| SelaVPR** |  5 |  91.2 |  96.4 |  97.4 |"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1824/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490480693,
                "cdate": 1700490480693,
                "tmdate": 1700490480693,
                "mdate": 1700490480693,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pmAdEPCPXV",
                "forum": "TVg6hlfsKa",
                "replyto": "oEDBgEKMBw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1824/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1824/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer LtSJ (2/2)"
                    },
                    "comment": {
                        "value": "The following are Responses for the Weaknesses:\n\n**1. About the insights and contributions**\n\nInsights: The pre-trained foundation models (e.g. DINOv2) have achieved remarkable performance on many vision tasks given their ability to produce well-generalized representations. The recent AnyLoc work [1] also uses DINOv2 as the backbone, but without any adaptation or fine-tuning. Although AnyLoc uses a larger version backbone (based on ViT-giant), its performance is obviously worse than ours. Our work shows that there is a gap between the tasks of model pre-training and VPR (as we described in the third paragraph of the Introduction section). Model adaptation is a necessary thing to fully unleash the capability of pre-trained foundation models for VPR.\n\nContributions: The main contribution is the hybrid global-local adaptation method to bridge the gap and fully unleash the capability of pre-trained models for VPR. Our work provides an effective and efficient way to seamlessly adapt pre-trained foundation models to produce both global and (dense) local features for two-stage VPR. Moreover, we propose a mutual nearest neighbor local feature loss to train the local adaptation module, which is combined with global feature loss for fine-tuning. Different from other two-stage methods, the local features produced by our model are directly used in cross-matching for re-ranking, without time-consuming geometric verification.\n\nFor different applications: Our method is designed to seamlessly adapt the pre-trained foundation model for the VPR task. However, we think the hybrid adaptation method (to get both global features and local features) may also be useful for other applications. \n\nWould it work for CNN: For CNN models pre-trained on ImageNet, adaptation is not necessary. We can fully fine-tune such a model directly. Most of the adaptation methods are designed for the foundation models (especially these transformer-based models) pre-trained on a large amount of data (larger than ImageNet).\n\nAbout the performance: The powerful representation ability of the foundation model is the basis for good performance. But more importantly, our method bridges the gap between the tasks of model pre-training and VPR through seamless adaptation, i.e., the implication of doing adaptation is bridging the gap. The adapted model can produce the global features and local features more suitable for VPR to address the challenges in VPR tasks and achieve excellent performance. \n\n**2. Design motivations**\n\nPlease see A2 for Q2. The design of our global adaptation is inspired by previous adapter-based work. However, we design a simple local adaptation method to adapt the ViT-based foundation models to produce dense local features for the local cross-matching in two-stage VPR. Our hybrid adaptation method combines the global adaptation and local adaptation to produce both global features and dense local features for two-stage VPR, which is different from previous adaptation works.\n\n**3. Data efficiency**\n\nThe adaptation method can keep the powerful pre-trained image representations intact. When there is only insufficient training data in the downstream task, it mitigates the risk of over-fitting. This is a benefit shared by most adaptation methods. It is also an advantage of our approach over most other VPR methods. The experimental evidence is in the Appendix.A of our paper.\n\n**4. Model parameters**\n\nPlease see A1 & A3 for the Q1 & Q3.\n\n**5. A missing reference**\n\nThanks for the reminder. We\u2019ll mention this work in our revised paper.\n\nReference\n\n[1] Nikhil Keetha, Avneesh Mishra, Jay Karhade, Krishna Murthy Jatavallabhula, Sebastian Scherer, Madhava Krishna, and Sourav Garg. Anyloc: Towards universal visual place recognition. arXiv preprint arXiv:2308.00688, 2023."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1824/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491657129,
                "cdate": 1700491657129,
                "tmdate": 1700491657129,
                "mdate": 1700491657129,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]