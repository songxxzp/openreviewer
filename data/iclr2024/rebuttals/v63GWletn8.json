[
    {
        "title": "Faster Sampling from Log-Concave Densities over Polytopes via Efficient Linear Solvers"
    },
    {
        "review": {
            "id": "NjVdlof2ny",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6187/Reviewer_mHXa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6187/Reviewer_mHXa"
            ],
            "forum": "v63GWletn8",
            "replyto": "v63GWletn8",
            "content": {
                "summary": {
                    "value": "This paper considers using a soft-threshold Dikin walk to sample from a polytope contained in a box of radius $R$. The sampling distribution is log-concave and specified by a Lipschitz/smooth function $f$. By using a pipeline introduced in [LLV20], they show how to speed up the algorithm introduced in [MV23] so that each iteration, instead of computing the Hessian of log-barrier using fast matrix multiplication, one can resort to the inverse maintenance data structure due to [LS15]. To this end, they achieve a similar per iteration cost as [LLV20], namely $\\mathrm{nnz}(A)+d^2$."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper improves the per iteration cost of [MV23] from $md^{\\omega-1}$ to $\\mathrm{nnz}(A)+d^2$. In most regimes where $m\\geq d$ and $A$ is relatively sparse, this is a strict upgrade from prior state-of-the-art."
                },
                "weaknesses": {
                    "value": "I'm very dubious of the novelty of this paper. What's the major difference between the algorithm in this paper and [LLV20]? While this paper only mentions [LLV20] sparingly, the algorithmic framework is almost identical. Specifically, [LLV20] shows that for sampling uniformly over a polytope with log-barrier, one can use the [LS15] inverse maintenance to compute a new sampling point. The determinant ratio term can be estimated via an unbiased estimator, which can further be estimated using Taylor expansion together with terms that can be quickly computed using the inverse maintenance data structure. The only difference is that this paper also needs to handle the regularization term, but it is neither surprising nor novel the machinery of [LLV20] also works here. \n\nIt is worth noting that [LLV20] does not provide any proof on why using an approximate solver and samples, the algorithm still converges. This paper provides a very simple argument to show it indeed works.\n\nOverall, I think this paper should provide comprehensive comparison with algorithm in [LLV20]. What's the difference here? What's new? Otherwise, it should acknowledge that the algorithm is largely derivative from [LLV20]. In its current writing, the authors acknowledge the fast linear solver part follows from [LLV20]. What about the determinant ratio part? What's the main difference between your approach for determinant and [LLV20]? What's the novelty of your algorithm?"
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6187/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6187/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6187/Reviewer_mHXa"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6187/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697177770411,
            "cdate": 1697177770411,
            "tmdate": 1699636673026,
            "mdate": 1699636673026,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pKYqcRiYUI",
                "forum": "v63GWletn8",
                "replyto": "NjVdlof2ny",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6187/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6187/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable comments and suggestions.  We address your questions and concerns below, specifically, in relation  to [LLV20]. Sorry about the confusion-- while we discuss the differences in the two papers in the proof overview (Section 4), we would be happy to make things more explicit and add a separate section outlining these differences.  \n\nIn short, the key differences between the algorithm and techniques in our paper and those of [LLV20] are as follows:  \n\n1. We use a different randomized estimator for the determinantal term in the Metropolis acceptance probability. We need a different estimator because the randomized estimator for the determinantal term in the Metropolis acceptance rule of [LLV20] does not lead to a valid unbiased estimator for the correct acceptance probability (even for the special case $f=0$). We think this is an error in their paper, and it is not easily fixable.  \n2. Additionally, we handle the more general case where $f \\neq 0$. This requires us to use a Markov chain based on an $\\ell_2$-regularized barrier function of [MV23], and leads to additional challenges in bounding the change of the Hessian matrix of the barrier function at each step of our algorithm.  \n\nWe explain each of these differences in detail below. If you would like to see these details in a separate section in a revision, we would be happy to do so. We hope you will reconsider your opinion of our paper.  Please feel free to reach out if something is not clear, or if you have additional questions."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700510337083,
                "cdate": 1700510337083,
                "tmdate": 1700510337083,
                "mdate": 1700510337083,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EXmBBvXgNO",
                "forum": "v63GWletn8",
                "replyto": "pKYqcRiYUI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6187/Reviewer_mHXa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6187/Reviewer_mHXa"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I agree with the first assessment, the [LLV20] acceptance rule likely has some issues even for uniform case. A fix for that is appreciated. However, this does not address my major concern of the paper --- from my point of view, this paper pretty straightforwardly combines the last section of [LLV20] and the soft-threshold Dikin walk in [MV23]. I strongly believe to justify the acceptance of this paper, authors should try to add more results in addition to the current $A+B$ result. For example, can you extend soft-threshold Dikin walk to other barriers for a better mixing? For more intricate barriers such as hybrid barrier or Lee-Sidford barrier, can you analogously design this [LLV20]-like fast solvers for them? At its current form, I will keep my score as is.\n\nP.S. (Not for this version), in the next revision, consider comparing with this paper: [KV23]. Authors show that it's possible to do log-concave sampling over polytopes using other barriers such as Lee-Sidford, get better mixing and without dependence on $R$. Moreover, they only require Lipschitz on the density instead of $f$. On the other hand, they do need $f$ to be $\\alpha$-relatively strong convex. I think no polynomial dependence on $R$ is essential for Dikin walk, as this (used to be) a key advantage over hit-and-run. Can you extend the [LLV20] machinery to their Dikin walk?\n\n[KV23] Y. Kook and S. Vempala. Gaussian Cooling and Dikin Walks: The Interior-Point Method for Logconcave Sampling. 2023."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700511603598,
                "cdate": 1700511603598,
                "tmdate": 1700511603598,
                "mdate": 1700511603598,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xvyTQaRlyE",
                "forum": "v63GWletn8",
                "replyto": "xkOoF7ds8S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6187/Reviewer_mHXa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6187/Reviewer_mHXa"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your followup. I apologize to authors for my initial review on the part of determinant term. Now I do understand that the authors try to fix a potential issue in [LLV20] without explicitly pointing out the possible errors in that paper. I chose confidence 5 in my initial review because I've tried to work on followups on both [LLV20] and [MV23] and have thoroughly studied the techniques and proofs from both of the papers. I acknowledge I've overlooked the determinant fix part in this paper and would like to apologize to the authors.\n\nHowever, I stand by my initial assessment that this paper lacks novelty. In my opinion, the main virtue of this paper is the fix and possibly a clearer explanation of the inverse maintenance algorithm in the last section of [LLV20]. However, I do feel this paper a bit too derivative from [MV23]. As authors mentioned in how they prove the data structure in turn works for their algorithm, they mainly use an argument from [MV23], saying that the regularized Hessian is actually the Hessian of log-barrier of *some* polytopes, therefore operating on the regularized Hessian is essentially ``change of variable'' on the Hessian of log-barrier. Once one realizes this reduction, then the framework and analysis from [LLV20] can be adapted, provided a few extra relatively straightforward lemmas on the regularized Hessian. In my opinion, the work [MV23] makes a large Delta in the sense that it bridges the gaps between uniform and log-concave sampling over polytopes using log-barrier Dikin walk, even though it has some polynomial dependence on $R$. This paper on the other hand, is a much smaller Delta from [MV23]: it fixes an issue in the algorithm of [LLV20] and deploys it to the [MV23] walk. Most of the analyses are also consequence of [MV23]. This is the major reason I asked for more results from my prior response, as at its current form, I feel the contribution of this paper is not significant enough to warrant an accept. \n\nI hope this clarifies my prior response and my rating. Again, thank you for your followup."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700543370277,
                "cdate": 1700543370277,
                "tmdate": 1700543370277,
                "mdate": 1700543370277,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s7x34ZT216",
                "forum": "v63GWletn8",
                "replyto": "Z8Icp8ykcJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6187/Reviewer_mHXa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6187/Reviewer_mHXa"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks authors for the followup. I think I've made my points quite clear in the prior response. The major contribution of this paper is a (potential) fix of the determinant estimation part of [LLV20]. Beyond that, I don't see explicitly where do you need *nontrivial* synthesis between [LLV20] and [MV23]. The algorithm is essentially identical to that of [LLV20] with a slightly different determinant estimation (it should be noted the fix also follows the general strategy of [LLV20] with a different acceptance rule and series, but the way it applies inverse maintenance is on a very similar term).\n\nI've never raised the bar for acceptance and have made myself very clear in the prior response. This paper makes a small Delta from [MV23], thus I feel the contributions are limited.\n\nFinally, as authors repeatedly mention they fix a major issue of [LLV20] (which I mostly agree, but this does not affect the main result of [LLV20] as theirs is identifying a sufficient condition for Dikin walk from uniform distribution, so that one can finally obtain a $d^2$ mixing time using Lee-Sidford barrier, improving the $d^{2.5}$ bound from [CDWY18]), then they should contain a section discussing the issues in [LLV20] and how they manage to fix it. This will improve the paper's presentation and signify the main contribution of this paper, albeit a different acceptance rule and series for logdet from [LLV20]."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674766668,
                "cdate": 1700674766668,
                "tmdate": 1700674766668,
                "mdate": 1700674766668,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qwSYq61QXu",
            "forum": "v63GWletn8",
            "replyto": "v63GWletn8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6187/Reviewer_CJYs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6187/Reviewer_CJYs"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents the current fastest algorithm for sampling from a log-concave distribution over a given polytope. In particular, they implement a more optimized version of the algorithm proposed by Mangoubi and Vishnoi [1], which is to appear in NeurIPS 2023, the arXiv version of which has been around for more than a year at this point. It is important to understand what [1] does, as the current paper heavily builds on it... \n\nThe algorithms considered in these papers are Metropolis-Hastings algorithms, which are a subclass of Markov chain Monte Carlo (MCMC) methods. For each step of the Markov chain, [1] needs time $O(md^{\\omega-1})$ as it involves some matrix inversion and determinant computation steps, which can all be done in $O(md^{\\omega-1})$ time by using algorithms for these problems which work for arbitrary matrices. The main claim in the current paper is that instead of recomputing these matrix operations from scratch in every step of the Markov chain, we can use the information from the previous step to speed up the computations of the current step.\n\nReferences:\n\n1: Sampling from Log-Concave Distributions over Polytopes via a Soft-Threshold Dikin Walk, Mangoubi, Oren and Vishnoi, Nisheeth K, arXiv preprint arXiv:2206.09384. To appear in NeurIPS 2023: https://neurips.cc/virtual/2023/poster/72502"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Significance: The problem of sampling from a log-concave distribution over a polytope is interesting and also has several ML applications.\n\nOriginality: Clearly, the paper is an advance over the state-of-the-art. However, I am not super sure about how original this is in terms of technical contributions...\n\nClarity: Paper is clear but a bit dense and slightly hard to read, especially if you are not already familiar with Mangoubi and Vishnoi [1]. I re-read this paper after reading [1] and it was much clearer the second time. It might have to do with space-constraints, as the arXiv version of [1] obviously has more space so can go over a lot of things in more detail...\n\nQuality: Overall seems to be of good quality. But I have not checked the correctness of the technical details. As it builds quite heavily on [1] and also a bunch of other results which I am not familiar with, I don't have the expertise to ascertain the correctness of the paper. But at least on the surface, it does seem like there are no major technical issues."
                },
                "weaknesses": {
                    "value": "I don't have too much to point here, except what I already wrote about clarity before. The authors should try to make the paper more accessible to people who might not have read [1]. I know this is a generic remark but honestly I have no clear idea how to improve the paper in terms of clarity... There were a bunch of typos here and there, some of which are:\n\nIn Theorem 2.1, 3) convex function $f:K \\mapsto \\mathbb{R}^d$. Here $f$ should be $K \\mapsto \\mathbb{R}$. I saw this repeated in a bunch of other places. I think it's a typo but since I saw it repeatedly, both in this paper, and also in [1], I tried to dig around to see if it makes sense for the codomain of a convex function to be $\\mathbb{R}^d$ instead of $\\mathbb{R}$ but in that case, $e^{-f}$ would not be real, but it has to be real for it to be a probability density function?\n\nPage 5, line 3: ball \"or\" radius -> ball of radius, again: $f: K \\mapsto \\mathbb{R}^d$\n\nPage 7, two lines below inequality (3): \u201carithemtic\u201d"
                },
                "questions": {
                    "value": "No additional questions"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6187/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809632829,
            "cdate": 1698809632829,
            "tmdate": 1699636672914,
            "mdate": 1699636672914,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lSJo3e5ogy",
                "forum": "v63GWletn8",
                "replyto": "qwSYq61QXu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6187/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6187/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable comments and suggestions. We are glad you find our contributions to be an advance over the state-of-the-art, and thank you for supporting our paper. We address your concerns below.\n\n>The authors should try to make the paper more accessible to people who might not have read [1].\n\nThank you for your suggestion. We will add a section in the appendix providing additional background on the soft-threshold Dikin walk Markov chain of [1].\n\n\n>convex function $f :K \\rightarrow \\mathbb{R}^d$.  Here $f$ should be $K \\rightarrow \\mathbb{R}$.\n\nSorry for the typo. It should read $f :K \\rightarrow \\mathbb{R}$, where $K\\subset \\mathbb{R}^d$ is a convex polytope.\n\n\n>Other typos:\n\nThank you for pointing out these typos. We will correct them in the final version."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700509170249,
                "cdate": 1700509170249,
                "tmdate": 1700509170249,
                "mdate": 1700509170249,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fkXbsqaAa6",
            "forum": "v63GWletn8",
            "replyto": "v63GWletn8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6187/Reviewer_nR3E"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6187/Reviewer_nR3E"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies a fundamental problem of sampling from convex sets, the problem of sampling with respect to a log-concave distribution. Previous work [Mangoubi-Vishnoi, NeurIPS'22] has given an algorithm with $O^*(md)$ iteration and each iteration takes $O^*(md^{\\omega-1})$ time. In this paper, the author improves the cost per iteration to $O^*(nnz(A)+d^2)$. This result directly answers the open problem proposed in [Lee-Sidford15, FOCS'15] which asks whether one can achieve such running time for the case $f\\equiv 0$. To achieve this, the authors make use of the inverse maintenance technique in  [Lee-Sidford15, FOCS'15] and show one can compute the estimation of determinant to high accuracy by cleverly constructing an unbiased estimator and making use of the linear system solver as a primitive."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "My general evaluation of this paper is very positive.  I did not manage to check the correctness of the proofs. Condition on the new Metropolis update rule and the log-determinant estimator is correct, I think everything goes through. This paper is technically solid and answers an important open problem."
                },
                "weaknesses": {
                    "value": "I think the primary weakness of this paper is the presentation. I understand that due to the nature of theory papers, it's hard to present everything important within the page limits. The most interesting part of the paper  for me is how to get a good estimation of log-determinant and I think it deserves some space in the main paper."
                },
                "questions": {
                    "value": "-"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6187/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6187/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6187/Reviewer_nR3E"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6187/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698867673471,
            "cdate": 1698867673471,
            "tmdate": 1699636672785,
            "mdate": 1699636672785,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qO4c5mlopS",
                "forum": "v63GWletn8",
                "replyto": "fkXbsqaAa6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6187/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6187/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable comments and suggestions. We are glad that you find that our paper is technically solid and answers an important open problem, and thank you for supporting our paper. We answer your specific question below.\n\n>The most interesting part of the paper for me is how to get a good estimation of log-determinant and I think it deserves some space in the main paper.\n\nThank you for the suggestion. We discuss the estimation of the log-determinant on page 8 of the proof overview in Section 4. We would be happy to add a separate section that outlines the proof of Lemma B.3 (which allows us to show that our randomized estimator concentrates near its mean) in the main body of the paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508838817,
                "cdate": 1700508838817,
                "tmdate": 1700508838817,
                "mdate": 1700508838817,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lA8JtCVkur",
            "forum": "v63GWletn8",
            "replyto": "v63GWletn8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6187/Reviewer_6Phh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6187/Reviewer_6Phh"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies sampling from a logconcave distribution on a polytope. To this end, it uses a soft-threshold Dikin walk introduced in MV22. The paper signficantly improves upon the per iteration cost by applying the inverse maintenance techniques from LS15 and LLV20. The technical contribution is in showing how to apply these to the soft-threshold logbarrier."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I wasn't able to check the proofs, but the results suggest that the paper overcomes a technical difficulty prior works such as LS15, MV22, and LLV20 hadn't been able to address. Specifically, showing that the soft threshold logbarrier Hessian is slow-changing in a certain norm is a novel contribution of the paper, and it has consequences to improving the runtime of what is clearly an important problem.\n\n-------- \n\nAfter rebuttal: I'm increasing my score and confidence."
                },
                "weaknesses": {
                    "value": "I think the paper is, currently, not as self-contained as it should be. I believe this is easily fixable by adding relevant background material in the appendix. I also believe perhaps there may not be too much relevance of this paper in ICLR, and the paper would be much better appreciated in the theoretical CS community's conferences such as SODA, COLT, etc. Perhaps it would be helpful to state a more direct connection to ICLR."
                },
                "questions": {
                    "value": "-"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6187/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6187/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6187/Reviewer_6Phh"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6187/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699843332649,
            "cdate": 1699843332649,
            "tmdate": 1700663331719,
            "mdate": 1700663331719,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wBbWL6f5j3",
                "forum": "v63GWletn8",
                "replyto": "lA8JtCVkur",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6187/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6187/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable comments and suggestions.  We are glad you appreciate the novelty of our contribution and our improvements to the runtime of an important problem, and thank you for supporting our paper.  We answer your specific questions below.\n\n>I think the paper is, currently, not as self-contained as it should be. I believe this is easily fixable by adding relevant background material in the appendix.\n\nThank you for the suggestion.  We will add additional background material to the appendix.  Specifically:  (i) We will include a more detailed discussion of the soft-threshold Dikin walk of [MV22] and of the regularized barrier function used in that walk.  (ii) We will include a more detailed discussion of the efficient inverse maintanance algorithm of [LS15].  (iii) We will also include additional background material on the implementation of the original Dikin walk given in [LLV20] for the special case when the target distribution is uniform on a polytope.\n\n\n\n>...the paper would be much better appreciated in the theoretical CS community's conferences such as SODA, COLT, etc.\n\nWhile the techniques in our paper would also be of interest to the theoretical CS community, we believe that our results on sampling from log-Lipschitz or log-smooth logconcave distributions constrained to polytopes have interesting applications to machine learning, including to training Bayesian machine learning models and to differential privacy. We discuss our improvements to the runtime over [MV22] for some of these applications in Section 2 and Table 1.  \n\nWe will include a section in the final version discussing runtime improvements in additional applications, including applications to differentially private convex empirical risk minimization for training privacy-preserving logistic regression and support vector machines models."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700508073486,
                "cdate": 1700508073486,
                "tmdate": 1700508073486,
                "mdate": 1700508073486,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FLqjrXLu9k",
                "forum": "v63GWletn8",
                "replyto": "wBbWL6f5j3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6187/Reviewer_6Phh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6187/Reviewer_6Phh"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response!"
                    },
                    "comment": {
                        "value": "Thank you for your response and for contextualizing your work (particularly in your responses to other reviews). Based on your responses, I'm happy to increase my score and confidence. All the best!"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6187/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663284301,
                "cdate": 1700663284301,
                "tmdate": 1700663284301,
                "mdate": 1700663284301,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]