[
    {
        "title": "Iterative Search Attribution for Deep Neural Networks"
    },
    {
        "review": {
            "id": "Y50t5dpeP4",
            "forum": "qIn2IgMWYg",
            "replyto": "qIn2IgMWYg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5128/Reviewer_VN3Q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5128/Reviewer_VN3Q"
            ],
            "content": {
                "summary": {
                    "value": "To ensure the reliability of a DNN model and achieve the trustworthiness of deep learning, it is critical to enhance the interpretability and explainability of deep neural networks. Attribution methods are effective means of Explainable Artificial Intelligence (XAI) research. This paper, inspired by the iterative generation of Diffusion models, proposes an iterative search attribution (ISA) method by capturing the importance of samples during gradient ascent and descent and clipping the unimportant features in the model.  The method achieves SOTA performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-written, with good organization and expressions.\n2. The paper has the corresponding code released.\n3. The achieved experimental results are State-of-the-art.\n4. The justification of the proposed method, i.e., ISA, with both gradient descent and ascent considered is reasonable.\n5. The authors perform detailed ablation studies to validate the proposed methodology."
                },
                "weaknesses": {
                    "value": "1. How the paper is related to the diffusion models is unclear. Diffusion models apply iterative sampling whilst this paper is more on searching.\n2. Though the method achieved good experimental results regarding the performance, and the qualitative evaluation, its efficiency is not largely improved, due to the iterative nature of the algorithm.\n3. From Figure 3, it seems that the step size, learning rate, and scale parameters are quite sensible regarding the performance, any explanation for this?"
                },
                "questions": {
                    "value": "1.  Could discuss more on why iterative search is good.\n2. Figure 3 shows several parameters are sensitive, please provide explanations. \n3. The efficiency of the method is not improved a lot, which should be discussed, especially regarding the iterative method."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5128/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5128/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5128/Reviewer_VN3Q"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5128/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698549265403,
            "cdate": 1698549265403,
            "tmdate": 1699636505437,
            "mdate": 1699636505437,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ae1CCtnit8",
                "forum": "qIn2IgMWYg",
                "replyto": "Y50t5dpeP4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5128/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5128/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weaknesses"
                    },
                    "comment": {
                        "value": "**Weaknesses:**\n\n**1.** Our method satisfies the property of autoregression, and diffusion models also satisfy it. Both our method and diffusion models iterate from their own state to obtain the best performance, because one-time generation often has poor quality, as evidenced by the relationship between VAE and Diffusion models.\n\n**2.** We fully acknowledge the viewpoint that the use of iterative methods can reduce algorithm efficiency. \n\n* In Section 5.5, we conducted a detailed analysis of the operational efficiency between our method and the state-of-the-art AGI. It is noteworthy that our algorithm achieves significant performance gains even when operating at a comparable efficiency to AGI (with 480 iterations of gradient propagation compared to AGI's 400 iterations). \n\n* We posit that, despite the efficiency gap arising from a slightly higher number of gradient propagations, the performance breakthrough achieved by our algorithm is a noteworthy outcome. This is why we prefer to use diffusion models to generate high-quality samples. Although we know that diffusion models usually take longer than non-diffusion models, we are actually willing to bear these efficiency reductions to obtain higher performance.\n\n**3.** Please see the following discussion for the clarification. We will update the explanations and motivations for introducing each parameter in the ablation experiments. \n\n* In Section 5.6.1 we conducted ablation experiments on the steps $T_{1}$ and $T_{2}$ of gradient ascent and gradient descent. These two parameters affect how well gradient ascent and gradient descent explore the input space as we described in **Weekness 1**. The higher the $T_{1}$ and $T_{2}$ values represent the deeper the exploration of the input space. In this article, we do not use different $T_{1}$ and $T_{2}$ for additional ablation, because a higher degree of exploration often means higher attribution accuracy, but it will increase the algorithm running time. , which is consistent with the intuition. We conducted ablation experiments on gradient ascent alone, gradient descent alone, and a combination of gradient ascent and gradient descent. The experimental results show that using both gradient ascent and gradient descent for input space exploration can achieve the highest performance.\n\n* In Section 5.6.2, the parameter \u2018STEP SIZE\u2019 means the number of unimportant attribution values to be removed in each iteration. A larger \u2018STEP SIZE\u2019 means more attribution values are removed in each iteration. We found that when \u2018STEP SIZE\u2019 is 5000, the algorithm achieves the best results. We believe that when 'STEP SIZE' is too low, the model may not be able to fully capture the contribution of different attribution values to the model's decision-making behavior. When 'STEP SIZE' is too high, noise may be introduced, some of which may be information irrelevant to the model, leading to inaccuracy in model interpretation.\n\n* The \u2018learning rate\u2019 in Section 5.6.3 is the learning rate corresponding to gradient ascent and gradient descent. Similar to the parameters $T_{1}$ and $T_{2}$ in Section 5.6.1, the learning rate affects the exploration process of the input space by gradient ascent and gradient descent. For gradient ascent, on the one hand, too high a learning rate may lead to over-exploration of the input space, making the interpretation too unstable. On the other hand, a learning rate that is too low may lead to an overly conservative exploration of the input space and miss potentially useful information. For gradient descent, on the one hand, a too high learning rate may cause the interpretation results to be too sensitive. On the other hand, a learning rate that is too low may increase the computational cost of interpreting results and limit the interpretation results to certain features in the input space.\n\n* The parameter S in Section 5.6.4 corresponds to our scaling factor in Section 4.4. Obviously, this parameter will affect the accuracy of our attribution. If the value of S is too large (close to 2), the corresponding importance of the attribution values removed in adjacent iterations will be closer, that is, it will be over-interming. Similarly, if the value of S is too small, it means under-interming, the estimation needs to be more accurate, and the attribution value removed in each iteration is not as good as the attribution value removed in the next iteration."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5128/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700463885460,
                "cdate": 1700463885460,
                "tmdate": 1700463885460,
                "mdate": 1700463885460,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yR6fMtf3Uo",
                "forum": "qIn2IgMWYg",
                "replyto": "Y50t5dpeP4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5128/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5128/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Questions"
                    },
                    "comment": {
                        "value": "**Questions:**\n\n**1.** We appreciate the constructive suggestion provided by the reviewer. The forthcoming response aims to supplement our algorithm with a comprehensive theoretical analysis.\n\n* We can start with a discussion of \"local\" versus \"global\" interpretability approaches. The \"local\" and \"global\" properties of attribution methods are relative to the input space. A local attribution method interprets the neural network's decisions within a specific neighborhood of an input data point $x_{0}$. It focuses on understanding the model's behavior in the vicinity of a particular instance rather than considering the entire input space. In contrast, a global attribution method assesses the importance of features across the entire range of possible inputs, considering the full input space. It provides insights into how different features contribute to the model's decisions on a broader scale.\n\n* So for conventional gradient-based interpretability methods, such as saliency maps [1] or Integrated Gradients [2], are often considered local because they operate based on the gradients computed at a specific anchor point or input instance. AGI [3] integrates the gradients from adversarial examples to the target example along the curve of steepest ascent to calculate the resulting contributions from all input features. Therefore, AGI does not rely on the selection of specific anchor points or input instances. It can calculate attributions over the entire input space and synthesize these attributions to provide a more global explanation.\n\n* Compared with AGI, our algorithm does not only consider the situation when gradient rises. We believe that gradient descent is also an important approach to explore the input space of deep neural networks. Therefore, we use samples after both gradient ascent and gradient descent exploration as baseline points for attribution to provide a more global interpretability explanation. This is the main innovation of our algorithm in Section 4.1. In Section 4.2, we discuss in detail which attribution results are less important when exploring the input space. After considering gradient ascent and gradient descent, we believe that smaller attribution values account for lower importance (we will reply to this argument in detail in global comment). Therefore, we creatively use the iterative attribution method in Sections 4.3 and 4.4 to remove unimportant smaller attribution values in each iteration. We use $max(attr_\\gamma) < min(attr_{\\gamma+1 })$ to constrain this removal. Also, importantly, we apply a scaling factor to ensure that the best attribution values in the previous iteration will perform worse than the worst attribution values in the latter iteration. Since each time we remove $k$ attribution values, we will normalize the remaining attribution results to between 0 and 1, so after multiplying by the scaling factor, a portion of the attribution values of neighboring iterations can be intermingled.\n\n**2.** Please refer to **Weakness 3**.\n\n**3.** Please refer to **Weakness 2**.\n\nReferences:\n\n[1] Simonyan, K., Vedaldi, A., & Zisserman, A. (2014, April). Deep inside convolutional networks: visualising image classification models and saliency maps. In Proceedings of the International Conference on Learning Representations (ICLR). ICLR.\n\n[2] Sundararajan, M., Taly, A., & Yan, Q. (2017, July). Axiomatic attribution for deep networks. In International conference on machine learning (pp. 3319-3328). PMLR.\n\n[3] Pan, D., Li, X., & Zhu, D. (2021, August). Explaining deep neural network models with adversarial gradient integration. In Thirtieth International Joint Conference on Artificial Intelligence (IJCAI)."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5128/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700463932006,
                "cdate": 1700463932006,
                "tmdate": 1700463932006,
                "mdate": 1700463932006,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DHmGkCaRvJ",
            "forum": "qIn2IgMWYg",
            "replyto": "qIn2IgMWYg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5128/Reviewer_8Joh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5128/Reviewer_8Joh"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an Iterative Search Attribution (ISA) method to enhance the interpretability of deep neural networks (DNNs) by improving the accuracy of attribution. The authors introduce a scale parameter during the iterative process to ensure that the parameters in the next iteration are always more significant than the parameters in the current iteration. Experimental results demonstrate that the ISA method outperforms other state-of-the-art baselines in image recognition interpretability tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The key contributions are laid out lucidly, helping readers discern the paper's main takeaways.\nThe method clips relatively unimportant features, leading to more accurate attribution results."
                },
                "weaknesses": {
                    "value": "1. The comparison methods are not novel enough. There are some newer methodscan be compared(e.g. Explain Any Concept:\u00a0Segment Anything Meets Concept-Based Explanation).\n2. The experiments are relatively simple and do not adequately demonstrate the importance of the method in enhancing model interpretability. \u00a0\n3. The absence of corresponding theoretical analysis makes the method less convincing."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5128/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698743338852,
            "cdate": 1698743338852,
            "tmdate": 1699636505326,
            "mdate": 1699636505326,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JDcKb4IUA8",
                "forum": "qIn2IgMWYg",
                "replyto": "DHmGkCaRvJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5128/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5128/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weaknesses"
                    },
                    "comment": {
                        "value": "**Weaknesses:**\n\n**1.** We appreciate the valuable suggestion from the reviewer. Our comparative experiments include several state-of-the-art attribution-based interpretability methods widely recognized in current computer vision tasks, such as IG [1], FIG [2], AGI [3], BIG [4], GIG [5]. We will incorporate an additional discussion on 'Segment Anything Meets Concept-Based Explanation' to enrich the argumentation of our experiments.\n\n**2.** We thank the reviewer for the suggestion. Our experiments meticulously adhere to the experimental setups of the current **SOTA methods AGI and BIG**, both of which employ Insertion score and Deletion score as quantitative metrics for evaluating interpretability. Our method has demonstrated performance far surpassing the current SOTA baselines. Moreover, as evidenced in Section 5.5, despite the utilization of iterative attribution, the efficiency of our algorithm is only slightly lower than that of AGI. Similar to diffusion models, our approach leverages the autoregressive nature to achieve outstanding performance while maintaining favorable efficiency.\n\n**3.** We appreciate the constructive suggestion provided by the reviewer. The forthcoming response aims to supplement our algorithm with a comprehensive theoretical analysis.\n\n* We can start with a discussion of \"local\" versus \"global\" interpretability approaches. The \"local\" and \"global\" properties of attribution methods are relative to the input space. A local attribution method interprets the neural network's decisions within a specific neighborhood of an input data point $x_{0}$. It focuses on understanding the model's behavior in the vicinity of a particular instance rather than considering the entire input space. In contrast, a global attribution method assesses the importance of features across the entire range of possible inputs, considering the full input space. It provides insights into how different features contribute to the model's decisions on a broader scale.\n\n* So for conventional gradient-based interpretability methods, such as saliency maps [6] or Integrated Gradients [1], are often considered local because they operate based on the gradients computed at a specific anchor point or input instance. AGI [3] integrates the gradients from adversarial examples to the target example along the curve of steepest ascent to calculate the resulting contributions from all input features. Therefore, AGI does not rely on the selection of specific anchor points or input instances. It can calculate attributions over the entire input space and synthesize these attributions to provide a more global explanation.\n\n* For our algorithm, it does not only consider the situation when gradient rises. We believe that gradient descent is also an important approach to explore the input space of deep neural networks. Therefore, we use samples after both gradient ascent and gradient descent exploration as baseline points for attribution to provide a more global interpretability explanation. This is the main innovation of our algorithm in Section 4.1. In Section 4.2, we discuss in detail which attribution results are less important when exploring the input space. After considering gradient ascent and gradient descent, we believe that smaller attribution values account for lower importance (we will reply to this argument in detail in global comment). Therefore, we creatively use the iterative attribution method in Sections 4.3 and 4.4 to remove unimportant smaller attribution values in each iteration. We use $max(attr_\\gamma) < min(attr_{\\gamma+1 })$ to constrain this removal. Also, importantly, we apply a scaling factor to ensure that the best attribution values in the previous iteration will perform worse than the worst attribution values in the latter iteration. Since each time we remove $k$ attribution values, we will normalize the remaining attribution results to between 0 and 1, so after multiplying by the scaling factor, a portion of the attribution values of neighboring iterations can be intermingled."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5128/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700463777061,
                "cdate": 1700463777061,
                "tmdate": 1700463777061,
                "mdate": 1700463777061,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fseTnYqCqm",
                "forum": "qIn2IgMWYg",
                "replyto": "DHmGkCaRvJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5128/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5128/Authors"
                ],
                "content": {
                    "title": {
                        "value": "(Continued) Response to Weakness"
                    },
                    "comment": {
                        "value": "References:\n\n[1] Sundararajan, M., Taly, A., & Yan, Q. (2017, July). Axiomatic attribution for deep networks. In International conference on machine learning (pp. 3319-3328). PMLR.\n\n[2] Hesse, R., Schaub-Meyer, S., & Roth, S. (2021). Fast axiomatic attribution for neural networks. Advances in Neural Information Processing Systems, 34, 19513-19524.\n\n[3] Pan, D., Li, X., & Zhu, D. (2021, August). Explaining deep neural network models with adversarial gradient integration. In Thirtieth International Joint Conference on Artificial Intelligence (IJCAI).\n\n[4] Wang, Z., Fredrikson, M., & Datta, A. (2022, June). Robust Models Are More Interpretable Because Attributions Look Normal. In International Conference on Machine Learning (pp. 22625-22651). PMLR.\n\n[5] Kapishnikov, A., Venugopalan, S., Avci, B., Wedin, B., Terry, M., & Bolukbasi, T. (2021). Guided integrated gradients: An adaptive path method for removing noise. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 5050-5058).\n\n[6] Simonyan, K., Vedaldi, A., & Zisserman, A. (2014, April). Deep inside convolutional networks: visualising image classification models and saliency maps. In Proceedings of the International Conference on Learning Representations (ICLR). ICLR."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5128/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700463797010,
                "cdate": 1700463797010,
                "tmdate": 1700463797010,
                "mdate": 1700463797010,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "U0KG1IC8t2",
                "forum": "qIn2IgMWYg",
                "replyto": "fseTnYqCqm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5128/Reviewer_8Joh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5128/Reviewer_8Joh"
                ],
                "content": {
                    "comment": {
                        "value": "I have carefully reviewed all the comments from the reviewers and the authors' responses. I appreciate this work. However, I will not adjust the score until the authors have genuinely implemented 'We will' and updated the revision."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5128/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547698280,
                "cdate": 1700547698280,
                "tmdate": 1700547698280,
                "mdate": 1700547698280,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uvzZw4HSuE",
            "forum": "qIn2IgMWYg",
            "replyto": "qIn2IgMWYg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5128/Reviewer_df7s"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5128/Reviewer_df7s"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new method for attributing a neural network prediction to input features. The proposed method integrates the gradient with respect to input features over both the gradient ascent and descent paths. The paper provides experiments on ImageNet where it shows better performance compared to prior methods under Insertion score."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper tackles an interesting problem, namely providing better saliency maps. It proposes interesting and novel modifications to existing gradient-based attribution methods, and seems to make some improvement upon existing methods."
                },
                "weaknesses": {
                    "value": "1- The paper makes many ad-hoc design choices which are not well justified. The provided explanations seem unclear to me (Section 4.2). It is helpful to provide experiments that show how each design choice affects the attribution, as well as more formal explanations (using clear mathematical notation).\n\n2- While the ablation studies show which hyper-parameters were most useful, they lack explanation of why and clear connection to the motivation for introducing the respective parameters.\n\n3- The metrics, Insertion and Deletion, are not formally defined. Since these two metrics are the basis of the main results in Table 1, it is important to clarify their definition and justify their meaningfulness. It is particularly useful to show what is the real-world consequences of lower Insertion or Deletion.\n\n4- The paper mentions that Grad-CAM and Score-CAM perform poorly in non-CNN models, however, all the results in the paper are reported with CNN based models, so a comparison to these methods are required.\n\n5- The paper can improve in writing (also Figure 1 is not clear to me). Please make sure claims are either backed by specific citations or experiments, for example: \u201cUnfortunately, these two methods are more suitable for CNNs and perform poorly in non-CNN cases\u201d.\n\n6- minor typo: Section 4.2 must be \\Delta x_k not x_t"
                },
                "questions": {
                    "value": "My suggestions are included in the issues I raised in the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5128/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698818894379,
            "cdate": 1698818894379,
            "tmdate": 1699636505235,
            "mdate": 1699636505235,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rqnQbBnAHW",
                "forum": "qIn2IgMWYg",
                "replyto": "uvzZw4HSuE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5128/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5128/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weaknesses"
                    },
                    "comment": {
                        "value": "**Weaknesses:**\n\n**1.** To address the concern, we herein provide a comprehensive clarification regarding the design of our algorithm.\n\n* We start with a discussion of \"local\" versus \"global\" interpretability approaches. The \"local\" and \"global\" properties of attribution methods are relative to the input space. A local attribution method interprets the neural network's decisions within a specific neighborhood of an input data point $x_{0}$. It focuses on understanding the model's behavior in the vicinity of a particular instance rather than considering the entire input space. In contrast, a global attribution method assesses the importance of features across the entire range of possible inputs, considering the full input space. It provides insights into how different features contribute to the model's decisions on a broader scale.\n\n* For conventional gradient-based interpretability methods, such as saliency maps [1] or Integrated Gradients [2], are often considered local because they operate based on the gradients computed at a specific anchor point or input instance. AGI [3] integrates the gradients from adversarial examples to the target example along the curve of steepest ascent to calculate the resulting contributions from all input features. Therefore, AGI does not rely on the selection of specific anchor points or input instances. It can calculate attributions over the entire input space and synthesize these attributions to provide a more global explanation.\n* For our algorithm, it does not only consider the situation when gradient rises. We believe that gradient descent is also an important approach to explore the input space of deep neural networks. Therefore, we use samples after both gradient ascent and gradient descent exploration as baseline points for attribution to provide a more global interpretability explanation. \n\n* This is the main innovation of our algorithm in Section 4.1. In Section 4.2, we discuss in detail which attribution results are less important when exploring the input space. After considering gradient ascent and gradient descent, we believe that smaller attribution values account for lower importance (we will reply to this argument in detail in global comment). Therefore, we creatively use the iterative attribution method in Sections 4.3 and 4.4 to remove unimportant smaller attribution values in each iteration. We use $max(attr_\\gamma) < min(attr_{\\gamma+1 })$ to constrain this removal. Also, importantly, we apply a scaling factor to ensure that the best attribution values in the previous iteration will perform worse than the worst attribution values in the latter iteration. Since each time we remove $k$ attribution values, we will normalize the remaining attribution results to between 0 and 1, so after multiplying by the scaling factor, a portion of the attribution values of neighboring iterations can be intermingled."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5128/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700463629866,
                "cdate": 1700463629866,
                "tmdate": 1700463629866,
                "mdate": 1700463629866,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pXoQYKGXKd",
                "forum": "qIn2IgMWYg",
                "replyto": "uvzZw4HSuE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5128/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5128/Authors"
                ],
                "content": {
                    "title": {
                        "value": "(Continued) Response to Weakness"
                    },
                    "comment": {
                        "value": "**2.** Thank you for the constructive suggestions! We will add relevant explanations and motivations for introducing each parameter in the ablation experiments. Please first allow me to provide some clarification here. \n\n* In Section 5.6.1 we conducted ablation experiments on the steps $T_{1}$ and $T_{2}$ of gradient ascent and gradient descent. These two parameters affect how well gradient ascent and gradient descent explore the input space as we described in **Weekness 1**. The higher the $T_{1}$ and $T_{2}$ values represent the deeper the exploration of the input space. In this article, we do not use different $T_{1}$ and $T_{2}$ for additional ablation, because a higher degree of exploration often means higher attribution accuracy, but it will increase the algorithm running time. , which is consistent with the intuition. We conducted ablation experiments on gradient ascent alone, gradient descent alone, and a combination of gradient ascent and gradient descent. The experimental results show that using both gradient ascent and gradient descent for input space exploration can achieve the highest performance.\n\n* In Section 5.6.2, the parameter \u2018STEP SIZE\u2019 means the number of unimportant attribution values to be removed in each iteration. A larger \u2018STEP SIZE\u2019 means more attribution values are removed in each iteration. We found that when \u2018STEP SIZE\u2019 is 5000, the algorithm achieves the best results. We believe that when 'STEP SIZE' is too low, the model may not be able to fully capture the contribution of different attribution values to the model's decision-making behavior. When 'STEP SIZE' is too high, noise may be introduced, some of which may be information irrelevant to the model, leading to inaccuracy in model interpretation.\n* The \u2018learning rate\u2019 in Section 5.6.3 is the learning rate corresponding to gradient ascent and gradient descent. Similar to the parameters $T_{1}$ and $T_{2}$ in Section 5.6.1, the learning rate affects the exploration process of the input space by gradient ascent and gradient descent. For gradient ascent, on the one hand, too high a learning rate may lead to over-exploration of the input space, making the interpretation too unstable. On the other hand, a learning rate that is too low may lead to an overly conservative exploration of the input space and miss potentially useful information. For gradient descent, on the one hand, a too high learning rate may cause the interpretation results to be too sensitive. On the other hand, a learning rate that is too low may increase the computational cost of interpreting results and limit the interpretation results to certain features in the input space.\n\n* The parameter $S$ in Section 5.6.4 corresponds to our scaling factor in Section 4.4. Obviously, this parameter will affect the accuracy of our attribution. If the value of S is too large (close to 2), the corresponding importance of the attribution values removed in adjacent iterations will be closer, that is, it will be over-interming. Similarly, if the value of S is too small, it means under-interming, the estimation needs to be more accurate, and the attribution value removed in each iteration is not as good as the attribution value removed in the next iteration."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5128/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700463654665,
                "cdate": 1700463654665,
                "tmdate": 1700463654665,
                "mdate": 1700463654665,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Q0dKy7bQyv",
                "forum": "qIn2IgMWYg",
                "replyto": "uvzZw4HSuE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5128/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5128/Authors"
                ],
                "content": {
                    "title": {
                        "value": "(Continued) Response to Weakness"
                    },
                    "comment": {
                        "value": "**3.** We are grateful to the reviewer's suggestion. Here we provide specific definitions of Insertion Score and Deletion Score. Starting from a blank image, the insertion game successively inserts pixels from highest to lowest attribution scores and makes predictions. If we draw a curve that represents the prediction values, the area under the curve (AUC) is then defined as the insertion score. higher the insertion score, the better the quality of interpretation. Similarly, starting from the original image, the deletion score is obtained by successively deleting the pixels from the highest to lowest attribution scores. The lower the deletion score, the better the quality of interpretation. More detailed references can be found in [3],[4].\n\n**4.** For the point of view \"Unfortunately, these two methods are more suitable for CNNs and perform poorly in non-CNN cases\", we would like to refer to the **AGI paper [3]**, in which the third paragraph of the Introduction Section: 'Within gradient models, although CAM based methods give promising results in various applications, a major limitation is that it applies only to Convolutional Neural Network (CNN) architectures'. \n\n\n**5.** Thank you for the suggestion. We will update the writing of the paper and reconstruct Figure 1 to provide readers with a clearer understanding of our article. We will review the article and add appropriate reference citations. For the point of view \"Unfortunately, these two methods are more suitable for CNNs and perform poorly in non-CNN cases\", we would like to refer to the third paragraph of the Introduction Section in **AGI paper [3]**: 'Within gradient models, although CAM based methods give promising results in various applications, a major limitation is that it applies only to Convolutional Neural Network (CNN) architectures' .\nWe express our gratitude for the time and efforts from the reviewers. We pledge to thoroughly review the article, rectify any identified issues, and take measures to prevent the recurrence of such errors.\n\nReferences:\n\n[1] Simonyan, K., Vedaldi, A., & Zisserman, A. (2014, April). Deep inside convolutional networks: visualising image classification models and saliency maps. In Proceedings of the International Conference on Learning Representations (ICLR). ICLR.\n\n[2] Sundararajan, M., Taly, A., & Yan, Q. (2017, July). Axiomatic attribution for deep networks. In International conference on machine learning (pp. 3319-3328). PMLR.\n\n[3] Pan, D., Li, X., & Zhu, D. (2021, August). Explaining deep neural network models with adversarial gradient integration. In Thirtieth International Joint Conference on Artificial Intelligence (IJCAI).\n\n[4] Petsiuk, V., Das, A., & Saenko, K. (2018). Rise: Randomized input sampling for explanation of black-box models. arXiv preprint arXiv:1806.07421."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5128/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700463700602,
                "cdate": 1700463700602,
                "tmdate": 1700463700602,
                "mdate": 1700463700602,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s3B5CoUtCm",
                "forum": "qIn2IgMWYg",
                "replyto": "Q0dKy7bQyv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5128/Reviewer_df7s"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5128/Reviewer_df7s"
                ],
                "content": {
                    "title": {
                        "value": "Remaining concerns"
                    },
                    "comment": {
                        "value": "Thank you for your response. My 4th concern regarding comparing to Grad-CAM remains (since all the reported results are based on CNNs). I am also still not clear about the motivation, and Sections 4.2 remains hard to understand for me. Specially, I do not understand the logic in this argument in the global response: \"For gradient descent, parameters with larger attribution values are unimportant. The reason is that, if we change these values, the gradient can still decrease and the model performance becomes better.\""
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5128/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640414244,
                "cdate": 1700640414244,
                "tmdate": 1700640414244,
                "mdate": 1700640414244,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CFYK609pbJ",
            "forum": "qIn2IgMWYg",
            "replyto": "qIn2IgMWYg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5128/Reviewer_PAUU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5128/Reviewer_PAUU"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a gradient-based iterative attribution method, the Iterative Search Attribution (ISA), which combines gradient descent and gradient ascent to construct the integration path. Experiments demonstrate the effectiveness of the proposed ISA."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- the idea of combining both gradient descent (GD) and gradient ascent (GA) in constructing the integration path in the gradient-based attribution method is interesting (and, based on their results, useful).\n- the qualitative results in Figure 2 look promising."
                },
                "weaknesses": {
                    "value": "- the discussion about the local and global attribution methods is vague. \n    - are \"local\" and \"global\" properties of the attribution method concerning the input space? namely, a local attribution method only interprets the NN within a neighborhood of an input $x_0$, while a global attribution method will assess the importance of features in $x_0$ with respect to the whole range of possible input (full input space), thus reflecting the property of the NN itself. \n    - If so, one may argue that the gradient-based methods are also local as they depend on specific anchor points.\n- the discussion about the relative importance of the features with \"larger attribution\" in GA and GD cases in Sec.4.2 is totally confusing and I fail to see connections to the algorithm\n- the algorithm is not clearly described and has minor typos, e.g.,\n    - $A_a$ is not introduced and initialized \n    - lines 4 and 8: $x_t = x_t \\dots$ should be $x_{t+1}=x_t \\dots$\n    - line 14: what does symbol $min_k$ mean? taking the lowest $k$ features from from $Attr_\\gamma$ (then $attr_\\gamma$ is a $k$-dim vector)? \n- the constraints in Sec. 4.3 seem unnecessary and the theoretical reason for choosing them is not stated.  In addition, the introduced hyper-parameter $S$ could break the previous constraint, i.e., $max(attr_\\gamma) < min(attr_{\\gamma+1})$.\n- the author should pay more effort to justify their claim that \"the Insertion score serves as a more representative indicator of the performance of attribution algorithms.\" (at least should be more than just one paragraph!)\n    - especially, it is interesting to note that the ISA always has the worst deletion score in Table 1"
                },
                "questions": {
                    "value": "- could you provide more concrete descriptions (and comparisons) about the local and global attribution methods?\n- could rephrase your arguments in Sec. 4.2 to make it clear to understand?\n- could you theoretically justify the proposed constraints stated in Sec. 4.3? Also, what if you do not add $\\gamma$ to $attr_\\gamma$?\n- could you provide in-depth (better to be quantitative) arguments about the claim that \"the Insertion score serves as a more representative indicator of the performance of attribution algorithms.\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5128/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699111206028,
            "cdate": 1699111206028,
            "tmdate": 1699636505146,
            "mdate": 1699636505146,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "G84pSXWEzm",
                "forum": "qIn2IgMWYg",
                "replyto": "CFYK609pbJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5128/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5128/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Weaknesses"
                    },
                    "comment": {
                        "value": "**Weaknesses:**\n\n1. For global attribution methods, it depends on the implementation details. \n\n* Local attribution method interprets the neural network's decisions within a specific neighborhood of an input data point $x_{0}$\n\n* Global attribution method assesses the importance of features across the entire range of possible inputs, considering the full input space. For conventional gradient-based interpretability methods, such as saliency maps [1] or Integrated Gradients [2], are often considered local because they operate based on the gradients computed at a specific anchor point or input instance.\n\n* However, AGI [3] integrates the gradients from adversarial examples to the target example along the curve of steepest ascent to calculate the resulting contributions from all input features. Therefore, AGI does not rely on the selection of specific anchor points or input instances. It can calculate attributions over the entire input space and synthesize these attributions to provide a more global explanation.\n\n* For our method, we further extend the capability of AGI as a global attribution method by considering gradient descent to the input space of deep neural networks. Therefore, we use samples after both gradient ascent and gradient descent exploration as baseline points for attribution to provide a more global interpretability explanation. This is the main innovation of our algorithm in Section 4.1. In Section 4.2, we discuss in detail which attribution results are less important when exploring the input space. After considering gradient ascent and gradient descent, we believe that smaller attribution values account for lower importance (we will reply to this argument in detail in **Global comment**). Therefore, we creatively use the iterative attribution method in Sections 4.3 and 4.4 to remove unimportant smaller attribution values in each iteration. We use $max(attr_\\gamma) < min(attr_{\\gamma+1 })$ to constrain this removal. Also, importantly, we apply a scaling factor to ensure that the best attribution values in the previous iteration will perform worse than the worst attribution values in the latter iteration. Since each time we remove $k$ attribution values, we will normalize the remaining attribution results to between 0 and 1, so after multiplying by the scaling factor, a portion of the attribution values of neighboring iterations can be intermingled.\n\n2. Thanks for bringing this into our attention. We have provided the updated details in the **Global comment**. The new version will incorporate these details. \n\n3. We promise to adjust the pseudocode to include initialization $A_{a}$ and correct errors on lines 4 and 8. Here, $min_{k}$ in line 14 means that the lowest $k$ features are taken.\n\n4. That is exactly the design of our algorithm. Both constraints in Sections 4.3 and 4.4 are interrelated. The constraint in Section 4.3 is a strict constraint, while the constraint in Section 4.4 is an approximate one. The premise of establishing such a strict constraint in Section 4.3 is that the features that are removed first are less important features. But in fact, such a constraint may be too hard to the model, so we introduce the hyperparameter $S$ to break such strict constraints and ensure that the best attribution values in the previous iteration will perform worse than the worst attribution values in the latter iteration .\n\n5. We agree with the reviewer on this point. However, it doesn\u2019t affect the conclusion of this work. The reasons are from two folds:\n \n      * The difference between our method and other baselines on Inception-v3, ResNet50 and VGG16 are extremely small for the deletion score. \n\n      * We would like to highlight that the insertion score is a more important metric compared to the deletion score. Since the insertion score measures the degree of change in the output of the model when pixels are inserted into the input, it is the accumulation of accuracy from scratch. The deletion score represents the accumulation of accuracy of the remaining parts when features are deleted. Thus, the accuracy calculated by the deletion score is based on the feature deletion process. At this time, the undeleted features will interfere with the accuracy and affect the effect of the interpretability evaluation. \n      * To sum up, we believe that \u2018the Insertion score serves as a more representative indicator of the performance of attribution algorithms\u2019.\n\n We will incorporate these discussions in Section 5.2."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5128/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700463433581,
                "cdate": 1700463433581,
                "tmdate": 1700463433581,
                "mdate": 1700463433581,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sfHYS6gJXu",
                "forum": "qIn2IgMWYg",
                "replyto": "CFYK609pbJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5128/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5128/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Questions"
                    },
                    "comment": {
                        "value": "**Questions:**\n\n1. Addressed in **Weakness 1**.\n\n2. As discussed before in **Weakness** and **Global Comment**, we will add the updated discussion in Section 4.2 to provide readers with a clearer understanding of our main contributions.\n\n3. As discussed in **Weakness 1**, we need to ensure that the $k$ attribution values removed in the previous iteration are always less important (i.e. smaller) than the attribution values removed in the later iteration. We use $max(attr_\\gamma) < min(attr_{\\gamma+1 })$ to limit this deletion. Since each time we remove $k$ attribution values, we will normalize the remaining attribution results to be between 0 and 1, so in order to mix a part of the attribution values from adjacent iterations together, ensuring the best attribution values in the previous iteration will perform worse than the worst attribution values in the latter iteration, we apply a scaling factor.\n\nHere, $\\gamma$ refers to the iteration of $\\gamma$. If $\\gamma$ is not added to $attr_{\\gamma}$, it will not be an iterative attribution.\n\n4. Addressed in **Weakness 5**.\n\nReferences:\n\n[1] Simonyan, K., Vedaldi, A., & Zisserman, A. (2014, April). Deep inside convolutional networks: visualising image classification models and saliency maps. In Proceedings of the International Conference on Learning Representations (ICLR). ICLR.\n\n[2] Sundararajan, M., Taly, A., & Yan, Q. (2017, July). Axiomatic attribution for deep networks. In International conference on machine learning (pp. 3319-3328). PMLR.\n\n[3] Pan, D., Li, X., & Zhu, D. (2021, August). Explaining deep neural network models with adversarial gradient integration. In Thirtieth International Joint Conference on Artificial Intelligence (IJCAI)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5128/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700463498650,
                "cdate": 1700463498650,
                "tmdate": 1700463498650,
                "mdate": 1700463498650,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]