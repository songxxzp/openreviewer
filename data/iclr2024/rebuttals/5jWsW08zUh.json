[
    {
        "title": "Some Intriguing Aspects about Lipschitz Continuity of Neural Networks"
    },
    {
        "review": {
            "id": "Mhgwo6oUIE",
            "forum": "5jWsW08zUh",
            "replyto": "5jWsW08zUh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7155/Reviewer_Sk7T"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7155/Reviewer_Sk7T"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents some empirical studies on the Lipschitz constant of neural networks. The studies reveal three major results:\n1. the evolution of Lipschitz constant during training; 2. the double descent of Lipschitz constant of neural networks; 3. The Lipchitz contant variation with respect to random labels. The study also shows the fidelity of lower Lipschitz bound."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The perspective and results are interesting: the Lipschitz constant of a function is an intrinsic property of neural networks, and it has connections with many learning theoretical properties. For example, the double descent of phenomenon of neural networks is known and understanding this phenomenon from the Lipschitz constant evolution can be an interesting persepctive."
                },
                "weaknesses": {
                    "value": "1. Overall this paper lacks coherence. Though this paper studies the Lipschitz constant of neural networks, each question studied in the paper is quite independent. Also these questions are important and each deserves an in-depth study. The empirical result is interesting yet insufficient to understand the phenomenon per se.\n\n2. The paper is also not rigorous. For example, in the intro, the paper stated that \"to put it more accurately, it is the maximum absolute change in the function per unit norm change in the input\". This statement is only true when the function is scaling invariant. Also because of each question is not studied thourouly, many of the aspects are not rigorously studied. for example, the paper studies effective Lipschitz constant but in reality distributional shift may occur and how the effective Lipschitz constant may change is not known. The scope of this paper is too big and each of the questions requires an in-depth analysis.\n\n3. The paper is also not well-organized. This comes from that the paper lacks coherence. Even though each point is explained, it is still unclear about the message of this paper."
                },
                "questions": {
                    "value": "The layerwise upper bound is known a loose measurement, and indeed there is a giant gap between the upper and lower bounds in the experiments. Is it possible to strengthen the measurement to have a more precise characterization of the Lipschitz constant?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7155/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7155/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7155/Reviewer_Sk7T"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7155/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698222374333,
            "cdate": 1698222374333,
            "tmdate": 1699636847418,
            "mdate": 1699636847418,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2d4UBqVWvh",
                "forum": "5jWsW08zUh",
                "replyto": "Mhgwo6oUIE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7155/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7155/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your critical review of our paper. We were glad to see that you have found that our offered **\u201cperspective and results are interesting\u201d**. In the response below we will go through your questions and other concerns, and, hopefully, alleviate all of them.\n\n&nbsp;\n\n## Coherence\n- On first glance, these questions might seem independent, but to us, having battled through these questions, we think that **the presented questions are the three most important and intriguing aspects that come naturally** in the context of Lipschitz continuity in neural networks. \n- We agree that each of them could deserve their own in-depth discussion, but it should be emphasized that **none of these aspects can be completely divorced from each other**. For instance, it is hard to measure generalization or optimization aspects of Lipschitz, without first affirming the fidelity of its bounds (as we have done here with the lower Lipschitz). Neither can one get a complete picture of the effect of Lipschitz on generalization, without understanding the role of the noise contained in the data distribution. \n- Thus, our work strived to put together these various interdependent aspects of the Lipschitz together, and more generally cover the breadth of its effects. We agree, in no uncertain terms, that studying the finer intricacies of each individual question is crucial --- but one cannot paint the finer strokes unless one first utilizes the broad strokes and brushes to obtain a proper sight and outline of the concerned subject. And we believe, while not all the finer strokes are in there yet, we have done our best to comprehensively outline the subject of Lipschitz continuity in neural networks.\n\n\n&nbsp;\n\n## paper is also not rigorous\nWe do not quite agree with this stance. Please allow us to make our case:\n\n1. Firstly, in the introduction, we just wanted to appeal to a broader audience and attempted to introduce the concept of Lipschitz continuity in simple terms, without encumbering the reader. But we are happy to add the mentioned caveat. \n\n2. Other reviewers also hold a similar stance as us. In particular, we would to note the mentions by: \n- Reviewer Jyw9: \u201cconducted extensive experiments to showcase its findings and offers a comprehensive exploration of experimental details and discussions\u201d.\n- Reviewer EeTb: \u201cExtensive experiment to show the different aspects in the discussion\u201d.\n- Reviewer F3fH:  \u201c it covers the topic pretty well\u201d, \u201cA lot of experiments in various regime with many different real life instances are quite convincing\u201d.\n\n3. Also, for instance, your very interesting suggestion on Distributional shift has already been studied in **Section S3.2 and S3.3** of the Appendix (in the form of  Adversarial examples and Out-Of-Distribution noise sampling experiments).\n\nWhile the presence of all the supplementary results and experiments in the Appendix might have led to your inference, we sincerely believe that our work is one of the most comprehensive studies on this topic. We will be more than happy to further streamline the organization of the supplementary results, so that this point gets clarified to the reader.\n\n&nbsp;\n\n## Method's precision\nWe have made several attempts to tackle this, by trying to compute local Lipschitz in several scenarios in a bid to increase its value (such as with convex combinations, or additive Gaussian noise, and even adversarial perturbtations). But these interventions still showed the fidelity of the original lower Lipschitz formulation. In general, this is an important open question.\n\n\n&nbsp;\n\n\n*We hope that the response above comprehensively addresses your concerns. However, if something remains unclear or if there are any further questions, we will gladly answer them. Lastly, in light of this response, we also earnestly hope that you will reconsider your review score.*"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7155/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580131772,
                "cdate": 1700580131772,
                "tmdate": 1700580131772,
                "mdate": 1700580131772,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3QNSCcoFbQ",
                "forum": "5jWsW08zUh",
                "replyto": "Mhgwo6oUIE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7155/Reviewer_Sk7T"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7155/Reviewer_Sk7T"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nI appreciate your response. Could you list your research questions (i.e., what your empirical evaluations try to address), what are your empirical findings, and how they address these questions?"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7155/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700611268837,
                "cdate": 1700611268837,
                "tmdate": 1700611370625,
                "mdate": 1700611370625,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pPCSnzkSkx",
                "forum": "5jWsW08zUh",
                "replyto": "Mhgwo6oUIE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7155/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7155/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply: Part 1"
                    },
                    "comment": {
                        "value": "Thanks for your prompt reply. Sure, below we list the key research questions which we tackle in our paper, what are our empirical findings, and how the corresponding empirical findings are a step forward in addressing them. \n\nBut, before we delve into that, to put it a single line, *the unifying theme across all these questions is **understanding how the interaction between over-parameterization and the Lipschitz continuity of the function pan out**.*\n\n&nbsp;\n\n----\n\n&nbsp;\n\n**Question 1.** One of our primary questions was *how to measure the Lipschitz constant to good fidelity while being efficient for modern over-parameterized networks* on large-scale datasets. \n\nWe also wanted such a measure to be valid throughout training. (This made us wonder, as a sub-question, about the dynamics of the Lipschitz continuity of neural networks: what is it like at initialization and how much does it develop during training?)\n\n\n  - **Empirical evaluation and findings:** Our experiments in Sections 3.1 and 3.2 probed how well the lower Lipschitz bound, evaluated on a finite set of samples, would form a good proxy of the effective Lipschitz constant. This was motivated by the fact that, theoretically, it would approach the effective Lipschitz constant in the limit of the number of samples drawn from the input distribution. But, remarkably, we observed its reliability to serve as a measure of the effective Lipschitz, since it changed by only a relatively feeble amount in the presence of varying inputs by considering the convex combinations of data points or with additive Gaussian noise.\n\n\n  - **Implications:**\n    - Consequently, this suggested *a practical and reliable way to gain insights into the Lipschitz behaviour* of neural networks, that was valid throughout training.  \n    - But more than that, while there might be some measure-zero (or simply, \u2018wild\u2019)  sets where the function behaviour could be arbitrarily complex and the Lipschitz hard to ascertain, *effectively on the distribution the Lipschitz constant is determined by \u2018tricky\u2019 points lying near the decision boundary*, as shown in Section 3.3 \n\n&nbsp;\n\n\n**Question 2.** The natural question, which we could pursue once the above question had been reasonably settled, was *if the Lipschitz constant was a suitable indicator of generalization for modern over-parameterized networks*. \n\nThis was motivated by the fact that, in several learning theory papers, one can often notice the occurrence of the Lipschitz constant in the generalization bounds. However, the relevance of these bounds had been put into question by the double descent phenomenon, especially since the nature of the Lipschitz constant was largely obscure previously.\n\n\n  - **Empirical evaluation and findings:** In Section 4, we noticed that both the lower and upper Lipschitz showed a *double descent trend that aligned tightly with the double descent behaviour in the test loss* (see Figure 7) in their non-monotonicity.\n\n  - **Implications:**\n     - Firstly, this *reaffirmed that the Lipschitz behaviour is still relevant to the generalization* abilities of neural networks (as well as reaffirming the traditional learning-theoretic works).  \n     - Besides, the observation of double descent at the level of the Lipschitz constant had wider implications than the usual double descent in the test loss since Lipschitz continuity also inherently characterizes the robustness of the network predictions to input perturbations. Hence, this would suggest that *over-parameterized networks would also fare better in the presence of noise (adversarial or o.o.d.) than networks with fewer parameters*, which we also noticed in Sections S3.2 & S3.3.\n\n&nbsp;\n\n*(continued below)*"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7155/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700652020139,
                "cdate": 1700652020139,
                "tmdate": 1700658008153,
                "mdate": 1700658008153,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "n9JEcHbV5U",
            "forum": "5jWsW08zUh",
            "replyto": "5jWsW08zUh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7155/Reviewer_EeTb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7155/Reviewer_EeTb"
            ],
            "content": {
                "summary": {
                    "value": "The paper discusses Lipschitz continuity of neural networks in the following aspects. Firstly, they show the fidelity of the local Lipschitz-based lower bound, by providing several experiments on different models and datasets (section 3.2, also in appendix), and provide their intuition based on a toy example (section 3.3). With that, they investigate the trend of the Lipschitz constant, from initialization and during the course of training. Secondly, they discuss the (implicit) Lipschitz regularisation and double descent behavior, mainly in the context of over parameterization setup of the deep models and a bias variance trade off argument is provided. Thirdly, they discuss about the Lipschitz constant in the presence of the label noise, with the main focus of the network capacities vs the noise strengths. A hypothesis is provided: \u201cwhile the network is able to fit the noise, Lipschitz constant should increase\u201d, beyond which the network will reach a memorization threshold, and collapse to a smoother function with a smaller Lipschitz constant. In all the 3 aspects discussed, empirical evidence is provided based on several models and datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Extensive experiment to show the different aspects in the discussion, as well as to provide evidence for their intuition and hypothesis."
                },
                "weaknesses": {
                    "value": "Less theoretical explanation of the different aspects discussed."
                },
                "questions": {
                    "value": "1. Is it possible to provide theoretical explanation of the different aspects found?\n2. Is it possible to extend some of the experiment in any of the language models?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7155/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7155/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7155/Reviewer_EeTb"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7155/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698498935293,
            "cdate": 1698498935293,
            "tmdate": 1699636847297,
            "mdate": 1699636847297,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7ZAxtXriOc",
                "forum": "5jWsW08zUh",
                "replyto": "n9JEcHbV5U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7155/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7155/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your review and for acknowledging the **extensiveness of our experiments**. We will address your concerns in the response below.\n\n\n&nbsp;\n\n\n## Theoretical explanations:\n- The focus of our work, as we have explicitly stated, is primarily experimental, i.e. an elaborate investigation of the behaviour of the Lipschitz constant in a vast set of scenarios covering various optimization, datasets, and architectural choices. In this regard, we should also bear in mind **reviewer F3fH\u2019s** comment that *\u201c[this] is an easy weakness to raise for any experimental paper\u201d*.\n- Nevertheless recognizing the value of theoretical insights, we have attempted to offer, wherever possible, useful *theoretical arguments to complement these insights*, such as those in sections 3.1 under \u201cA theoretical picture\u201d and section 4 \u201cA bias-variance trade-off argument\u201d, whose proofs are developed in more detail in appendix S4.1 and S4.2 respectively.\n- But surely, these are *not yet definitive*. Another concrete idea for an in-depth investigation, that we would have liked to explore, was to utilize the general form of Tikhonov regularization (Bishop 1995), in terms of the function Jacobian, when learning under noise and relate it to the noise spectra characteristics used inherently in various proofs of benign overfitting (Bartlett, et al. 2020).  But, given the current scope, we were forced to leave a thorough theoretical investigation to the next version of the paper.\n\nTo conclude, we believe that our empirical findings, together with the seeds for theoretical analyses, will serve as useful scaffolding for other researchers to develop more thorough theoretical explanations and, in general, foster future research, as noted by **reviewer Jyw9**.\n\nBishop, Chris M. \"Training with noise is equivalent to Tikhonov regularization.\" Neural computation 7.1 (1995): 108-116.\nBartlett, Peter L., et al. \"Benign overfitting in linear regression.\" Proceedings of the National Academy of Sciences 117.48 (2020): 30063-30070.\n\n## Extension to Language Models\n- Given their increasing relevance in the scientific community and broader society, experimenting with language models is another exciting venue, but something which we had to sadly leave out for future investigations, due to limited time. \n- Apart from the involved computational challenges in approximating Lipschitz for large language models, which could potentially be carried out without as much difficulty via our lower Lipschitz bounds, this experiment also requires reflecting on and honing upon the right definition of the Lipschitz constant in the language domain. Should it be defined based on token embeddings or should it be based on (semantic) perturbations of textual output? \n- For now, we would like to refer you to our results for the Vision Transformer model in Figures S7 and S16, which potentially suggest the nature of the Lipschitz behaviour in Transformers in the language domain.\n\nAll in all, this is an exciting question and we hope to include these results in future iterations of this work.\n\n&nbsp;\n\n*We hope our response helped to clarify your questions. If something remains unclear, we are more than happy to discuss further.*"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7155/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579876642,
                "cdate": 1700579876642,
                "tmdate": 1700579876642,
                "mdate": 1700579876642,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Tf137jXDuR",
                "forum": "5jWsW08zUh",
                "replyto": "7ZAxtXriOc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7155/Reviewer_EeTb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7155/Reviewer_EeTb"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for response"
                    },
                    "comment": {
                        "value": "Appreciate the response. Thank you"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7155/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657709233,
                "cdate": 1700657709233,
                "tmdate": 1700657709233,
                "mdate": 1700657709233,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bxo3dMv8nT",
            "forum": "5jWsW08zUh",
            "replyto": "5jWsW08zUh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7155/Reviewer_Jyw9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7155/Reviewer_Jyw9"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the importance of Lipschitz continuity in neural network models, which influences their robustness, generalization, and susceptibility to adversarial attacks. In contrast to previous research that aims to tighten Lipschitz bounds and enforce specific properties, this study delves into characterizing the Lipschitz behavior of Neural Networks. It conducts empirical experiments across various scenarios, including different architectures, datasets, label noise levels, and more, exploring the limits of lower and upper Lipschitz bounds. \n\nNotably, the paper highlights the strong adherence to the lower Lipschitz bound, identifies a noteworthy Double Descent trend in both upper and lower bounds for Lipschitz continuity, and offers insights into how label noise affects function smoothness and generalization."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper conducted extensive experiments to showcase its findings and offers a comprehensive exploration of experimental details and discussions.\n- The paper raised intriguing facets of Lipschitz continuity within neural network models, which are likely to attract substantial interest from the deep learning community aiming to develop theory and practical algorithms based on these observations."
                },
                "weaknesses": {
                    "value": "- While the paper provides thorough experiments and in-depth discussions, its novelty might be subject to question. As also mentioned in the paper, there is concurrent research with a similar focus that has also highlighted the connection between the Lipschitz constant and Double Descent, although they tracked only an estimate of the Lipschitz constant. I appreciate the authors\u2019 efforts in sharing more empirical observations and discussions. But I am not very confident that the paper is completely novel.\n\n- From the optimizer's perspective, this paper may have some shortcomings as it does not delve into the discussion of weight decay and dropout, which are widely employed regularization techniques in neural network training. It is noted in the paper that the intention is not to focus on regularization techniques, but there are a few concerns to address. The term \"weight decay\" is mentioned only once in S2.6.9 (Page 22) with non-zero values. This implies that all other experiments either employ zero weight decay or the paper lacks sufficient implementation details. Similarly, the term \"dropout\" appears only once in S2.6.7 (Page), and the paper does not provide any insights or discussions regarding the impact of dropout."
                },
                "questions": {
                    "value": "- As highlighted in the weaknesses section, my primary concern revolves around understanding how commonly-used and seemingly simple regularization techniques like \"weight decay\" and \"dropout\" influence the Lipschitz constants. The paper would benefit from thorough discussions in this regard, rather than solely focusing on scenarios without regularization.\n\n- In the context of classification problems, it would be advantageous for the paper to display C_lower values separately for mis-classified samples and correctly-classified samples."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7155/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698543927870,
            "cdate": 1698543927870,
            "tmdate": 1699636847149,
            "mdate": 1699636847149,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ak7aOcOOEU",
                "forum": "5jWsW08zUh",
                "replyto": "bxo3dMv8nT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7155/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7155/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your constructive feedback. We are happy to see that you found the paper had a \u201ccomprehensive exploration\u201d and that it **\u201dis likely to attract substantial interest from the deep learning community aiming to develop theory and practical algorithms\u201d**. Here, we will address your concerns one by one.\n\n&nbsp;\n\n## Novelty\nOur paper indeed bears similarities with some concurrent works (some occurring publically within a span of as few as ~ 20 days), and we have been quite upfront about it. But we believe there are still some notable differences, in regards to these works, due to which our paper provides for a rounder and more complete picture.\n\n1. By our extensive focus on empirically verifying, at scale, **the fidelity of the lower Lipschitz bound to the effective Lipschitz constant**, as well as the general approach of sandwiching the true Lipschitz, our results provide a direct and almost unequivocal implication about the Lipschitz behaviour of deep neural networks.  \n2.  **This paper unifies many aspects around the Lipschitz constant**, which albeit not as matured as here are scattered through the literature, and provides a holistic treatment with a thorough analysis. We strived to reflect all the key aspects that one would naturally expect the Lipschitz constant to have a bearing on, such as the architectures, datasets, noise levels, optimizers, and possible regularizations (see below).\n3. More specifically, we present an **expanded view upon concurrently observed concepts** (like the shift of the interpolation threshold in Double Descent in the presence of noise) and put them into a more general framework that gives a more complete picture of the phenomena (i.e. relation with label-noise-wise Double Descent, see Section 5). Wherever possible, we have also tried to complement the empirical findings by sketching theoretical insights and providing useful intuition via synthetic examples. \n\n\nTherefore, we strongly believe that our insights would be valuable in fostering further research on the Lipschitz properties of neural networks, despite concurrent work.   \n\n\n&nbsp;\n\n\n## Optimizer\u2019s perspective \u2026. Fresh experiments on weight decay and dropout:\n\n\nWe agree with you, this is a very valid concern. While we briefly discuss the effect of explicit Lipschitz regularization schemes, most of our experiments do not use any weight decay, except for the ResNet or ViT on ImageNet, and neither dropout. Given that these are some of the most frequently employed regularization methods in practice, it would be very interesting to see if they have an implicit effect on the Lipschitz or not. Without further adieu, these results can be found below:\n\n\n**New experiments and results:** We have taken this opportunity to expand our Lipschitz investigation to the cases of increasing weight decay and dropout probability for the case of ResNet20 on CIFAR-10. We include our results in the new version of the paper in Appendices **S3.21 and S3.22**. \n\n\n- The key finding is that as the regularization strength increases (either through weight decay or dropout), the lower Lipschitz bound decreases. \n- Hence, this provides a helpful assurance that even usual regularization strategies have an indirect effect of regularizing the Lipschitz behaviour. \n- While this is to be expected in the case of a linear model, where the Lipschitz constant is just the norm of weights (and enforcing weight decay would be exactly equivalent to a Lipschitz regularization), it is interesting to see that this effect seeps through for networks trained in practice. \n\n\n## C_lower separately for correctly-classified and misclassified samples. \nThat\u2019s a great suggestion. We have gone ahead and carried out this experiment, the results of which can be found in Section S3.23, page 42. We notice that while incorrectly classified tend to more frequently result in higher Jacobian norms, as you were hinting at, there are also correctly classified points with similar levels of high Jacobian norms. As the local Lipschitz definition is based on a supremum, basing the calculation on either of the sets alone would yield a similar value. Although, it seems to offer an empirical advantage, where one can prioritize computation of Jacobian on incorrectly classified when resource bound. \n\n&nbsp;\n\n*We hope that our response helped alleviate the concerns you had about our paper. Should you have any further questions or comments, we remain at your disposal. If we have resolved your concerns, we would be glad if you would consider raising your score.*"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7155/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579531491,
                "cdate": 1700579531491,
                "tmdate": 1700579531491,
                "mdate": 1700579531491,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wsGxoqUUSN",
                "forum": "5jWsW08zUh",
                "replyto": "bxo3dMv8nT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7155/Reviewer_Jyw9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7155/Reviewer_Jyw9"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors,\n\nThank you for your detailed response! It has addressed almost all my concerns. While I intend to increase my score at this moment, I would like to hold on a bit for the upcoming AC/Reviewer discussion phase.\n\nThe paper is well-written overall. However, my primary concern is the appropriateness of a 45-page compilation of multiple empirical studies for publication in the ICLR conference proceedings. In particular, as also pointed out by Reviewer F3fH, the paper is mostly located within its own appendix.\n\nI will work closely with other reviewers and AC in the next discussion phase to make a final decision.\n\nThanks!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7155/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638922124,
                "cdate": 1700638922124,
                "tmdate": 1700639022921,
                "mdate": 1700639022921,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DlBOp5hd6J",
            "forum": "5jWsW08zUh",
            "replyto": "5jWsW08zUh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7155/Reviewer_F3fH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7155/Reviewer_F3fH"
            ],
            "content": {
                "summary": {
                    "value": "The paper focus on the Lipschitz constant of Neural Networks, and in particular focus on different aspect: 1) how and what do we learn from different Lipschitz bounds from the literature and in particular the gap between lower and upper bounds and its evolution during training, 2) the impact of over-parametrisation on the Lispchitz constant and 3) the impact of noisy labels the network lipschitzness. \nThe author focus on an intense experimental protocol in order to shade some lights on many claims about the Lipschitz constant of neural networks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is very nice to read (if the appendix had been printed aside), it covers the topic pretty well and its relationship with related works is well described.\n\nIt is mostly an experimental study and it seems to me that the methodology is correct. A lot of experiments in various regime with many different real life instances are quite convincing to me. All experiments lead to a reasonable or theoretically supported interpretation.  \nI quite appreciate that many experiences are real life: with trained network of descent size on well known datasets."
                },
                "weaknesses": {
                    "value": "The article covers many subject and proposes many illustrations of their finding. However a limitation of this work relies in the lack of theoretical insights on the different findings that are discussed (which is an easy weakness to raise for any experimental paper, I admit).\n\nReading this article is a constant back-and-forth between the main article and its appendix. It often feel that the main article is a glossary to the appendix. As such it often feels like the article should be 'vectorized' and would be better put into a journal format.  \nFor this reason I find difficult to correctly judge the adequacy of this (paper+appendix) to a conference like ICLR and I am open to this discussion with my fellow reviewers and AC.\n\nBibliography:\n\n- many references are incomplete: they point at arXiv versions rather than their peer-reviewed publications.\n- Some references are doubled such as 'On lazy training in differentiable programming' by Chizat et al.\n- Are you certain that the reference to (Gomez et al. 2020) ('Lipschitz constant estimation of Neural network via sparse polynomial optimization') shouldn't be (Latorre et al.) as I do not see the name 'Gomez' in the original paper."
                },
                "questions": {
                    "value": "Definition 2.2 is more of a proposition following Definition 2.1.\n\nSection 3.2: How do you compute the convex combinations of the domain samples? Does it actually makes a lot of difference with looking at random points around the samples (say a gaussian centered at a specific sample)? Intuitively it is where one could find the steepest parts of the neural networks rather than convex combinations.\n\nFigure 8: does the practical variance follow the theoretical bound\n\nMore generally and in the actual context of the field, it would be very interesting to add experiments with Transformers (with constraint inputs to make it Lipschitz).\n\n**Appendix:**\n\nS4, Intermediate-points based analysis:  \nI am not sure about how to go from 2nd line to 3d line of the list of inequalities. It seems to me that $C^{discrete}$ is not the correct constant to consider and in order for the inequality to be correct, the supremum should be considered on all $\\theta$. It however doesn't seem to be critical to the result about the growth of the Lipschitz constant through training. Could you confirm or dismiss this comment?\n\nS3.17: I am very surprised with this and I agree with the comment made there, do you have any explanation to propose for such a marginal difference?\n\nS3.18: Have you try to apply the same methods as in S4.1 but for Adam (or maybe RMSprop as it might be easier) optimizer? With maybe minimum assumptions it could shade some lights on this difference of behaviour.\n\n## Typos:\n\nSection 5: 'emprical'\n\nAppendix S3.1: 'btheta' missing '' in tex file probably\n\n## Overall\n\nThe author present a very interesting experimental approach to many Lipschitz claims about neural network and are convincing at exploring, discussing and interpreting their finding. As such I think it is a good paper. My main limitation for a conference is the fact that the paper is mostly located within its own appendix."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7155/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698768472744,
            "cdate": 1698768472744,
            "tmdate": 1699636847045,
            "mdate": 1699636847045,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "g77tbAoKNJ",
                "forum": "5jWsW08zUh",
                "replyto": "DlBOp5hd6J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7155/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7155/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thank you for your positive review and thorough feedback. We are glad to hear that you found our paper covers the topic pretty well with an intense experimental protocol resulting in reasonable or theoretically supported interpretations. Below, we would like to take this opportunity to discuss in detail the concerns and questions raised by you:\n\n&nbsp;\n\n## Limitation: Copiousness of the work\n\nThat\u2019s a quite understandable point. A work aiming to be comprehensive runs the risk of being obscure, and we tried our best to tread this tightrope as far as possible. But, much to our dismay, this seems to have come at the cost of causing the reader a back-and-forth between the main paper and the appendix. \n\n*Presentation strategies*. We will make our presentation more succinct and, more generally, try to reorganize some of the presentation which we were unable to do before. More concretely, we will focus on covering at least one scenario in each of the subsections in its completeness, so that the back-and-forth is minimized and if there is still some, it becomes more accessory. We believe at the moment there is some redundancy in the presentation of the figures. For instance, some of them can be combined or superimposed into one (surely, Figures 2 and 3; but potentially also the subfigures in Figures 1, 5, 7). This additional space will readily allow us to cut down the back-and-forth and make the overall read more comfortable and unhindered. \n\n*Journal Version*. This is a welcome suggestion. At some point down the line, we definitely plan to indeed further develop and exposit our paper through the journal format. But more than the cosmetic changes, we would like the journal version to also rigorously characterise the theoretical aspect of these findings and look, in more detail, into some other application areas (such as NLP). However, given that this would require a rather non-trivial amount of time and effort, we believe that it would be highly worthwhile to already disseminate the current set of mature (albeit empirical) findings \u2014 about an important topic like Lipschitz continuity \u2014  in the scientific community via publishing at a highly relevant conference like ICLR.\n\n## Other concerns.\n\n> Lack of theoretical insights \u2026 easy weakness for an experimental paper.\n\n- As you have justly recognized, our focus has primarily been on uncovering intriguing **empirical aspects** of the Lipschitz constant \u2014 which makes for an easy weakness in any experimental paper.  \n- Also, we want to emphasize that such a focus is a natural consequence of our approaching this lofty problem while being grounded in empirical observations since the other direction of landing groundward from theory can often either overshoot or even be amiss. \n- Nevertheless recognizing the value of theoretical insights, we have attempted to offer, wherever possible, useful *theoretical arguments to complement these insights*, but surely, they are *not yet definitive*. \n\nThus, given the current scope, we were forced to leave a more in-depth theoretical investigation of the presented phenomena as well as the matured empirical insights to a future version of our work. \n\n*Bibliographic Issues*: We have fixed these issues. The issue about (Gomez et al. 2020) not being (Latorre et al.)  stems from the full name of its first author, i.e., Fabian Latorre G\u00f3mez, from the DBLP profile https://dblp.uni-trier.de/pid/244/9638.html.\n\n*Typos*: These have been fixed too. Thanks for pointing out."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7155/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578613143,
                "cdate": 1700578613143,
                "tmdate": 1700578920303,
                "mdate": 1700578920303,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ISLdJGbe6x",
                "forum": "5jWsW08zUh",
                "replyto": "DlBOp5hd6J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7155/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7155/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (continued)"
                    },
                    "comment": {
                        "value": "## Regarding the questions\n\n1. We have changed Definition 2.2 to a Proposition, it does indeed make more sense to name it so.\n\n2. About convex combinations and relation to points sampled via a Gaussian: \n\n*Geometry of convex combinations*. We compute convex combinations as a weighted (Euclidean) mean between two input points in the dataset. There can be two scenarios roughly: (a) When the considered points are distant, the convex combination (based on Euclidean mean) may not completely lie on the data manifold. (b) However, for closeby points, the convex combination should still yield a data point that is close to the data manifold, if not on it.  \n\nSo, it is a bit different from looking at random points around samples, which will yield points closer to the data manifold. But, in practice, as we consider a very large number of convex combinations, we expect to cover both these scenarios and the convex combinations should obtain results at least as good as that obtained through random sampling,  in terms of the lower Lipschitz bound. \n\n*Intuition*: As shown elsewhere, we can expect the highest Jacobian norms on points near the decision boundary, i.e., difficult points on the data manifold. Hence, in general, we think that your intuition is reasonable. \n\n*Empirical observations and hypothesis*. However, our empirical data suggest that sampling convex combinations results in larger $C_{lower}$ estimates. We refer to Figures 1 and S5 for this: even though S5 shows a Double Descent scenario, it shows that for most models the lower Lipschitz estimate barely changes when samples with Gaussian noise are considered. We guess this happens since we sample points from an isotropic Gaussian, of varying standard deviation, which consequently does not properly factor in the data covariance. In contrast, convex combinations would yield a point that better preserves the structure of the data covariance. \n\n3. In Figure 8, the practical variance generally follows the theory. They are not fully identical, but the curve with $C_{lower}$ is rather close.\n\n4. In case you might not have noticed, we do already show some results with a Vision Transformer (ViT) in Figures S7 and S16.  But, nevertheless, studying this in the context of a language modelling task would further the relevance of our work, and we intend to provide such experiments in a future expanded (journal) version of the paper. For now, given the architectural similarities, Figures S7 and S16 might point towards the nature of the behaviour that we can expect to see. \n\n&nbsp;\n\n## Regarding the Appendix\n\n1. > S4, Intermediate-points based analysis: \u2026 the supremum should be considered on all $\\theta$\n\nThat\u2019s a good observation, and ideally, we could do that. We didn\u2019t opt for it because:  \n\n(a) As the theoretical analysis considers all encountered iterates in the optimization trajectory, so the distance between consecutive iterates is usually small enough such that quadratic or higher order terms in this distance are negligible and the first-order Taylor approximation $|f(\\theta_{t+1}, x) - f(\\theta_t, x)| \\approx \\nabla_\\theta f(\\theta_t, x)^\\top (\\theta_{t+1} - \\theta_t)$ holds. This together with Cauchy-Schwarz should explain the choice of $C^{\\text{discrete}}$. We will make this more explicit.\n(b) Practically, this step also allows  **make our estimates tractable**. We aimed to work out a bound that could potentially be estimated with our Lipschitz bounds and evaluated on real data (which we did in Appendix S3.4).\n\nTo sum up, as you have pointed out, this isn\u2019t critical. Even if we considered the whole set of $\\theta$ values, then we would arrive at a result similar about the growth of the Lipschitz during training. \n\n2.\n> S 3.17 such a marginal difference? [Lipschitz with MSE marginally lower than with CE]\n\nWe attribute this difference to the effects of Softmax and the nature of CE loss. When minimizing the network parameters via CE, the solution is found at $\\infty$, i.e., by driving the parameter norms $\\rightarrow \\infty$ so as to crank up the logits resulting in a one-hot softmax output. In practice, when training for a large but finite number of epochs, the logits end up being quite large. So then, Figures 28a and 28b can be understood as:\n- Figure 28a: When the function definition (and thus the lower Lipschitz) does not include the Softmax operation, the Lipschitz values are large since the logits (and parameter norms) are large. \n- Figure 28b: If the function definition includes Softmax, the rescaling of the outputs achieved via Softmax, suppresses the Lipschitz value as compared to Figure 28a (without Softmax), and brings it closer to the eventual value achieved by MSE. \n- MSE imposes a stronger, more direct, constraint (forcing predictions, here for classification tasks, to be exactly one-hot), so it results in still marginally smaller Lipschitz values relative to above than the \u2018softer\u2019 constraint via CE loss."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7155/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578858171,
                "cdate": 1700578858171,
                "tmdate": 1700578858171,
                "mdate": 1700578858171,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "n5wsOUXJMF",
                "forum": "5jWsW08zUh",
                "replyto": "DlBOp5hd6J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7155/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7155/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response (continued, part 2)"
                    },
                    "comment": {
                        "value": "3. >  S3.18: Have you try to apply the same methods as in S4.1 but for Adam (or maybe RMSprop as it might be easier) optimizer?\n\nThat is a very interesting suggestion, and which indeed could form for a quite fruitful pursuit! But, carrying it out, with the quality we would like to strive for, would be outside the current confines. This will, undoubtedly, be in the top few of our list of questions for further consideration.\n\n&nbsp;\n\n*We hope that our above response comprehensively addresses your remaining concerns. However, if something remains unclear or if there are any further questions, we are happy to answer them.*"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7155/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578877332,
                "cdate": 1700578877332,
                "tmdate": 1700578877332,
                "mdate": 1700578877332,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]