[
    {
        "title": "Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models"
    },
    {
        "review": {
            "id": "FTPdTysVKW",
            "forum": "6LNTSrJjBe",
            "replyto": "6LNTSrJjBe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4113/Reviewer_15HT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4113/Reviewer_15HT"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes LATS, a general framework that unifies the capabilities of LLMs in planning, acting, and reasoning by deliberately constructing trajectories with MCTS and incorporating external feedback. The experiments demonstrate the superiority of LATS by achieving new sota on HumanEval and HotPotQA."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper proposes a general framework unifying the capabilities of LLMs in planning, acting, and reasoning\n\n- The paper is well-written and presented clearly.\n\n- The results look promising, achieving new sota on HumanEval and HotPotQA\n\n- The ablation provides some insights into the importance of various strategies when harnessing the power of LLMs."
                },
                "weaknesses": {
                    "value": "- LATS uses a higher computational cost to achieve a better performance, it would be better to add some table or figure explicitly discussing about the tradeoff here.\n\n- It would be clearer to add the exact number of API calls and tokens used etc. for each baseline in the results table since the inference time is not directly comparable to other methods.\n\n- Not much novelty compared to existing LLM prompting techniques.\n\n- It would be interesting to see how LATS performs in real complex planning environments, such as ALFWorld and Minecraft."
                },
                "questions": {
                    "value": "Please address the concerns raised in the Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4113/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698613836457,
            "cdate": 1698613836457,
            "tmdate": 1699636376280,
            "mdate": 1699636376280,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "twgG8kfNEa",
                "forum": "6LNTSrJjBe",
                "replyto": "FTPdTysVKW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4113/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4113/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 15HT (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for appreciating our work. Below we address each of the reviewer\u2019s concerns.\n\n \n**Q1.LATS uses higher computational cost and requires more results on the tradeoff.** \n\n**1. Theoretically, LATS consumes more token than CoT-SC but has the same token consumption as RAP and ToT.** The token consumption of LATS is higher relative to other simpler baselines (e.g., CoT-SC), as LATS expands with multiple samples every step; the overall token consumption is $n$ times higher than CoT-SC/ReAct with $k$ attempts, where $n$ is the number of samples per expansion. However, as the token cost of other tree-structured search methods (RAP and ToT) are also parameterized by the number of attempts $k$ and the number of nodes expanded $n$, it means that **LATS, RAP, and ToT have the same sample complexity, i.e., same asymptotic token cost**. We summarize the sample complexity in the table below, which is also added to Tab. 7 of our revised submission.\n\n**2. Empirically, LATS expands less nodes upon success than RAP and ToT, which means that it is cheaper.** To count the token consumption more accurately,  we compare the average number of nodes expanded upon success on HotPotQA for LATS, RAP, and ToT. We find that LATS does not only have better performance, but also tends to solve questions with fewer nodes (66.65) sampled than RAP (70.60) or ToT (84.05) with the same sample complexity, indicating a more effective, thus cheaper, search. The result is also shown in the table below.\n\n**3. Even LATS without expansion of multiple nodes (i.e., $n=1$) outperforms other methods with the same sample complexity (CoT-SC, best among multiple runs for ReAct).** To give a fairer comparison between LATS and CoT-SC which does not expand nodes, we also test a version of LATS with $n=1, k=50$, and a version of CoT-SC and ReAct with $k=50*5=250$. We find that ReAct and CoT-SC cannot reach the performance of LATS with lower $k$ and $n=1$, let alone LATS with the same sample complexity ($n=5, k=50$).\n\nBelow is the table mentioned above:\n\nMethod | Performance on HotPotQA (&uarr;) | Sample Complexity (&darr;) | Average \\# of Nodes (&darr;)\n---|---|---|---\nReAct (best k = 250) | 0.42 | $O(k)$  |\nCoT-SC (n = 1, k = 250) | 0.40 | $O(k)$ | \nLATS (n = 1, k = 50) | 0.48 | $O(k)$ |\nToT - ReAct (n = 5, k = 50) | 0.49 | $O(kn)$ | 84.05\nRAP - ReAct (n = 5, k = 50) | 0.54 | $O(kn)$ | 70.60\nLATS (n = 5, k = 50) | 0.61 | $O(kn)$ | 66.65\n\n**Q2. Add the exact number of API calls and tokens used etc. for each baseline in the results table since the inference time is not directly comparable to other methods.**\n\nThanks for the suggestion. We have added a table showing the general and exact inference cost of all of our tested methods in the updated Tab. 7 in Appendix C of our submission (and also Q1 in the response). We found that 1) with the same sample complexity, our method performs the best, 2) our method with $n=1, k=50$ performs better than other methods with higher $k$, and 3) while LATS has the same sample complexity as ToT and RAP, our method expands less nodes on average upon success, which indicates less token used.\n\n**Q3. Not much novelty compared to existing LLM prompting techniques.**\n\nAlthough previous work has investigated either search algorithms or self-reflection, the combination of both has not been explored. In contrast, LATS synergistically integrates them, and more importantly, introduces unique and advanced methods to leverage search algorithms and self-reflection, and makes non-trivial adaptation of tree-based search with external feedback. Based on these, we have made significant improvements across multiple testbeds, including 94.4 pass@1 accuracy with GPT-4 on HumanEval and performance comparable to gradient-based fine-tuning methods on WebShop.\n\n**1. Paradigm contributions.** To the best of our knowledge, we are **the first to explore the use of search algorithms with LLM agents for decision-making tasks**. Previous approaches like ReAct and Reflexion cannot sample and select more than one state at every step, limiting exploration in decision-making environments; ToT and RAP do not explore decision-making tasks. It is non-trivial to shift from non-interactive tasks to interactive ones, because an updated node, prompt, and more importantly, search algorithm must be designed to incorporate external input \u2013 an achievement we have successfully accomplished. We verify that our proposed method accommodates the external input, even when applied to original non-interactive tasks such as programming. \n\n(Q3 continued in next part)"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700550033120,
                "cdate": 1700550033120,
                "tmdate": 1700550083825,
                "mdate": 1700550083825,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4wZSJgs6bQ",
                "forum": "6LNTSrJjBe",
                "replyto": "FTPdTysVKW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4113/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4113/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 15HT (Part 2)"
                    },
                    "comment": {
                        "value": "**2. Our LATS improves the use of MCTS.** Compared to RAP, LATS also improves the use of MCTS. In RAP, states are scored with a value function based on self-consistency, while we directly prompt the LM to score the state with justification. Our design is more effective in decision-making, which enables the state to also incorporate an external observation that adds context for a more accurate score. Additionally, LATS samples states through environment interaction, eliminating the need to use the LM as a world model as in RAP. This reduces the risk of error and makes LATS a general LLM framework.\n\n**3. Our LATS improves the use of self-reflection.** Compared to the self-reflection in Reflexion, LATS leverages semantic feedback and failed trajectories in additional ways. These are also added to the context of LM for the value function, thereby refining subsequent state evaluations in addition to the base decision-making.\n\n**4. Our adaptation of search algorithms to external feedback is not trivial, as simply adapting existing tree-based methods, such as ToT and RAP, to external feedback hurts the performance.** In our updated submission, we design a version of ToT and RAP that can incorporate environment feedback in HotPotQA using the ReAct prompt, and find that the new version of ToT and RAP generally performs worse than the reasoning-only setting of HotPotQA (0.55 vs. 0.49 for ToT, and 0.6 vs. 0.54 for RAP). This result is because the information retrieval setting is harder than the multihop question-answering setting, and it highlights that our efforts of adapting search algorithms to decision-making scenarios is non-trivial. The detailed numbers are listed below as well as Tab. 5 of our updated submission:\n\nMethod | HotPotQA\n---|---\nToT (ReAct) | 0.49\nToT | 0.55\nRAP (ReAct) | 0.54\nRAP | 0.60\nLATS (DFS) | 0.53\nLATS (No self-reflection) | 0.56\nLATS | 0.61\nLATS (CoT + ReAct) | 0.71\n\n\n**Q4. It would be interesting to see how LATS performs in real complex planning environments, such as ALFWorld and Minecraft.**\n\nWe thank the reviewer for the suggestion, though we would like to note that:\n\n**1. ALFWorld is pretty much addressed.** Reflexion has a success rate of above 90% on ALFWorld, which does not leave much room for improvement; on the other hand, Reflexion only achieves a success rate of 35% on WebShop. We chose WebShop because it is a more complex and challenging benchmark while being widely adopted.\n \n**2. While Minecraft is a jewel in the crown for decision-making, it is not a standard benchmark for evaluating prompting methods, and requires too much engineering on the environment itself not relevant to the general decision-making framework.** For example, Plan4MC (Haoqi Yuan et al., 2023) uses a hierarchical RL (PPO+DQN) method to find items in the world, which together only serves as a low-level controller for LLM. Without a widely adopted test suite for LLM alone, the performance of an LLM agent in Minecraft largely relies on the quality of the low-level controller, which is out of our scope of proposing a general framework for LLM decision-making (and other tasks such as reasoning). We do agree with the reviewer that such an environment is an exciting challenge for the next step of LLM decision-making, and added this to the discussion part of the revised submission.\n\n\nWe hope that our response has addressed the reviewer\u2019s questions and concerns. We are happy to answer any further questions.\n\n**References:**\n\nHaoqi Yuan et al. Plan4MC: Skill Reinforcement Learning and Planning for Open-World Minecraft Tasks. ArXiv: 2303.16563, 2023."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700550068237,
                "cdate": 1700550068237,
                "tmdate": 1700550077233,
                "mdate": 1700550077233,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VTUQpTAdOy",
                "forum": "6LNTSrJjBe",
                "replyto": "4wZSJgs6bQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4113/Reviewer_15HT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4113/Reviewer_15HT"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the reply,  I'll keep my score at this stage."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717639856,
                "cdate": 1700717639856,
                "tmdate": 1700717639856,
                "mdate": 1700717639856,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wosFZ4d2kK",
            "forum": "6LNTSrJjBe",
            "replyto": "6LNTSrJjBe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4113/Reviewer_7QYU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4113/Reviewer_7QYU"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Language Agent Tree Search (LATS), which hierarchically expands the reasoning path and employs Monte-Calro Tree Search (MCTS) to find the correct reasoning path. Also, to deal with decision making problems, Reflexion mechanism (reflecting past failure episodes and leveraging it in the future rollout) is incorporated. LATS empirically achieves the strong performance in HotpotQA, HumanEval, MBPP, and WebShop, as done in original Reflexion paper."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "### quality and clarity\n- This paper is well-written and easy to follow.\n\n### significance\n- The empirical results are strong. 94.4 Pass@1 in HumanEval would be notable results."
                },
                "weaknesses": {
                    "value": "- LATS seems to be the naive combination of existing methods, MTCS from RAP [1] (or ToT [2]) and Reflexion [3], to leverage past (failure) experience. I cannot find a clear difference among those. The originality and significance could be limited from this perspective. \n- Evaluation is biased to decision making (HotPotQA \\& WebShop). Some reasoning benchmark should be included, such as Game 24, Crossword as done in ToT [2] or GSM8K in RAP [1] to clarify the difference between LATS and ToT/RAP.\n- Related to Table 1, I think ToT [2] also incorporates self-refinement process.\n- The results of ReAct in Table 5 (WebShop) are lower than the one reported in original paper (Score: 66.6 / SR: 40.0).\n- In WebShop, WebGUM [4], a finetuned language model agent, achieves the best performance in SR (Score: 67.5 / SR: 45.0).\n- The intention in Figure 4 is ambiguous. Is this a conceptual description of Tree Search?\n\n[1] https://arxiv.org/abs/2305.14992\n\n[2] https://arxiv.org/abs/2305.10601\n\n[3] https://arxiv.org/abs/2303.11366\n\n[4] https://arxiv.org/abs/2305.11854\n\n(Minor Issue)\n- In Section 4.2, the definition of $M$ in UCT algorithm is missing."
                },
                "questions": {
                    "value": "- RAP applies at most 20 reasoning iterations for MCTS. This is smaller than LATS (50 iters). Is there any reason for this?\n- What is the difference between decision-making and planning in Table 1? I guess both are the same concept.\n- What \"Memory\" means in Table 1?\n- How did you measure each metric? Reporting aggregated best among $k=50$ reasoning iterations for MCTS (I guess \"best of k\" in Table 2)? or reporting the result after $k=50$ reasoning iterations for MCTS? I'm curious about its \"learning curve\".\n- In Table 2 (right), what \"CoT+ReAct\" means?  In my understanding, ReAct is \"CoT\" in decision making problem. Also, are there LATS (w/ CoT) in Table 2 (left)?\n- On WebShop, it is reported that Reflexion cannot improve the performance as done in ALFWorld. Could you explain what could be the source of improvement of LATS (because LATS employs Reflexion process, too)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4113/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698614390779,
            "cdate": 1698614390779,
            "tmdate": 1699636376205,
            "mdate": 1699636376205,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "P5AZAccZpv",
                "forum": "6LNTSrJjBe",
                "replyto": "wosFZ4d2kK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4113/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4113/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7QYU (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful comments. We are glad that the reviewer found our empirical results significant and the paper well-written. Below we address each of the reviewer\u2019s concerns.\n\n**Q1. LATS is the naive combination of existing methods with limited originality and significance.**\n\nAlthough previous work has investigated either search algorithms or self-reflection, the combination of both has not been explored. In contrast, LATS synergistically integrates them, and more importantly, introduces unique and advanced methods to leverage search algorithms and self-reflection, and makes non-trivial adaptation of tree-based search with external feedback. Based on these, we have made significant improvements across multiple testbeds, including 94.4 pass@1 accuracy with GPT-4 on HumanEval and performance comparable to gradient-based fine-tuning methods on WebShop.\n\nMore concretely, LATS is distinguished from prior work through the following key novelties:\n\n**1. Paradigm contributions.** To the best of our knowledge, we are **the first to explore the use of search algorithms with LLM agents for decision-making tasks**. Previous approaches like ReAct and Reflexion cannot sample and select more than one state at every step, limiting exploration in decision-making environments; ToT and RAP do not explore decision-making tasks. It is non-trivial to shift from non-interactive tasks to interactive ones, because an updated node, prompt, and more importantly, search algorithm must be designed to incorporate external input \u2013 an achievement we have successfully accomplished. We verify that our proposed method accommodates the external input, even when applied to original non-interactive tasks such as programming. \n\n**2. Our LATS improves the use of MCTS.** Compared to RAP, LATS also improves the use of MCTS. In RAP, states are scored with a value function based on self-consistency, while we directly prompt the LM to score the state with justification. Our design is more effective in decision-making, which enables the state to also incorporate an external observation that adds context for a more accurate score. Additionally, LATS samples states through environment interaction, eliminating the need to use the LM as a world model as in RAP. This reduces the risk of error and makes LATS a general LLM framework.\n\n**3. Our LATS improves the use of self-reflection.** Compared to the self-reflection in Reflexion, LATS leverages semantic feedback and failed trajectories in additional ways. These are also added to the context of LM for the value function, thereby refining subsequent state evaluations in addition to the base decision-making.\n\n**4. Our adaptation of search algorithms to external feedback is not trivial, as simply adapting existing tree-based methods, such as ToT and RAP, to external feedback hurts the performance.** In our updated submission, we design a version of ToT and RAP that can incorporate environment feedback in HotPotQA using the ReAct prompt, and find that the new version of ToT and RAP generally performs worse than the reasoning-only setting of HotPotQA (0.55 vs. 0.49 for ToT, and 0.6 vs. 0.54 for RAP). This result is because the information retrieval setting is harder than the multihop question-answering setting, and it highlights that our efforts of adapting search algorithms to decision-making scenarios is non-trivial. The detailed numbers are listed in Q2 as well as Tab. 5 of our updated submission.\n\n**Q2. Evaluation is biased to decision-making, and some reasoning benchmark should be included to clarify the difference between LATS and ToT/RAP.**\n\n1. We would like to clarify that the evaluation is biased because LATS, as its title suggests, is primarily a framework for LM **agents**, and adapting search algorithms to decision-making is one of our core contributions; in a word, we mainly focus on improving performance on decision-making, while maintaining competitive performance on reasoning. The word \u201cunifying\u201d is to describe a single methodology that is able to deal with both reasoning and decision-making, the latter of which (beyond ReAct) is still in its infancy.\n\n(Q2 continued in next part)"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549902632,
                "cdate": 1700549902632,
                "tmdate": 1700549902632,
                "mdate": 1700549902632,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AmSIwQ1BcT",
                "forum": "6LNTSrJjBe",
                "replyto": "wosFZ4d2kK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4113/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4113/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7QYU (Part 2)"
                    },
                    "comment": {
                        "value": "Thus, the way we distinguish LATS from ToT/RAP is to evaluate it on benchmarks where an environment feedback is accessible. These planning methods do not natively support external observations, which limits overall performance to the LLM\u2019s existing internal ability. **In fact, trivial handling of external feedback over ToT and RAP even leads to performance drop.** To validate this, we would like to point the reviewer to the updated Tab. 5 in the submission, which we have also added below. Here, we extend ToT and RAP to the decision-making setting by using environmental interactions during sampling with the ReAct prompt, which we denote as **ToT (ReAct) and RAP (ReAct)**. We find that the baselines generally perform worse than the reasoning-only setting of HotPotQA (0.55 vs. 0.49 for ToT and 0.6 vs. 0.54 for RAP), which indicates that the acting-based setting is more challenging and the adaptation of search algorithms to decision-making scenarios is non-trivial.\n\nThe performance is listed as follows:\n\nMethod | Performance \n---|---\nToT (ReAct) | 0.49\nToT | 0.55\nRAP (ReAct) | 0.54\nRAP | 0.60\nLATS (DFS) | 0.53\nLATS (No self-reflection) | 0.56\nLATS | 0.61\nLATS (CoT + ReAct) | 0.71\n\n2. We did include results on a reasoning benchmark in Tab.1. This is the standard reasoning setting for HotPotQA as a question-answering benchmark without the external API. We find that while LATS-CoT outperforms ToT, its performance is comparable to RAP. In reasoning-only settings, the advantages of LATS are reduced, as the removal of external feedback also reduces the efficacy of self-reflection and the LM value function. But again, our focus is on maintaining performance on reasoning, while making adaptations for decision-making environments with external feedback.\n\n**Q3. In Table 1, ToT also incorporates a self-refinement process.**\n\nThe \u201cself-refine\u201d in Tab. 1 refers to the LM-based generation of semantic feedback. ToT only uses an LM to score states and does not summarize failed trajectories. We have updated the column of \u201cself-reflection\u201d in Tab. 1 to make this distinction clearer.\n\n**Q4. The results of ReAct in Table 5 (WebShop) are lower than the one reported in the original paper.**\n\nThe ReAct results in the original paper use PaLM, a proprietary language model that we have no access to. Our experiments use GPT-3.5 because it is available, widely used, and capable of in-context learning. GPT is similar in model scale to PaLM, but has some performance differences.\n\n**Q5. In WebShop, WebGUM achieves the best performance in SR.**\n\nThanks for pointing this out. We have added WebGUM to Tab. 4 in the updated submission. However, we would like to note that while WebGUM indeed has a higher SR, it uses a different base language model and involves task-specific **fine-tuning**, making it **an unfair comparison** with LATS and other **gradient-free** techniques. Even so, it is noteworthy that on the average score, LATS outperforms WebGUM (75.9 vs 67.5) despite being gradient-free. \n\n**Q6. The intention in Figure 4 is ambiguous.**\n\nFigure 4 is meant to show a qualitative example where LATS can improve over ReAct. To better capture this, we uploaded a new Figure 4 in the updated submission (now Fig. 5). The figure has been moved to Appendix C as Fig. 5 now.\n\n**Q7. In Section 4.2, the definition of M in UCT algorithm is missing.**\n\nThanks for pointing this out. This should have been N as well. We have fixed the formula in the updated submission.\n\n**Q8. RAP applies less reasoning iterations for MCTS than LATS (20 vs. 50 iters).**\n\n1. We would like to first clarify that in our original submission, as detailed in the experiment section, we use the same hyperparameters for LATS and RAP \u2013 they both use 50 iterations for HotPotQA with each node sampling 5 children; thus, the comparison is fair. Note that, in the RAP\u2019s paper, 20 iterations were used. However, our experiment showed that using 50 iterations improves RAP\u2019s performance, so we adopted this hyperparameter setting than that in RAP\u2019s paper. \n\n2. Experiment results further suggest that our LATS is more efficient than RAP when it comes to iterations. We test the average number of nodes expanded upon success for LATS, ToT, and RAP on HotPotQA, and find that LATS, while having higher success rate under the same set of hyperparameters, expands less nodes upon success (66.65; RAP uses 70.60 nodes, while ToT uses 84.05 nodes; see Tab. 7 in the updated submission) and thus needs less iteration than RAP and ToT to find the correct answer.  \n\n**Q9. What is the difference between decision-making and planning in Table 1? Are they the same concept?**\n\nIn LLMs, planning is **defined by algorithm**; it refers to using search algorithms like BFS or MCTS. Decision-making is **defined by problem settings**; it refers to the use of and acting within an external environment. We have updated the caption in Table 1 and changed \u201cdecision-making\u201d to \u201cacting\u201d to make this clearer."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549950221,
                "cdate": 1700549950221,
                "tmdate": 1700549950221,
                "mdate": 1700549950221,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "csaeyAFbLi",
                "forum": "6LNTSrJjBe",
                "replyto": "wosFZ4d2kK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4113/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4113/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7QYU (Part 3)"
                    },
                    "comment": {
                        "value": "**Q10. What does \"Memory\" mean in Table 1?**\n\n\u201cMemory\u201d refers to the storage of prior states and failed trajectories in an external memory for future use. We have changed the entry to \u201cExternal Memory\u201d and updated the caption to make this more clear.\n\n**Q11. How did you measure each metric? Reporting aggregated best among k=50 reasoning iterations for MCTS (I guess \"best of k\" in Table 2)? or reporting the result after k=50 reasoning iterations for MCTS? I'm curious about its \"learning curve\".**\n\nIt is the former; we report the best trajectory out of k. We have also ablated performance over reasoning iterations in Appendix C, Fig. 4 in the updated submission. The results show that LATS has a better learning curve than Reflexion.\n\n**Q12. In Table 2 (right), what \"CoT+ReAct\" means? In my understanding, ReAct is \"CoT\" in decision-making problems. Also, are there LATS (w/ CoT) in Table 2 (left)?**\n\nYes, CoT and ReAct are both base prompt designs for reasoning and decision-making, respectively. The CoT + ReAct version of LATS incorporates both types of base prompts to combine internal reasoning (CoT) and information retrieval (ReAct) strategies for HotPotQA. The API-based setting primarily evaluates the ability of the LM to retrieve information using external tools, while the standard setting evaluates internal reasoning and knowledge, so using both is optimal (Sec 5.1). \n\nAlso, we indeed have results of LATS using CoT, which has the same performance as RAP.\n\n**Q13. On WebShop, it is reported that Reflexion cannot improve the performance; then what could be the source of improvement of LATS?**\nIn Reflexion, the improvement of Reflexion over ReAct is only through the final summary/feedback generated by the LLM that is added as context. We also find that this summary is less useful in the WebShop difficult environment, but LATS is able to draw additional improvements through the search algorithm. While Reflexion samples one action at every step, LATS samples up to five actions and uses MCTS to optimally explore the state space.\n\nWe hope that our response has addressed the reviewer\u2019s questions and concerns. We are happy to answer any further questions."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549981639,
                "cdate": 1700549981639,
                "tmdate": 1700549981639,
                "mdate": 1700549981639,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7EBSt07Rh7",
                "forum": "6LNTSrJjBe",
                "replyto": "wosFZ4d2kK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4113/Reviewer_7QYU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4113/Reviewer_7QYU"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response and additional experiments. Here are the remaining concerns I still have:\n\n**Q1.**\n\nI still think the technical novelty is limited because of the combinational nature of the proposed method. For instance, the author mentioned LATS utilizes LLM scoring with justification compared to self-consistency used in RAP. But LLM scoring was adopted in ToT before. Also, the difference between LATS, RAP-ReAct, and ToT-ReAct can be a range of prompt-engineering. I partially agree that it is difficult to unify all. Still, because each component is off-the-shelf and unification in prompting methods can be a prompt-engineering effort, I cannot agree with the significance and technical novelty.\n\n**Q3.**\n\nToT employs self-refinement in the creative writing experiment, which iteratively incorporates their own feedback for writing.\n\n**Q8.**\n\nWhat I'd like to point out here is about \"its inefficiency\", rather than about \"fair comparison\". 50 trials in decision making tasks while accepting its failures would not be impractical. \n\n**Q12.**\n\nI'm still confused about this. ReAct has \"think\" action to organize the agent's observations. This could be equivalent to CoT. What is \"CoT\" in the CoT+ReAct setting?"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656705656,
                "cdate": 1700656705656,
                "tmdate": 1700656705656,
                "mdate": 1700656705656,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "188PxnFO18",
                "forum": "6LNTSrJjBe",
                "replyto": "wosFZ4d2kK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4113/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4113/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 7QYU (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the reply. We would like to further clarify the reviewer\u2019s remaining concerns.\n\n**Q1**\nWe respectfully disagree with the reviewer about the criteria of novelty. We would like to argue that the simplicity or combinational nature of an approach should not be misinterpreted as a lack of novelty. Instead, simplicity should be regarded as a commendable strength. To support this argument, we refer to the famous perspective from Michael Black on novelty [1]:\n\n> \u201cThe simplicity of an idea is often confused with a lack of novelty when exactly the opposite is often true. A common review critique is: the idea is very simple. It just changes one term in the loss and everything else is the same as prior work. However, if nobody thought to change that one term, then it is ipso facto novel. The inventive insight is to realize that a small change could have a big effect and to formulate the new loss.\u201d\n\n> \u201cThe idea is obvious because the authors just combined two well known ideas. Obvious is the opposite of novelty. So, if an idea is obvious after you\u2019ve heard it, reviewers quickly assume it isn\u2019t novel. The novelty, however, must be evaluated before the idea existed. The inventive novelty was to have the idea in the first place. If it is easy to explain and obvious in hindsight, this in no way diminishes the creativity (and novelty) of the idea.\u201d\n\nWe believe that our work precisely matches this notion of novelty: **It is simple, but is the first to explore a timely problem that leverages search algorithms with LLM agents for decision-making tasks, and demonstrates substantial performance improvement including 86.9 pass@1 accuracy with GPT-3.5 on HumanEval, a 27.6% improvement over Reflexion and higher performance than base GPT-4**. We would respectfully point out that our method makes non-trivial efforts to unify successful past designs and adapt to environments with feedback. In addressing the reviewer\u2019s specific follow-up concerns:\n\nAs acknowledged by the reviewer, \u201cI partially agree that it is difficult to unify all.\u201d \n* We have successfully accomplished this difficult task as the first work, and we believe that our LATS framework will lay the foundation for future research in LLM decision-making.\n\n\u201cLATS utilizes LLM scoring with justification compared to self-consistency used in RAP. But LLM scoring was adopted in ToT before\u201d \n* While LLM scoring was adopted in ToT, it is important to note that none of the prior works employed LLM scoring with self-reflection, making our approach novel, effective, and noteworthy within the community.\n\n\u201cThe difference between LATS, RAP-ReAct, and ToT-ReAct can be a range of prompt-engineering\u201d \n* We do not feel that using \u201cprompt-engineering\u201d to be a ground for lack of novelty. For instance, ReAct is about the design of prompts based on existing work of chain-of-thought; however, such a simple idea becomes fundamental in the field of LLM decision-making. To reiterate, **our core technical contribution is not based on prompting, but about adapting search algorithms and MCTS to LM agents**.\n\n**References:**\n[1] Novelty in Science. Michael Black. URL https://medium.com/@black_51980/novelty-in-science-8f1fd1a0a143. Accessed Nov. 22nd, 2023."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700716331285,
                "cdate": 1700716331285,
                "tmdate": 1700716428779,
                "mdate": 1700716428779,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "reihdNJvhp",
            "forum": "6LNTSrJjBe",
            "replyto": "6LNTSrJjBe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4113/Reviewer_j4Tk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4113/Reviewer_j4Tk"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a MCTS framework which uses LLM as the basic functioning component to perform sequential decision-making tasks. \nIt achieves strong performance across multiple important benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed framework is intuitive and easy-to-understand. \nIt combines modern LLM with classical MCTS algorithm, which is a neat idea. \n\nThe empirical results are strong."
                },
                "weaknesses": {
                    "value": "There are some key weaknesses that prevent me from giving an acceptance score. \n\nFirst, some key technical designs of the proposed framework are not well motivated and seem to be problematic. \n\nE.g., the Abstract says that this method is inspired by model-based RL but the proposed method is model-free. The authors argue that \"we can conveniently backup to any state by setting the input...\" but, without an environment model, one needs to actually interact with the environment to roll out future steps in order to compute values for possible actions at an earlier step; see Fig-2 and 3. How is that possible in a real application that you could go back up to an earlier step after execution? More importantly, in deployment/inference, using future states of actual interactions does not seem to be a fair comparison with other model-free approaches such as ReAct which doesn't roll out, because it is like an undo move, right? \n\nSecond, presentation has major issues. It is easy-to-follow, which is good, but it leaves out much important information so I find it hard to gauge its overall soundness. \n\nE.g., the LATS section is confusing even to a reader familiar with both LLM and MCTS. This section needs a high-level review about MCTS and how it is reshaped with LLM for this framework. Important technical details need to be added: e.g., I didn't find any math formula about backpropagation even after checking within appendices and algorithm boxes."
                },
                "questions": {
                    "value": "Why model-free and why you can still take a previous action after actual roll-out?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4113/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698720008900,
            "cdate": 1698720008900,
            "tmdate": 1699636376106,
            "mdate": 1699636376106,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cNG9rB1DYT",
                "forum": "6LNTSrJjBe",
                "replyto": "reihdNJvhp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4113/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4113/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer j4Tk"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful comments. Below we address each of the reviewer\u2019s concerns.\n\n**Q1. The method is inspired by model-based RL, but the proposed method is model-free and can back up; one needs an environment model to back up.**\n\nWe thank the reviewer for pointing this out. We would like to clarify the confusion from the following aspects:\n\n1. Our method is indeed model-free, which we did not explicitly state because it is not common to use this term to describe LM agents. \u201cThe inspiration from model-based RL\u201d only lies in the use of MCTS. We thank the reviewer for pointing out this potential source of confusion, and have updated the abstract to make this distinction clearer.\n\n2. Such statement is true for normal RL environments, but this is not the case for decision-making with LLM because **backing up in LLM decision-making in most settings is simply prompting with previous states of the frozen LLM**.\n\nStates in decision-making with LLM come from three sources: task context / prompt (initial state, unchanged during the task), thoughts / reflection (generated by the LLM itself), and external APIs. In LLM benchmarks, the calls of external APIs do not affect each other (e.g., the searches in database are independent in HotpotQA, evaluations are independent in programming, and the xmls can be reset conveniently by navigating to the previous website in Webshop.) Thus, it is easy to **back up in LLM decision-making without an environment model** \u2013 we simply store prior history of text and modify the input accordingly, when the search needs to return to an earlier step.\n\n**Q2. Backing up to earlier steps is unrealistic for real applications.** \n\nThough it is true that the ability to back up is a strong assumption in RL environments, **this is not the case for most LLM decision-making settings**, with the reason discussed in Q1. Perhaps in certain large-scale environments like Minecraft this would be harder, so we have acknowledged this in the updated limitations section in Sec. B in the Appendix.\n\n**Q3. The ability to back up is unfair to model-free approaches such as ReAct.**\n\nLATS is indeed model-free, and the ability to back up is fair to methods like ReAct, because LATS uses real environment interactions, which is the same as sampling the next step with ReAct. Such an ability to back up exists in all tasks of interest of all our baselines, including ReAct \u2013 however, they overlooked and did not use this capability. Recognizing this fact and leveraging this property is the main motivation of our work. \n\n**Q4. The presentation of LATS is unclear, with review of MCTS and backpropagation procedure missing.**\n\nThanks for pointing this out. In our updated submission, we have added an overview of standard MCTS, the key adaptations with LM agents, and added the backpropagation formula in Sec. 4.2 as well as the pseudocode in Appendix A. \n\nWe hope that our response has addressed the reviewer\u2019s questions and concerns. We are happy to answer any further questions."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549814760,
                "cdate": 1700549814760,
                "tmdate": 1700549814760,
                "mdate": 1700549814760,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9zkU1cElfB",
                "forum": "6LNTSrJjBe",
                "replyto": "cNG9rB1DYT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4113/Reviewer_j4Tk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4113/Reviewer_j4Tk"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. But my key concerns are still not resolved. \n\n> backing up in LLM decision-making in most settings is simply prompting with previous states of the frozen LLM.\n\nBut MTCS requires roll-outs. If you do not have an env model, you have to roll out with real environments. Once you move forward with a real-environment, then how can you back up afterwards? You can not undo what you have done in life, right? \n\n> Though it is true that the ability to back up is a strong assumption in RL environments, this is not the case for most LLM decision-making settings \n\nI am afraid that it does not sound correct to me. \n\nIf you use MTCS in an env without using env models, then you need the env to be able to back up---this is a hard constrain, not a strong assumption. \n\nAnd whether it allows a back-up is the property of the env, nothing to do with what kind of decision makers are being used. So why \"not the case for most LLM decision-making settings\"? \n\n> however, they overlooked and did not use this capability.\n\nThen you should be clearly upfront about this point in paper. \n\nIn addition, a fairer comparison will be considering # of steps you need to be correct. But this is not major. \n\nWith all the issues discussed above, I am not ready to raise the score of this paper. Sorry."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689505955,
                "cdate": 1700689505955,
                "tmdate": 1700689505955,
                "mdate": 1700689505955,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Go3K9HIJaY",
                "forum": "6LNTSrJjBe",
                "replyto": "reihdNJvhp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4113/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4113/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer j4Tk"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the reply. We would like to further clarify the reviewer\u2019s remaining concerns. \n\nWe would like to kindly point out that the **confusion from the reviewer lies in treating the newly emerging environments in which LM agents operates, and which we focus on, as the traditional RL environments \u2013 these two types of environments do have distinct properties, especially regarding back-up**. In fact, we would like to further emphasize that explicitly leveraging such back-up capability to propose a principled approach is a key contribution of our work. Below we clarify in more detail.\n\n**Q1. Without environment models, one must roll out with real environments where back-up is not feasible.**\n\nWe would like to emphasize that this is not true, at least for the environments we consider. **Our experiments cover API-calling / tool-use, programming, and web-browsing**. These are core benchmarks for LM agents precisely due to their practical value, and LATS can be directly deployed in real-world counterparts. **In WebShop, the environment can be reset to a prior state by changing the xml of the website; and for programming and API-usage, the actions of the LM do not directly affect the environment, so backing up is a matter of changing the prompt**. \n\n**Q2. Being able to back-up is not the property of decision-makers, but the environment.** \n\nThis is true, as one could of course use a more \u201ctraditional RL agent\u201d, e.g., a LSTM network with a small MLP as the decision head as an actor for the task and back-up. However, **the types of environment that we consider and allow us to back-up is closely related to LM**; for example, HotPotQA and webshop are both recent benchmarks that are considered as NLP tasks; they are largely ignored by the mainstream RL community, and it is only through large language models that the machine learning community gains major progress on solving such tasks. Like we acknowledged in the initial response, the only class of benchmarks where backup is indeed hard are RL environments like AlfWorld and Minecraft, where the agent\u2019s actions permanently change the environment \u2013 however, it is important to recognize that this scenario represents only a subset of the environments in which LM agents can be deployed. Thus we feel appropriate to say \u201cLM agents are not constrained by concerns in normal RL\u201d, as the normal RL community does not focus on our task of interest. \n\n**Q3. We should be clearly upfront about prior works overlooking property.**\n\nWe would like to emphasize we have already included this. \n\nIn our original submission:\n\n1. In the contribution part ,we write \u201c... construct the best trajectory from sampled actions, enabling more flexible and adaptive problem-solving compared to reflexive prompting methods\u201d, where reflexive means, as we write in the second paragraph, \u201cDespite this strength, these methods are reflexive and fall short of humans' deliberate and thoughtful decision-making characteristics to solve problems. In particular, such methods **fail to consider multiple reasoning paths** or to plan ahead.\u201d\n\n2.  In the \u201ctree-based search\u201d section of related work, we write \u201csuch a problem does not exist for LM tasks as we can conveniently backup to any state by setting the input to be the context and corresponding previous output by the LM.\u201d  \n\nIn our updated submission:\n\n1. We made a comparison of our work and prior works in Tab. 1, where we list the use of \u201cplanning\u201d as a feature and explain that \u201cWe refer to planning as the use of a search algorithm\u201d. We mark this to be true for ToT, RAP and LATS, while false for all other methods.\n\n2. We add the following text to the introduction of MCTS: \u201cHowever, such a limitation does not exist for LMs, as we can conveniently reset to any step by simply copy-pasting historical text input. Such a special property is the key motivation of our work.\u201d\n\nWe will further emphasize this point in the revision.\n\n**Q4. a fairer comparison will be considering # of steps you need to be correct.**\n\nBelow shows the comparison of the sample complexity (asymptotic token consumption, where $n$ is the node expanded every step and $k$ is the number of rollouts), the number of nodes expanded upon success and token consumption, which is also updated in the Tab. 7 of our submission. Compared to ToT and Rap with ReAct prompt, Our method uses less nodes and tokens upon success.\n\nMethod | Performance on HotPotQA (&uarr;) | Sample Complexity (&darr;) | Average \\# of Nodes (&darr;) | Token Consumption (&darr;)\n---|---|---|---|---\nReAct (best k = 250) | 0.42 | $O(k)$  | - | -\nCoT-SC (n = 1, k = 250) | 0.40 | $O(k)$ |  - | -\nLATS (n = 1, k = 50) | 0.48 | $O(k)$ | - | -\nToT - ReAct (n = 5, k = 50) | 0.49 | $O(kn)$ | 84.05 | 210215\nRAP - ReAct (n = 5, k = 50) | 0.54 | $O(kn)$ | 70.60 | 176500\nLATS (n = 5, k = 50) | 0.61 | $O(kn)$ | 66.65 | 173290\n\nWe hope this response has addressed the reviewer's remaining concerns."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700703394193,
                "cdate": 1700703394193,
                "tmdate": 1700703480107,
                "mdate": 1700703480107,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UOl4rQVObT",
                "forum": "6LNTSrJjBe",
                "replyto": "Go3K9HIJaY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4113/Reviewer_j4Tk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4113/Reviewer_j4Tk"
                ],
                "content": {
                    "comment": {
                        "value": "It is certainly not a confusion about the emerging paradigm of using LLMs as agents. \nI am super familiar with both classical RL and emerging LLMs. \n\n\"LLMs as agents\" only changes the policy part of RL, but not the environment, is it right? \n\nThen why may my arguments be valid in classical RL settings but not in this emerging setting? \n\nYou said \"This is true\" to my question about \"being able to back up is a property of environment\", right? \n\nI have been trying to keep my feedback concise and precise. \n\nI appreciate that you have written a lot in your response, but the most important thing is actually quite concise here: if back-up is a property of environments, then it doesn't make sense to have a model-free MTCS algorithm for decision-making, because whether or not to do any search is completely determined by the environment, but not your policy / agent. This is the most fundamental problem of this paper, and this is what makes the paper below the acceptance bar."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700717353804,
                "cdate": 1700717353804,
                "tmdate": 1700717353804,
                "mdate": 1700717353804,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hXiw59t3xG",
                "forum": "6LNTSrJjBe",
                "replyto": "reihdNJvhp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4113/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4113/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer j4Tk"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the quick and concise response. What we are trying to say is that we do not feel \"completely determined by the environment\" is a fundamental flaw, because our solution already applies to a wide range of LM benchmarks where backup property exists. Some prior methods that focus on only a subset of our tasks of interest have been published at ICLR in the past [1, 2, 3]. We are not developing a universal RL method that applies to all environments, but a language model agent that does decision-making for its corresponding tasks of interest - and we solve them well.\n\n**References:**\n\n [1] Zhang, S., Chen, Z., Shen, Y., Ding, M., Tenenbaum, J.B., & Gan, C. Planning with Large Language Models for Code Generation. ICLR 2023.\n\n[2] Chen, B., Zhang, F., Nguyen, A., Zan, D., Lin, Z., Lou, J., & Chen, W. CodeT: Code Generation with Generated Tests. ICLR 2023.\n\n[3] Haluptzok, P.M., Bowers, M., & Kalai, A.T. Language Models Can Teach Themselves to Program Better. ICLR 2023."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728005984,
                "cdate": 1700728005984,
                "tmdate": 1700728015563,
                "mdate": 1700728015563,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "V8gLiBirng",
            "forum": "6LNTSrJjBe",
            "replyto": "6LNTSrJjBe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4113/Reviewer_3Lmb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4113/Reviewer_3Lmb"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new framework called Language Agent Tree Search (LATS) to improve the reasoning and decision-making abilities of large language models (LLMs). Specifically, LATS is a framework that incorporates self-reflection and tree-of-thoughts into LLM-based agent problems. Evaluations across diverse tasks like programming, HotPotQA, and WebShop show LATS effectively harnesses LLM capabilities for reasoning and decision-making."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The writing is overall clear and easy to follow.\n2. The author provides sufficient technical details to understand the LATS framework and reproduce results.\n3. The authors evaluate LATS extensively across diverse tasks like programming, HotPotQA, and webshop to demonstrate generality and superiority."
                },
                "weaknesses": {
                    "value": "1. The idea is overall not that novel considering previous work like RAP and Reflextion.\n\n2.  As far as I can see, LATS definitely has much more token consumption compared with other baselines when sampling the same amount of trajectories. I think the author should try to increase the token consumption used in other baselines. For example, report the overall token consumption and try to increase the K set in CoT-SC so it may consume tokens on a similar scale to LATS.\n\n3. The author should try to at least incorporate one baseline with the external environment feedback as the ablation study. \n\n4. The author could provide more ablation studies to analyze the impact of different components, for example, search depth, exploration factor etc..\n\n4. The limitation of LATS is not fully addressed. For example, it seems the current version LATS cannot scale to large-scale problems."
                },
                "questions": {
                    "value": "1. Since the author utilizes LLM itself as the evaluation metric, how do you think of recent works that indicate LLM may not be good at self-critique, for example, Huang et al. 2023 mention that in the experiment of reflection (section 3.1.3 in Huang's paper), they use the correct answer as the criteria to stop the self-correction loop, which is not fair as I think. How do you handle this question during the evaluation of value function in LATS?\n\n2. I understand that LATS leverages the final environment feedback to guide the MCTS search (especially in the backward process). The final result of programming and Webshop is accessible (since you have the simulator), but how can you get the feedback on HotpotQA? Will the HotpotQA environment tell you whether your answer is right or not? If so, it means that you are using a ground-truth answer during the MCTS search process which is completely not reasonable (tbh, this is also related to the phenomenon of using the correct answer as the criteria in my question 1). Could the author elaborate more on what the environment feedback looks like in the HotpotQA environment?\n\nReference\nHuang, Jie, et al. \"Large language models cannot self-correct reasoning yet.\" arXiv preprint arXiv:2310.01798 (2023)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4113/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4113/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4113/Reviewer_3Lmb"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4113/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698749469370,
            "cdate": 1698749469370,
            "tmdate": 1699636376004,
            "mdate": 1699636376004,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aJSgMjMVVA",
                "forum": "6LNTSrJjBe",
                "replyto": "V8gLiBirng",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4113/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4113/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3Lmb (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful comments. Below we address each of the reviewer\u2019s concerns.\n\n**Q1. The idea is not that novel considering previous work like RAP and Reflexion.**\n\nAlthough previous work has investigated either search algorithms or self-reflection, the combination of both has not been explored. In contrast, LATS synergistically integrates them, and more importantly, introduces unique and advanced methods to leverage search algorithms and self-reflection, and makes non-trivial adaptation of tree-based search with external feedback. Based on these, we have made significant improvements across multiple testbeds, including 94.4 pass@1 accuracy with GPT-4 on HumanEval and performance comparable to gradient-based fine-tuning methods on WebShop.\n\nMore concretely, LATS is distinguished from prior work through the following key novelties:\n\n**1. Paradigm contributions.** To the best of our knowledge, we are **the first to explore the use of search algorithms with LLM agents for decision-making tasks**. Previous approaches like ReAct and Reflexion cannot sample and select more than one state at every step, limiting exploration in decision-making environments; ToT and RAP do not explore decision-making tasks. It is non-trivial to shift from non-interactive tasks to interactive ones, because an updated node, prompt, and more importantly, search algorithm must be designed to incorporate external input \u2013 an achievement we have successfully accomplished. We verify that our proposed method accommodates the external input, even when applied to original non-interactive tasks such as programming. \n\n**2. Our LATS improves the use of MCTS.** Compared to RAP, LATS also improves the use of MCTS. In RAP, states are scored with a value function based on self-consistency, while we directly prompt the LM to score the state with justification. Our design is more effective in decision-making, which enables the state to also incorporate an external observation that adds context for a more accurate score. Additionally, LATS samples states through environment interaction, eliminating the need to use the LM as a world model as in RAP. This reduces the risk of error and makes LATS a general LLM framework.\n\n**3. Our LATS improves the use of self-reflection.** Compared to the self-reflection in Reflexion, LATS leverages semantic feedback and failed trajectories in additional ways. These are also added to the context of LM for the value function, thereby refining subsequent state evaluations in addition to the base decision-making.\n\n**4. Our adaptation of search algorithms to external feedback is not trivial, as simply adapting existing tree-based methods, such as ToT and RAP, to external feedback hurts the performance.** In our updated submission, we design a version of ToT and RAP that can incorporate environment feedback in HotPotQA using the ReAct prompt, and find that the new version of ToT and RAP generally performs worse than the reasoning-only setting of HotPotQA (0.55 vs. 0.49 for ToT, and 0.6 vs. 0.54 for RAP). This result is because the information retrieval setting is harder than the multihop question-answering setting, and it highlights that our efforts of adapting search algorithms to decision-making scenarios is non-trivial. The detailed numbers are listed under Q3 as well as Tab. 5 of our updated submission.\n\n\n**Q2. LATS has more token consumption than other baselines.**\n\n**1. Theoretically, LATS consumes more token than CoT-SC but has the same token consumption as RAP and ToT.** The token consumption of LATS is higher relative to other simpler baselines (e.g., CoT-SC), as LATS expands with multiple samples every step; the overall token consumption is $n$ times higher than CoT-SC/ReAct with $k$ attempts, where $n$ is the number of samples per expansion. However, as the token cost of other tree-structured search methods (RAP and ToT) are also parameterized by the number of attempts $k$ and the number of nodes expanded $n$, it means that **LATS, RAP, and ToT have the same sample complexity, i.e., same asymptotic token cost**. We summarize the sample complexity in the table below, which is also added to Tab. 7 of our revised submission.\n\n**2. Empirically, LATS expands less nodes upon success than RAP and ToT, which means that it is cheaper.** To count the token consumption more accurately,  we compare the average number of nodes expanded upon success on HotPotQA for LATS, RAP, and ToT. We find that LATS does not only have better performance, but also tends to solve questions with fewer nodes (66.65) sampled than RAP (70.60) or ToT (84.05) with the same sample complexity, indicating a more effective, thus cheaper, search. The result is also shown in the table below.\n\n(Q2 continued in next part)"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549632163,
                "cdate": 1700549632163,
                "tmdate": 1700549632163,
                "mdate": 1700549632163,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hCigBRFWUC",
                "forum": "6LNTSrJjBe",
                "replyto": "V8gLiBirng",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4113/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4113/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3Lmb (Part 2)"
                    },
                    "comment": {
                        "value": "**3. Even LATS without expansion of multiple nodes (i.e., $n=1$) outperforms other methods with the same sample complexity (CoT-SC, best among multiple runs for ReAct).** To give a fairer comparison between LATS and CoT-SC which does not expand nodes, we also test a version of LATS with $n=1, k=50$, and a version of CoT-SC and ReAct with $k=50*5=250$. We find that ReAct and CoT-SC cannot reach the performance of LATS with lower $k$ and $n=1$, let alone LATS with the same sample complexity ($n=5, k=50$).\n\nBelow is the table mentioned above:\n\nMethod | Performance on HotPotQA (&uarr;) | Sample Complexity (&darr;) | Average \\# of Nodes (&darr;)\n---|---|---|---\nReAct (best k = 250) | 0.42 | $O(k)$  |\nCoT-SC (n = 1, k = 250) | 0.40 | $O(k)$ | \nLATS (n = 1, k = 50) | 0.48 | $O(k)$ |\nToT - ReAct (n = 5, k = 50) | 0.49 | $O(kn)$ | 84.05\nRAP - ReAct (n = 5, k = 50) | 0.54 | $O(kn)$ | 70.60\nLATS (n = 5, k = 50) | 0.61 | $O(kn)$ | 66.65\n\n**Q3. Try to incorporate baselines with the external environment feedback as the ablation study.**\n\n1. First, we would like to remind the reviewer that external feedback and observations are already used in ReAct and Reflexion. ReAct is a simple baseline that applies LM to interactive environments, and Reflexion extends ReAct with LM-generated semantic feedback.\n\n2. Second, we would like to point out that ToT and RAP **do not natively** support feedback. Following the reviewer\u2019s suggestion, we design a version of ToT and RAP that can incorporate environment feedback in HotPotQA using the ReAct prompt, which we denote as **ToT (ReAct) and RAP (ReAct)**. It is important to note that adapting RAP to this setting is non-trivial, as RAP is designed for scenarios where the LM can be repurposed as a world model. To address this issue, we follow the strategy in our proposed LATS and directly use environmental interactions during sampling, but keep the original value function of RAP. The result is shown below (and also Tab. 5 of our updated submission):\n\nMethod | HotPotQA\n---|---\nToT (ReAct) | 0.49\nToT | 0.55\nRAP (ReAct) | 0.54\nRAP | 0.60\nLATS (DFS) | 0.53\nLATS (No self-reflection) | 0.56\nLATS | 0.61\nLATS (CoT + ReAct) | 0.71\n\nThe results show the following findings:\n\n1) The DFS version of LATS outperforms ToT (ReAct), indicating the importance of self-reflection in the acting setting;\n\n2) There is an improvement over RAP (ReAct) with the version of LATS without self-reflection, reflecting the importance of our improved value function in MCTS;\n\n3) The baselines generally perform worse than the reasoning-only setting of HotPotQA (0.55 vs. 0.49 for ToT and 0.6 vs. 0.54 for RAP), which indicates that the acting-based setting is more challenging and the adaptation of search algorithms to decision-making scenarios is non-trivial.\n\n**Q4. More ablation studies, e.g., search depth, exploration factor, etc.**\n\nThanks for the suggestion. In the updated submission, we have added Tab. 6 with additional results on search depth and exploration factor for HotPotQA and added the \u201clearning curve\u201d of performance growth with respect to the increase of the number of attempts $k$ on HumanEval in Appendix C (now Fig. 4). The results show that LATS is robust to hyperparameters and scales better with more iterations than Reflexion.\n\n**Q5. Scalability concern of LATS.**\n\nWe thank the reviewer for pointing out this possible concern; we have updated our existing limitations section in Sec. B in the Appendix accordingly. However, we would like to point out that 1) Our method is scalable at least on par with RAP/ToT while outperforming them, and 2) **the sample size $n$ for expansion provides a natural, linear trade-off between computational cost and performance**, which can be decreased for large-scale tasks with limited resources. Specifically, when $n=1$, our method is as scalable as ReAct for the same number of trajectories and still performs better as shown in Q2.\n\n\n**Q6. Recent work suggests that LLM may not be good at self-reflection, which may affect the value function in LATS.**\n\nThanks for bringing the work of Huang et al. 2023 to our attention, though we kindly note that Huang et al. 2023 was released on arXiv after the ICLR submission deadline. We have updated the related work section with this work accordingly. Importantly, the findings presented in Huang et al. 2023 and our work are not contradictory but rather complementary:\n\n1. Huang et al. 2023 focuses on self-reflection *in the internal reasoning setting*, where there is no external feedback. In contrast, in settings like ours, the LM *has access to an external environment*; for instance, in LATS, the LM has access to test case feedback in programming and API feedback in HotPotQA, so it does not rely solely on itself for self-correction. This is another improvement of LATS over ToT/RAP, as Huang et al. 2023 suggest that LLMs are not good at evaluating their own outputs **without external feedback**.\n\n(Q6 continued in next part)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549689787,
                "cdate": 1700549689787,
                "tmdate": 1700549689787,
                "mdate": 1700549689787,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1P4aLYXUlF",
                "forum": "6LNTSrJjBe",
                "replyto": "V8gLiBirng",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4113/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4113/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3Lmb (Part 3)"
                    },
                    "comment": {
                        "value": "2. Huang et al. 2023 actually supports the argument that self-evaluation of LLM *with external feedback* makes sense. In their discussion section, they provide a list of papers that successfully utilizes self-correction with external feedback, including Gou et al. 2023, Chen et al. 2023, Olausson et al. 2023, Pan et al. 2023, and so on; they also notice that corrections with feedback is a common paradigm for everyday use of LLM. They conclude that \u201cUtilizing this type of feedback, though not perpetually accessible, to assist LLMs in correcting their responses is intuitively beneficial, particularly when the feedback is of high quality.\u201d \n\n**Q7. Getting feedback on the answer correctness of HotpotQA is unreasonable.**\n\nWe thank the reviewer for the comment and understand the reviewer\u2019s concern. Below, we would like to clarify the concern from two aspects:\n\n1. We would like to emphasize that **we match the setting used in Reflexion and ReAct for HotpotQA**. The environmental feedback in HotpotQA consists of all feedback from the API, including whether the answer was correct. It is important to note that we use the same environment for every method, and the use of the correctness labels is the same as that in Reflexion.\n\n2. The HotpotQA environment is **designed to evaluate information retrieval ability in decision-making** settings. Given the nature of decision-making, it is more reasonable for the environment to provide feedback based on the ground truth or task completion. Note that this might be different from the standard QA setting; to prevent any potential confusion, we have removed the supervised SOTA entry in Tab. 2 of our updated submission to ensure a fair comparison.\n  \nWe hope that our response has addressed the reviewer\u2019s questions and concerns. We are happy to answer any further questions.\n\n**References:**\n\nJie Huang et al. Large language models cannot self-correct reasoning yet. arXiv:2310.01798, 2023.\n\nZhibin Gou et al. Critic: Large language models can self-correct with tool-interactive critiquing. arXiv:2305.11738, 2023. \n\nXinyun Chen et al. Teaching large language models to self-debug. arXiv:2304.05128, 2023. \n\nTheo X Olausson et al. Demystifying gpt self-repair for code generation. arXiv:2306.09896, 2023.\n\nLiangming Pan et al. Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. arXiv:2308.03188, 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549753834,
                "cdate": 1700549753834,
                "tmdate": 1700549753834,
                "mdate": 1700549753834,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ET291cnC6S",
                "forum": "6LNTSrJjBe",
                "replyto": "1P4aLYXUlF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4113/Reviewer_3Lmb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4113/Reviewer_3Lmb"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response and revision\n\nFor Q2.3, what do k,n represent here? In addition, apart from the node visiting number, the actual token consumption is more important (because you mean consume more tokens in each node's expansion.)\n\nFor Q7, I understand that previous work adopted HotpotQA but it is still not reasonable. Especially considering that in the real world, the QA task won't have any external feedback like the ground-truth label. I would suggest the author try to replace this experiment with some other ones."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4113/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654974342,
                "cdate": 1700654974342,
                "tmdate": 1700654974342,
                "mdate": 1700654974342,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]