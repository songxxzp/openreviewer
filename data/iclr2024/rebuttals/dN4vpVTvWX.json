[
    {
        "title": "TUVF: Learning Generalizable Texture UV Radiance Fields"
    },
    {
        "review": {
            "id": "KIWYOgCHCJ",
            "forum": "dN4vpVTvWX",
            "replyto": "dN4vpVTvWX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission85/Reviewer_K7dY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission85/Reviewer_K7dY"
            ],
            "content": {
                "summary": {
                    "value": "This work studies generating high-fidelity textures of 3D  shapes. \n\nIt introduces TUVF that generates textures in a learnable UV sphere space, which allows the texture to be disentangled from the underlying shape and transferable to other shapes from the same category. \n\nIt uses a sampled texture code that represents a particular appearance style adaptable to different shapes and generates the texture in a canonical UV sphere space. It learns a canonical surface auto-encoder that maps any point on a canonical UV sphere to a point and normal on an object\u2019s surface, which is transformed to indicator function values using the Poisson Surface Reconstruction algorithm and further transformed to density. This step is learned by Chamfer Distance on the surface points and the L2 losses on the indicator grid. Finally, the texture is learned by neural rendering with a patch-based discriminator.\n\nThe correspondence between the UV space and the 3D shape is automatically established during training."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "TUVF achieves much more realistic, high-fidelity, and diverse 3D consistent textures compared to previous approaches.\n\nIt achieves state-of-the-art results in both the experiments on synthetic and real-world object datasets.\n\nIt improves texture control and editing.\n\nThe use of the Poisson Surface Reconstruction algorithm is very interesting and contributes insights for the community."
                },
                "weaknesses": {
                    "value": "There still exist distortions in texture.\n\nThe contribution of texture generation is limited due to recent text-driven texture synthesis work, such as Text2Tex and TEXTure: Text-Guided Texturing of 3D Shapes."
                },
                "questions": {
                    "value": "My main concern is about the contribution. The contribution of this work versus the work of text-driven texture synthesis should be further clarified."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission85/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission85/Reviewer_K7dY",
                        "ICLR.cc/2024/Conference/Submission85/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission85/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698250723645,
            "cdate": 1698250723645,
            "tmdate": 1700655221151,
            "mdate": 1700655221151,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qGirG44sWU",
                "forum": "dN4vpVTvWX",
                "replyto": "KIWYOgCHCJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission85/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission85/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer K7dY"
                    },
                    "comment": {
                        "value": "**Q**: There still exist distortions in texture.\n\n**A**: While distortions are a common issue in UV-based methods, our approach has notably improved in reducing seams and distortions.\n\nThe key factors contributing to this reduction include:\n\n* **Unified UV Mapping**: Unlike previous methods that cut shapes into pieces, we ensure that the UV mapping is shared across all parts of the shape. This results in a seamless appearance without distinct boundaries.\n\n* **Non-linear Mapping**: Our UV mapper employs a non-linear mapping function trained using Chamfer loss. This approach implicitly connects UV coordinates without the need for explicit stitching lines.\n\n* **Local Radiance Field**: Our MLP$_{F}$ incorporates local coordinates as an additional input, representing a local radiance field. This effectively minimizes seams in the generated textures.\n\nIt's important to note that while these design choices significantly reduce seam and distortion issues, they may not entirely eliminate them. In the future, we could improve by using better data-driven techniques (e.g., diffusion prior) or more advanced neural rendering methods (e.g., ray transformers), as we mentioned in Section 5 of the main paper.\n\n---\n\n**Q**: The contribution of texture generation is limited due to recent text-driven texture synthesis work, such as Text2Tex and TEXTure: Text-Guided Texturing of 3D Shapes.\n\n**A**: Please refer to the [General Response (B)](https://openreview.net/forum?id=dN4vpVTvWX&noteId=zW1K8ECNIl) for justification. We provide quantitative and qualitative evaluations and discussions highlighting the differences between recent text-driven texture synthesis works and TUVF. Our analysis concludes that these are distinct lines of work within TUVF (links: [AUV-Net](https://openreview.net/forum?id=dN4vpVTvWX&noteId=4Xylia94Au), [TEXTure](https://openreview.net/forum?id=dN4vpVTvWX&noteId=4Xylia94Au), [Point-UV Diffusion](https://openreview.net/forum?id=dN4vpVTvWX&noteId=uBGrwFvF1o), [Text2Tex](https://openreview.net/forum?id=dN4vpVTvWX&noteId=uBGrwFvF1o))."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission85/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470302276,
                "cdate": 1700470302276,
                "tmdate": 1700470986143,
                "mdate": 1700470986143,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xzzieQWtY3",
                "forum": "dN4vpVTvWX",
                "replyto": "qGirG44sWU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission85/Reviewer_K7dY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission85/Reviewer_K7dY"
                ],
                "content": {
                    "comment": {
                        "value": "I raise my rating. Thanks to the authors for the extra details in the rebuttal, particularly the comparison of Text2Tex and TEXTure in General Response (B). It would be helpful to include these and the reply of factors contributing to reducing seams and distortions in the camera-ready paper/supplement."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission85/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653178700,
                "cdate": 1700653178700,
                "tmdate": 1700653178700,
                "mdate": 1700653178700,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aQk5y2iyJ0",
            "forum": "dN4vpVTvWX",
            "replyto": "dN4vpVTvWX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission85/Reviewer_VxE5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission85/Reviewer_VxE5"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method to learn a category-level latent space of both canonical surface and UV texture fields, from a 3D shape dataset and an unpaired collection of 2D images depicting this category of shapes. There are mostly three model components: (1) a canonical surface autoencoder that encodes the ground-truth shape into a latent code, which then gets decoded into points on a sphere, (2) a generative model that produces texture features conditioned on the style code, and (3) a differentiable radiance field rendering module that renders the generated texture onto the autoencoded shape, producing an image that can be compared against real images of the same shape category.\n\nFor the canonical surface autoencoder, Chamfer distance is used in this autoencoding task, and the latent code learned should ideally \"instruct\" us where on the canonical sphere each surface point falls onto, therefore providing a common space to anchor all shapes within the class (i.e., dense correspondence). The authors opted to turn point clouds into density volumes with differentiable Poisson surface reconstruction.\n\nFor the texture feature generator, the authors adopted StyleGAN-like style injection and made sure there's no interaction between neighboring pixels since vicinity in the UV space is often not physically meaning. RGB colors are not explicitly decoded from these features until the final rendering.\n\nFor the differentiable rendering module, the authors chose to adopt volume rendering even if the native shape representation is mesh from ShapeNet. Because of that, the authors discussed how to efficiently sample rays (rendering only near the object surface), convert point clouds into volume density (with differentiable Poisson surface reconstruction), and define radiance fields for points (via interpolating nearest surface points).\n\nThe authors show reasonable qualitative results where they can transfer texture to another shape in the category, make 3D-consistent texture edits, and find dense correspondence among shapes in the same class."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper does a good job presenting what is done with helpful visuals and is easy to follow.\n\nIt also tackles an interesting problem where one needs to learn a canonical space for a category shapes and simultaneously put textures onto the shapes, without paired 3D-2D data. By leveraging autoencoding and adversarial learning, the model learns meaningful patterns/correlations without explicit, direct supervision.\n\nDense correspondence emerging from autoencoding is also interesting and makes sense."
                },
                "weaknesses": {
                    "value": "I have two significant concerns that need addressing before I can consider raising my ratings.\n\nWhile I understand how canonical surface autoencoding eventually leads to a mapping between a given shape and the canonical sphere, for rigid shapes like cars and airplanes, one can project the shape onto an enclosing sphere, e.g., via raycasting from the sphere to the shape, to obtain similar mappings -- \"similar\" as in all cars' front bumpers mostly map to the same location on the canonical sphere. If this simple approach produces similar results, the whole canonical surface autoencoding part becomes invalid. \n\nI understand the authors also show mappings learned for non-rigid objects like humans and animals, but these are all parametric models (SMPL or SMAL or whatever), so the canonical space for them is by definition well established and doesn't benefit from this work.\n\nThe second major concern is the whole deal of converting meshes or point clouds into density volumes. Volumetric approaches like NeRF are cool but I don't think everything needs to or should be volumetric. If we already have meshes, why throw away the face information, go into the point cloud regime, and then make the points volumetric? Each of these steps is lossy and complicates the method in an unnecessary way in my opinion. A concrete, much simpler alternative is retaining the face information for the points on the canonical sphere and use the same faces in the decoded shape. Then, we don't need differentiable surface reconstruction to return to the mesh domain in a suboptimal manner. What confuses me further is the adoption of volume rendering. With this alternative I proposed, one simply renders the mesh, which will be much more computationally efficient.\n\nIf these two concerns/confusions don't get cleared up, I view this paper as over-engineering a problem that could be tackled in a cleaner and simpler way, possibly also compromising the final quality given the extra lossy steps taken."
                },
                "questions": {
                    "value": "As mentioned in \"Weaknesses,\" can we produce a mapping without learning by just raycasting from an enclosing sphere or something similar? I can see self-occlusion might be a blocker, but the learned mapping is not perfect either; this simpler alternative seems to deserve a try. \n\nWhy do we go through the complicated pipeline to make the shape volumetric? This looks like an even more severe issue than the first question. I hope we have a good justification (or I'm misunderstanding the paper); our community would hate to see \"volumetric == cool\" as the motivation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission85/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission85/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission85/Reviewer_VxE5"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission85/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698731305003,
            "cdate": 1698731305003,
            "tmdate": 1700633967038,
            "mdate": 1700633967038,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JoeQ2MU2sd",
                "forum": "dN4vpVTvWX",
                "replyto": "aQk5y2iyJ0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission85/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission85/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer VxE5"
                    },
                    "comment": {
                        "value": "**Q**: For rigid shapes like cars and airplanes, one can project the shape onto an enclosing sphere to obtain similar mappings -- \"similar\" as in all cars' front bumpers mostly map to the same location on the canonical sphere. If this simple approach produces similar results, the whole canonical surface autoencoding part becomes invalid. \n\n**A**: As per the reviewer's suggestion, we develope a new baseline method to establish a correspondence mapping by casting rays from a sphere to shapes. We test the baseline approach on the KeypointNet dataset and evaluate its effectiveness by conducting experiments on the semantic keypoint transfer task. The table below showcases the qualitative results we obtained on Cars. Our findings suggest that this method can generate reasonable results for some samples, but it is prone to errors when it comes to local areas due to sensitivity to scale and offset. Moreover, we have observed noticeable seams while performing texture transfer with synthetic textures, which we show an example [here](https://drive.google.com/file/d/10PhutLoShKmxcoO-Ck-RI0-YyLHdWroq/view?usp=drive_link).\n\n| ShapeNet Car (Image) | ShapeNet Car (Video) |\n| -------- | -------- |\n|[results](https://drive.google.com/file/d/1xyVqqlm2xI_FSOd1xhwlp1LfSx-M_uUW/view?usp=drive_link)|[results](https://tuvf4iclr.github.io/carkpt)|\n\n\nTo further quantitatively evaluate the method, we measure the correspondence accuracy at different error thresholds, and the results are presented below. Additionally, we experimented with various radius parameters to improve the results. The findings indicate that the radius length significantly influences the correspondence accuracy. This is because only sparse points are cast to the surface when the sphere is large. And if the sphere is small, seams and occlusion will occur more frequently. Despite the variations in radius length, the Canonical Point Auto-encoder outperformed all other variants.\n\n| ShapeNet Airplane | ShapeNet Car | ShapeNet Chair |\n| -------- | -------- | -------- |\n| [results](https://drive.google.com/file/d/1BMD5FMIgmX6CQy0oAJt_6AalQDrzprpQ/view?usp=drive_link)     | [results](https://drive.google.com/file/d/1rc6oBhffNd-K_Nk7P7W6-p5xke8rouiX/view?usp=drive_link)     | [results](https://drive.google.com/file/d/1RBL5jQ64FJJdEHWekfG7ePWU-JuSfMhu/view?usp=drive_link)     |\n\n---\n**Q**: I understand the authors also show mappings learned for non-rigid objects like humans and animals, but these are all parametric models (SMPL or SMAL or whatever), so the canonical space for them is by definition well established and doesn't benefit from this work.\n\n**A**: As mentioned in our main paper, we have also evaluated generic man-made objects, including cars and chairs. Since these are topology-varying objects, it is not easy to fit them with parameterized primitives. As a result, these objects lack a clear, well-defined canonical space. Therefore, our work is particularly useful for these objects.\n\n---\n**Q**: Volumetric approaches like NeRF are cool but I don't think everything needs to or should be volumetric. If we already have meshes, why throw away the face information, go into the point cloud regime, and then make the points volumetric? Each of these steps is lossy and complicates the method in an unnecessary way in my opinion.\n\n**A**: We chose to use NeRF not because it's trendy but because it's the most intuitive option available. In [General Response (A)](https://openreview.net/forum?id=dN4vpVTvWX&noteId=HLOybPoVgu), we conducted a series of studies exploring alternative approaches with mesh representation. We found that without NeRF, it's not straightforward to maintain correspondence on mesh faces while ensuring good geometry. Therefore, using a continuous representation is a logical and intuitive choice and the best option available. We want to re-emphasize that no steps in our method are unnecessary. We use a point cloud regime because it can better represent the topology-varying geometry and correspondence compared to a mesh. Additionally, we make the points volumetric to maintain surface correspondence for rendering.\n\n\n---\n**Q**: A concrete, much simpler alternative is retaining the face information for the points on the canonical sphere and use the same faces in the decoded shape. Then, we don't need differentiable surface reconstruction to return to the mesh domain in a suboptimal manner. \n\n**A**: We follow the reviewer's suggestion and develop a new mesh-based baseline that uses the face information from the canonical sphere to form the reconstructed shape. However, we find that such a design is not feasible. The Canonical Point Auto-encoder allows complex transformations like folding or cutting. Therefore, using the same faces as the points on the canonical sphere would lead to incorrect meshes. Please refer to [General Response (A)](https://openreview.net/forum?id=dN4vpVTvWX&noteId=HLOybPoVgu) for details and results."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission85/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470228820,
                "cdate": 1700470228820,
                "tmdate": 1700474546150,
                "mdate": 1700474546150,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qV6uiDEFgf",
                "forum": "dN4vpVTvWX",
                "replyto": "aQk5y2iyJ0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission85/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission85/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer VxE5,\n\nThank you once again for the detailed feedback.\nWe are approaching the end of the author-reviewer discussion period. However, there are no responses yet to our rebuttal.\n\nPlease feel free to request any additional information or clarification that may be needed. We hope to deliver all the information in time before the deadline.\n\nThank you!"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission85/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633784456,
                "cdate": 1700633784456,
                "tmdate": 1700633784456,
                "mdate": 1700633784456,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Fj1KT91UYb",
                "forum": "dN4vpVTvWX",
                "replyto": "JoeQ2MU2sd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission85/Reviewer_VxE5"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission85/Reviewer_VxE5"
                ],
                "content": {
                    "title": {
                        "value": "Raising my rating"
                    },
                    "comment": {
                        "value": "I thank the authors for the additional experiments and helpful visualization to prove their points. Although I'm still slightly concerned about over-engineering, I'm convinced that this paper proposes a valid approach to a complex problem. Therefore, I'm willing to raise my rating provided that the authors will include all of these new experiments in their final manuscript."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission85/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633934833,
                "cdate": 1700633934833,
                "tmdate": 1700633934833,
                "mdate": 1700633934833,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aPG6ro6ABs",
            "forum": "dN4vpVTvWX",
            "replyto": "dN4vpVTvWX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission85/Reviewer_KY39"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission85/Reviewer_KY39"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a textured shape generation method based on learned UV mapper and neural texture features. \nThe training is supervised by images rendered on 2D space. \nAfter training the method can support random textured generation for a given shape, texture editing, and texture transfer. \nThough the proposed pipeline is supported with a large mount of qualitative and quantitative results. \nI find is that the paper's technical contribution is little and the results shown in the paper are OK but not exciting(not comparable to recent diffusion based method). \nMost aspects have already been explored by previous papers(some important references are missing). \nIn addition, the intuition of using neural radiance field is really unclear. \nI do not feel using neural radiance field can bring any benefit since every property before the rendering is on the surface. \nThus, I am leaning towards rejection but also listen suggestions from other reviewers."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is overall clear with a good structure. \nReaders can follow the text easily.\nThe paper shows a lot of results and comparisons, which makes the pipeline more convincing. \nDetails of the network architecture are given in the supp, making reproduction easier."
                },
                "weaknesses": {
                    "value": "The biggest issue is that the paper is not novel. \nMost part in the paper has been explored in previous papers. \nThough well combined, it only produces OK results instead of exciting results. \nFor example, recent PointUVDiffusion (Texture Generation on 3D Meshes with Point-UV Diffusion) can generate very realistic textures for a 3D shape.\nThough this paper can support more things like texture transfer via its UV mapper. \nBut actually, correspondence between shapes can also be obtained by postprocessing.\nSo I am wondering how the method compares to PointUVDiffusion.\n\nAnother issue is the intuition of using NeRF. \nThough NeRF is hot and can reconstruct 3D scenes very well and provide vivid results, for this given task, I strongly feel that NeRF is not necessary because everything before the rendering process is defined on the surface. \nWhy not use surface-based rendering?\nIs there any benefit to using NeRF (for example, view-dependent texture)? I do not see such results.\nFor example, AUV-Net learns an aligned UV space, which is pretty similar to the proposed pipeline, though it does not use a neural radiance field. \nSo I think comparing it to AUV-Net should be necessary to prove the advantage of NeRF. \n\nA formulation of the rendering process would be better in Sec. 3.2 or Sec3.3.\n\nReference:\nSINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing Field"
                },
                "questions": {
                    "value": "See the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission85/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission85/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission85/Reviewer_KY39"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission85/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698848184207,
            "cdate": 1698848184207,
            "tmdate": 1700695316260,
            "mdate": 1700695316260,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3p0cDE3ZO9",
                "forum": "dN4vpVTvWX",
                "replyto": "aPG6ro6ABs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission85/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission85/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer KY39 (1/2)"
                    },
                    "comment": {
                        "value": "**Q**: The biggest issue is that the paper is not novel. Most part in the paper has been explored in previous papers.\n\n**A**: We respectfully disagree with the reviewer. This comment can be applied to most deep-learning papers. Most new technologies proposed in the past 10 years in deep learning are based on existing tools used in previous papers. Thus, it is important to judge a work based on what new functionality has been achieved and how intellectually the techniques are used:\n\n-- **New Functionality**: TUVF is the first unsupervised work that enables generalizable texture synthesis with 3D dense correspondence at the same time. It not only achieves state-of-the-art texture synthesis results but also enables controllable synthesis by swapping texture while fixing the structure or the other way around, as well as transferring texture editing.\n\n-- **Technical Novelty**: TUVF is the first work that performs joint end-to-end Canonical Point Auto-encoder learning and surface learning. To perform rendering from the surface is non-trivial. Previous volumetric rendering in NeRF usually requires sampling in dense volume space, and our approach allows sampling near the surface. This not only largely improves the rendering efficiency but also leads to better texture. This results in a continuous field with dense and smooth correspondence on the surface, which is a representation never existed before. It also goes beyond texture synthesis and offers a new potential direction for efficient neural rendering. \n\n---\n\n**Q**:  it only produces OK results instead of exciting results. For example, recent PointUVDiffusion (Texture Generation on 3D Meshes with Point-UV Diffusion) can generate very realistic textures for a 3D shape.\n\n**A**: We respectfully disagree with the reviewer's opinion that our results were described as \"OK but not exciting.\" We believe that this statement is subjective and lacks specificity. For example, if we present texture synthesis results from the [Point-UV Diffusion sample](https://drive.google.com/file/d/14iMgNXLoXocWJXY8F9JkEHJXCE4I-6n6/view) and the [TUVF sample](https://tuvf4iclr.github.io/samples.html), which one can be viewed as performing better? Systematic quantitative and qualitative analysis is important for comparisons.\n\nAdditionally, it would be unfair to compare TUVF with supervised methods that were trained on synthetic setups, such as Point UV Diffusion. In our [General Response (B)](https://openreview.net/forum?id=dN4vpVTvWX&noteId=zW1K8ECNIl), we have provided both quantitative and qualitative results. We also provide discussions that highlight the fundamental differences between TUVF and these recent approaches (links: [AUV-Net](https://openreview.net/forum?id=dN4vpVTvWX&noteId=4Xylia94Au), [TEXTure](https://openreview.net/forum?id=dN4vpVTvWX&noteId=4Xylia94Au), [Point-UV Diffusion](https://openreview.net/forum?id=dN4vpVTvWX&noteId=uBGrwFvF1o), [Text2Tex](https://openreview.net/forum?id=dN4vpVTvWX&noteId=uBGrwFvF1o)).\n\n---\n\n**Q**: But actually, correspondence between shapes can also be obtained by postprocessing.\n\n**A**: We appreciate the reviewer's perspective but would like to emphasize the significance of learning dense correspondence in the context of 3D shape texture transfer. Learning dense correspondence for 3D shapes is a long-standing and challenging problem in the field. 3D shapes can be highly complex, with intricate details and variations. Notably, no heuristic-based tools can automatically and reliably establish dense correspondence between shapes without significant manual intervention. Meanwhile, manually assigning dense correspondence between such shapes can be extremely laborious and prone to error. This is unlike finding correspondence in 2D space, which can be achieved more conveniently by running DINOv2 given input images. Therefore, advancing our understanding of 3D dense correspondence remains a crucial and challenging goal.\n\n---\n\n**Q**: The intuition of using NeRF. I strongly feel that NeRF is not necessary. Is there any benefit to using NeRF?\n\n**A**: Please see the [General Response (A)](https://openreview.net/forum?id=dN4vpVTvWX&noteId=HLOybPoVgu) for clarification. We conduct a series of studies exploring alternative approaches if not using NeRF. Moreover, we explain the motivation and intuition behind using NeRF. We find that without the help of NeRF, there is no straightforward way to maintain correspondence on the faces while ensuring good geometry. We chose to use NeRF not because it is trendy but because it is the most intuitive option available."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission85/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469912895,
                "cdate": 1700469912895,
                "tmdate": 1700472910095,
                "mdate": 1700472910095,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "djrxWWR7Dp",
                "forum": "dN4vpVTvWX",
                "replyto": "aPG6ro6ABs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission85/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission85/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer KY39 (2/2)"
                    },
                    "comment": {
                        "value": "**Q:** Why not use surface-based rendering? \n\n**A**: We want to clarify that surface rendering techniques are already included as a crucial component of TUVF, which is discussed in detail in our paper and one of the **technical novelty** in this paper. Specifically, our approach involves a combination of surface and volume rendering methods. We render the color of a ray only on points near the object's surface, effectively leveraging the surface properties of the object. This approach results in superior rendering quality and efficiency. We kindly refer the reviewer to Section 3.3 of our main paper for more details.\n\n> **Efficient Ray Sampling.** Surface rendering is known for its speed, while volume rendering is known for its better visual quality (Oechsleetal.,2021). Similar to (Oechsleetal.,2021; Yarivetal.,2021; Wangetal.,2021), we take advantage of both to speed up rendering while preserving the visual quality, i.e., we only render the color of a ray on points near the object's surface.\n\n---\n\n**Q**: Comparing TUVF to AUV-Net should be necessary to prove the advantage of NeRF.\n\n**A**: We appreciate the reviewer's interest in comparing our approach to AUV-Net to showcase the advantages of NeRF. However, it's important to note that AUV-Net is, again, a supervised approach with a synthetic setup. Another critical difference is their use of preprocessed point colors as supervisory signals. As a result, AUV-Net does not involve \"rendering\" during training. Therefore, direct comparisons between TUVF and AUV-Net may not effectively demonstrate the unique advantages of NeRF. NeRF's continuous property is one of the specific advantages that make it ideal for TUVF. Furthermore, NeRF's strength lies in its ability to synthesize high-quality images through rendering. Please refer to the [General Response (B)](https://openreview.net/forum?id=dN4vpVTvWX&noteId=zW1K8ECNIl) and [here](https://openreview.net/forum?id=dN4vpVTvWX&noteId=4Xylia94Au) for further discussion regarding AUV-Net.\n\n---\n\n**Q**: A formulation of the rendering process would be better in Sec. 3.2 or Sec3.3.\n\n**A**: We have included the rendering process in Section 3.3 of our revised paper (colored in red). We use the quadrature rule to compute the color integral for each pixel.\n\n---\n\n**Q**: Reference: SINE: Semantic-driven Image-based NeRF Editing with Prior-guided Editing Field\n\n**A**: We have included the recommended citation in our revised paper. However, we would like to clarify that the suggested paper mainly focuses on NeRF editing, which is less relevant to TUVF. Our primary objective is texture synthesis, while texture editing is a secondary outcome. Moreover, TUVF can directly transfer texture editing between shapes, which SINE paper cannot. We sincerely hope that this missing reference will not serve as grounds for rejection."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission85/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470017374,
                "cdate": 1700470017374,
                "tmdate": 1700471293649,
                "mdate": 1700471293649,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hYA77yZXih",
                "forum": "dN4vpVTvWX",
                "replyto": "aPG6ro6ABs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission85/Reviewer_KY39"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission85/Reviewer_KY39"
                ],
                "content": {
                    "title": {
                        "value": "response"
                    },
                    "comment": {
                        "value": "Yes. I knew that you used the efficient ray sampling strategy. \nHowever, the key problem is whether NeRF can bring better results in this task, for example, view-dependent appearance. \nUnfortunately, the paper does not contain such view-dependent results. \nThen why bother learning a view-dependent appearance function? \nDirectly learning a color field is enough to produce the results in the paper. \nThat is why I think it is not necessary. \nIn addition, qualitative results with clear textures in AUV-NET are better than those in this paper though it requires GT 3D points\u2018 colors(TUVF requires points without colors)."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission85/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581791290,
                "cdate": 1700581791290,
                "tmdate": 1700584250530,
                "mdate": 1700584250530,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QeHU4XtLyN",
                "forum": "dN4vpVTvWX",
                "replyto": "HLOybPoVgu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission85/Reviewer_KY39"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission85/Reviewer_KY39"
                ],
                "content": {
                    "title": {
                        "value": "TUVF also requires 3D points as input."
                    },
                    "comment": {
                        "value": "`Unsupervised (w/o textured 3D shapes as input)` column.\nCorrect me if I am wrong, TUVF also requires 3D points without colors as input.\nThe input difference between TUVF and AUV-NET is whether there are colors for points.\nBut TUVF also requires multi-view images right?\nIt's hard to say which requires more input."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission85/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700584164534,
                "cdate": 1700584164534,
                "tmdate": 1700584603251,
                "mdate": 1700584603251,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "33uveE35kU",
                "forum": "dN4vpVTvWX",
                "replyto": "FwUkgBz8hs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission85/Reviewer_KY39"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission85/Reviewer_KY39"
                ],
                "content": {
                    "comment": {
                        "value": "OK. This point is clearer to me now. TUVF does not require paired data since it only uses GAN loss."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission85/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668745397,
                "cdate": 1700668745397,
                "tmdate": 1700668745397,
                "mdate": 1700668745397,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w8YVislAyL",
                "forum": "dN4vpVTvWX",
                "replyto": "aPG6ro6ABs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission85/Reviewer_KY39"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission85/Reviewer_KY39"
                ],
                "content": {
                    "comment": {
                        "value": "I agree that TUVF outperforms other unsupervised baselines. \nHowever, the answer still does not directly resolve my concerns about why using NeRF can get better results. \nReaders can only see the results and say OK your method gets better but miss intuition/reason. \nI really would like to see the authors replace the NeRF rendering module in TUVF with a simple color field(it's still neural rendering but not volume rendering) without sampling points near the surface and taking view direction as input. (that is to say, just predict the color on the surface.)\nWill there be difference on the quality side?\nIf there are differences, then maybe we can say the sampling process can better depict texture details or something (hopefully I can raise my score). \nOtherwise, I really do not feel that is necessary."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission85/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670034931,
                "cdate": 1700670034931,
                "tmdate": 1700670049137,
                "mdate": 1700670049137,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tcfSIIDTT0",
                "forum": "dN4vpVTvWX",
                "replyto": "aALEbAqxNo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission85/Reviewer_KY39"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission85/Reviewer_KY39"
                ],
                "content": {
                    "comment": {
                        "value": "With the additional ablation, I raised my score."
                    }
                },
                "number": 29,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission85/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695358790,
                "cdate": 1700695358790,
                "tmdate": 1700695358790,
                "mdate": 1700695358790,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "U3SWCvdvXH",
            "forum": "dN4vpVTvWX",
            "replyto": "dN4vpVTvWX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission85/Reviewer_9F5H"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission85/Reviewer_9F5H"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel approach for generating novel texture given a 3D shape. The key idea of this paper is to learn correspondence between UV coordinates and 3D points and apply a generative model in the UV space. The correspondence associates the generated texture features with the actual 3D location on the surface. It enables differential volume rendering wrt. the texture features. The authors use an adversarial learning setup on respective renderings.\nFor the geometry, they used the 3D cars and chairs from the ShapeNet dataset and photoshapes and CompCars datasets are used as 2D GT. A qualitative and quantitative comparison to Texturify, EpiGraf and more baselines show a decent performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well written and easy to understand. The method sections constraints useful figures and clear structure.\nThe research problem is important since a general formulation of UV mapping is an open topic.\nThe experimental sections contain many insights and support claims, e.g.Table 4 the ablation on the texture mapping network."
                },
                "weaknesses": {
                    "value": "Even though the method requires a GT shape as input, the rendered shapes appear to have over-smoothed regions, e.g. the mirrors of the cars. \n\nIt is unclear how well the learned UV correspondence preserves surface areas in the UV space."
                },
                "questions": {
                    "value": "Since the proposed method uses a shape encoder decoder part, I\u2019m wondering if this could be directly used to build a generative model for both shapes and textures."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission85/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission85/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission85/Reviewer_9F5H"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission85/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698875355181,
            "cdate": 1698875355181,
            "tmdate": 1699635933031,
            "mdate": 1699635933031,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b5SZeeAKu1",
                "forum": "dN4vpVTvWX",
                "replyto": "U3SWCvdvXH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission85/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission85/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q**: The rendered shapes appear to have over-smoothed regions. It is unclear how well the learned UV correspondence preserves surface areas in the UV space.\n\n**A**: We conduct experiments to evaluate our shape reconstruction method using the standard evaluation on ShapeNet, following previous studies. The table below shows the CD (Chamfer Distance) and EMD (Earth Mover's Distance) scores, comparing our method with several prior works. We also include the lower bound of the reconstruction errors in the \"Oracle\" column. Our results indicate that the geometry is similar to the oracle and comparable with previous works. When measured by EMD, our method consistently outperforms other methods, suggesting that our reconstructed point clouds have more uniformly distributed points on the surface. It is worth noting that EMD is considered a better metric to measure a shape's visual quality as it requires the outputs to have the same density as the ground-truth shapes. However, there is still room for improvement, as discussed in Appendix R. We will leave it as future work.\n\n| Dataset  | Metric | l-GAN (CD) [10] | l-GAN (EMD) [10] | AtlasNet (Sphere) [11] | AtlasNet (Patchs) [11] | PointFlow [12] |   ShapeGF [13] | DiffusionPM [14] |      Ours | _Oracle_ |\n|----------|--------|-----------:|------------:|------------------:|------------------:|----------:|----------:|------------:|----------:|---------:|\n| Airplane | CD     |      1.020 |       1.196 |             1.002 |             0.969 |     1.208 | **0.960** |       0.997 |     0.975 |  _0.837_ |\n|          | EMD    |      4.089 |       2.577 |             2.672 |             2.612 |     2.757 |     2.562 |       2.227 | **2.102** |  _2.062_ |\n| Chair    | CD     |      9.279 |       11.21 |             6.564 |             6.693 |    10.120 | **5.599** |       7.305 |     6.544 |  _3.201_ |\n|          | EMD    |      8.235 |       6.053 |             5.790 |             5.509 |     6.434 |     4.917 |       4.509 | **4.185** |  _3.297_ |\n| Car      | CD     |      5.802 |       6.486 |             5.392 |             5.441 |     6.531 |     5.328 |       5.749 | **5.178** |  _3.904_ |\n|          | EMD    |      5.790 |       4.780 |             4.587 |             4.570 |     5.138 |     4.409 |       4.141 | **3.555** |  _3.251_ |\n\n---\n\n**Q**: Build a generative model for both shapes and textures.\n\n**A**: We appreciate the suggestion. However, it's important to note that in TUVF, we use a disentangled representation that enables us to sample texture latents from its posterior distributions without any interference from the shape. This disentangled representation allows more controllable synthesis. We can either swap the geometry or swap the texture without interfering with the other, which is hard to achieve with a unified representation. One possible solution is to combine TUVF with a pre-trained shape generative model like CanonicalVAE [15]. CanonicalVAE is a recent work that expands canonical point auto-encoder into a generative model with dense correspondence. This could be a promising direction for our future research.\n\n\n\n---\n*[10] Panos Achlioptas and Olga Diamanti and Ioannis Mitliagkas and Leonidas Guibas. Learning Representations and Generative Models for 3D Point Clouds. International Conference on Machine Learning (ICML), 2018.*\n\n*[11] Thibault Groueix and Matthew Fisher and Vladimir G. Kim and Bryan C. Russell and Mathieu Aubry. AtlasNet: A Papier-M\u00e2ch\u00e9 Approach to Learning 3D Surface Generation. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.*\n\n*[12] Guandao Yang and Xun Huang and Zekun Hao and Ming-Yu Liu and Serge Belongie and Bharath Hariharan. PointFlow: 3D Point Cloud Generation with Continuous Normalizing Flows. IEEE International Conference on Computer Vision (ICCV), 2019.*\n\n*[13] Ruojin Cai and Guandao Yang and Hadar Averbuch-Elor and Zekun Hao and Serge Belongie and Noah Snavely and Bharath Hariharan. Learning Gradient Fields for Shape Generation. European Conference on Computer Vision (ECCV), 2020.*\n\n*[14] Shitong Luo and Wei Hu. Diffusion Probabilistic Models for 3D Point Cloud Generation. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.*\n\n*[15] An-Chieh Cheng, Xueting Li, Sifei Liu, Min Sun, Ming-Hsuan Yang. Autoregressive 3D Shape Generation via Canonical Mapping. European Conference on Computer Vision (ECCV), 2022.*"
                    },
                    "title": {
                        "value": "Author Response to Reviewer 9F5H"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission85/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469607848,
                "cdate": 1700469607848,
                "tmdate": 1700474248998,
                "mdate": 1700474248998,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cybVuyW4tX",
                "forum": "dN4vpVTvWX",
                "replyto": "U3SWCvdvXH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission85/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission85/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to our Rebuttal"
                    },
                    "comment": {
                        "value": "Dear Reviewer 9F5H,\n\nThank you once again for the detailed feedback. We are approaching the end of the author-reviewer discussion period (less than 24 hours). However, there are no responses yet to our rebuttal.\n\nPlease feel free to request any additional information or clarification that may be needed. We hope to deliver all the information in time before the deadline.\n\nThank you!"
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission85/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669068221,
                "cdate": 1700669068221,
                "tmdate": 1700669068221,
                "mdate": 1700669068221,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]