[
    {
        "title": "Revisiting DeepFool: generalization and improvement"
    },
    {
        "review": {
            "id": "EjmKqxsQBd",
            "forum": "CH6DQGcI3a",
            "replyto": "CH6DQGcI3a",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5966/Reviewer_2iqu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5966/Reviewer_2iqu"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a modification and improvement to the DeepFool adversarial attack method which aims to find minimal perturbations under the $\\ell_2$-norm. The authors identify that DeepFool does not generate minimal samples. Then they propose to guide DeepFool to optimal solution by an additional projection step. The proposed method is called SuperDeepFool (SDF) and is evaluated as an attack against previous methods, as an adversary during adversarial training, and as an adversary in the AutoAttack test bench."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper identifies weaknesses of DeepFool and undermines their theoretical observations (perturbations not located on the decision boundary/not orthogonal) through experimental evaluations\n2. Based on the identified weaknesses a new method is proposed to guide the attack to minimal examples\n3. The method is compared to prior work and appears to generate more minimal perturbations on natural and adversarially trained models.\n4. The method is integrated and evaluated with adversarial training.\n5. The method is integrated and evaluated within the popular AutoAttack benchmark where it shows an improvement in attack rates and/or computational time compared to the original implementation."
                },
                "weaknesses": {
                    "value": "1. SDF appears to consequently require more gradient steps than DF. As such the comparisons in Tab. 2/3/4 are hardly fair. As an additional row in the table, it would be fair to restrict the number of gradients in SDF to the number of DF steps.\n2. As the goal is to improve DF, comparisons to DF are missing in Tab. 5/19/20.\n3. The comparison in Sec. 4.3 against DDN is not fair and thus not meaningful: 1) SDF is trained with different hyperparameters - most notably with more epochs: (200 + 60) instead of (200 + 30). A fair comparison would require training with identical parameters. 2) The results are single-run/seed. Expressive results would require at least 3 runs and error bars - especially for unstable training methods like AT.\n4. \"SDF adversarially trained model does not overfit to SDF attack\" - this statement can - in general - not faithfully be made on a single training run and should be either backed theoretically or empirically on multiple architectures (with reported error bars). \n5. The research field has largely moved on from minimum-norm- and/or $\\ell_2$-based adversarial attacks and is mostly focusing on $\\ell_\\infty$-bounded attacks (if at all). As such, this submission may not be that relevant to the ICLR community.\n\nMinor:\n- Some sections are very hard to read as indirect citations are not in brackets.\n- In Sec. 2 $f_k$ is defined twice.\n- Wrong quotation marks in Tab. 8. In LaTeX: \"xxx\" instead of ``xxx''\n- Multiple tables in the appendix overflow\n- This is a personal taste, but I find it odd to cite (Long, 2015) as a breakthrough in computer vision. I'd have expected earlier works by Alex Krizhevsky or the likes"
                },
                "questions": {
                    "value": "1. How are perturbations \"renormalized\" for AutoAttack++?\n2. Why does AA++ perform worse for R4?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "As this paper deals with adversarial attacks and defenses it should add a \"potential negative impact for society\" section."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5966/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5966/Reviewer_2iqu",
                        "ICLR.cc/2024/Conference/Submission5966/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5966/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697730400535,
            "cdate": 1697730400535,
            "tmdate": 1700572502746,
            "mdate": 1700572502746,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QzIn0SxoLc",
                "forum": "CH6DQGcI3a",
                "replyto": "EjmKqxsQBd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5966/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5966/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part I"
                    },
                    "comment": {
                        "value": "> SDF appears to consequently require more gradient steps than DF.\n\nOur claim is that, in comparison to other algorithms, SDF offers the best tradeoff between computational cost and accuracy. As illustrated in Figure 1, compared to DF, SDF incurs a slightly higher computational cost but significantly gains in accuracy.\n\nBy design, both SDF and DF do not allow control over the number of gradient computations. They typically stop once a successful adversarial example is found. Terminating the process prematurely could prevent them from finding an adversarial example. One naive way would be the following: limiting SDF to a single iteration would equate its computational cost to DF, but this would essentially make SDF identical to DF, hence not a useful comparison.\n\n> As the goal is to improve DF, comparisons to DF are missing in Tab. 5/19/20.\n\nDF, developed nearly eight years ago, has been demonstrated in various instances [1], [2], and [4] to be fast but no longer the SotA in identifying minimal adversarial perturbations. Therefore, we primarily conducted our experiments using SotA $\\ell_2$ attacks. Table 1 is the exception, where we included a comparison with DF, as it was pertinent to compare SDF to DF \u2013 given that SDF is a modified version of DF.\n\nBut for the sake of completeness, we will include DF in the tables as suggested by the reviewer. Here are the corresponding results for DF:\n\n- In Table 5: the median $\\ell_2$ norm: 6.1, fooling rate: 96.2%.\n- In Table 19:  the median $\\ell_2$ norm: 1.51, fooling rate: 100%.\n- In Table 20: Naturally trained model: the median $\\ell_2$ norm: 0.17, per-sample time: 0.21s. Adversarially trained models (R1): the median $\\ell_2$ norm: 2.1, per-sample time: 1.32s.\n\n> The comparison in Sec. 4.3 against DDN is not fair\u2026\n\nThank you for the suggestion. We reran our experiments using three different random seeds for training, and we set the number of epochs used in the DDN paper. Despite these adjustments, our findings remain consistent: SDF AT continues to outperform DDN AT.\n\n| Attack | SDF Mean (std) | SDF Median (std) | DDN Mean (std)| DDN Median (std)|\n|--------|------------------|-------------------|----------|------------|\n| DDN    | $1.01~(0.05)$             | $1.00~(0.01)$               | $0.80~(0.08)$     | $0.68~(0.05)$       |\n| FAB    | $1.07~(0.06)$             |  $1.01~(0.03)$               | $0.90~(0.1)$     | $0.71~(0.2)$       |\n| FMN    | $1.40~(0.2)$             | $1.38~(0.4)$              | $1.41~(0.2)$     | $1.39~(0.3)$        |\n| ALMA   | $1.07~(0.03)$             | $1.01~(0.1)$              | $0.80~(0.07)$     | $0.68~(0.06)$        |\n| SDF    | $\\mathbf{1.00}~(0.01)$         | $\\mathbf{0.98}~(0.03)$          | $0.80~(0.02)$       | $0.69~(0.08)$        |\n\n> \"SDF adversarially trained model does not overfit to SDF attack\u201d\u2026\n\nWe hope that the experiment done above has addressed this issue, too.\n\n\n> The research field has largely moved on from minimum-norm\u2026\n\nWe respectfully disagree with the reviewer. The reasons for using $\\ell_2$ norm perturbations are manifold:\n\n- We acknowledge that $\\ell_2$ threat model may not seem particularly realistic in practical scenarios (at least for images); however, it can be perceived as a basic threat model amenable to both theoretical and empirical analyses, potentially leading insights in tackling adversarial robustness in more complex settings. The fact that, despite considerable advancements in AI/ML, we are yet to solve adversarial vulnerability, motivates part of our community  to return to the basics and work towards finding fundamental solutions to this issue.\n    \n- In particular, thanks to their intuitive geometric interpretation, $\\ell_2$ perturbations provide valuable insights into the geometry of classifiers. They can serve as an effective tool in the \"interpretation/explanation\" toolbox to shed light on what/how these models learn.\n\n- Moreover, it has been demonstrated that (see, for example,[4] and [9]), $\\ell_2$ robustness has several applications beyond security.\n      \n- While not the most compelling reason, the continued interest of the community in $\\ell_2$ perturbations is evidenced by publications in top venues, including FMN [7] in NeurIPS 2021, DDN [5] in CVPR 2019, ALMA [10] in ICCV 2021, FAB [8] in ICML 2021, among many others.\n\n> How are perturbations \"renormalized\" for AutoAttack++?\n\nFor a given $\\epsilon$ for AA++, if the norm of perturbation generated by SDF is greater than $\\epsilon$, we apply a scalar multiplication to bring its $\\ell_2$-norm down to $\\epsilon$. Hence, we ensure that all the norms of all SDF perturbations are at most $\\epsilon$."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5966/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700403164840,
                "cdate": 1700403164840,
                "tmdate": 1700403164840,
                "mdate": 1700403164840,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YhAGACeHHl",
                "forum": "CH6DQGcI3a",
                "replyto": "EjmKqxsQBd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5966/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5966/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part II"
                    },
                    "comment": {
                        "value": "> Why does AA++ perform worse for R4?\n\nFor this particular model, we identified a few samples that SDF could not successfully fool, whereas APGD$^\\top$ did. Therefore, we explored an alternative version of AA++ where we included SDF alongside the set of attacks, rather than replacing APGD$^\\top$ with it, ensuring the preservation of AA\u2019s original performance.\n\n[1]: Rony, Jerome et al. \"Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\n\n[2] Carlini, Nicholas and David A. Wagner. \u201cTowards Evaluating the Robustness of Neural Networks.\u201d 2017 IEEE Symposium on Security and Privacy (SP) (2016)\n\n[3] Madry, Aleksander et al. \u201cTowards Deep Learning Models Resistant to Adversarial Attacks.\u201d ArXiv abs/1706.06083 (2017)\n\n[4]: Pintor, Maura et al. \"Fast minimum-norm adversarial attacks through adaptive norm constraints.\" Advances in Neural Information Processing Systems, vol. 34, pp. 20052-20062, 2021.\n\n[5]: Croce, Francesco and Hein, Matthias. \"Minimally distorted adversarial examples with a fast adaptive boundary attack.\" International Conference on Machine Learning, pp. 2196-2205, 2020, PMLR."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5966/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700403190901,
                "cdate": 1700403190901,
                "tmdate": 1700403190901,
                "mdate": 1700403190901,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "URtAaK2yzM",
                "forum": "CH6DQGcI3a",
                "replyto": "YhAGACeHHl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5966/Reviewer_2iqu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5966/Reviewer_2iqu"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the rebuttal. The rebuttal has addressed some of my concerns. While I am still not in favor of acceptance - mostly due to the contribution - I revised my recommendation to \" 5: marginally below the acceptance threshold\" and have updated the presentation and contribution scores."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5966/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700572667674,
                "cdate": 1700572667674,
                "tmdate": 1700572667674,
                "mdate": 1700572667674,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6l8cCq9x3c",
                "forum": "CH6DQGcI3a",
                "replyto": "EjmKqxsQBd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5966/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5966/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for revising their score. To better address your remaining concerns, particularly regarding the contributions of our work, we would greatly appreciate more specific feedback.\n\nWe would like to bring to your attention that in Appendix F of our paper, we have dedicated a section to discussing how SDF has led us to an interesting observation regarding the effect of pooling layers on the adversarial robustness. It further highlights the importance of developing minimum-norm adversarial attacks."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5966/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700574163224,
                "cdate": 1700574163224,
                "tmdate": 1700583978554,
                "mdate": 1700583978554,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6g7fHDBK3z",
                "forum": "CH6DQGcI3a",
                "replyto": "6l8cCq9x3c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5966/Reviewer_2iqu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5966/Reviewer_2iqu"
                ],
                "content": {
                    "comment": {
                        "value": "My main argument for not increasing the score above the acceptance level is the limited relevance to the community. I acknowledge that there may be some use cases, but largely this is a minor spin on a niche/old topic. This is not addressable in the current rebuttal but in case the paper is not accepted, I'd suggest dropping out some sections like AT and showing why the proposed method is practical in today's DL landscape (e.g., you mention applications beyond security). You could show an example.\n\nThen some of the other weaknesses have not been addressed.\n - W1: I understand that early-stopping may not be reasonable for a robustness evaluation, but it would establish a comparison to the original DeepFool.\n- W4: I would recommend to change the wording. Experiments with more seeds give more evidence but no proof. You could provide even more evidence by testing different architectures, but overall this section is not that important for the paper (I think other reviewers have mentioned this as well).\n\nRegarding pooling: To be fair I only quickly scanned it but it feels heavily disconnected from the rest of the paper. Besides, there have been multiple works analyzing pooling in the context of robustness, e.g., Grabinski et al. \"FrequencyLowCut Pooling - Plug & Play against Catastrophic Overfitting\", ECCV 2022. If you want to make a story out of this, you'd have to frame it in the context of previous work."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5966/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700607129271,
                "cdate": 1700607129271,
                "tmdate": 1700607129271,
                "mdate": 1700607129271,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RskusYQsDp",
            "forum": "CH6DQGcI3a",
            "replyto": "CH6DQGcI3a",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5966/Reviewer_VFL7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5966/Reviewer_VFL7"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an improved version of the DeepFool (DF) attack. This new algorithm achieves better convergence and finds smaller perturbations compared to DF, by enforcing additional optimization strategies based on geometrical observations on the properties of the decision boundary."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+ clear writing and presentation\n+ extensive evaluation"
                },
                "weaknesses": {
                    "value": "- the other attacks are tested with default hyperparameters\n- the attack should produce valid perturbations"
                },
                "questions": {
                    "value": "## Major comments\n\nThe paper is well written and clear and easy to read. I believe it is over the acceptance threshold, however I suggest some improvements that would improve this work.\n\nFirst, limitations are not discussed. The authors should describe the limitations that at least should include:\n* the attack is not adaptive per-se, hence it can be blocked by gradient obfuscation or other defenses that break gradient descent. Is the attack easy to adapt in these cases (e.g., application of EoT to smooth the loss landscape, change of loss to incorporate detectors, ...)\n* the attack is still an empirical attack, thus the authors should discuss that even with the best optimization process there are no guarantees that the points found are global minima of the optimization function.\n* the attack formulation only works for the L2 norm\n* the attack is not tested in the targeted version.\n\nThe authors should also improve the evaluation to include additional insights and analyses:\n* analysis of complementarity with AA, do the two attacks find smaller perturbations for different points? This could suggest whether the best strategy is to add it to the ensemble or to remove directly some of the sub-optimal attacks of AA (e.g., FAB that seems sub-optimal w.r.t. the proposed method). Additionally, in AA the attacks are used in a cascade manner, i.e., if the first attack finds an adversarial perturbation within the eps bound, the other attacks are not launched. Would SDF be launched before or after, e.g., APGD?\n* the other attacks are tested with default hyperparameters, and the authors report that the performances degrade when changing the datasets. However, testing the attacks with a set of hyperparameters could potentially reveal that the proposed approach is sub-optimal w.r.t. the other attacks, while still defeating them because of the parameter-free advantage.\n* additionally, it would be interesting to see the query-distortion curves, such as in other related works (e.g., FMN), as the median is only a compact measure of the performances\n* the attack should also be tested and compared in the targeted version, otherwise, the authors should still list this as a limitation (as it has not been tested, thus it is difficult to evaluate whether this is the best approach in the case of targeted attacks).\n gradient computation $\\neq$ fast attack\n* to claim that the attack finds perturbations \"quickly\", the authors should perform a per-query runtime comparison such as the one done in other papers (FMN, ALMA) to compare the efficiency of the attack in a fair manner. The attack seem to win in any case (given the runtimes reported in the appendix), however a per-query comparison would still be interesting to see, given that there are additional operations within the steps (even if they are probably negligible w.r.t. the time for computing the gradients).\n\nOn the formulation, the attack seems to be clear enough, however the attack might produce invalid perturbations in some domains (e.g., images)\n\n>we clip the pixel-values of SDF-generated adversarial images to [0, 1], consistent with the other minimum-norm attacks\n- The attack algorithm should enforce this constraint \"by default\", as for the majority of applications there are input space constraints and they should be enforced to produce valid perturbations. \nThe authors should also discuss what happens when they enforce this constraint, as, for example, it is probably not possible anymore to enforce the orthogonality with the decision boundary. For this reason, also the plots added in the appendix comparing the orthogonality of FMN and CW might be biased by this additional constraints (while the comment on the line search is not clear, as FMN does not perform line searches unless initialized from the adversarial class).\n\n## Clarifications needed\n\nThere are some aspects that should be clarified to improve the quality of the paper.\n\n> For an optimal perturbation, we expect these two vectors to be parallel\n\n- The authors should clarify this aspect to make the paper accessible to a broader audience. Additionally, a figure with a 2D example might also help clarify this observation.\n\n> DeepFool, C&W, FMN, FAB, DDN, and ALMA are minimum-norm attacks, and FGSM, PGD, and momentum extension of PGD Uesato et al. (2018) are bounded-norm attacks.\n\n- The authors should state that the bounded-norm attacks solve a different problem than (1).\n\n- As it is a scalar multiplied by the gradient, is the first part of eq. 4 an estimation of the step size? Maybe this could be specified in the text to make the description clearer.\n\n- The authors should discuss why the other attacks are not parallelizable, as they state that attacks for AT should be and CW, for example, is not. \n\n## Comments on figures\n\n- caption of Figure 3 should be improved by clarifying all points shown (e.g., $x_2$). Moreover, f(x) used to denote the decision function does not seem to be defined in the text."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5966/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5966/Reviewer_VFL7",
                        "ICLR.cc/2024/Conference/Submission5966/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5966/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697961492176,
            "cdate": 1697961492176,
            "tmdate": 1700469960390,
            "mdate": 1700469960390,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oeFfMbJWvc",
                "forum": "CH6DQGcI3a",
                "replyto": "RskusYQsDp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5966/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5966/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate the reviewer's constructive feedback and positive view of our work. We address the questions raised in the following:\n\n## Limitations\nWe are going to add a new section in the paper to discuss the limitations of our work (Appendix L). However, let us first elaborate on them here.\n\n> The attack is not adaptive per-se\u2026\n\nWe acknowledge that, generally, extending geometric attacks to such cases is not immediately apparent. However, we do not view this as an inherent limitation of geometric attacks like DF and SDF. Instead, we believe the community's focus to obtain adaptive attacks has primarily been on loss-based attacks such as PGD, which allow for easy modification of the loss. Investigating ways to adapt geometric attacks could be of interest, particularly due to their computational benefits\n\n> The attack is still an empirical attack\u2026\n\nThis indeed can be said about almost any gradient-based optimization technique applied to non-convex problems. As such, we thought it is evident from the context that there is no guarantee for finding the globally optimal perturbations for state-of-the-art neural networks\u2014*neither for us nor for any other attacks we are aware of*. We apologize if our wording caused such confusion. If there is any specific sentence in the text implying otherwise, we would be grateful if the reviewers could point them out to us.\n\n> The attack formulation only works for the $\\ell_2$ norm.\n\nWe had reasons to limit the scope of the paper to $\\ell_2$ perturbations. We believed this approach would more clearly convey our meta-message, as summarized by Reviewer 34Xw: \u201dthe importance that fundamentally 'classic' approaches, if done right, might still serve as a strong baseline\". Anyway, we tried a simple idea to see the potential of our approach in finding $\\ell_\\infty$ perturbations, and it worked. We replaced the orthogonal projection with the $\\ell_\\infty$ projection.\n\nWe present our findings about the $\\ell_\\infty$ norm for two robust models M1 and M2 in the following table.\n\n| Attacks | M1    |  FR   | Grads | M2   | FR   | Grads |\n|---------|------------|---------|-------|------|------|-------|\n|     DF     | 0.031 | 96.7   | 24| 0.043 | 97.4 | 31|\n| FAB    | 0.025 | 99.1   | 100   | 0.038 | 99.6 | 100   |\n| FMN   | 0.024 | 100 | 100 | 0.035 | 100 | 100 |\n| SDF     | **0.019** | 100 | 33  | **0.027** | 100 | 46   |\n\n## Improvements\n> Analysis of complementarity with AA, do the two attacks find smaller perturbations for different points? This could suggest whether the best strategy is to add it to the ensemble or to remove directly some of the sub-optimal attacks of AA (e.g., FAB that seems sub-optimal w.r.t. the proposed method). Additionally, in AA the attacks are used in a cascade manner, i.e., if the first attack finds an adversarial perturbation within the eps bound, the other attacks are not launched. Would SDF be launched before or after, e.g., APGD?\n\nOur decision to replace APGD$^{\\top}$ with SDF was primarily motivated by the former being a computational bottleneck in AA. As it is shown in Table 8, AA and AA++ achieve similar fooling rates, with AA++ being notably faster. Following your suggestion, we compared the sets of points that were fooled or not fooled by SDF/APGD$^{\\top}$ across 1000 samples ($\\epsilon=0.5$). The results indicate that both algorithms fool approximately the same set of points, differing only in a handful of samples for this epsilon value. Therefore, the primary benefit of using SDF is the reduction in computation time.\n\n|                      | SDF Can Fool | SDF can't Fool |\n|----------------------|--------------|----------------|\n| APGD$^{\\top}$ Can Fool | 995          | 2              |\n| APGD$^{\\top}$ Can't Fool| 3            | 5              |\n\nWe are currently conducting experiments to provide a comprehensive analysis, which includes substituting SDF with various attacks, integrating SDF into the mix, etc. Given that they are time-consuming, it may not be feasible to complete them all by Nov 22. Anyway, we intend to include the full set of results in the final version of our work."
                    },
                    "title": {
                        "value": "Part I"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5966/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700424539631,
                "cdate": 1700424539631,
                "tmdate": 1700424593116,
                "mdate": 1700424593116,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Mfd1rDLQxN",
                "forum": "CH6DQGcI3a",
                "replyto": "RskusYQsDp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5966/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5966/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part II"
                    },
                    "comment": {
                        "value": "> The other attacks are tested with default hyperparameters\u2026\n\nTo ensure a fair comparison, for each dataset, we selected hyperparameters for each attack based on those recommended in their respective papers. The assumption was that the authors of these papers have either fine-tuned these parameters or claimed that their methods are not highly sensitive to them.\n\nGiven that comprehensive hyperparameter tuning for each attack across every model and dataset would be infeasible for us, we experimented with different variations of the attacks by varying their number of iterations (as detailed in Table 3). We consistently aimed to select this parameter so that it would work in favor of their performance. However, we acknowledge the possibility that there may still be hyperparameter settings for each (attack, model, dataset) combination that could equalize the performance of all attack algorithms. Similar scenarios have been observed previously in the context of training algorithms and GANs, (Lucic et al. \u201cAre GANs Created Equal? A Large-Scale Study\u201d, NeurIPS 2018.)\n\nThis further highlights the importance of designing algorithms with fewer number of hyperparamters such as SDF.\n\n> additionally, it would be interesting to see the query-distortion curves...\n\nMany thanks for this suggestion.\n\nNote that, SDF (and DF) do not allow control over the number of gradient computations. They typically stop once a successful adversarial example is found. Terminating the process prematurely could prevent them from finding an adversarial example. Hence, we opted to plot the median norm of achievable perturbations for a given maximum number of queries. Although this is not directly comparable to the query-distortion curves in FMN, it provides a more comprehensive view of the query distribution than the median alone. The figures are placed in the Appendix K.\n\n> the attack should also be tested and compared in the targeted version\u2026\n\nThanks for this suggestion. If time allows, we will include this experiment; otherwise, we will mention it as a future direction.\n\n> to claim that the attack finds perturbations \u201cquickly\u201d\u2026\n\nIt may be helpful to differentiate between two distinct objectives:\nA) With a fixed \u201ccomputational budget\u201d, which algorithm finds the smallest perturbation.\nB) For a given a level of distortion, which algorithm requires less computation.\n\nWhile there is not a straightforward method to directly address points A and B for SDF (as mentioned above), our results suggest that SDF is the fastest for \u201cvery small\u201d distortions. Also, with a \u201cvery small\u201d number of queries, it tends to find the smallest perturbation. It was the message we aimed to convey in Figure 1.\n\nWe also measured the number of forward and backward passes separately for a robust WRN-28-10 model. The median values for the forward and backward passes of SDF are 50 and 32 per sample, respectively. The time spent on other operations was less than 1% of the total time. It achieves a median norm of 0.77. While FMN-100 achieves a median norm of 1.43 using 100 forward and 100 backward passes per sample, FMN-1000 reaches a median norm of 0.87 with 1000 forward and 1000 backward passes.\n\n> The attack algorithm should enforce this constraint \"by default\"...\n\nWe report all our results with clipping as the final step in the algorithm. We acknowledge the reviewer's point that this might not be enough IF our focus is on the \"real-world\" security implications of these attacks. However, if the purpose is to use them for data augmentation, for example, then such a requirement might not be needed anymore.\n\nAnyway, we played with the in-loop clipping as well, yet observed no noticeable difference between the two methods in our experiments.\n\n## Clarifications\n> The authors should clarify this aspect to make the paper\u2026\n\nWe will clarify this sentence in the text, and will include a figure in the Appendix XX to visualize the concept.\n\n> The authors should state that the bounded-norm attacks solve a different problem than (1).\n\nThanks for the suggestion. Indeed. We modify the text to clarify this distinction.\n\n> As it is a scalar multiplied by the gradient, is the first part of eq. 4 an estimation of the step size? \n\nYes, indeed. We will make it clear in the text.\n\n> The authors should discuss why the other attacks are not parallelizable, as they state that attacks for AT should be and CW, for example, is not.\n\nOther algorithms usually use perform one backward pass per iteration, resulting in these backward passes being non-parallelizable as they need to be performed sequentially. However, in the case of SDF, some of the backward passes can be executed in parallel, especially the backward passes that are carried out in each iteration of the inner loop in the multi-class case."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5966/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700424559645,
                "cdate": 1700424559645,
                "tmdate": 1700424610825,
                "mdate": 1700424610825,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iAcn7pOzxR",
                "forum": "CH6DQGcI3a",
                "replyto": "Mfd1rDLQxN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5966/Reviewer_VFL7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5966/Reviewer_VFL7"
                ],
                "content": {
                    "title": {
                        "value": "Official response from reviewer VFL7"
                    },
                    "comment": {
                        "value": "I acknowledge the authors' responses and thank them for their time in clarifying the aspects of their work.\nMy points of concern have been fully addressed, and I stand by \"the importance that fundamentally 'classic' approaches, if done right, might still serve as a strong baseline\". I trust that the authors will improve the discussion on the limitation and will consider my feedback for their revision.\n\nI would be interested in seeing the targeted results and the results **with** the clipping operation, as they would demonstrate even further that the approach is competitive even outside the \"comfort\" zone under which it has been stress-tested. As the authors state \"we played with the in-loop clipping as well, yet observed no noticeable difference between the two methods in our experiments\", I encourage them to add this result to the results tables (perhaps highlighting if and when they see a noticeable difference). Even a small experiment could be enough to strengthen the paper even more.\n\nAfter all, I will raise my score for this paper, as it is clear to me that the authors invested time and effort in considering all aspects of the paper."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5966/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700469940434,
                "cdate": 1700469940434,
                "tmdate": 1700469940434,
                "mdate": 1700469940434,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "N1d9X9X0HV",
            "forum": "CH6DQGcI3a",
            "replyto": "CH6DQGcI3a",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5966/Reviewer_34Xw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5966/Reviewer_34Xw"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes SuperDeepFool, an extension of the well-known DeepFool adversarial attack, which is more accurate and less time-consuming. Although DeepFool tries to find minimal adversarial perturbation, it has a drawback: It doesn't search for the perturbation orthogonal to the decision boundary. Authors propose to solve the issue approximately by applying DeepFool and Orthogonality in an alternative manner. Superiority of SuperDeepFool was shown on MNIST, CIFAR, and ImageNet."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The revisiting of the 8 year-old 'classic' attack DeepFool and be able to find its drawback and improve it is novel. It shows the importance that fundamentally 'classic' approaches, if done right, might still serve as a strong baseline.\n2) The proposed method is more accurate and less time consuming in all benchmarks.\n3) Decent theoretical analysis and vivid geometric illustration.\n4) Perturbations are not only evaluated on naturally trained models, but also on adversarially trained models.\n5) It improves adversarial training and autoattack."
                },
                "weaknesses": {
                    "value": "1) DeepFool was proposed and evaluated not only for $\\ell_2-$adversarial robustness, but also had formulas (Holder's inequality) and evaluations for any $\\ell_p$ (including $\\ell_0$, $\\ell_1$, $\\ell_\\infty$) norm balls. In contrast, SuperDeepFool is only proposed and evaluated for $\\ell_2$, although claims to be more generalized version of DeepFool.\n2) Only convolutional networks were used in the experiments, while it is known that vision transformers are robust learners. \n3) Few additional hyperparameters $(n, m)$ are added and there is no methodology how to select them right\n\nMinor weaknesses:\n1) Lots of space could be saved by combining tables and figures in one row (for example Table 3 and Table 8)"
                },
                "questions": {
                    "value": "1) Why does white-box $\\ell_2$ adversarial robustness matter? What is the practical scenario where high-frequency pixel-level $\\ell_2$ perturbations, crafted with full access to the model and gradients, might cause problems? Maybe, it would be nice to show that more $\\ell_2$ robust models have some other useful properties.\n2) Why median $\\ell_2$ and number of grads are used as the metric, while DeepFool used $\\rho = \\frac{1}{|D|}\\sum_{x\\in D} \\frac{\\|r(x)\\|_2}{\\|x\\|_2}$?\n3) Why does experiment with adversarial training with SuperDeepFool not have models trained with PGD adversarial examples?\n\n$\\textbf{Overall,}$\n\n I liked the paper and its motivation. Authors are encouraged to reply, and I might consider raising my score if the weaknesses and questions are addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5966/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5966/Reviewer_34Xw",
                        "ICLR.cc/2024/Conference/Submission5966/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5966/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698177810373,
            "cdate": 1698177810373,
            "tmdate": 1700681631020,
            "mdate": 1700681631020,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MCqGXDTOt6",
                "forum": "CH6DQGcI3a",
                "replyto": "N1d9X9X0HV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5966/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5966/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part I"
                    },
                    "comment": {
                        "value": "We are glad that the reviewer has liked several aspects of our work. We have done our best to address the questions and comments. Please let us know if there is anything more we can do to positively influence your opinion and strengthen the paper.\n\n> DeepFool was proposed and evaluated not only for $\\ell_2$ adversarial robustness, but also had formulas (Holder's inequality) and evaluations for any $\\ell_p$(including $\\ell_0$, $\\ell_1$, $\\ell_\\infty$) norm balls\u2026.\n\nIt should be noted that the focus of our paper, as stated in the abstract, is on minimal $\\ell_2$-norm perturbations.  If the reviewer has identified any sentences that suggest otherwise, please let us know. We would greatly appreciate your feedback. Whenever we mention DF, we refer to its $\\ell_2$ variant. Moreover, up to our knowledge, DF was primarily introduced for $\\ell_2$-norm, and its application to general $\\ell_p$-norm perturbations was not extensively tested in the original paper.\n\nNevertheless, we tried an $\\ell_\\infty$ version of SDF by replacing the orthogonal projection with the $\\ell_\\infty$ projection (Holder's inequality). The table below shows our results for $\\ell_\\infty$ on M1 [1] and M2 [5]. Our results show that this version of SDF also outperforms other algorithms in finding smaller $\\ell_\\infty$ perturbations. We will add this result to the Appendix.\n\n| Attacks | M1    |  FR   | Grads | M2   | FR   | Grads |\n|---------|------------|---------|-------|------|------|-------|\n|     DF     | 0.031 | 96.7   | 24| 0.043 | 97.4 | 31|\n| FAB    | 0.025 | 99.1   | 100   | 0.038 | 99.6 | 100   |\n| FMN   | 0.024 | 100 | 100 | 0.035 | 100 | 100 |\n| SDF     | **0.019** | 100 | 33  | **0.027** | 100 | 46   |\n\n> Only convolutional networks were used in the experiments\u2026\n\nMany thanks for the suggestion. Given our available computational resources, we conduct experiments on a ViT-B-16 [2] trained on CIFAR-10, achieving 98.5$\\%$ accuracy. The results are summarized in the following table:\n\n| Attack | FR   | Median-$\\ell_{2}$ | Grads |\n|--------|------|--------------------|-------|\n| DF     | 98.2 | 0.29               | 19|\n| ALMA   | 100  | 0.12               | 100   |\n| DDN    | 100  | 0.14               | 100   |\n| FAB    | 100  | 0.14               | 100   |\n| FMN    | 99.1 | 0.15               | 100   |\n| C&W    | 100  | 0.15               | 91,208|\n| SDF    | 100  | **0.10**           | 32    |\n\nAs seen, this transformer model does not exhibit significantly greater robustness compared to CNNs, with only a negligible difference of 0.01 compared to a WRN-28-10 trained on CIFAR-10 (0.09 in Table 3 of the paper). These results support the notion that there might not be a substantial disparity between the adversarial robustness of ViTs and CNNs. This aligns with the findings of [3]. They argue that earlier claims of transformers being more robust than CNNs stems from an unfair comparison and evaluation methods. We believe that thorough evaluations using minimum norm attacks could be helpful in resolving this debate.\n\n\n> Few additional hyperparameters $(n,m)$ are added and there is no methodology how to select them right.\n\nWe regard $(m,n)$ as a way to parameterize a class of algorithms, where each pair corresponds to an algorithm. As shown in Table 2 of the paper, all these variants yield comparable results. However, SDF($\\infty$,1) empirically demonstrates a slightly superior performance (and hence is considered the default algorithm). For a new dataset, it is important to note that SDF($\\infty$,1) might not be always the optimal algorithm. This is why we kept the class of algorithms general (it is needless to say that even to train a model on a new dataset, even the selection of architecture, optimization algorithm, etc. are not known apriori, and many methods used to determine those, can be equally applied here.)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5966/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700331762996,
                "cdate": 1700331762996,
                "tmdate": 1700331762996,
                "mdate": 1700331762996,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3VZop3LgJw",
                "forum": "CH6DQGcI3a",
                "replyto": "N1d9X9X0HV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5966/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5966/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part II"
                    },
                    "comment": {
                        "value": "> Why does $\\ell_2$ white-box  adversarial robustness matter?\n\nThe reasons for using $\\ell_2$ norm perturbations are manifold:\n\n- We acknowledge that $\\ell_2$ threat model may not seem particularly realistic in practical scenarios (at least for images); however, it can be perceived as a basic threat model amenable to both theoretical and empirical analyses, potentially leading insights in tackling adversarial robustness in more complex settings. The fact that, despite considerable advancements in AI/ML, we are yet to solve adversarial vulnerability, motivates part of our community  to return to the basics and work towards finding fundamental solutions to this issue.\n    \n- In particular, thanks to their intuitive geometric interpretation, $\\ell_2$ perturbations provide valuable insights into the geometry of classifiers. They can serve as an effective tool in the \"interpretation/explanation\" toolbox to shed light on what/how these models learn.\n\n- Moreover, it has been demonstrated that (see, for example,[4] and [9]), $\\ell_2$ robustness has several applications beyond security.\n      \n- While not the most compelling reason, the continued interest of the community in $\\ell_2$ perturbations is evidenced by publications in top venues, including FMN [7] in NeurIPS 2021, DDN [5] in CVPR 2019, ALMA [10] in ICCV 2021, FAB [8] in ICML 2021, among many others.\n\n*A note on the spectral properties of $\\ell_2$ perturbations:* It is not entirely accurate to categorize $\\ell_2$ perturbations as \u201chigh-frequency\u201d. On the contrary, these perturbations tend to align more with \"low-frequency\" bands (see, e.g., [11]).\n\n\n> Why median and number of grads are used as the metric, while DeepFool used another one?\n\nTo ensure a fair comparison, we employed the metrics commonly used in recent literature on adversarial robustness. Specifically, we employed the two most prevalent metrics: the median/mean of perturbation norms (taken over dataset samples) to evalaute the method's accuracy, and the number of backward passes to measure the computational complexity.\n\nNevertheless, one could argue that DF's $\\rho$ (which normalizes the perturbation norm by the input sample norm) is conceptually similar to the mean of norms of perturbations. This is especially true given that the norm of samples tend to be quite concentrated for high dimensional data like CIFAR-10 and ImageNet. Hence, it can be said that they are almost equivalent up to a scaling factor. We can compute this metric, though, if the reviewer insists on its importance.\n\n\n> Why does experiment with adversarial training with SuperDeepFool not have models trained with PGD adversarial examples?\n\nWe excluded PGD AT, as DDN already outperformed it. For completeness, here is a modified version of Table 6 that includes PGD AT results.\n\n| Attack | SDF (Ours) Mean | SDF (Ours) Median | DDN Mean | DDN Median | PGD AT Mean | PGD AT Median |\n|--------|-----------------|-------------------|----------|------------|-------------|---------------|\n| DDN    | 1.09            | 1.02              | 0.86     | 0.73       | 0.68        | 0.61          |\n| FAB    | 1.12            | 1.03              | 0.92     | 0.75       | 0.84        | 0.71          |\n| FMN    | 1.48            | 1.43              | 1.47     | 1.43       | 1.31        | 1.27          |\n| ALMA   | 1.17            | 1.06              | 0.84     | 0.71   | 0.74        | 0.70          |\n| SDF    | **1.06**        | **1.01**          | 0.81 | 0.73       | 0.69        | 0.64          |\n\n> Lots of space could be saved by combining tables and figures in one row (for example Table 3 and Table 8)\n\nThank you for this comment. \u064ce have attempted to improve the presentation by putting Tables 3 and 8 in-line with the text."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5966/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700331800046,
                "cdate": 1700331800046,
                "tmdate": 1700401814583,
                "mdate": 1700401814583,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9QmzJu1viS",
                "forum": "CH6DQGcI3a",
                "replyto": "N1d9X9X0HV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5966/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5966/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part III (References)"
                    },
                    "comment": {
                        "value": "[1]: Madry, Aleksander, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. \"Towards deep learning models resistant to adversarial attacks.\" arXiv preprint arXiv:1706.06083, 2017.\n\n[2]: Dosovitskiy, Alexey et al. \"An image is worth 16x16 words: Transformers for image recognition at scale.\" arXiv preprint arXiv:2010.11929, 2020.\n\n[3]: Bai, Yutong et al. \"Are transformers more robust than cnns?\" Advances in neural information processing systems, vol. 34, pp. 26831-26843, 2021.\n\n\n[4]: Ortiz-Jim\u00e9nez, Guillermo, Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. \"Optimism in the face of adversity: Understanding and improving deep learning through adversarial robustness.\" Proceedings of the IEEE 109, no. 5 (2021): 635-659.\n\n[5]: Rony, Jerome et al. \"Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\n\n[6] Carlini, Nicholas and David A. Wagner. \u201cTowards Evaluating the Robustness of Neural Networks.\u201d 2017 IEEE Symposium on Security and Privacy (SP) (2016)\n\n[7]: Pintor, Maura et al. \"Fast minimum-norm adversarial attacks through adaptive norm constraints.\" Advances in Neural Information Processing Systems, vol. 34, pp. 20052-20062, 2021.\n\n[8]: Croce, Francesco and Hein, Matthias. \"Minimally distorted adversarial examples with a fast adaptive boundary attack.\" International Conference on Machine Learning, pp. 2196-2205, 2020, PMLR.\n\n[9]: Engstrom, Logan, et al. \"Adversarial robustness as a prior for learned representations.\" arXiv preprint arXiv:1906.00945 (2019).\n\n[10]: Rony, Jerome, Luiz G. Hafemann, Luiz S. Oliveira, Ismail Ben Ayed, Robert Sabourin, and Eric Granger. \"Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\n\n[11]: Ortiz-Jimenez et al. \u201cHold me tight! Influence of discriminative features on deep network boundaries\u201d, Advances in Neural Information Processing Systems, 2020."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5966/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700331817568,
                "cdate": 1700331817568,
                "tmdate": 1700331817568,
                "mdate": 1700331817568,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wU1OElRtXg",
                "forum": "CH6DQGcI3a",
                "replyto": "9QmzJu1viS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5966/Reviewer_34Xw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5966/Reviewer_34Xw"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your rebuttal! Many of my comments were addressed in details.\n\nI agree that adversarial robustness is less security problem and more deep learning theory problem. What are the potential insights can be concluded by trying hard to find the minimal possible distance to decision boundaries of classification models with limited number of categories? \n\nAnother question: Although SDF adversarially trained model improves robustness to 30 %, how does it change the natural accuracy? How's it compared to other adversarially trained techniques that seek accuracy-robustness trade-off?"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5966/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700608099226,
                "cdate": 1700608099226,
                "tmdate": 1700608099226,
                "mdate": 1700608099226,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FYgA8x5mQq",
                "forum": "CH6DQGcI3a",
                "replyto": "N1d9X9X0HV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5966/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5966/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We are happy that we could address many of your concerns.\n\n### Why $\\ell_p$?\n\nSeveral possibilities have been explored in the existing literature. For instance, training on $\\ell_p$-norm adversarial examples is shown to be a form of spectral regularization [A], while adversarial perturbations\u2014viewed as counterfactual explanations\u2014are linked to saliency maps in image classifiers [B]. The fast yet accurate generation of such perturbations is crucial for the empirical study of these phenomena. Additionally, minimal $\\ell_p$ adversarial perturbations can be viewed as \"first order approximations of the decision boundary\", shedding light on the local geometric properties of models in the vicinity of data samples. It similarly demands fast yet accurate methods for exploration. Furthermore, these minimal adversarial perturbations offer a data-dependent, worst-case analysis for certain test-time corruptions and enable worst-case assessments in the transformation space, not just the input space [C]. In the context of Large Language Models (LLMs), we can speculate that such perturbations could serve as probing tools within their embedding space to explore their geometric properties. Nevertheless, we must acknowledge that, like many other researchers, we were primarily motivated by academic curiosity rather than their practical applications in this particular work.\n\n[A] Roth et al., \"Adversarial Training is a Form of Data-dependent Operator Norm Regularization\", NeurIPS 2020.\n\n[B] Etmann et al., \"On the Connection Between Adversarial Robustness and Saliency Map Interpretability\", ICML 2019.\n\n[C] Kanbak et al., \"Geometric robustness of deep networks: analysis and improvement\", CVPR 2018.\n\n### Adversarial training\n\nThe natural accuracies are as follows; SDF AT: 90.8%, DDN AT: 89.0%, PGD AT: 87.3%.\n\nThe primary goal was to determine which **adversarial generation technique** improves robustness most effectively (PGD, DDN, or SDF), rather than comparing various **adversarial training strategies** (vanilla (Madry's) AT, TRADES, TRADES-AWP, HAT) Such approaches normally incorporate additional regularization techniques to improve Madry's method with PGD adversarial examples. Thus, our claim is not about developing a SotA robust model. Instead, our goal is to illustrate that vanilla AT, when coupled with minimum-norm attacks such as SDF, may yield better performance compared to PGD-based models. Therefore, we opted for vanilla adversarial training with SDF-generated samples and compared it to a network trained on DDN samples. Certainly, TRADES or similar AT strategies can be employed in conjunction with SDF. This is the subject of future work."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5966/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656279047,
                "cdate": 1700656279047,
                "tmdate": 1700656299096,
                "mdate": 1700656299096,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "taB8jV8tr0",
                "forum": "CH6DQGcI3a",
                "replyto": "FYgA8x5mQq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5966/Reviewer_34Xw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5966/Reviewer_34Xw"
                ],
                "content": {
                    "comment": {
                        "value": "I have read the comments of other reviewers and all replies. Thanks!\n\nIt is still not clear what benefits/insights might bring the search for the closest point on the decision boundary. We hope this curiosity-driven research will find some other useful applications. Despite this, the proposed methodology has several positive aspects that imho make this particular paper above the acceptance threshold:\n\n1) The method has been evaluated on several datasets, including large-scale ImageNet. Community has witnessed lots of methods that work on small datasets but fail in practice, and ImageNet-level experiments bring the method closer to practice.\n2) The attack was tested and outperformed not only on vanilla models, but also on robust models. There are indeed might be substantially different geometries of robust and non-robust models. Thus, the method that works on both is a good sign. Moreover, by request, authors also achieved positive results on ViT-B, which might also have completely different underlying geometries. It's encouraged to add ImageNet experiments with ViTs in the updated version.\n3) The attack can be easily implemented for a batch-parallelized version and computationally efficient, which makes it a good choice for training robust models.\n4) The method is indeed simple and surprisingly works\n\nTherefore, I am changing my score to 6."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5966/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681619866,
                "cdate": 1700681619866,
                "tmdate": 1700681619866,
                "mdate": 1700681619866,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DoT7KhnkkK",
            "forum": "CH6DQGcI3a",
            "replyto": "CH6DQGcI3a",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5966/Reviewer_Qken"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5966/Reviewer_Qken"
            ],
            "content": {
                "summary": {
                    "value": "The authors demonstrate that the l2 norm of the perturbations estimated by deepfool can be scaled down, while still fooling the model. Further, the authors demonstrate that these perturbations are not orthogonal to the decision boundary. These observations show that the perturbations generated by deepfool are not optimal. The authors propose a new projection step in the deep fool algorithm, which can help in aligning the perturbation to become perpendicular to the decision boundary. This helps in decreasing the minimum norm l2 distance required to fool the model and also helps in generating stronger attacks. Empirically the authors verify this by showing improved performance on metrics like the l2 norm of the perturbation required to fool the model and the attack success rate. Finally the authors incorporate the proposed super deepfool to the autoattack package and demonstrate around 0.1-0.3% improved attack strength."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The proposed approach is well motivated and the solution provided is simple.\n\n* The results demonstrate improved attack strength over the existing minimum norm attacks."
                },
                "weaknesses": {
                    "value": "* It is not clear why the attack is strongest when the number of projection steps (value of n) is 1 and the number of deep fool attacks (value of m) is as large as possible. A detailed discussion on this would be helpful.\n\n* The comparison is done primarily for the l2 norm, it would be nice if the authors could also try their method for other norms like l-infinity norm.\n \n* Some parts of the paper seem a bit disjoint/not relevant. For example, it is not clear why the authors propose SDF adversarial training. The purpose of the paper is to demonstrate that the attack strength of deepfool can be improved, showing results on using super deepfool for adversarial training, doesn't seem to help strengthening this claim. In case the authors want to demonstrate the utility of using super deepfool for adversarial training, the authors should consider a more rigorous comparison with adversarial training methods like PGD [3], Trades [4] and Trades-AWP [5]. Just including Trades-AWP [5] as a baseline should also work.\n\n* Authors should also include comparison with some strong adversarial attacks which are not trying to find the minimum norm. For instance a comparison against guided margin aware attack [1] can help in developing a better understanding. The authors can also try to determine the norm from the proposed super deepfool and then try strong attacks like carlini and wagner attack [2], multi-targeted attack and guided margin aware attack [1] on the threat model with this norm.\n\n* It would be nice if the authors consider improving the presentation of tables. For instance, Table-3,8, Fig-3 have a lot of whitespace left on the right and left hand, which does not look good. The quality of the figure could also be improved. Further, I think it would be fine if the authors present merge Algorithm-1 and 2. There is a lot of duplicated content in these algorithms.\n\n* It would be nice if the authors incorporate their attack with the autoattack package and analyze a few more defences. Since Table-8 currently shows mixed trends, therefore a more rigorous analysis is required to justify the claims.\n\n\n[1] Sriramanan, Gaurang et al. \u201cGuided Adversarial Attack for Evaluating and Enhancing Adversarial Defenses.\u201d ArXiv abs/2011.14969 (2020)\n\n[2] Carlini, Nicholas and David A. Wagner. \u201cTowards Evaluating the Robustness of Neural Networks.\u201d 2017 IEEE Symposium on Security and Privacy (SP) (2016)\n\n[3] Madry, Aleksander et al. \u201cTowards Deep Learning Models Resistant to Adversarial Attacks.\u201d ArXiv abs/1706.06083 (2017)\n\n[4] Zhang, Hongyang et al. \u201cTheoretically Principled Trade-off between Robustness and Accuracy.\u201d ArXiv abs/1901.08573 (2019)\n\n[5] Wu, Dongxian et al. \u201cAdversarial Weight Perturbations Helps Robust Generalization.\u201d ArXiv abs/2004.05884 (2020): n. pag."
                },
                "questions": {
                    "value": "I would request the authors to kindly address the comments in the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5966/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5966/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5966/Reviewer_Qken"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5966/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699093368092,
            "cdate": 1699093368092,
            "tmdate": 1700732965723,
            "mdate": 1700732965723,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aAuxA60iR5",
                "forum": "CH6DQGcI3a",
                "replyto": "DoT7KhnkkK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5966/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5966/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Part I"
                    },
                    "comment": {
                        "value": "We are pleased that the reviewer appreciates the simplicity and efficiency of our proposed method. We have addressed their comments in detail below. If there are any further aspects we can clarify or improve to positively influence their view of our work, please let us know.\n\n> It is not clear why the attack is strongest...\n\nLike any other gradient-based optimization method tackling a non-convex problem, providing a definitive explanation for why one algorithm outperforms others is not straightforward. We have the following speculation on why SDF($\\infty$,1) consistently outperforms the other configurations: Note that each projection step reduces the perturbation, while each DF step moves the perturbation nearer to the boundary. So when projection is repeated multiple times ($n>1$), it might undo the progress made by DF, potentially slowing down the algorithm's convergence. On the other hand, by first reaching a boundary point through multiple DF steps and then applying the projection operator just once, we at least ensure that the algorithm has reached intermediate adversarial examples. Each subsequent outer loop is hoped to incrementally move the adversarial example closer to the optimal point. (see Figure 3).\n\n\n> The comparison is done primarily for the $\\ell_2$ norm...\n\nMany thanks for the suggestion. Although, SDF is primarily designed to find minimal $\\ell_2$ perturbations, we tried a simple idea to extend it to $\\ell_\\infty$ norm by substituting the $\\ell_2$-projection step (line 5 in Algortihm 2) with the $\\ell_\\infty$ projection:\n\n$\\boldsymbol{x}\\gets \\boldsymbol{x}_0 + \\frac{(\\widetilde{\\boldsymbol{x}}-\\boldsymbol{x}_0)^\\top\\boldsymbol{w}}{||\\boldsymbol{w}||_1} \\text{sign}(\\boldsymbol{w})$\n\nThe table below displays the results obtained for the $\\ell_\\infty$ norm. We conducted multiple $\\ell_\\infty$ attacks, namely FMN, FAB, and DF, on M1 [3] and M2 [6] adversarially trained networks on the CIFAR10 dataset. Our findings indicate that this variant of SDF also exhibits superior performance compared to other algorithms in discovering smaller $\\ell_\\infty$ perturbations.\n\n| Attacks | M1    |  FR   | Grads | M2   | FR   | Grads |\n|---------|------------|---------|-------|------|------|-------|\n|     DF     | 0.031 | 96.7   | 24| 0.043 | 97.4 | 31|\n| FAB    | 0.025 | 99.1   | 100   | 0.038 | 99.6 | 100   |\n| FMN   | 0.024 | 100 | 100 | 0.035 | 100 | 100 |\n| SDF     | **0.019** | 100 | 33  | **0.027** | 100 | 46   |\n\nWe will add this result to the Appendix.\n\n\n> Some parts of the paper seem a bit disjoint/not relevant.\n\nAdversarial training is more effective when conducted with strong, yet computationally efficient adversaries. Our proposed method is both fast and more accurate than the SotA in identifying adversarial perturbations, making it a logical choice for evaluating its effectiveness in enhancing the robustness of models through adversarial training.\n\nThe primary goal was to determine which **adversarial generation** technique improves robustness most effectively (PGD, DDN, or SDF), rather than comparing various **adversarial training strategies** (vanilla (Madry's) AT, TRADES, TRADES-AWP, etc.)  Such approaches normally incorporate additional regularization techniques to improve Madry's method with PGD adversarial examples. Thus, our claim is not about developing a SotA robust model. Instead, our goal is to illustrate that vanilla AT, when coupled with minimum-norm attacks such as SDF, may yield better performance compared to PGD-based models. Therefore, we opted for vanilla adversarial training with SDF-generated samples and compared it to a network trained on DDN samples. Certainly, TRADES or similar AT strategies can be employed in conjunction with SDF, but we believe that falls beyond the scope of this paper.\n\nDespite DDN AT already outperforms PGD AT, we have added PGD results for the sake of completeness.\n\n| Attack | SDF (Ours) Mean | SDF (Ours) Median | DDN Mean | DDN Median | PGD AT Mean | PGD AT Median |\n|--------|-----------------|-------------------|----------|------------|-------------|---------------|\n| DDN    | 1.09            | 1.02              | 0.86     | 0.73       | 0.68        | 0.61          |\n| FAB    | 1.12            | 1.03              | 0.92     | 0.75       | 0.84        | 0.71          |\n| FMN    | 1.48            | 1.43              | 1.47     | 1.43       | 1.31        | 1.27          |\n| ALMA   | 1.17            | 1.06              | 0.84     | 0.71   | 0.74        | 0.70          |\n| SDF    | **1.06**        | **1.01**          | 0.81 | 0.73       | 0.69        | 0.64          |\n\n> Authors should also include comparison with some strong adversarial attacks...\n\nWe appreciate if the reviewer could clarify their point. From our understanding, C\\&W is a minimum-norm attack, and we have indeed included a comparison with it in our work. Once we fully grasp the specific type of comparison the reviewer is requesting, we will promptly provide it."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5966/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700323230756,
                "cdate": 1700323230756,
                "tmdate": 1700326263429,
                "mdate": 1700326263429,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DUEF8UryVa",
                "forum": "CH6DQGcI3a",
                "replyto": "mYJ2DR90SI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5966/Reviewer_Qken"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5966/Reviewer_Qken"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Authors"
                    },
                    "comment": {
                        "value": "I sincerely thank the authors for their efforts in responding to my concerns. I feel most of my concerns have been addressed other than these two:\n1) I think that authors should perform a fine-grained analysis of the effect of different values of m,n on the strength of the attack. A plot for this in the camera-ready would be helpful.\n2) I don't see any reason why authors performed AA+ analysis on [6], [7]. I checked the results reported on robustbench and these models don't seem to perform on par with other methods.\n\nI request the authors to address the above two points in the camera-ready version. It would be great if the authors could include a table comparing all the methods on the robustbench leaderboard. I believe penalising authors for these things at this stage would be wrong, so I am happy to raise my score from 5 to 6."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5966/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732948306,
                "cdate": 1700732948306,
                "tmdate": 1700732948306,
                "mdate": 1700732948306,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]