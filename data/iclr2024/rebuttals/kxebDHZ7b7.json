[
    {
        "title": "TRAM: Bridging Trust Regions and Sharpness Aware Minimization"
    },
    {
        "review": {
            "id": "CBLkHEBNUv",
            "forum": "kxebDHZ7b7",
            "replyto": "kxebDHZ7b7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5847/Reviewer_cqYa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5847/Reviewer_cqYa"
            ],
            "content": {
                "summary": {
                    "value": "This paper considered adopted trust regions and sharpness aware minimization. Specifically, the gradient update is based on two key steps. Equation (4) illustrates the update rule by KL divergence fine-tuning. Equation (5) illustrates the update of the input by adding a gaussian random variable. Then the model is further validated in GPT2 on different dataset benchmarks."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper considered an improved method in TRPO. Through smoothly updating the gradient, this method seems to transfer the information.\n- Empirical validation in the foundation model and large scale NLP dataset are done."
                },
                "weaknesses": {
                    "value": "- I could not understand *Why* such as transfer could encourage a better transfer. When it does not work? I would like to see a *rigor* mathematical analysis. The sharpness aware minimization could achieve better generalization. How is this mathematically to ensure a better transfer?\nI noticed the experiments illustrated \n> domain transfer in fine-tuned models by better leveraging the pre-trained structure from unseen domains within the smoother minima idealized by SAM-style training. \n\nHowever, without clear analysis. This reviewer feels quite difficult to understand why SAM could achieve this objective. Does this approach only work for the selected dataset? **When it fails?** \n\n- I would like to see a computational/memory complexity analysis. How it compares with other methods.\n\n- This paper proposes a general machine learning method while it is only validated in the NLP dataset. Unless the author clearly revised the title and contributions, I would like to see the results in other modalities such as image. \n\n- Equation (3) is not clearly defined. What does it mean by d_{\\theta, x}? It is not a rigorous expression.\n- Eq(4), Eq(5) why forward KL divergence is considered? Why not reverse KL divergence? Or Other general forms such as Renyi divergence?\n- In eq(5), how important is the noise variable? Does the variance of the noise matter?\n- Eq(12) may not be effective to correctly estimate the similarity in high dimensional regime. I think there is a complexity issue here (this is not a sample efficient estimator). I could think this value does not make sufficient sense to me in a high-dimensional case.\n- Table 4 Why only accuracy is considered a metric? Is this dataset balanced?"
                },
                "questions": {
                    "value": "I noticed the primary domain is about optimization.  While there are so many missing points in terms of rigorous analysis in the optimization. If this paper is an applied NLP paper, the paper should be revised in a major form."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5847/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698367214916,
            "cdate": 1698367214916,
            "tmdate": 1699636618388,
            "mdate": 1699636618388,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HDGTt0Bfu8",
                "forum": "kxebDHZ7b7",
                "replyto": "CBLkHEBNUv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5847/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5847/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Part 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their time and attention in reviewing our work. We hope our additional results and insight will consider the reviewer in revising their score. \n\n**Q1. On formal analysis**\n\nPlease see our response below adapting ViT-base-16 to CIFAR-100 / Cars / Flowers dataset for validation that TRAM also is applicable for computer vision tasks similar to [1,2,3]. We also address that TRAM requires some initial pre-training for applicability in fine-tuning. We can therefore broadly state that TRAM does not work training from scratch at this stage. \n\nWe did not discuss the generalization bound of TRAM as we generally consider TRAM as a subsolution to ASAM [2]. The generalization bound of ASAM is described as Theorem 3 from [2] and also derived formally in [4]. On the assumption that $d$ is normally distributed, we expect the generalization bound of TRAM to converge to the same bound as ASAM. ASAM already describes a bound valid for any $\\rho>0$ where we ensure that the TR distance is similarly nonzero. \n\n[1] Sharpness-Aware Minimization for Efficiently Improving Generalization https://arxiv.org/pdf/2010.01412.pdf \n[2] ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Networks https://arxiv.org/pdf/2102.11600.pdf \n[3] Fisher SAM: Information Geometry and Sharpness Aware Minimisation https://proceedings.mlr.press/v162/kim22f/kim22f.pdf\n[4] The intriguing role of module criticality in the generalization of deep networks https://arxiv.org/abs/1912.00528 \n\n**Q2. Complexity**\n\nWe generally address this by discussing the number of forward and backward passes required for each algorithm (Table 2). The complexity of SAM / ASAM is not discussed in detail by the original authors, similarly referring to the number of forward and backward passes [1,2,3]. \n\nAt a high level, SAM-style training also needs to permute each parameter in the network.  Permuting each parameter requires some operations to permute $\\theta$ to $\\theta + \\epsilon$. This scales with the number of parameters $N$ with some $k$ operations per parameter (e.g., to compute Equation (2) / Equation (7)).  Computing $\\epsilon^{\\ast}$ (e.g., Eq(2) or (7)) requires one computation of the model parameter global norm, $g$, for all algorithms. ASAM / TRAM requires one matrix-matrix product to compute $\\theta^2$ per parameter \u2014we denote $M^{3}$ for worst case complexity of this operation. TRAM does not require additional computation here as the distance replaces the scalar $\\rho$. \n\nWe can approximate this as:\n\n\tSGD: 1 forward pass, 1 backward pass\n\tSAM: 2 forward pass, 2 backward pass. $Nk + g$ operations for permuting $\\theta$\n\tASAM: 2 forward pass, 2 backward pass. $N(k + M^{3}) + g$ operations for permuting $\\theta$\n\tTRAM: 3 forward pass, 2 backward pass. $N(k + M^{3}) + g$ operations for permuting $\\theta$ + constant operation to compute the TR distance. \n\nWe empirically observe that the 2 backward passes dominate the learning process as the complexity bottleneck. TRAM has minimal additional overhead beyond ASAM. The permutation operations for $\\theta$ to $\\theta + \\epsilon$ can be parallelized to reduce wall clock runtimes. The memory overhead of TRAM is minimal compared to SAM or ASAM. There is no gradient passed through the permuted parameters for estimating the TR distance, and the final TR distance is a scalar replacing an existing hyper-parameter. Our empirical conclusion is that TRAM has minimal (<10%) increase in wall clock runtime and no additional memory requirements beyond what is required for ASAM.\n\n[1] https://arxiv.org/abs/2010.01412\n[2] https://arxiv.org/pdf/2102.11600.pdf\n[3] https://arxiv.org/abs/2206.04920"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5847/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689529414,
                "cdate": 1700689529414,
                "tmdate": 1700689529414,
                "mdate": 1700689529414,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0SlKGUf7xx",
            "forum": "kxebDHZ7b7",
            "replyto": "kxebDHZ7b7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5847/Reviewer_2Ruf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5847/Reviewer_2Ruf"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors aim to propose a SAM variant to contribute to the training in the area of model fine-tuning, where they propose the Trust Region Aware Minimization. In the method, specific distance measures in TRR are employed as the neighbourhood radius in SAM rather than the manually-set pre-defined radius. The authors claim that the proposed method can optimizer for informative representations without forgetting pre-trained structure. And the authors investigate the perplex on M2D2 Corpus with GPT2 to show the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**Strengths**\n\n1. The paper is clearly written and easy to follow.\n2. I think the paper aims to contribute to SAM from a very interesting perspective, i.e. fine-tuning techniques. Considering that fine-tuning has become a nearly necessary procedure in NLP tasks, the paper may provide some promising instructions further.\n3. Combine the proposed method with Fisher-SAM can reduce extra forward-propagation count when implementing to the same count as in vanilla SAM."
                },
                "weaknesses": {
                    "value": "**Weakness**\n\n1. The core of this proposed method is to adaptively change the neighbourhood radius in SAM (or ASAM) based on certain distance measure. This somehow does not follow the idea of Trust Region Regularization which adds additional constraint on top of the loss according to the measure. More accurately, they are two different things. And, I could not find a clear meaning why using such a distance as the neighbourhood radius could give the \"Trust\". Several questions arise: what does the \"trust\" indicate in the proposed method? Why we should not trust the region that is not in the proposed method but in SAM (and ASAM, Fisher-SAM)? Why the given region would not harm the pre-trained models while SAM could? Clear answers are missing in the current paper. Also, it is highly recommended that the authors use figures to illustrate this and the core of the presented method.\n\n2. The Stochastic Weight Averaging (SWA) could also lead to a similar effect as TTR methods. The authors may need to also consider or compare SWA with the TRR and the proposed method. The following papers may be helpful.\n\n    [1] Kaddour, Jean, et al. \"When do flat minima optimizers work?.\" Advances in Neural Information Processing Systems 35 (2022): 16577-16595.\n\n    [2] Wortsman, Mitchell, et al. \"Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time.\" International Conference on Machine Learning. PMLR, 2022.\n\n3. From the paper, I see nearly no discussions regarding why and how the proposed method could contribute the fine-tuning in the related section, given that the authors claim \"their method could not forget the pre-trained structure\". BTW, I think their abstract may be somewhat over-claimed. It is interesting to see that the authors are aiming to study the effect of SAM specifically in fine-tuning. But unfortunately, the current version could not present sufficient helpful insights.\n\n4. It would be more impactful that the the authors present results using the proposed method on some recent popular scalable pre-trained llm such as llama. \n\n\n5. It is highly recommended that the authors release their code."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I have not found any discussions about the limitations and potential negative societal impact. But in my opinion, this may not be a problem, since the work only focuses on the learning method in machine learning. Still, it is highly encouraged to add corresponding discussions."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5847/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5847/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5847/Reviewer_2Ruf"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5847/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698674123048,
            "cdate": 1698674123048,
            "tmdate": 1699636618281,
            "mdate": 1699636618281,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mT487E7P11",
                "forum": "kxebDHZ7b7",
                "replyto": "0SlKGUf7xx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5847/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5847/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Part 1A"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their time and attention in considering TRAM. We provide the first part of our response to address your questions. We are still working on experiments for the vision domain which we hope will provide further insight into TRAM for the reviewer. We will update our response when these are finalized. \n\n**Q1: the notion of \"Trust\" in the model.**\n\nWe thank the reviewer for highlighting where the explanation of trust regions can be improved in the paper. We will expand on this here and include this in future revisions to the paper. We will also improve the visual/graphical description of the method in future revisions. \n\nThe notion of the trust region is the local neighborhood around the current function where we can assume an adequately similar representation of the objective function [1,2]. Prior neural TR methods will include this constraint as a direct regularizer on the loss [2,3] without access to the representational probability density [2]. The brief interpretation of this constraint is to iteratively encourage smooth updates to the objective which retain local function similarity to the previous iterate. This regularizer translates the TR objective constraining large changes in the function space into an adjustment to the change in the parameters for a given learning step (i..e, a constraint in $\\delta f$ translated into $\\delta \\theta$). The prior work inspiring TRAM proposes two efficient strategies to approximate the region around the current objective function [2,3] and translating this to a penalty on $\\theta$. In either case (i.e., TRAM using Eq (4) or (5)), we introduce a measurement on change in the function space to inform a change in $\\theta$. \n\nOur connection to SAM [4] is based on the similarity in geometry between defining the aforementioned change in $\\theta$, permitting safe change in the function space, and the $\\rho$-ball domain containing sharpness maximization. As noted in [5], the domain of the SAM $\\rho$-ball is somewhat arbitrary in that the setting of $\\rho$ has little relationship to the parameter geometry or training dynamics. FSAM [4] reformulates SAM to respect the parameter geometry. However, in TRAM we instead propose to define this maximization domain by the space in $\\theta$ which permits only \u201ctrusted\u201d change in the function. The connection to \u201ctrust\u201d in TRAM is to reinterpret what is originally a regularizer in [3,4] to define a search space where SAM-style optimization is permitted. SAM-type algorithms do not contain this notion of \u201ctrust\u201d, as the maximization domain has no relationship to the function space and cannot ensure similar smooth changes in function density (e.g., there is no control for staying close to the original space within SAM/ASAM). Therefore, training with SAM does not ensure, or optimize for, smooth curvature between pre-trained and fine-tuned functions. We note that SAM-style algorithms could yield this assurance empirically with a large $\\rho$ to always contain the trust region (i.e., Eq (2) > Eq (7) for all $\\theta$ and $t$). However, we empirically observe that this is not the case \u2014 a larger setting of $\\rho$ for SAM produces poorer results than TRAM. TRAM could be considered a subsolution of ASAM\u2014our contribution highlights that sensitivity to the function space trust region produces better empirical outcomes with less catastrophic forgetting. \n\n[1] Better Fine-Tuning by Reducing Representational Collapse https://arxiv.org/abs/2008.03156\n[2] Natural Gradient Revisited https://openreview.net/forum?id=jbLdjjxPd-b2l \n[3] Trust Region Policy Optimization https://arxiv.org/abs/1502.05477 \n[4] Sharpness-Aware Minimization for Efficiently Improving Generalization https://arxiv.org/pdf/2010.01412.pdf \n[5] Fisher SAM: Information Geometry and Sharpness Aware Minimisation https://proceedings.mlr.press/v162/kim22f/kim22f.pdf"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5847/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576491227,
                "cdate": 1700576491227,
                "tmdate": 1700576491227,
                "mdate": 1700576491227,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5fNoP6aqwo",
                "forum": "kxebDHZ7b7",
                "replyto": "0SlKGUf7xx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5847/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5847/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Part 1B"
                    },
                    "comment": {
                        "value": "**Q2: commentary on SWA**\nWe thank the reviewer for highlighting this recent work. We did examine SWA in our early experiments but did not find the results to be very competitive with Trust region and SAM-style training in the cross-lingual transfer context. These results were omitted from the paper purely as we considered this additional discussion to be a distractor from the contrast between TRAM and the parent SAM and TR methods. Third party feedback has also requested this comparison and we will include this in future revisions.\n\nHere we show our results for SWA on XNLI compared to SAM/ASAM/FSAM and TRAM.\n\n| Algorithm               | {\\sc ZS Avg} |\n|-------------------------|--------------|\n| Adam                    | 72.9         |\n| SAM                     | 73.7         |\n| ASAM                    | 74.0         |\n| FSAM                    | 73.8         |\n| **SWA**                     | 73.4         |\n| TRAM-$d_{x}$            | 75.2         |\n| TRAM-$d_{\\theta_{t-1}}$ | 75.0         |\n| TRAM-$d_{\\theta_{0}}$   | 74.9         |\n| TRAM-Fisher             | 74.4         |\n\n\nWe identified several trends within our SWA results which warrant further study and we are actively exploring. The TL;DR of these findings is that weight averaging for optimal in-domain and out-of-domain performance is extremely variable across optimizers. We observe the same pattern as prior work in \u201cturning on\u201d SWA after some training, to average later checkpoints, works best for ID performance. However, we also identify that the best model for OOD uses only early checkpoints and training can stop after 30% of training. We also observe negligible correlation between performances for ID, OOD and where averaging starts/stops. We find SWA+SAM to perform below SAM alone but the inverse trend for Adam or SGD. We consider in-depth study of these phenomena as a different contribution which we plan to address in future work. \n\n**Q3: on the abstract claims**\n\nWe thank the reviewer for their feedback on the abstract and claims. We will revise these for the scope of our claims to be clearer. We ask the reviewer if they can clarify where they consider more insight to be necessary.\n\n**Q4: Larger model scales**\n\nWe agree with the reviewer that results on larger Llama scale models would be insightful. At present, we don\u2019t have the resources to run full fine-tuning of larger models with TRAM.  In Appendix C.2, we highlight that training GPT2-XL (1.5B params) with TRAM yields a more generalizable model than other baseline algorithms including SAM, ASAM, FSAM and Trust Region methods. This is intended to provide some insight into if larger scales diminish the benefit of optimization contributions (where we find that TRAM is still beneficial at the 1.5B model scale). This echoes other recent work which finds that Sharpness-Aware optimization is still beneficial at many model scales [1]. We conjecture that TRAM would benefit a larger model as it has for 100M \u2013 1.5B scales.\n\n[1] Sharpness-Aware Minimization Improves Language Model Generalization https://aclanthology.org/2022.acl-long.508/\n\n**Q5: Code release**\n\nWe include an anonymised code repository for critical code elements of TRAM https://anonymous.4open.science/r/tram_optimizer-0448/tram.py."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5847/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576644076,
                "cdate": 1700576644076,
                "tmdate": 1700576683332,
                "mdate": 1700576683332,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0O4h2d0fEN",
                "forum": "kxebDHZ7b7",
                "replyto": "5fNoP6aqwo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5847/Reviewer_2Ruf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5847/Reviewer_2Ruf"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response."
                    },
                    "comment": {
                        "value": "I have read the response, and thanks for the authors' experiments. It is interesting to see the results regarding the SWA. And thanks for the interpretation, but it seems not entirely clear for me. I know it is hard to contribute on this point, but this warrants further research. I think the paper may contribute to SAM from some interesting persepctive. Although the rationality behind isn't completely articulated , I still consider the paper as a viable candidate for acceptance."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5847/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625006724,
                "cdate": 1700625006724,
                "tmdate": 1700625006724,
                "mdate": 1700625006724,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aB7WnXOkh3",
            "forum": "kxebDHZ7b7",
            "replyto": "kxebDHZ7b7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5847/Reviewer_MZmv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5847/Reviewer_MZmv"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new optimization algorithm called TRAM that combines sharpness-aware minimization (SAM) with trust region regularization. SAM methods like ASAM optimize for low sharpness (flat minima) in parameter space. Trust region methods constrain optimization to a local neighborhood in representation space. TRAM unifies these approaches by bounding the SAM perturbation region using the trust region distance. This encourages flat minima while retaining representation smoothness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed method is intuitive and well-motivated. The combination of SAM and Trust region methods is reasonable and interesting.\n2. Extensive experiments on multiple NLP tasks demonstrate the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1. Theoretical motivation for unifying SAM and Trust region methods is not provided. \n2. Some results have high variance across runs. More runs may better characterize the performance."
                },
                "questions": {
                    "value": "1. How well does TRAM transfer to other modalities like images?\n2. There are several hyper-parameters of the proposed method. How to select them for out-of-distribution generalization?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5847/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5847/Reviewer_MZmv",
                        "ICLR.cc/2024/Conference/Submission5847/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5847/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698852328696,
            "cdate": 1698852328696,
            "tmdate": 1700719732482,
            "mdate": 1700719732482,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "oyzoT2U5TM",
                "forum": "kxebDHZ7b7",
                "replyto": "aB7WnXOkh3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5847/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5847/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Part 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their time and attention in reviewing our work. We now address each question in turn.\n\n**1. Theoretical Motivation**\n\nPlease see our response to 2RUf Q1 for further insight on the notion of Trust and cqYa Q1 for discussion on the generalisation bound. \n\n**2. Run variance**\n\nWe appreciate the concern for the run variation. At present, we do not have sufficient compute bandwidth for repeated runs of the LM objective but will endeavour for this in future revisions. All results for XNLI are averaged over 20 runs, which we consider sufficient for our claims and many more runs than is often claimed for experiments on cross-lingual transfer. The variance across different languages is more indicative of the task challenges than the run variance. We note that our vision experiments provided below do not report notably higher confidence intervals than other methods. \n\n**3. Vision modality**\n\nFor comparison, we implement the same experiments as [1] for FisherSAM training ViT-base-16 [2] for CIFAR-100 [3], Flowers [4] and Cars [5]. We fine-tune ViT-base-16 for 200 epochs training with TRAM and report the Top-1 accuracy(+ 95% confidence interval) averaged over 5 runs  to compare directly to Table 3 in [1]. We match the hyperparameter setup of [1] \u2014 the base optimizer is SGD with an initial learning rate of 5e-4 and a cosine decay LR schedule. Due to computational constraints, we only have the bandwidth to report the two main variants of TRAM on these tasks during the rebuttal. We plan to update with the other variants of TRAM (TRAM-$d_{\\theta_0}$ and TRAM-Fisher) in future revisions. \n\n|           | SGD        | SAM        | ASAM       | FSAM       | TRAM-$d_{\\theta_{t-1}}$ | TRAM-$d_{x}$ |\n|-----------|------------|------------|------------|------------|-------------------------|--------------|\n| CIFAR-100 | 87.97\u00b10.12 | 87.99\u00b10.09 | 87.97\u00b10.08 | 88.39\u00b10.13 | 88.47\u00b10.16              | **88.78**\u00b10.01   |\n| Cars      | 92.85\u00b10.31 | 93.29\u00b10.01 | 93.28\u00b10.02 | 93.42\u00b10.01 | **93.49**\u00b10.04              | 93.32\u00b10.11   |\n| Flowers   | 94.53\u00b10.20 | 95.05\u00b10.06 | 95.08\u00b10.10 | 95.26\u00b10.03 | **97.07**\u00b10.10              | 96.34\u00b10.03   |\n\nWe observe that TRAM performs competitively across all datasets, with one or both variants of TRAM performing above all other methods. The largest improvement for TRAM is Flowers where we perform +1.81% above FSAM. The smallest improvement is for Cars where we perform 0.07% above FSAM, but we not that the confidence intervals for these results do not overlap. \n\n[1] Fisher SAM: Information Geometry and Sharpness Aware Minimisation https://proceedings.mlr.press/v162/kim22f/kim22f.pdf\n[2] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale https://arxiv.org/abs/2010.11929 \n[3] Learning Multiple Layers of Features from Tiny Images https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf \n[4] C3D Object Representations for Fine-Grained Categorization http://vision.stanford.edu/pdf/3drr13.pdf \n[5] Automated Flower Classification over a Large Number of Classes https://ieeexplore.ieee.org/document/4756141"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5847/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689258285,
                "cdate": 1700689258285,
                "tmdate": 1700689258285,
                "mdate": 1700689258285,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cR9mM1QUCf",
                "forum": "kxebDHZ7b7",
                "replyto": "CelYWdanOR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5847/Reviewer_MZmv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5847/Reviewer_MZmv"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the rebuttal"
                    },
                    "comment": {
                        "value": "Thank the authors for addressing my concerns. The experiments on vision modality and the detailed explanation on hyper-parameter selection match my expectations. Therefore, I raise my score to 8."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5847/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719712337,
                "cdate": 1700719712337,
                "tmdate": 1700719712337,
                "mdate": 1700719712337,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MBkRWxvRGw",
            "forum": "kxebDHZ7b7",
            "replyto": "kxebDHZ7b7",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5847/Reviewer_AFpQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5847/Reviewer_AFpQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper develops a new SAM-style optimizer for better representation learning (especially for language modeling).\nThe key idea is simple-to-state (although the proposed algorithms are a bit more complicated than the original SAM).\nThe authors attempt to combine the best of both worlds between SAM and Trust-region:\n1. SAM encourages the solution to be \"flat\" in the parameter space (assuming that the flat minima is desired)\n2. Trust-region methods encourages the representation to be \"smooth\" or to stay close to the good \"pre-trained\" model initialization.\nIn particular, with the Trust-region feature, the authors claim that the proposed method has the benefit of not forgetting task-agnostic representations from pre-trained model and also learning \"smooth\" representation which is good for the transferability of representations.\n\nAlso, building on more advanced SAM algorithms like ASAM and FSAM, the authors develop other variants of TRAM which are used for the experiments and show better performance than the previous approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper did a good job summarizing the existing approaches and how the proposed method builds on top of them.\n- The experimental settings are detailed, and reasonable.\n- Experiments seem quite comprehensive at least for the settings considered in this work.\n- It's quite remarkable that the proposed methods achieve best performance across different fine-tuning tasks."
                },
                "weaknesses": {
                    "value": "- See the question section below."
                },
                "questions": {
                    "value": "- As far as I understood, the original SAM paper became popular because of its extensive experiments over standard benchmark datasets. In particular, I remember SAM achieving state-of-the art for various vision tasks (CIFAR, ImageNet etc...). Given that the effectiveness of SAM was first demonstrated on these benchmark tasks, **for future research I think it is required for follow-up works to sanity check the performance of the proposed methods on the same setting as the original SAM paper**. In particular, if the new methods end up giving a worse performance than SAM for the settings considered in the SAM paper, that would be an important information for practitioners.\n(As I mentioned in the strength part, the authors' quite comprehensive experiments on the language modeling tasks of choice look great; however, since this work follows up on the original SAM paper, **some experiments that benchmark the new method against the original SAM on the task that SAM did great seems required**.)\n\n- Also, it's great that the authors built their default algorithm on ASAM. But, given that SAM has been quite popular, as a reader, I'm quite curious how the SAM version of TRAM (instead of ASAM) performs. In particular, **is ASAM type of updates really necessary?**\n\n- Given that the main motivation of this work is to develop an optimizer for learning good-representation, I think at least one experiment is needed for pre-training from scratch. In particular, do you think having a reasonable pre-trained model is necessary for TRAM to work well? As far as I know, it's still debated in the literature **whether SAM-type of updates are required in the beginning of the training or at the end of the training**. Some theoretical works have claimed that it's only effective at the end of the training (**as did in this work**), but empirical works also have claimed that it's required from the beginning of the training.\n\n- In Figure 1, could you clarify what the negative and positive slopes are supposed to be interpreted as? Also I can't really understand how to interpret this plot.\n\n- In Table 5, why didn't you present the statistics for the other two variants of TRAM?\n\nI acknowledge the novelty of this work. However, given the extensive experiments in the previous works (e.g. original SAM work), in order to make a case about the effectiveness of the proposed methods, **I think some more \"sanity-check\" experiments are needed. Especially, because this work is empirical in nature.** I'm voting for weak accept at the moment, but I'll make the final decision based on how the authors address my questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5847/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5847/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5847/Reviewer_AFpQ"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5847/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699562561739,
            "cdate": 1699562561739,
            "tmdate": 1700710515359,
            "mdate": 1700710515359,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "k1UcckSUiA",
                "forum": "kxebDHZ7b7",
                "replyto": "MBkRWxvRGw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5847/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5847/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Part 1"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their time and attention in considering TRAM. We provide the first part of our response to address your questions. We are still working on experiments for the vision domain (Q1) and will update our response when these are finalized. \n\n**Q2. \"is ASAM type of updates really necessary?\"**\n\nWe thank the reviewer for correctly pointing out that TRAM can build on either ASAM or SAM. We pursue building on ASAM due to empirical benefits of ASAM vs SAM in our early experiments. We experimented with both techniques and found that just as ASAM generally outperforms SAM, TRAM building on ASAM outperforms TRAM built on SAM. We show XNLI accuracy for TRAM without and with adaptive scaling. This corresponds to TRAM using SAM or ASAM respectively. TRAM using SAM scaling still performs above most prior methods (SAM, ASAM, FSAM, TRPO, MESA) but below TRAM using ASAM.\n\n| Model                   | Without Adaptive Scaling (SAM) | With Adaptive Scaling (ASAM) |\n|-------------------------|--------------------------|-----------------------|\n| TRAM-$d_{x}$            | 74.7                     | 75.2                  |\n| TRAM-$d_{\\theta_{t-1}}$ | 74.1                     | 75.0                  |\n| TRAM-$d_{\\theta_{0}}$   | 74.1                     | 74.9                  |\n\n **Q3: \"whether SAM-type of updates are required in the beginning of the training or at the end of the training.\"**\n\nWe thank the reviewer for highlighting the ongoing debate of when to apply SAM-style optimization. \nGiven that \u201cpretrain then fine-tune\u201d is a standard paradigm in NLP, and more commonly in CV, we focus our efforts on applying TRAM here to learn a task-specific model. Our understanding of pre-training dynamics (in NLP) is that an algorithm requiring multiple forward and backward passes would be too expensive for large scale pre-training from scratch. Furthermore, the benefits of these steps may be weaker than simple additional pre-training on more data (i.e., the bitter lesson). We consider the value of TRAM greatest when adapting a task-agnostic model (or mostly task agnostic) to a task-specific model.  A principle of TRAM is to maintain generality from pre-training, therefore requiring some task-agnostic initial training to obtain a \u201ctrustable\u201d original model. \n\n**Q4: Fig 1.**\n\nThe slope of these curves represents the correlation between the training domain (ArXiv Math) and different groups of zero-shot evaluation domains. These curves represent the trend of domain correlation across all optimization algorithms (SAM-style, trust region and TRAM). For a positive correlation, a point above the curve indicates performing better than the trend. For a negative correlation, a point below the curve indicates a better performance than the trend. We do this to highlight that TRAM beats expectations of how negatively-correlated domain transfer impacts performance for hard OOD cases.\n\n**Q5: Table 5**\n\nTable 5 includes both \u201cmain\u201d variants of TRAM which directly compute a TR distance. We omitted the other variants due to last minute space issues. We provide the numbers below and will include these updates in revisions. In general, TRAM-$d_{\\theta_{0}}$ and TRAM-Fisher align with our trends comparing TRAM and other algorithms. All variants of TRAM have among the lowest sharpness and the highest CKA similarity across all variants. \n\n| XLM-R $\\epsilon$-sharpness $\\downarrow$ | EN   | ZS Avg.        |\n|-----------------------------------------|------|----------------|\n| Adam                                    | 2.16 | 1.98$\\pm$~0.79 |\n| SAM                                     | 1.43 | 3.32$\\pm$~0.96 |\n| ASAM                                    | 2.57 | 2.22$\\pm$~0.79 |\n| FSAM                                    | 2.34 | 2.62$\\pm$~0.29 |\n| TRPO                                    | 6.17 | 2.36$\\pm$~1.02 |\n| R3F                                     | 6.22 | 2.56$\\pm$~1.21 |\n| MESA                                    | 2.76 | 5.48$\\pm$~0.75 |\n| TRAM-$d_{x}$                            | 0.61 | 1.49$\\pm$~0.49 |\n| TRAM-$d_{\\theta_{t-1}}$                 | 0.50 | 1.19$\\pm$~0.38 |\n| TRAM-$d_{\\theta_{0}}$                   | 0.75 | 1.92$\\pm$~0.24 |\n| TRAM-Fisher                             | 1.67 | 2.02$\\pm$~0.49 |\n\n| XLM-R CKA $\\uparrow$    | EN   | ZS Avg.        |\n|-------------------------|------|----------------|\n| Adam                    | 0.69 | 0.44$\\pm$~0.1  |\n| SAM                     | 0.69 | 0.42$\\pm$~0.1  |\n| ASAM                    | 0.69 | 0.42$\\pm$~0.1  |\n| FSAM                    | 0.73 | 0.48$\\pm$~0.1  |\n| TRPO                    | 0.7  | 0.45$\\pm$~0.1  |\n| R3F                     | 0.66 | 0.4$\\pm$~0.1   |\n| MESA                    | 0.67 | 0.42$\\pm$~0.1  |\n| TRAM-$d_{x}$            | 0.75 | 0.54$\\pm$~0.11 |\n| TRAM-$d_{\\theta_{t-1}}$ | 0.77 | 0.57$\\pm$~0.1  |\n| TRAM-$d_{\\theta_{0}}$   | 0.69 | 0.45$\\pm$~0.1  |\n| TRAM-Fisher             | 0.72 | 0.49$\\pm$~0.1  |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5847/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576263048,
                "cdate": 1700576263048,
                "tmdate": 1700576263048,
                "mdate": 1700576263048,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UnV0K6rVf5",
                "forum": "kxebDHZ7b7",
                "replyto": "MBkRWxvRGw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5847/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5847/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response Part 2"
                    },
                    "comment": {
                        "value": "**Q1. Sanity check experiments**\n\nFor comparison, we implement the same experiments as [1] for FisherSAM training ViT-base-16 [2] for CIFAR-100 [3], Flowers [4] and Cars [5]. We fine-tune ViT-base-16 for 200 epochs training with TRAM and report the Top-1 accuracy(+ 95% confidence interval) averaged over 5 runs  to compare directly to Table 3 in [1]. We match the hyperparameter setup of [1] \u2014 the base optimizer is SGD with an initial learning rate of 5e-4 and a cosine decay LR schedule. Due to computational constraints, we only have the bandwidth to report the two main variants of TRAM on these tasks during the rebuttal. We plan to update with the other variants of TRAM (TRAM-$d_{\\theta_0}$ and TRAM-Fisher) in future revisions. \n\n|           | SGD        | SAM        | ASAM       | FSAM       | TRAM-$d_{\\theta_{t-1}}$ | TRAM-$d_{x}$ |\n|-----------|------------|------------|------------|------------|-------------------------|--------------|\n| CIFAR-100 | 87.97\u00b10.12 | 87.99\u00b10.09 | 87.97\u00b10.08 | 88.39\u00b10.13 | 88.47\u00b10.16              | **88.78**\u00b10.01   |\n| Cars      | 92.85\u00b10.31 | 93.29\u00b10.01 | 93.28\u00b10.02 | 93.42\u00b10.01 | **93.49**\u00b10.04              | 93.32\u00b10.11   |\n| Flowers   | 94.53\u00b10.20 | 95.05\u00b10.06 | 95.08\u00b10.10 | 95.26\u00b10.03 | **97.07**\u00b10.10              | 96.34\u00b10.03   |\n\n\nWe observe that TRAM performs competitively across all datasets, with one or both variants of TRAM performing above all other methods. The largest improvement for TRAM is Flowers where we perform +1.81% above FSAM. The smallest improvement is for Cars where we perform 0.07% above FSAM, but we not that the confidence intervals for these results do not overlap. \n\n - [1] Fisher SAM: Information Geometry and Sharpness Aware Minimisation https://proceedings.mlr.press/v162/kim22f/kim22f.pdf\n - [2] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale https://arxiv.org/abs/2010.11929 \n - [3] Learning Multiple Layers of Features from Tiny Images https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf \n - [4] C3D Object Representations for Fine-Grained Categorization http://vision.stanford.edu/pdf/3drr13.pdf \n - [5] Automated Flower Classification over a Large Number of Classes https://ieeexplore.ieee.org/document/4756141 \n---\nWe hope these experiments provide additional utility to the benefits of TRAM. We ask the reviewer to consider revising their score if these additional responses have sufficiently addressed your questions."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5847/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688911182,
                "cdate": 1700688911182,
                "tmdate": 1700688937570,
                "mdate": 1700688937570,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tyQeCMYXD1",
                "forum": "kxebDHZ7b7",
                "replyto": "UnV0K6rVf5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5847/Reviewer_AFpQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5847/Reviewer_AFpQ"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "I read through the responses and they sufficiently address my concerns.\nHence, I raise my score to 7 (it seems that there is no 7, so I marked my score to 8).\nPlease update the manuscript accordingly, I strongly believe that making those points will make the paper even stronger.\nThank you for taking time responding to my questions."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5847/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710666252,
                "cdate": 1700710666252,
                "tmdate": 1700710666252,
                "mdate": 1700710666252,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]