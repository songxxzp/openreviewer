[
    {
        "title": "ADELT: Transpilation Between Deep Learning Frameworks"
    },
    {
        "review": {
            "id": "RikSDTInEm",
            "forum": "FH7lfTfjcm",
            "replyto": "FH7lfTfjcm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7810/Reviewer_hxrU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7810/Reviewer_hxrU"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduce a novel source-to-source transpilation framework between deep learning frameworks, named ADELT.\nThe core innovation is to decouple the process into code skeleton transpilation and API keyword mapping, so more structural information and domain information could be injected through prompting and adversarial training. \nThe proposed method outperforms the SOTA method by a good margin on PyTorch-Keras and PyTorch-MXNet transpilation pairs."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "I think the proposed ADELT is solid and innovative.  \nI like the idea of decoupling an end-to-end generation process into separate parts, which allows more context/structural information to be incorporated into the process. But it's often hard to achieve because of the complexity of different tasks. For the transpilation task, I think the author find a good combination of the code skeleton and the API keywords mapping. And the experimental results show a significant improvement over the SOTA system, which is quite convincing.\nOverall I think it's a solid paper, well-written and easy to understand."
                },
                "weaknesses": {
                    "value": "One suggestion is that in the code skeleton process, all the API keywords are replaced by placeholders (namely PLACEHOLDER_1,2 ...).\nI wonder if it will be beneficial to add more context information into the placeholders, given them more concrete meanings. \nFor example in machine translation, the name entity is often replaced as <NUM>, <TIME>, <NAME> etc, which will give more context information in the downstream translation process.   So maybe the same idea could be applied in transpilation."
                },
                "questions": {
                    "value": "As the transpilation is decoupled into two sub-parts:  the code skeleton and the API keywords mapping.\nIs it possible to measure the precision of each process and how do they effect the final performance ?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7810/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7810/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7810/Reviewer_hxrU"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7810/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698753706449,
            "cdate": 1698753706449,
            "tmdate": 1700790250651,
            "mdate": 1700790250651,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aketvHzQHC",
                "forum": "FH7lfTfjcm",
                "replyto": "RikSDTInEm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7810/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7810/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hxrU"
                    },
                    "comment": {
                        "value": "Thank you for your encouraging comments! We address your concerns below:\n\n**W1. Why not add more context information into the placeholders**\n\nWe appreciate your insightful recommendations. Rather than using generic placeholders like `[PLACEHOLDER_1]`, `[PLACEHOLDER_2]`, we agree that using more specific tags such as `[FUNC_1]`, `[ARG_1]` could provide valuable context. However, given that our current approach has achieved 100% accuracy for code skeleton transpilation in our benchmark evaluations, this modification might be not necessary for this given task. That said, your suggestion will be useful for more challenging tasks, and will be considered in future work. Thank you again for your valuable input.\n\n**Q1. Is it possible to measure the precision of each process (the code skeleton and the API keywords mapping) and how do they effect the final performance?**\n\nWe appreciate your insightful question regarding the precision of our transpilation process's sub-parts and their subsequent effect on ADELT's overall performance.\n\nFor skeletal code transpilation, Section 3.4 describes its accuracy of 100% on the benchmark, because skeletal code transpilation is designed to be simple and straightforward.\n\nFor dictionary lookup, the performance is reported in Table 3 (ablation studies), where we show metrics such as Precision@1, Precision@5, and MRR. The dictionary of ADELT has Precision@1 of 87.1% and 90.0% for both transpilation directions respectively."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7810/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732413115,
                "cdate": 1700732413115,
                "tmdate": 1700732413115,
                "mdate": 1700732413115,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7qL10pkdhO",
            "forum": "FH7lfTfjcm",
            "replyto": "FH7lfTfjcm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7810/Reviewer_DL7Y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7810/Reviewer_DL7Y"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a method translate short code snippets from one deep learning framework to another. The authors create a mapping of keywords from one framework to another, using a new BERT-like model pre-trained on Python code. This model is augmented with a generator and discriminator, and finetuned using unsupervised data to maximize cosine similarity, similar to dense retrieval methods. The authors then pipeline prompting LLMs to translate the code, and then use their mapping to replace the keywords from the source framework to the target. The proposed method outperforms prompting LLMs, unsupervised NMT approaches, and simple heuristic approaches on a small evaluation set of 50 samples of around 10 lines each."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. A BERT model trained on Python code, which could have interesting applications - for example for code refactoring, static analysis, etc. \n1. Using retrieval cosine similarity to map API keywords from one framework to another, which is learnt using fully unsupervised and unaligned data."
                },
                "weaknesses": {
                    "value": "1. Very small evaluation dataset size of 50 samples. This makes any evaluation or comparisons made on this data less robust, with 95% deviations of upto +- 8.8 points.\n1. The evaluation dataset construction process itself introduces biases - \n    1. By selecting only pairs with high BLEU score, this makes the dataset somewhat simpler than what a more uniform sampling might have yielded (say with a manually created dataset, with humans transpiling the code). This \"simpler\" dataset then in turn hides how the proposed method may compare to others (such as prompting LLMs) in more complicated instances.\n    1. The BLEU score filtering also biases the dataset towards shorter snippets with $\\approx10$ lines of code - This restricts the scope of this method, to real world scenarios with very small code snippets to translate, and again makes realistic comparisons difficult.\n1. The pre-training dataset used may itself have been on interest, but perhaps will not be feasible to release due to copyright concerns."
                },
                "questions": {
                    "value": "1. For table 9, could the authors provide GPT4's 95% intervals, specifically for F1? (corresponding to scores in Table 1)\n\nPresentation suggestion - \n1. 2 decimal digits (in Table 1, etc) make it hard to read the table to quickly compare values. 1 decimal digit will be easier/faster to understand. However, given the 95% confidence intervals are so large, any decimal digit is meaningless - Perhaps no decimal digits should be shown.\n1. Datasets in A.1 are reported in GBs, but often a significant fraction of ipynb notebook files is binary data in output cells - a better metric to report here would be either tokens, or dataset size without the binary parts of ipynb."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7810/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698763071905,
            "cdate": 1698763071905,
            "tmdate": 1699636955884,
            "mdate": 1699636955884,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6akGwuUX4Z",
                "forum": "FH7lfTfjcm",
                "replyto": "7qL10pkdhO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7810/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7810/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DL7Y (1/2)"
                    },
                    "comment": {
                        "value": "**W1. Very small evaluation dataset size of 50 samples. 95% confidence interval of upto +- 8.8 points.**\n\nThank you for your insightful feedback regarding the evaluation dataset's size. Recognizing the importance of a robust and reliable assessment of ADELT's performance, we have expanded the evaluation dataset to 100 examples. The additional 50 examples were curated by identifying PyTorch modules sampled from GitHub repositories with over 1,000 stars. Human experts subsequently translated these into the target frameworks, and you can find the selection process detailed in Appendix A.4 of the revised manuscript.\n\nUpon re-evaluating the PyTorch-Keras transpilation with the augmented benchmark, ADELT maintains a substantial performance lead, achieving an improved exact match score increase of 17.4 percentage points over the baseline models. Updated results have been incorporated into the relevant sections of our manuscript. We believe that this improvement in our evaluation substantiates the reliability of our results.\n\nThe size of the revised dataset is comparable to the standard in domain-specific benchmarks: the general-purpose code generation dataset OpenAI HumanEval contains 164 examples, and most program synthesis benchmarks comprise less than 100 examples, like Casper (37 examples) [1], C2TACO (71 examples) [2], TF-Coder (70 examples) [3]. While we were able to only double the size of our evaluation dataset due to the limited time over the rebuttal period, we will expand the dataset further post-rebuttal.\n\nRegarding statistical significance, 95% deviation of 8.88 points was observed from F1 score of GPT-3 (35.62 \u00b1 8.88), which is statistically significantly inferior to ADELT originally reported (82.01 \u00b1 3.08).\n\nWe have introduced a full results table with error bars for GPT-4 and other models on our new 100-example benchmark (also shown in Table 10 of the revised manuscript):\n\n|   |    **Keyword**   | **(Precision@1)** | **Source-to-Source** | **(F1 Score)**   |\n|-|:-|:-|:-|:-|\n| | PyTorch-to-Keras | Keras-to-PyTorch  |   PyTorch-to-Keras   | Keras-to-PyTorch |\n| GPT-3 |     35.4\u00b16.1     | 39.1\u00b14.2          | 26.6\u00b15.1             | 32.1\u00b16.7         |\n| Codex  |     67.5\u00b18.3     | 79.1\u00b17.8          | 59.9\u00b12.7             | 67.1\u00b12.2         |\n| GPT-4 |     74.5\u00b15.8     | 83.2\u00b13.5          | 67.7\u00b12.6             | 74.9\u00b11.7         |\n| ADELT (Small) |     82.9\u00b11.2     | **90.0\u00b11.6**      | 79.0\u00b12.2             | 76.7\u00b11.5         |\n| ADELT (Base) |   **87.1\u00b11.2**   | **90.0\u00b12.5**      | **83.4\u00b10.8**         | **82.0\u00b12.2**     |\n\nThese results illustrate that ADELT (Base) surpasses GPT-4 in all benchmarks with a statistically significant difference. We hope that this larger dataset and comprehensive re-evaluation address your concerns and bring further rigor to our work.\n\n**W2. The evaluation dataset construction process itself introduces biases. By selecting only pairs with high BLEU score**\n\nThank you for your astute observations concerning potential biases caused by our initial data collection method. You are correct that our initial reliance on BLEU score heuristics may have inadvertently skewed our dataset towards simpler examples.\n\nHowever, it's important to clarify that the BLEU score was used as an initial guide to locate repositories potentially containing parallel examples of code across deep learning frameworks, rather than as a strict selection criterion.\n\nIn response to your suggestion, we have doubled our evaluation dataset with more challenging, realistic examples sourced from PyTorch repositories, which were then manually transpiled by human experts, as we discussed in our response to weakness 1.\n\n**W3. The pre-training dataset perhaps will not be feasible to release due to copyright concerns.**\n\nThank you for your comment regarding the dataset used in our study. We acknowledge your concern about potential copyright issues related to parts of the data such as StackOverflow. To address this, while we have added these data in the supplementary material, we will release the script used to gather and process such data upon publication of the paper.\n\n[1] Leveraging Parallel Data Processing Frameworks with Verified Lifting, https://arxiv.org/abs/1611.07623\n\n[2] C2TACO: Lifting Tensor Code to TACO, https://dl.acm.org/doi/abs/10.1145/3624007.3624053\n\n[3] TF-Coder: Program Synthesis for Tensor Manipulations, https://arxiv.org/abs/2003.09040"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7810/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732172435,
                "cdate": 1700732172435,
                "tmdate": 1700732172435,
                "mdate": 1700732172435,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JtoLdDRDjQ",
                "forum": "FH7lfTfjcm",
                "replyto": "7qL10pkdhO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7810/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7810/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DL7Y (2/2)"
                    },
                    "comment": {
                        "value": "**Q1. For table 9, could the authors provide GPT4's 95% intervals, specifically for F1?**\n\nWe apologize for the oversight in our original submission. The results with 95% confidence intervals for GPT-4, have now been added to Table 10 of the revised manuscript. We also discussed those results in our response to weakness 1. Thank you for bringing this to our attention.\n\n**PS1. 2 decimal digits (in Table 1, etc) make it hard to read the table to quickly compare values. 1 decimal digit will be easier/faster to understand.**\n\nThank you for your valuable suggestion. We appreciate your attention to clarity and have updated our tables accordingly, presenting all numbers rounded to 1 decimal digit.\n\n**PS2. Datasets in A.1 are reported in GBs, but often a significant fraction of ipynb notebook files is binary data in output cells**\n\nWe appreciate your insight. In response, we have revised Appendix A.1 to reflect the size of clean Python code in all datasets, measured in gigabytes, thereby providing a more accurate sizing metric."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7810/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732259006,
                "cdate": 1700732259006,
                "tmdate": 1700732259006,
                "mdate": 1700732259006,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NKKm31VXgA",
            "forum": "FH7lfTfjcm",
            "replyto": "FH7lfTfjcm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7810/Reviewer_TwWZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7810/Reviewer_TwWZ"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel approach called the Adversarial DEep Learning Transpiler (ADELT) for source-to-source transpilation between deep learning frameworks.\n\nThis work has two contributions, as follows:\n\n1. ADELT is trained on an unlabeled web-crawled deep learning corpus, avoiding the need for hand-crafted rules or parallel data. It outperforms several related methods and significantly surpass the state-of-the-art large language model Codex, by 19.33 and 12.50 points, respectively.\n\n2. The authors construct a PyTorch-Keras-MXNet corpus of deep learning code from various Internet sources, containing 19,796 PyTorch modules, 3,703 Keras layers/models, and 1,783 MXNet layers/models. An evaluation benchmark is then built to assess both the API keyword mapping algorithm and the overall source-to-source transpilation.\n\nIn summary, ADELT represents a significant advancement in the field of source-to-source transpilation for deep learning frameworks, offering improved performance without the need for labeled data and providing a valuable resource for the deep learning community by sharing code, corpus, and evaluation benchmarks openly."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Originality:\n\na. Decoupling of Transpilation Components: ADELT's approach to decoupling code skeleton transpilation and API keyword mapping is a unique and innovative contribution. This separation allows for more flexibility and adaptability in the transpilation process, and it sets ADELT apart from existing end-to-end methods.\n\nb. Few-shot Prompting and Domain-Adversarial Training: The use of few-shot prompting on large language models for code transpilation and domain-adversarial training of contextual embeddings for API keyword mapping is a novel combination. It leverages the strengths of these techniques to enhance transpilation accuracy.\n\n2. Quality:\n\na. Performance Improvement: ADELT demonstrates substantial improvements in transpilation accuracy compared to state-of-the-art transpilers. The increase in exact match scores for PyTorch-Keras and PyTorch-MXNet transpilation pairs is impressive, indicating the high quality of the proposed approach.\n\nb. Data Utilization: ADELT's ability to achieve these results without relying on labeled data is a testament to its quality. Training on an unlabeled web-crawled deep learning corpus showcases the effectiveness of the approach in real-world scenarios.\n\n\nBesides, the clear problem statement makes the problem of source-to-source transpilation accessible to a broad audience. The methodology, including the use of few-shot prompting, domain-adversarial training, and the construction of a corpus, is well documented, enhancing the clarity of the proposed approach. ADELT represents an important step forward in addressing the challenges of working with different deep learning frameworks."
                },
                "weaknesses": {
                    "value": "1. Lack of Negative Results: The paper primarily focuses on the strengths and successes of ADELT. Including discussions on potential limitations, challenges, or cases where ADELT might not perform as well would make the paper more balanced and provide a more realistic perspective.\n\n2. Generalization of the Approach: The paper primarily focuses on specific transpilation pairs like PyTorch-Keras and PyTorch-MXNet. To enhance the paper's significance, discussing the potential for ADELT's application to a broader range of deep learning frameworks and scenarios would be valuable."
                },
                "questions": {
                    "value": "1. How does ADELT handle situations where different versions of deep learning frameworks require different expressions? Can you give more explanations? Such as, \nfor the old version of Python:\nx = torch.autograd.Variable(torch.Tensor([1.0]), requires_grad=True)\nfor a newer version:\nx = torch.tensor([1.0], requires_grad=True)\n\n2. In the METHOD section, the authors mention that they \"convert each API call into its canonical form\". How exactly is this achieved? Can you provide a detailed explanation, or are there existing open-source tools for this purpose?\n\n3. I don't quite understand what ${e_i^{(1)}}\\_{i=1}^{m^{(1)}}$ and ${e_j^{(2)}}_{j=1}^{m^{(2)}}$ mean in the last paragraph of Section 2.3.\n\n4. Instead of an end-to-end model, ADELT is actually a pipeline solution, consisting of multiple components, such as \"Canonicalizion\" \"Extract API calls\" \"Code to Skeleton\" and \"Dictionary Lookup\" and so on, right? Have the authors tested the accuracy of each component? They will ultimately impact the overall quality of the code skeleton transpilation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7810/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811557356,
            "cdate": 1698811557356,
            "tmdate": 1699636955766,
            "mdate": 1699636955766,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "n10n51DigY",
                "forum": "FH7lfTfjcm",
                "replyto": "NKKm31VXgA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7810/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7810/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer TwWZ"
                    },
                    "comment": {
                        "value": "**W1. Lack of Negative Results**\n\nAlthough our paper centered on ADELT's strengths, we have indeed discussed its limitations in detail. \n\nIn Table 2, we provided a detailed case study illustrating instances where ADELT demonstrated less optimal performance. Notably, a case where the parameter `output_dim` was inaccurately mapped to `embeddings_initializer` due to an error in generating vocabulary mapping. \n\nAlso, ADELT is unable to transpile `layers.Dense(out_dim, activation='relu')` into corresponding PyTorch API calls, `nn.Linear` and `nn.ReLU`. This is because the training corpus does not include enough examples that reflect such use cases in Keras. In Appendix A.9, we describe the potential for ADELT's improved modeling of such mappings in a scenario where we add synthetic data into the corpus.\n\nWe value your perspective and will expand our discussion of both the pros and cons in our paper.\n\n**W2. Generalization of the Approach**\n\nWe appreciate your insightful suggestion on generalizing the application of ADELT. While our current study focuses on transpilation between specific deep learning frameworks, ADELT's approach can be extended to various API ecosystems within the same programming language. We believe that ADELT\u2019s approach to transpilation can also be applied to transpile between APIs from different domains, such as visualization (Matplotlib vs. Seaborn) or database access APIs (PyMongo vs SQLite). Your feedback has certainly contributed to outlining the future directions of our work.\n\n**Q1. \u200b\u200bHow does ADELT handle situations where different versions of deep learning frameworks require different expressions?**\n\nThank you for your insightful question. ADELT addresses variability across framework versions by treating each version-specific expression as a unique entry. Both `torch.autograd.Variable` and `torch.tensor` in PyTorch are mapped to `tf.Tensor` in Keras. The reverse process, subsequently, interprets `tf.Tensor` from Keras as the more recent `torch.tensor` expression in PyTorch. This approach maintains consistency across different framework versions and ensures accurate transpilation.\n\n**Q2. In the METHOD section, the authors mention that they \"convert each API call into its canonical form\". How exactly is this achieved?**\n\nWe convert each API call into its canonical form using Python's built-in `inspect` module, specifically `inspect.bind` (https://docs.python.org/3/library/inspect.html). This function interprets Python function calls internally and creates a mapping from positional and keyword arguments to parameters. We have further detailed this procedure in Section 2.1 of our paper.\n\n**Q3. What e_i and e_j mean in the last paragraph of Section 2.3.**\n\nThank you for your question. In Section 2.3, $e_i$ and $e_j$ correspond to the unique embeddings of each API keyword. To elaborate, every API keyword, like `nn.Conv2d` in PyTorch, is assigned its unique embedding, like `nn.Linear` or `nn.Linear.in_features`. In Equation (2), we incorporate these embedding matrices as weight matrices for the output classification layer, a strategy that is commonly adopted in Transformer LMs where a shared parameter tensor is used between the word embedding and the output layer [1].\n\n**Q4. Have the authors tested the accuracy of each component, such as \"Canonicalizion\" \"Extract API calls\" \"Code to Skeleton\" and \"Dictionary Lookup\" and so on?**\n\nYes, ADELT indeed follows a pipeline structure that includes \u201cCanonicalization\u201d, \u201cAPI call extraction\u201d, \u201cCode to Skeleton\u201d, \u201cSkeletal Code Transpilation\u201d and \u201cDictionary Lookup\u201d among other steps.\n\nFor \u201cCanonicalization\u201d, \u201cAPI call extraction\u201d, \u201cCode to Skeleton\u201d, our approach uses Python's internal library and does not involve machine learning, thereby not affecting transpilation accuracy.\n\nFor \u201cSkeletal Code Transpilation\u201d, Section 3.4 describes its accuracy of 100% on the benchmarks. In fact, one of the key novelties of ADELT is the decoupling of the transpilation process into transpiling the skeletal code followed by dictionary lookup. Skeletal code transpilation is designed to be simple using any off-the-shelf LLM, while the dictionary used in the second step requires more sophisticated techniques to learn via adversarial learning. \n\nFor \u201cDictionary Lookup\u201d, the performance is reported in Table 3  (ablation studies), where we show metrics such as Precision@1, Precision@5, and MRR. The dictionary of ADELT has Precision@1 of 87.1% and 90.0% for both transpilation directions respectively.\n\nWe hope that this clarifies the efficacy of ADELT's individual components.\n\n[1] Rethinking embedding coupling in pre-trained language models, https://arxiv.org/abs/2010.12821"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7810/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731835396,
                "cdate": 1700731835396,
                "tmdate": 1700731835396,
                "mdate": 1700731835396,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "r7AelBEPc2",
            "forum": "FH7lfTfjcm",
            "replyto": "FH7lfTfjcm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7810/Reviewer_MAQg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7810/Reviewer_MAQg"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces Adversarial DEep Learning Transpiler (ADELT) for source-to-source transpilation between deep learning frameworks. It accomplishes this by transpiling the code's skeleton using a pretrained language model and mapping keywords through a keyword translation dictionary generated by domain-adversarial training process. The author also built an evaluation benchmark for PyTorch-Keras and PyTorch-MXNet transpilation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Strengths:\n\n1. The method proposed in this article seems to be effective. ADELT uniquely decouples the code skeleton transpilation and API keyword mapping, which allows for greater flexibility and adaptability in the transpilation process.\n\n2. The authors establish a small evaluation dataset. The paper conducts comprehensive ablation studies."
                },
                "weaknesses": {
                    "value": "1. The description in the Method section is somewhat challenging to understand. For instance, the example in Figure 1 is too simple, and it's not clear how the complete actions of step 2 and step 3 are reflected. Additionally, based on the description, it seems like steps 3, 4, and 5 are executed sequentially. However, in Figure 1, it appears that step 4 is executed in parallel with the steps 3,5, which is somewhat perplexing.\n\n2. The size of the evaluation dataset is somewhat small, with only 50 samples, making it challenging to ensure the reliability of the experiments and comparisons. \n\n3. Some details of the comparisons were not clearly explained, such as what prompts were used for GPT-4, as the choice of prompts can significantly impact the model's performance.\n\n4. If I did not overlook, the article only provides an account of the construction process for the PyTorch-Keras benchmark in Appendix A.4, but, it does not cover the construction process for the PyTorch-MXNet benchmark.\n\n5. The execution efficiency also needs some comparisons, such as runtime, as the speed of a pipeline is typically expected to be slower than a single end-to-end model. However, the paper does not quantify the magnitude of this difference."
                },
                "questions": {
                    "value": "In section 2.4, what do you mean by \u201cIn dictionary generation, we do not allow callable names to be translated to callable names\u201d? Can you provide a detailed explanation or an example to explain it?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7810/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822104287,
            "cdate": 1698822104287,
            "tmdate": 1699636955646,
            "mdate": 1699636955646,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jU5ttfwnGu",
                "forum": "FH7lfTfjcm",
                "replyto": "r7AelBEPc2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7810/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7810/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MAQg (1/2)"
                    },
                    "comment": {
                        "value": "**W1. Figure 1 is too simple. It appears that step 4 is executed in parallel with the steps 3, 5**\n\nThank you for your insightful comments regarding the Method section of our paper. We apologize for any confusion caused by our initial presentation in Figure 1. We have revised the figure in the updated manuscript to better depict our approach. The updated Figure 1 now provides a more detailed illustration, clarifying how keyword substitutions from step 4 integrate with the other steps.\n\nYes, steps 3 and 4 are executed concurrently, each building upon the output of step 2, while step 5 synthesizes the results from both steps 3 and 4.\n\n**W2. The size of the evaluation dataset is somewhat small, with only 50 samples**\n\nThank you for your insightful feedback regarding the evaluation dataset's size. Recognizing the importance of a robust and reliable assessment of ADELT's performance, we have expanded the evaluation dataset to 100 examples. The additional 50 examples were curated by identifying PyTorch modules sampled from GitHub repositories with over 1,000 stars. Human experts subsequently translated these into the target frameworks, and you can find the selection process detailed in Appendix A.4 of the revised manuscript.\n\nUpon re-evaluating the PyTorch-Keras transpilation with the augmented benchmark, ADELT maintains a substantial performance lead, achieving an improved exact match score increase of 17.4 points over the baseline models. Updated results have been incorporated into the relevant sections of our manuscript. We believe that this improvement in our evaluation substantiates the reliability of our results.\n\nThe size of the revised dataset is comparable to the standard in domain-specific transpilation benchmarks: the general-purpose code generation dataset OpenAI HumanEval contains 164 examples, and most program synthesis benchmarks comprise of less than 100 examples, like Casper (37 examples) [1], C2TACO (71 examples) [2], TF-Coder (70 examples) [3]. While we were able to only double the size of our evaluation dataset due to the limited time over the rebuttal period, we will expand the dataset further post-rebuttal.\n\n**W3. Some details of the comparisons were not clearly explained, such as what prompts were used for GPT-4**\n\nThank you for your insightful comments. We apologize for missing the GPT-4 prompt details. To address this, we have included a comprehensive explanation of the prompts used for GPT-4 in Appendix A.6 and detailed the post-processing protocol in Table 8 of the revised manuscript. As we explain in the appendix, our experiments leverage Markdown-formatted prompts in GPT-4's chat mode, and then GPT-4 sometimes generates a mixture of natural language and code. After generation, we programmatically extract code from the first code block from GPT-4's responses as the finalized transpilation. Our prompts for GPT-4 are designed to mimic the straightforward and intuitive approach of a human user, as done in prior approaches in evaluating such models [4].\n\n\n**W4. The paper only provides the construction process for the PyTorch-Keras benchmark in Appendix A.4, but not cover the construction process for the PyTorch-MXNet benchmark**\n\nThank you for pointing out this oversight. We have now detailed the construction method for the PyTorch-MXNet benchmark in the revised Appendix A.4. Briefly, human experts manually transpiled the PyTorch snippets from our PyTorch-Keras dataset into MXNet, ensuring consistency across frameworks. Your feedback is greatly appreciated for the improvement of our manuscript.\n\n**W5. The execution efficiency also needs some comparisons, such as runtime**\n\nWe appreciate your concern on ADELT's computational efficiency. Efficiency is actually one of the strengths of ADELT.\n\nADELT is composed of two main procedures: large language model (LLM) prompting for code transpilation and dictionary lookup for API keyword mapping. Using Codex for transpiling the code skeleton is as efficient as using Codex to do transpilation end-to-end. The API dictionary lookup process is highly efficient and only takes nanoseconds since it does not involve neural network inference for each example. \n\nTo illustrate, ADELT outperforms GPT-4 in transpilation results, and its skeletal code transpilation component uses Codex, which is of a similar scale as GPT-3. Because GPT-3/Codex is significantly faster, ADELT is faster than GPT-4.\n\n[1] Leveraging Parallel Data Processing Frameworks with Verified Lifting, https://arxiv.org/abs/1611.07623\n\n[2] C2TACO: Lifting Tensor Code to TACO, https://dl.acm.org/doi/abs/10.1145/3624007.3624053\n\n[3] TF-Coder: Program Synthesis for Tensor Manipulations, https://arxiv.org/abs/2003.09040\n\n[4] GPT-4 Technical Report, https://arxiv.org/abs/2303.08774"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7810/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731537948,
                "cdate": 1700731537948,
                "tmdate": 1700731537948,
                "mdate": 1700731537948,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ujDAJczXE1",
                "forum": "FH7lfTfjcm",
                "replyto": "r7AelBEPc2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7810/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7810/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MAQg (2/2)"
                    },
                    "comment": {
                        "value": "**Q1. In section 2.4, what do you mean by \u201cIn dictionary generation, we do not allow callable names to be translated to callable names\u201d?**\n\nWe appreciate your keen observation. The mentioned statement was indeed a mistake. The sentence should be, \"In dictionary generation, we do not allow callable names to be translated to **parameter** names\". This mistake has been rectified in the revised manuscript. Thanks for pointing it out!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7810/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731595172,
                "cdate": 1700731595172,
                "tmdate": 1700731595172,
                "mdate": 1700731595172,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]