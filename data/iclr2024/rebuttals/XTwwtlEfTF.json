[
    {
        "title": "Robust Multimodal Learning with Missing Modalities via Parameter-Efficient Adaptation"
    },
    {
        "review": {
            "id": "qvVUzavbSS",
            "forum": "XTwwtlEfTF",
            "replyto": "XTwwtlEfTF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4704/Reviewer_Gaao"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4704/Reviewer_Gaao"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces parameter-efficient adaptation to make multi-modal segmentation more robust in the face of missing modalities, and it compares various methods, demonstrating promising results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1, The method in this paper utilizes parameter-efficient fine-tuning to make the model adapt to modality loss, which I find relatively straightforward. The performance is also quite good.\n\n2, The paper is well-written and relatively easy to read."
                },
                "weaknesses": {
                    "value": "1, The method proposed in this paper may not be directly applicable to scenarios where the training set has missing modalities.\n\n2, The method proposed in this paper lacks extensive comparisons with other methods~(for example, [1,2,3]) in the context of multimodal classification tasks, which significantly undermines the persuasiveness of Table 5. \n\n\n[1] Are Multimodal Transformers Robust to Missing Modality?\n\n[2] Multi-modal Learning with Missing Modality via Shared-Specific Feature Modelling\n\n[3] What makes for robust multi-modal models in the face of missing modalities?"
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4704/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4704/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4704/Reviewer_Gaao"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4704/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698673374752,
            "cdate": 1698673374752,
            "tmdate": 1699636452121,
            "mdate": 1699636452121,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pUUe5MJCVX",
                "forum": "XTwwtlEfTF",
                "replyto": "qvVUzavbSS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4704/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4704/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Gaao"
                    },
                    "comment": {
                        "value": "Thank you very much for your review. We thank you for acknowledging the strengths, simplicity and performance of our method. We respond to your other comments below.\u00a0\n\n1. **The method proposed in this paper may not be directly applicable to scenarios where the training set has missing modalities.** \\\nThis is a good point. Indeed if a modality is completely missing from the training set, then our method (and almost all existing methods) cannot compensate for that. However, if the training set has some samples missing for some modalities, we can pretrain a network with the available modalities and then perform parameter-efficient network adaptation to compensate for the missing modalities at the test time. Since our approach is mainly focused on adapting a pretrained model to different missing modality situations, we can use the same adaptation procedure using a different base network. The performance in this case will probably depend on the quality of the base model pretrained with missing samples from different modalities.    \n######\n2. **The method proposed in this paper lacks extensive comparisons with other methods\\~(for example, \\[1,2,3]) in the context of multimodal classification tasks, which significantly undermines the persuasiveness of Table 5.** \\\nOur work focuses on parameter efficient adaptation techniques for existing multimodal models. The main goal is to adapt already pretrained models to different missing modality scenarios. While we cited the first two papers (\\[1, 2]) in our work, we did not compare with them for the following reasons:\\\n\\\n**Paper \\[1]** suggested multitask learning and search for optimal fusion strategy to enhance robustness (mainly for two modalities). Extending the methods for masking out attention for more than two modalities and for tasks that rely on all the tokens is not obvious. Furthermore, the paper did not provide a codebase or results for the datasets we used in our experiments; therefore, we could not compare with them.\\\n\\\n**Paper \\[2]** suggested Shared Specific Feature Modelling (ShaSpec), which is based on modeling and fusing shared and specific features. The paper focused on medical imaging (brain tumor segmentation) and classification. They did not provide any baseline for the datasets and tasks that we used in our work. That is why we did not compare our results with the paper.\\\n\\\n**Paper \\[3] -** This paper was submitted to arxiv 12 days after the ICLR submission deadline. For this reason, we could not cite or compare with this paper. We will be happy to cite the paper in the revised version. The paper did not provide code or baseline results for the datasets and tasks that we used in our paper. For these reasons, we are unable to provide a comparison."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4704/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700103686284,
                "cdate": 1700103686284,
                "tmdate": 1700103686284,
                "mdate": 1700103686284,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "txEA2lTqVR",
            "forum": "XTwwtlEfTF",
            "replyto": "XTwwtlEfTF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4704/Reviewer_ibUc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4704/Reviewer_ibUc"
            ],
            "content": {
                "summary": {
                    "value": "The paper tackles the problem of missing modalities at test time for multimodal learning. The motivation is to introduce small number of (new) parameters as much as possible during adpatation of a pre-trained model. The paper proposes parameter-efficient adpatation of multimodal networks.  Three different parameter-efficient adaptation techniques, including low-rank adpatation, scaling and shifting of features, and BitFit, have been investigated towards learning a model robust to missing modalities. Given a pre-trained network, in order to adapt this model to a subset of modalities, small set of parameters (as a layer) are introduced after each layer of the network. During adaptation to this subset of modality combination, only these small set of parameters are trained. Experiments have been performed with two different base networks for the task of multimodal semantic segmentation and multimodal sentiment analyses. Results claim to achieve better performance than competing robust methods and often surpass dedicated models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Development of multimodal learning algorithms robust to missing modalities is crucial since observations from some modality could go often be missing due to sensor/hardware failure.\n\n- The proposed method is effective and importantly efficient as it introduces a very small set of parameters towards making a pre-trained model robust to missing modalities.\n\n- The paper is mostly well-written and easier to read.\n\n- Experiments have been performed on two different tasks of multimodal semantic segmentation and multimodal sentiment analyses. Results claim to surpass the existing methods and (often) dedicated methods when the modalities are missing at test time."
                },
                "weaknesses": {
                    "value": "- Although the idea is simple and effective for achieving parameter-efficient adaptation of models, the application of existing techniques (i.e. low rank adaptation, scale and shift transformations, and BitFit) to adapt a pre-trained model on a subset of modalities seems low from novelty aspect.\n\n\n- There is no insight and/or analyses into how the proposed paramater-efficient is helpful towards making the model robust to missing modalities. It is important to develop the grounding of the proposed method since only showing improvements over the baseline with empirical results doesn't help much towards this.\n\n- The paper doesn\u2019t mention any clear justification on why SSA was chosen over the LoRA and BitFit to report the results in the paper.\n\n- Comparison is missing with any of the existing method for the task of multimodal sentiment analyses. \n\n- Why CMU-MOSI and CMU-MSEI datasets were chosen to evaluate and report the performance of the method?"
                },
                "questions": {
                    "value": "- There is no study that fairly compares the parameter efficiency of the proposed method with competing methods?\n\n- What are the possible reasons for relatively lower performance of other adaptation strategies, compared to SSA, in Table 4? Some explanation would be helpful to better understand their comparison in missing modalities scenario."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4704/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4704/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4704/Reviewer_ibUc"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4704/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698738394483,
            "cdate": 1698738394483,
            "tmdate": 1699636452021,
            "mdate": 1699636452021,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uCs97cbLLq",
                "forum": "XTwwtlEfTF",
                "replyto": "txEA2lTqVR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4704/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4704/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ibUc (Part 1/3)"
                    },
                    "comment": {
                        "value": "Thank you very much for your insightful and detailed review. Thank you for nicely summarizing the strengths and contributions of our paper. We respond to your questions and other comments below.\u00a0\n\n1. **Although the idea is simple and effective for achieving parameter-efficient adaptation of models, the application of existing techniques (i.e. low rank adaptation, scale and shift transformations, and BitFit) to adapt a pre-trained model on a subset of modalities seems low from a novelty aspect.**\\\nWe agree with you that the idea is simple and effective. Low rank adaptation, BitFit and Scale and Shift are originally designed for efficient model finetuning and transfer learning. We show that they can also be applied to adapt models to different missing modality scenarios.\\\n   1\\) To the best of our knowledge, we are the first to show that parameter efficient adaptation can be used to enhance performance in different missing modality scenarios. While it may be simple, we hope this will be informative and useful for the broader community.\\\n   2\\) We provide a generic framework for missing modality adaptation that can be applied to a wide range of multimodal datasets, tasks, and models.\\\n   3\\) The adapted model can easily switch to different network states based on the available modalities with minimal latency, computational, or memory overhead.\n######\n2. **There is no insight and/or analyses into how the proposed parameter-efficient is helpful towards making the model robust to missing modalities. It is important to develop the grounding of the proposed method since only showing improvements over the baseline with empirical results doesn't help much towards this.**\\\nOur main motivation and insight for the proposed approach is to adapt the functional representation of the network according to the available modality combination. For instance, if one of the modalities is missing, we want to modify the feature representations and fusion of other modalities. We could do this by changing the entire network for every modality combination, which is infeasible due to computational and storage cost. In this paper, we perform parameter-efficient adaptation to achieve the same goal. A rigorous analysis of how the network adaptation changes the functional representation of the network will certainly be great, but it is a nontrivial task and beyond the scope of one paper.\n######\n3. **The paper doesn\u2019t mention any clear justification on why SSA was chosen over the LoRA and BitFit to report the results in the paper.**\\\nWe have discussed this in the second paragraph of Section 3.3. We primarily selected the SSF technique because of its simplicity and effectiveness. SSF has several benefits over other adaptation methods:\\\n   1\\) The parameters (\u03b3, \u03b2) are independent of the input features, which makes SSF applicable to diverse tasks and input modality combinations.\u00a0\\\n   2\\) We can easily insert SSF layers in the existing model without changing the model architecture.\\\n   3\\) We can easily switch/select the corresponding SSF parameters for a given input modality combination.\\\n   4\\) SSF works relatively better than LoRa, BitFit and Norm in most of the scenarios as shown in Table 4."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4704/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700108334494,
                "cdate": 1700108334494,
                "tmdate": 1700108334494,
                "mdate": 1700108334494,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cZkooL38v5",
                "forum": "XTwwtlEfTF",
                "replyto": "txEA2lTqVR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4704/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4704/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ibUc (Part 3/3)"
                    },
                    "comment": {
                        "value": "6. **There is no study that fairly compares the parameter efficiency of the proposed method with competing methods?**\\\nTable 4 provides comparison of different parameter-efficient adaptation methods that can be used in our proposed framework.\nWe compared the performance of our adaptation-based method with existing robust methods in Table 2 and 3.\u00a0 Since these methods are primarily designed to be robust to missing modalities, their emphasis is not necessarily to learn parameter-efficient networks. For this reason, we felt a comparison in terms of parameter efficiency would be unnecessary or unfair. We are listing the total number of parameters for some of these models here for completeness.\u00a0\n\n   |                             |                                                        |\n   | --------------------------- | ------------------------------------------------------ |\n   | Method                      | Total Number of Parameters (M)                         |\n   | CEN                         | 118.2                                                  |\n   | MDRNet                      | 64.60                                                  |\n   | RTFNet (ResNet-152)         | 254.51                                                 |\n   | SAGate                      | 110.85                                                 |\n   | AsymFussion (ResNet-101)    | 118.2                                                  |\n   | Ours (Adapted CMNext Model) | 117.3\u00a0(Base Model: 116.56M, Adaptation Layers: 0.789M) |\n   |  |  |\n######\n7. **What are the possible reasons for relatively lower performance of other adaptation strategies, compared to SSA, in Table 4? Some explanation would be helpful to better understand their comparison in the missing modalities scenario.**\\\nSSF learns parameters ($\\gamma$, $\\beta$) for scaling and shifting the intermediate features. We can consider BitFit as a subset of SSF which only shifts the intermediate features. LoRA is only applied to the attention layers ($W_q$ and $W_v$) following the original paper and the number of parameters for LoRA is less than SSF. Norm only optimizes the norm layers while SSF is applied after every linear, convolutional and norm layer. Which gives SSF more advantage over other methods to modulate the intermediate features and perform better on downstream tasks."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4704/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700112951291,
                "cdate": 1700112951291,
                "tmdate": 1700113081243,
                "mdate": 1700113081243,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IYk6lhoUdV",
                "forum": "XTwwtlEfTF",
                "replyto": "cZkooL38v5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4704/Reviewer_ibUc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4704/Reviewer_ibUc"
                ],
                "content": {
                    "comment": {
                        "value": "I thank authors for submitting the responses to all my comments. Particularly, i appreciate the new comparisons provided on multimodal sentiment analyses task for comment 4 and the parameter efficiency table for comment 6. However, the responses to two very important comments (1 & 2) on novelty and grounding of the proposed method are largely unsatisfactory. For novelty, applying/adopting an existing method to a new problem is difficult to accept as a novelty per ICLR standards. Likewise, it is important to know that how the proposed idea is bringing performance improvements claimed via some analyses and insights in the context of the problem solved and so it cannot be considered out-of-scope. In fact, the 1 & 2 are related in some sense, especially when the novelty aspect of the method can sometimes be justified by providing new and/or rigourous analyses."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4704/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700639551154,
                "cdate": 1700639551154,
                "tmdate": 1700639551154,
                "mdate": 1700639551154,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yCtDeeS54U",
            "forum": "XTwwtlEfTF",
            "replyto": "XTwwtlEfTF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4704/Reviewer_wJeo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4704/Reviewer_wJeo"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a parameter-efficient adaptive method based on pre-trained multi-modal networks. This method recovers the missing modality by introducing a low-rank adaptive layer and a modulation scheme of intermediate features. Furthermore, this paper proves a simple linear operations can partially compensate for the performance degradation caused by missing modality. Finally, this paper conducts sufficient experiments on different datasets, showing the robustness and versatility of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.This paper proposes an efficient approach to drive a multimodal model that adapts to different modality combinations, effectively addressing the performance degradation caused by missing modalities during testing. \n\n2.The proposed method introduces only 0.7% of model parameters and achieves comparable performance to dedicated models on certain datasets. \n\n3.Extensive experiments on multiple datasets demonstrate the superior performance of the proposed method compared to mainstream MML algorithms in handling the issue of missing modalities. \n\n4.The writing of this paper is coherent, presenting clear contributions."
                },
                "weaknesses": {
                    "value": "1.The method only uses zero to replace missing modalities, lacking other compensation methods such as duplicating available modal data or utilizing cross-modal generation techniques to fully showcase the impact of missing modalities.\n \n2.The method's SSF strategy is an input-independent adaptation approach, it struggles to maintain reliable performance in scenarios with significant distribution differences between the training and testing sets. \n\n3.Table 9 shows that there is still a gap compared to other adaptation method (BitFit), which makes the advantages of the proposed SSF strategy not obvious enough. \n\n4.In Table 8 of the experimental section, the proposed method still exhibits a considerable performance gap compared to dedicated models. Can increasing the number of adaptation layer parameters further improve performance?"
                },
                "questions": {
                    "value": "See above weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4704/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698745316872,
            "cdate": 1698745316872,
            "tmdate": 1699636451928,
            "mdate": 1699636451928,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "F50lL39aGb",
                "forum": "XTwwtlEfTF",
                "replyto": "yCtDeeS54U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4704/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4704/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wJeo (Part 1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your detailed and insightful review and acknowledging the strengths of our paper. Below we respond to your other comments and questions.\u00a0\n\n1. **The method only uses zero to replace missing modalities, lacking other compensation methods such as duplicating available modal data or utilizing cross-modal generation techniques to fully showcase the impact of missing modalities.**\\\n   We used zeros to replace missing modalities because that seems to be a common practice in recent papers \\[1, 2]. We compared the performance of duplicating available modalities and replacing zeros for missing modalities in the table below (same settings as Table 1 in the paper). The results show that duplicating available modalities is inferior to the adapted model (and in some cases worse than replacing zeros for missing modalities). For MFNet and NYUDv2 datasets, we replaced the missing modality with a copy of the available modality. For the MCubeS dataset, we replaced the missing modalities with a copy of RGB modality. In all the combinations, the adapted model works much better than the modality duplication method.\n\n   |         |             |         |                                |                                    |           |\n   | :-----: | :---------: | :-----: | :----------------------------: | :--------------------------------: | :-------: |\n   | Dataset |    Input    | Missing | Pretrained (Zeros for missing) | Pretrained (Duplicate for missing) |  Adapted  |\n   |  MFNet  | RGB-Thermal |    -    |              60.10             |                  -                 |     -     |\n   |         |     RGB     | Thermal |              53.71             |              52.33             | **55.22** |\n   |         |   Thermal   |   RGB   |              35.48             |                44.43               | **50.89** |\n   |  NYUDv2 |  RGB-Depth  |    -    |              56.30             |                                    |           |\n   |         |     RGB     |  Depth  |              51.19             |              46.19             | **52.82** |\n   |         |    Depth    |   RGB   |              5.26              |                13.94               | **36.72** |\n   |  MCubeS |  RGB-A-D-N  |    -    |              51.54             |                  -                 |     -     |\n   |         |   RGB-A-D   |    N    |              49.06             |                49.93               | **51.11** |\n   |         |    RGB-A    |   D-N   |              48.81             |                49.23               | **50.62** |\n   |         |     RGB     |  A-D-N  |              42.32             |                48.96               | **50.43** | \n\n   Utilizing cross-modal generation is also a very good idea for missing modality compensation. Table 3 in our paper provides a comparison with one such method, TokenFusion, which dynamically detects uninformative tokens and substitutes these tokens with projected and aggregated tokens from available modalities. The CRM method in Table 2 uses self-distillation between the clean and masked modalities to learn complementary and non-local representations for better performance on missing modality scenarios. Our method performs better than both these methods in most of the scenarios.\n######\n2. **The method's SSF strategy is an input-independent adaptation approach; it struggles to maintain reliable performance in scenarios with significant distribution differences between the training and testing sets.**\\\n   This is a good point and SSF may perform poorly in some cases, but in our experience SSF provides quite robust adaptation. According to \\[3], parameters learnt by SSF are input-independent that makes it more suitable for representing the distribution of the different downstream dataset by modulating the intermediate feature. We could learn the modulation parameters by conditioning them on the input features, but that would require some additional modules (e.g. MLP and activation functions) at the expense of additional parameters.\n****\n\\[1] Mengmeng Ma, Jian Ren, Long Zhao, Davide Testuggine, Xi Peng; \u201cAre Multimodal Transformers Robust to Missing Modality?\u201d, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 18177-18186\n\n\\[2] Sangmin Woo, Sumin Lee, Yeonju Park, Muhammad Adi Nugroho, Changick Kim;\n\u201cTowards Good Practices for Missing Modality Robust Action Recognition.\u201d, AAAI 2023: 2776-2784\n\n\\[3] Lian, Dongze and Zhou, Daquan and Feng, Jiashi and Wang, Xinchao; \u201cScaling & Shifting Your Features: A New Baseline for Efficient Model Tuning\u201d, Advances in Neural Information Processing Systems (NeurIPS), 2022"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4704/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700107311453,
                "cdate": 1700107311453,
                "tmdate": 1700107311453,
                "mdate": 1700107311453,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Cbzx7TFAah",
            "forum": "XTwwtlEfTF",
            "replyto": "XTwwtlEfTF",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4704/Reviewer_WPGU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4704/Reviewer_WPGU"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method for multimodal learning with missing modalities. The writing is good. The experiments are suffcient. However, the math behind it is not clear, and the author does not give a reason why the data with missing modalities is not considered in the training phase."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The writing is good. The experiments are suffcient."
                },
                "weaknesses": {
                    "value": "The math behind this paper is not clear, and the author does not give a reason why the data with missing modalities is not considered in the training phase."
                },
                "questions": {
                    "value": "The datasets are not diverse enough. The authors only considered segmentation and sentiment analysis problems, different types of data should be considered. Figure 1 is not clear."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4704/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698829688504,
            "cdate": 1698829688504,
            "tmdate": 1699636451826,
            "mdate": 1699636451826,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yni2XN8Dt4",
                "forum": "XTwwtlEfTF",
                "replyto": "Cbzx7TFAah",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4704/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4704/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WPGU"
                    },
                    "comment": {
                        "value": "Thank you for your review. We address your comments below.\u00a0\n\n1. **The math behind this paper is not clear.** \\\nThe main problem formulation is presented in Sec. 3. We believe we provided a concise mathematical description of our approach without unnecessary details. Equation 1 can be viewed as a model/network that is trained for all modalities $\\mathcal{M}$ (consider it a baseline model). Equation 2 can be viewed as a model that is trained for a subset of modalities $\\mathcal{S}$. We can in principle train one such model for every possible modality combination (dedicated training), but that would be prohibitive with a large number of possible modality combinations. Equation 3 represents our approach where we can learn a small set of parameters for different missing modality situations to adapt an already trained model. Mathematical formulations for different adaptation techniques are presented in Sec. 3.2. Please let us know if you have any other questions, and we will be happy to elaborate further.\n######\n2. **The author does not give a reason why the data with missing modalities is not considered in the training phase.** \\\nIt is unclear what you mean by this comment. We suspect you mean that we could use data with missing modalities at the training stage and either train a single robust network that performs well with all possible missing modalities or train one network for every specific missing modality combination. The latter approach is obviously infeasible as the number of networks required to train can become large. The former approach is something we considered and compared against (see results in Table 2 for VPFNet, SpiderMesh, CRM and Table 3 for TokenFusion). Our proposed network adaptation method clearly outperforms a single network trained to perform well with all possible missing modalities.\\\nIn our experiments, we wanted to separate the effects of robust training and network adaptation, but our proposed network adaptation scheme can be easily combined with a network pretrained in a robust manner. Furthermore, we compared our method with CRM in Table 2 that is trained in a robust manner and performs worse than our method on an average.\n######\n3. **The datasets are not diverse enough. The authors only considered segmentation and sentiment analysis problems, different types of data should be considered.** \\\nWe performed experiments for 3 different tasks (semantic segmentation, material segmentation and sentiment analysis). The tasks involve 5 different datasets and a total of 8 different modalities (RGB images, thermal images, depth maps, angle of linear polarization, degree of linear polarization, video, audio and text). For semantic segmentation we considered 2 different datasets: MFNet (RGB-thermal) and NYUDv2 (RGB-depth). For material segmentation we considered the MCubeS dataset that has four modalities: RGB images, thermal images, angle of linear polarization and degree of linear polarization. Multimodal sentiment analysis datasets (CMU-MOSI and CMU-MOSEI) have video, audio and text as modalities. We believe these datasets are diverse enough for a single paper. Please let us know if you have any specific dataset or task in mind. We will be happy to address that.\n######\n4. **Figure 1 is not clear.** \\\nFigure 1 illustrates our overall approach to missing modality adaptation. Figure 1(a) shows that we can use a pretrained model, freeze all the weights, and insert adaptable layers after linear, convolutional, and norm (both batchnorm and layernorm) layers. Frozen and adaptable layers are illustrated with different colors for better understanding. Figure 1(b,c) shows two types of parameter-efficient adaptation schemes that can be used to modify the network output. We explained the figure in the first paragraph of Section 3.3 and explained the overall approach in Section 3.1 Equation 3. Please let us know which part is still not clear and we will explain or modify it as necessary."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4704/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700105488823,
                "cdate": 1700105488823,
                "tmdate": 1700105488823,
                "mdate": 1700105488823,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "K81UlggdWr",
                "forum": "XTwwtlEfTF",
                "replyto": "Cbzx7TFAah",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4704/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4704/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\\\nWe hope you had a chance to read our response to your questions. We appreciate your feedback. Please let us know if you have any additional question that we can answer today. Thank you for your kind attention."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4704/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690396200,
                "cdate": 1700690396200,
                "tmdate": 1700690432415,
                "mdate": 1700690432415,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]