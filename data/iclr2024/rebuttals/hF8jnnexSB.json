[
    {
        "title": "The Power of Minimalism in Long Sequence Time-series Forecasting"
    },
    {
        "review": {
            "id": "sGGB51J2MZ",
            "forum": "hF8jnnexSB",
            "replyto": "hF8jnnexSB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6882/Reviewer_gotF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6882/Reviewer_gotF"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the problem of time series forecasting. The paper investigates the performance of a simple model, namely a one-layer convolutional network applied to every feature independently and combined with a linear layer. The paper shows that such a simple approach could improve upon the existing baselines in most of the cases with significantly reduced computational costs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper shows that a simple network structure could be very effective in the widely used time-series forecasting datasets. The paper thus provides a valuable baseline that future methods should all consider when dealing with such kinds of tasks.\n\n2. The paper conducts extensive experiments and studies to make their results convincing."
                },
                "weaknesses": {
                    "value": "1. As also mentioned in the paper, time series with multiple periodic intervals could not be captured by a single convolutional layer. I think it might make the paper stronger by generating synthetic data with various periodic behaviors and testing various models on it.\n\n2. Every univariate is now processed independently in the current convolutional network. Is there a specific reason for doing so except for the efficiency concerns? How would the performance change if we also include the feature dimension in the convolutional filter?\n\n3. Why just one layer of convolution? How does the performance change if multiple layers are applied? This may help to capture longer periodic patterns or even help with the multi-period issue.\n\n4. As also mentioned in the paper, the effectiveness of relatively simple models such as DLinear and the convolutional network may largely depend on the nature of the current tasks. For much more complex time series with more features and periodic complexities, such simpler methods may not be as good as the transformer-based models.\n\n5. How do we determine the kernel size of the convolution, which should be critical for the forecasting task, especially for the cases where we don't know the periodic interval of the data streams?"
                },
                "questions": {
                    "value": "Please check the weaknesses part.\n\nUpdate after the rebuttal:\nThanks for the detailed response. It addresses some of my concerns but some remain. E.g., there is no empirical evidence to support the claim of estimating the period. And the solution for more complex tasks is reasonable but not convincing enough. Overall, I think the work could serve as a solid baseline for the time series forecasting tasks, so I keep my score for weak acceptance."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6882/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6882/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6882/Reviewer_gotF"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6882/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698551919091,
            "cdate": 1698551919091,
            "tmdate": 1700976473061,
            "mdate": 1700976473061,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "K6AadBwfg8",
                "forum": "hF8jnnexSB",
                "replyto": "sGGB51J2MZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6882/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6882/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gotF"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your detailed comments and positive ranking. We provide point-wise responses to your concerns below.\n\nQ1 : *Every univariate is now processed independently in the current convolutional network. Is there a specific reason for doing so except for the efficiency concerns? How would the performance change if we also include the feature dimension in the convolutional filter?*\n* Many thanks for your careful reading and suggestion. In LTSF tasks, maintaining channel independence has been observed to improve prediction performance as compared to channel mixing, as reported in prior research [2]. **Our model utilizes group-wise convolution to cleverly achieve channel independence, while concurrently reducing model complexity.** However, standard CNN uses the idea of channel mixing, which suffers from noise disturbance among the channels and reduces performance. We have conducted additional ablation experiments about depth-wise CNN and general CNN. The results are in Section C.2, Page 13 of the original manuscript. Channel independence helps reduce information confusion. When each channel focuses on capturing specific time series long-term patterns, the model is more capable of distinguishing and understanding these patterns without the noise that channel mixing might introduce. If we also include the feature dimension in the convolutional filter, the performance will degrade.\n* Moreover, we also apply channel independence and channel dependence to the MLPs-based models respectively. The results can be found in Table 5 of the attached PDF file. Overall, the accuracy of the CI-based models was higher than the CD-based models.\n\nQ2 : *Why just one layer of convolution? How does the performance change if multiple layers are applied? This may help to capture longer periodic patterns or even help with the multi-period issue.*\n* We are grateful for your constructive comments. **We want to emphasize that this manuscript does not only aim to showcase the contributions of SOTA results. Simple yet valuable basic units can serve as foundational building blocks, laying the groundwork for scalable and complex networks.**  We firmly believe that LTSF-Conv models serve as straightforward yet competitive basic units, exhibiting great potential for further expansion of complex structures. As the reviewers pointed out, based on our extended experiments, when applying complex network structures, the performance will be further improved. In the future, there is the potential for more valuable research to be explored in this new track. \n\nQ3: *Also mentioned in the paper, the effectiveness of relatively simple models such as DLinear and the convolutional network may largely depend on the nature of the current tasks. For much more complex time series with more features and periodic complexities, such simpler methods may not be as good as the transformer-based models.*\n* Thank you for pointing out this. When dealing with more complex datasets, increasing the depth, or introducing additional hierarchical structures to the network helps the model adapt better to the diversity of the data. For LTSF-Conv basic unit, exhibiting great potential for further expansion of complex structures. Whether based on MLPs, simpler convolution, or transformers, each model has contributed to the field of long sequence prediction. The development of these models has advanced the field of LTSF, providing diverse modeling choices for various tasks.\n\nQ4: *How do we determine the kernel size of the convolution, which should be critical for the forecasting task, especially for the cases where we don't know the periodic interval of the data streams?*\n* Thank you for pointing out this. In essence, finding the most suitable convolutional kernel size is a dynamic process, combining experimentation and consideration of specific data intricacies for optimal model performance. It's important to iterate and evaluate the model's performance to find the optimal kernel size for your specific forecasting task. In the absence of known periodic intervals, we usually employ a multi-scale analysis strategy, which enables the model to capture features at different time scales without relying on prior knowledge of periodicity. We can also use cross-validation to assess the performance of different kernel sizes."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6882/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700313155105,
                "cdate": 1700313155105,
                "tmdate": 1700313155105,
                "mdate": 1700313155105,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RZeTZJWvFH",
            "forum": "hF8jnnexSB",
            "replyto": "hF8jnnexSB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6882/Reviewer_1dcB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6882/Reviewer_1dcB"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to address the challenge of long-term time series forecasting (LTSF) and introduces LTSF-Conv models as a solution. The authors discuss the limitations of existing methods, such as Transformer-based models and MLP-based models, and highlight the need for a balance between performance and efficiency. The experiments show that the proposed LTSF-Conv models, based on convolutional neural networks (CNNs), consistently outperform complex Transformer-based models and state-of-the-art MLP-based models, while maintaining efficiency. The paper provides some insights into input window sizes, encoder-decoder structures, and handling time series with multiple periods among channels."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper has the following advantages:\n\n1. Novel solution: The paper introduces the LTSF-Conv model as a new approach to address long-term time series forecasting. By utilizing convolutional neural networks (CNNs), the model outperforms complex Transformer-based and MLP-based models in most cases while maintaining efficiency.\n\n2. Empirical research and extensive experiments: The authors conduct comprehensive experimental evaluations on multiple real-world datasets across various domains such as weather, traffic, and electricity. The results consistently demonstrate that LTSF-Conv models outperform other complex models in terms of average performance. The paper provides concrete performance comparison data to support their findings.\n\n3. Analysis and discussion of existing models' limitations: The paper thoroughly analyzes the limitations of existing Transformer-based and MLP-based solutions, particularly in handling long-term time series and multi-channel data. This analysis helps to understand their constraints and guides future research.\n\n4. Efficiency and reduced computational resources: Compared to complex models, LTSF-Conv models achieve high performance while significantly reducing computational resource requirements. This is particularly valuable in practical applications with limited computing resources, enhancing the model's practical usability and scalability.\n\n5. Insights for other aspects in the field: The paper also explores issues related to input window sizes, encoder-decoder structures, and handling multi-channel time series, providing valuable insights for future research in the LTSF domain."
                },
                "weaknesses": {
                    "value": "Based on your understanding of the AI industry, you believe this paper has the following shortcomings:\n\n1. Lack of innovation:\n   - The model used in the paper consists of only two layers of convolutional networks, along with a decomposition of trend and periodic components. The loss function used is the classical MSE loss. There is a lack of innovation in the model design.\n   - The innovation mainly lies in explaining the good performance of the simple convolutional model. However, the paper only provides a simple \"proof\" that convolutional kernels larger than a certain duration can capture periodic information shorter than that duration. The subsequent heatmaps only qualitatively observe that the model captures some periodic information, without explaining why the simple convolutional model is competitive.\n   - Obvious conclusions, such as the smaller memory footprint, shorter training and inference time, and fewer parameters of the simple model, are extensively analyzed and explained in the paper.\n\n2. Experimental limitations:\n   - The paper lacks several important baseline models based on CNN architectures, such as SCINet and TimesNet, in the Conv-based model category (this is particularly severe, as all models in this category are proposed in this paper).\n   - MLP-based models lack models like N-Hits, and there is also a lack of references to the aforementioned models.\n   - The discussion of \"model performance with respect to lookback\" in Section C.1 lacks the inclusion of DConv. Considering Table 1 and Table 2, which compare the model results, the \"best\" model used in the tables is actually the model with a lookback of 1600, which naturally performs better than other baseline models with smaller lookback values that have not reached their optimal states. Moreover, even the \"best\" model is outperformed by PatchTST, which does not have data with lookback values of 720, 1000, and 1600.\n   - The performance of the proposed models in complex datasets like Traffic is poor.\n\n3. Presentation issues:\n   - The quality of the figures illustrating the model is low and overly simplified.\n   - There are formatting issues with the caption of Figure 6.\n\nIn summary, the identified shortcomings of the paper include a lack of innovation in the model design, experimental limitations in terms of missing baseline models and dataset performance, and presentation issues with figures and captions."
                },
                "questions": {
                    "value": "What I'm concerned about are listed in the weakness. I won't refuse to raise my points if the authors can address my concerns."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6882/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6882/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6882/Reviewer_1dcB"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6882/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698718958200,
            "cdate": 1698718958200,
            "tmdate": 1699636800126,
            "mdate": 1699636800126,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uunToG32Wm",
                "forum": "hF8jnnexSB",
                "replyto": "RZeTZJWvFH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6882/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6882/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1dcB"
                    },
                    "comment": {
                        "value": "Many thanks for your careful reading and suggestion. We appreciate that you found us some advantages. \n\nMeanwhile, we would like to provide some insights into the contribution of this paper. When the DLinear model [1] was initially introduced, its simple linear structure sparked considerable controversy. However, it ultimately won the Best Paper Award, defying complex Transformer models in the LTSF field. Subsequently, more works have been dedicated to refining and advancing linear models, including architectural modifications and innovative training methodologies. **We want to emphasize that our manuscript does not only aim to showcase the contributions of SOTA results. Our model structure is as simple as DLinear, while extensive experiments have consistently demonstrated that depth-wise convolutional units possess inherent advantages over linear units (such as DLinear, and RLinear) in LTSF.** Linear mappings often struggle to capture intricate dependencies when handling data with multiple periods among channels. Although modeling each channel independently can alleviate this issue, it will significantly increase the computational cost. More experimental analyses are described in Section 7, Page 9.  **Based on our basic unit, more complex networks can be proposed to further improve the prediction performance of such tasks. In the future, there is the potential for more valuable research to be explored in this new track.**\n\nWe would like to address your concerns point-by-point below:\n \nQ1 : *The paper lacks several important baseline models based on CNN architectures, such as SCINet and TimesNet, in the Conv-based model category (this is particularly severe, as all models in this category are proposed in this paper). MLP-based models lack models like N-Hits, and there is also a lack of references to the aforementioned models.*\n* Thank you for your comments. Due to space limitations, we compared representative SOTA models from recent years in original our manuscripts. Following the reviewer\u2019s suggestion, **we have added SCINet, TimesNet, MICN, and N-Hits baseline models.** We adopt their official codes and only change the length of input sequences. The results are summarized in Table 2. In Table 2, we conducted an experiment within a broader window size range of \\{96, 336, 512, 720, 1600\\} for a fair comparison. **It can be found in the attached supplementary material file.**  The results demonstrate that LTSF-Conv models consistently surpass all SCINet, TimesNet, MICN, and N-Hits on seven LTSF benchmarks. The symbol * denotes the experimental results we conducted after increasing the input length (re-implementation). For N-Hits, authors have adopted an extended step size hyperparameter search, and we directly adopt the results. We will add the related references in the revised paper.\n\nQ2 : *Considering Table 1 and Table 2, which compare the model results, the \"best\" model used in the tables is actually the model with a lookback of 1600, which naturally performs better than other baseline models with smaller lookback values that have not reached their optimal states. Moreover, even the \"best\" model is outperformed by PatchTST, which does not have data with lookback values of 720, 1000, and 1600.*\n* Thank you for pointing out this.  Previous research [1] has also shown that Transformer-based baselines have not benefited from a longer look-back window. Their performance fluctuates or gets worse as the input lengths increase. We have conducted experiments to analyze it. **The reason why most transformer models do not increase the lookback window length is because the performance will become worse.** More analysis of the look-back windows is described in Section C.1, Page 12. in the original Appendix. \n* We have confidence in the fairness of our previous comparison. **The experimental results not only include Conv-Best and DConv-Best but also the default look-back window length is set at 512, such as Conv and Dconv.** This is using the same lookback window length as PatchTST.\n* Different from other Transformer-based baselines, PatchTST can extend the lookback window to 512, but does not benefit from longer windows. It's important to note that this extension comes at the cost of a substantial increase in the model\u2019s computational resource requirements. On our server resources, PatchTST ran out of GPU memory for a look-back window size greater than 720.\n* To alleviate your concerns, we switched to a higher-performance server to validate PatchTST with lookback-window size $\\in$ \\{720, 1600\\}. Since our model only experiments from the extended window {336, 512, 720, 1600}, we ignore the 1000 step size. **The results are in Table 3 of the attached supplementary material file.** As expected, **PatchTST did not benefit from longer lookback windows. As the input length increases, the significant memory overhead poses limitations for its practical application.**"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6882/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700324672235,
                "cdate": 1700324672235,
                "tmdate": 1700324672235,
                "mdate": 1700324672235,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SdAYObdfMh",
            "forum": "hF8jnnexSB",
            "replyto": "hF8jnnexSB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6882/Reviewer_dk3x"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6882/Reviewer_dk3x"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents an innovative depthwise convolution model to perform long-term time series forecasting. The key idea is to apply unique filters to each channel to achieve channel independence. The experiment results on public benchmark datasets justified the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper is well written and organized. \n2. The proposed convolution based long-term forecasting technique is well-motivated based on a theoretical insight over the periodicity assumption of the time series.\n3. Applying RevIN over the one-depthwise convolution operation to deal each channel independently is new. Based on that, a simple yet effective LTSF-Conv models for long term forecasting tasks is developed.\n4. The experiment results are comprehensive and quite solid in this paper. State-of-the-art transformer based methods such as PatchTST and MLP-based model TiDE are both compared. The proposed Convolution-based models significant outperform baselines on most cases. In addition, they also consume small GPU memory and exhibit less trainable parameters."
                },
                "weaknesses": {
                    "value": "1. Whether the Conv-LTSF still works for time series that does not exhibit strong periodicity?\n2. I wonder whether explicitly considering the channel dependencies can help further improve the forecasting performance."
                },
                "questions": {
                    "value": "Please see the weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6882/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6882/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6882/Reviewer_dk3x"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6882/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698756204536,
            "cdate": 1698756204536,
            "tmdate": 1700361894735,
            "mdate": 1700361894735,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "muCEMM6Fgt",
                "forum": "hF8jnnexSB",
                "replyto": "SdAYObdfMh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6882/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6882/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dk3x"
                    },
                    "comment": {
                        "value": "We sincerely appreciate your comments and positive ranking. In order to ensure an unbiased evaluation, we rigorously adhered to an identical code structure as that of the other baseline models. This encompassed utilizing the same data preprocessing procedures, including the data provider module. Our contribution was seamlessly integrated by adding our method to the model folder. ALL experimental results can be repeated by utilizing the provided source code. This simple basic unit serves as a robust starting point, showcasing its potential in handling LTSF tasks.\n\nWe provide point-wise responses to your concerns below.\n\nQ1 : *Whether the Conv-LTSF still works for time series that does not exhibit strong periodicity?*\n*  Many thanks for your careful reading and suggestion. Following the reviewer\u2019s suggestion, we compared the proposed model (Conv) with the DLinear and strongest Transformer baseline (PatchTST) on the illness benchmark. The illness benchmark describes the ratio of patients seen with illness and the number of patients from 2002 to 2021. It includes weekly data from the Centers for Disease Control and Prevention of the United States (https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html).  Table 4 (in the attached PDF file) provides the corresponding forecasting results. It can be found that Conv outperforms DLinear and PatchTST for all horizons. \n\n| Methods   |Conv|      | PatchTST|      | DLinear|      |\n|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| Metric    | MSE|MAE| MSE| MAE  | MSE     | MAE  |\n| 24        | 0.451 | 0.552| 0.637| 0.552| 0.725   | 0.681|\n| 36        | 0.508 | 0.574| 0.765| 0.634| 0.792   | 0.744|\n| 48        | 0.597 | 0.636| 0.756 | 0.692| 0.886   | 0.815|\n| 60        | 0.675 | 0.680| 0.776 | 0.741| 0.960   | 0.859|\n| Avg       | **0.558** | **0.611** | 0.734 | 0.655 | 0.841 | 0.775 |\n\n**Table 4**: The forecasting results on the ILI benchmark under the setting, where sl = 104.\n\nQ2 : *I wonder whether explicitly considering the channel dependencies can help further improve the forecasting performance.*\n* In time series forecasting tasks, we intuitively think that channel-mixing techniques are often a superior approach, which allows the model to effectively capture the interdependencies and interactions between different channels. In channel dependencies, information from various channels is integrated efficiently, facilitating a more comprehensive representation of the complex relationships within the data. However, previous studies [2] have found that channel independence can improve performance compared to channel mixing in LTSF.\n* CI: so-called channel independence; CD: so-called channel dependencie. Following the reviewer\u2019s suggestion, we apply channel independence and channel dependence to the different models respectively. Table 5 (in the attached PDF file) summarizes the results of different channel strategies on the Weather and ETTm1 datasets. **Most of the CI-based models have a higher testing accuracy than the CD-based models.** For MLP-based models, the overall accuracy of the CI-based models was approximately 1\\%\u20138\\% higher than the CD-based models.\n* For our models, we have conducted additional ablation experiments about depth-wise CNN (channel independent) and general CNN (channel dependent). The results are in Section C.2, Page 13 of the original manuscript. It can be found that CI-based models also outperform CD-based models.\n\n\n|  |  | |  | Weather  |  |  | |  | ETTm1  |  |  |\n|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n|  |  | 96 | 192 | 336 | 720 | Avg | 96 | 192 | 336 | 720 | Avg |\n| RLinear_CD | MSE | 0.175 | 0.217 | 0.265 | 0.328 | 0.246 | 0.301 | 0.34 | 0.373 | 0.43 | 0.361 |\n|  | MAE | 0.225 | 0.259 | 0.293 | 0.339 | 0.279 | 0.342 | 0.366 | 0.385 | 0.417 | 0.377 |\n| RLinear_CI | MSE | 0.146 | 0.189 | 0.241 | 0.313 | **0.222** | 0.289 | 0.332 | 0.368 | 0.426 | **0.353** |\n|  | MAE | 0.194 | 0.234 | 0.274 | 0.327 | **0.257** | 0.335 | 0.361 | 0.38 | 0.413 | **0.372** |\n| DLinear_CD | MSE | 0.175 | 0.219 | 0.265 | 0.323 | 0.245 | 0.299 | 0.335 | 0.369 | 0.424 | 0.356 |\n|  | MAE | 0.237 | 0.282 | 0.318 | 0.361 | 0.299 | 0.343 | 0.365 | 0.386 | 0.42 | 0.378 |\n| DLinear_CI | MSE | 0.146 | 0.19 | 0.243 | 0.317 | **0.224** | 0.285 | 0.327 | 0.367 | 0.428 | **0.351** |\n|  | MAE | 0.212 | 0.257 | 0.301 | 0.358 | **0.282** | 0.334 | 0.358 | 0.383 | 0.417 | **0.373** |\n| NLinear_CD | MSE | 0.181 | 0.225 | 0.27 | 0.339 | 0.253 | 0.305 | 0.348 | 0.375 | 0.433 | 0.365 |\n|  | MAE | 0.232 | 0.268 | 0.3 | 0.348 | 0.287 | 0.347 | 0.375 | 0.388 | 0.421 | 0.382 |\n| NLinear_CI | MSE | 0.146 | 0.189 | 0.242 | 0.321 | **0.224** | 0.293 | 0.337 | 0.379 | 0.435 | **0.361** |\n|  | MAE | 0.196 | 0.238 | 0.28 | 0.335 | **0.262** | 0.341 | 0.367 | 0.39 | 0.422 | **0.38** |\n\n**Table 5**: Multivariate prediction results on two benchmarks with an input length of 336. CI denotes channel-independence, and CD represents channel-dependence."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6882/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700308314652,
                "cdate": 1700308314652,
                "tmdate": 1700308314652,
                "mdate": 1700308314652,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jrWlpDitgq",
                "forum": "hF8jnnexSB",
                "replyto": "muCEMM6Fgt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6882/Reviewer_dk3x"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6882/Reviewer_dk3x"
                ],
                "content": {
                    "comment": {
                        "value": "The authors have addressed my concerns."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6882/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700361844188,
                "cdate": 1700361844188,
                "tmdate": 1700361844188,
                "mdate": 1700361844188,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ppzGwpc4ow",
            "forum": "hF8jnnexSB",
            "replyto": "hF8jnnexSB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6882/Reviewer_3Fmc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6882/Reviewer_3Fmc"
            ],
            "content": {
                "summary": {
                    "value": "**Summary:**\n\nThe paper aims to address the challenges faced by Transformer-based models in long-term time series forecasting (LTSF) tasks, mainly when dealing with long sequence lengths. The authors propose a new model, LTSF-Conv, which utilizes depthwise convolution models to enhance forecasting performance while significantly reducing computational costs. \n\n**Strengths:**\n\nFrom the computational efficiency (memory usage/flops) perspective, this paper reports very promising results on several datasets. \n\n**Weaknesses:**\n\nFrom the accuracy perspective, two recent CNN baseline models, TimesNet and MICN [1] are missing in Table 1 and Table 2. Moreover, the hyperparameter-searching is used, which makes the comparison a little bit unfair. For example, in TimesNet and MICN, the model configurations and lookback window remain the same for most of the experiments, and in PatchTST, only two configurations are considered. Based on the current results, it is hard for me to tell whether the performance gain is from the better model configuration or the proposed structure. \n\nMoreover, based on my understanding of Section 4.1, the main takeaway message would be there are two useful structures, depth-wise 1Dconv, and/or trend/seasonality decomposition. A similar idea (i.e., 1Dconv + decomposition) is also mentioned in MICN (e.g.,  Figure 1 in [1]). One more interesting thing here is the usage of depth-wise CNN instead. As shown in Table 4, deep-wise gives significant performance improvements. I would expect a more in-depth analysis of why it reaches better results than vanilla CNN. Based on the current presentation, it is a little bit difficult for me to understand what inductive bias can only be utilized by depth-wise CNN but not general CNN.\n\nThe theoretical analysis is also kind of weak. Theorem 1 and Corollary 1 consider simple autoregressive state-space structure and MLP/RNN models can also have the same prediction power. MLP can be viewed as a CNN with kernel size equal to the sequence length. RNN is commonly used to model state-space structures. Theorem 2 considers the sequence with both trend and seasonality. From my understanding, the MLP/RNN may also reach a similar performance guarantee.  \n\nAfter reviewing the sample codes in the supplementary material. I also have some concerns about the numerical results reported in the paper. When dealing with test samples, the data_provider function sets the drop_last = True and shuffle_flag = False. The consequence would result in the last several test samples being ignored. Those samples are usually the hardest to predict since they are far away from the training set. Moreover, it seems the main results in Table1 and Table2 are only run with one fixed random seed 1024. The random control experiment is only reported in Figure 5 in the Appendix.\n\n\n**Questions and Suggestions:**\n\n1. As the title used the word *minimalism*, I would conjecture the main advantage of using simple depth-wise CNN would be its robustness. The time series forecating usually contains a lot of time-varying noise especially when using longer inputs. The usage of a simpler model would have less risk of overfitting that noise but a potential drawback would be more modeling bias may be introduced due to limited representation power. Therefore, I would expect the analysis from the theoretical part to consider the high noise system, such as $x(t)  = x(t-p) + \\epsilon\\_t$ where $\\epsilon\\_t$  could be on the same order of $x(t)$, and analyze the generalization ability of depth-wise CNN to show it will have better variance bias trade-off.\n\n2. Please add TimesNet and MICN as benchmarks in Table 1 and Table 2. \n\n3. Please fix the dataloader issue in the test part and rerun the relevant experiments. It would be better to also report the random control results in Table 1 and Table 2.\n\n4. Please provide the detailed experimental configurations for each setting in Table 1 and Table 2 to help the reviewer verify those results.\n\n5. Could the author elaborate more on the seq_last in ConvNet.py file? It seems not to be discussed in Section 4. Moreover, since Revnorm has been used, the sequence would already be centered, why do we still need to subtract the sequence mean?\n\n\n**Conclusion:**\n\nWhile the paper explores an intriguing concept that simpler models might suffice for certain datasets, the current depth of analysis and the reliability of numerical results do not yet support a strong case for acceptance at a top-tier machine learning conference like ICLR. Despite this, the reviewer is willing to reconsider the decision after the authors' rebuttal.\n\n\n\n\n**Reference**\n\n[1] Wang, Huiqiang, Jian Peng, Feihu Huang, Jince Wang, Junhui Chen, and Yifei Xiao. \"Micn: Multi-scale local and global context modeling for long-term series forecasting.\" In The Eleventh International Conference on Learning Representations. 2022."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Please refer to the Strengths section in Summary."
                },
                "weaknesses": {
                    "value": "Please refer to the Weaknesses section in Summary."
                },
                "questions": {
                    "value": "Please refer to the Questions and Suggestions section in Summary."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6882/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6882/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6882/Reviewer_3Fmc"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6882/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698795164704,
            "cdate": 1698795164704,
            "tmdate": 1700743463505,
            "mdate": 1700743463505,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VzSHwGizlB",
                "forum": "hF8jnnexSB",
                "replyto": "ppzGwpc4ow",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6882/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6882/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3Fmc"
                    },
                    "comment": {
                        "value": "We appreciate that you found some strengths. Before responding point-by-point, we would like to provide some insights into the contribution of our work. In its initial publication, the DLinear model [1] faced widespread controversy due to its one-layer linear model structure. Despite this initial skepticism, the model ultimately achieved success and exerted a significant influence within the LTSF community. Based on this research, lots of variant models based on MLPs have emerged. However, our model structure is as simple as DLinear, while extensive experiments have consistently demonstrated that depth-wise convolutional units possess inherent advantages over Linear units of DLinear in LTSF. **We firmly believe that LTSF-Conv models serve as straightforward yet competitive basic units, exhibiting great potential for further expansion of complex network structures.**\n\nQ1 : *From the accuracy perspective, two recent CNN baseline models, TimesNet and MICN [1] are missing in Table 1 and Table 2. Moreover, the hyperparameter-searching is used, which makes the comparison a little bit unfair. For example, in TimesNet and MICN, the model configurations and lookback window remain the same for most of the experiments, and in PatchTST, only two configurations are considered. Based on the current results, it is hard for me to tell whether the performance gain is from the better model configuration or the proposed structure.*\n\n* Thank you for pointing out this. We have confidence in the fairness of our previous and future comparisons. First, **the experimental results not only include Conv-Best and DConv-Best but also the default look-back window length is set at 512, such as Conv and Dconv in Table 1 and Table 2 of the original manuscript.** This is using the same lookback window length as PatchTST (sl=512). Then, Transformer-based baselines have not benefited from a longer look-back window. The reason why most transformer models do not increase the lookback window length is because the performance will become worse. More analysis of the look-back windows is described in Section C.1, Page 12. in the original Appendix. Reference [1] has also proved it. \n* PatchTST can extend the lookback window to 512, but does not benefit from longer windows. It's important to note that this extension comes at the cost of a substantial increase in the model\u2019s computational resource requirements. On our server resources, **PatchTST ran out of GPU memory for a look-back window size greater than 720.** Reference [3] also conducted similar experiments, encountering the same issue of OOM. This is also a significant limitation of the PatchTST model.\n* Following the reviewer\u2019s suggestion, we switched to a higher-performance server to validate PatchTST with lookback-window size $\\in$ \\{720, 1600\\}. The results are in Table 3 of the attached supplementary material. **It can be found that not only the overall performance has not improved, but the training cost is almost unacceptable.**\n* Finally, We have added MICN and TimesNet baseline models. We adopt their official codes and only change the length of input sequences. **The results are summarized in Table 2 of the attached PDF file.**  In Table 2, we conducted an experiment within a broader window size range of \\{96, 336, 512, 720, 1600\\}. We consistently chose the most optimal results, thus establishing robust baselines. The results demonstrate that LTSF-Conv models consistently surpass all MICN and TimesNet on seven LTSF benchmarks.\n\nQ2 : *I would expect a more in-depth analysis of why it reaches better results than vanilla CNN.*\n\n* Thank you for pointing out this. In LTSF tasks, channel independence can enhance prediction performance compared to channel mixing. Previous research  [2] has also found it. For MLP-based variants, achieving channel-independent techniques typically involves multiple independent MLP sub-models, each responsible for handling a specific channel or feature within the time series. It comes at the cost of a substantial increase in the model\u2019s computational resource requirements. As a comparison, our model utilizes group-wise convolution to cleverly achieve channel independence, while concurrently reducing model complexity. This is because depthwise convolution exhibits higher efficiency than standard convolution. In the context of group-wise convolution, a critical consideration lies in aligning the number of channels with the variable dimension and the number of filters, as defined in the initial setup. However, standard CNN uses the idea of channel mixing, which suffers from noise interference among the channels and reduces performance. We have conducted additional ablation experiments about depth-wise CNN and general CNN. The results are in Section C.2, Page 13 of the original manuscript."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6882/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700326998017,
                "cdate": 1700326998017,
                "tmdate": 1700326998017,
                "mdate": 1700326998017,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]