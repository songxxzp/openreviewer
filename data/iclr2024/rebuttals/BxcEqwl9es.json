[
    {
        "title": "Microenvironment Probability Flows as Proficient Protein Engineers"
    },
    {
        "review": {
            "id": "OezHCfcmlG",
            "forum": "BxcEqwl9es",
            "replyto": "BxcEqwl9es",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3899/Reviewer_XuyZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3899/Reviewer_XuyZ"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript introduces an inverse folding method based on probability flow models, featuring data augmentation with sequences bearing high similarity to enhance prediction diversity. The manuscript also contends that leveraging the microenvironment during the inverse folding process yields performance comparable to the use of global structural information."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. Since only the microenvironment is utilized in the model, the proposed method offers significantly faster inference compared to existing methods while maintaining performance comparable to state-of-the-art methods.\n\n2. The experiments are comprehensive, assessing the performance of the proposed method from various perspectives."
                },
                "weaknesses": {
                    "value": "1. The pLDDT score is designed to evaluate the accuracy of structure predictions generated by AlphaFold. Using this metric to assess the authenticity of generated sequences and concluding that 'the model is likely to work in the real world' is logically inconsistent.\n\n2. There are come typos in the manuscript."
                },
                "questions": {
                    "value": "When constructing the microenvironment dataset, why was random sampling used to backfill up to 200 residues instead of employing distance measures for sampling?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3899/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698507672657,
            "cdate": 1698507672657,
            "tmdate": 1699636349754,
            "mdate": 1699636349754,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Nn8VvdlD7p",
                "forum": "BxcEqwl9es",
                "replyto": "OezHCfcmlG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3899/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3899/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XuyZ"
                    },
                    "comment": {
                        "value": "**The pLDDT score is designed to evaluate the accuracy of structure predictions generated by AlphaFold. Using this metric to assess the authenticity of generated sequences and concluding that 'the model is likely to work in the real world' is logically inconsistent.**\n\nThank you for your thoughtful comments. Our choice to utilize pLDDT and RMSE in assessing the generated sequences is motivated by two key considerations. Firstly, the decision is aligned with practices found in relevant literature, where one or both of these metrics have been commonly employed.\nSecondly, in the context of evaluating inverse folding or engineering models, researchers are particularly interested in ensuring the 'consistency' between structure-to-sequence and sequence-to-structure predictions. Given AlphaFold's proficiency in predicting protein structures, leveraging it to assess this consistency appears logical and is in line with the objectives of our study.  \n\n**There are come typos in the manuscript.**\n\nThanks for your comments. In the revision pdf, we have addressed typos mentioned by other reviewers.\n\n\n**When constructing the microenvironment dataset, why was random sampling used to backfill up to 200 residues instead of employing distance measures for sampling?**\n\nThank you for your insightful suggestions. We employed a 200 residues threshold to prevent oversampling in the context of long protein sequences. Considering your suggestion and the importance of maintaining a balanced dataset, we recognize the value of incorporating distance measures to achieve a more nuanced understanding of local chemical environments. We view this approach as a promising alternative for future work in our data engineering efforts."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700503216495,
                "cdate": 1700503216495,
                "tmdate": 1700503216495,
                "mdate": 1700503216495,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IjHsLikZ8p",
            "forum": "BxcEqwl9es",
            "replyto": "BxcEqwl9es",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3899/Reviewer_VmMV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3899/Reviewer_VmMV"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the Inverse Protein design problem, where the goal is to predict the amino acid sequence corresponding to a given 3D protein backbone. In contrast to previous approaches that rely on the entire protein backbone, this work introduces three key contributions. First, it employs a generative flow network to generate each amino acid based solely on the local environment information of its intended structural placement. During inference, their model only requires one step of integration resulting in a significant speed improvement. Additionally, through data augmentation, the model demonstrates the ability to provide a more diverse set of sequences compared to state-of-the-art methods. Finally, the paper showcases the unique capability of their generative flow network to perform zero-shot single-point mutation fitness prediction, a feat not easily achievable with other methods"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Originality:\nThe primary source of innovation lies in the utilization of rectified flow in conjunction with microenvironment data to enhance both speed and data diversity in the model. Of particular interest is the zero-shot experiment; however, it remains somewhat challenging to discern whether any additional modeling decisions influenced its success, or if it can be primarily attributed to the adoption of the rectified flow model.\n\nQuality:\nThe experiments conducted in this study are comprehensive and serve to highlight the method strengths: speed, data diversity, zero-shot stability prediction, and robustness when varying the number of integration/diffusion steps."
                },
                "weaknesses": {
                    "value": "Clarity:\nSubstantial effort is required to refine the content for improved clarity. For example, there is a need for a more detailed explanation of the classifier in Table 6.\n\nThis work involves the application of the rectified flow network to the domain of inverse protein design. This choice yields a unique capability in zero-shot stability prediction. However, when examining the overall performance and speed enhancements, they appear relatively minor when compared to certain state-of-the-art methods like PiFold.\n\nThe utilization of data augmentation contributes to increased sequence diversity, yet it is worth noting that this technique could be similarly applied to other methods to achieve comparable diversity benefits."
                },
                "questions": {
                    "value": "Suggestions:\nImprove the writing and submit the work to a more applied venue since the ML contribution is very limited."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3899/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698702399068,
            "cdate": 1698702399068,
            "tmdate": 1699636349674,
            "mdate": 1699636349674,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sIQDYPnNPN",
                "forum": "BxcEqwl9es",
                "replyto": "IjHsLikZ8p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3899/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3899/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VmMV"
                    },
                    "comment": {
                        "value": "**Clarity: Substantial effort is required to refine the content for improved clarity. For example, there is a need for a more detailed explanation of the classifier in Table 6.**\n\nThank you for your feedback. As we mentioned in experiment settings of section 4.2, this is a zero-shot classifier built with log(p_mut / p_wt), which is a common practice in literature (Lin et al., 2022; Notin et al., 2022).\n\n**This work involves the application of the rectified flow network to the domain of inverse protein design. This choice yields a unique capability in zero-shot stability prediction. However, when examining the overall performance and speed enhancements, they appear relatively minor when compared to certain state-of-the-art methods like PiFold.**\n\nThanks for your comments. \nFirst, It's important to note that the inverse folding problems inherently lack side chain information in the structure. Consequently, the achievable recovery ratio is inherently limited in this context. \nSecondly, we hold the view that the recovery ratio may not be an optimal metric in practical applications. This metric considers only one amino acid as the correct answer for each position. However, in protein engineering practice, multiple amino acids can be valid at a given position, aligning with the diversity inherent in the field. We are actively exploring ways to establish more appropriate metrics and benchmarks to comprehensively evaluate all published models. \n\n**The utilization of data augmentation contributes to increased sequence diversity, yet it is worth noting that this technique could be similarly applied to other methods to achieve comparable diversity benefits.**\n\nFrom our perspective, the data augmentation is better than other data augmentation methods, e.g. label smoothing, dropout, etc. The sequence alignment augmentation contains more biology information, and therefore, as demonstrated in Table 6, it improves the zero-shot regressor and classifier by a large margin. We further display the dropout baselines in the revision and show that it cannot contribute. While this method can be applied to the other prior arts. We propose it here so in the future other methods can use it to improve their performance. We will make the generalization of our data augmentation technique more clear in the writing. \n\n**Suggestions: Improve the writing and submit the work to a more applied venue since the ML contribution is very limited.**\n\nThank you for your feedback. We have carefully addressed the typos mentioned by other reviewers in the revised version of the document. For the machine learning contributions, from our perspective, in this work we make the following contributions:\n\nA Simplified Approach for Discrete Outputs: Our empirical findings demonstrate that for discrete outputs, the conventional design of discrete noise [1, 2] or the addition of Gaussian noise to the embedding space of discrete data [3] is unnecessary. We show that generating one-hot encoding without altering the algorithm is sufficient. This is an advantage given by the flow model that we demonstrate works in proteins.\n\nBiology-based Data Augmentation: We propose to do data augmentation with sequence alignment information. For the protein problems, validated experimental structures are limited (20k~30k). Therefore, data augmentation is an interesting problem and a contribution to the protein machine learning community.\n\nPerformance Improvement in Practice: In practical applications, we have observed performance enhancements, as detailed in the paper, that may be used by others in the protein machine learning community.\n\nTo enhance clarity, we will emphasize these key points in the manuscript, ensuring a more straightforward understanding of our contributions once we have the extra page.  We appreciate your attention to these aspects and believe that these refinements will strengthen the overall quality of our work.\n\n[1] Structured denoising diffusion models in discrete state-spaces\n\n[2] Argmax flows and multinomial diffusion: Learning categorical distributions\n\n[3] Diffuseq: Sequence to sequence text generation with diffusion models\n\n[4] Diffusion-LM improves controllable text generation."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700503173780,
                "cdate": 1700503173780,
                "tmdate": 1700513143487,
                "mdate": 1700513143487,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FKk11LQuos",
            "forum": "BxcEqwl9es",
            "replyto": "BxcEqwl9es",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3899/Reviewer_1cV9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3899/Reviewer_1cV9"
            ],
            "content": {
                "summary": {
                    "value": "The authors create a probability flow model to predict the amino acid from the 3D microenvironment around the given amino acid."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Generally, I think this is an interesting and tractible approach.\n\nI think the data augmentation process makes sense. I think the authors could likely be even more aggressive with this approach (more sequence diversity).\n\nI think the speed and efficiency of this approach is good.\n\nI think the comparison across multiple tasks is good to see where this method shines."
                },
                "weaknesses": {
                    "value": "As someone that has undergone an actual wet-lab protein engineering campaign, the tools presented here should be defined as annotation or sequence generation. There are not clear steps to move these entities out of in silico predictions.\n\nIt would be great to define \u201crecovery ratio\u201d before using it a number of times in the work. Ideally the reader doesn\u2019t have to hunt through a number of references to find its definition. This is also less familiar than the pLDDT or pTM scores.\n\n\u201cCompare with Classification Model: When we use the same architecture and input, but train a classifier instead of a generative model\u2026\u201d What is the classifier? What are you training it on? This should be clearly defined to the reader. Coupled with no definition of the \u201crecovery ratio\u201d, this section is very hard to follow.\n\nFor all the tables (including Table 1), it\u2019d be great to have up or down arrows if a \u201cgood\u201d metric should be large or small, respectively.\n\nThe cosine similarity results in Table 3 are difficult to make conclusions from. What are meaningful, statistically significant distances for cosine similarity? Each of these models have quite different representations and architectures, many of which haven\u2019t been designed for this task.\n\n\u201cExperiment Settings To further verify the ability of protein engineering, we apply zero-shot \u2206\u2206G prediction on FireProtDB\u2026.\u201d This is sequence annotation.\n\nThere are many different zero-shot mutation effect predictors, one of which is in the ESM family of models. I would like to see that baseline in section 4.2.\n\nNit: \u201cand\u201d is spelled wrong a number of times in the manuscript, sometimes as \u201cnad\u201d or \u201cadn\u201d."
                },
                "questions": {
                    "value": "\u201cAs demonstrated in Table 3, we evaluate the diversity of different method. To assess diversity, we first consider the relative Hamming distance since there is no insertion or deletion operation in the design space.\u201d - What about Levenshtein Distance? This is pretty common to deal with indels.\n\n\u201cHence, we propose that differences within this hidden space could represent a \u2018hidden\u2019 dimension for important concepts, such as MSA, and therefore applying cosine similarity to measure diveristy in the ESM2 latent space.\u201d I don\u2019t think I fully follow or understand here.\n\n\u201cwe apply log(pmut/pwt) as the zero-shot score to predict whether a mutation is good.\u201d What exactly do you mean by \u201cgood\u201d? I would prefer if the phrasing was \u201cif a mutation affects function\u201d.\n\n\u201cIn real protein design problems, a given backbone can come to multiple potential sequences, and our interest is to identify these candidate sequences.\u201d What does this mean? If this is a real problem biologists can encounter, can you provide and example or citation?\n\nI see in this work that ESM is used for creating embeddings and some of the other validations. Was this version of ESM blinded to CATH as well, as your splits have been done in model creation?\n\nIn the mutation effect prediction task, can this method predict the effect of multiple mutations?\n\nDoes sequence augmentation actually help with your method? If not, why include it?\n\nHow well does the \u201cClassification\u201d model in Table 1 compare in all the other experiments below?\n\nWhile I appreciate the number of metrics in Figure 3, I think it is a bit overkill. Also, the 1/rmse for Myolobin is off the chart."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3899/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698722905194,
            "cdate": 1698722905194,
            "tmdate": 1699636349459,
            "mdate": 1699636349459,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Wo6L5uOtLz",
                "forum": "BxcEqwl9es",
                "replyto": "FKk11LQuos",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3899/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3899/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 1cV9"
                    },
                    "comment": {
                        "value": "**As someone that has undergone an actual wet-lab protein engineering campaign, the tools presented here should be defined as annotation or sequence generation. There are not clear steps to move these entities out of in silico predictions.**\n\nThank you for your comments, we emphasize this in the revision appendix \u201cWe recognize the disparity between machine learning-based sequence generation and experimental validation of computationally generated protein sequences. To address your question, we first develop methods, iteratively improve them, and submit them to machine learning conferences with the intent of sharing our in silico experimental result with the machine learning community. Once we feel our methods cannot be improved further in silico, we collaborate with experimental protein engineers to experimentally validate our methods. However, these experimental collaboration projects get submitted to protein journals such as Nature, Cell, ACS, journals rather than machine learning conferences.\nWe will move the discussions to the main text once we have an extra page. \n\n**It would be great to define \u201crecovery ratio\u201d before using it a number of times in the work. Ideally the reader doesn\u2019t have to hunt through a number of references to find its definition. This is also less familiar than the pLDDT or pTM scores.**\n\nThank you for your comments, we highlight the recovery ratio refers to the wildtype accuracy in the revision.\n\n**\u201cCompare with Classification Model: When we use the same architecture and input, but train a classifier instead of a generative model\u2026\u201d What is the classifier? What are you training it on? This should be clearly defined to the reader. Coupled with no definition of the \u201crecovery ratio\u201d, this section is very hard to follow.**\n\n The classification model is trained using cross-entropy loss against the wildtype label and lacks adding random noise to the input. Throughout both training and inference, it takes the backbone micro-environment as input and outputs the corresponding wildtype amino acid class. In contrast, the flow model necessitates a linear combination of the target and random noise as input during training. During inference, it first starts with random noise and the flow process uses the backbone microenvironment to denoise it into the predicted amino acid class. Additional details addressing these distinctions have been incorporated in the revised version.\n\n**For all the tables (including Table 1), it\u2019d be great to have up or down arrows if a \u201cgood\u201d metric should be large or small, respectively.**\n\nThank you for your comments, we add these arrows in the updated version.\n\n**The cosine similarity results in Table 3 are difficult to make conclusions from. What are meaningful, statistically significant distances for cosine similarity? Each of these models have quite different representations and architectures, many of which haven\u2019t been designed for this task.**\n\nThank you for bringing this confusion to our attention. In Table 3, we generate sequences for 50 randomly selected proteins/backbones. For each of these backbone scaffolds, we run MPNN, PiFold, and MeFlow to generate 10 sequences, giving us a total of 500 sequences for each model. For the 10 sequences generated, we embed them with ESM2 and measure their cosine similarity in the ESM2 latent space. Since the sequences are being compared in solely ESM2 latent space, the cosine similarity of the sequence\u2019s embedding for a given scaffold provides a fair comparison and a meaningful metric. We do this for each of the 50 randomly selected scaffolds with MPNN, PiFold, and MeFlow separately.\nWe will clarify this in the Table caption and paper to make this more clear once we have extra page. \n\n**\u201cExperiment Settings To further verify the ability of protein engineering, we apply zero-shot \u2206\u2206G prediction on FireProtDB\u2026.\u201d This is sequence annotation.**\n\nThank you for your comment. Yes, \u2206G is sequence annotation and \u2206\u2206G is an annotation for a point mutation calculated by given \u2206G of two sequences. We use these annotations to evaluate how well our model correlates with them.  In our context, \u2206\u2206G represents the change free energy of folding based on a point mutation. \n\n\n\n**There are many different zero-shot mutation effect predictors, one of which is in the ESM family of models. I would like to see that baseline in section 4.2.**\n\nThank you for your comments, we list some supervised models and zero-shot models for the comparison. We present ESM-1b, ThermoNet and Stability Oracle in Table 6. ThermoNet is a well-known supervised trained ddG predictor, while Stability Oracle is a recently published SOTA model. We demonstrate that our zero-shot performance is even better than the supervised ThermoNet model.\n\n**Nit: \u201cand\u201d is spelled wrong a number of times in the manuscript, sometimes as \u201cnad\u201d or \u201cadn\u201d.**\n\nThank you for your comments, we fix these typos."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700503061042,
                "cdate": 1700503061042,
                "tmdate": 1700511164969,
                "mdate": 1700511164969,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qJihKaA53C",
                "forum": "BxcEqwl9es",
                "replyto": "FKk11LQuos",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3899/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3899/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reponse to Reviewer 1cV9 [2]"
                    },
                    "comment": {
                        "value": "**\u201cAs demonstrated in Table 3, we evaluate the diversity of different method. To assess diversity, we first consider the relative Hamming distance since there is no insertion or deletion operation in the design space.\u201d - What about Levenshtein Distance? This is pretty common to deal with indels.**\n\nThank you for your comments. In our problem set, the sequence length is fixed (no insertions and deletions), and the position in the sequence corresponds to the position in the structure. In this case, there is no distinction  between Levenshtein Distance and Hamming distance and they can be considered equal. \n\n**\u201cHence, we propose that differences within this hidden space could represent a \u2018hidden\u2019 dimension for important concepts, such as MSA, and therefore applying cosine similarity to measure diversity in the ESM2 latent space.\u201d I don\u2019t think I fully follow or understand here.**\n\nThanks for the comments, we simplify this to \u2018we measure the similarity with the pretrained ESM2 latent space\u2019. Our key insight is based on the fact that ESM2 replaced the need of explicit MSAs in ESMFold. This enabled us to hypothesize that its latent space must  encompass rich information about MSAs. \n\n**\u201cwe apply log(pmut/pwt) as the zero-shot score to predict whether a mutation is good.\u201d What exactly do you mean by \u201cgood\u201d? I would prefer if the phrasing was \u201cif a mutation affects function\u201d.**\n\nThanks for the comments, \u2018good\u2019 is confusing and we rewrite this to \u2018if a mutation can potentially improve a protein phenotype, such as stability (\u2206\u2206G)\u201d.\n\n**\u201cIn real protein design problems, a given backbone can come to multiple potential sequences, and our interest is to identify these candidate sequences.\u201d What does this mean? If this is a real problem biologists can encounter, can you provide and example or citation?**\n\nAs we mentioned in the third paragraph in the introduction section, It is quite common in nature for proteins that have a sequence similarity of at least \u223c30% to have a structurally similar scaffold (Stern, 2013; Rost, 1999). Once we go below 30% sequence similarity, this fact begins to break down and is known as the \u201ctwilight zone\u201d (Rost, 1999). This scaffold degeneracy is the underlying principle that drives the grouping of proteins into families and domains (e.g., Fox et al., 2014; Sillitoe et al., 2021; Paysan-Lafosse et al., 2023).\n\nWe refer the readers and the reviewers to all these related works we mentioned, for the background and related information. We will add a citation to this statement to help the reader. \n\n**I see in this work that ESM is used for creating embeddings and some of the other validations. Was this version of ESM blinded to CATH as well, as your splits have been done in model creation?**\n\nThanks for your valuable insight. To clarify, ESM is used as one way to measure the similarity in the hidden space. The ESM is trained on sequence similarity splitting while CATH is structure splitting, therefore, the ESM is not blind to the CATH sequences. We highlight this in the revision. However, we do not see this resulting in any issues since we are not training or evaluating the performance of ESM2.\n\n**In the mutation effect prediction task, can this method predict the effect of multiple mutations?**\n\nAs mentioned in the experiment settings of section 4.2, we construct zero-shot predictors based on the flow model. For the multiple mutations, we can get a zero-shot model by adding single-mutation effects as demonstrated in [1] and ESM-1b paper. However, this naive addition does not take into account epistatic interactions. To get an accurate multiple-mutation predictor, it\u2019s necessary to do supervised training with multiple mutation data, which is not the key focus of this paper.\n\n[1] Predicting a Protein\u2019s Stability under a Million Mutations\n\n**Does sequence augmentation actually help with your method? If not, why include it?**\n\nAs we mentioned in Table 6, the augmentation improves the zero-shot regressor performance by a large margin (for example, on FireProtDB, the spearman is improved from 0.30 to 0.41). Considering the literature, it even out-performs the supervised models, e.g., [1]. We list these numbers for further comparison in revision, and therefore we think our augmentation helps the model.\n\n[1]  Predicting changes in protein thermodynamic stability upon point mutation with deep 3D convolutional neural networks"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700503085957,
                "cdate": 1700503085957,
                "tmdate": 1700512439442,
                "mdate": 1700512439442,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NUgDccbvCM",
            "forum": "BxcEqwl9es",
            "replyto": "BxcEqwl9es",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3899/Reviewer_UWpB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3899/Reviewer_UWpB"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce MeFlow, a diffusion model for inverse design of protein sequences from local atomic environments. They contrast MeFlow with inverse folding methods like ProteinMPNN and ESM-IF, which condition on global backbone structure information to generate sequences. They show that models conditioned on microenvironments achieve similar results, and they propose a data augmentation strategy based on sequence similarity. They show results on CATH, FireProtDB, and a newly introduced Interface dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The work shows a complementary approach for inverse protein design that works about as well as inverse folding approaches that condition on global backbone structure. The method is evaluated in three different settings, going well beyond the usual evaluation for inverse design methods. Using sequence augmentation to improve diversity is an interesting idea."
                },
                "weaknesses": {
                    "value": "There are some basic issues of clarity that make it difficult to interpret the results, particularly for the newly introduced tasks. The central claim that conditional generation using microenvironments allows for designing more diverse sequences that recover a target backbone structure is not sufficiently supported. The sequence augmentation strategy is underexplored."
                },
                "questions": {
                    "value": "1. What atoms comprise the local environment? Are these all the backbone atoms? \n2. The authors claim that their overarching goal is to explore conditioning on local atomic environments rather than global backbone structure for inverse design; the usual sequence recovery and diversity metrics do not show much difference between these paradigms. Have the authors considered tasks more related to remote homology detection?\n3. Data augmentation via Uniref100 is interesting but underexplored. Do the authors have ideas or additional experiments that could improve the results using this style of sequence augmentation?\n4. What physical properties are used to characterize the atoms?\n5. What are the correlation coefficients in Fig 2b and c? Is this similar to other methods? What references or evidence suggest that achieving comparable pLDDT and pTM scores indicate that a model will \u201cwork in the real world\u201d? \n6. What baselines are relevant for Table 5?\n7. Is the Interface dataset publicly available?\n8. It may improve clarity to give names to the Losses in equations 4 and 7 to refer to them throughout the paper.\n9. Figure 3 is visually interesting but not particularly informative and largely redundant with Table 6."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3899/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698764171851,
            "cdate": 1698764171851,
            "tmdate": 1699636349364,
            "mdate": 1699636349364,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UzWTuoM0aw",
                "forum": "BxcEqwl9es",
                "replyto": "NUgDccbvCM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3899/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3899/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reponse to Reviewer UWpB"
                    },
                    "comment": {
                        "value": "**Weaknesses**\n\nThanks for your comments, here we explain our innovations. \nWe do not introduce new tasks. In Table 1, we compare recovery ratio which is commonly used in inverse folding models (e.g., PiFold). In Table 6, we compare zero-shot transferability, which is also commonly used in literature (e.g. ESM [1]).  The introduced Interface dataset is for better performance on the zero-shot task. \nWe do not agree with the point \u201cThe central claim that conditional generation using microenvironments allows for designing more diverse sequences that recover a target backbone structure is not sufficiently supported\u201d. In literature, researchers either compare recovery ratio (e.g., PiFold), or compare zero-shot single-mutation predictions (e.g., ESM). We do both settings, and demonstrate that our method is comparable or even better.\nFor Data Augmentation: We propose to do data augmentation with sequence alignment information. For the protein problems, validated experimental structures are limited (20k~30k). Therefore, data augmentation is an interesting problem and a contribution, and we do not notice related works and therefore do not set up more comparisons.\n[1] Language models enable zero-shot prediction of the effects of mutations on protein function\n\n\n**What atoms comprise the local environment? Are these all the backbone atoms?**\n\nThe microenvironment consists of the backbone atoms that are within 20\u00c5 of the Ca atom of the center residue. We will make edits in the language to make the definition of the inverse folding microenvironment more clear. We only task the backbone atoms to build the local chemical environment.\n\n**The authors claim that their overarching goal is to explore conditioning on local atomic environments rather than global backbone structure for inverse design; the usual sequence recovery and diversity metrics do not show much difference between these paradigms. Have the authors considered tasks more related to remote homology detection?**\n\nThanks for valuable comments. We think an inverse folding model can be used as a zero-shot remote homology detector. However we have yet to run these experiments ourselves. \n\n**Data augmentation via Uniref100 is interesting but underexplored. Do the authors have ideas or additional experiments that could improve the results using this style of sequence augmentation?**\n\nThanks for valuable comments. We think generating augmentations with sequence similarity is a new and not-explored idea. We know similar sequences can come to similar scaffolds and we plan to do more future works on this topic.\n\n**What physical properties are used to characterize the atoms?**\n\nAs we mentioned in the appendix, Our included physical properties contain two channels, the partial charges and the surface or core information. The physical properties contain categorized partial charge (negative, neutral and positive) and whether the atom is buried.\n\n**What are the correlation coefficients in Fig 2b and c? Is this similar to other methods? What references or evidence suggest that achieving comparable pLDDT and pTM scores indicate that a model will \u201cwork in the real world\u201d?** \n\nWe update the manuscript and mention this is the Pearson correlation. We also add comments \u201cIn terms of Pearson correlation, the difference between our method and PiFold is not significant\u201d in the updated pdf. We use pLDDT since ProteinMPNN [1] evaluate relative AlphaFold success rates in Figure 2. We notice this is not accurate and therefore remove this sentence \u201cwork in the real world\u201d in the updated pdf.\n\n[1] Robust deep learning\u2013based protein sequence design using ProteinMPNN\n\n**What baselines are relevant for Table 5?**\n\nThanks for your comments, we further provide a classification model baseline in the revision. We mainly do the comparison in table 6, which shows training on our dataset, we get a better zero-shot model.\n\n**Is the Interface dataset publicly available?**\n\nThe dataset is not publicly available yet, we will make the dataset we used available after acceptance.\n\n**It may improve clarity to give names to the Losses in equations 4 and 7 to refer to them throughout the paper.**\n\nThanks for your comments, we will modify this once we have more space for the main text.\n\n**Figure 3 is visually interesting but not particularly informative and largely redundant with Table 6.**\n\nIn Figure 3, we want to show more metrics to compare these two methods. Table 6 only shows two correlation metrics. Therefore, we do not think Figure 3 is redundant. A model with high correlation does not always refer to a model with high recall/precision."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502759367,
                "cdate": 1700502759367,
                "tmdate": 1700508309285,
                "mdate": 1700508309285,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wRu5HQQF31",
                "forum": "BxcEqwl9es",
                "replyto": "UzWTuoM0aw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3899/Reviewer_UWpB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3899/Reviewer_UWpB"
                ],
                "content": {
                    "comment": {
                        "value": "I have the read the author's response and appreciate their rebuttal. Along with some of the other reviewers, my concerns related to the clarity of the work and the claims remain unchanged."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580172792,
                "cdate": 1700580172792,
                "tmdate": 1700580172792,
                "mdate": 1700580172792,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]