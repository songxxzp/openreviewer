[
    {
        "title": "MaSS: Multi-attribute Selective Suppression for Utility-preserving Data Transformation from an Information-theoretic Perspective"
    },
    {
        "review": {
            "id": "Vdgq2lWcvG",
            "forum": "JAKcnjzQI3",
            "replyto": "JAKcnjzQI3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6515/Reviewer_CAg3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6515/Reviewer_CAg3"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a framework called MASS (Multi-Attribute Selective Suppression) for privacy-preserving data transformation that selectively suppresses sensitive attributes while preserving useful ones. The authors provide a formal definition of privacy protection and an information-theoretic perspective on the problem. They also present a data-driven approach that uses a combination of supervised and unsupervised learning to identify sensitive attributes and suppress them while preserving useful ones. The authors provide rigorous theoretical analyses and comprehensive experimental evaluations that demonstrate the effectiveness of their approach. The contributions of this paper include a formal definition of privacy protection, a data-driven framework for privacy-preserving data transformation, and a comprehensive evaluation of the proposed approach against several baseline methods using multiple datasets of varying modalities."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "S1. In terms of originality, the paper introduces a novel approach for protecting unannotated attributes in datasets. While previous works have focused on protecting annotated attributes or using heuristics, the paper proposes a data-driven learnable data transformation framework called MaSS (Multi-Attribute Selective Suppression) that can selectively suppress sensitive attributes while preserving other useful attributes, regardless of whether they are known in advance or explicitly annotated. This approach is unique and addresses a gap in the existing literature.\nS2. The quality of the paper is high, as it provides rigorous theoretical analyses of the operational bounds of the proposed framework. The authors derive mathematical formulations and provide proofs for the theorems presented in the paper [5, 7]. This demonstrates a strong understanding of the underlying principles and ensures the reliability of the proposed methods.\nS3. The clarity of the paper is commendable. The authors provide clear explanations of the problem formulation, the proposed techniques, and the evaluation methodology. The paper is well-structured, making it easy for readers to follow the flow of ideas. Additionally, the authors provide visualizations and tables to support their findings."
                },
                "weaknesses": {
                    "value": "W1. The dataset lacks a detailed description. It would also be good to have a table that describes the size of the dataset along with some other information that would give the reader a clearer picture of the dataset.\nW2. All six methods of experimental comparison rely on adversarially training a sensitive attribute inference model and lack the ability to compare state-of-the-art dp-based methods (e.g. \"Mingxuan Sun, Qing Wang, Zicheng Liu: Human Action Image Generation with Differential Privacy. ICME 2020: 1-6\").\nW3. Lack of comparison with a state-of-the-art method (\"Li M, Xu X, Fan H, et al. STPrivacy: Spatio-Temporal Privacy-Preserving Action Recognition[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 5106-5115.\")"
                },
                "questions": {
                    "value": "The authors need provide more detailed explanations and justifications for their proposed techniques. why contrastive learning is suitable for protecting unannotated attributes and how it ensures the predictability of annotated attributes? Additionally, the authors could consider providing more detailed explanations of the loss functions used in their method, such as the InfoNCE Contrastive Learning Loss."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6515/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6515/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6515/Reviewer_CAg3"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6515/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698654136490,
            "cdate": 1698654136490,
            "tmdate": 1699636732137,
            "mdate": 1699636732137,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jNcXUH1Z97",
                "forum": "JAKcnjzQI3",
                "replyto": "Vdgq2lWcvG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6515/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6515/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer CAg3 - W1, W2, W3"
                    },
                    "comment": {
                        "value": "**W1. The dataset lacks a detailed description...**\n\n**A1**. We thank the reviewer for pointing out this problem. The detailed descriptions of datasets, including size, classes, split, preprocessing, and other basic information, are included in Appendix B of the revised manuscript. \n\n**W2. All six methods of experimental comparison rely on adversarially training a sensitive attribute inference model and lack the ability to compare state-of-the-art dp-based methods...**\n\n**A2**. We thank the reviewer for pointing out this related work. However, as stated in Section 2 of our original manuscript, our method aims at providing Information-theoretic Privacy, instead of defending against Membership Inference Attacks as Differential Privacy (DP) does. These are two distinct privacy notions in the literature (Hsu et al., 2021). Thus our method is not comparable with DP-based works. Specifically, Differential Privacy for deep learning generally focuses on creating a randomized optimization mechanism, so that the distributions of learned model parameters are similar for adjacent subsets of the original dataset [Abadi et al. (2016), Zhang et al., (2018), Sun et al., (2020)]. For Information-theoretic Privacy, on the other hand, the goal is to learn a data transformation, so that the information of sensitive attributes contained in the transformed data can be constrained.\n\nReferences:\n\n- Hsiang Hsu, Natalia Martinez, Martin Bertran, Guillermo Sapiro, and Flavio P Calmon. A survey on statistical, information, and estimation\u2014theoretic views on privacy. IEEE BITS the Information Theory Magazine, 1(1):45\u201356, 2021\n- Abadi, Martin, et al. \"Deep learning with differential privacy.\" Proceedings of the 2016 ACM SIGSAC conference on computer and communications security. 2016.\n- Zhang, Xinyang, Shouling Ji, and Ting Wang. \"Differentially private releasing via deep generative model (technical report).\" arXiv preprint arXiv:1801.01594 (2018).\n- Sun, Mingxuan, Qing Wang, and Zicheng Liu. \"Human action image generation with differential privacy.\" 2020 IEEE International Conference on Multimedia and Expo (ICME). IEEE, 2020.\n\n**W3. Lack of comparison with a state-of-the-art method...**\n\n**A3**. We thank the reviewer for pointing out this related work. STPrivacy (Li et al., 2023) proposed an insightful framework for privacy-preserving action recognition (PPAR) for 3D video data, which combined sparsification (removing privacy-leaking action-irrelevant tubelets from ViT input) and anonymization (removing privacy information adversarially by maximizing CE loss). STPrivacy was specially tailored for video data and ViT model, overcoming the dynamic utility loss and privacy leakage of previous frame-based PPAR methods. However, this specialization also hindered its generalizability to other domains, which made it inapplicable to our settings. However, as a remedy, we compared our method with two baseline methods mentioned in the STPrivacy paper, namely PPDAR (Wu et al., 2020) (named VITA in the STPrivacy paper) and SPAct (Dave et al., 2022). The results of PPDAR is shown in Table 1, 2, 4, 5, 7 in our paper. The results of SPAct is shown in the Table 1 below. We can observe that MaSS achieves slightly lower NAG on digit compared to SPAct, but significantly lower NAG on all sensitive attributes (up to 5\\%), which shows that our method can achieve a better trade-off between suppression and preservation. However, please note that SPAct did not consider preserving unannotated useful attributes.\n\n**Table 1**: Comparison of the classification accuracy and NAG between MaSS and SPAct on AudioMNIST. We suppress gender, accent, age, id, while preserve digit as annotated useful attribute.\n\n| Method | gender ($\\downarrow$) | accent ($\\downarrow$) | age ($\\downarrow$) | ID ($\\downarrow$) | digit ($\\uparrow$) |\n|---|---|---|---|---|---|\n|No suppression| 0.9962 (1.0000) | 0.9843 (1.0000) | 0.9657 (1.0000) | 0.9808 (1.0000) | 0.9975 (1.0000) |\n|Guessing| 0.8000 (0.0000) | 0.6833 (0.0000) | 0.1667 (0.0000) | 0.0167 (0.0000) | 0.1000 (0.0000) |\n| SPAct | 0.8087 (0.0442) | 0.6833 (0.0001) | 0.1753 (0.0108) | 0.0707 (0.0560) | 0.9948 (0.9970) |\n| MaSS | 0.8000 (0.0000) | 0.6833 (0.0000) | 0.1662 (0.0000) | 0.0183 (0.0017) | 0.9933 (0.9953) |\n\nReferences: \n\n- Li, Ming, et al. \"STPrivacy: Spatio-Temporal Tubelet Sparsification and Anonymization for Privacy-preserving Action Recognition.\" arXiv preprint arXiv:2301.03046 (2023).\n\n- Wu, Zhenyu, et al. \"Privacy-preserving deep action recognition: An adversarial learning framework and a new dataset.\" IEEE Transactions on Pattern Analysis and Machine Intelligence 44.4 (2020): 2126-2139.\n\n- Dave, Ishan Rajendrakumar, Chen Chen, and Mubarak Shah. \"Spact: Self-supervised privacy preservation for action recognition.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6515/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714797989,
                "cdate": 1700714797989,
                "tmdate": 1700715177474,
                "mdate": 1700715177474,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "T9wntAdMjF",
                "forum": "JAKcnjzQI3",
                "replyto": "Vdgq2lWcvG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6515/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6515/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer CAg3 - Q"
                    },
                    "comment": {
                        "value": "**Q. The authors need provide more detailed explanations and justifications for their proposed techniques...**\n\n**A**. InfoNCE contrastive learning loss (Oord et al., 2018) is a classical contrastive learning loss, which learns useful representations of data by making the representations of positive samples (similar or related samples) closer while pushing the representations of negative samples further apart from the anchor. The sampling strategy in our framework is as follows. Suppose we have ${K+1}$ samples $ \\\\{ x _i \\\\}  _{i=1}^{K+1}$ in a mini-batch. We first pass them through the feature extractor $P _\\eta(F|X)$ and data transformation module to sample a batch of $ \\\\{ f _i \\\\}  _{i=1}^{K+1}$ and $ \\\\{ x' _i \\\\}  _{i=1}^{K+1}$ respectively. Then suppose we choose the $j$-th feature $f _j$ as the anchor. Then the corresponding $x' _j$ would be designated as a positive sample, and all other $x' _{i \\neq j}$ are  designated as negative samples. After sampling, we calculate the contrastive learning loss as Equation 14 in our paper. For training stability, in our implementation each of the $K+1$ features in a batch are used as an anchor once and then averaged. An analogous strategy was used when anchors are chosen from $x'$.\n\nInfoNCE Contrastive learning is suitable in our design because it is a proper approximation to $I(F;X')$ (and $I(X;X')$), so that it can be used to maximize $I(F;X')$ thus protect unannotated useful attributes in the transformed data. Contrastive learning loss may help with preserving annotated useful attributes as well. But the annotated useful attributes preserving module is taking the primary responsibility, by ensuring the predictability of useful attributes from the transformed data.\n\nBesides, contrastive learning loss also succeeds in that it does not assume the distribution of $X, X'$. It can be universally applied to images, language, sensor signals, etc, with few adjustments. Another important merit of contrastive learning is its advanced empirical performance. We conducted an ablation experiment where we replaced the contrastive learning loss with MSE reconstruction loss (denoted as MaSS-MSE). The results are shown in Table 2 and Table 3. MaSS-NF means MaSS trained without unannotated useful attributes preservation. We can observe that the NAG of digit is significantly lower without unannotated attribute preservation. We can also observe that MaSS achieves higher NAG on digit than MaSS-MSE, as well as lower NAG on other sensitive attributes, which demonstrates the strong empirical performance of contrastive learning loss compared to MSE reconstruction loss.\n\n**Table 2**: Comparison of the classification accuracy and NAG between MaSS and ablations on AudioMNIST. We suppress gender, accent, age, ID, while preserve digit as if an unannotated attribute.\n\n| Method | gender ($\\downarrow$) | accent ($\\downarrow$) | age ($\\downarrow$) | ID ($\\downarrow$) | digit ($\\uparrow$) |\n|---|---|---|---|---|---|\n| No suppression| 0.9962 (1.0000) | 0.9843 (1.0000) | 0.9657 (1.0000) | 0.9808 (1.0000) | 0.9975 (1.0000) |\n|Guessing| 0.8000 (0.0000) | 0.6833 (0.0000) | 0.1667 (0.0000) | 0.0167 (0.0000) | 0.1000 (0.0000) |\n| MaSS-NF | 0.8000 (0.0000) | 0.6833 (0.0001) | 0.1658 (0.0000) | 0.0152 (0.0000) | 0.2657 (0.1846) |\n| MaSS-MSE | 0.8002 (0.0008) | 0.6833 (0.0001) | 0.1683 (0.0020) | 0.0462 (0.0306) | 0.9542 (0.9517) |\n| MaSS | 0.8000 (0.0000) | 0.6833 (0.0000) | 0.1667 (0.0000) | 0.0195 (0.0029) | **0.9683 (0.9675)** |\n\n**Table 3**: Comparison of the classification accuracy and NAG between MaSS and ablations on MotionSense.  We suppress gender, ID, while preserve activity as if unannotated useful attribute.\n\n| Method | gender ($\\downarrow$) | ID ($\\downarrow$) | activity ($\\uparrow$) |\n|---|---|---|---|\n| No suppression| 0.9026 (1.0000) | 0.9817 (1.0000) | 0.9764 (1.0000) |\n|Guessing| 0.0533 (0.0000) | 0.5699 (0.0000) | 0.4663 (0.0000) |\n| MaSS-NF| 0.0508 (0.0000) | 0.5699 (0.0000) | 0.8374 (0.7275) |\n| MaSS-MSE | 0.0754 (0.0260) | 0.5734 (0.0085) | 0.8823 (0.8156) |\n| MaSS | 0.0555 (0.0026) | 0.5686 (0.0000) | **0.9242 (0.8977)** |\n\n\n\nReference:\n\n- Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6515/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700715116450,
                "cdate": 1700715116450,
                "tmdate": 1700715195019,
                "mdate": 1700715195019,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GPvmPubwHv",
            "forum": "JAKcnjzQI3",
            "replyto": "JAKcnjzQI3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6515/Reviewer_YVvf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6515/Reviewer_YVvf"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel approach, referred to as MASS (Multi-Attribute Selective Suppression), which addresses the challenge of privacy protection in the context of large-scale datasets used for machine learning. It introduces a formal information-theoretic definition for utility-preserving privacy protection and offers a data-driven, learnable data transformation framework. This framework enables the selective suppression of sensitive attributes while preserving other useful attributes, regardless of whether they are known in advance or explicitly annotated. The paper includes rigorous theoretical analyses of the operational bounds of the proposed framework and conducts extensive experimental evaluations across diverse modalities, such as facial images, voice audio clips, and motion sensor signals. The results demonstrate the effectiveness and generalizability of MASS across different tasks and configurations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. One of the notable strengths of this paper is the introduction of a formal information-theoretic definition for utility-preserving privacy protection. This theoretical foundation provides a solid framework for addressing privacy concerns in large-scale datasets, contributing to the theoretical underpinning of data privacy solutions.\n\n2. The proposal of a data-driven learnable data transformation framework is innovative. This approach allows for the selective suppression of sensitive attributes, enhancing privacy protection while preserving the utility of the data. \n\n3. The comprehensive experimental evaluations across various data modalities, including facial images, voice audio clips, and motion sensor signals, highlight the generalizability of the MASS framework. This breadth of experimentation underlines its versatility and applicability to a wide range of real-world scenarios."
                },
                "weaknesses": {
                    "value": "1. The paper introduces an interesting concept in Theorem 3.1, highlighting the importance of mutual information constraints 'm' and 'n' in the context of privacy and utility trade-offs. However, it is essential to note that the experiments lack a corresponding exploration of these constraints. These constraints likely play a pivotal role in balancing sensitive attribute accuracy and useful attribute accuracy. The reported experimental results suggest that MaSS might not simultaneously achieve the best performance for both types of attributes. Therefore, it is recommended to conduct experiments with varying constraints to gain a deeper understanding of their impact.\n\n2. The theoretical argument presented in Theorem 3.2 may raise some questions. Specifically, the relevance of unannotated useful attributes to the tasks is highlighted, which could vary across different scenarios. The paper should address this issue and provide an ablation study of the contrastive learning module to support the claims made in Theorem 3.2. This would provide stronger evidence and clarity regarding the relationship between learned attributes and sensitive attributes.\n\n3. Clarification is needed on how the positive and negative samples for the InfoNCE loss are determined. Figure 3 suggests that both positive and negative samples come from the transformed data X', but the paper should explain how these samples are chosen, given that the anchor sample is the original data X.\n\n4. Regarding the evaluation of sensitive attribute accuracy, it appears that the accuracy of the adversarial classifier is used. It is recommended to consider the approach of training a classifier from scratch on the transformed data, similar to the methodology employed for calculating useful attribute accuracy.\n\n5. To provide a more comprehensive assessment of the proposed method, the paper should include comparisons with recent baselines, such as SPAct (CVPR 2022 [1]).\n\n6. The topic of concept removal for generative models, while not a central focus, could be related to this paper's context. It is suggested to discuss concept removal in the related works section to provide a broader perspective on the field and to highlight the paper's contributions in relation to existing research.\n\n[1] Dave, Ishan Rajendrakumar, Chen Chen, and Mubarak Shah. \"Spact: Self-supervised privacy preservation for action recognition.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022."
                },
                "questions": {
                    "value": "Please see weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6515/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698796639606,
            "cdate": 1698796639606,
            "tmdate": 1699636732016,
            "mdate": 1699636732016,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xjufIFbyhW",
                "forum": "JAKcnjzQI3",
                "replyto": "GPvmPubwHv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6515/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6515/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer YVvf - Q1, Q2, Q3"
                    },
                    "comment": {
                        "value": "**Q1. The paper introduces an interesting concept in Theorem 3.1, highlighting the importance of mutual information constraints 'm' and 'n' in the context of privacy and utility trade-offs...**\n\n**A1**. We conducted experiments to show the effect of varying constraint, in which we took gender, accent, age and ID as sensitive attributes and digit as an annotated useful attribute on the AudioMNIST dataset. We fixed $m=0$ for gender, accent and age and $n=2.3$ for digit (its maximal value). Then we varied $m$ for ID from $0$ to $1.46$ (its maximal value). The results are shown in Table 1. We can observe that as $m$ increases, the constraint is gradually loosened, which results in the gradually increasing accuracy and NAG for ID.\n\n**Table 1**: Varying the suppression constraint $m$ on AudioMNIST. We suppress gender, accent, age, ID, while preserve digit as if an annotated useful attribute.\n\n| Method | gender ($\\downarrow$) | accent ($\\downarrow$) | age ($\\downarrow$) | ID ($\\downarrow$) | digit ($\\uparrow$) |\n|---|---|---|---|---|---|\n|No suppression| 0.9962 (1.0000) | 0.9843 (1.0000) | 0.9657 (1.0000) | 0.9808 (1.0000) | 0.9975 (1.0000) |\n|Guessing| 0.8000 (0.0000) | 0.6833 (0.0000) | 0.1667 (0.0000) | 0.0167 (0.0000) | 0.1000 (0.0000) |\n|MaSS $m_{id}$=0.0| 0.8000 (0.0000) | 0.6833 (0.0000) | 0.1662 (0.0000) | 0.0183 (0.0017) | 0.9933 (0.9953) |\n|MaSS $m_{id}$=0.3| 0.8000 (0.0000) | 0.6833 (0.0000) | 0.1665 (0.0000) | 0.0598 (0.0447) | 0.9938 (0.9959) |\n|MaSS $m_{id}$=0.6| 0.8000 (0.0000) | 0.6833 (0.0000) | 0.1668 (0.0002) | 0.1120 (0.0988) | 0.9940 (0.9961) |\n|MaSS $m_{id}$=0.9| 0.8000 (0.0000) | 0.6833 (0.0000) | 0.1670 (0.0004) | 0.1493 (0.1376) | 0.9937 (0.9957) |\n|MaSS $m_{id}$=1.2| 0.8000 (0.0000) | 0.6833 (0.0000) | 0.1667 (0.0000) | 0.1963 (0.1863) | 0.9928 (0.9948) |\n|MaSS $m_{id}$=1.46| 0.8000 (0.0000) | 0.6833 (0.0000) | 0.1667 (0.0000) | 0.2597 (0.2520) | 0.9937 (0.9957) |\n\n\n**Q2. The theoretical argument presented in Theorem 3.2 may raise some questions...**\n\n**A2**. Theorem 3.2 provided an operational upper bound for the learning objective $I(X';F)$, namely $I(X';F) \\leq H(X|S _i) + m _i$. The detailed proof can be found in Appendix A.2. The value of the upper bound  ($H(X|S _i) + m _i$) is independent from the parameters of our model, which means that the unannotated useful attributes preservation module can at most preserve $H(X|S _i) + m _i$ nats (Naperian bits) of information of $F$ in $X'$. \n\n\nBesides, $H(X|S _i) + m _i$ is indeed dependent on the dataset and the task. For an extreme example, in a dataset where $X$ is fully decided by $S _i$ ($H(X|S _i)=0$) and $m _i$ is set to 0, then we can have $I(X';F) \\leq 0$, which means we can not preserve any information of $F$ in $X'$. On the other hand, if $X$ is conditionally independent from $S _i$ ($H(X|S _i)=H(X)$), we can have $I(X';F) \\leq H(X) + m _i$, which is already guaranteed by Data Processing Inequity $I(X';F) \\leq I(X'; X) \\leq H(X)$. Therefore in this case, Theorem 3.2 no longer casts any additional constraint on $I(X';F)$.\n\n**Q3. Clarification is needed on how the positive and negative samples for the InfoNCE loss are determined...**\n\n**A3**. The sampling strategy in our framework is as follows: Suppose we have ${K+1}$ samples $ \\\\{ x _i \\\\}  _{i=1}^{K+1}$ in a mini-batch. We first pass them through the feature extractor $P _\\eta(F|X)$ and data transformation module to sample a batch of $ \\\\{ f _i \\\\}  _{i=1}^{K+1}$ and $ \\\\{ x' _i \\\\}  _{i=1}^{K+1}$ respectively. Then suppose we choose the $j$-th feature $f _j$ as the anchor. Then the corresponding $x' _j$ would be designated as the positive sample, and all other $x' _{i \\neq j}$ are  designated as negative samples.  For training stability, in our implementation each of $K+1$ features in a batch are used as an anchor once and then averaged. An analogous strategy was used when anchors are chosen from $x'$."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6515/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714326845,
                "cdate": 1700714326845,
                "tmdate": 1700715240475,
                "mdate": 1700715240475,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yDxOLxEyTi",
            "forum": "JAKcnjzQI3",
            "replyto": "JAKcnjzQI3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6515/Reviewer_AsFx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6515/Reviewer_AsFx"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an information theorem based multi-attribute selective suppression (MaSS) to solve the problem of highlighting the utility attributes while suppressing the private attributes. The introduced problem is interesting and important, as privacy becomes a central concern for many applications. The paper presents a clear pipeline leveraging three effort stream lines: (1) sensitive attribute suppression, (2) annotated useful attribute preservation and (3) unannotated useful attribute preservation. The optimization are then jointly optimized."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper study into an interesting and practical problem by discussing the limitations and compare to the literature approaches, providing a sufficient background for the problem study.\n\n2. The paper provides a theoretical analysis from the information theory perspective, showing the relationship between utility and sensitive attributes.\n\n3. The paper presents a clear and tractable learning scheme to achieve the three stream lines.\n\n4. There are extensive experimental comparison against the representative literature methods and some state-of-the-arts. Consistently advantageous results demonstrate the method\u2019s effectiveness."
                },
                "weaknesses": {
                    "value": "1. For the sensitive attribute suppression, the objective is to minimize the the expectation of the entropy between P(Si|x) and P_phi(Si|X\u2019). Drawing connection to adversarial learning, it tries to push P_phi(Si|x\u2019) close to P(Si|x) by pushing the discriminator cannot tell the difference between the two.\n\nFirstly, the paper lacks the interpretation of their proposed method, and drawing the connection to the literature method, e.g., adversarial learning. It would be good the authors can conduct an in-depth analysis comparing the literature to the proposal in this paper, and further highlight the method\u2019s novelty.\n\n2. For unannotated useful attribute, depending on the definition of the problem, the setting will be different from the other method. In the paper of experiments, the authors mention \u201cALR, BDQ and PPDAR overlook the preservation of unannotated useful attributes\u201d.\n\nIt could be that those methods, from their problem definition and setting, they do not consider so termed \u201cunannotated useful attributes\u201d into their framework. But one cannot say it is the limitation or fault of those methods. In the most fair way, because of setting difference, this paper should compare to only those considering \u201cunannotated useful attributes\u201d. Please carefully phrase the comparison to other methods.\n\n3. Still, for those methods that are sharing exactly the same setting, e.g., GAP and MSDA, from technical frame design, what is the difference? I noticed there is some slight comparison, e.g., arguing that some of the methods lack theoretical analysis. This is the advantage of this paper. But other than that, if there is an empirical design that is exactly the same as this paper, this paper will only go for the theoretical contribution.\n\nThus, please provide a towards thorough comparison to those literature under the same setting, which will be helpful to claim the method contribution and novelty."
                },
                "questions": {
                    "value": "Please refer to weakness session for detail."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6515/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6515/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6515/Reviewer_AsFx"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6515/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699224773356,
            "cdate": 1699224773356,
            "tmdate": 1699636731912,
            "mdate": 1699636731912,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "43vGR7lXp9",
                "forum": "JAKcnjzQI3",
                "replyto": "yDxOLxEyTi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6515/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6515/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer AsFx - Q1, Q2, Q3"
                    },
                    "comment": {
                        "value": "**Q1. For the sensitive attribute suppression, the objective is to minimize the the expectation of the entropy between P(Si|x) and P_phi(Si|X\u2019)...**\n\n**A1**. The relationship between our sensitive attributes suppression and adversarial learning is as follows:\n\nFirst, our sensitive attributes suppression can be viewed as one type of adversarial learning. When we use linear relaxation and $I(X'; S _i) \\geq m _i$, it can be derived from Equation 6 and Equation 8 that our sensitive attributes inference model $\\phi _i$ is trained **adversarially** with the data transformation module by solving a min-max optimization as:\n\n$ \\max _\\theta \\min _{\\phi _i} \\mathbb{E} _{P(X)P _\\theta(X'|X)} [H(P(S _i|X),P _{\\phi _i}(S _i|X'))], $\n\nwhere $\\phi _i$ is trained to narrow the gap between $P(S _i|X)$ and $P _{\\phi _i}(S _i|X')$, while $\\theta$ is trained to widen the gap between $P(S _i|X)$ and $P _{\\phi _i}(S _i|X')$.\n\nHowever, the objective of our min-max optimization is different from other adversarial learning methods. In adversarial training for adversarial robustness, the objective is the cross entropy between prediction and groun truth (Bai et al., 2021). In classical GAN (Goodfellow et al., 2014), the objective is the cross entropy loss of differentiating real and fake samples; in WGAN (Arjovski et al., 2017), the objective is the estimated Wasserstein distance between real and fake data distributions. \n\n\nReferences:\n\n- Bai, Tao, et al. \"Recent advances in adversarial training for adversarial robustness.\" arXiv preprint arXiv:2102.01356 (2021).\n- Goodfellow, Ian, et al. \"Generative adversarial nets.\" Advances in neural information processing systems 27 (2014).\n- Arjovsky, Martin, Soumith Chintala, and L\u00e9on Bottou. \"Wasserstein generative adversarial networks.\" International conference on machine learning. PMLR, 2017.\n\n\n\n**Q2. For unannotated useful attribute, depending on the definition of the problem, the setting will be different from the other method...**\n\n**A2**. We thank the reviewer for pointing out this problem. We have revised the discussion in our revised manuscript to establish a fair and clear view on all baselines. For example, we changed the original term \"overlook\" to \"do not consider\". \n\n**Q3. Still, for those methods that are sharing exactly the same setting, e.g., GAP and MSDA, from technical frame design, what is the difference?..**\n\n**A3**. We thank the reviewer for the suggestion. The key difference of technical design between our method and GAP, MSDA is that, both GAP and MSDA utilize a heuristic MSE reconstruction loss for preserving unannotated useful attributes, which has inferior empirical performance, and can not be readily generalized to other datasets (e.g. language, etc.). On the other hand, the contrastive learning loss used in our method does not assume the distribution of $X, X'$, and therefore can be universally applied to images, language, sensor signals, etc, with few adjustments to yield good performance. \n\n\nReferences:\n\n- Huang, Chong, et al. \"Generative adversarial privacy.\" arXiv preprint arXiv:1807.05306 (2018).\n\n- Malekzadeh, Mohammad, et al. \"Mobile sensor data anonymization.\" Proceedings of the international conference on internet of things design and implementation. 2019."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6515/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713881274,
                "cdate": 1700713881274,
                "tmdate": 1700714047363,
                "mdate": 1700714047363,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Cf3ekbRgVL",
            "forum": "JAKcnjzQI3",
            "replyto": "JAKcnjzQI3",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6515/Reviewer_wuPW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6515/Reviewer_wuPW"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method for learning censored data transformations providing guarantees on the quantity of information about both annotated and unannotated features preserved by said transformations. The authors motivate the method using an information-theoretic calculus and establish operational bounds on the entailed objective. Practically, censoring of the designated sensitive attributes is achieved through a standard (margin-based) adversarial information-minimisation (infomin) procedure; useful annotated and unannotated information is preserved through the use of supervised and contrastive learning, respectively. The authors conduct experiments on datasets covering a range of modalities in AudioMNIST, Motion Sense, and Adience and demonstrate favourable performance of their method relative to the baseline suite."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Figures and tables are well-put-together; Figures 1 and 2 illustrate the problem setup and methodological pipeline, respectively, in an easily digestible manner -- one can understand the essence of the method based on its illustration alone.\n- Experiments cover a good range of datasets and configurations.\n- Proofs and implications of the consequent theoretical statements are easy to follow.\n- The problem under consideration is well-motivated and clearly formulated.\n- Reasonable assortment of baseline methods and strong empirical performance of the proposed method relative to these. Experimental setups are described with clarity.\n- Good contextualization w.r.t. prior work, with clear delineation of the subtle but differentiating qualities of the current work."
                },
                "weaknesses": {
                    "value": "- The paper is limited in terms of novelty. The main contribution of the paper seems to be in its proposal to preserve of *unannotated* features using a self-supervised-learning objective yet this idea of maximising $\\mathcal{I}(X; \\tilde{X})$, understanding $\\tilde{X}$ to be some representation generally, is certainly not novel in and of itself (vide Madras et al., 2018 in the context of the adjacent field of fair-representation learning); the method chosen used to accomplish this maximisation seems, to me, largely incidental -- one can simply view the contrastive loss as an alternative reconstruction loss.\n- The paper proposes learning a data transformation instead of a representation but the codomain of the transformation is another seemingly incidental factor given that interpretability does not appear to be a major concern, based on the narrative and analysis; indeed, in order to compute the contrastive learning objective in a space in which distances are meaningful, the transformed and original samples ultimately have to be embedded in such a representation anyway. The learning of a data transformation instead of a low-dimensional representation leads to a method that is more complicated than seemingly need be, and, moreover, is a design choice that comes at a steep computational cost -- computing all losses in representation space would be much more efficient though there may be some sound theoretical barrier to do doing so.\n- The mathematical formalism is confusing and, at times, unrigorous. Random variables and their realisations are seemingly conflated without reference to the abuse: mutual information, $\\mathcal{I}(\\cdot; \\cdot)$ is defined between pairs of random variables, not their empirical counterparts. While there is an argument to be made that such overloading is conventional and remedied by the context, I don't think that the latter is entirely satisifed here, especially with their being no express mention of this overloading being adopted throughout the paper.\n- While their meaning, as analogues of $X_p$ and $X_n$, respectively, can be easily inferred, $F_p$ and $F_n$, appearing in Eq.17, seem to be missing explicit definitions. The explanation given in Sec. 4.4,both textually and notationally, is generally muddled considering that the method amounts to SimCLR with the original and transformed samples acting as anchors and positive pairs.\n- Why compute cross-entropy terms w.r.t. the estimates of $P(U_i|X)$ and $P(S_i|X)$ as opposed to simply using the (degenerate ground-truth distribution (the annotations) used in the fitting of those estimates? There may be good reason for it but there should clear explanation given for why this choice is unprincipled, should that indeed be the case.\n- The quality of the writing, in terms of clarity and structure, could generally do with improvement.\n- Lack of ablation studies, such as those investigating the influence of the loss prefactor.\n- No discussion of the practical challenges entailed by adversarial infomin (vide Song and Shmatikov, 2021, for instance).\n\n\n### References\nMadras D, Creager E, Pitassi T, Zemel R. Learning adversarially fair and transferable representations. In International Conference on Machine Learning 2018 Jul 3 (pp. 3384-3393). PMLR.\n\nSong C, Shmatikov V. Overlearning Reveals Sensitive Attributes. In8th International Conference on Learning Representations, ICLR 2020 2020 Jan."
                },
                "questions": {
                    "value": "See Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6515/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699544131038,
            "cdate": 1699544131038,
            "tmdate": 1699636731772,
            "mdate": 1699636731772,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Zzb1B3jaoX",
                "forum": "JAKcnjzQI3",
                "replyto": "Cf3ekbRgVL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6515/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6515/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wuPW - Q1"
                    },
                    "comment": {
                        "value": "**Q1. The paper is limited in terms of novelty...**\n\n**A1**. Our key novelty lies in our proposal of utility preservation, especially for the unannotated attributes from an **information theoretic** perspective, as opposed to purely **heuristic-driven** methods. \n\nAlthough similar ideas of preserving generic features and maximizing $I(X;X')$ can be found in the literature, these existing approaches have noticeable limitation in that they only preserve generic features by minimizing a heuristic reconstruction loss, which is limited in that, 1) it is oftentimes not general enough to be applied in different datasets as they might require different ways to measure the reconstruction loss, for example, MSE for image and Cross-entropy for language; and 2) its performance is not supported by information theoretic analysis. \n\nFor example, Madras et al. (2018) and Edwards et al. (2015) based their fair representation learning methods on an MSE reconstruction loss. Although they provided insightful and sound analyses on the fairness of the learned representation, they did not embed their reconstruction loss into a theoretical framework. In addition, Huang et al. (2019) and Malekzadeh et al. (2019) targeted similar tasks as ours, but resorted to a heuristic MSE reconstruction loss. On the contrary, in our work we proposed to approach the preservation of the generic features from an information theoretic point of view, and accordingly derived a contrastive learning-based framework. \n\nContrastive learning plays a vital role in our design because InfoNCE contrastive learning loss is not only a proper \"reconstruction loss\", but also a proper approximation to $I(F;X')$ (and $I(X;X')$), which is consistent with our problem definition. Besides, contrastive learning loss does not assume the distribution of $X, X'$. It can be universally applied to images, language, sensor signals, etc, with few adjustments. Another important merit of contrastive learning is its advanced empirical performance. We conducted an ablation experiment where we replaced the contrastive learning loss with MSE reconstruction loss (denoted as MaSS-MSE). The results are shown in Table 1 and Table 2. MaSS-NF means MaSS trained without unannotated useful attributes preservation. We can observe that the NAG (normalized accuracy gain) of digit is significantly lower without unannotated attribute preservation. We can also observe that MaSS achieved higher NAG on digit than MaSS-MSE, as well as lower NAG on other sensitive attributes, which demonstrate the strong empirical performance of contrastive learning loss compared to MSE reconstruction loss.\n\n**Table 1**: Comparison of the classification accuracy and NAG between MaSS and ablations on AudioMNIST. We suppress gender, accent, age, ID, while preserve digit as if an unannotated attribute.\n\n| Method | gender ($\\downarrow$) | accent ($\\downarrow$) | age ($\\downarrow$) | ID ($\\downarrow$) | digit ($\\uparrow$) |\n|---|---|---|---|---|---|\n| No suppression| 0.9962 (1.0000) | 0.9843 (1.0000) | 0.9657 (1.0000) | 0.9808 (1.0000) | 0.9975 (1.0000) |\n|Guessing| 0.8000 (0.0000) | 0.6833 (0.0000) | 0.1667 (0.0000) | 0.0167 (0.0000) | 0.1000 (0.0000) |\n| MaSS-NF | 0.8000 (0.0000) | 0.6833 (0.0001) | 0.1658 (0.0000) | 0.0152 (0.0000) | 0.2657 (0.1846) |\n| MaSS-MSE | 0.8002 (0.0008) | 0.6833 (0.0001) | 0.1683 (0.0020) | 0.0462 (0.0306) | 0.9542 (0.9517) |\n| MaSS | 0.8000 (0.0000) | 0.6833 (0.0000) | 0.1667 (0.0000) | 0.0195 (0.0029) | **0.9683 (0.9675)** |\n\n**Table 2**: Comparison of the classification accuracy and NAG between MaSS and ablations on MotionSense.  We suppress gender, ID, while preserve activity as if unannotated useful attribute.\n\n| Method | gender ($\\downarrow$) | ID ($\\downarrow$) | activity ($\\uparrow$) |\n|---|---|---|---|\n| No suppression| 0.9026 (1.0000) | 0.9817 (1.0000) | 0.9764 (1.0000) |\n|Guessing| 0.0533 (0.0000) | 0.5699 (0.0000) | 0.4663 (0.0000) |\n| MaSS-NF| 0.0508 (0.0000) | 0.5699 (0.0000) | 0.8374 (0.7275) |\n| MaSS-MSE | 0.0754 (0.0260) | 0.5734 (0.0085) | 0.8823 (0.8156) |\n| MaSS | 0.0555 (0.0026) | 0.5686 (0.0000) | **0.9242 (0.8977)** |\n\nReferences:\n\n- Madras, David, et al. \"Learning adversarially fair and transferable representations.\" International Conference on Machine Learning. PMLR, 2018.\n- Edwards, Harrison, and Amos Storkey. \"Censoring representations with an adversary.\" arXiv preprint arXiv:1511.05897 (2015).\n- Huang, Chong, et al. \"Generative adversarial privacy.\" arXiv preprint arXiv:1807.05306 (2018).\n- Malekzadeh, Mohammad, et al. \"Mobile sensor data anonymization.\" Proceedings of the international conference on internet of things design and implementation. 2019."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6515/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712801670,
                "cdate": 1700712801670,
                "tmdate": 1700715301538,
                "mdate": 1700715301538,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Knw1Poj3PE",
                "forum": "JAKcnjzQI3",
                "replyto": "Cf3ekbRgVL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6515/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6515/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wuPW - Q2, Q3, Q4, Q5"
                    },
                    "comment": {
                        "value": "**Q2. The paper proposes learning a data transformation instead of a representation but the codomain of the transformation is another seemingly incidental factor given that interpretability does not appear to be a major concern...**\n\n**A2**. This paper is targeted at learning a data transformation instead of a compact representation for the following reasons:\n\nFirst, transformed data can deliver better **reusability for the community**. For example, users can utilize the transformed data for training generative models. For another case, users can develop their own possibly more generalizable feature extractors on the transformed data. Data reusability is crucial for the proliferation of Machine Learning community. \n\nSecond, transformed data can be readily used by **pre-trained off-the-shelf models**, whereas compact representations can only be used by models specially trained for them. To empirically support this claim, we ran an experiment of facial landmark detection on both the original and the transformed Adience datasets based on torchlm (https://github.com/DefTruth/torchlm), with the PIPNet (Jin et al., 2021) with ResNet18 backbone. This model generates 98 landmarks for each facial image. We then took the landmarks detected from the original images as the ground truth and evaluated the  landmarks detected from the transformed images, using normalized mean error (NME) (Jin et al., 2021) as the quantitative metric. The results are shown in Table 3. We can observe that the NME between the transformed Adience and the original Adience is comparable to the NME between the original WLFW dataset (Wu et al., 2018) and the ground truth label, which suggests that the transformed dataset can be accurately exploited by a pre-trained model. \n\n**Table 3**: The NME(\\%) of PIPNet between transformed Adience and original Adience, in comparison to the NME(\\%) of PIPNet between original WLFW dataset and ground truth label. The comparable performance showed that transformed Adience dataset can be accurately exploited by pre-trained PIPNet. On the contrary, compact representations would not be compatible with the pre-trained PIPNet.\n\n| Dataset | WLFW (original vs. label) | Adience (transformed vs. original) |\n|---|---|---|\n| NME $(\\downarrow)$  |3.94|3.30|\n\nReferences:\n\n- Haibo Jin, Shengcai Liao, and Ling Shao. 2021. Pixel-in-Pixel Net: Towards Efficient Facial Landmark Detection in the Wild. Int. J. Comput. Vision 129, 12 (Dec 2021), 3174\u20133194. https://doi.org/10.1007/s11263-021-01521-4\n- Wayne Wu, Chen Qian, Shuo Yang, Quan Wang, Yici Cai, Qiang Zhou. 2018 CVPR. Look at Boundary: A Boundary-Aware Face Alignment Algorithm\n\n\n**Q3. The mathematical formalism is confusing and, at times, unrigorous...**\n\n**A3**. We thank the reviewer for pointing out these problems. We adjusted the notations in our revised paper. Specifically, we use uppercase (e.g. $X,F$) for random variables and lowercase (e.g. $x,f$) for their realizations. We also changed the original function name $f$ to $\\mathcal{F}$ to correct the notation collision.\n\n**Q4. While their meaning, as analogues of $X _p$ and $X _n$, respectively, can be easily inferred...**\n\n**A4**.  We thank the reviewer for pointing out these problems. We included the definitions of $f _p$ and $f _n$ in the revised paper.\n\n**Q5. Why compute cross-entropy terms w.r.t. the estimates of $P(U _j|X)$ and $P(S _i|X)$ as opposed to simply using the (degenerate ground-truth distribution (the annotations) used in the fitting of those estimates?...**\n\n**A5**. We would like to point out that we did already use this degenerate distribution to compute cross entropy in our original manuscript, as stated in Section 3, Section 4.2 and Section 4.3. We updated the description in the revision for improved clarity."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6515/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713288174,
                "cdate": 1700713288174,
                "tmdate": 1700715286194,
                "mdate": 1700715286194,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Vbi4zjzwaT",
                "forum": "JAKcnjzQI3",
                "replyto": "Cf3ekbRgVL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6515/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6515/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wuPW - Q6, Q7, Q8"
                    },
                    "comment": {
                        "value": "**Q6. The quality of the writing, in terms of clarity and structure, could generally do with improvement.**\n\n**A6**. We thank the reviewer for the suggestion. We have updated our manuscript to improve its clarity and structure.\n\n**Q7. Lack of ablation studies, such as those investigating the influence of the loss prefactor.**\n\n**A7**. Apart from the ablation we provided in Table 1 and 2 examining the unannotated attributes preservation module, we also conducted experiments to show the effect of varying the constraint on sensitive attributes suppression ($m$). We took gender, accent, age and ID as sensitive attributes and digit as an annotated useful attribute on the AudioMNIST dataset. We fixed $m=0$ for gender, accent and age and $n=2.3$ for digit (its maximal value). Then we varied $m$ for ID from $0$ to $1.46$ (its maximal value). The results are shown in Table 4. We can observe that as $m$ increases, the constraint is gradually loosened, which results in the gradually increasing accuracy and NAG for ID.\n\n**Table 4**: Varying the suppression constraint $m$ on AudioMNIST. We suppress gender, accent, age, ID, while preserve digit as if an annotated useful attribute.\n\n| Method | gender ($\\downarrow$) | accent ($\\downarrow$) | age ($\\downarrow$) | ID ($\\downarrow$) | digit ($\\uparrow$) |\n|---|---|---|---|---|---|\n|No suppression| 0.9962 (1.0000) | 0.9843 (1.0000) | 0.9657 (1.0000) | 0.9808 (1.0000) | 0.9975 (1.0000) |\n|Guessing| 0.8000 (0.0000) | 0.6833 (0.0000) | 0.1667 (0.0000) | 0.0167 (0.0000) | 0.1000 (0.0000) |\n|MaSS $m_{id}$=0.0| 0.8000 (0.0000) | 0.6833 (0.0000) | 0.1662 (0.0000) | 0.0183 (0.0017) | 0.9933 (0.9953) |\n|MaSS $m_{id}$=0.3| 0.8000 (0.0000) | 0.6833 (0.0000) | 0.1665 (0.0000) | 0.0598 (0.0447) | 0.9938 (0.9959) |\n|MaSS $m_{id}$=0.6| 0.8000 (0.0000) | 0.6833 (0.0000) | 0.1668 (0.0002) | 0.1120 (0.0988) | 0.9940 (0.9961) |\n|MaSS $m_{id}$=0.9| 0.8000 (0.0000) | 0.6833 (0.0000) | 0.1670 (0.0004) | 0.1493 (0.1376) | 0.9937 (0.9957) |\n|MaSS $m_{id}$=1.2| 0.8000 (0.0000) | 0.6833 (0.0000) | 0.1667 (0.0000) | 0.1963 (0.1863) | 0.9928 (0.9948) |\n|MaSS $m_{id}$=1.46| 0.8000 (0.0000) | 0.6833 (0.0000) | 0.1667 (0.0000) | 0.2597 (0.2520) | 0.9937 (0.9957) |\n\n\n**Q8. No discussion of the practical challenges entailed by adversarial infomin (vide Song and Shmatikov, 2021, for instance).**\n\n**A8**. We thank the reviewer for pointing out this related work. For both de-censoring and re-purposing mentioned in Song et al. (2020), they require the access to the trained black box feature extractor (or, equivalently, the data transformation module in our case). However, the objective of our work is to release the transformed data in order to unleash its utility without the risk of leaking its sensitive attributes, whereas the transformation module itself will not be released. \n\nReference: \n\n- Song, Congzheng, and Vitaly Shmatikov. \"Overlearning reveals sensitive attributes.\" arXiv preprint arXiv:1905.11742 (2019)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6515/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700713577617,
                "cdate": 1700713577617,
                "tmdate": 1700715269485,
                "mdate": 1700715269485,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]