[
    {
        "title": "Statistical Rejection Sampling Improves Preference Optimization"
    },
    {
        "review": {
            "id": "j6lAIhNV3A",
            "forum": "xbjSwwrQOe",
            "replyto": "xbjSwwrQOe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1925/Reviewer_nTnq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1925/Reviewer_nTnq"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces an innovative method, Statistical Rejection Sampling Optimization (RSO), for RLHF. RSO employs rejection sampling to extract preference data from the optimal target policy, facilitating a more precise estimation of this policy. Additionally, this paper provides a unified framework for previous approaches, i.e., SLiC and DPO. Experimental outcomes indicate that RSO consistently surpasses existing methods, as evaluated by both large language models and human raters."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Presents a novel approach for RLHF using rejection sampling.\n2. Unifies prior work, including SLiC and DPO, within a comprehensive review.\n3. Provides comprehensive and robust experiments with SOTA, demonstrating clear enhancements of RSO."
                },
                "weaknesses": {
                    "value": "1. The unifying link between DPO and SLiC appears to be deliberately designed, particularly noticeable in the normalization term, i.e., $\\pi_{sft}(y|x)$, in Eq 10.\n2. The theoretical conclusion that the policy induced from RSO in Eq 4 is optimal seems questionable. RSO continues to use the proxy reward model to label responses, regardless of whether they're from the pre-trained or learned policy. However, the reward model is learned from $D_p$ and not $D_p^*$ without further updates, which could introduce approximation errors and bias in subsequent labeling and learning steps. In the other word, the policy induced from Eq 4 is not optimal because of the non-optimal reward model.\n3. Works with a similar focus on rejection sampling, such as ReST(http://arxiv.org/abs/2308.08998), ALMoST (https://arxiv.org/abs/2305.13735) RAFT (https://arxiv.org/abs/2304.06767), should be involved in comparison."
                },
                "questions": {
                    "value": "Why do not directly sample responses from $\\pi_{r_{\\psi}}(y|x)$ (with a reward threshold like ReST) rather than sampling from $\\pi_{sft}(y|x)$ with the statistical rejection sampling to approximate the samples from  $\\pi_{r_{\\psi}}(y|x)$?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1925/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1925/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1925/Reviewer_nTnq"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1925/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698460766852,
            "cdate": 1698460766852,
            "tmdate": 1700620546217,
            "mdate": 1700620546217,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hM0gzbduag",
                "forum": "xbjSwwrQOe",
                "replyto": "j6lAIhNV3A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1925/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">Re1: The unifying link between DPO and SLiC appears to be deliberately designed, particularly noticeable in the normalization term, i.e., $\\pi_\\text{sft}(y|x)$, in Eq 10.\n\nAnswer: Your observation about the unifying link between DPO and SLiC in the context of Eq 10 is indeed insightful. However, I believe there are a few additional points to consider, which might offer a different perspective.\n\n1. The purpose of integrating $\\pi_\\text{sft}(y|x)$ into Eq 10 goes beyond merely unifying DPO and SLiC. This integration is primarily aimed at establishing a more robust theoretical framework for the SLiC-HF approach. While it's true that SLiC-HF and DPO emerged around the same time, leading to some confusion about their connection, their starting points are fundamentally different. \n\n    * SLiC-HF is grounded in the concept of likelihood calibration, where a better response is expected to have a higher likelihood than a less desirable response, by a defined margin.\n\n    * On the other hand, DPO builds on the Reinforcement Learning from Human Feedback (RLHF) objective and the Bradley-Terry model, leading to the derivation of the sigmoid loss function as a Maximum Likelihood Estimation (MLE).\n\n2. Our contribution, as outlined in the paper, is to theoretically link these two approaches. By substituting the Bradley-Terry model with a Support Vector Machine (SVM) model, we obtain the hinge-norm of SLiC, effectively a normalized version. The original SLiC calibration loss overlooks the SFT policy normalization, relying solely on preference pairs labeled by the reward model. The hinge-norm loss, conversely, uses the SFT likelihood as a baseline to compute the relative likelihood ratio between the current policy and the SFT policy. This approach not only calibrates the likelihood ratio towards human preferences but also acts as an adaptive margin version of the SLiC loss. The margin, in this case, is influenced by the SFT likelihoods of the better and worse responses. Furthermore, this paper [1] highlights issues with the Bradley-Terry model and proposes the IPO loss, which aligns more closely with the hinge-norm loss than the original SLiC.\n\n3. In terms of implementation, DPO and SLiC variants share similarities by the choice of link functions and normalization. There isn't conclusive evidence yet to prefer one over the other. The paper provides a comprehensive framework for choosing between these options, underscoring the need for further research in this area.\n\nIn conclusion, while the connection between DPO and SLiC might seem deliberate, especially in the context of Eq 10, it is part of a broader effort to establish a more theoretically sound approach to SLiC-HF, taking into account the nuances of different models and the importance of the SFT policy.\n\n\n**References**\n\n[1] Azar, Mohammad Gheshlaghi, et al. \"A general theoretical paradigm to understand learning from human preferences.\" arXiv preprint arXiv:2310.12036 (2023)."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699918334334,
                "cdate": 1699918334334,
                "tmdate": 1699918334334,
                "mdate": 1699918334334,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pPV65QK5F7",
                "forum": "xbjSwwrQOe",
                "replyto": "j6lAIhNV3A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1925/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">Re2: The theoretical conclusion that the policy induced from RSO in Eq 4 is optimal seems questionable. RSO continues to use the proxy reward model to label responses, regardless of whether they're from the pre-trained or learned policy. However, the reward model is learned from $D_p$ and not $D_p^\u2217$ without further updates, which could introduce approximation errors and bias in subsequent labeling and learning steps. In the other word, the policy induced from Eq 4 is not optimal because of the non-optimal reward model.\n\nAnswer: Thank you for pointing out this important issue. We agree that RSO in Eq 4 is not strictly optimal. We have addressed the questions one-by-one.\n\n(1) \u201cRSO continues to use the proxy reward model to label responses, regardless of whether they're from the pre-trained or learned policy.\u201d\n\nIn our revised approach, we acknowledge the impracticality of directly sampling from an optimal policy. To address this, we've shifted from using the term 'optimal policy' to 'estimated optimal policy' in our documentation, specifically when referencing the sourcing of preference pairs from the rejection sampled policy. This change underlines our strategy to approximate the optimal policy via a rejection sampling method. The distribution shift of reward model training data also exists in RLHF, even though it conducts sampling from online policy. The errors in the proxy reward model can result in reward hacking. To mitigate the issue, we can collect preference data from the current best policy iteratively. In Llama2 [1], they collect human preference data from the current best policy on a weekly basis, as the policy keeps improving. Our work can follow the same approach as we improve the policy iteratively using rso-sample-rank.\n\n(2) \u201cHowever, the reward model is learned from $D_p$ and not $D_p^*$ without further updates, which could introduce approximation errors and bias in subsequent labeling and learning steps\u201d\n\nWe acknowledge the concerns regarding approximation errors and bias in our reward model learned from $D_p$. However, we assert that our pairwise reward model holds significant advantages over the implicit pointwise reward model (Equation (5)) induced by the Bradley-Terry model as per DPO, for several reasons:\n\n1. **Pointwise vs Pairwise**: Our pairwise comparison approach, structured as \u201c[Context] {context} [Response A] {response a} [Response B] {response b}\u201d, aligns more naturally with human cognitive processes, compared with assigning a real-valued reward score for a response given a context.  The pairwise approach not only simplifies the task but also leverages the extensive knowledge transfer from pre-trained language models in classification and comparison tasks. Unlike the Bradley-Terry model, which imposes a rank-1 approximation thus potentially losing information, our model directly represents the complexity of pairwise interactions without such assumptions.\n\n2. **Implicit vs Explicit**: Our model is an explicit text-to-text classification model, a task that is straightforward and less prone to the complexities associated with language generation. In contrast, the implicit reward model in DPO, based on likelihood scoring or generation, navigates a more intricate language output space. This complexity can hinder the effective transfer of knowledge from pre-trained models. Our approach, grounded in the principle that classification tasks are inherently more manageable than generation tasks, offers a more robust and reliable framework.\n\nTo substantiate our claims, our comprehensive experimental studies demonstrate the superiority of our model. For instance, rso-sample-rank shows to be significantly better than DPO. These results highlight not just the theoretical but also the practical advantages of our approach.\n\n\n**References**\n\n[1] Touvron, Hugo, et al. \"Llama 2: Open foundation and fine-tuned chat models.\" arXiv preprint arXiv:2307.09288 (2023)."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699918609827,
                "cdate": 1699918609827,
                "tmdate": 1699918609827,
                "mdate": 1699918609827,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MZ1alWfAPx",
                "forum": "xbjSwwrQOe",
                "replyto": "j6lAIhNV3A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1925/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">Re3: Works with a similar focus on rejection sampling, such as ReST(http://arxiv.org/abs/2308.08998), ALMoST (https://arxiv.org/abs/2305.13735) RAFT (https://arxiv.org/abs/2304.06767), should be involved in comparison.\n\nAnswer: Thank you for your suggestions regarding our baseline comparisons. We acknowledge the significance of ALMoST in the context of iterative LLMs and PPO algorithms. However, we opted not to include it in our baseline due to its distinct approach, particularly its reliance on synthetic preference pairs, which diverges significantly from RSO's methodology. This fundamental difference in approach could lead to an unfair comparison.\n\nIn contrast, RAFT and ReST share more similarities with RSO in terms of objectives and methodologies. For RAFT, we continue to fit the SFT model using cross-entropy loss on the best response selected by tournament ranking using a pairwise reward model. For ReST, we fix $\\tau=0.7, I=1, G=1$ as suggested by the ReST paper. We normalize the reward scores between 0 and 1 and choose the responses with reward greater than 0.7 as new SFT targets. We did not include multiple rounds of grow and improve stages because of a fair comparison consideration. RSO can also be done in multiple rounds to generate better rso-sample-rank preference pairs.\n\nOur results demonstrate that RSO outperforms these robust baselines across various metrics, highlighting its effectiveness. While we are confident in our findings, we remain open to future studies that may explore these comparisons further, potentially integrating models like ALMoST under a framework that allows for a fair comparison.  \n\n| Task          | Approach           | Proxy Reward (%) | Gold Reward (%) | AutoSxS (%) |\n| ------------- | ------------------ | ---------------- | --------------- | ----------- |\n| Reddit TL; DR | RAFT               | 74.84            | 68.51           | 53.77       |\n|               | ReST               | 49.03            | 46.17           | 34.36       |\n|               | $RSO_{sigmoid-norm}$ | 92.37            | 82.22           | **71.86**       |\n|               | $RSO_{hinge-norm}$   | **92.80**           | **83.45**           | 70.84       |\n| AnthropicHH   | RAFT               | 58.21            | 40.00           | 24.99       |\n|               | ReST               | 43.48            | 30.33           | 15.58       |\n|               | $RSO_{sigmoid-norm}$ | **86.94**            | **59.15**           | **40.98**       |\n|               | $RSO_{hinge-norm}$   | 84.44            | 57.75           | 38.58       |"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699918772419,
                "cdate": 1699918772419,
                "tmdate": 1699918772419,
                "mdate": 1699918772419,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ygXhPe01A8",
                "forum": "xbjSwwrQOe",
                "replyto": "j6lAIhNV3A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1925/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">Re4: Why do not directly sample responses from $\\pi_{r_\\psi}(y|x)$ (with a reward threshold like ReST) rather than sampling from $\\pi_\\text{sft}(y|x)$ with the statistical rejection sampling to approximate the samples from $\\pi_{r_\\psi}(y|x)$?\n\nAnswer: Thank you for raising this important question. There are two reasons that we don\u2019t directly sample responses from $\\pi_{r_\\psi}(y|x)$:\n\n1. **Online vs Offline**: ReST is an online on-policy algorithm, while RSO is an offline algorithm. Online algorithm allows the model to sample responses from the current policy, while the offline algorithm only has access to the initial policy (SFT policy) to generate responses. RSO bridges the gap by sampling responses from a better policy that is closer to the optimal policy. Online algorithms like ReST and PPO are not time and memory efficient. ReST needs policy network and value network to co-exist during training, and sampling from the online policy is not parallelizable. PPO needs two more networks (reference policy network and value network). On the other hand, RSO can generate rso-sample-rank preference pairs from SFT policy with a rejection sampling algorithm in a fully parallelizable way. The training of the RSO policy also only needs the policy network, which is much more memory efficient than ReST/PPO.\n\n2. **Connection with ReST**: in our paper, we demonstrate that the prevalent best-of-N or top-k-over-N rejection sampling methods are merely specific instances of our more comprehensive statistical rejection sampling algorithm, particularly when the beta parameter is set to zero. The ReST paper is very similar to an online version of the top-k-over-N. In Figure 3(b), we show RSO is better than the best-k-over-N. We also verify that in our updated draft in comparison with ReST. If beta approaches infinity, our method aligns seamlessly with sampling from an SFT policy. This crucial insight underpins our proposed method, which adeptly balances reliance on the SFT policy with the accuracy of the reward model. We use the hyper-parameter beta to control the balance between the two and thus offer a more robust and adaptable solution for practical applications."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699919007612,
                "cdate": 1699919007612,
                "tmdate": 1699919007612,
                "mdate": 1699919007612,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dsBrrlyg0v",
                "forum": "xbjSwwrQOe",
                "replyto": "j6lAIhNV3A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1925/Reviewer_nTnq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1925/Reviewer_nTnq"
                ],
                "content": {
                    "title": {
                        "value": "Official Response from Reviewer nTnq"
                    },
                    "comment": {
                        "value": "The reviewer thanks the authors for their prompt response.\n> R1: The unifying link between DPO and SLiC appears to be deliberately designed.\n\nThe reason why I say it was deliberately designed is that I find there is a regularization term in Eq. (1) (the second term). However, I also find the statement from the authors: \"The \ufb01rst difference is that we drop the regularization loss (second term in Equation (1)) due to lack of signi\ufb01cantly improvement the \ufb01nal metrics (Appendix A.6).\". I do not 100% buy this statement because the second term is important to avoid the model collapse. More, the ablation study of Table 1 (Zhao et al., 2023) shows further performance improvement with best-ranked candidates in the regularization term. \n\n> R3: Experiments comparison between more benchmarks. \n\nThe experiments demonstrate that RSO significantly outperforms (x1.8) ReST! Could the author provide further clarification for this? Further, I think ReST can also use the sft-sample-rank and rso-sample-rank for further improvement. Please correct me if I'm wrong somewhere.\n\n> Re4: Why do not directly sample responses from $\\pi_{r_\\psi}(y \\mid x)$ (with a reward threshold like ReST) rather than sampling from $\\pi_{\\mathrm{sft}}(y \\mid x)$ with the statistical rejection sampling to approximate the samples from $\\pi_{r_\\psi}(y \\mid x)$?\n\n1. online vs. offline: RSO is somewhat of an offline algorithm. However, we only call it offline when we never sample any response from any policy. As a result, the memory efficiency is not valid for me as RSO still samples actions from the behavior policy. \n2. Based on the statement \"The ReST paper is very similar to an online version of the top-k-over-N.\", I can not observe the clear superiority of RSO from the methodology perspective, as well as the memory view, compared with ReST. Besides, this \"offline\" approximation brings some disadvantages such as statistical approximation, and computation cost. Regarding \"balances reliance on the SFT policy with the accuracy of the reward model.\", why should use the balance word because the reward model never changes? Suppose I have a misunderstanding with the reward model, I can simply set a relaxed threshold in ReST to \"balance\" these two."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700450692497,
                "cdate": 1700450692497,
                "tmdate": 1700450738063,
                "mdate": 1700450738063,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Jlu5GcbFKR",
                "forum": "xbjSwwrQOe",
                "replyto": "j6lAIhNV3A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1925/Reviewer_nTnq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1925/Reviewer_nTnq"
                ],
                "content": {
                    "title": {
                        "value": "Official Response from Reviewer nTnq"
                    },
                    "comment": {
                        "value": "Thanks for the further clarification. However, I still have some questions but first with the most important one: \n\n> Offline vs. online: RSO uses the data sampled from a static SFT policy, followed by rejection sampling based on a static reward model. \n\nDoes RSO sample responses from $\\pi_{sft}$ during the training? I think it is from Section 3.2. If it is, then RSO is not offline and the memory benefit is not valid."
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700604636684,
                "cdate": 1700604636684,
                "tmdate": 1700604670846,
                "mdate": 1700604670846,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5KnEf5lIYS",
                "forum": "xbjSwwrQOe",
                "replyto": "j6lAIhNV3A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1925/Reviewer_nTnq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1925/Reviewer_nTnq"
                ],
                "content": {
                    "title": {
                        "value": "Regarding Offline vs. Online Further Question"
                    },
                    "comment": {
                        "value": "Thanks for the further clarification. To the best of my understanding, the response from the authors seems like steps 1 and 2 are totally decoupled with step 3 (optimization). Yes, I would like to agree with the statement. However, when I back to Section 3.2: \n\n1. empty y \n2. generate samples \n3. rank \n4. repeat steps 2 and 3\n\n5. optimize \n\nIs it a one-step optimization, regarding the learned policy $\\pi_r$? In other words, Is $\\pi_r$ never changed in Statistical Rejection Sampling? Could the authors provide a rough pseudocode including key steps for the whole training process to make it more clear?"
                    }
                },
                "number": 30,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700613030660,
                "cdate": 1700613030660,
                "tmdate": 1700613093618,
                "mdate": 1700613093618,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HIHw7ZIZo6",
                "forum": "xbjSwwrQOe",
                "replyto": "4QUPJgPJdt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1925/Reviewer_nTnq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1925/Reviewer_nTnq"
                ],
                "content": {
                    "title": {
                        "value": "Official Response from Reviewer nTnq"
                    },
                    "comment": {
                        "value": "Thanks for the throughout and patient clarification. Based on the strong experiments, responses, and revised paper, I decided to raise my score."
                    }
                },
                "number": 32,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700620349734,
                "cdate": 1700620349734,
                "tmdate": 1700620349734,
                "mdate": 1700620349734,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QoTyial1iX",
            "forum": "xbjSwwrQOe",
            "replyto": "xbjSwwrQOe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1925/Reviewer_DcKN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1925/Reviewer_DcKN"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel approach called Statistical Rejection Sampling Optimization (RSO) to improve preference optimization in language models. The authors address the limitations of existing methods by introducing rejection sampling to source preference data from the optimal policy. They also propose a unified framework that enhances the loss functions used in both Sequence Likelihood Calibration (SLiC) and Direct Preference Optimization (DPO). Through extensive experiments, the authors demonstrate that RSO consistently outperforms SLiC and DPO in terms of both Large Language Models (LLMs) and human raters' evaluations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors provide a well-structured review of relevant literature, highlighting the limitations of existing methods and positioning their work in the context of prior research."
                },
                "weaknesses": {
                    "value": "1. The core idea of utilizing rejection sampling to filter the response feels somewhat trivial \n\n2. Why is there a significant improvement in rso-sample-rank compared to sft-sample-rank on Reddit TL;DR, while the improvement is not as noticeable on AnthropicHH? What could be the potential reasons behind this?\n\n3. There might be flaws in the design of the experimental part: the reward for training and testing is the same. Although there are evaluations from GPT/human, generally speaking, these are not very convincing.  You can refer to [1, 2] for Synthetic Data Setup, Recalibration, gold reward, and proxy reward to have a fair comparison.\n\n[1] Let\u2019s Verify Step by Step\n\n[2] Scaling Laws for Reward Model Overoptimization"
                },
                "questions": {
                    "value": "Please refer to the weakness section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1925/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698836601608,
            "cdate": 1698836601608,
            "tmdate": 1699636122946,
            "mdate": 1699636122946,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QMiirwzCWn",
                "forum": "xbjSwwrQOe",
                "replyto": "QoTyial1iX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1925/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Re1: The core idea of utilizing rejection sampling to filter the response feels somewhat trivial\n\nAnswer: Your feedback about the perceived triviality of using rejection sampling in our approach is appreciated. We understand that at first glance, rejection sampling might seem simplistic. However, the key contribution of our work lies in the context of its application and the nuanced improvements we have implemented.\n1. **Bridging a Crucial Gap**: Our primary aim is to bridge the gap between offline off-policy algorithms and approximately on-policy algorithms. Current approaches like DPO and SLiC-HF have not effectively addressed the distribution-shift issue. Our work is pioneering in conducting a systematic study to approximate the optimal policy among offline human preference alignment algorithms.\n2. **Connect with Existing Best-of-N**: In our paper, we demonstrate that the prevalent best-of-N or top-k-over-N rejection sampling methods are merely specific instances of our more comprehensive statistical rejection sampling algorithm, particularly when the beta parameter is set to zero. More importantly, we establish that as beta approaches infinity, our method aligns seamlessly with sampling from an SFT policy. This crucial insight underpins our proposed method, which adeptly balances reliance on the SFT policy with the accuracy of the reward model. Unlike existing methods, ours provides a nuanced approach that adapts dynamically to varying degrees of confidence in either the SFT policy or the reward model, thereby offering a more robust and adaptable solution for practical applications.\n3. **Efficiency Enhancements**: We acknowledge the concerns raised by reviewer hBgS regarding the computational expense of calculating $M$. To address this, we have innovated by not computing\n$M$ directly. Instead, we estimate $\\frac{\\pi_{r_{\\psi}}(y|x)}{M\\pi_{\\text{sft}}(y|x)}$ in its entirety using 64 sequences sampled through the SFT policy. This methodological change, detailed in Appendix A.1, significantly enhances efficiency.\n4. **Improved Rejection Sampling Method**: Our method diverges from traditional statistical rejection sampling by sampling from the proposal distribution without replacement. This approach is more efficient in high-dimensional settings where the probability of rejecting many proposals is high. By recalculating the maximum reward at each round\u2019s start among unselected candidates, we ensure the selection of at least one candidate\u2014the one with the highest reward. This modification guarantees efficiency, particularly when the target is a limited number of selected responses.\n5. **Theoretical Backing and Simplicity**: Our algorithm, while simple in its execution as demonstrated in Algorithm 1 in Appendix A.1, is grounded in robust theoretical principles. The calculation of the threshold for a uniform random sample is simplified to computing exp((reward-max_reward) / beta), a result of deriving a closed-form solution for the optimal policy.\n\nIn summary, our approach, although founded on a basic concept like rejection sampling, is a novel and effective solution for the challenges in offline human preference alignment algorithms. The improvements we have made are not just in terms of algorithmic efficiency, but also in bridging a significant research gap. The simplicity of the method should not undermine its effectiveness and the novelty of its application in this context."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699917800224,
                "cdate": 1699917800224,
                "tmdate": 1699917800224,
                "mdate": 1699917800224,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2bXYqv9W7a",
                "forum": "xbjSwwrQOe",
                "replyto": "QoTyial1iX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1925/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Re2: Why is there a significant improvement in rso-sample-rank compared to sft-sample-rank on Reddit TL;DR, while the improvement is not as noticeable on AnthropicHH? What could be the potential reasons behind this?\n\nAnswer: We appreciate the reviewer's insightful observation regarding the varying degrees of improvement in rso-sample-rank compared to sft-sample-rank across the Reddit TL;DR and AnthropicHH datasets. Our analysis suggests two primary reasons for this phenomenon:\n\n1. **Dataset Characteristics and Their Impact**:\n\n    * **Reddit TL;DR Dataset**: This dataset uniquely combines SFT and human preference data sources. The human preference responses, generated through a mix of policies from OpenAI, including best-of-N rejection sampling, often yield higher quality outputs than SFT targets. Existing literature provides evidence on this. For example, in the right panel of Figure 2 in the DPO paper [1], it shows that Prefered-FT (SFT on the better responses from human preference data) is generally better than SFT. This divergence in quality potentially enlarges the gap between the SFT policy and the optimal policy, thereby making the improvements in rso-sample-rank more pronounced.\n\n    * **AnthropicHH Dataset**: Contrarily, in the AnthropicHH dataset, both the SFT targets and reward model are derived exclusively from human preference data. The proximity in quality between the SFT policy and the reward model narrows the gap with the optimal policy, resulting in less noticeable improvements in rso-sample-rank.\n\n2. **Evaluation Methodology**:\n    * Our human evaluation methodology, which deviates from the standard side-by-side comparison, might contribute to these observations. We asked raters to choose the best response among direct, sft-sample-rank, and rso-sample-rank options. While this approach has its merits, it may not fully capture the nuanced differences between sft-sample-rank and rso-sample-rank. Future studies could benefit from a more conventional comparative analysis to validate these findings further.\n\n**References**\n\n[1] Rafailov, Rafael, et al. \"Direct preference optimization: Your language model is secretly a reward model.\" arXiv preprint arXiv:2305.18290 (2023)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699917973046,
                "cdate": 1699917973046,
                "tmdate": 1699917973046,
                "mdate": 1699917973046,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5p1T1O79PE",
                "forum": "xbjSwwrQOe",
                "replyto": "QoTyial1iX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1925/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Re3: There might be flaws in the design of the experimental part: the reward for training and testing is the same. Although there are evaluations from GPT/human, generally speaking, these are not very convincing. You can refer to [1, 2] for Synthetic Data Setup, Recalibration, gold reward, and proxy reward to have a fair comparison.\n\nAnswer: Thank you for emphasizing the need for a robust evaluation framework in our experimental design. Your suggestions regarding the utilization of distinct rewards for training and testing phases, as well as the incorporation of a gold reward model, are indeed pertinent.\n\nIn response, we have revised our approach following the methodologies outlined in [1]. Specifically, we have now implemented larger reward models (PaLM-2 S) to serve as our Gold Reward model. This model is not only used as an additional metric but also helps in differentiating the rewards between the training and testing phases, thereby addressing your concern about the potential overlap in rewards.\n\nMoreover, to ensure transparency and thoroughness in our evaluation, we have included detailed metrics on the evaluation accuracy of the Gold Reward model in Section 5 of our updated draft. This additional layer of evaluation not only bolsters the validity of our findings but also aligns our methodology more closely with current best practices in the field.\nWe believe these modifications greatly enhance the rigor of our experimental setup and appreciate your valuable input in guiding these improvements.\n\n**References**\n\n[1] Gao, Leo, John Schulman, and Jacob Hilton. \"Scaling laws for reward model overoptimization.\" International Conference on Machine Learning. PMLR, 2023."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699918112465,
                "cdate": 1699918112465,
                "tmdate": 1699918112465,
                "mdate": 1699918112465,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "W27JKkY1j9",
                "forum": "xbjSwwrQOe",
                "replyto": "QoTyial1iX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1925/Reviewer_DcKN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1925/Reviewer_DcKN"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response and for adding a golden reward, which I believe is a crucial element.\n\nLike reviewer hBgS, I am also interested in the cost aspect of the method. I have two questions:\n\n1. What is the value of 'num_samples'? To obtain this number of samples, how many additional samples need to be generated?\n\n2. What is the specific training cost (GPU hours) for generating new samples plus the 'best of 64' to estimate M, and how long does it take to train DPO? I am aware that DPO trains quickly, so I am concerned about the potential excess time introduced by additional training."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580636290,
                "cdate": 1700580636290,
                "tmdate": 1700580636290,
                "mdate": 1700580636290,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "54Yk9862mD",
                "forum": "xbjSwwrQOe",
                "replyto": "kWgGxOrHX1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1925/Reviewer_DcKN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1925/Reviewer_DcKN"
                ],
                "content": {
                    "comment": {
                        "value": "*'If we use more accelerators, the speedup on our rso-sample-rank is linear to'*\n\nWhen calculating the additional time introduced, you need to use the same amount of resources as when training dpo.\n\nIn the table you provided, the Input length is 1024, while the Decode length is only 128. However, in practical chat tasks when fine-tuning LLMs, such as llama, most of the time the Decode length equals the Input length, which is 1024. Therefore, the relative training cost for rso-sample-rank (before training) is a tremendous burden for training DPO.\n\nMy main concern about this article is that the idea is useful but straightforward, more like a trick needed when using dpo. In terms of improving dpo, there are many similar and more solid works in the same period.\n\n[1] Wu, Tianhao, et al. \"Pairwise proximal policy optimization: Harnessing relative feedback for llm alignment.\" *arXiv preprint arXiv:2310.00212* (2023).\n\n[2] Li, Ziniu, et al. \"ReMax: A Simple, Effective, and Efficient Method for Aligning Large Language Models.\" *arXiv preprint arXiv:2310.10505* (2023).\n\n\n\nMy initial score is 6, and I will not change my score. But a score of 6 does not mean that I am inclined to accept; my inclination is between acceptance and rejection."
                    }
                },
                "number": 36,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737619885,
                "cdate": 1700737619885,
                "tmdate": 1700737619885,
                "mdate": 1700737619885,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qKocNcN4Nt",
                "forum": "xbjSwwrQOe",
                "replyto": "QoTyial1iX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1925/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks again for further follow-up on this. Let us make a few more clarifications.\n\n>Re1: When calculating the additional time introduced, you need to use the same amount of resources as when training dpo.\n\nAs stated in \"With the same usage of TPU-v4, the relative TPU hours for rso-sample-rank (before training) is less than 10% of that for training DPO\", we indeed use the same amount of resources. This is because training DPO is not fully parallelizable across the whole dataset. The training time increase linearly with the number of steps. For Reddit TL;DR, it takes us 22 hours to train 160k steps. For AnthropicHH, it takes us 56 hours to train 320k steps. On the contrary, all decoding are conducted on the **same** training data. Decoding is fully parallelizable across the whole dataset, because there is not parameter updates or dependencies across samples. The speed is much faster. Decoding on Reddit TL;DR and AnthropicHH takes us less than 2 hours.\n\n>Re2: In the table you provided, the Input length is 1024, while the Decode length is only 128. However, in practical chat tasks when fine-tuning LLMs, such as llama, most of the time the Decode length equals the Input length, which is 1024. Therefore, the relative training cost for rso-sample-rank (before training) is a tremendous burden for training DPO.\n\nAs stated above, decoding is fully parallelizable and efficient compared with training. Even with 1024 decode length, the relative cost for rso-sample-rank before training is still much faster than the training. Batch decoding is scalable and efficient due to the nature of parallelism [1], the long decode sequence length can be further optimized (Figure 8 and Table 1 in [1]). Thus, we do not agree that rso-sample-rank before training to be a tremendous burden.\n\n> Re3: My main concern about this article is that the idea is useful but straightforward, more like a trick needed when using dpo. In terms of improving dpo, there are many similar and more solid works in the same period.\n\nI think there is a misunderstanding of the major contribution of this paper. Our major contribution is that we **identify a key problem existing in DPO** with the **response DISTRIBUTION SHIFT** between the reward model training data and the real policy to optimize. DPO adjusts the relative probability mass at two responses. If the likelihoods of both responses are low, this adjustment is not meaningful. If you have experience on running DPO, you will observer a phenomenon that it converges and overfits fast, sometimes even less than 1 epoch! The issue with that is: DPO loss can go to zero if we push down the likelihood of the negative response or the SFT likelihood is very small for the positive response (Equation (8)). rso-sample-rank solves the issue by making sure that the likelihood of the negative response is not low (because of rejection sampling) and the SFT likelihood of positive is not low (because we sample from SFT policy). Meanwhile, we unify two frameworks (SLiC and DPO) together. To address the distribution shift, we proposed the rejection sampling as a simple yet effective approach. We believe our work set a direction that future research can benefit on addressing the distribution shift problem in DPO.\n\nRegarding other research works [2][3], they are contemporaneous and we can add them in the related work in our camera-ready version. \n\nHope we have made everything clear and we are more than happy to follow up with any further questions/concerns you may have.\n\n**References**\n\n[1] Pope, Reiner, et al. \"Efficiently scaling transformer inference.\" Proceedings of Machine Learning and Systems 5 (2023).\n\n[2] Wu, Tianhao, et al. \"Pairwise proximal policy optimization: Harnessing relative feedback for llm alignment.\" arXiv preprint arXiv:2310.00212 (2023).\n\n[3] Li, Ziniu, et al. \"ReMax: A Simple, Effective, and Efficient Method for Aligning Large Language Models.\" arXiv preprint arXiv:2310.10505 (2023)."
                    }
                },
                "number": 38,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740681403,
                "cdate": 1700740681403,
                "tmdate": 1700740782222,
                "mdate": 1700740782222,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "huNSj6k1hk",
                "forum": "xbjSwwrQOe",
                "replyto": "qKocNcN4Nt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1925/Reviewer_DcKN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1925/Reviewer_DcKN"
                ],
                "content": {
                    "comment": {
                        "value": "Why did you choose a T5-large (770M) SFT policy and a T5-XXL (11B) pairwise reward ranking model? Does this suggest that the parameters of the reward model must be significantly larger than those of the SFT model?\n\n*\"If you have experience with running DPO, you will observe a phenomenon where it converges and overfits quickly, sometimes in less than one epoch! \"*\n\nIn my experiments with llama2 on the AnthrophicHH dataset using a cosine learning rate with 3 epoches, the accuracy was just 64%. The difference between chosen and rejected rewards kept increasing during training. With 64% traning accuracy, I don\u2019t believe this is overfitting; it seems more like underfitting. The Reddit TL;DR task might be too simple, leading to overfitting. Therefore, I'm curious to know what the final reward accuracy of DPO and RSO was in your AnthrophicHH experiments. Similar results can be seen in the rewards_train/accuracies and rewards_train/margins on the official wandb of DPO (https://wandb.ai/eric_anthony_mitchell/dpo-demos)."
                    }
                },
                "number": 40,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700742239685,
                "cdate": 1700742239685,
                "tmdate": 1700742239685,
                "mdate": 1700742239685,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "I8anCGXYqH",
            "forum": "xbjSwwrQOe",
            "replyto": "xbjSwwrQOe",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1925/Reviewer_hBgS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1925/Reviewer_hBgS"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new method to improving language model alignment using Statistical Rejection Sampling Optimization (RSO). Their method allows for more accurate estimation of the optimal policy, leading to better alignment with human preferences. Empirically, the authors demonstrate the effectiveness of RSO on two tasks: Reddit TL;DR summarization and AnthropicHH dataset. They further show that RSO works well on cross-task generalization from Reddit TL;DR. Lastly, the authors evaluate RSO using three different approaches: Reward Model, AutoSxS, and Human Evaluation. The results show that RSO variants outperform DPO and SLiC variants on both tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Strengths:\n\n- This paper introduces a new approach for improving language models alignment by exploiting rejection sampling to allow for a more precise estimation of the optimal policy. To the best of my knowledge, this proposed method is novel and well-founded. Moreover, it also include exsisting methods (e.g., DPO) as special cases.\n\n- The paper is mostly well-written and the illustration figures are helpful for the undertsanding of this work.\n\n- The empirical evaluation is thorough and demonstrate the effectiveness of the proposed method. Most of the claims / arguments are well-supported."
                },
                "weaknesses": {
                    "value": "Weakness:\n\n- There are still rooms for improving the presentation. For example, it would be better if the authors can explain a bit more on $\\rho_{\\psi}(x,y,y_b)$ when first introducing it. The authors could provide some examples or details about what would be an instantiation of $\\rho_{\\psi}(x,y,y_b)$.\n\n- Step 3 in section 3.2 might be overly time-consuming and computationally expensive. Especially the set $M = \\min\\\\{m | m\\pi_{sft}(y|x) \\geq \\pi_{r_{\\psi}}(y|x) \\text{ for all } y \\notin \\mathcal{Y}\\\\}$ is expensive to compute. Rejection sampling is also slow.\n\n- The results in Figure 3 (b) is not statistically signifcant and the confidence intervals are overlapping."
                },
                "questions": {
                    "value": "1. The paper focuses on improving language models by aligning them with human preferences. I wonder how might this approach be adapted to address issues of bias and fairness in language models? Could the authors provide some additional discussions on this?\n\n2. How does the proposed method compare against other methods, such as DPO, in terms computational efficiency?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "see weakness."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1925/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699102506687,
            "cdate": 1699102506687,
            "tmdate": 1699636122877,
            "mdate": 1699636122877,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "T4vv1tczDo",
                "forum": "xbjSwwrQOe",
                "replyto": "I8anCGXYqH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1925/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Re1: There are still rooms for improving the presentation. For example, it would be better if the authors can explain a bit more on $\\rho_{\\psi}(x,y,y_b)$ when first introducing it. The authors could provide some examples or details about what would be an instantiation of $\\rho_{\\psi}(x,y,y_b)$.\n\nAnswer: Thanks for highlighting the need for additional clarity. We admit that the presentation is not very clear at this part. In response, we have expanded the explanations of $\\rho_{\\psi}(x,y,y_b)$ in our revised draft. Furthermore, to illustrate its practical application, we have included specific examples of inputs and outputs, ensuring a clearer understanding of its implementation."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699916584138,
                "cdate": 1699916584138,
                "tmdate": 1699916584138,
                "mdate": 1699916584138,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "oy4GUEPbpy",
                "forum": "xbjSwwrQOe",
                "replyto": "I8anCGXYqH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1925/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Re2: Step 3 in section 3.2 might be overly time-consuming and computationally expensive. Especially the set $M = \\min ({m | m\\pi_{sft}(y|x) \\geq \\pi_{r_{\\psi}}(y|x) \\text{ for all } y \\notin \\mathcal{Y} })$ is expensive to compute. Rejection sampling is also slow.\n\nAnswer: Thank you for your insightful observation regarding the computational aspects. The reviewer is correct that directly computing $M$ can be quite expensive and rejection sampling can be slow. We clarify the following two points regarding the issue:\n\n1. **Computing $M$**: To address this, it's important to clarify that we do not directly compute $M$. Instead, our approach involves estimating $\\frac{\\pi_{r_{\\psi}}(y|x)}{M\\pi_{\\text{sft}}(y|x)}$ using 64 sequences sampled through the SFT policy, which offers a practical solution to the computational challenge you mentioned. For detailed insight into this methodology, including both the Python implementation and the algorithmic derivation, please refer to Appendix A.1 of our paper. We added a footnote to clarify this in our updated draft.\n\n2. **Rejection sampling efficiency**:\n    * Concerning the speed of the rejection sampling process, our method begins by sampling 64 sequences via the SFT policy, followed by rejection sampling. This initial SFT sampling step is notably efficient and highly parallelizable, particularly when prompts are shared, thereby enhancing the overall computational efficiency of the process. \n    * Regarding the statistical rejection sampling algorithm, as described in Algorithm 1, it exhibits enhanced efficiency by employing a sampling-without-replacement strategy. This is achieved by excluding the selected sequences subsequent to each sampling round. Furthermore, at the commencement of each round, the maximum reward is recalculated. This recalibration ensures that, in every round, at least one sequence is invariably chosen. Specifically, the sequence whose reward is equivalent to the maximum reward is selected with a probability of one, thereby guaranteeing the selection of at least one optimal sequence in each round. This approach not only optimizes the selection process but also maintains the algorithm's effectiveness throughout its execution.\n\nWe have added a section discussing the computational cost and efficiency of our approach in Appendix A.10 of the updated draft."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699916960747,
                "cdate": 1699916960747,
                "tmdate": 1699916960747,
                "mdate": 1699916960747,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hWcNYpw7Sj",
                "forum": "xbjSwwrQOe",
                "replyto": "I8anCGXYqH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1925/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Re3: The results in Figure 3 (b) is not statistically signifcant and the confidence intervals are overlapping.\n\nAnswer: Thank you for your insightful feedback regarding the statistical significance and confidence intervals in Figure 3(b). We appreciate the opportunity to clarify and reinforce the findings of our study.\n\n1. **Purpose of the Plot**: The primary objective of Figure 3(b) is to illustrate our process of hyper-parameter selection, focusing exclusively on the proxy reward model. This model's win rate against the SFT targets is depicted on the y-axis. It's crucial to note that the plot is to guide hyper-parameter choices. \n\n2. **Comparison among loss functions**: We don\u2019t claim the preferred choice between sigmoid-norm and hinge-norm loss functions. Regarding hinge loss, it performs the best (not statistically significantly) because hinge loss ignores the SFT policy completely and fully trusts the preference order determined by the proxy reward model. That explains why the proxy reward model win rate is high.\n\n3. **Significance of rso-sample-rank**: Despite the overlapping confidence intervals, a critical observation from the plot is that the rso-sample-rank consistently outperforms sft-sample-rank across all three loss functions. This is visually represented by the dotted lines (sft-sample-rank) remaining outside the shaded regions of all rso-sample-rank curves. \n\n4. **Evaluating top-k-over-N and Beta Selection**: The plot also challenges the notion that top-k-over-N is universally optimal. When beta equals zero, rso-sample-rank aligns with top-k-over-N. Interestingly, we observe that a beta of 0.5 achieves a higher proxy reward model win rate than beta=0. Although this difference is not statistically significant, it indicates that a nuanced approach to beta selection could yield better results than the default top-k-over-N strategy.\n\n5. **Inclusion of Additional Baselines**: To robustly validate our findings and address the concern of statistical significance, we have incorporated additional baselines, namely RAFT [1] and ReST [2], into our revised draft. These inclusions will provide a broader comparative framework, enhancing the validity and reliability of our conclusions.\n\n**References**\n\n[1] Dong, Hanze, et al. \"Raft: Reward ranked finetuning for generative foundation model alignment.\" arXiv preprint arXiv:2304.06767 (2023).\n\n[2] Gulcehre, Caglar, et al. \"Reinforced self-training (rest) for language modeling.\" arXiv preprint arXiv:2308.08998 (2023)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699917201494,
                "cdate": 1699917201494,
                "tmdate": 1699917201494,
                "mdate": 1699917201494,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2ZBsH8Io3i",
                "forum": "xbjSwwrQOe",
                "replyto": "I8anCGXYqH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1925/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Re4: The paper focuses on improving language models by aligning them with human preferences. I wonder how might this approach be adapted to address issues of bias and fairness in language models? Could the authors provide some additional discussions on this?\n\nAnswer: Thank you for highlighting the critical issues of bias and fairness in language models. We fully appreciate their significance in the broader context of AI ethics. \n\n(1) \u201cI wonder how might this approach be adapted to address issues of bias and fairness in language models?\u201d\n\nOur current research focuses on preference alignment in language models. In practical scenarios, reward scores are often multi-dimensional, and the aim of alignment is to attain a Pareto optimal frontier [1]. This allows for the introduction of additional objectives such as harmlessness, safety, and bias preference pairs. Our method is adaptable, functioning with either weighted-averaged reward scores or through integration with multi-objective DPO loss functions [2]. Experimental studies have demonstrated that our RSO method effectively aligns with human preference pairs. We posit that our approach has the potential to enhance fairness and reduce bias in language models, provided it is applied with appropriate human preference pairs. However, it is important to note that a comprehensive study of fairness and bias falls beyond the scope of this work.\n\n(2) \u201cCould the authors provide some additional discussions on this?\u201d\n\nWe have added Appendix A.9 for a deeper examination of bias and fairness in language models.\n\n\n**References**\n\n[1] Bai, Yuntao, et al. \"Constitutional ai: Harmlessness from ai feedback.\" arXiv preprint arXiv:2212.08073 (2022).\n\n[2] Zhou, Zhanhui, et al. \"Beyond One-Preference-for-All: Multi-Objective Direct Preference Optimization.\" arXiv preprint arXiv:2310.03708 (2023)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699917452692,
                "cdate": 1699917452692,
                "tmdate": 1699917452692,
                "mdate": 1699917452692,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vszmgrMsJy",
                "forum": "xbjSwwrQOe",
                "replyto": "I8anCGXYqH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1925/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Re5: How does the proposed method compare against other methods, such as DPO, in terms computational efficiency?\n\nAnswer: Thank you for raising the computational complexity question. In terms of computational efficiency, our approach is closer to SLiC-HF-sample-rank than DPO. We list the comparisons with other algorithms as follows: \n\n* Compared to DPO and SLiC-HF-direct, our approach requires an additional trained pairwise reward model, and also sample and rank stages. The pairwise reward model training is a standard text-to-text model without Bradley-Terry model assumption. The sample and rank are scalable and efficient with increased number of servers, especially with shared prompts in SFT sample and short decoding length with reward model.\n\n* Compared to SLiC-HF-sample-rank, we need to sample more sequences from SFT policy as candidates, followed by a rejection sampling algorithm. Both steps are scalable and efficient as described in response to reviewer hBgS.\n\n* Compared to RLHF, RSO is offline with efficient memory usage (only one copy of policy network instead of four copies in PPO: policy, reference, value, reward), and the sample-rank stage is fully parallelizable (no need to conduct the online sample, which is not parallelizable).\n\nTo clarify this, we added Appendix A.10 for thorough discussion in our updated draft."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699917599745,
                "cdate": 1699917599745,
                "tmdate": 1699917599745,
                "mdate": 1699917599745,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9ZxttMuQn6",
                "forum": "xbjSwwrQOe",
                "replyto": "I8anCGXYqH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1925/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Re1: However, my concern regarding the computation of \"M\" still persists. As stated by the authors, M is estimated by sampling \"64\" examples through the SFT policy. I believe this approach is quite costly and may not be a practical solution for deploying large-scale language models (e.g., >= 70B), as the inference cost would be significant.\n\nAnswer: thank you for your follow-up questions. Sampling decoded sequences on large-scale language models seems to be costly.  However, 64 is not a big value compared to some existing best-of-n studies (Fig.8 in LLAMA2 [1], Sec 3.1 in [2]). Even for large-scale language models, 64 samples used in our experiment are not expensive and very achievable in practical settings based on a few reasons:\n\n1. **Parallelism of the algorithm**: unlike the PPO algorithm that can only be parallelized within each batch, RSO can be parallelized across the whole training set. Accelerators like GPU, TPU are very good at parallel computations and are scalable linearly with more servers for inference. Public libraries like llama.cpp [3] support this feature.\n\n2. **Prompt caching**: prompt caching is supported in modern LLM servers [4][5]. To sample 64 responses, the input query/key/value embeddings can be pre-computed in a parallelized way and cached in memory. The sampling cost will be smaller to generate 64 responses from one prompt versus generating them from different prompts. Moreover, batching prompts can further be sped up as shown in [5] and total memory/time turns out to be (significantly) sublinear as one would expect (Figure 1 in [5]).\n\nCompared with online algorithms such as PPO, we save three networks (reference, value, reward) during policy training, which saves up to 3x memory usage. Furthermore, RSO is fully parallelizable on decoding and reward inference on the whole training data.  \n\n**References**\n\n[1] Touvron, Hugo, et al. \"Llama 2: Open foundation and fine-tuned chat models.\" arXiv preprint arXiv:2307.09288 (2023).\n\n[2] Gao, Leo, John Schulman, and Jacob Hilton. \"Scaling laws for reward model overoptimization.\" International Conference on Machine Learning. PMLR, 2023.\n\n[3] https://github.com/ggerganov/llama.cpp/tree/master/examples/batched \n\n[4] Pope, Reiner, et al. \"Efficiently scaling transformer inference.\" Proceedings of Machine Learning and Systems 5 (2023).\n\n[5] Kwon, Woosuk, et al. \"Efficient memory management for large language model serving with pagedattention.\" Proceedings of the 29th Symposium on Operating Systems Principles. 2023."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700522161167,
                "cdate": 1700522161167,
                "tmdate": 1700533331879,
                "mdate": 1700533331879,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uo2vjpnBN6",
                "forum": "xbjSwwrQOe",
                "replyto": "I8anCGXYqH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1925/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1925/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">Re2: Furthermore, using samples to estimate M tends to overestimate its value. This could potentially undermine the derivation of the proposed method, RSO. Additionally, it remains unclear to me how the number of sampled sequences affects performance. Would it be possible to conduct a study similar to Figure 36 in the Anthropic paper (https://arxiv.org/pdf/2204.05862.pdf)?\n\nAnswer: thanks again for raising the above important points.\n\n(1) \"Furthermore, using samples to estimate M tends to overestimate its value. This could potentially undermine the derivation of the proposed method, RSO.\"\n\nWe agree that the estimation of $M$ is biased given limited response candidates. We show that in Theorem 1, as $M$ goes to infinity, we can sample the responses from the exact optimal policy. In practice, $M$ is finite and our algorithm is indeed an approximation. There are two evidence we would like to emphasize regarding this:\n\n1. **The rejection sampling algorithm**: from the Algorithm 1 we can see, at each iteration in the while loop, the highest remaining response will always be selected (unless $\\beta=\\infty$, which goes back to sft-sample-rank scenario). So we ensure that the responses always contain high reward ones. From that point of view, we include the traditional rejection sampling as a special case and our bias goes towards that. On the other hand, as $\\beta=\\infty$, Algorithm 1 will sub-sample random responses from the SFT generated candidates. So on two extremes, our algorithm is well calibrated and behaved. For other $\\beta$'s, the sampled responses will follow a distribution that is closer to the optimal policy than the SFT one. We can intuitively observe that from Figure 2 in the paper.\n\n2. **Practical evidence**: from Figure 3(b) and Table 1-4, we can conclude that rso-sample-rank shows consistent gains over sft-sample-rank, which suggests that the rejection sampled responses are indeed from a better distribution than SFT.\n\n\n(2) \u201cAdditionally, it remains unclear to me how the number of sampled sequences affects performance. Would it be possible to conduct a study similar to Figure 36 in the Anthropic paper (https://arxiv.org/pdf/2204.05862.pdf)?\u201d\n\nThis is indeed a great suggestion. We may not have enough time to conduct this study given the limited time remaining before rebuttal ends. But we can seriously consider adding it in the camera-ready version."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1925/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700522467012,
                "cdate": 1700522467012,
                "tmdate": 1700533394657,
                "mdate": 1700533394657,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]