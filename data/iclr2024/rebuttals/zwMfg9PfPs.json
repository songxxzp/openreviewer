[
    {
        "title": "Out-of-Variable Generalisation for Discriminative Models"
    },
    {
        "review": {
            "id": "EOWYeM4s8B",
            "forum": "zwMfg9PfPs",
            "replyto": "zwMfg9PfPs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5166/Reviewer_qFZD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5166/Reviewer_qFZD"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces out-of-variable (OOV) generalization, which is an the ability to generalize in environments with variables that have never been jointly observed before. OOV is an issue in settings where different variables (e.g. diagnostic tests) are available for different environments (e.g. different patients). The paper investigates challenges for common approaches when faced with the OOV problem, and proposes an OOV predictor that leverage moments of the error distribution. The work contributes to theoretical understandings of OOV and offers a proof-of-concept for a predictor capable of non-trivial OOV transfer."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper formally studies a new perspective on generalization.\n- The methods employed in the paper are sound."
                },
                "weaknesses": {
                    "value": "- The paper does not demonstrate the practical applicability of the concept of OOV generalization, and the setting feels a bit contrived. Also it seems like OOV generalization can be thought of just a case of OOD generalization--if we think about all the variables together as the input, the OOV generalization is just a case of OOD generalization (e.g. covariate shift) where some inputs have clear signal from some features and other inputs have clear signal from other features. \n- It would be helpful to include more intuitive discussion throughout the paper providing more analysis on the sections. For example, more discussion on the assumptions of the settings/theorems would be helpful, and it's not clear exactly under what assumptions the proposed predictor is appropriate."
                },
                "questions": {
                    "value": "Please see weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5166/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698724264822,
            "cdate": 1698724264822,
            "tmdate": 1699636511957,
            "mdate": 1699636511957,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wOsP4Zy0Qz",
                "forum": "zwMfg9PfPs",
                "replyto": "EOWYeM4s8B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5166/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5166/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Also it seems like OOV generalization can be thought of just a case of OOD generalization--if we think about all the variables together as the input, the OOV generalization is just a case of OOD generalization (e.g. covariate shift) where some inputs have clear signal from some features and other inputs have clear signal from other features.\n\nWe are sorry for the misunderstanding. We would like to clarify OOV and OOD are quite distinct generalization notions. Covariate shift, for example, considers the input space $X$ remains the same but the distribution on input space shifts from source to target $P_s(X)$ to $P_t(X)$. However, OOV considers the input space between source and target contains different sets of variables, even though they could come from the same data generating process, as illustrated in Fig. 1a where X_s = [X1, X2] and X_t = [X2, X3], but X1, X2, X3 all come from the same distribution P(X1, X2, X3).  As a concrete example, each country records own population\u2019s vaccine data, e.g., [Age, Is_vaccined]. Such dataset have different distributions for each country (OOD) but share the same set of variables (In variable). However, if hospital A in the same country records their patients vaccine data as [Age, city, is_vaccined], and hospital B in the same country records data as [Age, health_history, is_vaccined], then though they share the same distribution as coming from the same country (in-distribution) they have different variable sets (OOV). We refer to Appendix E.3 for a more detailed discussion on OOD and OOV.\n\n> The paper does not demonstrate the practical applicability of the concept of OOV generalization\n\nWe thank you for your comment, though we hope to demonstrate the practical applicability of OOV through examples in AI in medicine (re introduction) and provide a concrete example in section 3.1. As the other reviewers also acknowledges our efforts in explicitly grounding the concept, for example, reviewer xwQY acknowledges that the paper provides \u201csimple, clear real-world examples to elucidate the problem\u201d, reviewer woi7 comments that \u201cexamples also made things concrete and easy to follow\u201d and reviewer s9Ga thinks we \u201cprovide an extensive study \u2026 including\u2026examples\u201d. \n\nWe think your comment is maybe more directed on the lack of real world dataset. We carefully take your feedback into consideration and following your suggestion, we include an additional real world experiment with details in Appendix D.4. \n\n> the setting feels a bit contrived. \n\nWe are sorry for this impression, though this paper, unlike many mainstream papers, aims to navigate a new research direction and inspire the community on a surprising overlooked but important research area. \n\nWe studied how common transfer learning approaches would fail in OOV setting and show under what circumstance identifiability is achievable for dependent and independent covariates. We exposes a problem that, in the stated form, may appear unsolvable at first sight, and then goes on to provide a mathematically valid and experimentally corroborated solution. In addition, we openly discuss our limitations and point the readers to potential avenues of future research for the area (re conclusion).  \n\nWe feel that the paper is already useful to the community at this point and sometimes, asking an important question, and giving an incomplete (but non-trivial) answer is more interesting and impactful in the long term. We realize this is highly subjective, and hope that you share our taste in research and our work could as reviewer woi7 says, \u201cwith the rigor and simplicity \u2026 can act as a foundation to build OOV research\u201d. \n\n> It would be helpful to include more intuitive discussion throughout the paper providing more analysis on the sections. For example, more discussion on the assumptions of the settings/theorems would be helpful, and it's not clear exactly under what assumptions the proposed predictor is appropriate.\n\nWe thank the reviewer for the suggestion and have included \n* a bullet list of assumptions on theorems and identifiability of the proposed predictors (Appendix E.2)\n* a discussion on violations to each of the assumptions from causal graph, to function class and to noise distributions (Appendix E.2.1). \n* On experimental analysis side, in addition to experiments on real world dataset (Appendix D.4), we conducted additional experiments on the robustness of our approaches when noise level changes (Appendix D.5), and when the distribution of noise is heavy tailed (Appendix D.6) \n\nOverall, we thank you for taking time, which we carefully take into account your feedback and included both experimental results and theoretical discussions in the updated version. We hope that our response has addressed all your questions and concerns. We kindly ask you to let us know if you have any remaining criticism, and - if we have answered your questions - to consider reevaluating your score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5166/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699959122096,
                "cdate": 1699959122096,
                "tmdate": 1699959122096,
                "mdate": 1699959122096,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fZAon7Cssu",
            "forum": "zwMfg9PfPs",
            "replyto": "zwMfg9PfPs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5166/Reviewer_s9Ga"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5166/Reviewer_s9Ga"
            ],
            "content": {
                "summary": {
                    "value": "This work investigates out-of-variable (OOV) generalization, which is a sub-problem to OOD generalization, and refers to scenarios where an agent needs to generalize to environments containing variables that were never jointly observed before. The paper shows that if the source and target environments contain some overlapping variables (and under certain conditions), information from the predictor in the source environment can improve predictions in the target environment. More specifically,  the moments of the residual distribution from the optimal classifier in the source environment can be used to calculate the generating function with respect to the unobserved variable in the target domain.\n\nBased on this observation, the paper proposes a practical algorithm for OOV prediction, evaluates its performance, and compares it against the marginal predictor and imputed predictor, as well as an Oracle predictor."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper proposes a new and important problem-setting - OOV generalization, which can occur in real-world situations, on its own or alongside OOD aspects. The work also provides an extensive study of the identification problems of various variants of OOV scenarios, including theoretical proofs and examples. \n\nIn addition, the paper proposes a practical algorithm to solve several OOV scenarios that achieves non-trivial OOV transfer on synthetic data.\n\nThe ideas presented in the paper are novel and the conclusion that information from source domains can be used for prediction in the target domain in this setting is important, and can potentially have a broad impact on future research in the field."
                },
                "weaknesses": {
                    "value": "The main limitation of the paper is that the proposed approach was tested on only synthetic data, and was not validated using more challenging datasets. \n\nIn addition, the extension of OOV in multi-environments is mentioned mainly in the appendix and the algorithm was not tested empirically for that extension."
                },
                "questions": {
                    "value": "I would like to ask the following questions:\n\n1. For future work, is there a more complicated/realistic dataset to validate the algorithm?\n2. Is it possible to compare the algorithm to state-of-the-art marginal or causal methods such as Mejia et al. (2021) or Janzing (2018)? To validate if Vapnik\u2019s principle holds and whether the proposed approach indeed improves results due to solving a less general problem.\n3. Theorem 3 connects all moments of the residual distribution to the partial derivatives with respect to the unique variable of the target environment. If additional moments were to be calculated as part of the proposed algorithm, would it improve results (for the general function case)? \n4. In general, since the paper's main claim is that in the real world, it is likely to encounter both aspects of OOD and OOV - How simple is it to combine state-of-the-art  OOD methods with the proposed approach? I cannot imagine at the moment a straightforward way to do that."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5166/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5166/Reviewer_s9Ga",
                        "ICLR.cc/2024/Conference/Submission5166/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5166/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698762596885,
            "cdate": 1698762596885,
            "tmdate": 1700684618252,
            "mdate": 1700684618252,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XY07xd7908",
                "forum": "zwMfg9PfPs",
                "replyto": "fZAon7Cssu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5166/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5166/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> For future work, is there a more complicated/realistic dataset to validate the algorithm?\n\nWe thank the reviewer for the suggestion and included results on real-world dataset in Table 2 (Appendix D.4). \n\n> Is it possible to compare the algorithm to state-of-the-art marginal or causal methods such as Mejia et al. (2021) or Janzing (2018)? To validate if Vapnik\u2019s principle holds and whether the proposed approach indeed improves results due to solving a less general problem.\n\nWe thank the reviewer for the suggestion, though we consider Janzing (2018)\u2019s work is more on the theoretical fronts and Mejia et al. (2021)\u2019s elegant analysis considers a merging dataset problem which does not directly fit to our particular setting. \n\n> Theorem 3 connects all moments of the residual distribution to the partial derivatives with respect to the unique variable of the target environment. If additional moments were to be calculated as part of the proposed algorithm, would it improve results (for the general function case)?\n\nThank you for the question, yes incorporating higher-order moments would improve the result as it allows us to estimate general function beyond first-order Taylor approximations. The higher moments provide a space of system of equations that can help to identify the first order partial derivatives and higher order partial derivatives, which assist in estimating a more complex function class. \n\n> In general, since the paper's main claim is that in the real world, it is likely to encounter both aspects of OOD and OOV - How simple is it to combine state-of-the-art OOD methods with the proposed approach? I cannot imagine at the moment a straightforward way to do that.\n\nThe reviewer asks re insights on combining both OOD and OOV approaches: we think sparse mechanism shift (SMS) might be a useful notion to characterize distributions shifts as SMS and our current approach both uses causal framework to assist analysis. It assumes the distribution shifts are caused by sparse causal mechanism shifts. We think depending on different variables to which the distribution shift happens would call for a different approach. For example, say, a functional relationship on $Y$ is $\\alpha X_1 + X_2 + X_2X_3$. Changing $\\alpha$ between the source and target environment does not affect our proposed approach on OOV generalization. However, if $\\alpha$ is placed on $X_2X_3$ term, a more sophisticated method need to be developed. We do not have broader results yet, and we feel that the paper is already useful to the community as a starting point to tackle real-world problems that exhibit both OOD and OOV."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5166/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699958192343,
                "cdate": 1699958192343,
                "tmdate": 1699958192343,
                "mdate": 1699958192343,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OgXczd0gh3",
                "forum": "zwMfg9PfPs",
                "replyto": "fZAon7Cssu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5166/Reviewer_s9Ga"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5166/Reviewer_s9Ga"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for answering my questions and appreciate their effort in providing additional experiments.\nMy main concern regarding the applicability of the proposed OOV setting to real-world scenarios was partially addressed by the additional experiment presented in Table 2 of the rebuttal. The experiment clearly validates the superiority of the proposed algorithms. However, a more convincing argument would be to validate the proposed approach on more than one real-world dataset (and perhaps a more challenging one). \nFurthermore, since most real-world scenarios are susceptible to aspects of OOV and OOD, I think that the paper would benefit from validating the feasibility of combining the proposed approach with a leading OOD algorithm. \n\nWith the above in mind, the paper is well written and I recognize its potential value for the community. For these reasons, I\u2019m updating my score."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5166/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684523434,
                "cdate": 1700684523434,
                "tmdate": 1700684554578,
                "mdate": 1700684554578,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bF4y5DIQ4i",
            "forum": "zwMfg9PfPs",
            "replyto": "zwMfg9PfPs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5166/Reviewer_woi7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5166/Reviewer_woi7"
            ],
            "content": {
                "summary": {
                    "value": "This paper describes the out-of-variable OOV problem, which in its simplest form, aims to learn a predictor Y = f_t(X2, X3) given an OOV predictor Y = f_s(X1, X2) and a dataset (X2, X3), but without any instance of (X2, X3, Y). The authors describe the setting in which this is possible and develops an algorithm. The key observation is that the third moment of the residue Y - f_s(X1,X2) contains information about X3 that is least polluted by the noise."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The key observation/discovery is clever, and the algorithm is straight-forward to use.\n- The writing is clear, clean, and well-referenced. The examples also made things concrete and easy to follow.\n- The rigor and simplicity of the work can act as a foundation to build OOV research."
                },
                "weaknesses": {
                    "value": "- The main weakness is the applicability of the method. The authors only showed results for proof-of-concept, not for real-world usage.\u00a0\n- It is unclear how one could identify whether the assumptions are satisfied given a dataset.\n- It is unclear how bad the predictor would be if the assumptions are not satisfied.\n- It is not yet clear what realistic problem can be well modeled by OOV generalization."
                },
                "questions": {
                    "value": "Intro:\n- It seems OOV fits very well the frame of missing-not-at-random and covariate-dependent missingness. Could the authors comment on that?\n\nSection 2:\n- Theorem 2 is slightly confusing for me at first glance because I thought PA_Y by definition includes all parents of Y (so x1,x2, x3 in the example) and not just those in the target environment (x2, x3). It may be helpful to clarify.\n\nSection 3:\nAs I am trying to get a sense of the restriction and applicability of the approach, I was wondering the following questions:\u00a0\n- How does the method fair with the oracle as the magnitude of the noise increases?\u00a0\n- What if the noise is not gaussian but more heavy tailed?\u00a0\n- Does the performance degrade or improve with increasing number of variables?\u00a0\n- I assume Theorem 3 does not apply to discrete variables because of the violation of differentiability; is that right?\n\nSection 4:\n- Can include missing-not-at-random imputation and covariate-missing imputation as two more baseline models (a search in Google scholar using the two key phrases yields some methods).\n- It would be really interesting if the authors could find some real-world datasets, create source and target environments by sub-setting the columns, and see how the method performs.\n- Figure 3: I don\u2019t quite understand the figure. It would be helpful to define OOV loss, be explicit about the number of samples on the y-axis being (x2,x3,y) or (x1,x2,y) or something else. I also don\u2019t understand why relative loss is zero means the method is on par with the oracle predictor. Why not just show how the fine-tuning error compares with oracle training, which seems easier to interpret? Anyway, I am overall a bit confused about the figure, so my questions may not make sense."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5166/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5166/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5166/Reviewer_woi7"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5166/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698788842803,
            "cdate": 1698788842803,
            "tmdate": 1699636511769,
            "mdate": 1699636511769,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9WboJS8aOC",
                "forum": "zwMfg9PfPs",
                "replyto": "bF4y5DIQ4i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5166/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5166/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> It seems OOV fits very well the frame of missing-not-at-random and covariate-dependent missingness. Could the authors comment on that?\n\nWe thank the reviewer for the question. Missing-not-at-random and covariate-dependent missingness refer to a scenario where whether a variable is observed or not contains information about other covariates, or about certain other properties of the data point. One can thus hope to exploit this assumption to recover more information about the dataset. We agree that such problems are related to OOV, though think OOV addresses a larger set of problems. For example, the setting of our paper (Fig. 1a) studies when covariates are independent. Such assumption is even more challenging as it provides less information on the remaining dataset. We surprisingly find that even in such challenging scenario, one can enable OOV generalization for certain cases.\n> Theorem 2 is slightly confusing for me at first glance because I thought PA_Y by definition includes all parents of Y (so x1,x2, x3 in the example) and not just those in the target environment (x2, x3). It may be helpful to clarify.\n\nThank you for your feedback on improving the paper. We have incorporated an explanation on the difference of Theorem 2 setting with the original setup in the paragraph between section 3.3 and section 3.3.1 in the updated version. \n\n> How does the method fair with the oracle as the magnitude of the noise increases? \n\nThank you for the question, we performed an additional systematic analysis similar in Table 1 with increase in noise standard deviation from 0.01 to 1 in an interval of 0.2. We averaged results over 5 repeated runs and observe our method are robust to increasing noise level and consistently outperforms remaining benchmarks despite noise increases. Table 3 (Appendix D.5) in the updated pdf shows the results. \n\n> What if the noise is not gaussian but more heavy tailed? \n\nThank you for your question, we included an additional systemic noise when noise is heavy tailed. Specifically, when noise follows a log-normal distribution with mean 0 and sigma 0.5. We repeated the experiment over 5 runs and averaged over a hyperparameter sweep. Table 4 (Appendix D.6) shows the results. We observe as expected by our theorems (Theorem 3), the skew of the noise pollutes the skew estimation of the residual distribution, which deteriorates performance.\n\n> Does the performance degrade or improve with increasing number of variables? \n\nThank you for your question. When the number of variables increases in the source environment, we expect the performance remains the same as the base network $f_s$\u2019s accuracy is indifferent to the number of variables given sufficient sample size; when the number of variables increases in the space of missing variables, we would expect our method would still be able to transfer under OOV setting. This is due to one can leverage multiple moments from the residual distribution to create a system of equations and solve multiple unknowns. We provide an additional theoretical analysis on a toy example facing two unobserved variables from the source environment in Appendix C.6. \n\n> I assume Theorem 3 does not apply to discrete variables because of the violation of differentiability; is that right?\n\nYes that is correct. \n\n> Can include missing-not-at-random imputation and covariate-missing imputation as two more baseline models (a search in Google scholar using the two key phrases yields some methods).\n\nThank for the suggestion, though (re discussion above) missing not at random and covariate dependent imputation do not apply in our setting as our assumptions are different. We assume our covariates to be independent whereas the proposed methods assume covariates are dependent. \n\n> Figure 3: I don\u2019t quite understand the figure. It would be helpful to define OOV loss, be explicit about the number of samples on the y-axis being (x2,x3,y) or (x1,x2,y) or something else. I also don\u2019t understand why relative loss is zero means the method is on par with the oracle predictor. Why not just show how the fine-tuning error compares with oracle training, which seems easier to interpret? Anyway, I am overall a bit confused about the figure, so my questions may not make sense.\n\nWe apologize for the confusion, the number of samples on the x-axis refer to (x2, x3, y), and the relative loss is calculated as $log(loss_{pred}/loss_{oracle})$, the log ratio of predictor\u2019s loss divided by the oracle\u2019s loss. If the predictor achieves the same loss as the oracle loss then the ratio would be 1 and $log(1) = 0$. The purpose of the experiment is to show what is the number of samples needed in the target environment, at which we would prefer the MomentLearn over the joint predictor. Figure 3 shows that up to ~100 samples in the target environment, MomentLearn outperforms the joint predictor."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5166/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699957593239,
                "cdate": 1699957593239,
                "tmdate": 1699957593239,
                "mdate": 1699957593239,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4cj05Qj806",
                "forum": "zwMfg9PfPs",
                "replyto": "KFbnE8dijm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5166/Reviewer_woi7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5166/Reviewer_woi7"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for the responses. The 3 extra experiments nicely addressed my main requests. I especially like the experiment on the mtcars dataset. I think this is one step closer to seeing OOV generalization and the proposed method in a real-world application. Having said that, I think the work is still a few steps removed from what I would consider naturally real-world, so I will keep my score as is for the time being."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5166/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700272578133,
                "cdate": 1700272578133,
                "tmdate": 1700272578133,
                "mdate": 1700272578133,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gmtb8dbv8B",
            "forum": "zwMfg9PfPs",
            "replyto": "zwMfg9PfPs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5166/Reviewer_xwQY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5166/Reviewer_xwQY"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates out-of-variable generalization, namely the ability for a predictive model to generalize to target domains in which the agent has never seen the joint variables in the target domain in a single source domain before. Under certain assumptions as well as when these assumptions don't fully hold, the paper shows that the error residual distribution in an environment provides information on the unobserved causal parent variable in this environment, and they use this information to derive an algorithm that performs OOV generalization with source and target domains that have overlapping sets of causal factors."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**Originality**\n- As far as I know, though the problem the paper addresses is well-known as a significant problem, the paper provides several theoretical results, mathematical derivations, and supports these with simple empirical results that are novel.\n\n**Quality**\n- The quality of the paper is high. It addresses a high-value problem in a principled fashion, shows how certain assumptions help obtain certain results and how and in which cases these assumptions can be bypasses while maintain approximately accurate results, and evaluates these cases in terms of loss accuracy as well as sample complexity of its approach versus baseline approaches.\n- The paper openly highlights limitations in its work, such as assumptions made for theorems to hold, and proposes prospective future work in multiple avenues. This refreshingly is (1) included at all and (2) doesn't seem like a mere afterthought.\n\n**Clarity**\n- The paper is mostly clear in its explanation of motivation, preliminaries, approach, baseline usage, results, and limitations.\n- The paper does a great job providing simple, clear real-world examples to elucidate the problem and applications of the various theorems included in multiple cases.\n\n**Significance**\n- The significance of the problem the paper addresses is high and the problem is ubiquitous. The approach is promising and can be applied in many real-world settings through Monte-Carlo sampling or similar methods. The paper shows that their approach can perform relatively well in \"few\"-shot settings though this depends on the number of variables involved and the complexity of the problem.\n\nFrom what I can tell, this is excellent work that I hope motivates further addressing this *out-of-variable* generalization problem by the research and applied AI community. My only reservation is my limited knowledge on the understanding of and state-of-the-art theoretical and applied approaches addressing this problem."
                },
                "weaknesses": {
                    "value": "- Referring to Figure 1, in the first paragraph in page 3, the claim \"it would seem all but impossible...(orange box)\" could be better explained.\n- In Figure 1, it is unclear whether \"With $Y$ not observed in the target domain\" is an assumption made or is somehow indicated in the diagram or earlier in the paper. Eventually I realized that it's an assumption made, but the illustration Figure 1a alone isn't enough to show this assumption. This ambiguity may clear for some or compound for some later in Section 3."
                },
                "questions": {
                    "value": "- The abstract states \"merely considering differences in data distributions is inadequate for fully capturing differences between learning environments.\" Doesn't out-of-variable technically fall under out-of-distribution, so shouldn't this be adequate? Perhaps more specificity is needed here.\n- The abstract states \"Mathematically, out-of-variable generalization requires the efficient re-use of past marginal information...\" Why does it require efficient re-use? Could it work with \"non-efficient\" or inefficient re-use?\n- On page 2, should \"modal\" be \"model?\"\n- On page 6, do you mean \"parentheses\" instead of \"brackets\" between Eq (9) and Eq (10)?\n- Why is the joint predictor considered an oracle predictor if MomentLearn outperforms it?\n- Could you explain why MomentLearn is reliably more sample efficient than the oracle predictor for \"few\"-shot prediction?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I have no ethics concerns."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5166/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699441328198,
            "cdate": 1699441328198,
            "tmdate": 1699636511667,
            "mdate": 1699636511667,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Z6iZ6Jipih",
                "forum": "zwMfg9PfPs",
                "replyto": "gmtb8dbv8B",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5166/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5166/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the suggestions to help us clarify Figure 1. We have incorporated the proposed changes in the updated version, by explaining the claim further and changing the border of the orange box.  We also thank the reviewer for their detailed observations on typos and have corrected them in the updated version. \n\n> The abstract states \"merely considering differences in data distributions is inadequate for fully capturing differences between learning environments.\" Doesn't out-of-variable technically fall under out-of-distribution, so shouldn't this be adequate? Perhaps more specificity is needed here.\n\nHere we refer to settings exhibit in-distribution if the environments share the same data-generating process. For example, in Fig 1a, though the environments share the same data generating process (in-distribution) but observed different sets of variables (out-of-variable). To give an example on OOD but not OOV, consider vaccine data from different countries: they have the same variables [Age, is_vaccined], but coming from different distributions due to country differences. Thank you for your question and we will make this point clearer in the next version. \n\n> Why is the joint predictor considered an oracle predictor if MomentLearn outperforms it?\n\nWe apologize for the confusion. In Table 1, we refer to the joint predictor as oracle predictor in cases where there are enough data samples in the target environment such that regressing on the joint variable will lead to a near optimal predictor. In Figure 3, we showed that when the sample size is small (~100 samples) in the target environment, joint predictor can lead to estimation error. MomentLearn predictor thus is able to outperform it.\n\n> Could you explain why MomentLearn is reliably more sample efficient than the oracle predictor for \"few\"-shot prediction?\n\nMomentLearn is more sample efficient than the joint predictor in \u201cfew\u201d-shot prediction because with small sample size, the joint predictor would lead to estimation error, whereas MomentLearn leverages the information observed in the source environment (due to large sample size in source) are able to mitigate the problem of small sample size in the target environment."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5166/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699957115900,
                "cdate": 1699957115900,
                "tmdate": 1699957115900,
                "mdate": 1699957115900,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rBSvNYxj8F",
                "forum": "zwMfg9PfPs",
                "replyto": "Z6iZ6Jipih",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5166/Reviewer_xwQY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5166/Reviewer_xwQY"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks Authors for addressing the Weaknesses and Questions I presented.\n\nI understand that OOD $\\nRightarrow$ OOV. My claim is that OOV $\\nRightarrow$ OOD (in the interpretation of OOD that nearly everyone defaults to), but it seems like you are implying that you are treating in or out-of-distribution as a environmental property rather than a task, or what is seen by the task, property. Please correct me if I am incorrect and if you're able to. Regardless, it isn't a big deal, as I'm pretty sure I understand your explanation. However, I do think it would be useful to make this more clear, as some readers may have qualms about this.\n\nThanks for explaining your reasoning about why and in which cases you refer to the joint predictor as an oracle predictor. I do see that in the caption for Figure 1(c) the paper states \"an oracle solution trained as if we have sufficient data...\" I would recommend making this explicit also in the main text when \"oracle\" is first mentioned in the main body in the first paragraph of 4 Experiments. I see that it says \"...large datasets,\" but you may want to make it at least as clear as you do in the caption and a reminder that the oracle doesn't hold when there isn't enough data or all the variables aren't observed when you introduce Figure 3, if you haven't done something similar already.\n\nThank you for your insight into why MomentLearn is reliably more sample efficient than the oracle predictor for \"few\"-shot prediction?\n\nI will retain my ratings as they are for now. Thank you, Authors! Nice work."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5166/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700741639385,
                "cdate": 1700741639385,
                "tmdate": 1700741639385,
                "mdate": 1700741639385,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]