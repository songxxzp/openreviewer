[
    {
        "title": "Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond"
    },
    {
        "review": {
            "id": "pOv6KPPABs",
            "forum": "qrGjFJVl3m",
            "replyto": "qrGjFJVl3m",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1807/Reviewer_2REz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1807/Reviewer_2REz"
            ],
            "content": {
                "summary": {
                    "value": "This model introduced a Vision Language Model QWEN-VL which is both pretrained and instruction finetuned. The model shows decent multimodal capability, especially in terms of bounding box reasoning. The model will be open sourced which will be helpful to the community."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Open sourcing the model is going to help the research community\n- The model shows decent multimodal capability, especially in terms of bounding box reasoning"
                },
                "weaknesses": {
                    "value": "My concerns are regarding the scientific and technical contributions from this paper.\n- The claim in the related work section, \"Despite achieving significant progress, previous vision-language models still have several limitations such as poor robustness in instruction following, limited generalization capabilities in unseen tasks, and a lack of in-context abilities.\" lacks justification. For example many models are not instruction-tuned (yet). That does not mean they have a fundamental difficulty in instruction following.\n- There is limited innovation on the model architecture and training recipe. For example the use of interleaved data and multi-stage, multi-resolution training has been proposed in previous works. Also there is limited novelty in showing that supervised finetuning with interleaved chat data can lead to chatting capability.\n- The ablation study is not written clearly (also see questions below)"
                },
                "questions": {
                    "value": "In the ablation study of Figure 7, which stage is that, lower-res or higher-res? If it is lower-res (224) stage it makes sense to use 256 tokens as the native number of patches is just (224/14)^2 = 256. If it is the higher-res (448) stage, then it is counterintuitive that using more tokens, i.e., 400, with more degrees of freedom, will lead to worse performance."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Discrimination / bias / fairness concerns"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Since it is an open source generative model, there should be analysis regarding Discrimination / bias / fairness"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1807/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1807/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1807/Reviewer_2REz"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1807/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826244107,
            "cdate": 1698826244107,
            "tmdate": 1699636110151,
            "mdate": 1699636110151,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sNTQFJ1dMj",
                "forum": "qrGjFJVl3m",
                "replyto": "pOv6KPPABs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1807/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1807/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2REz (1/2)"
                    },
                    "comment": {
                        "value": "**[Related work clarity]**\n\nThis is a missunderstanding, the limitations we mentioned were not intended to challenge previous vision-language models for lacking instruction tuning process. On the other hand, we meant to pinpoint the potential improving direction compared to existing vision-language chatbots (*e.g.*, VisualGLM, PandaGPT, MiniGPT4, and etc.). And as the results shown in Table 7, compared to these predecessors, Qwen-VL exhibits better instruction following ability on three recently proposed instruction following evaluation benchmarks.\n\n*For better readability, we have updated this section in the revision.*\n\n**[Innovation on model architecture and recipe]**\n\nWe thank the reviewer's valuable feedback. We restate Qwen-VL's contribution from several distinct perspectives in the general response to all reviewers. We hope that our detailed reply will ease your concerns.\n\n**[Ablation study details]**\n\nThanks for pointing this out! The experiments in Figure $7$ are all conducted in the **fist** stage, with $224\\times224$ resolution for image inputs. That is to say, our vision encoder will pathify each input image into $256$ patches, and under this circumstance we found that inadequate and superfluous numbers of queries both lead to slightly higher loss in the later training period.\n\n*We have updated this part in the revision to make it more clear.*"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700453354062,
                "cdate": 1700453354062,
                "tmdate": 1700453476511,
                "mdate": 1700453476511,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "glowkWZBU4",
                "forum": "qrGjFJVl3m",
                "replyto": "pOv6KPPABs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1807/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1807/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2REz (2/2)"
                    },
                    "comment": {
                        "value": "**[Analysis on model fairness, biases and other potential issues]**\n\nWe thank the reviewer for the suggestion. We acknowledge that in-depth analysises on models' fairness, biases and other potential issues can largely help the communities better understand our models and should be considered as an important part of our open source project. In this light, we follow previous vision-language models [1, 2] and evaluate the overall level of toxicity and profanity of Qwen-VL's generated captions.\n\nWe instruct Qwen-VL to generate descriptions for each image in FairFace dataset [3] val split, then the generated captions are scored by Perspective API [1] for their degree of toxicity and profanity. Three tables below depict the percentage of varying degree of toxicity or profanity splitted by subgroups related to race, age, and gender, respectively. The conclusions are:\n\n* Qwen-VL shows a general low degree of toxicity across all slices, and an extremly low level of profanity (more than $99$% captions' profanity are below $0.2$).\n* Taking $0.8$ as a threshold for both toxicity and profanity, Qwen-VL's outputs are under great satefy (not a single caption is toxic nor profane).\n* Compared to previous method, *i.e.*, PaLI-X, Qwen-VL is much more safe across all slices.\n\nThe last table compare our model with PaLI-X on the degree of toxicity and profanity. As the results shown, taken $0.2$ as the threshold, our Qwen-VL is more harmfulless than PaLI-X.\n\n| RACE  |   toxicity < 0.2 |   0.2 <= toxicity <= 0.8 |   0.8 < toxicity |   profanity < 0.2 |   0.2 <= profanity <= 0.8 |   0.8 < profanity |\n|:----------------|-----------------:|:-------------------------:|:-----------------:|:------------------:|:--------------------------:|:------------------:|\n| Black |   84.1% | 15.9% | 0% |    99.7% |   0.3% |  0% |\n| East Asian |   86.2% | 13.8% | 0% |    99.2% |   0.8% |  0% |\n| Indian|   80.5% | 19.5% | 0% |    99.7% |   0.3% |  0% |\n| Latino_Hispanic |   86.3% | 13.7% | 0% |    99.3% |   0.7% |  0% |\n| Middle Eastern  |   86.4% | 13.6% | 0% |    99.3% |   0.7% |  0% |\n| Southeast Asian |   86.7% | 13.3% | 0% |    99.2% |   0.8% |  0% |\n| White |   84.9% | 15.1% | 0% |    99%   |   1%   |  0% |\n\n| AGE|   toxicity < 0.2 |   0.2 <= toxicity <= 0.8 |   0.8 < toxicity |   profanity < 0.2 |   0.2 <= profanity <= 0.8 |   0.8 < profanity |\n|:-------------|:-----------------:|:-------------------------:|:-----------------:|:------------------:|:--------------------------:|:------------------:|\n| 0-2|   68.8% | 31.2% | 0% |   100%   |   0%   |  0% |\n| 3-9|   83.4% | 16.6% | 0% |    99%   |   1%   |  0% |\n| 10-19   |   86.4% | 13.6% | 0% |    99.2% |   0.8% |  0% |\n| 20-29   |   85.9% | 14.1% | 0% |    99.4% |   0.6% |  0% |\n| 30-39   |   85.5% | 14.5% | 0% |    99.4% |   0.6% |  0% |\n| 40-49   |   85.7% | 14.3% | 0% |    99.2% |   0.8% |  0% |\n| 50-59   |   83.4% | 16.6% | 0% |    99.6% |   0.4% |  0% |\n| 60-69   |   86.6% | 13.4% | 0% |    99.7% |   0.3% |  0% |\n| >= 70 |   75.4% | 24.6% | 0% |    99.2% |   0.8% |  0% |\n\n| GENDER   |   toxicity < 0.2 |   0.2 <= toxicity <= 0.8 |   0.8 < toxicity |   profanity < 0.2 |   0.2 <= profanity <= 0.8 |   0.8 < profanity |\n|:-------------|:-----------------:|:-------------------------:|:-----------------:|:------------------:|:--------------------------:|:------------------:|\n| Male|   86%   | 14%   | 0% |    99.6% |   0.4% |  0% |\n| Female   |   83.8% | 16.2% | 0% |    99.1% |   0.9% |  0% |\n\n| Model | - | Threshold | Ethnicity (Low/High) (&darr;) | Age (Low/High) (&darr;) | Gender (Low/High) (&darr;) |\n|:-------------|:-----------------:|:-------------------------:|:-----------------:|:------------------:|:------------------:|\n| PaLI-X | Toxicity | $0.2$ | $35.8$%/$40.4$% | $33.9$%/$40.0$% | - |\n| Qwen-VL | Toxicity | $0.2$ | $\\mathbf{13.3}$%/$\\mathbf{19.5}$% | $\\mathbf{13.4}$%/$\\mathbf{31.2}$% | $14.0$%/$16.2$% |\n| PaLI-X | Profanity | $0.2$ | $5.1$%/$8.5$% | $3.5$%/$10.3$% | - |\n| Qwen-VL | Profanity | $0.2$ | $\\mathbf{0.3}$%/$\\mathbf{1.0}$% | $\\mathbf{0.3}$%/$\\mathbf{1.0}$% | $0.4$%/$0.9$% |\n\n```text\n[1] Lees, Alyssa, et al. \"A new generation of perspective api: Efficient multilingual character-level transformers.\" Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2022.\n[2] Chen, Xi, et al. \"PaLI-X: On Scaling up a Multilingual Vision and Language Model.\" arXiv preprint arXiv:2305.18565 (2023).\n[3] K\u00e4rkk\u00e4inen, Kimmo, and Jungseock Joo. \"Fairface: Face attribute dataset for balanced race, gender, and age.\" arXiv preprint arXiv:1908.04913 (2019).\n```"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700453465980,
                "cdate": 1700453465980,
                "tmdate": 1700453775001,
                "mdate": 1700453775001,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YKkwYli1yM",
                "forum": "qrGjFJVl3m",
                "replyto": "pOv6KPPABs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1807/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1807/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking forward to your reply"
                    },
                    "comment": {
                        "value": "Dear reviewer 2REz,\n\nWe sincerely appreciate your efforts and valuable time spent reviewing our work, as well as your constructive contribution to improving the quality of our paper.\n\nHave our responses effectively addressed your concerns? If you still have any issues or new questions, please feel free to let us know so that we may continue the discussion."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700540750733,
                "cdate": 1700540750733,
                "tmdate": 1700540750733,
                "mdate": 1700540750733,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sNvWpABWE7",
                "forum": "qrGjFJVl3m",
                "replyto": "pOv6KPPABs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1807/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1807/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking for discussion before the period ends"
                    },
                    "comment": {
                        "value": "Dear Reviewer 2REz,\n\nThis is a kind reminder about the approaching due date for the author-reviewer discussion period. We have provided responses to your questions above, including the clarification of some misunderstandings, restatement of our novelty and contributions, and a comprehensive study toward the fairness and bias of our model. We also made some improvements to our manuscript following your suggestions.  Given the limited time remaining, we would be very grateful if you could take the invaluable time to review our responses. If our responses have addressed your concerns, we would be most appreciative if you could consider changing your initial rating. If you still have any remaining concerns, we are also glad to continue this discussion with you during this final window.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673282315,
                "cdate": 1700673282315,
                "tmdate": 1700673282315,
                "mdate": 1700673282315,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Q7EqLOlK8L",
            "forum": "qrGjFJVl3m",
            "replyto": "qrGjFJVl3m",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1807/Reviewer_8ZLo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1807/Reviewer_8ZLo"
            ],
            "content": {
                "summary": {
                    "value": "This paper showcase the qwen-vl as a versatile LMM, being able to perceive and understand both texts and images. The  qwen-vl series contains a multitask finetuned 7B model and a chatbox trained with interleaved data. The modeling is similar to flamingo but the trainable parts are different in different stage. The model achieves reasonable generalist scores."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) Very clear pretraining data size and mixture weights that helps the general audience get a sense of the pretraining distribution, though the paper uses some internal data, which is understandable\n\n(2) good ablation study over different parts, window attention for highres\n\n(3) good experiment setups that consider sufficient academic benchmarks, \n\n(4) well written and easy to follow"
                },
                "weaknesses": {
                    "value": "(1) seems missing generalist PaLI results. The PaLI-X authors also have multitask finetuned model for VQA and captioning mixtures separately.  \n\n(2) missing the design / motivation or ablation of which part being trained during different stage. The stage 2 unfreezes ViT is for adapting to higher solution?"
                },
                "questions": {
                    "value": "Please comment on the weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1807/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698887459608,
            "cdate": 1698887459608,
            "tmdate": 1699636110091,
            "mdate": 1699636110091,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cyaSv2Cz7g",
                "forum": "qrGjFJVl3m",
                "replyto": "Q7EqLOlK8L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1807/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1807/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8ZLo"
                    },
                    "comment": {
                        "value": "**[Miss PaLI results]**\n\nThanks for pointing this our. However, PaLI-X's multi-task finetuning remains treating different tasks (*e.g.*, image caption, VQA and etc.) seperately instead of as a whole (*i.e.*, one model to inference on all tasks). We are also glad to add more PaLI-X's multi-task finetuning results as specialist SOTAs for comparision.\n\n**[Motivation and ablation on the training part in different training stages]**\n\nFirst of all, we thank the reviewer for suggesting this ablation on the training part in different training stages. The motivation and intention of our choice on which part being tuned in different stage are listed as following:\n\n* For the first stage, where billion-scale image-text pairs are leveraged, we tune the vision part only due to: (i) Efficiency: Tuning solely the vision component is much more efficient than joint tuning both ViT and LLM. (ii) Potential for catastrophic forgetting: There exists a significant likelihood of the LLM suffering from catastrophic forgetting if we simultaneously fine-tune ViT and LLM without access to a dedicated pure-text corpus. Notebaly, compared to the text corpus used for LLM pre-training, text lengths in image-text pair dataset (*e.g.*, CC3M, CC12M, Laion, and etc.) are much more noisy and short. To overcome this catastrophic forgetting issue, in the second and third stages, we incorporate pure-text corpus when LLM is tuned.\n* For the second stage, the reasons behind jointly tuning ViT and LLM are two folds: (i) Additional Vision Tasks: We incorporate some additional vision tasks (*e.g.*, grounding and OCR) in this stage and all these tasks require fine-grained visual perception ability. To this end, we increase the input image resolution from $224$ to $448$ and tune ViT to adapt to this high resolution. (ii) Specific Output Requirements: Visual grounding asks the LLM to generate output in a specific behavior (*i.e.*, bounding box). Therefore, we choose to fune LLM to make this task doable.\n* In the third stage, the focus shifts primarily towards supervised fine-tuning for dialogue capacity, rather than introducing additional visual capacities (which have been incorporated into ViT during the previous two stages). Consequently, we opt to freeze the ViT component while keeping the remaining parts trainable.\n\nAs per your request, we conduct additional ablation studies on the training part during the second stage. Given the limited amount of time and resources available to us in the response period, we are only able to get these experiments done in a small scale. Specifically, we employe a LLM with $1.8$B parameters and CLIP-ViT-L/$14$ as vision encoder, the overall parameters in our model are about $2$B. We train this model on the same dataset composition as we used in our paper, in spite of much fewer iterations. During the first stage, we tune the model with a batch size of $30720$ for $8000$ steps. During the second stage, in addition to the default setup, we also try to train our model with either ViT or LLM being frozen. All these three models for the second stage are tuned for $6000$ steps with a batch size of $2048$.\n\n| Benchmark | Qwen-VL($2$B) | Freeze ViT in Stage$2$ | Freeze LLM in Stage$2$ |\n|:---|:---:|:---:|:---:|\n| Nocaps (CIDEr) | $114.7$ | $109.7$ | $99.3$ |\n| VQAv2-val (VQA Score) | $74.6$ | $68.9$ | $64.4$ |\n| OKVQA-val (VQA Score) | $45.7$ | $43.2$ | $32.7$ |\n| VizWiz-val (VQA Score) | $29.4$ | $22.5$ | $19.3$ |\n| TextVQA-val (VQA Score) | $52.5$ | $27.7$ |  $40.1$ |\n\nThere are several observations:\n\n* Compared to jointly tuning both ViT and LLM, freeze ViT or freeze LLM both lead to significant performance decrease.\n* On most benchmarks, freeze ViT shows better results than freeze LLM. This observation is in line with some current vision-language machines who prefer to tune LLM and keep the ViT frozen.\n* However, for text-oriented VQA(i.e, TextVQA), it's interesting to see that freeze LLM (tune ViT) shows significant higher accuracy compared to freeze ViT (tune LLM). We ascribe this phenomenon to text-oriented VQA requires more fine-grained visual perception capacity which has not been learnt in previous ViT training process(*e.g.*, vision-language contrastive learning in CLIP, and coarse-grained image captioning in the 1st stage)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700453285761,
                "cdate": 1700453285761,
                "tmdate": 1700453285761,
                "mdate": 1700453285761,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kRcmdqlBJh",
            "forum": "qrGjFJVl3m",
            "replyto": "qrGjFJVl3m",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1807/Reviewer_RE8P"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1807/Reviewer_RE8P"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes QWEN-VL, a series of large-scale vision-and-language models (LVLMs). Further details can be found in Strengths."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- S1: This is one of a few open-source models where the model weights are released (though I don\u2019t think the data is; there is also some \u201cin-house\u201d data; see Table 2). This can benefit the research community; the claim is that while the performance of QWEN-VL is still behind private models, it excels in the open-source community, especially in terms of capabilities it supports (Figures 4-7).\n\n- S2: The training pipeline (Figure 3) is sound and simple."
                },
                "weaknesses": {
                    "value": "- W1: Weak research significance and contributions. This work is a huge engineering effort and it is appreciated. However, research-wise, I am not convinced that it can provide any insights in terms of large-scale model training, architecture, or evaluation.\n\n- W2: Weak discussion of related work and clarity: To make W1 worse, the paper does not properly discuss the relevant work. If the paper would like to focus on the open-source aspect, I think it can expand this part much more heavily. What are the existing open-source LVLMs and what are \u201copen\u201d about them? What are the capabilities they support and so on? However, based on the current presentation this is unclear."
                },
                "questions": {
                    "value": "- Is the train-test overlap between benchmarks taken care of? Especially COCO-based datasets.\n\nPlease address the two points in my Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1807/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699257281491,
            "cdate": 1699257281491,
            "tmdate": 1699636109970,
            "mdate": 1699636109970,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MmiCjcg07V",
                "forum": "qrGjFJVl3m",
                "replyto": "kRcmdqlBJh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1807/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1807/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RE8P (1/2)"
                    },
                    "comment": {
                        "value": "**[Research contributions and insights]**\n\nWe thank the reviewer for valuable feedback. In addition to the contributions listed in our paper, we restate Qwen-VL's contribution, technical insights, and novelty point by point in the general response. We hope that our detailed reply will ease your concerns.\n\n**[Discussions of open-source work]**\n\nWe thank your kind suggestion. In the below table, we compare Qwen-VL with previous open-source large vision-language models (LVLMs) in terms of their supporting language and capable task. As shown, Qwen-VL can support both English and Chinese and is capable to finish four kinds of vision-language tasks within the same model. Moreover, as mentioned in our paper, Qwen-VL also outperforms these predecessors across several benchmarks on these four tasks. Based on the versatile ability of Qwen-VL and its superior performance on a wide range of benchmarks, we believe that Qwen-VL can serve as a strong foundation for both future research and application purpose.\n\n*We trust our response can make the capabilities of previous LVLMs much more clear. We also include this discussion in our revised paper.*\n\n> Note: For text-oriented VQA, we refer to whether the model is designed or optimized to tackle this problem explicitly.\n\n| Model          | English | Chinese | Caption | General VQA | Text-oriented VQA | Grounding |\n|----------------|:---------:|:---------:|:---------:|:-----:|:------------:|:-----------:|\n| Kosmos         | $\\checkmark$ |         | $\\checkmark$ | $\\checkmark$ |            |           |\n| BLIP2          | $\\checkmark$ |         | $\\checkmark$ | $\\checkmark$ |            |           |\n| LLaVA          | $\\checkmark$ |         | $\\checkmark$ | $\\checkmark$ |            |           |\n| MiniGPT-4      | $\\checkmark$ |         | $\\checkmark$ | $\\checkmark$ |            |           |\n| ChatGLM        | $\\checkmark$ | $\\checkmark$ | $\\checkmark$ | $\\checkmark$ |            |           |\n| mPLUG-Owl      | $\\checkmark$ |         | $\\checkmark$ | $\\checkmark$ |            |           |\n| InstructBLIP   | $\\checkmark$ |         | $\\checkmark$ | $\\checkmark$ | $\\checkmark$ |           |\n| mPLUG-DocOwl   | $\\checkmark$ |         | $\\checkmark$ | $\\checkmark$ | $\\checkmark$ |           |\n| Kosmos2        | $\\checkmark$ |         | $\\checkmark$ | $\\checkmark$ |            | $\\checkmark$ |\n| Shikra         | $\\checkmark$ |         | $\\checkmark$ | $\\checkmark$ |            | $\\checkmark$ |\n| Qwen-VL        | $\\checkmark$ | $\\checkmark$ | $\\checkmark$ | $\\checkmark$ | $\\checkmark$ | $\\checkmark$ |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700453162889,
                "cdate": 1700453162889,
                "tmdate": 1700453162889,
                "mdate": 1700453162889,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hbF7vVnNbF",
                "forum": "qrGjFJVl3m",
                "replyto": "kRcmdqlBJh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1807/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1807/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RE8P (2/2)"
                    },
                    "comment": {
                        "value": "**[Train-test overlap]**\n\nWe appreciate the reviewer for highlighting the potential image contamination between our training dataset and various evaluation benchmarks, notably the coco-based benchmarks. To assess and address this concern, we conducted two contrast experiments. However, due to constraints in both time and resources within the response period, these experiments were performed on a smaller scale. Below, we outline the setups of our experiments:\n\n* We use a LLM with $1.8$B parameters and CLIP-ViT-L/$14$ as vision encoder. Total parameters are about $2$B.\n* We perform the first and second training stages, and evaluate the models on several caption/VQA/Grounding benchmarks.\n* For the first stage, we train the model with a batch size of $30720$ for $8000$ iterations. In other words, the total consumed image-text pairs are about $245.76$M.\n* For the second stage, we train our model with a batch size of $2048$ for $6000$ steps.\n\nFor the baseline experiment, we use exact the same composition of datasets as in our paper. For the second experiment, we follow previous work to perform near-deduplication. Specifically, we deduplicate all coco images used in some coco-based evaluation benchmarks (*e.g.*, VQA, OKVQA, Refcoco/Refcoco+/Refcocog). The final results are shown in table below:\n\n| Benchmark | Is COCO-based? | Qwen-VL($2$B) | Qwen-VL($2$B) + Data Deduplication |\n|:---|:---:|:---:|:---:|\n| Nocaps (CIDEr) | | $114.7$ | $114.3$ |\n| VQAv2-val (VQA Score) | $\\checkmark$ | $74.6$ | $74.3$ |\n| OKVQA-val (VQA Score) | $\\checkmark$ | $45.7$ | $45.9$ |\n| VizWiz-val (VQA Score) | | $29.4$ | $29.8$ |\n| TextVQA-val (VQA Score) | | $52.5$ | $52.5$ |\n| RefCOCO-val (Accuracy) | $\\checkmark$ | $79.9$ | $79.6$ |\n| RefCOCO+-val (Accuracy) | $\\checkmark$ | $66.1$ | $66.5$ |\n| RefCOCOg-val (Accuracy) | $\\checkmark$ | $74.2$ | $73.6$ |\n\nFrom the results, we observe little to no performance perturbation between two experiments for both coco-based and non coco-based benchmarks, which demonstrates the reliability of our results. We also appreciate the reviewer's professional comment.\n\n**[In-house data]**\n\nWe thank the reviewer for approving our efforts in opening source our models. And we would like to further explain that despite some in-house data is indeed included in our training process (*e.g.*, Chinese image-caption dataset in Table $2$), we have made extensive efforts to provide detailed descriptions on construction and pretreating process of the datasets, the training mixture, and how the models are trained. We believe the information can be very helpful to experts who would like to reproduce our dataset construction as well as the whole training process at the same scale. Moreover, we are actively engaged in efforts to make additional information and data publicly available in the near future."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700453203737,
                "cdate": 1700453203737,
                "tmdate": 1700453249206,
                "mdate": 1700453249206,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hjYhoZlHz8",
                "forum": "qrGjFJVl3m",
                "replyto": "kRcmdqlBJh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1807/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1807/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Do our responses address your concerns?"
                    },
                    "comment": {
                        "value": "Dear reviewer RE8P,\n\nWe appreciate your efforts and time in reviewing our work and participating in the rebuttal process to improve the quality of our paper.\n\nHave our responses adequately addressed your concerns? If you still have any issues with our responses or if there are any new questions, we are more than willing to continue the discussion with you."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700540611669,
                "cdate": 1700540611669,
                "tmdate": 1700540611669,
                "mdate": 1700540611669,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pd79jvlQ2q",
                "forum": "qrGjFJVl3m",
                "replyto": "kRcmdqlBJh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1807/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1807/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Looking for discussion before the discussion period ends"
                    },
                    "comment": {
                        "value": "Dear Reviewer RE8P,\n\nThis is a kind reminder about the approaching due date for the author-reviewer discussion period. We have provided responses to your questions above and made some improvements to our manuscript following your suggestions. Given the limited time remaining, we would be very grateful if you could take the invaluable time to review our responses. If our responses have addressed your concerns, we would be most appreciative if you could consider changing your initial rating. If you still have any remaining concerns, we are also glad to continue this discussion with you during this final window.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1807/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672945801,
                "cdate": 1700672945801,
                "tmdate": 1700672945801,
                "mdate": 1700672945801,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]