[
    {
        "title": "Evaluating graph generative models with graph kernels: what structural characteristics are captured?"
    },
    {
        "review": {
            "id": "yzkW0BAW6V",
            "forum": "UjYeectI4p",
            "replyto": "UjYeectI4p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2368/Reviewer_oaCD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2368/Reviewer_oaCD"
            ],
            "content": {
                "summary": {
                    "value": "This paper claims that graph kernels are commonly used to evaluate the performance of graph generative models. The paper then investigates whether some standard graph kernels can capture structural properties of graphs such as the degree distribution, the presence of community structure and others. The empirical results demonstrate that the shortest path and graphlet kernels can better capture the considered properties than the rest of the kernels."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This is an interesting paper. Even though graph kernels have been extensively studied for two decades, it is still not clear what graph properties are encoded into the explicit (or implicit) representations created by those methods. This work actually complements [1] which theoretically investigates whether graph kernels can capture properties such as triangle-freeness, connectivity and planarity.\n\n- The work is useful to practitioners since the reported results suggest that some kernels can better capture some properties than other kernels. Thus, if a practitioner is interested in a specific property, this work can help them choose the right kernel.\n\n- The presentation is reasonably clear.\n\n[1] Kriege, N. M., Morris, C., Rey, A., & Sohler, C. \"A Property Testing Framework for the Theoretical Expressivity of Graph Kernels\". In Proceedings of the 27th International Joint Conference on Artificial Intelligence, pp. 2348-2354, 2018."
                },
                "weaknesses": {
                    "value": "- The paper claims that graph kernels are commonly used to evaluate the performance of graph generative models, however, no references are given. The authors should add some references and also motivate the use of graph kernels for this task. What are the advantages of graph kernels over competing evaluation approaches?\n\n- The paper focuses on 7 graph properties, but it is not clear why those 7 specific properties were chosen and not others. Are there any applications of graph generative models where graphs that exhibit those properties need to be generated? Please provide more details.\n\n- Since no theoretical results are provided, I would expect the authors to provide some explanation or intuitions about why some kernels fail to capture some of the considered properties. An explanation is provided in the case of the WL kernel, but not for all the kernels. \n\n- The experimental evaluation is not fully convincing because all the generated graphs consist of 50 nodes and 190 edges (in expectation). It is not thus clear whether the results generalize to graphs of different sizes. I would suggest the authors construct other datasets where graphs are of different size and different density."
                },
                "questions": {
                    "value": "- In page 9, it is mentioned that \"there are only two different graphlets of size k=3: wedges and triangles\". This is true if connected graphlets are only considered. However, the standard graphlet kernel also counts disconnected graphlets and there are 4 such graphlets for k=3. Did the authors use an implementation that counts connected graphlets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2368/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698502614729,
            "cdate": 1698502614729,
            "tmdate": 1699636169567,
            "mdate": 1699636169567,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0XKQl3eADr",
                "forum": "UjYeectI4p",
                "replyto": "yzkW0BAW6V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2368/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2368/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the thorough review and useful suggestions! We address your concerns below.\n\n**W4. The experimental evaluation is not fully convincing because all the generated graphs consist of 50 nodes and 190 edges (in expectation). It is not thus clear whether the results generalize to graphs of different sizes. I would suggest the authors construct other datasets where graphs are of different size and different density.**\n\nThank you for this suggestion, the experiments on larger graphs (1000 nodes) are currently running, we will update the paper when they are ready. \n\nThe main reason that we focus on small graphs, is that these methods (evaluating graph generative models using graph kernels) are typically applied to relatively small networks. For example, in molecular modeling, the graphs consist of a relatively small number of atoms. Other applications where small graphs are relevant include modeling ego networks in sociology.\n\nAdditionally, let us note that not all kernels are scalable: for instance, the graphlet kernel has a prohibitively high computational cost. We chose these smaller graphs so that we can include all popular graph kernels in our experiment.\n\nAlso, as suggested, we conduct additional experiments where we consider density as a characteristic that we vary. The results are the following:\n\n| Kernel | Density |\n|---------|----------|\n| SP | 0.993 / 0.996 |\n|WL | 0.996 / 0.996 |\n|WL-OA | 0.996 / 0.996 |\n| Graphlet-3 | 0.996 / 0.996 |\n| NSPDK | 0.389 / 0.341 |\n| PM | 0.982 / 0.979 |\n| NetLSD | 0.996 / 0.996 |\n| RandGIN | 0.903 / 0.973 |\n\nWe see that detecting density is an easy task for most of the kernels, while there are some exceptions, e.g., NSPDK.\n\n**W1. The paper claims that graph kernels are commonly used to evaluate the performance of graph generative models, however, no references are given. The authors should add some references and also motivate the use of graph kernels for this task. What are the advantages of graph kernels over competing evaluation approaches?**\n\nWe refer to Section 2.2 which discusses the evaluation of graph generative models. In particular, O\u2019Bray et al. (2022) focus on this pipeline of evaluating generative models (but they use very simple kernels). Thompson et al. (2022) go beyond kernel-based evaluation, but their proposed approach can also be fitted to the kernel-based framework as we do in the paper.\n\n**W2. The paper focuses on 7 graph properties, but it is not clear why those 7 specific properties were chosen and not others. Are there any applications of graph generative models where graphs that exhibit those properties need to be generated? Please provide more details.**\n\nWe have chosen high-level properties that are meaningful, diverse, and easy to interpret. Of course, one can expect a good graph kernel to be sensitive to all of them, but there is usually a trade-off. Regarding potential applications, for molecular modeling it can be important that the measure is sensitive to clustering or complementarity as the presence of certain small substructures is crucial. For ego-network modeling, heterogeneity, and community structure are important.\n\n**W3. Since no theoretical results are provided, I would expect the authors to provide some explanation or intuitions about why some kernels fail to capture some of the considered properties. An explanation is provided in the case of the WL kernel, but not for all the kernels.**\n\nLet us note that most of the kernels are intractable for theoretical analysis. For Graphlet3, we might be able to provide the expected values of kernel values, but getting the variances is already much less tractable. Regarding the intuitive explanations, we will try to improve this part in the revised version. \n\n**Q1. In page 9, it is mentioned that \"there are only two different graphlets of size k=3: wedges and triangles\". This is true if connected graphlets are only considered. However, the standard graphlet kernel also counts disconnected graphlets and there are 4 such graphlets for k=3. Did the authors use an implementation that counts connected graphlets?**\n\nYes, we use the GraKeL implementation that counts connected graphlets."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2368/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700178314756,
                "cdate": 1700178314756,
                "tmdate": 1700178492873,
                "mdate": 1700178492873,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Mf5y7U6jpb",
                "forum": "UjYeectI4p",
                "replyto": "0XKQl3eADr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2368/Reviewer_oaCD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2368/Reviewer_oaCD"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for their response. However, I feel that my concerns have not been properly addressed.\n\nW4: I think there has been a misunderstanding. I never implied that the size of the graphs is small. The results are not convincing because all the generated graphs consist of the same number of nodes and edges. Thus, there is no diversity in the constructed dataset. I suggest the author experiment with graphs of various sizes and densities.\n\nW2: I actually meant application papers that use graph kernels to evaluate the proposed models. As far as I know, most papers use MMD to evaluate the generated graphs (see [1]) or application-specific metrics such as validity in chemoinformatics (see [2]).\n\nW3: This is really important for understanding the limitations of the different kernels. Such an analysis would greatly strengthen the paper. \n\n[1] Garg, S., Dhamo, H., Farshad, A., Musatian, S., Navab, N., & Tombari, F., Unconditional scene graph generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 16362-16371, 2021.\\\n[2] Luo, Y., Yan, K., & Ji, S., Graphdf: A discrete flow model for molecular graph generation. In International Conference on Machine Learning, pp. 7192-7203, 2021."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2368/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700598370467,
                "cdate": 1700598370467,
                "tmdate": 1700598370467,
                "mdate": 1700598370467,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VTRdO3Z2wq",
            "forum": "UjYeectI4p",
            "replyto": "UjYeectI4p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2368/Reviewer_7Dfc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2368/Reviewer_7Dfc"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a graph similarity technique based on graph kernels and their capacity to capture high-level structural properties. The properties considered by the paper include: degree distribution, community structure, latent geometry, and others. The paper presents a continuous transitions between the models and measures the sensitivity of the graph kernels to the changes.\nThe experiments show this property \u2014 that different graph kernels have different sensitivity with Shortest Path kernel and Graphlet kernel showing the best performance in terms of capturing graph properties."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Among the strengths this paper has is that the proposed model is relatively easy to follow and the writing and organization are relatively good. Some aspects are subtle due to the nature of the kernel organization and abstraction of features. The most relevant positive aspect is the number of baselines used, and the transparency of the global architecture comparison. That is, this paper could serve as a reference of graph kernel performance across metrics, which is in itself an interesting idea."
                },
                "weaknesses": {
                    "value": "However, the paper comes with several weaknesses. The most noticeable one is the nature of the graphs used for evaluation. With n = 50 nodes and m = 190 edges, the size of the graphs is hardly what we see in practice, where datasets contain [hundreds of] thousands of nodes and many more edges. While this does not disqualify this paper's potential application, it limits the applicability of the insights provided. Other considerations could be improved in the paper. For instance, the Chung Lu models require the use of very heavy parameterization where every node's weight is used for modeling the degree distribution of a graph. Another weakness is that the concept of dimensionality, in page 5, is introduced in the paper in a very general manner and without sufficient rigor to incorporate it in the framework. Thus, something like a pseudocode could be useful to clarify some of these imprecisions."
                },
                "questions": {
                    "value": "What would be your main argument to ensure the method is generalizable to larger graphs?\n\nWhat is the effect of using Chung Lu models computationally?\n\nCan you describe algorithmically how you incorporated/modeled dimensionality in your framework?\n\nHow did you encoded these graph characteristics in the kernels?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2368/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698638886550,
            "cdate": 1698638886550,
            "tmdate": 1699636169479,
            "mdate": 1699636169479,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "z0O2lBQOrD",
                "forum": "UjYeectI4p",
                "replyto": "VTRdO3Z2wq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2368/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2368/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the review! We address your concerns below.\n\n**W1. With n = 50 nodes and m = 190 edges, the size of the graphs is hardly what we see in practice, where datasets contain [hundreds of] thousands of nodes and many more edges. While this does not disqualify this paper's potential application, it limits the applicability of the insights provided.**\n\nThank you for this suggestion, the experiments on larger graphs (1000 nodes) are currently running, we will update the paper when they are ready. Let us note that graphs significantly larger than this size are rarely considered in settings we focus on, where one needs to compare two sets of graphs (and not just two graphs).\n\nThe main reason that we focus on small graphs, is that these methods (evaluating graph generative models using graph kernels) are typically applied to relatively small networks. For example, in molecular modeling, the graphs consist of a relatively small number of atoms. Other applications where small graphs are relevant include modeling ego networks in sociology.\n\nAdditionally, let us note that not all kernels are scalable: for instance, the graphlet kernel has a prohibitively high computational cost. We chose these smaller graphs so that we can include all popular graph kernels in our experiment.\n\n**Q1. What would be your main argument to ensure the method is generalizable to larger graphs?**\n\nTo extend our previous reply, let us also note that our method (framework for comparison of graph kernels) is applicable to graphs of any size. The only bottleneck here is that not all popular kernels are computationally efficient.\n\n**W2. For instance, the Chung Lu models require the use of very heavy parameterization where every node's weight is used for modeling the degree distribution of a graph.** and **Q2. What is the effect of using Chung Lu models computationally?**\n\nAs we describe in Section 3.2, for the Chung Lu model the weights are drawn from a Pareto distribution with some power-law exponent. Thus, this version of the Chung-Lu model takes only one additional parameter, which is used for the interpolation. Then, a naive implementation for generating a graph requires O(n^2) time (and there exist more efficient approaches). Note that this is faster than computing many graph kernels, thus this part is not a bottleneck of our analysis.\n\n**W3. Another weakness is that the concept of dimensionality, in page 5, is introduced in the paper in a very general manner and without sufficient rigor to incorporate it in the framework. Thus, something like a pseudocode could be useful to clarify some of these imprecisions.** and **Q3. Can you describe algorithmically how you incorporated/modeled dimensionality in your framework?**\n\nTo model latent geometry, we consider a random geometric graph, as described in \u201cLatent geometry\u201d above. The only modification we make in \u201cDimensionality\u201d is varying the height of the torus from 1 to 0. We will add more details to the text, but if you have any specific questions - please, tell us as it will help us to improve the readability. Note that the code of our models and transformations is also available.\n\n**Q4. How did you encoded these graph characteristics in the kernels?**\n\nLet us note that we take popular graph kernels and do not modify them in any way. Our aim is to assess what graph characteristic each kernel is sensitive to."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2368/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700178719110,
                "cdate": 1700178719110,
                "tmdate": 1700178719110,
                "mdate": 1700178719110,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "l9K2lbHnJb",
                "forum": "UjYeectI4p",
                "replyto": "z0O2lBQOrD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2368/Reviewer_7Dfc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2368/Reviewer_7Dfc"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the reply"
                    },
                    "comment": {
                        "value": "Thank you for your answers. The comments to Q1, W2, W3, and Q4 do clarify my concerns. However, there is a need for incorporate this to the text of the paper. This will substantially improve the paper. However, I feel that W1 and the overall weakness of the evaluation, i.e., not sufficient evidence that shows applicability, make me hesitant to raise my score. I do appreciate the ideas proposed in the paper and think that it has great potential. However, the paper is still not ready for publication."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2368/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677409156,
                "cdate": 1700677409156,
                "tmdate": 1700677409156,
                "mdate": 1700677409156,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GVCIsfYvnZ",
            "forum": "UjYeectI4p",
            "replyto": "UjYeectI4p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2368/Reviewer_3Bx8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2368/Reviewer_3Bx8"
            ],
            "content": {
                "summary": {
                    "value": "The paper generate random graphs with different models and compute different kernels of the graphs. It then show how the kernels are sensitive to the interpolations of parameters between generative models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- I think the question is reasonable to ask and the paper is an attempt in the right direction."
                },
                "weaknesses": {
                    "value": "- I find the paper shows some relationship between generative models of graphs and kernels, it is hard to see what are the applications of these findings in real life. The conclusion is rather qualitative.\n- One of the key step of the paper is to interpolate between generative models, its formula is not clearly stated. It seems to \"interpolate\" probabilities of different models, and the idea of \"interpolating graph characteristics\" has to be properly defined as the characteristics are not mutually exclusive. It is difficult to understand what it means by having \"100%\" of a characteristic and \"0%\" of another when the two are related. \n- The \"performance\" of kernels is difficult to grasp and it is related to what is needed to be performed as in also the last point. \nOverall, I'd suggest to go on this direction with a clear, deeper study of what exactly are the different between generative models. At the moments, it doesn't show concrete conclusion yet for any practical purpose."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2368/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698644757183,
            "cdate": 1698644757183,
            "tmdate": 1699636169377,
            "mdate": 1699636169377,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "igB9tjvMGh",
                "forum": "UjYeectI4p",
                "replyto": "GVCIsfYvnZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2368/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2368/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the review! We address your concerns below.\n\n**W1. I find the paper shows some relationship between generative models of graphs and kernels, it is hard to see what are the applications of these findings in real life. The conclusion is rather qualitative.**\n\nKernels can be used to evaluate graph generative models. However, some kernels are insensitive to certain graph characteristics. For example, our experiments show that the widely-used Weisfeiler-Lehman kernel is insensitive to geometry. In turn, geometry can be especially relevant, e.g., for the application of molecular modeling because atoms have spatial locations. Such findings help practitioners decide which kernels they should and shouldn't use for their evaluations.\n\n**W2. One of the key step of the paper is to interpolate between generative models, its formula is not clearly stated. It seems to \"interpolate\" probabilities of different models, and the idea of \"interpolating graph characteristics\" has to be properly defined as the characteristics are not mutually exclusive. It is difficult to understand what it means by having \"100%\" of a characteristic and \"0%\" of another when the two are related.**\n\nWe agree that varying one characteristic may also affect other graph properties as it can be hard (or even impossible) to vary only one property in isolation. For each property, we have chosen a graph generator that produces graphs in which this property is strongly present (we try to consider the simplest possible model). Then, we fix a starting point (usually the ER graph) and an ending point (a given model where the property is present to some extent) and make a transition between the two. Section 3.2 states how the parameters of the graph generators depend on the interpolation parameter. If some particular transitions are not clear, we are happy to clarify.\n\n**W3. The \"performance\" of kernels is difficult to grasp and it is related to what is needed to be performed as in also the last point. Overall, I'd suggest to go on this direction with a clear, deeper study of what exactly are the different between generative models. At the moments, it doesn't show concrete conclusion yet for any practical purpose.**\n\nFollowing your suggestions, we will extend the discussion to clearly state some practical conclusions that follow from our analysis. For instance, we observe that random GIN is insensitive to communities, while WL is insensitive to latent geometry. If the generator is supposed to generate graphs that match a certain sample of graphs in terms of having a similar community structure or geometry, then these kernels should be avoided."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2368/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700179293403,
                "cdate": 1700179293403,
                "tmdate": 1700179293403,
                "mdate": 1700179293403,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9kMTjqRFS2",
                "forum": "UjYeectI4p",
                "replyto": "igB9tjvMGh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2368/Reviewer_3Bx8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2368/Reviewer_3Bx8"
                ],
                "content": {
                    "title": {
                        "value": "Appreciate the response"
                    },
                    "comment": {
                        "value": "Thanks to the author for the reply. \n\nI find that the transition betweens graph properties, which are the target to find suitable kernels, is not justified in the properties it retains or losses, nor in its usefulness. I will keep my current evaluation."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2368/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700628293191,
                "cdate": 1700628293191,
                "tmdate": 1700628293191,
                "mdate": 1700628293191,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8gKYH99OVy",
            "forum": "UjYeectI4p",
            "replyto": "UjYeectI4p",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2368/Reviewer_nxnq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2368/Reviewer_nxnq"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on identifying the topological or structural features captured by different graph kernels, enabling them to be compared, in the context of generative graph models.\n\nThe authors consider the Erd\u00f6s-R\u00e9nyi model as a baseline from which the features under consideration are absent. They propose different ways of departing from this model by randomly adding one of the features under consideration to the graph. For each feature (e.g. degree heterogeneity), they obtain a probabilistic graph model parameterized by $\\theta$, where $\\theta=0$ corresponds to the absence of the feature (Erd\u00f6s-R\u00e9nyi model), and $\\theta=1$ to maximum presence.\n\nThe sensitivity of a kernel to each of these features is then numerically evaluated by Spearman's correlation between $\\theta$ and maximum mean discrepancy along the interval with respect to each of the two endpoints.\n\nNumerical results show that most kernels identify some features easily and others less well, although the Shortest Path kernel and the Graphlet kernel have good performance on all models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The subject covered in this article is certainly very interesting, and in my opinion goes beyond the question of generative graph models.\n\nThe methodology employed seems to me original and could be reapplied to other contexts.\n\nThe observations made on the various kernels provide further insight into their ability to detect certain topological features, which may already be valuable for practitioners.\n\nLast but not least, the paper is well written and pleasant to read."
                },
                "weaknesses": {
                    "value": "From my point of view, the paper's main weakness lies in the numerical part: the only results presented concern graphs with 50 nodes and 190 edges (on average). We would like to know whether the results obtained are robust when we vary the size of the graph (in number of nodes and number of edges).\n\nAnother limitation is that the results obtained depend on the probabilistic models chosen. For instance, each may introduce random features other than the one targeted, which may bias the results obtained. Figure 2 gives some interesting examples of this phenomenon. This may be an intrinsic limitation of the approach, but it can be overcome by considering different probabilistic models for the same feature."
                },
                "questions": {
                    "value": "In addition to the comments above, I think the points below should be addressed.\n\nTo have a self-contained paper, the Erd\u00f6s-R\u00e9nyi and Chung-Lu models should be described.\n\nFor some models, the authors explain precisely the link between the parameters and the average number of edges (triadic closure in Appendix B.1 and dimensionality in Appendix B.2). For others (e.g. latent geometry) there is no explanation at all.\n\nAs a reader, we are left a little disappointed by the two correlation measures. A single indicator would greatly simplify the statement of results. The data suggest that, in most cases, they are equal. For features such as the triadic model and for the Graphlet kernel, the link between the two correlations might be studied theoretically, which would provide a valuable piece of information on this question.\n\nOn page 2, one reads that computing a distance is as hard as graph isomorphism, while a few lines down, it says that any kernel can be easily transformed to a distance measure. Can the authors address this apparent contradiction?\n\nTypos:\n\npage 2: popular\n\npage 3: $\\mathcal{G}_i$ is not defined\n\npage 4: triadic model in Appendix B.1\n\npage 8: colors in Table 1?\n\npage 8: 4.1 Results?\n\npage 13 (B.2): $h\\geq 2h$\n\npage 13 (B.2): Solving... $r^=$"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2368/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698756120183,
            "cdate": 1698756120183,
            "tmdate": 1699636169311,
            "mdate": 1699636169311,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xqDwJP62DH",
                "forum": "UjYeectI4p",
                "replyto": "8gKYH99OVy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2368/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2368/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the thorough review and useful suggestions! We will improve the paper accordingly. We address the questions and concerns below.\n\n**W1. The only results presented concern graphs with 50 nodes and 190 edges (on average). We would like to know whether the results obtained are robust when we vary the size of the graph (in number of nodes and number of edges).**\n\nThank you for this suggestion, the experiments on larger graphs (1000 nodes) are currently running, we will update the paper when they are ready.\n\nThe main reason that we focus on small graphs, is that these methods (evaluating graph generative models using graph kernels) are typically applied to relatively small networks. For example, in molecular modeling, the graphs consist of a relatively small number of atoms. Other applications where small graphs are relevant include modeling ego networks in sociology.\n\nAdditionally, let us note that not all kernels are scalable: for instance, the graphlet kernel has a prohibitively high computational cost. We chose these smaller graphs so that we can include all popular graph kernels in our experiment.\n\n**W2. Another limitation is that the results obtained depend on the probabilistic models chosen. For instance, each may introduce random features other than the one targeted, which may bias the results obtained. Figure 2 gives some interesting examples of this phenomenon. This may be an intrinsic limitation of the approach, but it can be overcome by considering different probabilistic models for the same feature.**\n\nWe agree with this comment and plan to add a clarification about this limitation in the revised text. Our aim was to model each of the properties in the most straightforward way, and in a way that minimally affects other graph characteristics. For some graph characteristics, it can be challenging (or even impossible) to vary them in isolation from everything else.\n\nThe idea of considering different models for the same property is interesting - we already do this for clustering: this property can be modeled via triadic closure, latent geometry, or community structure. Other possible variations we can think of are: modeling complementarity by disassortative community structure and modeling degree inhomogeneity by preferential attachment. We consider adding these experiments in the revised version of the paper. However, we may not have time to finalize them during the discussion period as we also need to conduct experiments on larger graphs, as discussed above.\n\n**Q1. To have a self-contained paper, the Erd\u00f6s-R\u00e9nyi and Chung-Lu models should be described.**\n\nThank you for pointing this out, we will add the definitions in the revised version.\n\n**Q2. For some models, the authors explain precisely the link between the parameters and the average number of edges (triadic closure in Appendix B.1 and dimensionality in Appendix B.2). For others (e.g. latent geometry) there is no explanation at all.**\n\nThe result for latent geometry also follows from Appendix B.2 by setting h=1. We will add a reference to Appendix B.2 to the paragraph about latent geometry.\n\n**Q3. As a reader, we are left a little disappointed by the two correlation measures. A single indicator would greatly simplify the statement of results. The data suggest that, in most cases, they are equal.**\n\nThank you for this comment - we agree that having two numbers can be somewhat inconvenient and that they are in most cases consistent. We decided to simplify the presentation by reporting the average value in the main table (or average\u00b1difference/2). We will update the table in the revised version.\n\n**Q4. On page 2, one reads that computing a distance is as hard as graph isomorphism, while a few lines down, it says that any kernel can be easily transformed to a distance measure. Can the authors address this apparent contradiction?**\n\nThanks for noticing! As we write above, graph \u2018distances\u2019 usually violate the positivity axiom, and if we apply the mentioned transformation - positivity can also be violated. We will clarify this in the text.\n\nWe hope that our response addresses your concerns. We are happy to answer any further questions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2368/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700179992400,
                "cdate": 1700179992400,
                "tmdate": 1700179992400,
                "mdate": 1700179992400,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mGhHND7933",
                "forum": "UjYeectI4p",
                "replyto": "8gKYH99OVy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2368/Reviewer_nxnq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2368/Reviewer_nxnq"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for their response.\n\nI am curious to see what additional numerical results they will be able to offer to the paper, both on larger graphs and on the study of one feature from several models.\n\nIn my opinion, with such further outcomes, the paper could be published at the conference.\n\nI also agree with Reviewer oaCD: beyond the size of the graphs, it is important to see the results over a wide range of graphs. It could be the case with the new experiments if they do not impose a single number of edges in the simulated graphs.\n\nEDIT: Comments from other reviewers are taken into account."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2368/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576446704,
                "cdate": 1700576446704,
                "tmdate": 1700637618375,
                "mdate": 1700637618375,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]