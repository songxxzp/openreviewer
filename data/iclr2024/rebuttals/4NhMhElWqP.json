[
    {
        "title": "DAM: A Foundation Model for Forecasting"
    },
    {
        "review": {
            "id": "hnKVZkz7W6",
            "forum": "4NhMhElWqP",
            "replyto": "4NhMhElWqP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission418/Reviewer_PhNp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission418/Reviewer_PhNp"
            ],
            "content": {
                "summary": {
                    "value": "The author proposes a foundation model for time series forecasting called DAM (deep data-dependent approximate analytical model). \nDAM takes input time series data and samples (time, value) pairs using HSR (history sampling regime) technique. HSR is not a regular and fixed-length sampling method unlike other time series forecasting techniques. Instead, it defines a probability distribution that is closer to the top-head section as the sampling points are closer to the prediction target point, and closer to the long-tail section as they are farther away. The author claims that this method is a major difference from other time series forecasting models and prevents overfitting and improves generalization performance. The sampled (time, value) pairs and the coefficient values initialized by the linear solver are used in the transformer to determine the coefficient values of 219 basis functions. The final predicted time-series is a combination of the basis functions. \nThe author evaluated 12 benchmark datasets used at training and 2 datasets not used at training (for validation of generalization performance), and compared them with 6 existing sota models trained specifically for each dataset, showing better or similar performance. In particular, DAM showed excellent performance in long-term forecasting. Additionally, DAM can adjust the trade-off between inference cost and performance by changing the context size, and also provides interpretabilty features based on basis function composition and attention map analysis."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. Addressing foundation model for time-series forecasting model is a significant and timely task (and it is a very difficult problem). \n\n2. The irregular and non-fixed size sampling technique is interesting.\n\n3. Various and numerous experiments were conducted to verify the ability of DAM."
                },
                "weaknesses": {
                    "value": "1. To be a foundation model, it is necessary to verify that the computational amount and performance follow the scaling law, but there is no related content. Even if the computational amount and performance do not follow the scaling law in time series forecasting, the authors should have explained why they had to use 10 datasets to train DAM (not single dataset or many more), and what was the bottleneck for improving the foundation model performance.\n\n2. The paper too much depends on appendix. The content provided in the main paper should be a complete argument, but it is difficult to follow without looking at the appendix. For example, in 4.1 Results and discussion, the authors need to summarize the content of Table 1 and argue the authors\u2019 claim, but it is omitted. Instead, the content of the appendix is summarized. To be a regular paper, it seems that the structure of the paper needs to be more organized.\n\n3. One of the most important abilities of a time series forecasting model is peak time prediction, but as the authors pointed out in Limitations, DAM does not seem to predict well. This limits the role of DAM in real-world problems."
                },
                "questions": {
                    "value": "1. According to the description of Table 1, DAM performed multivariate forecasting. I guess that when creating (time, value) pairs, multiple variables were included in the value part. Is that right? If so, I think it should be more clarified in the paper.\n\n2. At Figure 6, it seems that the inference time can be reduced from 200ms to below 100ms depending on the context size. However, the inference time and cost seem to be very small compared to LLM (~seconds), so it seems possible to reduce costs by increasing the computing resources not expensively at around 200ms. I don\u2019t fully understand what the authors want to deliver in 5.1 FLEXIBLE INFERENCE COST. Could you explain more about it? (sorry)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission418/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission418/Reviewer_PhNp",
                        "ICLR.cc/2024/Conference/Submission418/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission418/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698836572118,
            "cdate": 1698836572118,
            "tmdate": 1700694484513,
            "mdate": 1700694484513,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Z1MbuBIHXt",
                "forum": "4NhMhElWqP",
                "replyto": "hnKVZkz7W6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission418/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission418/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 1/2"
                    },
                    "comment": {
                        "value": "Thank you for your insightful review. Please note that we updated the paper shortly after the rebuttal period commenced, including the following: an improved model that uses 437 basis functions that is trained on 25 datasets (an additional 15 training datasets that we sourced and collected after submission), improved performance across all tasks, and improved figures. Please see the comment summarising these and other initial changes. That said, we will be updating the paper before the end of the rebuttal period, accounting for any of your concerns and suggestions. \n\n## Regarding the DAM as a foundational model\nThe process of collecting, curating, and training on many time series datasets is time intensive and challenging. While there are many very short time series datasets readily available, these are less useful for training when the task is long-term forecasting, hence we do not include short datasets. More data means that we can train for longer without overfitting, and typically generalise better on test set performance and in the zero-shot setting. However, longer training is also costly and time-intensive. As we mentioned in the conclusion: future work will entail scaling up the DAM both in terms of model size and training corpus. As it is, the DAM is the only forecasting model that is designed for training across many time series datasets. We had to rethink the way time series were ingested and predicted in order to design the DAM; and that is our primary contribution. The generality, flexibility, high-performance, and utility of the DAM is what makes it a foundational model.\n\n### The nature of time series data\nThere are several challenges to overcome for training a time series foundational model, namely:\n1. Collection and curation of time series data.\n2. The non-iid nature of time series datasets (i.e., temporally close data points are highly dependent).\n3. Owing to this non-iid property, balancing amongst data points, variables, and datasets is non-trivial and remains an open question. For example, an extremely long time series dataset that is highly repetitive (i.e., a daily repeating signal) is less 'useful' for training than a shorter time series that contains many diverse patterns that are 'useful' to learn on. Thus, the optimal training regime is complex and not yet known. Perhaps this is the performance bottleneck.\n4. Scaling the model up results in longer training runs. \n\nThat said, the DAM is a foundational model because **it can be trained across many datasets** by design. Hopefully the inclusion of the additional 15 time series datasets, and consequent improved performance, is good evidence for this. We are open to any analysis suggestions you may have, however. Please let us know if there are any additional experiments you would like to see. \n\n## Many appendices\nAs you rightly mentioned, addressing a foundational model for time series forecasting is very challenging. Hence, we required to include longer appendices. . Another reason for the large number of appendices is that we endeavoured to be as open and clear as possible with this work. For example, including the PyTorch code of the model directly in an Appendix makes it far easier for other researchers to reuse and replicate our work. Most of the appendices are not strictly necessary, but rather a consequence of us trying open our research, since we believe that the DAM is an actual important contribution to the field of time series forecasting because of what it enables.\n\nIn the current update of the paper we improved the results discussion such that it relies less on the appendices."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission418/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699872035110,
                "cdate": 1699872035110,
                "tmdate": 1699872035110,
                "mdate": 1699872035110,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lUHYuZXhxZ",
                "forum": "4NhMhElWqP",
                "replyto": "hnKVZkz7W6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission418/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission418/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 2/2"
                    },
                    "comment": {
                        "value": "## Peak prediction\nOne of the contributions of this paper is that we compare 14 SoTA baseline methods (with 8 of in the appendix, owing to space constraints and their relatively lower performance compared to other baselines) across 10 benchmark datasets. It is rare in this field to see comparisons across this many datasets. We intentionally did not include weak or outdated methods (e.g., LSTMs) because of how strong the DAM's performance is, and we wanted to posit it amongst the absolute current SoTA. This is one reason for the DAM to show weaker peak time predictions in comparison to some of the selected methods, as we are only comparing against the best models in the SoTA. The updated model (see above) has improved performance on 'peak' data.\n\n## The DAM is univariate\nPlease see the separate comment entitled 'A note on inference and the univariate nature of the DAM', where we explain precisely how inference is accomplished, and how the DAM is a fully univariate model. To predict a given variable the DAM only consumes context **from that variable**. We will clarify this by adding a section in an incoming update. Future work will involve extending the DAM by adding a generalised multivariate component. \n\n## Flexible inference cost\nThe DAM is far smaller than common LLM models. What we are trying to show in this section is that the DAM is flexible, and that a single model can incur low or high computational cost, depending on user choice. The ability to balance performance and inference cost, without having to train additional models, can be extremely useful in practical scenarios. For example, in cloud resource allocation, 200ms might be too high a cost. Although we agree that 200ms is already inexpensive, this might not hold for much larger versions of the DAM (future work)."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission418/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699872047054,
                "cdate": 1699872047054,
                "tmdate": 1699872047054,
                "mdate": 1699872047054,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aYbCNaKfk9",
                "forum": "4NhMhElWqP",
                "replyto": "Z1MbuBIHXt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission418/Reviewer_PhNp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission418/Reviewer_PhNp"
                ],
                "content": {
                    "comment": {
                        "value": "From your further experiments (437 basis functions, additional 15 train datasets), it seems that you also assume the scaling laws in the time-series domain or at least have belief that more data will increase the performance of DAM (please correct me if not, but in this case, please explain why did you increase the scale of DAM to improve performance)\nHow can we further increase performance of a foundation model of the time-series domain similar to the computer vision domain (https://arxiv.org/abs/2103.00020) and the natural language domain (https://arxiv.org/abs/2001.08361)? \nIs it enough for us to increase the scale of the computation? \nI think a foundation model paper should give a direction about it because that paper should be the foundation of the future time-series research as well.\n\nI also want to hear the author's opinion about the necessity conditions for a model to be considered a foundation model. \nAs this paper (https://fsi.stanford.edu/publication/opportunities-and-risks-foundation-models) pointed out, could you find any emergence property from DAM? \nYou said 'the DAM is a foundational model because it can be trained across many datasets by design'. Yes, it is important. However, many machine learning-based forecasting models are already using a cross-learning strategy that trains multiple time-series dataset at once (Section 4.3, https://www.sciencedirect.com/science/article/pii/S0169207021001874). What distinguishes the DAM from existing methods (or the multi-task learning method) that makes it a foundational model?\n\nFor the rest of your answer, I will get back to you soon!\n\nThanks!"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission418/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700059940079,
                "cdate": 1700059940079,
                "tmdate": 1700059940079,
                "mdate": 1700059940079,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hOrNYRX7Cq",
                "forum": "4NhMhElWqP",
                "replyto": "hnKVZkz7W6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission418/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission418/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal changes"
                    },
                    "comment": {
                        "value": "Good day. \n\nPlease note that we have updated the paper as per your commentary and requests. There is a summary entitled \"Rebuttal Summary - After addressing reviewers' comments and questions\".\n\nOf particular note for your consideration:\n1. Appendix N - a small scale test of the DAM's scaling with model size (for a perspective of 'scaling laws'). We put this experiment together based our discussions with you. We find that the DAM does indeed scale well with increased model size, but also that training longer and across all 25 training datasets evidences a strong benefit. \n2. A reworked discussion on the the limitations of the DAM, explaining how a future version could take cross-variable (i.e., multivariate) information into account. This *might* be the current limiting factor to the DAM, but there are many avenues requiring exploration in the future.\n3. We have included an ablation study of all the components of the DAM (now Section 5.2). This indicates which components of the DAM are important, and therefore should be focussed on for future work. We hope that this gives a better insight into what the 'bottlenecks' are for improving performance. \n\nPlease let us know if you have any more questions or requests that we can address before the end of the rebuttal period. \n\nThank you again for your review."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission418/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700482573973,
                "cdate": 1700482573973,
                "tmdate": 1700482573973,
                "mdate": 1700482573973,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sTkQUSGkL9",
                "forum": "4NhMhElWqP",
                "replyto": "hOrNYRX7Cq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission418/Reviewer_PhNp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission418/Reviewer_PhNp"
                ],
                "content": {
                    "title": {
                        "value": "DAM as a Foundation Model"
                    },
                    "comment": {
                        "value": "Thanks for your updating.\nRegarding the foundational properties of DAM. I think there are still some aspects that are unclear.\n\n1) How many basis functions do we need? By increasing # of basis functions, how much can we improve its performance across various tasks? The same questions can be applied to the number of datasets (\bHow many dataset do we need? How much improvement can we expect?) \n\n2) For the tasks that DAM underperformed compared to competitive models, why did DAM fail on those task? Is there any domain requirement for DAM to be a foundation model? Or was it due to the lack of scale (data, basis function, parameters, etc..) and is it expected to be solved by DAM by addressing the scale issue? \n\nThanks!"
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission418/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558064128,
                "cdate": 1700558064128,
                "tmdate": 1700558064128,
                "mdate": 1700558064128,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JDNow0uQPC",
                "forum": "4NhMhElWqP",
                "replyto": "mNSsEAU4Py",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission418/Reviewer_PhNp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission418/Reviewer_PhNp"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply.\n\nI think your opinion is sound.\nHowever, I still think that DAM is just one of the potential candidate methods to become a foundation model but not enough at this time.\nYour explanation of the emerging properties of DAM is weak. It should be defined as functionality of DAM, e.g., DAM can have function 'A' even though we didn't intentionally train it to do that. For me, finding high basis coefficients for very long periods for ETTh1 is just similar to what an ensemble model does, i.e., giving high coefficient values for weak learners even though their performances are not good.\nAdditionally, the strategy for further performance improvement is unclear at this time.\nTo me, the subject of this paper seems more closely related to generalization ability of the model, which should be different from a foundation model. \nFor readers, I presume most will expect foundation model like GPT, i.e., large-scale training, and surprising emerging abilities of the model. However, as you pointed out, there was not much discussion about foundation model in the context of time-series model. To define a foundation model in the time-series domain, we need more discussion from various experts. I hope this paper can become a stepping stone.\n\nI can raise my score if the authors remove 'foundation' term in the paper and replace it to other word that represent 'generalization'"
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission418/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665594105,
                "cdate": 1700665594105,
                "tmdate": 1700665594105,
                "mdate": 1700665594105,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9XBhQmBVM0",
                "forum": "4NhMhElWqP",
                "replyto": "hnKVZkz7W6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission418/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission418/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Toward a foundation model?"
                    },
                    "comment": {
                        "value": "Thank you for your comment. Your perspective suggests that you think that the DAM takes a significant step toward being a foundation model, but that we have not yet provided sufficient evidence to convince you *that it is already foundational*.  Would it suffice if we used the term **\"towards\"** when referring to the DAM in the context of a foundation model? \n\nThat said, we propose to change the following parts of the paper:\n1. The title, from \"DAM: A Foundation Model for Forecasting\" to \"**DAM: Towards A Foundation Model for Time Series Forecasting**\". This also better captures the scope of the work. *Please note: changing the title and abstract will not reflect in this openreview setting, but rather only on the paper itself. This will need to be accounted for for the camera ready version, and we will do that.*\n2. In the introduction, from \"We present the deep data-dependant approximate analytical model (DAM) as a foundation\nmodel for universal forecasting\" to \"**We present the deep data-dependant approximate analytical model (DAM) as a significant step toward a foundation model for universal forecasting**\".\n3. Also in the introduction, from \"To the best of our knowledge, the DAM is the first foundation model for universal time series forecasting\" to \"**To the best of our knowledge, the DAM is the first model for universal time series forecasting that can be trained simultaneously across many diverse datasets, of different resolutions, and for various horizons, such that it generalises well both within and outwith the training set**\".\n4. In Section 4.2, from \"This is strong evidence that the DAM is a foundation model.\" to \"**The DAM generalises well outside of its training set**\".\n5. In the conclusion, from \"We presented the DAM \u2013 a foundation model for universal forecasting\" to \"**We presented the DAM as a significant step toward a foundation model for universal forecasting.**\".\n\n___\n**Will that be sufficient for you to raise your score?** If so, we would very much appreciate if you did that before the end of the rebuttal period (a few hours from now). That said, please also consider all of the significant updates we have made to the paper when considering what to change your score to. Specifically, as a step \"toward\" a foundation model for time series forecasting, we believe that our work on the DAM overcomes many significant practical hurdles in the time series domain (e.g., cross-resolution training, variable horizons, interpretability, etc.), and, as such, deserves serious consideration. \n\nWe will update the paper as soon as the changes are made and after we receive your confirmation."
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission418/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669021524,
                "cdate": 1700669021524,
                "tmdate": 1700672252396,
                "mdate": 1700672252396,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nC2i28wJRz",
                "forum": "4NhMhElWqP",
                "replyto": "9XBhQmBVM0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission418/Reviewer_PhNp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission418/Reviewer_PhNp"
                ],
                "content": {
                    "comment": {
                        "value": "I changed my score"
                    }
                },
                "number": 29,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission418/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694512335,
                "cdate": 1700694512335,
                "tmdate": 1700694512335,
                "mdate": 1700694512335,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "VFKPKX1C1U",
            "forum": "4NhMhElWqP",
            "replyto": "4NhMhElWqP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission418/Reviewer_sH5F"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission418/Reviewer_sH5F"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a foundation model for universal time-series forecasting. Building such a model is challenging because the sample resolution, periodicity, and prediction task are different. To address these challenges, the paper proposes to take randomly sampled histories and output coefficients of basis functions. The designed basis function enables forecasting to non-fixed horizons.\nExperiments show that the single model trained on 10 datasets can outperform or match existing SOTA dataset-specific models. It also performs well on held-out datasets and has good interpretability."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Building a foundation model for time series forecasting is challenging because different datasets have different patterns and resolutions. This paper proposes reasonable solutions to address these challenges. The experimental results also show that the proposed method is promising. Additionally, it has good interpretability and is robust to missing and irregularly sampled data."
                },
                "weaknesses": {
                    "value": "Training such a foundational model requires high computational cost. The biggest advantage of the foundation model is its ability for zero-shot forecasting. However, the paper only reports the performance on two held-out datasets which is not sufficient. This restricts the practical use of the model in the real-world application."
                },
                "questions": {
                    "value": "How does the training cost of the proposed method compare with the baselines?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission418/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698982780175,
            "cdate": 1698982780175,
            "tmdate": 1699635968532,
            "mdate": 1699635968532,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GgOlfl8ieq",
                "forum": "4NhMhElWqP",
                "replyto": "VFKPKX1C1U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission418/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission418/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial response"
                    },
                    "comment": {
                        "value": "Thank you for your insightful review. Please note that the current updated version of the paper has a number of improvements, including using more basis functions (437 vs 219), better results, improved figures, and more training datasets (an additional 15). \n\n## Regarding zero-shot experiments\nBased on these reviews we have identified that the main weakness of our paper is the limited analysis on the zero-shot setting. You are correct that the main advantage of a foundational model for forecasting is its applicability to unseen datasets. There is another very important advantage, namely that training on many datasets simultaneously improves performance on those datasets, when compared to training on each individually. We postulate that this is owing to a combination of factors, including but not limited to:\n1. Mitigating the early overfitting that is common when training time series models. Many SoTA methods train for 1-10 epochs, which is only 3125 iterations using a minibatch size of 32 on a training dataset of length 10000. This is 336x shorter than the training of the DAM. We did not witness overfitting when training. See the section below entitled 'Computational cost' for a discussion on the importance of this.\n2. Learning from patterns in one dataset being useful for generalising to another dataset. For example, weather data from Germany will likely contain patterns that are useful to learn from when generalising to weather data from the US.\n\nThat said, we agree that more demonstrations on the zero-shot setting would be pertinent. To this end, we are in the process of analysing zero-shot performance across the following 8 datasets (6 new): Illness, wiki-weekdays, Monash (including \u2018temperature rain\u2019, London smart meters, weather, web traffic),  UCI 'individual household electric power consumption' (https://archive.ics.uci.edu/dataset/235/individual+household+electric+power+consumption), and the Azure 2019 function trace. \n\n### How to assess zero-shot transfer?\nAnother reviewer (wavr) suggested that when comparing the DAM to other methods in the zero-shot setting, those comparison methods should be trained on said datasets (i.e., not zero-shot transfer). Given how poorly PatchTST and DLinear transferred to new datasets (Table 2), it is understandable to have a closer comparison. However, the issue is that the DAM is the only method we know of that is designed as a universal forecaster that is dataset agnostic: there are no fully fair comparison methods. We are happy to do what is suggested (even though this advantages other methods quite strongly), but would like to know your opinion regarding how to perform this analysis? What would you, ideally, like to in such an experiment? We are open to discussion and suggestions during this rebuttal period, and will endeavour to make ready an updated paper, with more zero-shot analysis, by the end of the rebuttal period. \n\n\n## Computational cost\nAs mentioned above and in the paper itself, the DAM takes much longer to train. At first this seems to be a disadvantage, but we argue that it signifies a positive shift toward a better match between deep models and time series. Consider other fields, such as NLP or vision: training runs are typically far longer than several thousand iterations. This is simply because the datasets available are far larger than time series datasets. That is one of the primary motivating factors for our development of the DAM; we had to rethink the way time series were processed and predicted in order to design a modelling approach that could extend to any time series dataset. Only then could we actually apply this model to a collection of datasets and move toward training runs comparable to other fields.\n\nPerhaps a pertinent comparison to make is that of training cost of the DAM against the cumulative training cost of other methods. Assume an average training run of 3125 iterations (see above) per dataset. Assume also that we would have to train baseline methods on each horizon-dataset combination. For $25+8=33$ datasets (considering the 25 datasets we used for training and additional 8 for zero-shot transfer) we would need to train $4*33=132$ variants of a baseline model. This is close to the scale of training we used for the DAM, but with the important caveat that baseline methods cannot be trained for longer owing to overfitting, while the DAM benefits from longer training runs by design. Further, these 132 variants do not necessarily transfer well to other datasets."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission418/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699798330993,
                "cdate": 1699798330993,
                "tmdate": 1699798330993,
                "mdate": 1699798330993,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xOkIcSkDG2",
                "forum": "4NhMhElWqP",
                "replyto": "VFKPKX1C1U",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission418/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission418/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal changes"
                    },
                    "comment": {
                        "value": "Good day. \n\nPlease note that we have updated the paper as per your commentary and requests. There is a summary entitled \"Rebuttal Summary - After addressing reviewers' comments and questions\".\n\nOf particular note for your consideration:\n1. A considerable extension to the held-out experiments, including an additional 6 datasets (now 8 in total) tested in zero-shot mode, and compared against 3 SoTA methods in zero-shot mode **and when trained on the target datasets**. We also included performance of the DAM when fine-tuned on target datasets for comprehensive comparisons. The DAM beats SoTA methods at zero-shot transfer across 14/16 metrics and **even outperforms SoTA methods trained on the target datasets** in some cases.\n2. Improved interpretability demonstrations, including a video version submitted as supplementary material.\n\nPlease let us know if you have any more questions or requests that we can address before the end of the rebuttal period. \n\nThank you again for your review."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission418/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700482130580,
                "cdate": 1700482130580,
                "tmdate": 1700482130580,
                "mdate": 1700482130580,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1YuSmwTF8H",
            "forum": "4NhMhElWqP",
            "replyto": "4NhMhElWqP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission418/Reviewer_wavr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission418/Reviewer_wavr"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes deep data-dependant approximate analytical model (DAM) as a \"foundational model\" for time series forecasting. DAM uses a long tail distribution to sample from the history of the time series. These irregularly-sampled time-value pairs are fed into a transformer-based model which outputs basis coefficients. The basis coefficients are then used in a basis function composition to generate forecasts. The authors trained a single DAM model across multiple datasets and show that this model is competitive against dataset-specific baselines on long-term forecasting benchmarks. Small-scale analyses have also been conducted on very long-term forecasting and imputation tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The combinations of ideas presented in this work involving history regime sampling, basis composition-based forecasting, and training a single model across multiple datasets are original and interesting.\n- The model provides the flexibility to forecast arbitrarily far into the future which is an attractive property. While autoregressive models can already do that, they are generally slow.\n- The paper is well-written in general. Some discussion and visualizations can be improved (see weaknesses)."
                },
                "weaknesses": {
                    "value": "- The main weakness of this paper is that it overclaims and underdelivers. In its current state, the study is not strong enough to claim the title of a \"foundational model\".\n    - The authors mention that they use datasets from diverse domains. However, out of the 12 datasets studied, 6 come from a single domain. The distribution of sampling frequencies of these datasets are also not diverse with 6 hourly datasets and a limited representation of other frequencies (and some popular frequencies completely missing).\n    - Another aspect that could have justified the term \"foundational\" is a diversity of tasks. However, the paper mostly focuses on the long-term forecasting tasks with limited discussion of other tasks. Importantly, the practically relevant task of short-term forecasting (e.g., Monash time series forecasting archive) gets very less attention.\n    - The claim _Most existing forecasting models were designed to process regularly sampled data of fixed length. We argue that this restriction is the central reason for poor generalisation in time series forecasting_ has not been justified convincingly.\n- The visualizations are poorly done and confusing for a serious academic paper. Please consider using cleaner figures. It is unclear how exactly inference on a new dataset is performed. It would improve the clarity of the paper if a specific paragraph on inference is added. Please see specific questions in the questions section.\n- The results on the long-term forecasting benchmarks, while reasonable, are not impressive for a \"foundational model\" that has been trained on a larger corpus of datasets.\n- The very-long-term forecasting task is of limited practical significance. Despite that, the discussion requires improvement, e.g., by conducting experiments on more datasets and training the baseline models with the \"correct\" forecast horizon to put the results in a proper context.\n- The zero-shot analysis (Sec. 4.3) has only been conducted on two datasets. Moreover, since prior works such as PatchTST and NHITS do not claim to be foundational models, a proper comparison would be with baselines trained specifically on these held-out datasets. DAM would most likely be worse in that case but it would be a better gauge for the zero-shot performance."
                },
                "questions": {
                    "value": "- How exactly is inference performed on a dataset? Is basis function initialization also required during inference?\n- How costly is context selection during inference, in general?\n- Can you clarify what is meant by \"No training of the backbone is even required in this case because the initialisation coefficients are optimal for past data\"? Is model training not needed for imputation?\n- You mention \"The DAM produces relatively high basis coefficients for ETTh2 in the year range, suggesting that it captures the long-term trend.\" but the ETTh2 dataset only has data over two years (from 2016 to 2018) while relatively high positive values are seen for 3, 5, 7, 9 yr basis components. How do you explain this phenomenon?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission418/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission418/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission418/Reviewer_wavr"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission418/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699129529861,
            "cdate": 1699129529861,
            "tmdate": 1700735253694,
            "mdate": 1700735253694,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "u9fofxFFO4",
                "forum": "4NhMhElWqP",
                "replyto": "1YuSmwTF8H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission418/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission418/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial response 1/2: A foundational model"
                    },
                    "comment": {
                        "value": "Thank you for your informative review. Since the submission deadline we have worked to improve the paper and have updated it accordingly. A summary of the changes is provided in another comment. Pertinent to your review, and in summary, please note that the new version of the paper uses a model with more basis coefficients (437 vs 219) for improved performance across all datasets, making it a stronger competitor as a foundational model; has improved figures and visualisations to aid in clarity of understanding; and includes an additional 15 **training** datasets from various sources. These additional training datasets cover an additional 2 resolutions (30 minutes and 1 minute), and additional domains, namely air quality, stocks, sunspot index, and cloud resources.\n\nWe have broken the rest of our response into two parts (this and another), covering the main weakness you mentioned (this) and other weaknesses and questions (in the next comment).\n\n## Addressing the main weakness\n\nOur model now uses 25 datasets for training, with 8 held out (including 6 we will add to the paper by the end of the rebuttal deadline). We note that while we could have trained on much larger datasets, many available time-series data have very low signal-to-noise ratios. In our work, we have carefully selected our data to increase the signal in the training and evaluations. In addition, we have used open datasets to improve reproducibility. To the best of our knowledge, we are among very few papers that select datasets from very different domains.\n\nThat said, we claim the title of a foundational model because of the novel design of the DAM, not simply because it was trained on many datasets from diverse domains. We believe that  the term 'foundational model' does not necessarily only apply to models trained on massive datasets, at huge cost,  but  that it  is the model itself that **is foundational by design** if it can be used across disciplines. Otherwise,  that claim will necessarily be reserved for big companies with high resources. This is particularly true for time series forecasting because designing a foundational model in this field requires more than just scaling up an existing model to more data. Instead, it required a rethinking of the way data was sampled and ingested to yield a flexible, efficient, and global context (i.e., the HSR and TV-tokens), and a non-fixed horizon forecast (i.e., the basis composition) for multiple use-cases. \n\n### Dataset-horizon combinations as individual tasks\nIn some sense, each individual dataset-horizon combination (in Tables 1 and 2) is a separate 'task' when considering how these are traditionally **solved by training variants of a single model design**. Please consider that the DAM is currently the only SoTA forecasting model that is, *by design*, agnostic to resolution, missing data, forecast horizon, and data-domain. That is the core novelty of this paper and we believe it is hence a foundational model. Please note that the updated version of the paper includes an additional 15 datasets for training and posits the DAM far more clearly. One of our requirements for using a dataset is that it should be sufficiently long. We are open to suggestions for sourcing additional datasets (aside from the 25 we use for training and the new datasets we used for the held out tests)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission418/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699803336796,
                "cdate": 1699803336796,
                "tmdate": 1699803336796,
                "mdate": 1699803336796,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QRvBJ48CFV",
                "forum": "4NhMhElWqP",
                "replyto": "1YuSmwTF8H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission418/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission418/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial response 2/2: addressing remaining weajnesses and questions"
                    },
                    "comment": {
                        "value": "## Regarding weaknesses\n1. We have replaced all visualisations with new and improved digital versions. \n2. Regarding inference, we have made an additional comment, titled \u2018 A note on inference and the univariate nature of the DAM\u2019 to address this. The DAM is always univariate, inference **does** involve fitting $\\mathbf{\\theta}_0$ to the context data. This is because $\\mathbf{\\theta}_0$ is not part of the model itself, but is instead an input to the model. We will ensure that there is an \u2018inference\u2019 section in an upcoming updated version of the paper.\n3. Regarding very-long-term forecasting: the notion of \u2018long\u2019 is dependent on the resolution of the data. For example, 6000 time steps is roughly 41 days (as in Figure 5) for weather, but is 250 days for an hourly dataset and 16 years for a daily dataset. The practical significance for very-long-term forecasting is therefore reliant on dataset resolution. Testing the same sets of horizons amongst datasets of different resolution is for comparison against existing literature. We chose to demonstrate the very-long-term forecasting for weather because (1) longer term forecasting (e.g., 1008 steps=1 week) is useful for weather, and (2) the high resolution of the weather dataset (10 mins) requires longer term forecasting for practical use. We will clarify this in an incoming update to the paper. Regarding an experiment that uses the 'correct' horizon (+- 6000 steps in this case): it is practically very challenging to train conventional models with this horizon because of the way that they forecast (via a vector the length of the horizon). The DAM, on the other hand, does not require assessing all points between `now' and the distant future, making it elegant for the very-long term case. That all said, we can train PatchTST and DLinear using a very-long horizon of 6000 steps and update the paper by the end of the rebuttal period, should you require that. Please let us know if you still think this is necessary. \n4. Thank you for pointing this weakness out; it is a pertinent deduction. We will ensure that this analysis is sufficiently extended. Specifically, we be analysing zero-shot performance across the following 8 datasets: Illness, wiki-weekdays, Monash (including \u2018temperature rain\u2019, London smart meters, weather, web traffic),  UCI 'individual household electric power consumption', and the Azure 2019 function trace. We will keep the results for illness and wiki-weekdays (as in the current version of the paper), but also include results for PatchTST and DLinear when trained on these datasets. Since this is not strictly a fair comparison (i.e., zero-shot DAM against standard baslines), we will endeavour to make that clear in the revised version of the paper. It is likely that we will be removing Figure 1 in favour of additional space for this extension. \n\n## Regarding questions\n1. See 1, above.\n\n2. In the aforementioned comment we show how context selection is as simple as sampling indices (`np.random.choice(np.arange(len(v)), p=phsr, replace=False)`) and is thus extremely lightweight. Your question actually opens up an interesting discussion about how one might alter $p_{hsr}$ according to some intrinsic statistics about the context (which would be more costly) but currently the $p_{hsr}$ is static and independent of the query time series.\n\n3. Hopefully our clarifications above explained the role of the initialisation coefficients, $\\mathbf{\\theta}_0$ , and how these are fit to context data (even when data is missing; hence being relevant for imputation). Figure 4 shows the basis initialisation for the past, and how well this fits to the context data. It is *this* that is used for imputation. So, effectively, you are correct: model training is not needed for imputation. Fitting the initial basis coefficients is perhaps an additional contribution of our paper, but there are other more important contributions we chose to focus on. \n\n4. The high basis coefficients outside of the effective \u2018visibility\u2019 of the data is interesting and worth considering. One explanation might be that trends of that scale exist in the data even though the collection procedure could not have captured a full cycle of these trends, and that the DAM is capable of approximating those trends to improve forecasting. There are a number of potential alternative explanations, but it should be noted that this seems like a common behaviour across most datasets (Appendix L). \n\nWe believe that the combination of the already updated paper, additional writing and clarifications to be made, and additional zero-shot experiments cover the weaknesses and questions you raised. Please kindly let us know if there is anything else you would like seen done in order to cover your concerns or reticence."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission418/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699803865554,
                "cdate": 1699803865554,
                "tmdate": 1699865160617,
                "mdate": 1699865160617,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fzwntSfv1r",
                "forum": "4NhMhElWqP",
                "replyto": "u9fofxFFO4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission418/Reviewer_wavr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission418/Reviewer_wavr"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response and the revision. The updated results look interesting. The figures are much better than the previous (although still dense). Looking forward to the zero shot results.\n\n> That said, we claim the title of a foundational model because of the novel design of the DAM, not simply because it was trained on many datasets from diverse domains. We believe that the term 'foundational model' does not necessarily only apply to models trained on massive datasets, at huge cost, but that it is the model itself that is foundational by design if it can be used across disciplines. Otherwise, that claim will necessarily be reserved for big companies with high resources. \n\nI never said that a foundation model is only when one trains large models on massive datasets at a _huge cost_. I was commenting on respresentation of the different domains in the training set. I also highlighted the aspect of applicability to a diversity of tasks. \n\n> This is particularly true for time series forecasting because designing a foundational model in this field requires more than just scaling up an existing model to more data.\n\nThis statement hasn't been proven in the literature yet."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission418/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700246764201,
                "cdate": 1700246764201,
                "tmdate": 1700246764201,
                "mdate": 1700246764201,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JwZRlCuYaQ",
                "forum": "4NhMhElWqP",
                "replyto": "1YuSmwTF8H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission418/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission418/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal changes"
                    },
                    "comment": {
                        "value": "Good day.\n\nPlease note that we have updated the paper as per your commentary and requests. There is a summary entitled \"Rebuttal Summary - After addressing reviewers' comments and questions\".\n\nOf particular note for your consideration:\n1. More diverse training datasets and extensive evaluations on held-out datasets, with improved performance throughout the paper.\n2. Improved visualisations.\n3. Description of inference process (Section 3.5).\n4. A discussion on the high basis coefficients for ETTh2 (Section 5.1).\n\nPlease let us know if you have any more questions or requests that we can address before the end of the rebuttal period. \n\nThank you again for your review."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission418/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700481872929,
                "cdate": 1700481872929,
                "tmdate": 1700481872929,
                "mdate": 1700481872929,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bZcKuvRNF0",
                "forum": "4NhMhElWqP",
                "replyto": "1YuSmwTF8H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission418/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission418/Authors"
                ],
                "content": {
                    "title": {
                        "value": "TOWARDS foundation"
                    },
                    "comment": {
                        "value": "Good day,\n\nPlease take note we have heeded your and another reviewer's comments (PhNP) and decided to adjust the title and claim of our paper such that it is \"towards foundational\". \n\nThis and many of the changes made throughout this rebuttal period have been in responsive to your thought-provoking review. Thank you. \n\n**Do you believe that these changes go even a little way towards addressing some of your concerns? An answer from you, prior to the closing of the rebuttal period in a few hours, would be extremely appreciated.** Moreover, if we have satisfactorily addressed your concerns, could you also consider changing your score? We firmly believe that the DAM is good work and deserves attention. \n\nMany thanks once more for all of your hard work in reviewing this paper."
                    }
                },
                "number": 31,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission418/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727602396,
                "cdate": 1700727602396,
                "tmdate": 1700727602396,
                "mdate": 1700727602396,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SuzIvENt3Y",
                "forum": "4NhMhElWqP",
                "replyto": "bZcKuvRNF0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission418/Reviewer_wavr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission418/Reviewer_wavr"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for all the changes to the paper. The new zero shot comparisons, in particular, look very appealing. I am raising my score to 6. I hope the authors will be able to release their pretrained model and code in the interest of the community at large."
                    }
                },
                "number": 32,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission418/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700735229774,
                "cdate": 1700735229774,
                "tmdate": 1700735229774,
                "mdate": 1700735229774,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QgjhJPTGii",
            "forum": "4NhMhElWqP",
            "replyto": "4NhMhElWqP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission418/Reviewer_yXbk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission418/Reviewer_yXbk"
            ],
            "content": {
                "summary": {
                    "value": "This is an interesting paper proposing to solve a very general time series problem. This paper aims at flexible (long or short horizon) time series forecasting with long or short context input for any general time series. Moreover the context could also be irregularly sampled. The paper additionally proposes a strategy for efficient long context inputs.\n\nThe model is therefore a general time series forecaster and trained on many time series for generalization to any held out datasets. As such it could be considered a foundational model for time series forecasting.\n\nExperimental results are promising, especially on held out datasets. One of the main advantages of a foundational model is to be general enough to be able to predict on held out datasets. While the results are most positive on held out datasets, the model performs at par with the baselines on the training datasets. To summarize, I believe this to be an interesting contribution to the time series literature, and I am willing to increase my score if the concerns below are addressed.\n\nUpdate: Increased the score"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The problem being solved in the paper is one of the most interesting problems in the time series community i.e. general time series forecasting for flexibly sampled time series.\n- The proposed solution (described as a foundational model) involving attention is a befitting solution to this problem by virtue of its ability to adapt to flexible input sizes.\n- Experimental results are promising on both training and held out datasets."
                },
                "weaknesses": {
                    "value": "- The model architecture is quite complex and the rationale behind such a choice is not fully explained. One important question that is raised is the usefulness of each of the components in the model. An ablation study may be performed to measure the importances of each of the components proposed in the paper.\n- Simple linear forecasting models are not considered as baselines. For comparison between different models a normalized score is preferred rather than a absolute score. Normalization helps interpret the improvements easier since they are scaled. As such, the experimental section need improvement."
                },
                "questions": {
                    "value": "- When using a trained model for forecasting a single time series, how does inference look like in such a simple setting? Do attention models work well in such scenarios? In other words, do attention models produce better forecasts when a larger context in provided? The larger context could be in the form of multiple time series or a larger history."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission418/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission418/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission418/Reviewer_yXbk"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission418/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699132377162,
            "cdate": 1699132377162,
            "tmdate": 1701056799012,
            "mdate": 1701056799012,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WbkLCzqdmP",
                "forum": "4NhMhElWqP",
                "replyto": "QgjhJPTGii",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission418/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission418/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Initial response to review (prior to making adjustments to paper, if necessary)"
                    },
                    "comment": {
                        "value": "Thank you for your review. Please note that we have uploaded a revised version of the paper and have made a comment describing the changes thus far. In summary, we improved the model by adding more basis functions (437 instead of 219), trained it on 25 datasets (instead of 10), and improved the figures and results in the paper. In the coming days we will be uploading more updated versions of the paper that addresses concerns for all reviewers. That said, we would like to address some of your concerns prior to this, and clarify some points before making the necessary changes to the paper. \n\nRegarding the weaknesses you mentioned:\n - The model architecture is actually far closer to the standard transformer architecture (from NLP) than many transformer methods in the literature for forecasting. This is mentioned in the 'Model structure' paragraph in Section 3.1., but we have updated the architecture figure into a better digital format. We chose to be explicit about every component instead of hiding details in collective 'blocks' as is often the case for modern architectures. \nThat said, we can add an additional paragraph in Section 3.1 to address the intended purposes of MHSA, Cross attention, and the feed forward (cross) block that acts across the basis dimension. The MHSA acts to share information across TV-tokens, and thus build a latent context for the model to adjust basis coefficients. The cross attention acts to make use of this latent context to update basis coefficients for prediction. The cross feed-forward block explicitly models patterns across basis coefficients in order to share information in the frequency domain. After seeing the new Figure 2, please confirm whether you think this will be necessary. ~~Running ablation studies is, unfortunately, not feasible within the rebuttal period because the DAM is expensive to train. Should this be an absolute necessity for you to increase your score, please let us know so that we can figure out how to do this in the time available.~~EDIT: we have performed an ablation study and discussed this in the next comment.\n \n - DLinear is a linear forecasting model and a well-known strong baseline. Are you suggesting a less capable linear model should be included for comparison? We endeavoured to include results form SoTA methods to highlight the strength of the DAM, testing 14 methods (including Table 6 in Appendix I), instead of testing it against poorer performing baselines. However, we can certainly test a simpler linear method if you insist, and replace a method in Table 1 with these results.\n - Could you please clarify what you mean by a normalized score? We have listed results in the same way that many papers in this field list them. Normalising the results (by whatever mechanism you suggest/clarify) is certainly possible, but this would also require additional space should we wish to keep unnormalised scores for easy comparison to other papers in the field. We prefer the latter, but are open to a discussion about this.\n \nRegarding your question:\n - The DAM is always univariate (although we are working on a version that can use information across variables). Inference simply involves sampling time-value pairs from the HSR using the inference time series, fitting initial coefficients to those context points, a forward pass through the DAM, and an assessment of the resultant basis composition for some query time. Figure 10 in Appendix G essentially answers your question regarding the reliance on context size. For the updated model (see the updated paper PDF), a longer context helps in most cases, but this seems to be highly dataset dependent. Thankfully the DAM can be tuned accordingly (HSR tuning) using some held out set (validation in the case of the paper results). It seems that the inference procedure is not immediately obvious (based on question and another), so we will endeavour to make this fully clear in an updated version of the paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission418/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699639886738,
                "cdate": 1699639886738,
                "tmdate": 1700081776610,
                "mdate": 1700081776610,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YbEAb7zOda",
                "forum": "4NhMhElWqP",
                "replyto": "QgjhJPTGii",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission418/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission418/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Ablation study"
                    },
                    "comment": {
                        "value": "We have performed an ablation study as per your recommendation. The results averaged across 4 horizons on the ETT datasets are listed below. We will include this table and a discussion in an update to the paper before the rebuttal period closes. Your suggestion was incredibly useful and enabled an insight into the model functionality that was missing - thank you. \n\n|  | **Nothing** | **Nothing** | **Self-Attn** | **Cross-Attn** | **$FF_{TV}$** | **$FF_{B}$** | **$FF_{B,cross}$** | **ToME** |\n|-------|-------------|-------------|---------------|----------------|-------------|------------|-----------------|----------|\n|       | MSE         | MAE         | MSE           | MAE            | MSE         | MAE        | MSE             | MAE      | MSE   | MAE   | MSE    | MAE   | MSE   | MAE   |\n| ETTh1 | 0.405       | 0.404       | 0.439         | 0.469          | 0.606       | 0.539      | 0.483           | 0.493    | 0.599 | 0.561 | 31.099 | 2.797 | 0.423 | 0.478 |\n| ETTh2 | 0.355       | 0.371       | 0.393         | 0.437          | 0.469       | 0.479      | 0.349           | 0.416    | 0.434 | 0.432 | 13.411 | 1.688 | 0.359 | 0.409 |\n| ETTm1 | 0.352       | 0.385       | 0.411         | 0.439          | 0.642       | 0.528      | 1.354           | 0.498    | 0.706 | 0.531 | 51.323 | 3.519 | 0.364 | 0.425 |\n| ETTm2 | 0.240       | 0.300       | 0.335         | 0.392          | 0.574       | 0.473      | 0.258           | 0.380    | 0.316 | 0.387 | 18.481 | 2.090 | 0.241 | 0.366 |\n___\n### Crucial components\nSpecifically, it is clear that $FF_{B,cross}$ is the most crucial component. This part of the architecture is a weighted connection that acts across B-tokens to update them at each step (i.e., it is not a transformer or attention mechanism). The second most impactful component is the cross-attention, which suggests that the DAM has learnt to extract additional information from TV-tokens that are useful in updating B-tokens, aside from that information that is captured in $\\mathbf{\\theta}_0$.\n\n### The impact of ToME\nFinally, and perhaps most interestingly, removing token merging (ToME) is detrimental to performance, suggesting that a concise collection of TV-tokens is advantageous. This behaviour is the inverse of what was observed by the authors of ToME, although the technique was designed for image transformers. \n___\nPlease note that we were able to perform this ablation study without requiring any additional training. We simply 'switched off' parts of the model to compute these results. This is possible because the latent dimension of the model, $D_{model}$, is consistent throughout the forward pass."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission418/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699893000568,
                "cdate": 1699893000568,
                "tmdate": 1699893000568,
                "mdate": 1699893000568,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jiJbBuSLyE",
                "forum": "4NhMhElWqP",
                "replyto": "QgjhJPTGii",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission418/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission418/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal changes"
                    },
                    "comment": {
                        "value": "Good day. \n\nPlease note that we have updated the paper as per your commentary and requests. There is a summary entitled \"Rebuttal Summary - After addressing reviewers' comments and questions\".\n\nOf particular note for your consideration:\n1. The ablation study (now Section 5.2)\n2. Normalised MSE and MAE (Appendix I.1.)\n3. Description of inference process (Section 3.5)\n\nPlease let us know if you have any more questions or requests that we can address before the end of the rebuttal period. \n\nThank you again for your review."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission418/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700481700652,
                "cdate": 1700481700652,
                "tmdate": 1700481700652,
                "mdate": 1700481700652,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ReOSyGmbxD",
                "forum": "4NhMhElWqP",
                "replyto": "QgjhJPTGii",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission418/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission418/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Final change: TOWARDS a foundation model"
                    },
                    "comment": {
                        "value": "Good day,\n\nPlease note that we have decided to adopt the term \"**toward a foundation model**\" throughout the paper. This is based on reviews and discussions with two other reviewers (wavr and PhNp), but worth pointing out to you. See the recent general comment for details of this. \n\nWe believe that we have addressed your concerns (particularly regarding the complex architecture and the new ablation study) and have improved the paper in a number of meaningful ways. \n\n**Do you believe that the changes we have made go even a little way towards addressing some of your concerns? An answer from you would be extremely appreciated.** Since there is little over 1 hour left of the rebuttal period, we would very much appreciate it if you decided to alter your score at this stage, given the considerable effort we have put in during this period, and your statement of \"I am willing to increase my score if the concerns below are addressed\". We believe that the DAM is good work and deserves attention. \n\nMany thanks again for all of your hard work and considerations."
                    }
                },
                "number": 33,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission418/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736291444,
                "cdate": 1700736291444,
                "tmdate": 1700736291444,
                "mdate": 1700736291444,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]