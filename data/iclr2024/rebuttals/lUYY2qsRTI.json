[
    {
        "title": "Delphic Offline Reinforcement Learning under Nonidentifiable Hidden Confounding"
    },
    {
        "review": {
            "id": "N3esIAtYKJ",
            "forum": "lUYY2qsRTI",
            "replyto": "lUYY2qsRTI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5633/Reviewer_Lkaz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5633/Reviewer_Lkaz"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the problem of nonidentifiable confounding in RL. In this regard, a new notion of so-called delphic uncertainty is introduced in addition to aleatoric and epistemic uncertainties. An offline RL algorithm is proposed that penalizes taking actions with high delphic uncertainty. The performance is reported on both synthetic and real data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The presentation is excellent with adequate illustrations that helped my understanding.\n- The new notion of uncertainty is insightful and important in the real application of RL.\n- The algorithm and employed strategies sound reasonable to me."
                },
                "weaknesses": {
                    "value": "I generally enjoyed reading this paper, but there are a few things that I wish were discussed in more depth or clarified:\n1. I'm a little confused about how should I decide whether a world is compatible or not. Apparently, I can start with any confounder space dimensionality, prior $p(z)$ and model architectures and then estimate the parameters using ELBO, and then I have a compatible world? How far I can go here or what should I see to say a world is not compatible here?\n2. The behavior policy is in general a context-aware policy. So, I'd expect enforcing similarity to the behavior policy might result in some sort of context awareness. Is this the case? For instance, in the illustrative bandit example, an optimal context-independent policy only explores $a_0$ and $a_2$, which in World 2 is very different from the behavioral policy and seems to be suboptimal compared to a uniform policy. So, if I get it right, the similarity to behavioral policy is encouraged in this setting with unobserved confounders.\n3. As a similar question to the previous question, don't we expect avoiding actions $(s,a)$ with high delphic similarity to result in a policy more similar to the behavioral policy? It seems to be the opposite: page 8 \"... we also studied the discrepancy of our trained policy with that in the data. Particularly, we compared the actions taken by our policy and the policy in the data and found that ... our policy was significantly different.\"\n4. Could you elaborate on how counterfactual $Q_w^\\pi$ is estimated using importance sampling in Section 5.1? Also, I'm not sure where in appendix C is referred to.\n5. I'm not sure how to think about a reasonable $\\Gamma$. Isn't Figure 7 concerning?\n\nRecommendations:\nI do not see the name of real data or the details explicitly mentioned in the main text. But I can see, as you have cited, Raghu et al. have pioneered using RL in the setting of sepsis treatment using a publicly available MIMIC dataset. If this is not your data, I recommend reporting performance on MIMIC for further reproducibility. Also, looking at the recent citations of Raghu et al., it seems CQL might not be the best baseline here. For instance, Shirali, Schubert, and Alaa have included medical guidelines as potential contexts for RL formulation, with better performance and higher similarity to behavioral policy. You may want to consider more recent works as a baseline or use their observations in favor of context awareness."
                },
                "questions": {
                    "value": "Please refer to the Weaknesses. I'm happy to update my scores after hearing your thoughts."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5633/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5633/Reviewer_Lkaz",
                        "ICLR.cc/2024/Conference/Submission5633/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5633/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698617076836,
            "cdate": 1698617076836,
            "tmdate": 1700519226466,
            "mdate": 1700519226466,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cHoIqli6BD",
                "forum": "lUYY2qsRTI",
                "replyto": "N3esIAtYKJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5633/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5633/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Lkaz (1/3)"
                    },
                    "comment": {
                        "value": "Dear Reviewer Lkaz,\n\nMany thanks for taking the time to read our work and for your positive feedback.\n\n## Q1. Compatible worlds\n*\"I'm a little confused about how should I decide whether a world is compatible or not. Apparently, I can start with any confounder space dimensionality, prior $p(z)$ and model architectures and then estimate the parameters using ELBO, and then I have a compatible world? How far I can go here or what should I see to say a world is not compatible here?\"*\n\nThank you for your question, which allows us to stress the different contributions of our work.\n\n**Formalising compatible worlds.** In our basic definition in Section 4, a compatible world is any model that *maximises the likelihood of the observational distribution*. In the limit of infinite data and no environment stochasticity (i.e., no epistemic or aleatoric uncertainty), finding a compatible world is equivalent to solving this optimisation problem.\n\nIn the identifiable setting, all compatible world models should collapse to the true identifiable distribution of confounders and confounder-dependent policy and value function model. In the non-identifiable setting, our motivation is not to attempt to recover the confounders directly, but rather to obtain a plausible *set* of confounder models that fit the observational data.\n\nOne of the main contributions of our work is therefore this formalism of compatible world models and of delphic uncertainty. This basic definition does not require any assumptions: any measure of the disagreement between compatible world models captures delphic uncertainty.\n\n**Modelling compatible worlds in practice.** A second major contribution is therefore one practical instantiation of delphic uncertainty quantification. Within many possible modelling choices, we use variational inference to estimate confounder distributions. This approach demonstrates the effectiveness of delphic offline RL in our experiments but leaves open further work in investigating alternative modelling approaches for compatible worlds (e.g. different priors, potentially based on prior knowledge such as the sensitivity analysis assumption). We investigate this empirically in our answer to Reviewers 3wVu and x1vz.\n\nWe hope this answer addresses your confusion and look forward to hearing your thoughts in follow-up.\n\n## Q2. Expected pessimistic behaviour\n*\"The behavior policy is in general a context-aware policy. So, I'd expect enforcing similarity to the behavior policy might result in some sort of context awareness. Is this the case? For instance, in the illustrative bandit example, an optimal context-independent policy only explores $a_0$ and $a_2$, which in World 2 is very different from the behavioral policy and seems to be suboptimal compared to a uniform policy. So, if I get it right, the similarity to behavioral policy is encouraged in this setting with unobserved confounders.\"*\n\nThis is a very interesting question that resonates with offline RL work, in which it was found that pessimism with respect to epistemic uncertainty, in practice, does result in regularization towards the behaviour policy (Fujimoto and Gu, 2021). Still, by exploiting the reward signal, offline RL methods result in a **different policy than that in the data**.\n\nThe behaviour policy is only accessible in the data in its marginalised form equal to $E_{\\nu(z)} [ \\pi_b (a|s,z)]$ where the true confounder distribution $\\nu(z)$ is nonidentifiable. While enforcing similarity to this marginal $\\tilde{\\pi}_b(a|s)$ should help minimise epistemic uncertainty (Fujimoto and Gu, 2021; Levine et al., 2020), we wonder how this would provide context-awareness. It is **possible** that the counterfactual $Q^{\\tilde{\\pi}_b}$ could be **less prone to delphic uncertainty** than the $Q^{\\tilde{\\pi}}$ of any other context-independent policy $\\tilde{\\pi}$. Still, as our algorithm is formulated to measure and penalise the uncertainty for the policy $\\tilde{\\pi}$ at hand, we obtain a different behaviour than simply regularising to $\\tilde{\\pi}_b$ (as implemented by our offline RL baselines). This would be an interesting theoretical investigation in further work, and we have made a note of this possible behaviour of the penalty in Section 5.2 of our revised manuscript.\n\nIt is also an interesting observation that pessimism wrt delphic uncertainty in our bandit example could result in a policy that resembles the marginalised behaviour policy. We explicitly investigate this in Appendix E.1 and detect higher delphic uncertainty on actions prone to confounding, thus identifying the optimal context-independent policy as choosing $a_1$. In terms of probabilities, it is indeed **closer to the uniform marginal policy** $\\tilde{\\pi}_b(a|s)$ than the offline RL policy, but still **differs from** -- and performs better than -- $\\tilde{\\pi}_b$."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5633/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700260784834,
                "cdate": 1700260784834,
                "tmdate": 1700260784834,
                "mdate": 1700260784834,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "io1qi8gPxK",
                "forum": "lUYY2qsRTI",
                "replyto": "1kohAlaTHb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5633/Reviewer_Lkaz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5633/Reviewer_Lkaz"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comprehensive response. I increased my score as I believe this paper is a major contribution, addressing an interesting topic with sufficient detail and support. Below are quick replies to some of the points you made:\n\nQ1. Thank you for clarifying the contributions. I agree formalizing compatible worlds is itself a contribution. On the modeling compatible worlds in practice, your choice made sense to me. Your response to other reviewers was convincing for me at this point. Still, I think alternative modeling approaches for compatible worlds will remain an outstanding challenge for future works. \n\nQ3. Table 7 and the clarification on page 8 were helpful for my understanding. This question is resolved. \n\nQ4. Your clarification was very helpful. I agree; it's interesting that, at some point, for very large $\\Gamma$s, physicians start to prefer Delphic ORL. However, this happens when $\\Gamma$ is too large. That's why I was thinking what $\\Gamma$ is a large gamma and potentially unrealistic. Anyway, it's always tricky to evaluate BC vs. offline RL by asking physicians, as BC is supposed to mimic physicians, and it's natural that they find it promising. To me, the most interesting observation is how CQL and Delphic ORL diverge in two different directions."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5633/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700519198024,
                "cdate": 1700519198024,
                "tmdate": 1700519198024,
                "mdate": 1700519198024,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NBlfr4LHx0",
            "forum": "lUYY2qsRTI",
            "replyto": "lUYY2qsRTI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5633/Reviewer_qrjW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5633/Reviewer_qrjW"
            ],
            "content": {
                "summary": {
                    "value": "The paper uses the contextual Markov Decision Process to model unobserved confounders in offline reinforcement learning. First, the authors define the class of contextual MDP that are compatible with the dataset. Then, based on a variance decomposition formula, the authors introduce the delphic uncertainty. Delphic uncertainty means the variance of policy performance across all compatible worlds. Then based on the delphic uncertainty term, the authors propose a penalty term for offline RL."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The method is applied to two real/semi-real-world medical datasets, which is very nice."
                },
                "weaknesses": {
                    "value": "The probability setup of the paper is a bit unclear to me."
                },
                "questions": {
                    "value": "1. On page 4, what does the notation $P_w \\mapsto \\Delta W$ mean? Do you mean $ P_w \\in \\Delta W$?\n\n2. Right after Def 4.1, the author proposes to model the Q function as a random element \"value model $Q_{\\theta w}$ is defined by some stochastic model\". Where is this randomness coming from? Isn't the value function of a policy just a deterministic function?\n\n3. Related to the previous remark, the meaning of Theorem 4.2 is unclear to me. Could the authors detail the probability setup for this theorem? From my understanding, there is a measure over the space of compatible worlds (by the notation $E_w$), and then there is a measure over Q-value functions (by the notation $E_{\\theta_w}$). What is the relationship between these measures?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5633/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698705496401,
            "cdate": 1698705496401,
            "tmdate": 1699636584916,
            "mdate": 1699636584916,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bN13DFmtlt",
                "forum": "lUYY2qsRTI",
                "replyto": "NBlfr4LHx0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5633/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5633/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer qrjW"
                    },
                    "comment": {
                        "value": "Dear Reviewer qrjW,\n\nThank you very much for your feedback.\n\n## Q1. Notation\nThank you for pointing out the notation issue on $P_w$. We have corrected this in our manuscript.\n\n## Q2. Value function stochasticity\n*\"Right after Def 4.1, the author proposes to model the Q function as a random element \"value model $Q_{\\theta_w}$ is defined by some stochastic model\". Where is this randomness coming from? Isn't the value function of a policy just a deterministic function?\"*\n\nIn Section 4, we consider $Q$ as a random variable capturing the sum of discounted rewards in a trajectory, $Q = \\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t, z)$, for a given $(s,a)$ pair acting as initial values. In the limit of infinite data and under no confounding, randomness in estimating this variable can be attributed to stochasticity in environment transition and reward functions or in the acting policy. This irreducible uncertainty is qualified as *aleatoric*.\n\nLimited data and hidden confounding introduce additional stochasticity in estimating $Q$. We formalise this in our next answer (Q3) where we clarify our probability setup. This approach mirrors that of prior work in offline RL (Yu et al., 2020), where stochasticity in the environment model or value function is both inherently modeled (measuring aleatoric uncertainty over $Q$) and quantified with respect to different data bootstraps (epistemic uncertainty). \n\nWe look forward to hearing your thoughts on our answer.\n\n\n## Q3. Probability setup\n\n*\"Related to the previous remark, the meaning of Theorem 4.2 is unclear to me. Could the authors detail the probability setup for this theorem? From my understanding, there is a measure over the space of compatible worlds (by the notation $E_w$), and then there is a measure over Q-value functions (by the notation $E_{\\theta_w}$). What is the relationship between these measures?\"*\n\nThank you for your question. We clarify our probability setup in the following. \n* Within the context of a world model $w$ and a parameterization of this world model $\\theta_w$, we model a **probability distribution over episode returns**, $P(Q|\\theta_w, w)$. For instance, this captures the environment stochasticity discussed above.\n* Next, the distribution of random variable $\\theta_w$ depends itself on the specific world model $w$ it parameterises (i.e. the local confounder distribution being estimated, and how policy and environment depend on it). Our data and optimisation algorithm therefore define a distribution $P(\\theta_w|w)$ **over the world model's parameter space**. \n* Finally, we posit a distribution $P(w)$ over the **compatible worlds** $\\mathcal{W}$. \n\nCombining these measures, we obtain the following decomposition for the distribution of $Q$:\n\n$$P(Q) = \\int \\int P(Q | \\theta_w, w) P(\\theta_w | w) P(w) d\\theta_w dw$$\n\nNote that in our work, $Q$ is subscripted with $\\theta_w$ as a reminder for this dependence on $\\theta$ and $w$. We have dropped this here for clarity.\n\nWithin this framework, we denote the following expectations, using $x$ as a dummy random variable:\n\n$$E [x| \\theta_w, w] = \\int x P(x | \\theta_w, w) dx$$\n$$E_{\\theta_w} [x| w] = \\int x P( \\theta_w | w) d\\theta_w$$\n$$E_{w} [x] = \\int x P( w) dw$$\n\nFor instance, this allows us to write:\n$$E [ Q ] = \\int \\int \\int Q P(Q | \\theta_w, w) P(\\theta_w | w) P(w) dQ d\\theta_w dw$$\n$$= E_{w} \\left[E_{\\theta_w} \\left[E[Q | \\theta_w, w] | w \\right] \\right]$$\n\nFor variances, we define:\n\n$$Var (x| \\theta_w, w) = \\int x^2 P(x | \\theta_w, w) dx - \\left( \\int x P(x | \\theta_w, w) dx \\right) ^2$$\n$$Var_{\\theta_w} (x| w) = \\int x^2 P( \\theta_w | w) d\\theta_w - \\left( \\int x P( \\theta_w | w) d\\theta_w \\right) ^2$$\n$$Var_{w} (x) = \\int x^2 P( w) dw - \\left( \\int x P( w) dw \\right) ^2$$\n\nFollowing the proof in Appendix B, we obtain the variance decomposition in Theorem 4.2:\n$$Var(Q)= E_{w} \\left[E_{\\theta_w} \\left[ Var(Q | \\theta_w, w) | w \\right] + Var_{\\theta_w}\\left(E[Q |  \\theta_w, w] | w\\right) \\right] + Var_{w} (E_{\\theta_w}[\\mathbb{E}[Q| \\theta_w, w]|w ])$$\n\n\nWe hope this helps clarify our probability setup and have modified our manuscript to reflect this.\n\n---\nThank you again for your feedback and questions which helped improve our paper. We would be very grateful if you would increase your score if we have addressed your remaining concerns.\n\n\n### References\n\nYu et al. Mopo: Model-based offline policy optimization. NeurIPS, 2020"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5633/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700260161335,
                "cdate": 1700260161335,
                "tmdate": 1700260161335,
                "mdate": 1700260161335,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "weVMUVdZVb",
            "forum": "lUYY2qsRTI",
            "replyto": "lUYY2qsRTI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5633/Reviewer_x1vz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5633/Reviewer_x1vz"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of offline learning an optimal independent policy. To achieve this goal, the authors propose the Delphic Offline RL algorithm that:\n1. identifies the compatible world model for confounded MDP and learn the world-dependent value function $Q_w^\\pi$;\n2. incorporates pessimistic policy optimization using the estimated delphic uncertainty;\nThe experimental results show that Delphic ORL method achieves better performance under large confounding strength."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well motivated and the author proposed an interesting idea to incorporate the delphic uncertainty in pessimistic offline policy optimization. The writing is generally clear and easy to follow."
                },
                "weaknesses": {
                    "value": "1. It seems that the estimator $\\mathbb{Var}_w (Q^\u03c0_w (s, a))$ is not an unbiased estimator for the delphic uncertainty as the other two forms of uncertainty can still enter the estimation (noting that $Q_w^\\pi$ is not the conditional expectation given by Theorem 4.2). I didn't see the author making effort to justify this point. \n\n2. Is it necessary to evaluate the delphic uncertainty on a state-action level rather than evaluating the same thing for the total reward of the whole trajectory under  policy $\\pi$? I'm not convinced of the soundness of the method here, as there might be some correlations between different state-action pairs in the Q function."
                },
                "questions": {
                    "value": "It is assumed that the offline policy is known. What if $\\pi_b$ is unavailable so that the importance sampling method has biased in estimating the value function? Is it possible to incorporate other unbiased estimation method in confounded POMDP setting like (Shi. et al, 2022)?\n\n\n### Reference:\n\nShi, Chengchun, et al. \"A minimax learning approach to off-policy evaluation in confounded partially observable markov decision processes.\" International Conference on Machine Learning. PMLR, 2022."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5633/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5633/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5633/Reviewer_x1vz"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5633/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698732815802,
            "cdate": 1698732815802,
            "tmdate": 1700625972443,
            "mdate": 1700625972443,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zlOtiBHl6v",
                "forum": "lUYY2qsRTI",
                "replyto": "weVMUVdZVb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5633/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5633/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer x1vz (1/2)"
                    },
                    "comment": {
                        "value": "Dear Reviewer x1vz, \n\nMany thanks for your feedback.\n\n## Q1. Practical definition of delphic uncertainty\n*\"It seems that the estimator $Var_{w} (Q_{w}^{\\pi} (s,a))$ is not an unbiased estimator for the delphic uncertainty as the other two forms of uncertainty can still enter the estimation (noting that $Q^{\\pi}_{w}$ is not the conditional expectation given by Theorem 4.2).\"*\n\nThank you for pointing out this possible source of confusion. \n\nFrom our practical estimator of delphic uncertainty in Section 5.1 ($Var_{w} (Q_{w}^{\\pi} (s,a))$), we recover its definition in Equation 1 ($Var_{w} (E_{\\theta_w}[E [Q_{\\theta_w}^{\\pi} |  \\theta_w, w]]$) iff $Q_{w}^{\\pi} (s,a) = E_{\\theta_w}[ E [Q_{\\theta_w}^{\\pi} |  \\theta_w, w]| w ]$.\n\n* In practice, we achieve this by implementing each world model $w$ as an ensemble, with each particle parameterised by a value of $\\theta_w$. We approximate the outer expectation over $\\theta_w$ as follows:\n$$Q_{w}^{\\pi} (s,a)= \\frac{1}{m}\\sum_{\\theta_w} \\mathbb{E} [ Q^{\\pi}_{\\theta_w}(s,a) | \\theta_w, w ]$$\nwhere $m$ is the number of particles in the world model ensemble. This sample mean is an unbiased estimator of the true expectation.\n\n* Next, the inner expectation can be obtained by implementing each ensemble particle $Q^{\\pi}_{\\theta_w}(s,a) | \\theta_w, w$ as a probabilistic model. We model the distribution of $Q^{\\pi_b}$ (details in Appendix C.1) and use its mean to estimate $Q^{\\pi}$ using importance sampling and marginalising over $z$. \n\nWe hope this decomposition clarifies how our practical implementation of delphic uncertainty approximates our theoretical definition in Section 4. These uncertainty computation details are discussed in Appendix C.1 (page 19, now in bold), but we have modified Section 5.1 to reflect this better.\n\nFinally, note that both **pessimism with respect to *both* delphic and epistemic uncertainty** is generally desirable in the confounded offline RL setting we consider. As a result, while our separation of uncertainties proves useful to theoretically and empirically validate our proposed approach, retaining a biased estimate of both uncertainties (e.g. to avoid modelling ensembles of $\\theta_w$) is still a useful quantity to penalise.\n\n## Q2. Delphic uncertainty on a trajectory-level\n\n*\"Is it necessary to evaluate the delphic uncertainty on a state-action level rather than evaluating the same thing for the total reward of the whole trajectory under policy $\\pi$ ?\"* \n\nThis is an interesting question, as we could certainly compute the variance over compatible worlds for the return of a trajectory. Our reasons for focusing on the state-action level are two-fold:\n* First, focusing on $(s,a)$ pairs allows us to leverage the **MDP structure** of our problem, and thus to pool together delphic uncertainty estimates for similar pairs in different trajectories.\n* Next, and perhaps even more importantly, estimating the uncertainty at a state-action level allows us to use this in a **practical pessimistic RL algorithm**, in which we penalize $(s,a)$ pairs with high uncertainty as in prior work (Yu et al., 2020). Leveraging uncertainty over trajectories could also be conceivable (e.g. re-weighting offline trajectories based on their cumulative delphic uncertainty), but we are not aware of state-of-the-art offline RL algorithms that take this approach.\n\n\n*\"I'm not convinced of the soundness of the method here, as there might be some correlations between different state-action pairs in the Q function.\"*\n\nYu et al., 2020, also propose a method of estimating uncertainty at the state-action level (over the transition function in their model-based setting) and prove that penalising the value in such a way results in a provably pessimistic behaviour (see their Theorem 4.4) and a practical, effective algorithm. Correlations between state-action pairs do not pose a problem in their framework, but we are curious to discuss more if this remains a concern.\n\n\n## Q3. Identifiability of the behavioural policy\n*\"It is assumed that the offline policy is known. What if  $\\pi_b$ is unavailable so that the importance sampling method has biased in estimating the value function?\"*\n\nTo clarify, our work does not assume that $\\pi_b$ is known or identifiable. We learn it in the context of the local confounder distribution $\\nu(z)$ in each world model. If $\\pi_b$ were known, confounding would not be a source of error, and our problem setting would correspond to offline RL with partial-observability. In this setting (unlike the confounded one), $z$ could be identified from its effect on transitions or rewards -- up to epistemic and aleatoric uncertainty."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5633/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259591746,
                "cdate": 1700259591746,
                "tmdate": 1700259591746,
                "mdate": 1700259591746,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Cp3ieZJiv6",
                "forum": "lUYY2qsRTI",
                "replyto": "weVMUVdZVb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5633/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5633/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer x1vz (2/2)"
                    },
                    "comment": {
                        "value": "## Q4. Partial identification methods\n*\"Is it possible to incorporate other unbiased estimation method in confounded POMDP setting like (Shi. et al, 2022)?\"*\n\nThank you for providing this additional reference, we will ensure to cite it as an additional related work.\n\nThe challenge of nonidentifiable confounding bias in deep offline RL is paramount (Gottesman et al., 2018; Tennenholtz et al., 2022), but remained unaddressed until now. Prior works consider the challenge in OPE (Kallus and Zhou, 2020; Shi et al., 2022) or online RL (Lu et al., 2018; Zhang and Bareinboim, 2019), but our work is the *first* to propose a practical solution to hidden confounding bias in offline RL. Our work builds on two key contributions:\n1. Defining compatible worlds, i.e. that there exists a set of models that maximise the likelihood of the data but may disagree over counterfactual quantities. We propose one (out of many possible) implementation to find this set, but this is not the focus of our work. In fact, as noted in Table 1, any **other partial identification assumption** (e.g. known $\\Gamma$ in sensitivity analysis) **could be adopted** to achieve this. We now also emphasize this point in Section 5.1.\n2. Next, and more importantly, we propose a practical offline RL algorithm to mitigate this challenge. Rather than optimising for worst-case returns and obtaining excessively pessimistic behaviour (as is done in prior work, e.g. Shi et al., 2022, Wang et al., 2020; Kallus and Zhou, 2019), we propose to **measure uncertainty and to apply a pessimism penalty**. This is motivated by the theoretical guarantees and empirical success of offline RL algorithms that follow this approach (Levine et al., 2020; Yu et al., 2020).\n\nFrom this perspective, we agree that the following baselines are useful to measure the added value of our approach. We present our results below, for the sepsis environment with $\\Gamma=46$, and include this in Appendix E.2, page 28.\n1. comparing to the sensitivity analysis framework for estimating the set of compatible worlds (Kallus and Zhou, 2019). We implement this through our variational approach, discarding world models that do not satisfy the $\\Gamma$ assumption (all but 2 out of 10 trained compatible world models). This results in a poorer estimate of delphic uncertainty as the resulting delphic ORL performance is negatively affected. [Updated Nov 19]\n\n2. comparing to worst-case value optimisation (Wang et al., 2020; Kallus and Zhou, 2019).  As expected, optimising for the worst-case value function is excessively pessimistic:\n\n| Compatible worlds | Offline RL Algorithm | Environment Returns |\n|---|---|---|\n| Variational (ours) | Pessimism  (ours) | 54.9 $\\pm$ 4.7 |\n| Variational  | Worst-case (Wang et al., 2020) | 18.1 $\\pm$ 2.0  |\n| Sensitivity analysis bound (Kallus and Zhou, 2019) | Pessimism      | 32.7 $\\pm$ 4.8      |\n| Sensitivity analysis bound (Kallus and Zhou, 2019) | Worst-case (Wang et al., 2020) | 18.6 $\\pm$ 2.3      |\n---\n\nThank you again for your feedback and questions which helped improve our paper. We would be very grateful if you would increase your score if we have addressed your remaining concerns.\n\n### References\n\nGottesman et al. Evaluating reinforcement learning algorithms in observational health settings. arXiv preprint arXiv:1805.12298, 2018.\n\nKallus and Zhou. Confounding-robust policy evaluation in infinite-horizon reinforcement learning. NeurIPS, 2020.\n\nLevine et al. Offline RL: Tutorial, review, and perspectives on open problems. 2020.\n\nLu et al. Deconfounding reinforcement learning in observational settings. arXiv:1812.10576, 2018.\n\nTennenholtz, et al. On covariate shift of latent confounders in imitation and reinforcement learning. ICLR, 2022.\n\nWang et al. Provably efficient causal reinforcement learning with confounded observational data. NeurIPS, 2021.\n\nYu et al. Mopo: Model-based offline policy optimization. NeurIPS, 2020\n\nZhang, J. and Bareinboim, E. Near-optimal reinforcement learning in dynamic treatment regimes. NeurIPS, 2019."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5633/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259716928,
                "cdate": 1700259716928,
                "tmdate": 1700422308180,
                "mdate": 1700422308180,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lrZx3JzUtd",
                "forum": "lUYY2qsRTI",
                "replyto": "47GeK1Tqtf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5633/Reviewer_x1vz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5633/Reviewer_x1vz"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for clarifying their methods on estimation of the delphic uncertainty and incorporating pessimism in a practical way. Based on the authors' response, I believe that the authors can make a better argument on these points in the revision and perhaps unify the penalties for different sources of uncertainty in the algorithm (while I understand that the delphic uncertainty is still the highlight). I have increased my score accordingly."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5633/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625904159,
                "cdate": 1700625904159,
                "tmdate": 1700625904159,
                "mdate": 1700625904159,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "i32Fx1PUjq",
            "forum": "lUYY2qsRTI",
            "replyto": "lUYY2qsRTI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5633/Reviewer_3wVu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5633/Reviewer_3wVu"
            ],
            "content": {
                "summary": {
                    "value": "* The authors address the unobserved confounding problem in offline reinforcement learning with pessimism over possible \"worlds\" (confounder values) compatible with the observation (distribution of trajectory).\n* They define a new type of uncertainty \"Delphic uncertainty\" as the variance of Q value over the compatible (thus unidentifiable) worlds with theoretical decomposition with other types of uncertainties.\n* Simulated evaluation and evaluation by experts clearly indicated that their method outperformed existing methods that do not address the Delphic uncertainty such as CQL and BC when strongly confounded."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "* The unobserved confounding is a major issue in offline reinforcement learning.\n* They investigate a minimal problem setting (contextual MDP) to reproduce it and propose a simple and intuitive method that models the uncertainty related to the confounding.\n* The theory that decomposes the variance into several types of uncertainties motivates the approach well.\n* Empirical evidence including evaluation by experts clearly supports their claim."
                },
                "weaknesses": {
                    "value": "1. Baselines and environments tested are relatively limited (see also Question 1 and 2).\n1. Not being a major concern, it would be more intuitively superior if an end-to-end formulation was possible, as in the CQL. The proposed method is divided into a step of learning multiple possible worlds and a step of pessimism using them."
                },
                "questions": {
                    "value": "1. Intuitively, it seems that estimating $z$ from the trajectory and using it as $\\pi(a|s,z)$ as in POMDP methods would improve performance for later steps $t$, but is such an extension possible? Also, is the proposed method still superior when such a POMDP method is used as a baseline? I'm wondering if the online identification of the world is possible within an episode through such a formulation.\n1. The existing approaches for a similar setting are discussed (e.g. using partial identification) but not compared. Isn't it possible to compare them?\n1. Is the $\\max$ taken w.r.t. not only $z,z'$ but also $s,a$? If not, how $\\Gamma(s,a)$ is summarized for an environment?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5633/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5633/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5633/Reviewer_3wVu"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5633/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698837594632,
            "cdate": 1698837594632,
            "tmdate": 1699636584735,
            "mdate": 1699636584735,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FFOBF0UqSW",
                "forum": "lUYY2qsRTI",
                "replyto": "i32Fx1PUjq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5633/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5633/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3wVu (1/2)"
                    },
                    "comment": {
                        "value": "Dear Reviewer 3wVu,\n\nMany thanks for taking the time to read our work and for your positive feedback.\n\n## Q1. Leveraging confounder estimation at inference time\n*\"Intuitively, it seems that estimating $z$ from the trajectory and using it as $\\pi(a|s,z)$ as in POMDP methods would improve performance for later steps $t$, but is such an extension possible? Also, is the proposed method still superior when such a POMDP method is used as a baseline? I'm wondering if the online identification of the world is possible within an episode through such a formulation.\"*\n \nThank you for this great question, which really highlights the challenge of our problem setting.\n\nIn Appendix E.2, we investigate whether estimating and using confounder values $z$ at inference time helps improve the performance of our main baseline CQL. We implement this by allowing both $\\{Q,\\pi\\}$ models to access the history of trajectories (from which $z$ can be estimated). We reproduce our results from Table 6 below: we find that **history-dependence actually degrades performance**.\n\n| Algorithm  | Environment Returns |\n|---|---|\n| BC | 38.5 $\\pm$ 4.5      |\n| BCQ   | 18.4 $\\pm$ 2.6      |\n| CQL   | 31.1 $\\pm$ 3.3      |\n| CQL with history-dependent $Q$      | 26.9 $\\pm$ 3.1      |\n| CQL with history-dependent $Q, \\pi$ | 23.5 $\\pm$ 2.0      |\n| Delphic Offline RL | 54.9 $\\pm$ 4.7      |\n\nThis result can be explained as follows. In partially-observed offline settings, incorporating a learned estimate for the confounders $z$ based on the history can be even more prone to confounding, as the identification model is trained on the history distribution of the *behavioural policy* and not that of the inference policy. This leads to a compounding of distribution shift at inference time, showing \"latching behaviour\" in imitation (Swamy et al., 2022) and \"model delusion\" in offline RL (Ortega et al., 2021). For this reason, in our approach to the problem, we chose to focus on history-independent policies where confounding biases only affect the value function, which we directly address through pessimism.\n\n## Q2. Partial identification baselines\n\n*\"The existing approaches for a similar setting are discussed (e.g. using partial identification) but not compared. Isn't it possible to compare them?\"*\n\nThe challenge of nonidentifiable confounding bias in deep offline RL is paramount (Gottesman et al., 2018; Tennenholtz et al., 2022), but remained unaddressed until now. Prior works consider the challenge in OPE (Kallus and Zhou, 2020), online RL (Lu et al., 2018; Zhang and Bareinboim, 2019), or using proxy variables (Wang et al., 2021), but our work is the *first* to propose a practical solution to hidden confounding bias in offline RL.\n\nOur work builds on two key contributions:\n1. Defining compatible worlds, the set of models that maximise the likelihood of the data but disagree over counterfactual quantities. We propose one (out of many possible) implementation to find this set, but this is not the focus of our work. In fact, as noted in Table 1, any **other partial identification assumption** (e.g. known $\\Gamma$ in sensitivity analysis) **could be adopted** to achieve this. We now also emphasize this point in Section 5.1.\n2. Next, and more importantly, we propose a practical offline RL algorithm to mitigate this challenge. Rather than optimising for worst-case returns and obtaining excessively pessimistic behaviour (as is done in prior work, e.g. Wang et al., 2020; Kallus and Zhou, 2019), we propose to **measure uncertainty and to apply a pessimism penalty**. This is motivated by the theoretical guarantees and empirical success of offline RL algorithms that follow this approach (Levine et al., 2020; Yu et al., 2020).\n\nFrom this perspective, we agree that the following baselines are useful to measure the added value of our approach. We present our results below, for the sepsis environment with $\\Gamma=46$, and include this in Appendix E.2, page 28.\n1. comparing to the sensitivity analysis framework for estimating the set of compatible worlds (Kallus and Zhou, 2019). We implement this through our variational approach, discarding world models that do not satisfy the $\\Gamma$ assumption (all but 2 out of 10 trained compatible world models). This results in a poorer estimate of delphic uncertainty as the resulting delphic ORL performance is negatively affected. [Updated Nov 19]\n\n2. comparing to worst-case value optimisation (Wang et al., 2020; Kallus and Zhou, 2019).  As expected, optimising for the worst-case value function is excessively pessimistic:\n\n| Compatible worlds | Offline RL Algorithm | Environment Returns |\n|---|---|---|\n| Variational (ours) | Pessimism  (ours) | 54.9 $\\pm$ 4.7 |\n| Variational  | Worst-case (Wang et al., 2020) | 18.1 $\\pm$ 2.0  |\n| Sensitivity analysis bound (Kallus and Zhou, 2019) | Pessimism      | 32.7 $\\pm$ 4.8      |\n| Sensitivity analysis bound (Kallus and Zhou, 2019) | Worst-case (Wang et al., 2020) | 18.6 $\\pm$ 2.3      |"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5633/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700258513150,
                "cdate": 1700258513150,
                "tmdate": 1700422254752,
                "mdate": 1700422254752,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7Ld3aP70W2",
                "forum": "lUYY2qsRTI",
                "replyto": "t07x4FQDD6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5633/Reviewer_3wVu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5633/Reviewer_3wVu"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the additional results and discussion.\nFor Q2, the contribution would be clearer in a discussion comparing such methods that have similar concepts.\nFor Q1, a fully history-dependent policy may be too complex, but it may be conceivable to design a (semi-)online policy that schedules the degree of optimism/pessimism to identify the world and behaves optimistically while $t$ is small.\nAnyway, based on the additional experimental results, I am generally satisfied with the authors' answers and I would like to keep this score."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5633/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700577966214,
                "cdate": 1700577966214,
                "tmdate": 1700577966214,
                "mdate": 1700577966214,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]