[
    {
        "title": "Learning transferrable and interpretable representation for brain network"
    },
    {
        "review": {
            "id": "Mmwlj1LXKE",
            "forum": "ajG8vLTHh5",
            "replyto": "ajG8vLTHh5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8443/Reviewer_nn5J"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8443/Reviewer_nn5J"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a Brain Masked Auto-Encoder (BrainMAE) that consists of two main components: an embedding-based graph attention mechanism and a self-supervised masked auto-encoding framework. BrainMAE uses a static graph transient state encoder (SG-TSE) and dynamic graph TSE (DG-TSE) to learn ROI representations. The trained ROI embeddings showed the distinctive traits of ROIs, resulting in the consistency of ROI embeddings across the different datasets. BrainMAE achieved improved performances in three distinct downstream tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is written clearly and well-organized.\nThe analysis of ROI embeddings for inter-individual consistency demonstrates the generalizability of ROI embeddings. \nThe paper conducts numerous experiments to validate the efficacy of the proposed BrainMAE, outperforming the comparative methods."
                },
                "weaknesses": {
                    "value": "The proposed BrainMAE is based on self-supervised masked auto-encoding, so it needs to be compared with the existing self-supervised learning-based models, including the following:\n[1] Shi, Chenwei, et al. \"Self-supervised pretraining improves the performance of classification of task functional magnetic resonance imaging.\" Frontiers in Neuroscience 17 (2023).\n[2] Malkiel, Itzik, et al. \"Self-supervised transformers for fMRI representation.\" International Conference on Medical Imaging with Deep Learning. PMLR, 2022.\n[3] Thomas, Armin, Christopher R\u00e9, and Russell Poldrack. \"Self-supervised learning of brain dynamics from broad neuroimaging data.\" Advances in Neural Information Processing Systems 35 (2022): 21255-21269.\n  \nMany fMRI studies have demonstrated that the length of each time segment significantly affects the performance. Most related studies have empirically converged to window size values between 30 and 60 seconds [4]. However, the proposed method uses 15 seconds, which is too short for the window size. Do you have a rationale for this window size?\n[4] Savva, Antonis D., Georgios D. Mitsis, and George K. Matsopoulos. \"Assessment of dynamic functional connectivity in resting\u2010state fMRI using the sliding window technique.\" Brain and Behavior 9.4 (2019): e01255.\n \nSince the word embedding includes information about its meaning, the word embeddings in NLP can be used in all sentence positions.\nHowever, the position of ROIs is never changed in training sessions, which could limit the model's ability to learn ROI traits but could learn only absolute position information. It can be considered as learnable positional encoding. Since the graph attention mechanism is permutation-invariant, the changes in ROI order should not affect the performance if the ROI embeddings have their characteristics. Even though the paper shows the evaluation of pretrained ROI embeddings in Figure 3, it still needs additional evidence that these embeddings do not incorporate absolute position information.\n\nAfter learning, the fixed ROI embeddings E are used repeatedly in SG-TSE for an attention mechanism. However, since the same attention is obtained with a fixed Q and K, there is no need to use a transformer block.\n\nOne of the main reasons for learning and exploiting the ROI embeddings was to mitigate and circumvent the inherent noise in the fMRI signal. However, from the experimental results, the authors concluded that the lower performance of DG-BrainMAE than that of SG-BrainMAE was due to such inherent noise. These are conflicting."
                },
                "questions": {
                    "value": "Check the comments in Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8443/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698195043906,
            "cdate": 1698195043906,
            "tmdate": 1699637053187,
            "mdate": 1699637053187,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OMSWocpM3J",
                "forum": "ajG8vLTHh5",
                "replyto": "Mmwlj1LXKE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8443/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8443/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nn5J (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for this insightful review! We sincerely appreciate your time in reading the paper. The comments and concerns are extremely helpful for us to improve the paper.  We've incorporated several new analyses to address these concerns.\n\n> Q1: Comparison with the existing self-supervised learning-based models.\n\nThank you for raising this concern. Following your suggestion, we added a downstream experiment in $\\textbf{Appendix F}$ of our revised submission using the HCP Task Dataset, same to the setup in [1]. The models are trained to make classification between 20 mental states using fMRI signals, with each state lasting tens of seconds \u2014 a notable difference from the downstream tasks in section 3.3. Despite this, $\\textbf{Tables 10 and 11}$ show our models outperforming other state-of-the-art, self-supervised learning models designed for transient state modeling. Further details are available in $\\textbf{Appendix F}$.\n    \nAdditionally, we added $\\textbf{Appendix G.3}$ to compare our model's performance in gender prediction with [2], as shown in $\\textbf{Table 15}$. Our model outperforms TTF by a significant margin.\n    \nWe haven't included a comparison with [3] due to the unavailability of their code. However, we would be happy to compare with them and following their exact experiment setup for our model evaluation if you have any additional concerns.\n\n> Q2: Why use 15 seconds for the length of time segment?\n    \nThanks for bringing up this question. It is true that computing dynamic FC usually requires longer time window (30 to 60 seconds) to handle spurious connectivity as well as intrinsic noise within the fMRI. However, distinct from computing dynamic FC, our model exhibits another advancement by embracing recent insights into \"transient state\" in neuroscience research. The slow cortical fluctuations are hypothesized to fluctuate at ~0.1 Hz [4], and in [5] a 10s time window was used for track the state changes. On the other hand, recent study suggest brain exhibits spatial-temporal dynamics has been shown as a major contributor to functional connectivity (FC). These fluctuations take the form from 10 seconds to 20 seconds [6][7]. Therefore, in our study, to incorporate such new understanding about the brain, we choose the 15 seconds as the length of the time segments. For handling the fMRI intrinsic noise, we propose the embedding-informed attention module with constraints encoded within the ROI embeddings.\n    \n> Q3: Graph attention mechanism is permutation-invariant.\n    \nApologies for the confusion. To clarify, in our implementation of embedding-informed graph attention, we use a transformer block, where the ROI embeddings are linearly transformed using different learnable weight matrices for the key and query. This design makes the adjacency matrix inherently asymmetric, enabling it to capture the asymmetric information flow between ROIs. The introduction of these learnable weight matrices means that the attention mechanism $\\textbf{isn't permutation invariant}$. We have thoroughly revised Section 2.1 in our revised submission to better explain this aspect.\n\n> Q4: More evidence to justify the learned ROI embeddings does not only incorporate absolute position information.\n\nThanks for bringing up the insightful comments on the ROI embedding. To some extent, we believe the ROI embedding learn the position information as it itself has real positional meaning in the brain. But other than that, the ROI embeddings are designed to learn the functional information of the ROIs. To justify this hypothesis, other than the results shown in Figure 3, we additionally performed two analyses from different perspectives.\n\n1. Age Effect on Learned ROI Embeddings ($\\textbf{Appendix D.3}$): We partition the HCP-Aging dataset into young, middle, old age groups and learn the ROI embeddings for each of the groups. We form graphs based on the similarity of the ROI embeddings and study the modualr structure of the resulting graph. The resulting modularity shows decrease with age, consistent with established findings in the neuroscience literature [8]. This result suggests that indeed there is functional information contained within the ROI embeddings that reflect aging. \n    \n2. Replacing ROI Embeddings with Position Embedding ($\\textbf{Appendix G.1 Analysis1}$): We replace the learnable ROI embedding with fixed position embedding, and follow the same pre-training and fine-tuning procedure. The resulting model's performance significantly dropped in downstream tasks compared to SG-BrainMAE, highlighting the importance of learned ROI information.\n    \nIn summary, the ROI embeddings contain valuable information, evident in age-related differences and enhanced transfer learning performance."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8443/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700289577034,
                "cdate": 1700289577034,
                "tmdate": 1700290238656,
                "mdate": 1700290238656,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "21DeWkrXPC",
            "forum": "ajG8vLTHh5",
            "replyto": "ajG8vLTHh5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8443/Reviewer_DteR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8443/Reviewer_DteR"
            ],
            "content": {
                "summary": {
                    "value": "The paper explores the use of functional magnetic resonance imaging (fMRI) and its representation as graph of regions of interest (ROIs) to model the human brain. It categorises these graphs based on the way the functional connectivity (FC) is handled: (1) Fixed-FC, which relies on the FC representing the linear temporal correlations within the brain network, and (2) Dynamic-FC  which aims to model the evolving FC profile over time. Each one of these two approaches has their own pros and cons in the literature. To address this problem, the paper introduces the Brain Masked Auto-Encoder (BrainMAE), which consistently outperforms existing models in various tasks. Furthermore, leveraging the model's inherent interpretability, the paper provides insights into how the model its making its decisions. BrainMAE combines a graph attention mechanism and masked autoencoding to effectively capture the dynamics of brain activity while handling the inherent noise in fMRI data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I found the approach in the paper interesting and novel, with the key strength being the obvious consistent outperformance to other models in the literature. Differently to common papers in this area, the paper systematically analysed both classification and regression tasks, the latter being known to be more challenging and thus sometimes not explored in methodological literature. With the exceptions that I explain in \"Weaknesses\", overall the paper is clear and the reader can understand what are the different components, as well as their distinct contributions in the ablation analysis provided. The way the paper introduced the three characteristics that each ROI embedding should have (in section 2.1) is also a strength of this paper, and thus making the evaluation of the paper significant, as it is later explored in a satisfactory way in section 3.2.3. The interpretability part is insightful and positive - while showing that expected results are achieved (e.g., with gender being in the 1st principal component), it also shows other unexpected results for future discussion by the community.\n\nI would like to say that the apparent similarities between the representations of brain regions and words in natural languages (as described in \"ROI Embeddings\" in Section 2.1) is interesting and new to me, and therefore I found this framing a (weaker) strength of the paper. Finally, the balance between figures and tables is devised well, bringing more points to both the quality and clarity of this work.\n\nBased on the good results compared with previous literature, and the novel way to combine previous methods in deep learning literature into a new field, I recommend acceptance. I only recommend marginally above the acceptance threshold at the moment because of what I've written in the Weaknesses and Questions sections of my review."
                },
                "weaknesses": {
                    "value": "Beyond the questions and suggestions I will leave in \"Questions\", I identify what for me are two weaknesses of this paper:\n\n1. The paper doesn't use \"traditional\" ML models (eg, SVM, random forests) directly on the flatten upper-triangle of the FC matrix as baselines. Based on my experience when running new deep learning models on FC connectivity, what I've found is that a simple traditional ML model (with a reasonable simple hyperparameter search) achieves comparable, if not better, metrics than many DL-based models on some datasets. Therefore, to evaluate the utility of this model, it would be important to see how BrainMAE compares to these more \"traditional\" ML models.\n2. The paper is missing important metrics (e.g., sensitivity and specificity) in the evaluation section of the (binary) gender prediction. Given there's an appendix, and well-known limitations with accuracy and AUROC, I don't think it is a subjective comment to request these two other metrics in a paper that has a clear connection with the medical domain.\n\n\nTwo minor weaknesses that I've identified are:\n1. Page 5 includes a part that is confusing in terms of readability. We have figure 2 mentioning SG-, DG-, and FDG- models, but right below in \"BrainMAE Variants\" only two are mentioned. Technically, they are not connected in the flow of the text, but visually they are together in the paper.\n2. I understand that it's difficult to come up with these names, but in a work in which there is a key division between Static- and Dynamic-fMRI modelling, I find the static/dynamic naming for the graph attention component not the best choice as they seem to convey different meanings (one relates to use or not the entire fMRI timeseries, and the other relates to whether use the timeseries directly in the representation or one changed by an attention mechanism). Thus, I think it doesn't help with the clarity/readability of the paper to use these terms in two distinct contexts (unless I've missed something in the reasoning of the authors)."
                },
                "questions": {
                    "value": "1. Was there a particular reason for the authors to choose masked AEs in favour of other AEs? From the Introduction section it doesn't seem clear to me why this was the case. If the reason is just that they haven't been explored before in this field, I think it's a reasonable motivation (and results show it might have been a good choice). However, it would be good to say it why this was preferred to, for example, (V)AEs or other encoding variants as motivation for this new method.\n2. I found the description on the \"Static/Dynamic graphs attention\" component confusing and unexpected. Calling it graph \"attention\" led me to think that some learning (for example in the form of some GNN) would happen, but from Section 2.1, it seems there's no learnable parameters, and what happens is that the representations are updated just once before being inputted to the TSE modules. If this is the case, and considering the important mentions to GNNs in the abstract, Introduction and Related Work sections, why haven't the authors decided to use a GNN instead of an attention transformation on a graph representation followed by a transformer? The authors do compare their work to other GNN-based baselines, but no baseline seems to have the same pipeline and pretraining procedure for a more direct comparison on the utility of GNNs. I do understand that at the end of the day there are methodological decisions that just need to be made, but this one is so close to GNNs that I do not understand why it hasn't been done, and would appreciate a clarification. \n3. Why have self-loops been removed in the graph attention component? Self-loops are common standard in graph/GNN literature to keep the information from the initial node on the representation. Thus, it's not clear why the authors made this decision and why that's not at least in the ablation analysis.\n4. The paper does a very good job explaining the different components of the model with regards to how it learns/creates the unsupervised brain representation. However, the paper gets a bit confusing (and even a bit unexpected) when in section 2.1 it is said in the \"Autoencoder\" component that the decoder is only used in pre-training phase (why is there such a division?); then, on section 3.1 when it is said that behaviour/demographic measurements are used (for what?); then, section 3.2.3 once again mentions that somehow we have a pretrained model for all datasets (why so much computational complexity?). Only in section 3.3.1 it is explained that the authors appended a task-specific linear head based on previous studies, and tables 1/2/etc also make it obvious that there are classification and regression tasks. For all these reasons, I hope it's clear why figure 1 (and consequently section 2) needs a quick mention to this task-specific head for better clarity/readability.\n5. Why haven't the authors included the task-specific fMRI data from HCP, but decided to include the task-specific fMRI data from the NSD dataset?\n6. Can the authors clarify what is the difference between the DG- and FDG-BrainMAE models? The FDG-BrainMAE is introduced as the model but without the \"static graph mechanism\". Isn't this basically the same as the DG-BrainMAE? My guess is that what the authors mean is that instead of the static graph mechanism in *the first layer* of DG-TSE, the FDG-BrainMAE's TSE module used only the dynamic graph representation across all blocks from the very beginning?\n7. How are the folds created in the 5-fold cross-validation procedure? Is it in a stratified fashion independently for each downstream task? Are the authors careful to include subjects never seen before in training completely separated in the test sets?\n8. Do the authors have any hypothesis on why the initial and final task blocks were particularly important for the model to make the final predictions?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8443/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698300706879,
            "cdate": 1698300706879,
            "tmdate": 1699637053077,
            "mdate": 1699637053077,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0Aa29KjSIC",
                "forum": "ajG8vLTHh5",
                "replyto": "21DeWkrXPC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8443/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8443/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer DteR (1/3)"
                    },
                    "comment": {
                        "value": "We greatly appreciate your excellent summary of the key aspects of our paper and careful examination of our paper. Thank you for the careful and insightful review! We've incorporated many of your suggestions. Below, we respond to each individual point from the review.\n\n> Q1: Missing comparison with traditional ML models.\n\nThank you for raising this point. To address this concern, we analyzed the performance on the three downstream tasks with three classification models (SVM, Logistic Regression and Random Forrests), and three regression models (ordinary linear regression, ridge regression, Elastic Net) with details in $\\textbf{Appendix G.4}$ and $\\textbf{Tables 16-19}$ of our latest revised submission. \n\nIndeed, traditional ML models demonstrate performance levels comparable to those of baseline deep learning methods in Gender classification tasks. This observation can be attributed to the well-established understanding that FC matrix harbors significant information of human traits. However, for more complex regression tasks, such as task performance prediction on NSD dataset, which necessitate inferring intricate brain states from the dynamic of fMRI signals, simple ML models are less effective. In such scenarios, deep learning methods, endowed with the robust capability for representation learning, are able to achieve superior results. Nevertheless, our models continuously outperform these baseline methods by significant margin.\n> Q2: Missing metrics (e.g., sensitivity and specificity) in classification evaluation.\n    \nThank you for pointing this out. We acknowledge the importance of these metrics and have added them (sensitivity, specificity, and F1-score) to $\\textbf{Tables 16-19}$ in the appendix of our revised submission. With these additional metrics, our method continues to significantly outperform other baseline models.\n\n> Q3: Readability in Page 5 regarding FDG-BrainMAE.\n    \nThank you for bringing this to our attention. To address this confusion, we will rename the FDG-BrainMAE to 'vanilla-BrainMAE' in our final revision. This reflects its use of TSE without embedding-informed graph attention, aligning with the typical use of 'vanilla' as model for comparison, which should improve clarity.\n\n> Q4: Confusion regarding the static/dynamic naming for the graph attention.\n    \nThank you for the suggestion. We acknowledge that the terms \"static/dynamic\" might be confusing in the context of fMRI, where \"dynamic\" typically refers to the time-evolving nature of fMRI signals. In our model, these terms were intended to denote spatially static or dynamic graphs for capturing transient representations. To clarify, we will rename 'dynamic' to 'adaptive' to better reflect the self-attention's adaptability to input signals in our final revision. Consequently, DG-TSE and DG-BrainMAE will be referred to as Adaptive-Graph TSE (AG-TSE) and Adaptive-Graph BrainMAE (AG-BrainMAE), respectively. This new terminology should reduce confusion with the convention.\n\n> Q5: Why choose masked AEs in favor of other AEs?\n    \nOur motivation stems from the idea that masking a large set of ROIs and predicting their signals from unmasked ROIs can enhance the model's understanding of ROI relationships and improve ROI embedding learning. This approach is a key component of our method and has shown robust ROI representation learning. Additionally, in $\\textbf{section 3.3.4's Ablation Study}$, we compared FDG-BrainMAE and FDG-BrainAE. Despite having the same architecture, they differ in pretraining methods: masked autoencoding for FDG-BrainMAE and standard autoencoding for FDG-BrainAE. Our results show that FDG-BrainMAE consistently surpasses FDG-BrainAE in all three downstream tasks by large margin, highlighting its effectiveness."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8443/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700288890660,
                "cdate": 1700288890660,
                "tmdate": 1700288890660,
                "mdate": 1700288890660,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mZ7zJ3XBdG",
                "forum": "ajG8vLTHh5",
                "replyto": "21DeWkrXPC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8443/Reviewer_DteR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8443/Reviewer_DteR"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the really very good work on this new revision of the paper! Well done! \n\nI would suggest the authors to include most of the explanations they gave me in which I was very confused regarding parts of the paper. I believe particularly the one explaining the difference between the DG- and FDG-BrainMAE models would really benefit the readability of the paper, as the explanations are very simple but when I read the paper I believe it is not clear. \nI am a bit concerned about the fact that in the three downstream tasks, the authors did not use stratified cross-validation nor separated training and testing set based on subject identity. The fact they included that in the newly added HCP task-specific classification is reassuring, though unfortunately not completely \"perfect\" for the entire paper.\n\nBefore I change my scores, I just have two follow-up questions:\n1. How was the hyperparameter selection/search done for the newly added traditional ML models?\n2. I think the choices of renaming \"FDG-\" to \"vanilla-\" and \"Dynamic-\" to \"Adaptive-\" are good ones! I was just wondering why haven't the authors included these changes yet given all the work they already did for this new version of the paper? Just lack of time?"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8443/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700481867558,
                "cdate": 1700481867558,
                "tmdate": 1700481867558,
                "mdate": 1700481867558,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "M4AjIgiN1W",
                "forum": "ajG8vLTHh5",
                "replyto": "Xkn7z1ro8f",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8443/Reviewer_DteR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8443/Reviewer_DteR"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your further clarification. I'd say you should include those details about the hyperparameter search on the traditional ML models in appendix. As a suggestion, you might want to consider using random search in the future.\n\nI don't have any further questions, and I'll wait for the other reviewers before making a final decision on how to change my scores. Thanks once again for your time tackling my review."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8443/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700578804546,
                "cdate": 1700578804546,
                "tmdate": 1700578804546,
                "mdate": 1700578804546,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lYd2zMN2Go",
            "forum": "ajG8vLTHh5",
            "replyto": "ajG8vLTHh5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8443/Reviewer_3Zps"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8443/Reviewer_3Zps"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a method BrainMAE which uses Graph Neural Networks (GNNs) to analyze functional magnetic resonance imaging (fMRI) data for understanding brain network connectivity. BrainMAE employs a Transformer-based architecture to reconstruct masked segments of fMRI signals. The approach is validated by its ability to encode functional connectivity in a reproducible manner across different datasets and downstream tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is well-written and easy to follow.\n2. The paper is well structured and has a smooth and concrete flow.\n3. The experimental part in this paper is abundant."
                },
                "weaknesses": {
                    "value": "1. While the BrainMAE model is presented as a fresh approach to fMRI data, it's hard to overlook the fact that all Transformer Layers, Masked Autoencoders (MAE) and Dynamic Graph Neural Networks are somewhat antiquated techniques. These methods are not only dated in the broader machine learning landscape but have also lost their novelty in computational neuroscience applications [1,2,3]. Frankly, the proposed method comes across as a 'engineering model'.\n2. No theoretical guarantees of the proposed method. The BrainMAE seems to be too heuristic. \n3. You mention your method is 'interpretable' in the title. However, I don't figure out the true interpretability of the method since BrainMAE has no scientific inductive bias incorporated. This is just a stacking of deep-learning-based techniques, which make the model highly black-boxed. Please refer to the Questions Section for my further concerns.\n\n\n[1] MAEEG: Masked Auto-encoder for EEG Representation Learning.\n[2] Pooling regularized graph neural network for fmri biomarker analysis.\n[3] Graph neural network for interpreting task-fmri biomarkers."
                },
                "questions": {
                    "value": "For your Subsection 3.3.5, Representation and Interpretation Analysis. I hold the point that analyzing the representations of fMRI using principal component analysis (PCA) and self-attention scores are too post hoc  and lacks scientific insights. In fact, the representations extracted by BrainMAE and attention layers are still black-box."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I have no concerns at this step."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8443/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8443/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8443/Reviewer_3Zps"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8443/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698725711649,
            "cdate": 1698725711649,
            "tmdate": 1700613740942,
            "mdate": 1700613740942,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "llWsVOdLhr",
                "forum": "ajG8vLTHh5",
                "replyto": "lYd2zMN2Go",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8443/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8443/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3Zps (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for bringing up the concerns and for your insightful feedback. We apologize for the lack of clarity in our initial description. In response, we have revised relevant sections of the paper to more clearly highlight the scientific inductive biases. We've additionally conducted comprehensive analyses of the intermediate model generated representations and aiming to more directly connect them with scientific insights. Below, we respond to each individual point from the review.\n> Q1: The proposed methods have lost their novelty in computational neuroscience applications and come across as a 'engineering model'.\n\nThank you for raising this concern. ML methods are indeed not new in computational neuroscience, but there are inherent challenges with fMRI, a critical tool for brain research. Unlike other signals such as EEG, fMRI's unique temporal resolution and spatial coverage require distinct modeling approaches.\n\nMany existing ML models for fMRI, including the two papers you cited, primarily compute functional connectivity matrices and often overlook the importance of dynamic brain state changes. These dynamics carry rich information, proved to be a major contribution to the FC, and relates to a variety of brain function, showing promise for understanding how the brain actually works[1].\n    \nOn the other hand, deep learning methods that consider fMRI's dynamic nature frequently underperform compared to fixed-FC approaches, partly due to fMRI's inherent noise. Moreover, several dynamic-FC approaches employ extended time windows (30-60 seconds) to manage noise and spurious connections, but this compromises their sensitivity to brief, transient brain state changes that typically happen within 10-20 seconds[1][2].\n    \nOur model addresses these challenges by introducing a novel ROI embedding-informed attention mechanism and a uniquely designed MAE training scheme. This approach not only overcomes the limitations seen in previous dynamic-FC models but also consistently outperforms state-of-the-art methods. The significance of these new components is evident in our ablation study, where their removal results in a substantial performance decrease. \n\nBecause of these specifically designed features, our model holds promise for use in neuroscience and medical research. It's particularly suited for unraveling how transient fMRI fluctuations contribute to overall brain representations in various circumstances, aiding in the study of brain traits, states, and diseases.\n\n\n> Q2: No theoretical guarantees of the proposed method. The BrainMAE seems to be too heuristic.\n\nWe acknowledge that, at this stage, our approach is more heuristic and grounded in empirical insights from neuroscience and machine learning. The absence of formal theoretical guarantees is a valid point, and we view it as an important area for our future research. Our current focus has been on leveraging intuitive understandings of brain functionality and the nature of fMRI signals to develop a model that shows promising empirical results. We aim to complement these findings with more theoretical analysis in our subsequent work, and we believe this will strengthen the method's scientific contribution."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8443/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700288177424,
                "cdate": 1700288177424,
                "tmdate": 1700288177424,
                "mdate": 1700288177424,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Lwml79iVI7",
                "forum": "ajG8vLTHh5",
                "replyto": "lYd2zMN2Go",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8443/Reviewer_3Zps"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8443/Reviewer_3Zps"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the revised manuscript and the additional experiments. These empirical results have addressed some of my concerns and enhance the overall quality of this work. I have raised my score accordingly."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8443/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700613716343,
                "cdate": 1700613716343,
                "tmdate": 1700613716343,
                "mdate": 1700613716343,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pb74FvzFFV",
            "forum": "ajG8vLTHh5",
            "replyto": "ajG8vLTHh5",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8443/Reviewer_8czh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8443/Reviewer_8czh"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors developed Brain Masked Auto-Encoder (BrainMAE) for representation learning on brain fMRI data. The authors combined two newly-defined graph attention modules with MAE to obtain the latent representations for downstream analysis. The method is evaluated on various datasets to demonstrate the robustness and functional relevance of the obtained representations, and compared with several existing baselines to demonstrate superior performances in downstream tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed method is original and serves as one of the first attempts to use MAE in the field of brain imaging representation learning. Quality of the evaluations is good and convincing: the learned representations show good consistency between different datasets and good functional relevance."
                },
                "weaknesses": {
                    "value": "The writing is generally good, but the flow of the paper can be improved as certain parts can be hard to follow. For example, the notations in Section 2 is not listed clear enough: \"node features\" are described in equation (3) before the actual input fMRI segment X is formally introduced in Section 2.2. Some of the statements may not be very well supported, mostly regarding the dynamic graph attention. I have listed my questions below."
                },
                "questions": {
                    "value": "1. What exactly is shown in Figure 3A? Is this a representative t-SNE plot for one subject? Or did the authors take the mean ROI embeddings across all subjects and plotted t-SNE afterwards? Or something else?\n2. From the evaluations in all tables, it seems that SG-BrainMAE consistently outperforms DG-BrainMAE. If that is the case, I can't see the reasoning of introducing dynamic graph attention modules. I hope the authors could expand a bit on that. Also, why is it called \"dynamic\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8443/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8443/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8443/Reviewer_8czh"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8443/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699468205853,
            "cdate": 1699468205853,
            "tmdate": 1699637052817,
            "mdate": 1699637052817,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "i5hnHqSmf9",
                "forum": "ajG8vLTHh5",
                "replyto": "pb74FvzFFV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8443/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8443/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8czh"
                    },
                    "comment": {
                        "value": "Thank you very much for your acknowledgment of our work\u2019s quality and your valuable suggestions, your questions provided us with a chance to enhance our paper and improve the flow of our writing. We sincerely appreciate your time in reading the paper, and our point-to-point responses to your comments are given below.\n> Q1: Node features are described before formally defined.\n\nThank you for pointing this out. In our revised submission, for the definition of the node features, we added sentence as follows: \"Within this graph, each ROI is considered a node, with its node feature represented by the ROI's time-series signal of length $\\tau$ or $x\\in R^{\\tau}$\". \n> Q2: Some of the statements may not be very well supported, mostly regarding dynamic graph attention.\n\nSorry for the confusion and thank you for sharing your concern. We recognize the potential misunderstanding about 'dynamic graph attention,' which is simply the self-attention used in transformer models and not a major novelty of our work. To improve readability, we've omitted this description in our revised submission. Additionally, we've extensively revised Section 2.1 to more clearly convey our motivation, the novelty, and the inductive biases considered in our model's development. Feel free to suggest more comments about them and we would be happy to address them further.\n> Q3: How to get t-SNE plot in Figure 3A? Is this for one subject? Or the t-SNE of mean ROI embedding across all subjects?\n\nApologies for the confusion. The ROI embeddings are not derived from a single subject or an average across subjects. They are, in fact, model weights learned globally using data from all subjects in the dataset, akin to word embeddings in NLP. These pretrained ROI embeddings represent the rich ROI-related information \"summarized\" from all the training samples. \n> Q4: SG-BrainMAE consistently outperforms DG-BrainMAE.\n\nIndeed SG-BrainMAE consistently outperforms DG-BrainMAE. However, In $\\textbf{Appendix F}$ of our revised submission, we newly add a downstream task using the HCP task dataset. Here, models classify between 20 mental states, each lasting tens of seconds. In this context, both SG-BrainMAE and DG-BrainMAE outperform current state-of-the-art models[1], with DG-BrainMAE showing a slightly better performance relative to SG-BrainMAE. This suggests that incorporating dynamic components can be beneficial for detecting transient state changes in fMRI data over short timescales.\n\nIn tasks requiring inference of 'static' measurements (such as behaviors, gender, or overall brain states) from longer-duration fMRI data, SG-BrainMAE's consistent superiority might imply that dynamic components in attention are less critical in these scenarios.\n> Q5: What is the reasoning for introducing dynamic graph attention modules? Also, why is it called \"dynamic\"?\n\nThanks for bringing up this question. Dynamic graph attention essentially is self-attention, where the attention is directly derived from the input node feature without being constrained by the ROI embeddings. Therefore, the attention is \"dynamic\" and conditioned on the temporal fluctuation of ROI signals, thus representing the brain dynamic reconfiguration of ROIs connectivity. DG-BrainMAE, introduced as a complement to SG-BrainMAE, focuses on this dynamic aspect, unlike SG-BrainMAE, which assumes a static functional brain organization. \n\nOur experiments show that SG-BrainMAE is more effective in predicting static measurements like behaviors and gender. Conversely, DG-BrainMAE excels in identifying transient mental state fluctuations ($\\textbf{Appendix F}$ of our revised submission). These results provides support for our desgin principles.\n\n### References\n\n[1] A. Thomas, C. R \u0301e, and R. Poldrack. Self-supervised learning of brain dynamics from broad neuroimaging data. Advances in Neural Information Processing Systems, 35:21255\u201321269, 2022"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8443/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700287673046,
                "cdate": 1700287673046,
                "tmdate": 1700287673046,
                "mdate": 1700287673046,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]