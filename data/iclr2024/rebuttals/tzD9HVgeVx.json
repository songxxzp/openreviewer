[
    {
        "title": "AgentMixer: Multi-Agent Correlated Policy Factorization"
    },
    {
        "review": {
            "id": "a1pvZ88bTd",
            "forum": "tzD9HVgeVx",
            "replyto": "tzD9HVgeVx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7899/Reviewer_tDVs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7899/Reviewer_tDVs"
            ],
            "content": {
                "summary": {
                    "value": "The goal of this paper is to learn agent policies that satisfy a correlated equilibrium, which can be executed in a fully decentralized manner, under partial observability for each agent. The authors propose the AgentMixer method. The main two aspect of the method are (1) the Policy Modifier, which takes the fully decentralized policies and state information, producing a correlated joint policy from the decentralized policies, and (2) a method to extract decentralized policies from the centralized policy, while ensuring the Individual Global Consistency (IGC) condition. The AgentMixer method follows the CTDE framework, assuming that a centralized controller can both observe the full state during training, and send joint actions to the environment."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I enjoyed reading this paper, which focuses on how we might represent an optimal joint policy in the form of a product policy (where the latter can be executed using decentralized execution. The motivation and proposed method, based on posterior inference (?) is also interesting. The paper provided a good view of the literature + state-of-the-art for value factorization work in MARL, and the selected experimental settings were challenging."
                },
                "weaknesses": {
                    "value": "While the idea is interesting, the paper is weak in many aspects. The points below are listed in order of most to least important for the authors to fix.\n\n- Incompletely specified method: \n\t1) Preserving the IGC is clearly an important part of the method. While the paper clearly defines IGC (Def 6) and discusses why it is important to preserve that condition, it doesn\u2019t explain how the proposed method preserves IGC. The only provided explanation is Equation 13, which presents a centralized reward-maximization objective for MARL, subject to  the constraint that IGC is preserved. This is clearly not a sufficient explanation, and the objective isn't a novel insight. Arguably many value-factorization methods maximize this objective (e.g. VDN, QMIX, QTRAN, etc.) -- the crux is how IGC is maintained. \n\n- Incomplete results + limited ablation studies: \n\t1) Figure 3, the curve for AengMixer is cut off. It is important to show the full curve, to let the reader verify the asymptotic behavior of the method. \n\t2) Overall, it seems there should be other evaluations. For example, it seems natural to have an ablation to study the degree of partial observability that the IGC can handle. Also, the authors should attempt using another method to generate the fully observable joint policy. What about directly running a single-agent RL algorithm on the joint action space (rather than the proposed AgentMixer architecture)?\n- Weak improvement overall on both MAMujoco, and StarCraft\n- Some missing references to prior work. For example, Greenwald et al.'s classic paper on [Correlated Q-learning](https://dl.acm.org/doi/10.5555/3041838.3041869)  should be added and discussed. The authors should also add a discussion of related work on handling partial observability in multi-agent systems. \n\n- Notation  and clarity should be improved: \n\t1) It's confusing that there's no difference in notation or wording between Definition 1 and 2. Further, it seems worth defining correlated equilibria, and commenting on the distinction from a coarse correlated equilibria in more detail.\n\t2) The explanation of the policy mixer in Section 3.1 is very confusing. For example, what is W in equation 6? What is \"f^I\" in the paragraph after eq 6? How are the individual policies combined to form a joint policy? \n\t3) The key figure, Figure 2 should be much earlier in the paper. \n\t4) Typos: see \"AgnetMixer\" under Figure 2; \"AengMixer\" in Figures 3-5\n\t5) The placement of the related works section is very odd; usually it is the last section in the paper before the conclusion, or the second section in the paper (after introduction)\n\t6) Definition 4 involves a whole posterior inference procedure which is not described enough. Is this procedure purely for illustrative purposes? Or is it actually part of the method proposed by the paper? If the latter, the authors need to add a lot more explanation. The authors should add at least a paragraph of explanation for this, preferably also including an Algorithm detailing this procedure and a figure in the Appendix. For example, can you elaborate further on what pi_psi is and where does it come from? Also, the text states that the procedure given by Eq. (8) converges to a fixed point. Can the authors add references or proof?"
                },
                "questions": {
                    "value": "Please see the Weaknesses section for the most important questions. Some additional questions are below: \n- Why does AgentMixer show the most significant performance improvements in Ant and not the other tasks? \n- Does AgentMixer consistently solve relative overgeneralization problems that multi-agent policy gradient methods frequently suffers from? Perhaps this evaluation on RO domains might highlight the benefits of this method. The authors could try running experiments on the predator-prey w/punishment domain specified in [this paper](https://www.ifaamas.org/Proceedings/aamas2021/pdfs/p764.pdf)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7899/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7899/Reviewer_tDVs",
                        "ICLR.cc/2024/Conference/Submission7899/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7899/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698092114774,
            "cdate": 1698092114774,
            "tmdate": 1700607315078,
            "mdate": 1700607315078,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vJHcuBnsWe",
                "forum": "tzD9HVgeVx",
                "replyto": "a1pvZ88bTd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7899/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7899/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Response to Reviewer tDVs:\n\nWe thank you for your kind comments and apologize for the unclear statements in our paper. We believe our careful response together with additional experiments could help mitigate the concerns.\n\n### W1- How IGC is maintained:\n**Answer:**\nIn general, we preserve IGC by explicitly disentangling exploration and mode. Please refer to Common Response CR1.\n\n### W2- Incomplete results + limited ablation studies:\n**Answer:**\nThe curve of AengMixer is obscured by the curve of MAT. We changed the color to make Figure 3 clearer. We conducted ablation study demonstrating the effect of full state information and comparing a single-agent RL algorithm on the joint action space (AIL). Please refer to Common Response CR3.\n\n### W3- Weak improvement:\n**Answer:**\nThe main focus of this work is addressing asymmetric information failure. The performance of AgentMixer is highly related to how severe the partial observation of the task is. Please also refer to Common Response CR2&3.\n\n### W4- Missing references:\n**Answer:**\nWe cited the suggested reference. To handle partial observability, Centralized Training with Decentralized Execution (CTDE) is proposed as a popular learning framework for MARL. In existing CTDE methods, including value decomposition and policy gradient methods, decentralized agent policies are trained by a centralized (action-)value function with additional global information, while agents make decisions only based on their own local observation. However, the centralized training in CTDE is not centralized enough as agent policies are assumed to be independent of each other. Besides, the CTDE framework only introduces global information in the value function while policies are not granted direct access to global information even when centralized training. As far as we know, AgentMixer is the first work introducing both global information and dependencies into policy gradient.\n\n### W5- Improve notation:\nWe apologize for the unclear notations and typos.\n**Answer:**\n> difference in notation or wording between Definition 1 and 2\n> \nThe difference between Definition 1 and 2 is that 1 is for product policy while 2 is for joint policy. \n\n> confusing notation\n> \n$w_{agent}$ and $w_{channel}$ denote different fully connected layers used in agent- and channel-mixer. $f$ denotes a strategy modification defined in Section 3.2. \n\n> location of Figure 2\n\nWe modified Figure 2 and its location. \n\n> Typos\n> \nWe fixed the typos.\n\n> placement of the related works\n\nWe rearranged the placement of the related works.\n\n> Confusion about Definition 4\n\nWe added a discussion of this posterior inference procedure in Section 4.2. Implicit product policy is defined as a posterior inference procedure, marginalizing the conditional occupancy $\\rho^{\\pi_{\\psi }}(s|b)$. Since the observations/belief may not contain information to distinguish two different latent states, the $\\rho^{\\pi_{\\psi }}(s|b)$ is a stochastic distribution, and the implicit product policy is the average of the fully observable policy. \n\n### W1- Why AgentMixer performs best in Ant?\n**Answer:**\nThe reason why AgentMixer achieves superior performance on Ant-v2 where partial observability poses a critical challenge is that AgentMixer keeps consistency between joint fully observable policy and individual partially observable policies. To verify this, we presented ablation studies. Please also refer to Common Response 2&3.\n\n### W2- Additional experiment on Predator-Prey:\n**Answer:**\nWe conducted experiments on the predator-prey w/punishment domain in Appendix. Figure 12 shows that AgentMixer can effectively alleviate the relative overgeneralization problem."
                    },
                    "title": {
                        "value": "Response to Reviewer tDVs"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700247792894,
                "cdate": 1700247792894,
                "tmdate": 1700331460726,
                "mdate": 1700331460726,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vXqR1cKnpy",
                "forum": "tzD9HVgeVx",
                "replyto": "OWSyVQRYlC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7899/Reviewer_tDVs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7899/Reviewer_tDVs"
                ],
                "content": {
                    "title": {
                        "value": "Updated score"
                    },
                    "comment": {
                        "value": "Thanks to the authors for their revisions. The newly added sections on the implementation of AgentMixer in the discrete/continuous cases improve my understanding of the method. I also appreciate the addition of AIL as a baseline. As such, I have raised my score.  \n\n\nHowever, I still don't think the paper is ready for acceptance. I'm still not sure why AgentMixer does better on Ant-v2. The authors claim it is because partial observabalitity is a bigger challenge on Ant-v2 than other domains, but what evidence do they have? \n\nSecond, it still needs a lot of writing improvement -- there are too many ambiguities, which makes reading and understanding very challenging. I found this to be the case even in the newly added sections. Finally, seeing as partial observability doesn't seem to greatly impact performance in Starcraft v2, I encourage the authors to explore alternative domains which might better showcase the benefits of their method."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700607923761,
                "cdate": 1700607923761,
                "tmdate": 1700607923761,
                "mdate": 1700607923761,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CiEoQO0u5T",
                "forum": "tzD9HVgeVx",
                "replyto": "a1pvZ88bTd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7899/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7899/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A kind reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer tDVs,\n\nWe would like to express our sincere gratitude to you for reviewing our paper and providing valuable feedback. We believe that we have responded to and addressed all your concerns with our response and revision.\n\nNotably, given that we are approaching the deadline for the rebuttal phase, we hope we can have the discussion soon. Thanks\uff01\n\nBest,\n\nAll authors"
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700723116844,
                "cdate": 1700723116844,
                "tmdate": 1700723131088,
                "mdate": 1700723131088,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5OjyLqmYAl",
            "forum": "tzD9HVgeVx",
            "replyto": "tzD9HVgeVx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7899/Reviewer_Qpfs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7899/Reviewer_Qpfs"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors target the challenge of stabilizing partially observable multi-agent reinforcement learning (MARL) by proposing AgentMixer, a novel approach that leverages centralized training with decentralized execution (CTDE) to learn correlated decentralized policies. AgentMixer consists of a Policy Modifier (PM) module that models the correlated joint policy and Individual-Global-Consistency (IGC) that maintains consistency between individual policies and joint policy while allowing correlated exploration. They theoretically prove that AgentMixer converges to \u03f5-approximate Correlated Equilibrium. Furthermore, they evaluate AgentMixer on three MARL benchmarks, demonstrating its effectiveness in handling partial observability and achieving strong experimental performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Originality: The paper introduces AgentMixer, a novel MARL approach that combines Policy Modifier and Individual-Global-Consistency to address partial observability challenges.\n- Quality: The theoretical analysis provided includes convergence to an \u03f5-approximate Correlated Equilibrium, which showcases the robustness of the proposed approach.\n- Significance: Experimental performance shows that AgentMixer outperforms existing state-of-the-art MARL methods on three benchmarks, confirming the method's effectiveness and applicability to real-world problems."
                },
                "weaknesses": {
                    "value": "1. Some claims about existing works may be inappropriate;\n2. The method part is not clear written;\n3. The experiments can be improved with more critical baselines and better aligning the motivation."
                },
                "questions": {
                    "value": "1.  \"While this is attractive, the pre-defined dependence among agents in auto-regressive methods may limit the representation expressiveness of their joint policy classes. \" Why auto-regressive limit the expressiveness? [1]'s eq (5 ) and [2]'s Theorem A.1 provide opposition for the claim. \n2. The claim about \" However, note that both auto-regressive methods and existing correlated policies violate the requirement for decentralized execution. \"  is questionable. As auto-regreesive[1]  and correlated[3] are decentralized execution. \n3. In the method part, eq6, what is the superscript of W_agent mean? why we specially need channel mixing? can you provide some intuitions?\n4. For definition 5, why should we care about identifiability? as in 3, we only need the divergence between local one and global optimal as closer as it can be. Does the ``closer'' surely be the mode consistent?\n5. For Figure 2, you not show show IGC. Also hwo your IGC been used? as a constraint on optimization of 13? Then can you concretely write it and derive the grad?\n6. [1] is very relevant and I do think it should be involved in your baseline.\n\n\n- Minors\n7. The [1], [2], and [3] should be cited. \n8. The IGC should be clearly showed in both loss function and your main figure. \n9. The SMAC-v2 seems still not a good testbed for your motivation or method, I suggest you put it to appendix. \n\n---\n[1] Wang, Jiangxing, Deheng Ye, and Zongqing Lu. \"More Centralized Training, Still Decentralized Execution: Multi-Agent Conditional Policy Factorization.\" ICLR 2022.\n\n[2] Sheng, Junjie, et al. \"Negotiated Reasoning: On Provably Addressing Relative Over-Generalization.\" arXiv preprint arXiv:2306.05353 (2023).\n\n[3] Wen, Ying, et al. \"Probabilistic recursive reasoning for multi-agent reinforcement learning.\" ICLR 2019."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7899/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7899/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7899/Reviewer_Qpfs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7899/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698762839471,
            "cdate": 1698762839471,
            "tmdate": 1700708824318,
            "mdate": 1700708824318,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "R8o6WBj39u",
                "forum": "tzD9HVgeVx",
                "replyto": "5OjyLqmYAl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7899/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7899/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Response to Reviewer Qpfs:\n\nWe appreciate your constructive comments and apologize for the unclear statements in our paper. We believe our careful response and additional experiments could help mitigate your concerns.\n\n### W1 and Q1,2- Inappropriate claims on auto-regressive methods:\n**Answer:**\nWe thank you for this constructive suggestion. We have modified the corresponding texts and cited the suggested reference. However, note that existing auto-regressive methods assume a pre-defined execution order, while our method doesn't assume any execution order.\n\n### W2 and Q5- How IGC is used:\n**Answer:**\nWe apologize for the unclear explanation. In general, we preserve IGC by explicitly disentangling exploration and mode. We believe Common Response CR1 has clarified this question.\n\n### Q3- Notation of $w_{agent}$ and why channel mixing:\n**Answer:**\n$w_{agent}$ and $w_{channel}$ denote different fully connected layers used in agent- and channel-mixer. The agent-mixer allows 'communication' between agents' dims, which operates on each dim/channel independently (each agent has $m$ feature dims). The channel-mixer operates on each agent independently allowing 'communication' between different channels. These two types of mixers are interleaved to enable interaction of both input dimensions (agents and dims).\n\n### Q4- Why care about identifiability?\n>For definition 5, why should we care about identifiability? as in 3, we only need the divergence between local one and global optimal as closer as it can be. Does the ``closer'' surely be the mode consistent?\n\n**Answer:**\nIdentifiability is the crucial insight to guarantee that the partially observed policy recovered through imitation learning can exactly reproduce the actions of the fully observing policy. According to the definition of identifiability, identifiability is measured by the divergence between joint policy and individual policies. However, directly adding divergence constraint to learning of individual policies may lead to unexpected results. To verify this, we compared with asymmetric imitation learning (AIL) where individual policies directly imitate from a centralized PPO with full state information. Please also refer to Common Response CR3.\n\n### Q6- Additional baseline of MACPF:\n**Answer:**\nWe added MACPF as one of the baselines."
                    },
                    "title": {
                        "value": "Response to Reviewer Qpfs"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700247775566,
                "cdate": 1700247775566,
                "tmdate": 1700331431954,
                "mdate": 1700331431954,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kHQyuoMgXS",
                "forum": "tzD9HVgeVx",
                "replyto": "5ToRKUucyj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7899/Reviewer_Qpfs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7899/Reviewer_Qpfs"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your thorough and detailed response"
                    },
                    "comment": {
                        "value": "Thank you for your thorough and detailed response addressing most of my concerns and questions. I appreciate the effort that has gone into conducting additional experiments during the rebuttal period, which is no easy task, and I recognize the hard work that you have put forth. \n\nHowever, I still believe that the most critical concern has not been adequately addressed. The major motivation of this paper is the insufficient representational capacity of autoregressive methods. As I mentioned in my previous communication, some existing approaches have presented contrary conclusions. Although you have clarified in the revised version that autoregressive methods require a fixed execution order for agent actions, a deeper analysis comparing the representational capacity of autoregressive methods with AgentMixer is lacking.\n\nSimilarly, while the experimental section now includes MACPF as one of the baselines and demonstrates superior performance by AgentMixer, I feel that a mere numerical comparison is insufficient.\n\nIn summary, considering the current version's inadequacy in discussing the representational capacity of autoregressive methods and its comparison to AgentMixer, I believe it has not yet reached the standard for publication. Therefore, I maintain my original score for the paper."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631370267,
                "cdate": 1700631370267,
                "tmdate": 1700631370267,
                "mdate": 1700631370267,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yVtKNmffTd",
                "forum": "tzD9HVgeVx",
                "replyto": "5OjyLqmYAl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7899/Reviewer_Qpfs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7899/Reviewer_Qpfs"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "I would like to thank you for your response. I understand that the core of this paper is emphasizing the importance of asymmetric learning failure and proposing AgentMixer to address this issue. The point I wanted to emphasize in my previous response was that certain autoregressive methods (such as those mentioned in Question 2 [1,3]) **can also achieve decentralized execution**; however, the paper does not explicitly analyze why these methods **cannot** solve the asymmetric learning failure problem.\n\nFurthermore, regarding the proof-of-concept task used in section 5.1 of the paper, its purpose should be to answer the aforementioned question from an experimental perspective, i.e., why AgentMixer can solve the asymmetric learning failure problem, but the baselines cannot. However, I cannot intuitively establish a connection between this task and the asymmetric learning failure problem, and the paper introduces the new issue of relative overgeneralization. The paper does not provide a clear explanation of the relationship between this issue and the asymmetric learning failure problem.\n\nAdditionally, the authors mentioned an interesting question during their discussion with Reviewer 9QDT, namely the **relationship between IGC and IGM**. The authors suggested that, **informally**, IGC is similar to an IGM for policies. Given the relationship between optimal policies and optimal Q value functions from the probabilistic graphical model perspective [^], can IGC be easily extended from IGM based on this relationship? I believe a discussion of this aspect could more clearly position this paper.\n\nIn summary, I believe that the asymmetric learning failure problem addressed in this paper, as well as the proposed IGC concept and AgentMixer algorithm, contribute to the MARL community. However, in the current version, the paper's content does not present a logical flow throughout, and the experiments (particularly the proof-of-concept task) do not fully corroborate the series of conclusions presented in the paper. If the authors can better address these issues, I will raise my score to 6.\n\n[^] Levine, Sergey. \"Reinforcement learning and control as probabilistic inference: Tutorial and review.\" arXiv preprint arXiv:1805.00909 (2018)."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674203509,
                "cdate": 1700674203509,
                "tmdate": 1700674321765,
                "mdate": 1700674321765,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qirVXPWwea",
                "forum": "tzD9HVgeVx",
                "replyto": "BJ1OjFmR23",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7899/Reviewer_Qpfs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7899/Reviewer_Qpfs"
                ],
                "content": {
                    "title": {
                        "value": "Update the score"
                    },
                    "comment": {
                        "value": "Thank you for the prompt response. Your discussion above has helped address my main concerns and provided me with a deeper understanding of the contributions and positioning of this work in the context of MARL. As a result, I have raised my score to a 5. Nevertheless, I still believe that additional revision round is necessary to address these issues and ensure that the paper meets publication standards."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708784372,
                "cdate": 1700708784372,
                "tmdate": 1700708784372,
                "mdate": 1700708784372,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vWllyVpO1V",
            "forum": "tzD9HVgeVx",
            "replyto": "tzD9HVgeVx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7899/Reviewer_9QDT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7899/Reviewer_9QDT"
            ],
            "content": {
                "summary": {
                    "value": "This paper discusses the issue that agents make decisions based on their local observation independently, which could hardly lead to a correlated joint policy with sufficient coordination.\n\nTwo key ideas are introduced, i.e., Policy Modifier and Individual-Global-Consistency. Policy Modifier takes the individual partially observable policies and state as inputs and produces correlated joint fully observable policy as outputs. Individual-Global-Consistency keeps the mode consistency among the joint policy and individual policies.\n\nOverall, the presentation is clear, while the authors have clearly expressed their results. The authors have clearly explained the MARL problem they wanted to solve and how to solvem. However, the necessity and importance of work have not been clearly expressed, and there are still doubts which are explained in details as follows."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The core of multi-agent reinforcement learning is strategy alignment and we need to align the local policies obtained from local observation information with the joint policy. This paper aims to solve this issue by polishing the obtained local policies following a well-defined Individual-Global-Consistency condition."
                },
                "weaknesses": {
                    "value": "The given Individual-Global-Consistency condition seems to be too high-level, which is similar with Individual-Global-Max condition for value decomposition MARL methods. The authors directly consider this condition as the constraint in (13). The most important issue of the proposed algorithm is that we need to check whether this constraint can be satisfied by following the proposed algorithm. If not, can we measure this distance? Both theoretically and experimentally? This has impaired the contribution of this work."
                },
                "questions": {
                    "value": "1. Clarify the connection between IGC and IGM, while compare the corresponding algorithms would be better.\n2. Indeed, there are limitations to the length of the paper, and it is still recommended that the author provide a clear algorithm pesudo code.\n3. The authors discussed the performance of the algorithm during the experimental phase, but did not validate the key technologies proposed in this article, especially the issue of whether modifying local polies can lead to better alignment of joint policy.\n4. I still emphasize the necessity of emphasizing CTDE. This article should be compared with value decomposition or strategy decomposition methods, rather than emphasizing CTDE as a computational framework."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "~"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7899/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698898743352,
            "cdate": 1698898743352,
            "tmdate": 1699636969052,
            "mdate": 1699636969052,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "n7Ls2A8mpQ",
                "forum": "tzD9HVgeVx",
                "replyto": "vWllyVpO1V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7899/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7899/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Response to Reviewer 9QDT:\nWe thank you for your kind comments and apologize for the unclear statements in our paper. We believe our careful response together with additional experiments could help mitigate the concerns.\n\n### W1- How IGC is satisfied:\n\n> The most important issue of the proposed algorithm is that we need to check whether this constraint can be satisfied by following the proposed algorithm. If not, can we measure this distance?\n\n**Answer:**\nIn general, we preserve IGC by explicitly disentangling exploration and mode. Please also refer to Common Response CR1 where we provide a detailed explanation.\n\n### Q1- Connection between IGC and IGM:\n> Clarify the connection between IGC and IGM, while compare the corresponding algorithms would be better.\n> \n**Answer:**\nThanks for your constructive insights! We find that both IGM and IGC aim to maintain the optimal action consistent between joint policy and individual policies. While IGM is imposed on joint action-value function (Q value) and individual Q value, IGC **explicitly maintains mode consistency** between joint policy and individual policies. We believe such a connection may benefit the MARL community more. \n\nOne of the compared baselines, MAVEN, based on QMIX requires IGM. As far as we know, AgentMixer is the first work that imposes IGC on policies.\n\n### Q2- Pseudo code:\n**Answer:**\nThanks for your constructive suggestion. We added a pseudo code in Appendix.\n\n### Q3- Validate the key technologies:\n> The authors discussed the performance of the algorithm during the experimental phase, but did not validate the key technologies proposed in this article, especially the issue of whether modifying local polies can lead to better alignment of joint policy.\n\n**Answer:**\nTo validate AgentMixer, we perform ablation experiments by adding two baselines, MAPPO_FULL conditioned on global information in both training and testing; asymmetric imitation learning (AIL) which uses fully observable PPO to supervise learning decentralized partially observable policies. Results demonstrate that AgentMixer mitigates asymmetric information failure by guaranteeing consistency between joint policy and individual policies. Please also refer to Common Response CR1,2,3.\n\n### Q4- Compare with value decomposition or strategy decomposition methods:\n> I still emphasize the necessity of emphasizing CTDE. This article should be compared with value decomposition or strategy decomposition methods, rather than emphasizing CTDE as a computational framework.\n> \n**Answer:**\nThe baselines compared are all based on CTDE, except for MAT and MAT-dec which are CTCE (centralized training with centralized execution). MAVEN and MACPF are value decomposition and policy decomposition methods respectively."
                    },
                    "title": {
                        "value": "Response to Reviewer 9QDT"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700247743041,
                "cdate": 1700247743041,
                "tmdate": 1700331082219,
                "mdate": 1700331082219,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Lsx7FQ93xz",
                "forum": "tzD9HVgeVx",
                "replyto": "vWllyVpO1V",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7899/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7899/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A kind reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer 9QDT,\n\nWe would like to express our sincere gratitude to you for reviewing our paper and providing valuable feedback. We believe that we have responded to and addressed all your concerns with our response and revision.\n\nNotably, given that we are approaching the deadline for the rebuttal phase, we hope we can have the discussion soon. Thanks\uff01\n\nBest,\nAll authors"
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700693863952,
                "cdate": 1700693863952,
                "tmdate": 1700693863952,
                "mdate": 1700693863952,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rULAgu44Y1",
            "forum": "tzD9HVgeVx",
            "replyto": "tzD9HVgeVx",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7899/Reviewer_yFNn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7899/Reviewer_yFNn"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript aims to the asymmetric learning failure problem in Centralized training with decentralized execution (CTDE). To fully take advantage of CTDE to learn correlated decentralized policies, the authors propose the AgentMixer algorithm. It has two key module. The first one is Policy Modifier (PM), which explicitly models the correlated joint policy via composing the partially observable individual policies conditioned on global state information. The second one is Individual-Global-Consistency (IGC), which maintains the mode consistent between the state-based joint policy and partially observable decentralized policies. It is theoretically proofed that AgentMixer converges to \u03b5-approximate Correlated Equilibrium, and the experimental results show that it can achieve comparable performance to existing methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper present a novel solution to factorize the joint policy in MARL, i.e. policy mixing and distilling. It also introduce a \u03b5-approximate Correlated Equilibrium perspective to measure the consistence and propose the Individual-Global-Consistency (IGC) to guarantee."
                },
                "weaknesses": {
                    "value": "Although it's theoretically proofed that AgentMixer converges to \u03b5-approximate Correlated Equilibrium, the results in the experiments show that the performance of AgentMixer is even or not better than compared methods in many settings. \nThe convergence of AgentMixer is also proofed via the mode consistency. But, The IGC is defined based on the mode of the policy distribution, and the PM is defined via MLP mixer (agent- and channel). It may have some gap here.\nBesides, some contents seem inconsistent in the presentation, figures in the manuscript."
                },
                "questions": {
                    "value": "1. Is the Figure 2 a final version? There are no Individual-Global-Consistency components, the gradient forward and backward procedure is not mentioned in the content, and where is the policy distilling?\n2. The performance of AgentMixer is shown even or not better than compared methods in many experiments. The authors should give more analysis.\n3. It may need to be more clear, that how the policy modifier (mixing) can keep the mode consistency.\n4. Some details are not given clearly in the manuscripts, ie. \n    How does the embedding is achieved with $\\pi_{\\theta_1}$ and $s$? \n    what is the meaning of $b$ in the product policy $\\pi_{\\phi}(a|b)$?\n5. The information of this paper cited is incomplete.\nJianing Ye, Chenghao Li, Jianhao Wang, and Chongjie Zhang. Towards global optimality in cooperative marl with the transformation and distillation framework, 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7899/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7899/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7899/Reviewer_yFNn"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7899/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699257866483,
            "cdate": 1699257866483,
            "tmdate": 1699636968956,
            "mdate": 1699636968956,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "z7YvXkCnaZ",
                "forum": "tzD9HVgeVx",
                "replyto": "rULAgu44Y1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7899/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7899/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Response to Reviewer yFNn:\nWe thank you for your constructive comments. We have done additional experiments and explaination and hope it can address your concerns.\n\n### W1, Q2- Flat performance:\n> the results in the experiments show that the performance of AgentMixer is even or not better than compared methods in many settings\n\n**Answer:**  \nThe main focus of this work is addressing asymmetric information failure. The performance of AgentMixer is highly related to how severe the partial observation of the task is. To justify this, we perform ablation experiments by adding two baselines, MAPPO_FULL conditioned on global information in both training and testing; asymmetric imitation learning (AIL) which uses fully observable PPO to supervise learning decentralized partially observable policies. As shown in Figure 6, directly distilling from joint policy will suffer from asymmetric information failure while our method achieves strong performance by mitigating asymmetric information failure. Results in Figure 12 demonstrate that when global information is not critical, the compared methods perform similarly. Please also refer to Common Response CR2&3.\n\n### W2- Gap between IGC and PM:\n> The IGC is defined based on the mode of the policy distribution, and the PM is defined via MLP mixer (agent- and channel). It may have some gap here.\n\n**Answer:**\nPM takes as input the individual partially observable policies and state and outputs correlated joint fully observable policy, where the agent- and channel-mixer aim to introduce dependency among agents. IGC enables the actions that occur most frequently in the joint policy and the individual policies to be equivalent, which keeps consistency between the joint policy and individual policies, thereby mitigating asymmetric information failure. **In short, PM achieves IGC by only modifying the exploration of individual policies, for example, the std. in Gaussian policy, while maintaining the mean (mode) unchanged.** Please also refer to Common Response CR1 for a detailed explanation.\n\n### Q1- Unclear Figure 2:\n> There are no Individual-Global-Consistency components, the gradient forward and backward procedure is not mentioned in the content, and where is the policy distilling?\n\n**Answer:**\nWe modified Figure 2 to clarify the relationship among components in AgentMixer.\n\n### Q3- Mode consistency:\n> It may need to be more clear, that how the policy modifier (mixing) can keep the mode consistency.\n> \n**Answer:**\nIGC aims to keep the mode of joint policy and individual policies consistent. In general, we preserve IGC by explicitly disentangling exploration and mode. Please also refer to Common Response CR1.\n\n### Q4- Notation details:\n> Some details are not given clearly in the manuscripts\n\n**Answer**\nWe added some notation for clarity. The embedding of $\\pi_{\\phi}$ and $s$ is achieved by MLP. $b$ is a belief state which is a sufficient statistic for joint history.\n\n### Q5- Suggested citation:\n> The information of this paper cited is incomplete. Jianing Ye, Chenghao Li, Jianhao Wang, and Chongjie Zhang. Towards global optimality in cooperative marl with the transformation and distillation framework, 2023.\n\n**Answer**\nWe modified this citation."
                    },
                    "title": {
                        "value": "Response to Reviewer yFNn"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700247715115,
                "cdate": 1700247715115,
                "tmdate": 1700331283639,
                "mdate": 1700331283639,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Rh6w3n7nte",
                "forum": "tzD9HVgeVx",
                "replyto": "rULAgu44Y1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7899/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7899/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A kind reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer yFNn,\n\nWe would like to express our sincere gratitude to you for reviewing our paper and providing valuable feedback. We believe that we have responded to and addressed all your concerns with our response and revision.\n\nNotably, given that we are approaching the deadline for the rebuttal phase, we hope we can have the discussion soon. Thanks\uff01\n\nBest,  \nAll authors"
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7899/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700693827575,
                "cdate": 1700693827575,
                "tmdate": 1700693827575,
                "mdate": 1700693827575,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]