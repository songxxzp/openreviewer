[
    {
        "title": "How To Train Your Covariance"
    },
    {
        "review": {
            "id": "9UZffkgqHH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7380/Reviewer_Q3zL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7380/Reviewer_Q3zL"
            ],
            "forum": "r6NMqADLGQ",
            "replyto": "r6NMqADLGQ",
            "content": {
                "summary": {
                    "value": "The paper studies a conditional covariance estimation problem, where the covariance can vary depending on the conditioned random variable $x$. The paper points out that using NLL can be problematic and propose an alternative formulation and a metric. The paper applies the proposed scheme on various datasets and compares with other baselines."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- Related papers are thoroughly reviewed.\n\n- Experiment is comprehensive and covers various datasets."
                },
                "weaknesses": {
                    "value": "- Although the paper studies a theoretical problem (conditional covariance estimation), a precise probabilistic statement of the problem is nowhere provided. I was confused starting from the first paragraph, which says ``$p(y | x)$ follows $N(y, \\Sigma_{y | x})$''. Isn't $y$ a random variable? How can a random variable be a conditional mean of itself? Not only in this paragraph, but in many other places throughout the paper, the notations $y, \\hat{y}, f(x), f_\\theta(x)$ were carelessly used, making the paper almost impossible to understand. I believe the paper can be much improved by clearly defining the problem (under which probability distribution the data is generated, which parameter is being estimated, what assumptions are used, etc.).\n\n- Most equations in the paper remain at the level of heuristic. While it is okay to have a heuristic explanation of the proposed method, the way it is presented in the current draft is unnecessarily confusing. For example, how do we even define the limit in Eq. (1)? I think the paper should try to minimize the use of non-rigorous math."
                },
                "questions": {
                    "value": "- To my knowledge, conditional mean/covariance estimation is impossible without further assumption (e.g. regularity of $\\mu_{y | x}, \\Sigma_{y | x}$). I wonder how the authors avoided this problem.\n\n- Most figures are missing axis labels. For example, Figure 1 is placed in the very beginning of paper without explaining what the curves are."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7380/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7380/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7380/Reviewer_Q3zL"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7380/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697137143668,
            "cdate": 1697137143668,
            "tmdate": 1699636883181,
            "mdate": 1699636883181,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WYdZOl8FUF",
                "forum": "r6NMqADLGQ",
                "replyto": "9UZffkgqHH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7380/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7380/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your review. We would like to clarify below:\n\n1. **To my knowledge, conditional mean/covariance estimation is impossible without further assumption.**\\\nConditional mean/covariance estimation has been studied in multiple previous works, e.g.: [1-5]. The common approach in these works is to assume that the target distribution $p(Y | X = x)$  conditioned on the input $x$ (can also be an image) follows a normal distribution $\\mathcal{N}(\\mu_{Y|X}, \\Sigma_{Y|X})$. The mean and covariance of the target are predicted using two neural networks which takes $x$ as input and are trained using the negative log-likelihood. Our work follows  this paradigm, and proposes a new way to model the covariance. Therefore, it is not impossible to assume conditional mean/covariance estimation within our problem statement.\n\n1. **Most equations in the paper remain at the level of heuristic... how do we even define the limit in Eq. (1) ...**\\\nThe limit in Eq. 1 is a standard definition for continuous distributions, and we have added references for the same. The only heuristic is in treating the limit as a random variable with zero mean, the variance of which is learnt through neural network optimization. We take a principled approach in deriving our closed-form expression for the covariance, and fail to understand the reviewer's concern.\n\n1. **Although the paper studies a theoretical problem ...** \\\nWe have addressed this point by rearranging the content of the paper. The notation table has been moved from the related work section to the introduction. We have moved the negative log-likelihood equation from the related work to the introduction, since we optimize our covariance formulation through the negative log-likelihood. Our introduction already stated our assumption that the target distribution is a multivariate normal distribution, the standard assumption in machine learning. Furthermore, we also specify the standard assumption that we do not know the data generating distribution p(X, Y). Instead, we obtain samples (x, y) from it to form our dataset. We understand that our notation may have been ambiguous, and we have revised the draft to address this.\n\n1. **Most figures are missing axis labels ...**\\\nThank you for pointing this out. We have addressed this in the **revised draft.**\n\nWe would like to conclude the response by reiterating that this work is in line with previous works which propose models and training strategies to learn the covariance in applied settings.\n\n\n[1] Andrew Stirn, Harm Wessels, Megan Schertzer, Laura Pereira, Neville Sanjana, and David Knowles.\nFaithful heteroscedastic regression with neural networks. In International Conference on Artificial\nIntelligence and Statistics, pp. 5593\u20135613. PMLR, 2023\n\n[2] Maximilian Seitzer, Arash Tavakoli, Dimitrije Antic, and Georg Martius. On the pitfalls of het-\neroscedastic uncertainty estimation with probabilistic neural networks. In International Confer-\nence on Learning Representations, 2022\n\n[3] Nitesh B Gundavarapu, Divyansh Srivastava, Rahul Mitra, Abhishek Sharma, and Arjun Jain. Struc-\ntured aleatoric uncertainty in human pose estimation. In CVPR Workshops 2019.\n\n[4] Ivor JA Simpson, Sara Vicente, and Neill DF Campbell. Learning structured gaussians to approximate\ndeep ensembles. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition 2022\n\n[5] Garoe Dorta, Sara Vicente, Lourdes Agapito, Neill DF Campbell, and Ivor Simpson. Structured\nuncertainty prediction networks. In Proceedings of the IEEE conference on computer vision and\npattern recognition 2018."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7380/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700064394176,
                "cdate": 1700064394176,
                "tmdate": 1700064394176,
                "mdate": 1700064394176,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "580Kaicn0i",
                "forum": "r6NMqADLGQ",
                "replyto": "WYdZOl8FUF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7380/Reviewer_Q3zL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7380/Reviewer_Q3zL"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. I decided to keep my score for the following reasons.\n\n**1. To my knowledge, conditional mean/covariance estimation is impossible without further assumption.**\n\nIn nonparametric statistics literature, it is standard to have a certain regularity condition (Holder or Sobolev condition) on the mean/covariance function. See e.g. https://arxiv.org/pdf/1202.5134.pdf and references therein. Without this assumption, estimating the mean/covariance function is simply impossible. To see this point, let's say we have a finite sample $(x_1, y_1), \\dots, (x_n, y_n)$ from a continuous distribution $p_{X, Y}$. Then, with probability 1, none of $x_i$ will appear more than once, so $y_i$ is the only information we know about the conditional distribution $p_{Y | X = x_i}$. Therefore, unless we assume some regularity, mean/covariance function estimation is a statistically ill-posed problem. I would suggest to (1) clarify the condition under which the mean/covariance estimation is possible (2) discuss why neural network is a good choice to model those functions.\n\n**2. Most equations in the paper remain at the level of heuristic.**\n\nEq. (2) in the revision is still a mathematically nonsense expression. Please clarify which part of the added reference I can refer to. I cannot find an immediate way to justify such an expression in standard measure-theoretic probability theory.\n\n**3. Although the paper studies a theoretical problem..**\n\nDespite the changes made, many of the notations remain unclear and confusing. The problem needs to have a clearer statistical description, and the notations $y, \\hat{y}, f(x), f_\\theta(x)$ have to be carefully distinguished."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7380/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700251581985,
                "cdate": 1700251581985,
                "tmdate": 1700251581985,
                "mdate": 1700251581985,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9jNMoJvsmH",
                "forum": "r6NMqADLGQ",
                "replyto": "9UZffkgqHH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7380/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7380/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nOur work builds upon a wide range of successful parametric variance / covariance estimation methods. This includes the highly cited: Kendall and Gal [NeurIPS \u201817], Dorta et al. [CVPR \u201818], Seitzer et al. [ICLR \u201822], Stirn et al. [AISTATS \u201823]. Many more works use parametric covariance estimation for downstream tasks, e.g.: Lu and Koniusz [CVPR \u201822], Liu et al. [ICRA \u201818], Gundavarapu et al. [CVPRW \u201820], Russel and Reale [T-NNLS \u201821], Simpson et al. [CVPR \u201822] among many others. These references are available in the draft. We follow and describe a similar (and _same_  in some cases) probabilistic framework, and maintain standard machine learning notation which is explicitly described through a table. \n\nWe therefore confess that we are perplexed by the evaluation of the draft by the reviewer.\n\n1. **In nonparametric statistics literature, it is standard ...** \\\nNeural networks with differentiable activation functions and optimized using negative log-likelihood with Tikhonov regularization are assumed to be Lipschitz and smooth [6]. These assumptions are implied and well known in deep learning research, if needed we can add them in the draft. Additionally, results from the theory of neural networks highlight that neural networks with non-constant, bounded, continuous activation functions, with continuous derivatives up to order K belong to Sobolev spaces of order K [7]. Further, the use of neural networks to model the variance / covariance has already been explored in previous work on parametric covariance estimation, hence we do not emphasize this in the draft. The biggest advantage of neural networks is that it allows us to learn complex representations from varying kinds of inputs (such as images) to predict the covariance.\n\n1. **Eq. (2) in the revision is still a mathematically nonsense expression...** \\\nLet us assume we have $N$ predictions $(x, \\hat{y})^{(1)} \\ldots (x, \\hat{y})^{(N)}$. However, the probability of exactly observing $p(X = x)$ is zero for continuous variables. Instead, the standard approach (Section 2.4 in [8]) is to observe over the set $X \\in lim\\_{\\varepsilon \\rightarrow 0} [x, x + \\varepsilon]$. This implies that for continuous variables, we do not observe a specific but a range of values around $X = x$. We can incorporate this definition in the covariance too. Since $\\hat{Y}$ is a deterministic transformation of $X$ through a parametric network, $\\hat{Y} = f\\_{\\theta}(X)$, we have: $Cov(\\hat{Y} | X = x) = Cov( f\\_{\\theta}(X) | X \\in lim\\_{\\varepsilon \\rightarrow 0} [x, x + \\epsilon])$. \\\nFinally, the only heuristic in our derivation is in treating  this neighborhood $\\lim\\_{\\varepsilon \\rightarrow 0} [x, x + \\varepsilon]$ as $x + \\epsilon$, where $\\epsilon$ is an m-dimensional random variable with a zero-mean isotropic Gaussian distribution $p(\\epsilon) = \\mathcal{N}(0, \\sigma^2\\_{\\epsilon}(x) I_m)$. Introducing $\\epsilon$ imposes a distribution on the neighborhood which is centred at $x$. This also allows us to represent $\\hat{y} = f\\_{\\theta}(x + \\epsilon)$ stochastically. While the variance of this random variable is unknown (we later show that it can be learnt), we assume heteroscedasticity which allows us to represent neighborhoods of varying spatial extents for each $x$.\nWe elaborate on expression 2 in subsection 3.1 of the revised draft. \\\nWe take a principled approach in deriving our closed-form expression for the covariance, which we believe is not reflected in the current review.\n\n1. **Despite the changes made, many of the notations remain unclear and confusing ...** \\\nWe already follow standard machine learning conventions for $y, \\hat{y}, f\\_{\\theta}$ and described them in Table 1. Further, our introduction also stated that $p(Y | X)$ is normally distributed, and parameterized by two networks $f\\_{\\theta}$ and $g\\_{\\Theta}$, which are trained through the negative log-likelihood. We emphasized that our work assumes the same probabilistic setting as prior literature on parametric covariance estimation, and the only change we propose is in the way we model the covariance through a novel closed form expression. Without specific examples we are not able to understand what in the notation is confusing.\n\nNevertheless, we thank you for your review.\n\n[6] Du, S., Lee, J., Li, H., Wang, L. and Zhai, X., 2019, May. Gradient descent finds global minima of deep neural networks. In International conference on machine learning (pp. 1675-1685). PMLR.\n\n[7] Czarnecki, W.M., Osindero, S., Jaderberg, M., Swirszcz, G. and Pascanu, R., 2017. Sobolev training for neural networks. Advances in neural information processing systems, 30.\n\n[8] Evans, Michael J., and Jeffrey S. Rosenthal. Probability and statistics: The science of uncertainty. Macmillan, 2004."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7380/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700551504735,
                "cdate": 1700551504735,
                "tmdate": 1700558115210,
                "mdate": 1700558115210,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "18jX8AucFR",
            "forum": "r6NMqADLGQ",
            "replyto": "r6NMqADLGQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7380/Reviewer_kqRb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7380/Reviewer_kqRb"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles the problem of unsupervised covariance estimation when the covariance is not homogeneous across samples. Current solution use neural networks with negative log-likelihood objectives. They show that the obtained solutions for the covariance do not take into account the randomness in the mean estimation. To tackle that, they propose a solution that capture the randomness in the mean by incorporating local curvature around the samples. Furthermore, they propose an evaluation metric Conditional Mean Absolute Error (C-MAE) to quantify the covariance estimation in the absence of annotations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper tackles an important practical problem for statistical machine learning. The proposed spatial variance motivated by taking into curvature around samples is a nice approach to account for uncertainty in mean and covariance estimation. The new proposed metric C-MAE would also be useful in other applications involving statistics estimations in unsupervised settings as an alternative to log-likelihood."
                },
                "weaknesses": {
                    "value": "The computational complexity of the approach could be an issue as the paper tackles a practical estimation problem. The paper does not compare to other approaches to log-likelihood such as Lotfi, S., Izmailov, P., Benton, G., Goldblum, M., & Wilson, A. G. (2022, June). Bayesian model selection, the marginal likelihood, and generalization. In International Conference on Machine Learning (pp. 14223-14247). PMLR."
                },
                "questions": {
                    "value": "- In Section 2.1, \\sigma_{\\Theta} has not been defined. Is it a scalar that is assuming a diagonal covariance matrix?, How do you go from Cov(\\hat{y}) to \\sigma_{\\Theta} ?\n- The sentence after Equation (6) is incomplete: \"We note that both both Cov() and Cov()...\"\n- What is the theoretical explanation motivating the use of the thirs matrix term k_3(x). It is said in the paper, that the curvature of the function at x cannot alone explain the stochasticity of the samples which motivate the use of k_3(x). It would then be appropriate to motivate the definition of \"spatial covariance\" by the the \"curvature of x\" and of \"?\" coming from k_3(x). Could the authors please elaborate more on this?\n-The following paper proposes \"Conditional Marginal likelihood\" as an alternative to the likehood (although for generalisation context): Lotfi, S., Izmailov, P., Benton, G., Goldblum, M., & Wilson, A. G. (2022, June). Bayesian model selection, the marginal likelihood, and generalization. In International Conference on Machine Learning (pp. 14223-14247). PMLR. This relates to your definition of C-MAE. Could you elaborate on the differences between the two metrics and possibly compare them in the current setting?\n- The computational complexity of the approach could be an issue as the paper tackles a practical estimation problem. Could the authors please provide an exact analysis of the computational complexity of the method and suggest possible ways of improvement?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7380/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698469284028,
            "cdate": 1698469284028,
            "tmdate": 1699636883035,
            "mdate": 1699636883035,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UIeNaxjD5u",
                "forum": "r6NMqADLGQ",
                "replyto": "18jX8AucFR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7380/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7380/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your review. We address your questions below:\n\n1. **In Section 2.1, $\\sigma_{\\Theta}$ has not been defined ...** \\\nWe have clarified this in the related work in the **revised draft**. $\\sigma_{\\Theta}$ (or Var$_\\theta$) is a vector encoding a diagonal covariance matrix. The authors of Beta-NLL specify Cov($\\hat{y}$) to be a diagonal matrix.\n1. **The sentence after Equation (6)** \\\nWe have completed the sentence.\n1. **What is the theoretical explanation motivating the use of the thirs matrix term $k_3(x)$ ...** \\\nWe have addressed this in the revised draft. The sum of two independent Gaussians is the sum of the means and covariances of the two Gaussians. Therefore, the covariance of a sample can be attributed to the nature of the function underlying the sample as well as independent stochasticity, which is not a function of the input. We attributed $k_1(x)$ and $k_2 (x)$ to the former and $k_3(x)$ to the latter. Given the independence assumption, the final formulation can be written as the sum of all three terms. \n1. **Conditional Marginal Likelihood comparison.** \\\nThank you for pointing us to this paper, which we have included in our related work. However, we believe that not comparing to Lotfi et al. should not be considered a weakness since the two works are fundamentally different. Conditional Marginal Likelihood is a metric to evaluate generalization. The metric does so by creating two non-overlapping subsets of the dataset: $D_1$ and $D_2$, training a model on $D_1$, and quantifying its performance on $D_2$. By contrast, CMAE is a metric with a different objective. Given a sample $(x, y)$ with its prediction $(\\hat{y}, \\textrm{Cov}(\\hat{y}))$, the metric evaluates the covariance by measuring the improvement in $\\hat{y}$ when $y$ is partially observed. Hence, the two metrics are fundamentally different.\n1. **Computational complexity ...** \\\nIndeed, we note in our limitations section that the primary bottleneck of our approach is in computing the Hessian. While determining the computational complexity of the Hessian for a generalized network is non-trivial, we draw your attention to [1]. Computing the Hessian for a function that maps an m-dimensional x to an n-dimensional y has a complexity of $O(nm^3)$.\nIn practice, covariance estimation can be performed using a smaller, proxy model in place of a large model (which could be retained for mean estimation). The reduced parameter count would decrease the computational requirements of computing the Hessian.\n\n\n\n[1] Yao, Zhewei, et al. \"Pyhessian: Neural networks through the lens of the hessian.\" 2020 IEEE international conference on big data (Big data). IEEE, 2020."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7380/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700064472159,
                "cdate": 1700064472159,
                "tmdate": 1700064472159,
                "mdate": 1700064472159,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "f6A4xlo6wu",
                "forum": "r6NMqADLGQ",
                "replyto": "UIeNaxjD5u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7380/Reviewer_kqRb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7380/Reviewer_kqRb"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for modifying the manuscript and for providing some details about the unclear parts."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7380/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700480478255,
                "cdate": 1700480478255,
                "tmdate": 1700480478255,
                "mdate": 1700480478255,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DdPg0Vl8Q6",
            "forum": "r6NMqADLGQ",
            "replyto": "r6NMqADLGQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7380/Reviewer_r64o"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7380/Reviewer_r64o"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a novel approach to train covariances, stemming from the observation that, when learning Gaussian models, the mean and the covariance are independently parametrised but they affect one another during training. Consequently, they define a new parametrisation for the covariance that is *tied* to the curvature of the mean function around its argument. The approach is compared to ML and other recent methods using synthetic and real-world data, such comparison is performed under a novel performance indicator for covariance modelling introduced in this paper too."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea of questioning the standard approach to training covariances is undoubtedly of interest to the community. Furthermore, developing this new approach and a performance indicator is a valuable contribution."
                },
                "weaknesses": {
                    "value": "Although the general idea is attractive and, to some extent, promising, the concept is not properly exploited in the article. In this regard, the most relevant weaknesses of the paper are: \n\n- Generally, the paper could be clearer, and its format can be improved. For instance: \n    - the abstract (5th line) defines the mean of a Gaussian as $f(x)$, and the covariance as $Cov(f(x))$. So, it is not clear whether $f(x)$ refers to the RV to be modelled or its mean.\n    - Figs 1 and 2 are not referred to in the body of the paper, and they are not self-explanatory either. To this Reviewer, their purpose is not clear. \n    - Tables span beyond the margins of the text\n    - A few times, it is mentioned that the experiments are run over _multiple network architectures_; however, in the experiments, there is no mention of specific architectures used\n    - gaussian, hessian -> Gaussian, Hessian\n    - axis labels in Fig 3 are too small\n\n- Also, in the line of clarity, the paper is motivated by the pitfalls of maximum likelihood (ML). However, the proposal in the paper results in a specific parametrisation of the covariance, which ties the structure of the covariance and the mean (eq 8 shows how the covariance contains the Jacobian of the mean function). Therefore, the proposal is not _another training strategy_ but rather a covariance parametrisation. As a matter of fact, after eq 8 the authors state that their parametrisation is used alongside ML. \n\n- Benchmarks are unclear: The experiments compare the proposed parametrisation against **NLL** (though the proposed method also uses NLL as far as I understand), **Diagonal**, which I assume also uses NLL and other methods. It is thus confusing if the paper compares approaches to training covariances or models for covariances.  \n\n- Another point worth noticing is the fact that the paper proposes a variance parametrisation and also a performance indicator (C-MAE). However, this is the only performance indicator used in the experiments, meaning that other than the conceptual justification of C-MAE (which I find valid), there is no experimental validation. This means that the authors propose a model and use their own defined metric to assess it.   \n\n- There should be given more details about the choice of $k_1,k_2,k_3$, the networks, and the learning objectives."
                },
                "questions": {
                    "value": "Please refer to the comments in the previous section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7380/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698696839683,
            "cdate": 1698696839683,
            "tmdate": 1699636882914,
            "mdate": 1699636882914,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "h2ppKy8eDB",
                "forum": "r6NMqADLGQ",
                "replyto": "DdPg0Vl8Q6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7380/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7380/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your review. We understand that the concerns majorly revolve around presentation and formatting. We have addressed the figures, tables and capitalisation in the **revised draft**. We also answer your specific questions:\n\n1. **So, it is not clear whether  refers to the RV to be modelled or its mean.** \\\n$f(x)$ refers to the mean of the random variable being modelled. We have updated this in the abstract.\n1. **...multiple network architectures...** \\\nThe architecture used depends on the task being solved. For our synthetic experiments and UCI Regression, we use fully connected networks to learn $y$ in $R^m$ from $x$ in $R^n$. We specifically choose the task of human pose estimation to showcase the use of convolutional neural networks, and regress heatmaps of shape $R^{j \\times 64 \\times 64}$ from images of shape $R^{256 \\times 256}$. Collectively, our experiments address different shapes and types of input-target pairs. We have rephrased this sentence in the draft.\n1. **Also, in line of clarity ...** \\\nWe agree with your observations. The proposal is indeed a way to model the covariance which is learnt via the negative log-likelihood. To avoid ambiguity, we have renamed the paper from \"_How To Train Your Covariance_\" to \"_How To Model Your Covariance_\". We also rephrased the text to address this ambiguity wherever applicable in the draft.\n1. **Benchmarks are unclear.** \\\nIndeed, we compare our method against other approaches to model and train the covariance. While NLL, Diagonal and our proposed model for the covariance use NLL as a training objective, Beta-NLL and Faithful Heteroscedastic Regression have their own training objectives, which are variants of NLL. All these models/training strategies give a covariance prediction with which we compare our proposed method.\n1. **Another point worth noticing ...** \\\nWhile we understand your concern, we believe that this point should not be considered as a weakness. The reason we propose a new metric is because of the lack of any measure to directly assess the covariance. Moreover, CMAE in our opinion elegantly combines the notion of L1/L2 error with correlations through the fundamentals of conditioning a distribution. Additionally, the metric is independent of the training objective for all methods, and hence we believe it is a fair comparison. We hope that this paper sparks discussion in the community not only on covariance estimation but more importantly on covariance evaluation.\n1. **There should be more details about $k_1, k_2, k_3$ ...** \\\nWe have addressed this towards the end of our methodology section.  The covariance estimator $g_{\\Theta}(x)$ predicts $k_1(x), k_2(x)$ and $k_3(x)$, where $k_1(x), k_2(x)$ are positive scalars. We enforce $k_3(x)$ to be positive semi-definite by predicting an unconstrained matrix and multiplying it with its transpose, similar to previous work."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7380/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700064425586,
                "cdate": 1700064425586,
                "tmdate": 1700738621307,
                "mdate": 1700738621307,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OVteRuia5C",
                "forum": "r6NMqADLGQ",
                "replyto": "DdPg0Vl8Q6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7380/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7380/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Discussion Period Ends Soon"
                    },
                    "comment": {
                        "value": "Dear Reviewer r64o,\\\nWe thank you for your time and efforts. \\\nSince the discussion period ends soon, please let us know if we could resolve your questions.\\\nWe believe that our revised draft addresses your concerns.\\\nWe welcome you to read our general response as well.\\\nIf satisfied, we would be grateful if you would consider accepting the manuscript.\\\nOnce again, thank you for your time and efforts!"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7380/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647466422,
                "cdate": 1700647466422,
                "tmdate": 1700647466422,
                "mdate": 1700647466422,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1AovS9bEbF",
            "forum": "r6NMqADLGQ",
            "replyto": "r6NMqADLGQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7380/Reviewer_G5cy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7380/Reviewer_G5cy"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new method to lean the multivariate target distribution, namely the mean and heteroscedastic covariance. The authors contributions are two folds: 1) a concept of spatial variance by studying the curvature around input x. 2) conditional mean absolute error for evaluations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Overall I believe the paper is well motivated, and new concepts such as spatial variance are clearly presented. The experiment sections are also thorough to demonstrate the effectiveness of the proposed methods.\n\nOriginality: Two key concepts presented in this paper (spatial variance and C-MAE) are novel. \n\nQuality: The paper is written with good quality. The authors motivated the problem well, provided detailed derivations and extensive experiment results.\n\nClarity: The paper is easy to follow.\n\nSignificant: The paper is important as it provides a new method for heteroscedastic covariance learning."
                },
                "weaknesses": {
                    "value": "See questions below."
                },
                "questions": {
                    "value": "I mainly have the following several questions:\n\nThe C-MAE operates like a leave one out fashion. In general for multi-variate Gaussian, we can do any leave-k-out. Would it provide more info if the consider any k greater than 1? Or mainly, the authors may want to illustrate the specific choice of leave one out here.\nWhat are the specific choice considerations for k1, k2 and k3? Would any regularity conditions further help with supervision?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7380/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698810750189,
            "cdate": 1698810750189,
            "tmdate": 1699636882791,
            "mdate": 1699636882791,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xbhKTT5Q83",
                "forum": "r6NMqADLGQ",
                "replyto": "1AovS9bEbF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7380/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7380/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your review. We address your questions below:\n\n1. **leave-one-out vs leave-k-out** \\\nWe have updated the CMAE section in the **revised draft** to address this. While _leave-one-out_ can be generalized to _leave-k-out_, we do not observe any change in the evaluation trend. A method having a lower _leave-one-out_ error also has a lower _leave-k-out_ one. Moreover, _leave-k-out_ requires taking nCk combinations, i.e., significantly more than the $n$ combinations required in _leave-one-out_. This motivates the use of the _leave-one-out_ strategy.\n1. **What are the specific choices for $k_1, k_2, k_3$?** \\\nWe have addressed this towards the end of our methodology in our revised draft.  The covariance estimator $g_{\\Theta}(x)$ predicts $k_1(x), k_2(x)$ and $k_3(x)$, where $k_1(x), k_2(x)$ are positive scalars. We enforce $k_3(x)$ to be positive semi-definite by predicting an unconstrained matrix and multiplying it with its transpose, similar to previous work.\n1. **Would any regularity conditions further help with supervision?** \\\nWe believe that having priors would allow for faster and better convergence of the covariance. To maintain parity with previous work, we do not assume priors. Moreover, we work within the framework of unsupervised heteroscedastic covariance estimation, which means that we do not have supervisory signal for the covariance."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7380/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700064501663,
                "cdate": 1700064501663,
                "tmdate": 1700064501663,
                "mdate": 1700064501663,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]