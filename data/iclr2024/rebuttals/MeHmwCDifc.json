[
    {
        "title": "The Trickle-down Impact of Reward Inconsistency on RLHF"
    },
    {
        "review": {
            "id": "3r0WV196I3",
            "forum": "MeHmwCDifc",
            "replyto": "MeHmwCDifc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8541/Reviewer_n2Hx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8541/Reviewer_n2Hx"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates an important yet often overlooked problem - the robustness of the reward model (RM). The authors propose a benchmarking technique called Contrast Instructions that gauges the reward consistency of an RM. The reward consistency is measured by consistency in preference ranking if given a pair of lexically similar instructions with different ground truth responses. Concretely, it is quantified by (1) response consistency, if the RM can identify the better response for a given instruction, and (2) instruction consistency, if the RM can identify the most fitting instruction for a given response. The benchmark dataset is constructed based on four open-source human preference datasets of various NLP tasks. The authors showcased that an RLHF model trained with a more reward-consistent RM outperforms an RLHF model trained with the original RM in human evaluations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper is well-written and it is easy to follow.\n2. The idea is straightforward but the underlying research problem is significant and yet often overlooked.\n3. The trickle-down effect of reward consistency on RLHF training is an interesting observation, which intuitively makes sense.\n4. Table 7 is helpful in seeing that reward consistency and test set accuracy (if I understood correctly) do not necessarily correlate. This is similar in the sense that the (dis-)correlation between human score and FID is often discussed in generative models."
                },
                "weaknesses": {
                    "value": "1. The way that the authors constructed the dataset, is filtering by the cosine similarity between SimCSE embeddings that are in the range of [0.75, 0.9]. This seems convenient but I wonder how reliable is this method. Have you done a manual inspection to measure the agreement rate between the method and human evaluators? Or maybe you could try using a model-based approach like prompting GPT-4?\n2. As I am more interested in the benchmark dataset itself, the evaluation for dataset validation seems limited. **The author should focus on providing a reliable benchmark as the major scientific contribution**; I believe the finetuning methods discussed in the manuscript are not as significant. Therefore, the authors should provide more evaluation results on more LLMs (open-source + closed-source) to validate your benchmark dataset. The evaluation results on popular models like GPT-4 would be very helpful. Although you can't get the weights, test by prompting would be sufficient. I am also curious to know the difference in consistency between the pre-trained model vs their SFT-ed version (i.e. llama2-7b vs llama2-7b-chat). If the compute resource allows, the parameter scaling on reward consistency could also be an interesting point for investigation.\n3. The benchmark dataset should be submitted along with the paper, as I believe this is the core contribution of the paper.\n4. The data variances in Figure 3 make the comparison in Sec. 7.2 rather inconclusive. However, I praised the authors' honesty in showing error bars."
                },
                "questions": {
                    "value": "1. How concerned are you about the risk of data leakage? What implications would arise if instances from the benchmark dataset were also present in LlaMa-7B's pre-training data or the dataset used to train the reward model? Have such overlaps impacted the assessment of reward consistency?\n2. I need more details on Section 4. For \"Single-Task\", what is the split between train and test?\n3. For Section 4, why would you fine-tune a benchmark dataset? And the fact that it performs so poorly after being trained and tested on the same dataset distribution is surprising. The consistency improvement seems marginal.\n4. What is RMEval in Table 7? Test set accuracy on binary classification task?\n\nOverall I am positive about this paper. This is an interesting piece of research. If the authors can properly address mines and other reviewer's concerns, I will agree to raise my review score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8541/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8541/Reviewer_n2Hx",
                        "ICLR.cc/2024/Conference/Submission8541/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8541/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698683215175,
            "cdate": 1698683215175,
            "tmdate": 1700642218256,
            "mdate": 1700642218256,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LhoboDSeEV",
                "forum": "MeHmwCDifc",
                "replyto": "3r0WV196I3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8541/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8541/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q1**:The way that the authors constructed the dataset, is filtering by the cosine similarity between SimCSE embeddings that are in the range of [0.75, 0.9]. This seems convenient but I wonder how reliable is this method. Have you done a manual inspection to measure the agreement rate between the method and human evaluators? Or maybe you could try using a model-based approach like prompting GPT-4?\n\n**A1**: Regarding cosine range, we have consideration for the following factors:\n- Upper Bound: We set the upper limit (e.g., 0.9) to avoid retrieving identical instructions. This is crucial to ensure  that $I_{A} \\circ r_{A}$ is better than $I_{B} \\circ r_{A}$.\n\n- Lower Bound: We avoided setting the lower bound too low (e.g., 0.5) to maintain the challenge for the RM. A lower range would make it too easy for the RM to judge, failing to test whether RM has a nuanced understanding of human preferences.\n\n- Range Interval: We didn\u2019t opt for a narrow range (e.g., [0.85, 0.9]) as it would greatly limit the size of our Contrast Set.\n\nTo satisfy all of three factors, we can only handcraft a proper range, so we set an upper limit (e.g. 0.9) to avoid retrieving identical instructions, and the lower limit (e.g. 0.75) is adjusted mostly to make sure we can sample enough test examples in each human preference dataset.   \n\nWe provide an automatic evaluation for the quality of our benchmarks, which uses GPT4 to evaluate our contrast instructions. Specifically, we use In-context learning (appending four demonstrations where each has a better and a worse response) to help GPT4 distinguish better or worse responses in our benchmarks. The results are shown in Appendix G. We can observe that GPT4 achieves around 95% accuracy on our benchmarks, indicating a high matching degree between our benchmark and GPT4. This guarantees the quality of our Contrast Instructions to a certain extent.\n\nPlease note that our human performance (avg. 80% accuracy) on our contrast instruction does not indicate flaws in our Contrast Instructions. Since this performance is calculated under scenarios where the annotators are forbidden to use networks and search engines, the annotators\u2019 performance would be limited to their knowledge. With the help of search engines and other tools, our annotators achieve an average performance beyond 95%.\n\n**Q2**: As I am more interested in the benchmark dataset itself, the evaluation for dataset validation seems limited. The author should focus on providing a reliable benchmark as the major scientific contribution; I believe the finetuning methods discussed in the manuscript are not as significant. Therefore, the authors should provide more evaluation results on more LLMs (open-source + closed-source) to validate your benchmark dataset. The evaluation results on popular models like GPT-4 would be very helpful. Although you can't get the weights, test by prompting would be sufficient. I am also curious to know the difference in consistency between the pre-trained model vs their SFT-ed version (i.e. llama2-7b vs llama2-7b-chat). \n\n**A2**: For GPT4\u2019s results on our Contrast Instruction, please refer to our answer to your **Q1**.\n\nRM consistency does not equal the consistency of the final DPO-trained LLM, that\u2019s why we use the \u2018trickle-down effect\u2019 in our title. However, exploration on SFT/RLHF LLMs would be interesting, we have explored potential methods for assessing preference consistency in LLMs trained via SFT/RLHF, each with its own limitations:\n- Prompting DPO-trained LLMs to choose the better response often leads to them not following the instruction correctly, such as repeating the instruction instead.\n- Calculating the likelihood of instruction-response pairs is hindered by length bias, with DPO-trained LLMs tending to favor longer responses, resulting in poor performance on preference benchmarks.\n\n**Q3**:The benchmark dataset should be submitted along with the paper, as I believe this is the core contribution of the paper.\n\n**A3**: We\u2019ve just uploaded the data in the system, feel free to check."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700369388623,
                "cdate": 1700369388623,
                "tmdate": 1700369388623,
                "mdate": 1700369388623,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qWdN0Midrv",
                "forum": "MeHmwCDifc",
                "replyto": "3r0WV196I3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8541/Reviewer_n2Hx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8541/Reviewer_n2Hx"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your rebuttal"
                    },
                    "comment": {
                        "value": "Hi,\n\nI have taken a look at the newly added Appendix G and H. I praised the efforts for running these experiments during this short amount of time. The results are interesting to see - a human with a search engine performs nearly perfectly and GPT-4 performs on par with humans, while other smaller models perform nearly as random-picking. I wouldn't be able to draw any significant conclusion from the scaling tests, but I understand that it would be too much to ask you here to finetune larger models.\n\nOverall I like the idea of this work, though the rigorousness of the methods could be improved. My opinion on this paper remains the same - the authors should focus on providing a reliable benchmark supported by lots of validations instead of focusing on bringing more things into the basket of \"novel contributions\".\n\nMost of my questions have been resolved in this rebuttal. I agree to raise my review score. I hope the authors can continue working on this study in the future."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642202403,
                "cdate": 1700642202403,
                "tmdate": 1700642525884,
                "mdate": 1700642525884,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7JROTldfix",
            "forum": "MeHmwCDifc",
            "replyto": "MeHmwCDifc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8541/Reviewer_Kxr1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8541/Reviewer_Kxr1"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on reward inconsistency while doing Reinforcement from Human Feedback studying the various reasons around such inconsistencies by developing relevant metrics to measure the same. They demonstrated the failure of the current reward models typically used in RLHF (with comparison to avg human)  on the CONTRAST INSTRUCTIONS introduced in the paper, which the authors attribute to reward inconsistency. Finally, the authors propose two strategies to mitigate such reward inconsistency with improved downstream performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The primary objective of the paper is to demonstrate the reward inconsistency with the standard RLHF training, as shown in Figure 1 where it shows for similar (but distinct) prompts, the rewards assigned are inconsistent with the current reward models. A primary reason attributed to the over-optimization is the fact that the current reward models are trained on datasets that don't represent close preferences in other words, the current reward models are not optimized for prompts where both the preferences are near-optimal and one is slightly better than the other. According to the authors, those are the places where the current reward models suffer and that's where CONTRAST INSTRUCTIONS helps in providing a meaningful evaluation and mitigation to such over-optimization."
                },
                "weaknesses": {
                    "value": "1. The paper claims the issue in reward inconsistency is due to the reward over-optimization issue (citing Gao et, al). Still, the reason for such reward over-optimization and how that causes the inconsistencies in the particular Contrast Dataset is unclear.  More specifically the author claims that \"From the RMs\u2019 perspective, correctly distinguishing between a clearly good vs. bad response is easy\". But the contrast dataset for example shown in Figure 1, they are very different questions although textually there are common words. Does that mean the current LLMs are not able to produce representations that can separate the two is not very clear and needs further clarification. For ex: \"A is a  good student\" and \"A is a bad student\", they have a lot of words in common but in representation space they should be extremely different and should be trivially separated if the representations are reasonable. Thus it's not very clear how Contrast Dataset is providing a challenging dataset to test RM model inconsistency.\n\n2. Another point is that why such reward inconsistencies are not observed in standard available RLHF datasets like Carper AI, hh, etc. are not made explicit. Does it mean that the majority of the datasets are easily separable and lacks samples towards the optima which is hard for human to segregate? A comparison with current RLHF methods is critical on standard datasets to understand the significance and mitigation of the problem properly.\n\n3. \"Surprisingly, we observe close to random-chance performance ..while humans are able to rank the responses correctly in \u2248 80% of the cases\" So, is it the case that humans are able to identify it but the reward models are not able to learn the same and failing is not very clear since its a supervised learning problem and can be shown to be strongly convex under certain settings. Hence, a clear description is missing on the same, and will be interesting to have a discussion with reference to the recent works showing convergence [1,2] and where this issue can arise in that context.\n\n[1]. Banghua Zhu, Jiantao Jiao, Michael I. Jordan \"Principled Reinforcement Learning with Human Feedback from Pairwise or K-wise Comparisons\"\n\n[2]. Souradip Chakraborty, Amrit Singh Bedi, Alec Koppel, Dinesh Manocha, Huazheng Wang, Mengdi Wang, Furong Huang \"PARL: A Unified Framework for Policy Alignment in Reinforcement Learning\""
                },
                "questions": {
                    "value": "1. Why augmentation helps as a solution to mitigate the problem is not clear in the context of the Contrast Dataset. Will be helpful to have a more rigorous discussion on the same?\n2. ContrastDataset provides is challenging dataset, how are humans able to do good on it? Is it mainly hard for the LLMs since the representations are sub-optimal?\n3. A comparison with current SOTA RLHF or reward models on standard/open-sourced datasets will be helpful in understanding the crux of the problem.\n4. Recent work on Direct Pref Optimization (DPO) shows that it can learn with learning reward models, will such an issue happen there as well? \n5. Reward ensemble seems to work well in RLHF to mitigate over-optimization as also followed in [1] for Robotics. Will be interesting to see if that helps and have a discussion around the same.\n6. The notations used in the equation after 1 (missing no) are not very clear and it will be helpful if they can be updated as per standard RL notations of trajectory, state, etc.\n\n[1]. Thomas Coste, Usman Anwar, Robert Kirk, David Krueger \"Reward Model Ensembles Help Mitigate Overoptimization \" https://arxiv.org/abs/2310.02743"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8541/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8541/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8541/Reviewer_Kxr1"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8541/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698799662250,
            "cdate": 1698799662250,
            "tmdate": 1700712631475,
            "mdate": 1700712631475,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "roh3KMc4Zi",
                "forum": "MeHmwCDifc",
                "replyto": "7JROTldfix",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8541/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8541/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks you for your comments and questions. We hope our responses can resolve your concerns:\n\n**Q1**: The paper claims the issue in reward inconsistency is due to the reward over-optimization issue (citing Gao et, al). Still, the reason for such reward over-optimization and how that causes the inconsistencies in the particular Contrast Dataset is unclear. More specifically the author claims that \"From the RMs\u2019 perspective, correctly distinguishing between a clearly good vs. bad response is easy\". But the contrast dataset for example shown in Figure 1, they are very different questions although textually there are common words. Does that mean the current LLMs are not able to produce representations that can separate the two is not very clear and needs further clarification. For ex: \"A is a good student\" and \"A is a bad student\", they have a lot of words in common but in representation space they should be extremely different and should be trivially separated if the representations are reasonable. Thus it's not very clear how Contrast Dataset is providing a challenging dataset to test RM model inconsistency.\n\n**A1**: \n\nPlease note that we are **NOT** claiming the specific benchmark itself is challenging for all reward models or LLMs. What we are claiming here is the **Contrast Instructions** benchmarking strategy, when applied to a specific RM training set to create an evaluation set, can reveal inconsistencies of the resulting RMs. \n\nFirst, we want to clarify that the (short) example shown in Figure 1 is mostly for illustration purposes due to space constraints. Most examples in contrast instructions involve much longer responses that are difficult for the models to distinguish \u2013 some of the examples can be found in Table 1. \n\nBut even with the \u201cA is a good student\u201d vs. \u201cA is a bad student\u201d example, the representation would be \u201cextremely different\u201d **only when your model is properly trained to generalize to the task that it\u2019s solving**. e.g. sentiment classification. Let\u2019s say if the task prompt asks \u201chow topically relevant are the two sentences\u201d, the two would have similar representations. Here you are making the assumption that the model generalizes well from training, and so it would be sensitive to semantic changes even when there\u2019s a lot of lexical overlap between responses. What we intend to show with our $C_{res}$ and $C_{ins}$ metric is exactly that RMs do NOT generalize well with the standard training objective. In NLP, similar generalization issues and insensitivity to examples with high lexical overlap has been observed across different settings [e.g. 1, 2]. We are observing similar patterns with RMs as well.\n\nReward Over-optimization \u2013 For example, within the open-source community (e.g. w/ Llama1 and 2), examples from StackExchange make up for majority of the RM training data.  StackExchange follows a specific \u201cproxy\u201d way of defining human preferences, e.g. within the same question, each human preferences example are constructed from responses to same question/thread, and responses with the highest votes are considered the preferred responses. What we intend to show with our $C_{res}$ metric is that RMs trained this way only fits the training distribution, and don\u2019t generalize to what we think RM should be doing \u2013 i.e. being able to distinguish between good/bad responses consistently. With contrast instructions, we observe that RMs fail to do so even when the questions come from the domain (e.g. StackExchange) as seen in training. \n\n[1] Gardner, Matt, et al. \"Evaluating Models\u2019 Local Decision Boundaries via Contrast Sets.\" Findings of the Association for Computational Linguistics: EMNLP 2020. 2020.\n\n[2] McCoy, Tom, Ellie Pavlick, and Tal Linzen. \"Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference.\" Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700368002988,
                "cdate": 1700368002988,
                "tmdate": 1700368002988,
                "mdate": 1700368002988,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ItMU62aGU2",
                "forum": "MeHmwCDifc",
                "replyto": "7JROTldfix",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8541/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8541/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q5**: ContrastDataset provides is challenging dataset, how are humans able to do good on it? Is it mainly hard for the LLMs since the representations are sub-optimal?\n\n**A5**: While Reward Models (RMs) currently struggle to differentiate patterns within Contrast Instructions, this challenge is not mirrored in human performance. Humans are adept at discerning subtle semantic differences, an ability that RMs have yet to develop fully. \n\nThis disparity highlights the difficulty RMs face in learning representations that effectively separate human-preferred responses from non-preferred ones. At least, our results show the limitation of the main-stream protocol for modeling RM, thus motivating more sophisticated designs for RM training.\n\n**Q6**: Recent work on Direct Pref Optimization (DPO) shows that it can learn with learning reward models, will such an issue happen there as well?\n\n**A6**: Given that DPO algorithms do not incorporate an explicit RM, evaluating their consistency in the same manner as RMs is not feasible.\n\n\nMoreover, RM consistency does not equal the consistency of the final DPO-trained LLM. That\u2019s why we use \u2018trickle-down effect\u2019 in our title. However, we have explored potential methods for assessing preference consistency in LLMs trained via DPO, each with its limitations:\n- Prompting DPO-trained LLMs to choose the better response often leads to them not following the instruction correctly, such as repeating the instruction instead.\n- Calculating the likelihood of instruction-response pairs is hindered by length bias, with DPO-trained LLMs tending to favor longer responses, resulting in unfair evaluation on preference benchmarks.\n\n**Q7**: Reward ensemble seems to work well in RLHF to mitigate over-optimization as also followed in [1] for Robotics. Will be interesting to see if that helps and have a discussion around the same.\n\n**A7**: We tested the efficacy of ensembling various Reward Models (RMs) using checkpoints from Stack-exchange available on Huggingface, including \n- https://huggingface.co/trl-lib/llama-7b-se-rm-peft\n- https://huggingface.co/mnoukhov/llama-7b-se-rm-peft\n- Our trained LLaMa-7B reward model (On Stack-exchange)\n- Our trained LLaMa2-7B reward model (On Stack-exchange)\n\n\nOur experiments incorporated three ensembling techniques referenced in the paper [5]: mean, worst, and uncertainty-based ensembling. However, as shown in our results below, this approach did not yield significant improvements over our baseline model. Overall, there is no statistical significance between performance of three ensembling methods, based on a pair-wise t-test where p value is not smaller than 0.05. This outcome indicates that simply ensembling different RMs can not address the core issues we are investigating.\n\n|  | None | Mean | Worst-Case | Uncertainty-Weighted |\n|---|---|---|---|---|\n| Performance | 50.1 | 49.2 | 50.3 | 50.6 |\n\n[5]: Reward Model Ensembles Help Mitigate Overoptimization\n\nIf you have any other questions, please feel free to ask."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700368748285,
                "cdate": 1700368748285,
                "tmdate": 1700369481154,
                "mdate": 1700369481154,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hGrjJw4cZ5",
                "forum": "MeHmwCDifc",
                "replyto": "ItMU62aGU2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8541/Reviewer_Kxr1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8541/Reviewer_Kxr1"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Comment by Authors"
                    },
                    "comment": {
                        "value": "Thanks for the rebuttal and providing details to my questions. I have a few more clarification questions to better understand the proposed method\n\n1. Regarding Q2, I am still not very sure that the representation learned by the LLMs are unable to distinct  \u201cA is a good student\u201d vs. \u201cA is a bad student\u201d, and also the references provided are of 2020 and before, so not sure if it applies to the evaluation performance of the SOTA LLMs, will be helpful to provide some latest evaluation studies if any (Poor generalization of SOTA LLMs )?. I agree that very challenging or complex contrast examples, it can be a concern but are there some examples of such, will be helpful to point there (sorry in case I might have overlooked them).\n\n2. Also, I didn't quite understand the answer to Q3, that if humans have given the correct preferences, still the reward models can't learn is because of the poor generalization of reward models? What is the specific reason? I see that reward overoptimization is used in the context? Can you please explain specifically what the relationship between the two is with concrete references? \n\n3. Also, the reason for the references was to understand which component of the analysis might break based on your hypothesis, which is not clear. \"The reward model trained from human preference is non-linear.\" ---> Its important to note that it has been considered linear [3] in the representation space. We know that pre-trained SOTA LLM embeddings provide a good representation space, and linearity on that might be restrictive I agree but not an unreasonable assumption to study. To summarize, a more rigorous discussion is required which will help in understanding both points 1 and 2.\n\n4. \"DPO-trained LLMs tending to favor longer responses,\" I guess that is also a problem with the current RLHFs as shown in [1]. Hence, it's not clear why this concern is appearing only for DPO and not for RLHF. \n\n[1]. Prasann Singhal, Tanya Goyal, Jiacheng Xu, Greg Durrett . A Long Way to Go: Investigating Length Correlations in RLHF https://arxiv.org/abs/2310.03716"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636113725,
                "cdate": 1700636113725,
                "tmdate": 1700636113725,
                "mdate": 1700636113725,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fSh3B1rqNE",
                "forum": "MeHmwCDifc",
                "replyto": "TQ9M8FnusG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8541/Reviewer_Kxr1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8541/Reviewer_Kxr1"
                ],
                "content": {
                    "title": {
                        "value": "Response to Comment by Authors"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response to my comments. It helps to clarify some of my confusion and hopefully will improve the draft for better readability.\n\n> \"Overall, we believe that the RM issues in our paper are not caused by poor generalization of LLM (which is also not our claim) but by the standard and commonly accepted protocol of RM training. This can be due to the model architecture, training objective (we have applied margin-based loss from LLaMa2 to train RM but failed to solve the inconsistency issues, as shown in our response to Reviewer iZzr), and other elements. The exact reason for this remains to be explored in future work. \" \n\nSo, if I understand correctly as the author states, the generalization of LLMs is not the reason for such issues arising in the paper which is reasonable, and I also believe that should not be the issue. On the other hand, it's still unclear what the reason actually is -->\"model architecture, training objective, and other elements\". Since it is critical to understand the reason for being able to reliably comment on the contribution of the paper. Hence, an ablation will be important to understand the reason behind this and why in spite of humans providing correct labels (so y is correct), and LLMs providing correct representation space (x, x-->y mapping is correct) a supervised model is not able to learn and generalize. Thus ablation in model architectures and on more reliable benchmarks will be critical."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688988905,
                "cdate": 1700688988905,
                "tmdate": 1700688988905,
                "mdate": 1700688988905,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rqKMR1PpCf",
                "forum": "MeHmwCDifc",
                "replyto": "pgOPbiupxR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8541/Reviewer_Kxr1"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8541/Reviewer_Kxr1"
                ],
                "content": {
                    "title": {
                        "value": "Response to Comment by Authors"
                    },
                    "comment": {
                        "value": "Thanks for the response. I agree with most of the points mentioned in the final response and definitely would request the authors to add the points of discussion to the paper. I will increase my score, but still, as I mentioned more technical clarity is needed with further ablation with more emphasis on the point of why the reward model is unable to learn even when human preferences are accurate and also LLM representations are generalizable. Some of these points are mentioned during our discussion which if added will help the readers to understand the significance."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700712598682,
                "cdate": 1700712598682,
                "tmdate": 1700712598682,
                "mdate": 1700712598682,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wOdAhbhVf8",
            "forum": "MeHmwCDifc",
            "replyto": "MeHmwCDifc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8541/Reviewer_iZzr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8541/Reviewer_iZzr"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies reward models in RLHF under the lens of consistency -- whether the RM can adapt its scores to semantic changes to different prompt response pairs. The authors create benchmarks to verify consistency of rewrd models called Contrast Instruction where the authors study the reward model score for lexically similar instructions with different responses. They claim that current RM with the standard loss functions suffer in these contrast instructions compared to human preferences. The authors provide two techniques -- one at train time and one at inference time that improve reward consistency through extrapolation across similar examples. They also claim that reward model consistency has a correlation with the usefulness of the RLHF responses."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper exposes an interesting and useful aspect of reward modeling in RLHF -- reward model consistency. Apart from standard RM eval, evaluating RMs for consistency is an important aspect that future works can take into consideration or the current eval sets be expanded to include this as a benchmark task. The authors clearly define what consistency means and setup constrast instructions dataset to evaluate the reward models. While the dataset needs to be vetted more carefully, having this dataset as part of standard RM evals could be a useful exercise for practitioners. The methods that the authors propose to improve consistency are simple to implement."
                },
                "weaknesses": {
                    "value": "While the paper exposes an important aspect of reward models, one thing that could be improved about in  the paper is the limited nature of experiments and evaluation.\n\n*  I was not sure of whether the contrast instructions generated automatically do mean something different where one answer is strictly better than the other -- for instance . The authors say they restricted this based on a particular cosine distance range, but some more rigorous evaluation around this could have made the impact of the dataset a bit stronger. For instance, one can query a bigger open LLMs in order to validate that the contrast instructions are fit for purpose. The other option could have been to conduct human eval on a randomly selected subset of the contrast instructions dataset. \n\n*  The authors could have just chosen one single baseline LLAMA 7B and use that to validate their claims. The behaviour of LLAMA 7B could be ground in the kinds of datasets and training procedures that it was subject to. Having few other RM baselines trained with similar RM training loss could have made the claims even more stronger.\n\n *  The human evaluation was done on 100 randomly selected data points and no standard errors and variance numbers were reported around the results. Since some of the results are close to each other, having the error bands around the result should help clarify the significance of improvements.\n\nOne other concern about human evaluation that I have is about the possibilities of potential bias as the authors themselves serve as human annotators for human eval of the results. There are several important statements made in this paper based on human evaluation. I would have preferred at least a mixture of external annotators (who are not aware of the work or how the responses were generated) to de-bias the evaluations. \n\"Finally, we report human performance resulting from the majority vote of three human annotators (the authors) on 100 randomly selected data points.\""
                },
                "questions": {
                    "value": "I have the following questions for the authors\n\n1. There were certain choices of methods made in terms of methods for response/instruction similarity and the dataset construction including the augmented data points for ConvexDA. Have other methods been evaluated and ablated against before choosing these methods ? I wanted to understand if we are doing the best we can in terms of construction of these datasets and methods.\n\n2. I know you have considered the classic RM loss, log (sigmoid (R_chosen - R_rejected)) ? Have you considered other loss functions such as the margin loss used in LLAMA 2 ? I am not specifically looking for one kind of loss, but just a choice of few other RM losses. Considering different losses can help us understand whether RM inconsistency arises from the dataset or the kind of losses used or a combination of both. This would be a good ablation to understand where RM developers should focus more of their efforts on.\n\n3. Lot of times, RM training can easily lead to overfitting to the training datasets and creating a generalization gap in the process. Can you kindly explain how you do model selection for RM training for the baseline and your variants ?\n\n4. I know that you choose only a single example in ConvexDA while you construct a few semantically similar examples. Have you evaluated what happens if you include all of them ? I know this is unfair to be compared to the baseline, but it would help us understand having how many semantically similar examples will be useful.\n\n5.In table 2, I was not sure what (estimated) human performance means. Can you kindly clarify ?\n\n6. I know evaluating with larger models would be resource intensive. But would it be possible to run the analysis with either a smaller or larger model than 7B to understand if and how reward model sizes influences reward consistency."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8541/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699189962835,
            "cdate": 1699189962835,
            "tmdate": 1699637068074,
            "mdate": 1699637068074,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "p4CpLaBsWL",
                "forum": "MeHmwCDifc",
                "replyto": "wOdAhbhVf8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8541/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8541/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your efforts in reviewing our paper. Here are the responses that we hope to resolve your concerns.\n\n**Q1**: I was not sure of whether the contrast instructions generated automatically do mean something different where one answer is strictly better than the other -- for instance . The authors say they restricted this based on a particular cosine distance range, but some more rigorous evaluation around this could have made the impact of the dataset a bit stronger. For instance, one can query a bigger open LLMs in order to validate that the contrast instructions are fit for purpose. The other option could have been to conduct human eval on a randomly selected subset of the contrast instructions dataset.\n\n**A1**: Thank you for the comments. We have human evaluation results on $C_{res}$ on each dataset in table 8 of Appendix B. Due to space constraints, we only reported human performance averaged over 4 datasets in Table 2. \n \nAfter reading the reviews, we conducted a more thorough human evaluation on each of the dataset. On average over 4 datasets , we see that humans rank responses correctly ~82% of the time. If we allow for Google search to access relevant knowledge, the accuracy goes up to 96%. We also conducted the same evaluation with GPT4 prompting with 4-shot demonstrations, from which we observed 95% accuracy in terms of $C_{res}$. We think the gap between RM vs. human performance helps justify our observations on RM inconsistency. \n\nRegarding cosine range, we have consideration for the following factors:\n- Upper Bound: We set the upper limit (e.g., 0.9) to avoid retrieving identical instructions. This is crucial to ensure  that $I_{A} \\circ r_{A}$ is better than $I_{B} \\circ r_{A}$.\n\n- Lower Bound: We avoided setting the lower bound too low (e.g., 0.5) to maintain the challenge for the RM. A lower range would make it too easy for the RM to judge, failing to test whether RM has a nuanced understanding of human preferences.\n\n- Range Interval: We didn\u2019t opt for a narrow range (e.g., [0.85, 0.9]) as it would greatly limit the size of our Contrast Set.\n\nTo satisfy all of three factors, we can only handcraft a proper range, so we set an upper limit (e.g. 0.9) to avoid retrieving identical instructions, and the lower limit (e.g. 0.75) is adjusted mostly to make sure we can sample enough test examples in each human preference dataset.   \n\nWe have updated the human eval and GPT4 results in Appendix G, where both human and GPT4 obtain over 90% accuracy on our Contrast Instruction. We believe these results can serve as a quality check for the data built by our Contrast Instruction benchmarking strategy. Moreover, we\u2019ll try to allot more space to show human performance in the main body as well.\n\n\n**Q2**: The authors could have just chosen one single baseline LLAMA 7B and use that to validate their claims. The behaviour of LLAMA 7B could be ground in the kinds of datasets and training procedures that it was subject to. Having few other RM baselines trained with similar RM training loss could have made the claims even more stronger.\n\n**A2**: Thanks for you this suggestion! \u2013 In Appendix H, we've added experiments with RMs trained with backbone models of different sizes. We observe that scaling model size leads to (1) increased test performance on the original human preference dataset, but (2) the $C_{res}$ and $C_{ins}$ performance remains flat. This echoes with our hypothesis that \u2013 standard RM training fits the only the training distribution, but do not lead to a generalized model that can distinguish good/bad responses consistently. Just like what our general response said, this RM consistency issue is model-agnostic.\n\n**Q3**: The human evaluation was done on 100 randomly selected data points and no standard errors and variance numbers were reported around the results. Since some of the results are close to each other, having the error bands around the result should help clarify the significance of improvements. One other concern about human evaluation that I have is about the possibilities of potential bias as the authors themselves serve as human annotators for human eval of the results. There are several important statements made in this paper based on human evaluation. I would have preferred at least a mixture of external annotators (who are not aware of the work or how the responses were generated) to de-bias the evaluations.\n\n**A3**: Thank you for your comments. \n\nRegarding your suggestion, we recruit two emergency Computer Science expert annotators (as understanding StackExchange responses requires domain knowledge in CS). The new results are shown in Appendix J, which aligns with our human evaluation. Besides, we provide the Cohen kappa correlation to show consistency between human annotation, which show that these two human evaluations share a certain agreement."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700367316308,
                "cdate": 1700367316308,
                "tmdate": 1700369201888,
                "mdate": 1700369201888,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yQUHud619k",
            "forum": "MeHmwCDifc",
            "replyto": "MeHmwCDifc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8541/Reviewer_gxZU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8541/Reviewer_gxZU"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of Reward Model (RM) inconsistency and its impact on the downstream RLHF model. The paper introduces the \"Contrast Instruction\" benchmark to measure the ability of a reward model to consistently recognize semantic changes in prompts and adapt reward assignments accordingly. It then shows that current RMs trained with standard ranking objectives underperform compared to human judgment on this benchmark. To address this inconsistency issue, the paper proposes two innovative techniques: \"ConvexDA\" and \"RewardFusion,\" which leverage extrapolation during RM training and inference, respectively, to enhance RM consistency without requiring additional training resources. The authors demonstrate that these advancements lead to more useful responses from RLHF models trained with a more consistent RM, highlighting the crucial role of consistency in maximizing the effectiveness of the RLHF process."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper presents a novel way to investigate inconsistency in reward model via constructing benchmarks with inconsistent instruction and response pairs. The paper further proposes a fine-tuned reward model on top of the contrastive instruction-response pairs, which leads to improved RLHF performance. I think the investigation sheds insight in the RLHF research field and the proposed method is neat and effective.\n\n2. The authors have performed extensive empirical study of the inconsistency of RMs, which clearly show that the current RMs do suffer from inconsistencies. The paper also provides thorough empirical evidence that shows the RLHF model can improve a lot with a more consistent RM trained on the constructed contrastive pairs of instruction and response."
                },
                "weaknesses": {
                    "value": "1. It's a little unclear why CONVEXDA is used as a way to robustify the RM. It seems that it might work because it specifically targets the proposed benchmark, which mainly tests inconsistent instruction-response pairs with lexically similar words, but I'm not sure if the method can work well in scenarios where inconsistent responses/instructions are beyond just lexically similar words with different meanings. Moreover, I wonder if it would work equally well to simply use a different LLM to generate those similar pairs for augmentation.\n\n2. I'm also less clear about why REWARDFUSION is needed and how it contributes to fixing inconsistency. Again, it seems to mainly target the type of inconsistency due to lexically similar words with different meanings, which may not be general enough."
                },
                "questions": {
                    "value": "1. Please clarify how general the CONVEXDA and REWARDFUSION are in fixing inconsistency beyond lexically similar but different words.\n2. Please compare to methods that simply use LLMs for data augmentation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8541/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699514568874,
            "cdate": 1699514568874,
            "tmdate": 1699637067975,
            "mdate": 1699637067975,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "swVJMHUUIj",
                "forum": "MeHmwCDifc",
                "replyto": "yQUHud619k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8541/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8541/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comments and questions! Here are the response that we hope to resolve your concerns.\n\n**Q1**: It's a little unclear why CONVEXDA is used as a way to robustify the RM. It seems that it might work because it specifically targets the proposed benchmark, which mainly tests inconsistent instruction-response pairs with lexically similar words, but I'm not sure if the method can work well in scenarios where inconsistent responses/instructions are beyond just lexically similar words with different meanings. \n\n**Q2**:I'm also less clear about why REWARDFUSION is needed and how it contributes to fixing inconsistency. Again, it seems to mainly target the type of inconsistency due to lexically similar words with different meanings, which may not be general enough.\n\n**A1+A2**: One can embed lexical variations in training when training RMs, but that is not the point. Our goal was to introduce methodologies to improve RM consistency in a manner that is **agnostic** to the choice of evaluation (i.e., not presupposing any knowledge of the patterns inherent in Contrast Instructions), thereby ensuring a fair evaluation.\n\nOur proposal approaches (ConvexDA and RewardFusion) are not informed of the pattern Contrast Instructions, and serve as general augmentation methods. Moreover, they both contribute to performance enhancements on the original test sets, as evidenced in Table 7 (RMEval) and Table 8 (Original test set), indicating that they are beneficial beyond the Contrast set pattern. Therefore, our approaches are designed not to be informed by the patterns specific to Contrast Instructions. They are crafted to address the general generalization of RM, like other data augmentation tricks. This overarching goal explains why the improvements (on the Contrast set) attributed to these methods are incremental.\n\n**Q3**: Moreover, I wonder if it would work equally well to simply use a different LLM to generate those similar pairs for augmentation.\n\n**A3**: We use different LLM paraphrases to generate augmented data for ConvexDA, and the results are updated in the Appendix I. We apply three different paraphrasers, including T5-based, Falcon-based, and Parrot-based paraphrases, as shown in Table 13, the choice of paraphraser in ConvexDA does not substantially influence the effectiveness of ConvexDA.\n\n**Q4**: Please compare to methods that simply use LLMs for data augmentation.\n\n**A4**: In Appendix F, we present a comparison between ConvexDA and standard data augmentation (DA) techniques, such as paraphrasing. ConvexDA is not aimed at outperforming traditional data augmentation methods in terms of raw performance. Figure 8 shows that ConvexDA achieves the desired augmentation effect with greater efficiency, making it a superior choice when considering the trade-offs inherent in the data augmentation process.\n\nIf you have any other questions, please feel free to ask."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700366768633,
                "cdate": 1700366768633,
                "tmdate": 1700366768633,
                "mdate": 1700366768633,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WREReQ7S8n",
            "forum": "MeHmwCDifc",
            "replyto": "MeHmwCDifc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8541/Reviewer_5JFN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8541/Reviewer_5JFN"
            ],
            "content": {
                "summary": {
                    "value": "The authors study reward models that are used for RLHF  to tune LLMs for desirable generations. Specifically the aspect of (in-)consistency is studied. The paper studies the following research questions:\n\n- How to measure in-consistency of reward models? : \n  - The authors introduce the Contrast Instructions benchmarking strategy to measure in-consistency.\n  - The benchmark consists of quadruplets consisting of lexically similar instructions but different responses. Two metrics are proposed: \n    - Response consistency: Can the RM assign a higher score to the correct response given the instruction \n    - Instruction consistency: Can the RM assign a higher score to the correct instruction given the response. \n  - The benchmarking is automatically constructed using existing open source human preference datasets. A sentence embedding model SimSE is used to find pairs of instructions that are lexically similar, but semantically different. \n  - This strategy has been explored with 4 popular open source human preference datasets. \n  - They find a huge performance gap on this benchmark between human judgements and reward models trained with the 7B LLaMa checkpoint\n- How to reduce the gap and improve consistency of reward models? : Two techniques ConvexDA and RewardFusion are introduced that can be incorporated into the training and inference stage of reward models at no additional computation cost. The authors show this helps improve consistency. \n  - ConvexDA: At training time, the data is augmented by substituting words in the responses with synonyms generated using WordNet \n  - RewardFusion:  At inference time, a weighted average reward score between similar training examples and given instruction response pair is used\n- How does reward inconsistency influence downstream performance of chatbots after RLHF? : The authors show that using a more consistent RM for RLHF can lead to more preferable downstream generations."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper proposes a new way of evaluating reward models and focuses on the new important aspect of consistency of reward models. It also highlights how existing reward model evaluation methods do not capture this aspect. \n- The benchmark creation for evaluation is intuitive and automatic and can be easily applied to any existing instruction tuning dataset \n- The paper highlights the importance of using more consistent reward models for RLHF \n- The paper also propose simple methods that show slight improvements in performance on the proposed evaluation metric"
                },
                "weaknesses": {
                    "value": "- Contrast instructions benchmark: To sample lexically similar but semantically different pairs of instructions the authors only sample instruction pairs that lie within the similar range of 0.75 and 0.9. It would be useful to explain the sensitivity of this hyperparameter and study how well this prevents sampling semantically similar instructions. \n- When evaluating existing RM on this benchmark the reward models get a low C_res score of 53.6, even though C_res conceptually resembles the RM learning objective. This is surprising given the benchmark was constructed using the same datasets used for training. It would be useful to verify if appropriate hyper-parameter tuning was performed and if the model is able to overfit and get a high C_res score. \n- Limited models and scales: It would be useful to explore more pretrained models and perform a model and dataset scaling analysis to measure if and how the consistency of reward models varies across different models and scales and if similar observations are found. Would be useful to do a similar analysis when evaluating the impact of the proposed improvement methods ConvexDA and RewardFusion to see how well they perform with different models. \n- It would be useful to explain how the proposed data augmentation and inference time technique are designed to help with improving the performance in contrast instructions benchmark. This is especially important given the very small improvement on the Contrast Instructions benchmark with the proposed approaches. \n- The two methods ConvexDA and RewardFusion that are proposed are not used to build more consistent reward models that are used during the RLHF stage. Instead fine tuning on contrast instructions format is used. However, in appendix C, it is mentioned the \u201cmore consistent\u201d reward model is equipped with ConvexDA. \n  - It would be useful to evaluate models that have been trained (during RLHF) using reward models that are equipped with the proposed strategies (ConvexDA and RewardFusion) to show the improvements brought by this approach\n  - Please clarify what \u201cfinetuning with contrast instructions format\u201d consists of \n  - Please clarify if ConvexDA was used to train the reward models for this stage as mentioned in the appendix."
                },
                "questions": {
                    "value": "- Could you please explain how the proposed approaches are designed to help address the inconsistency in the reward model?\n- Could you explain the reason behind the very low C_res score of the trained reward model on the same dataset, even though C_res resembles the RM training objective?\n- Could you explain what finetuning on contrast instruction format dataset involves?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8541/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8541/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8541/Reviewer_5JFN"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8541/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699610050779,
            "cdate": 1699610050779,
            "tmdate": 1699637067842,
            "mdate": 1699637067842,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TkDPeU4rpr",
                "forum": "MeHmwCDifc",
                "replyto": "WREReQ7S8n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8541/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8541/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful comments and questions! We\u2019ve made a revision to the draft and updated results that could address some of your comments.\n\n**Q1**: The authors only sample instruction pairs that lie within the similar range of 0.75 and 0.9. It would be useful to explain the sensitivity of this hyperparameter and study how well this prevents sampling semantically similar instructions.\n\n**A1**: We hand-crafted our cosine similarity range for the following reasons:\n- Upper Bound: We set the upper limit (e.g., 0.9) to avoid retrieving identical instructions. This is crucial to ensure  that $I_{A} \\circ r_{A}$ is better than $I_{B} \\circ r_{A}$.\n\n- Lower Bound: We avoided setting the lower bound too low (e.g., 0.5) to maintain the challenge for the RM. A lower range would make it too easy for the RM to judge, failing to test whether RM has a nuanced understanding of human preferences.\n\n- Range Interval: We didn\u2019t opt for a narrow range (e.g., [0.85, 0.9]) as it would greatly limit the size of our Contrast Set.\nTo satisfy all of three factors, we can only handcraft a proper range.\n\nIn Appendix G, we include a human analysis on each of the dataset, to see how well humans can do in terms of $C_{res}$ and $C_{ins}$. On average over 4 datasets , we see that humans rank responses correctly ~82% of the time. If we allow for Google search to access relevant knowledge, the accuracy goes up to 96%. We think the gap between RM vs. human performance helps justify our observations on RM inconsistency, as well as contrast instructions as a benchmarking strategy.  \n\n**Q2**: When evaluating existing RM on this benchmark the reward models get a low C_res score of 53.6, even though C_res conceptually resembles the RM learning objective. This is surprising given the benchmark was constructed using the same datasets used for training. It would be useful to verify if appropriate hyper-parameter tuning was performed and if the model is able to overfit and get a high C_res score.\n\n**A2**: We want to clarify that \u2013 $C_{res}$ only **conceptually** resembles the RM training objective, as in, both involve distinguishing between two responses given an instruction. In practice, however, RM training data is usually constructed using \u201cproxies\u201d of human preference labels. For example, within the open-source community (e.g. w/ Llama), examples from StackExchange make up for the majority of the RM training data.  StackExchange follows a specific \u201cproxy\u201d way of defining human preferences, e.g. within the same question, each human preferences example is constructed from responses to the same question/thread, and responses with the highest votes are considered the preferred responses. What we intend to show with our $C_{res}$ metric is that RMs trained this way only fits the training distribution, and don\u2019t generalize to what we think RM should be doing \u2013 i.e. being able to distinguish between good/bad responses consistently. With contrast instructions, we observe that RMs fail to do so even when the questions come from the domain (e.g. StackExchange) as seen in training.    \n\nTo your concern about our implementation \u2013 In our experiments, we actually tested on an **off-the-shelf** open-source RM trained on StackExchange data\u2013 Stack-LLaMa from the Hugging Face RLHF team (https://huggingface.co/trl-lib/llama-7b-se-rm-peft). The results are shown in Table 3. We see close to random performance from the off-the-shelf RM under $C_{res}$ as well.\n\n**Q3**: Limited models and scales\n\n**A3**: Thanks for your suggestion \u2013 In Appendix H, we've added experiments with models of different sizes. We observe that scaling model size leads to (1) increased test performance on the original human preference dataset, but (2) the $C_{res}$ and $C_{ins}$ performance remains flat. This echoes our hypothesis that \u2013 standard RM training fits only the training distribution, but does not lead to a generalized model that can distinguish good/bad responses consistently. \n\n**Q4**: The two methods ConvexDA and RewardFusion that are proposed are not used to build more consistent reward models that are used during the RLHF stage. Instead fine tuning on contrast instructions format is used. However, in appendix C, it is mentioned the \u201cmore consistent\u201d reward model is equipped with ConvexDA.\n\n**A4**: We apologize for a typo in Appendix C related to our experimental setup. In our research, we fine-tuned the consistent RM with our contrast set without ConvexDA. Using ConvexDA and RewardFusion to improve the RM for RLHF didn't significantly improve the chatbot's performance due to their limited improvements in enhancing RM consistency. Since we focus on the trickle-down effect of RM consistency, we select the most effective method (training on Contrast Instruction) in RLHF to better observe such effects."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700366300792,
                "cdate": 1700366300792,
                "tmdate": 1700366300792,
                "mdate": 1700366300792,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qgM375EwhW",
                "forum": "MeHmwCDifc",
                "replyto": "WREReQ7S8n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8541/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8541/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q5**: Please clarify what \u201cfinetuning with contrast instructions format\u201d consists of.\n\n**A5**: The Contrast Instruction format is identical to the original benchmark, where each pair has an \"Instruction\" and \"response\" joined together. We've corrected this writing to clear up any potential confusion.\n\nIf you have any other questions, please feel free to ask."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8541/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700366347499,
                "cdate": 1700366347499,
                "tmdate": 1700366789352,
                "mdate": 1700366789352,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]