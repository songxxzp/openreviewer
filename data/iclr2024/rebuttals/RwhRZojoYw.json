[
    {
        "title": "On information dropping and oversmoothing in graph neural networks"
    },
    {
        "review": {
            "id": "b9JDusT2ik",
            "forum": "RwhRZojoYw",
            "replyto": "RwhRZojoYw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8797/Reviewer_yTML"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8797/Reviewer_yTML"
            ],
            "content": {
                "summary": {
                    "value": "The paper delves into a crucial issue within Graph Machine Learning, namely, the challenge of excessive smoothing in graph data. The authors aim to tackle this problem by applying the information bottleneck principle and demonstrate its effectiveness through the introduction of a new metric they've coined, the \"normalized Dirichlet energy.\" In their research, the authors illustrate the advantages of their method, as it succeeds in reducing the Dirichlet energy while also yielding some improvements in classification accuracy."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper outlines a novel application of the information bottleneck principle for mitigating oversmoothing in graph data. Additionally, the paper endeavors to demonstrate that there is no discernible correlation between oversmoothing and test performance, as suggested by their research findings."
                },
                "weaknesses": {
                    "value": "Please refer to questions."
                },
                "questions": {
                    "value": "I encountered challenges in comprehending certain sections of the paper, particularly sections 4.1 and 4.2. I believe the clarity of these sections could be significantly improved with revisions by the authors. Additionally, I have a few follow-up questions:\n\n1. Although the authors correctly emphasize the absence of a strong correlation between oversmoothing and test performance, it is essential to explore the specific conditions under which this correlation holds. This inquiry is vital because scenarios exist where the Normalized Dirichlet Energy may approach zero, yet the test accuracy remains high. For instance, consider a stochastic block matrix with two classes, where all nodes within one class map into a single representation, and the same occurs for the other class (to a different single representation. In this case, the Normalized Dirichlet Energy would be small, but test accuracy would be high. This raises questions about the circumstances in which oversmoothing genuinely matters and whether the metric employed is appropriate.\n\n2. Building upon the first point, the connection between oversmoothing and generalization performance appears to be meaningful primarily when oversmoothing occurs across different classes. Oversmoothing within the same class might not be as relevant. Therefore, it would be valuable for the authors to provide commentary, theory, and experimental evidence in this context, while also demonstrating the efficacy of their method in comparison to other approaches.\n\n3. My concerns extend to the topic of reproducibility. I was unable to locate any provided code or information regarding hyperparameter ranges. Inclusion of this information is crucial for transparency and for ensuring that other researchers can replicate the experiments.\n\nIn conclusion, the authors do illustrate that their method reduces the Normalized Dirichlet Energy. However, the actual benefits of their method for node classification tasks remain unclear, as well as its performance when applied to more complex graph architectures. Furthermore, it's uncertain whether their method is effective in the context of homophilic or heterophilic graphs. Therefore, based on these questions and concerns, I am inclined to recommend rejecting the paper. However, I am open to reconsidering this recommendation if the authors provide satisfactory responses and address these issues adequately."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8797/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8797/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8797/Reviewer_yTML"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8797/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697826169718,
            "cdate": 1697826169718,
            "tmdate": 1699637105681,
            "mdate": 1699637105681,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZhaONXheQm",
                "forum": "RwhRZojoYw",
                "replyto": "b9JDusT2ik",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8797/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8797/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer yTML"
                    },
                    "comment": {
                        "value": "Dear Reviewer yTML\n\nThank you for your response and important feedback! We agree with you and realize that the original draft of our paper had major clarity issues, especially in sections 4.1 and 4.2, and the overall motivations and conclusions of the paper were not clearly explained. **We have revised large portions of the paper to address the clarity issues**.\n\n> I encountered challenges in comprehending certain sections of the paper, particularly sections 4.1 and 4.2. I believe the clarity of these sections could be significantly improved\n\nWe have made Section 4 clearer to read \u2013 in particular we have corrected notation that was inconsistent and convoluted (especially regarding the optimized representation / message matrix). The derivation there is standard in the literature and is not the main contribution of our work. We have made clearer references to the foundational works on the IB principle in neural networks. We hope this section is now more understandable.\n\nThe introduction, literature review and conclusion sections have been revised to make it clearer what the paper\u2019s objectives and takeaways are. To clarify, we focus on investigating the effect of random dropping approaches on oversmoothing, and the degree to which oversmoothing is relevant to improving model performance in practice. Our proposal of Learn2Drop mainly serves to address the problems of using existing random dropping approaches at test time.\n\n> scenarios exist where the Normalized Dirichlet Energy may approach zero, yet the test accuracy remains high.\n\nYour concerns on the use of Normalized Dirichlet energy (NDE) are valid and it is true that we did not motivate its use properly. Reviewer K9Rv had similar concerns.  **As such we have completely removed the use of normalized Dirichlet energy from the paper and have redone the experiments using both standard Dirichlet energy and MAD**. We added a discussion on the benefits and pitfalls of using either metric in Section 3.1. The empirical results after switching to standard Dirichlet energy and MAD are very similar to our original results, and our conclusions remain the same. This suggests that the theoretical differences between each metric are not significant in practice, at least for our investigation. \n\n > Oversmoothing within the same class might not be as relevant\n\nThis is an interesting take and suggestion. The existing metrics of Dirichlet energy and MAD look indiscriminately at all node pairs (which are connected by an edge). It would be interesting to observe oversmoothing for only the pairs of nodes that are of different classes, since differentiating between nodes is what matters for classification accuracy. We did some additional experiments (for section 3.2) where we compute Dirichlet energy and MAD while only considering these pairs of nodes in different classes. The plots for those are placed in the Appendix. The overall results are highly similar to when comparing all pairs, although the absolute value of the smoothing differs. However, we are mostly concerned with the trend of this metric, rather than its absolute value. \n\n> My concerns extend to the topic of reproducibility.\n\nWe have tried to improve the reproducibility of our work by including model training and parameter selection details in a new experimental setup section in the Appendix. \n\nWe hope our revisions to the paper have addressed your concerns and would be grateful for any further comments or feedback.\n\nThank you,\n\nThe authors."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241944940,
                "cdate": 1700241944940,
                "tmdate": 1700241944940,
                "mdate": 1700241944940,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vQ3Y09Ptp5",
                "forum": "RwhRZojoYw",
                "replyto": "ZhaONXheQm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8797/Reviewer_yTML"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8797/Reviewer_yTML"
                ],
                "content": {
                    "comment": {
                        "value": "I've reviewed both the paper and its revised version, and I've identified a few comments and questions.\n\n1. Upon examining the paper, it is evident that there have been substantial changes, transitioning from presenting a novel method to focusing on benchmarking and understanding oversmoothing. Given this shift, wouldn't it be prudent to take a step back and refine the conceptual framework and  resubmit to the next conference? Considering the considerable alteration in messaging, this approach might be more judicious.\n\n2. I find myself somewhat uncertain about grasping the paper's contribution fully. Comparing it to the paper: GraphCon by Rusch (2022), which had some theoretical contributions (albeit not highly rigorous), and noting that the current version also makes similar contributions of a somewhat theoretical nature, I'm still grappling with a comprehensive understanding. At a broad level, it seems that the paper aims to raise inquiries about scenarios where reducing oversmoothing may not necessarily lead to performance improvements. However, this conclusion appears somewhat obvious, especially when considering the illustrative example I presented in my previous comment. Consequently, I remain unclear about how or why this paper would bring tangible benefits to the broader community.\n\nIt would help if the authors could respond to the above two comments."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700388801804,
                "cdate": 1700388801804,
                "tmdate": 1700388801804,
                "mdate": 1700388801804,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qItnuFNzT5",
                "forum": "RwhRZojoYw",
                "replyto": "b9JDusT2ik",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8797/Reviewer_yTML"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8797/Reviewer_yTML"
                ],
                "content": {
                    "comment": {
                        "value": "I've reviewed the responses from both other reviewers and your comments. I recognize the potential in this work, but it's still in the early stages, with the paper's narrative having shifted, raising additional unanswered questions. It's important to note that reducing oversmoothing doesn't always directly lead to performance enhancements, a point known in the community.\n\nTo strengthen the paper, consider these suggestions:\n\n1. Identify instances where simple methods like dropedge contribute to performance improvement, regardless of oversmoothing considerations.\n2. Examine cases where such simple methods prove ineffective, outlining reasons for their failure or proposing alternative approaches.\n3. Conclude the paper by exploring when reducing oversmoothing correlates with performance enhancements and when it \n does not.\n\nFocusing on these aspects could enhance the paper's impact. I would encourage the authors to pursue some of these directions and submit to the next conference, considering the fact that the rebuttal process is coming to a close, soon."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642502866,
                "cdate": 1700642502866,
                "tmdate": 1700642518794,
                "mdate": 1700642518794,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "scL1BmLJzt",
            "forum": "RwhRZojoYw",
            "replyto": "RwhRZojoYw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8797/Reviewer_K9Rv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8797/Reviewer_K9Rv"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates the effects of random dropping approaches for addressing the problem of oversmoothing in GNNs. The paper empirically found that methods like DropEdge and DropMessage which applied randomly and exclusively during training, has limited effect in reducing oversmoothing at test time. Then the paper proposes learning to drop (Learn2Drop) which learns which elements to drop and shows that Learn2Drop is able to successfully mitigate oversmoothing at test time and has better performance than DropEdge and DropMessage."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tOverall the paper is clearly written and easy to follow. \n2.\tInvestigating the effects of current methods on oversmoothing under both metrics for oversmoothing and model performance is important in order to understand them better and develop more powerful GNNs.\n3. Dropping information in an informed way is well-motivated.\n4.\tIf we take NDE as a valid metric to interpret oversmoothing, it is an interesting observation that random dropout methods such as DropEdge and DropMessage applied exclusively during training has little effect on oversmoothing at test time. However, see weakness below why the NDE metric can be not well-defined and not interpretable."
                },
                "weaknesses": {
                    "value": "1. Wrong reference. To the best of my knowledge, Rusch et. al (2022) is not the original paper which first proposes the use of Dirichlet energy to analyze oversmoothing in GNNs. The first paper is Cai and Wang (2020). Please correct the citations in Section 3.1.\n\n2. Missing state-of-the-art literature on oversmoothing. Recently there has been substantial progress made in terms of theoretical understanding of oversmoothing in GNNs such as Keriven (2022) and Wu et al. (2023), where these works rigorously show that there is a \u201csweet spot\u201d between smoothing and oversmoothing in order for GNNs to perform well for node classification tasks. It is thus not appropriate to claim that \"an important takeaway from our work is that the true causes and consequences of oversmoothing are vaguely understood, especially its relationship with overfitting and generalization.\" Please improve literature review.\n\n3.  No explicit mathematical form for the normalized Dirichlet energy (NDE) and NDE is not a good metric to measure oversmoothing due to at least the three following issues:\n\n- It does not satisfy the criterion proposed for node similarity measure in Rusch et al. (2023), which the authors has relied on as a gold standard for measuring oversmoothing. NDE does not satisfy the criterion proposed in Rusch et al. (2023) because NDE = 0 does not imply all nodes having the same representation vector. It is thus invalid to conclude from NDE that \"oversmoothing is still occurring according to the formal definition introduced by Rusch et al. (2023)\" from NDE.\n\n- It is obscure to interpret. The reasoning in the paragraph above section 3.2 is entirely heuristic. \n\n- It is not well-defined for 1-d features (all 1-d features have NDE = 0) and cannot be grounded in practice, as it disconnects oversmoothing from model performance.  Consider the following example: suppose that one wants to do a classification task for two group of nodes, where we have one-dimensional, linearly separable features: group 1 all have representation -1, group 2 all have representation 1. Now NDE is 0, which indicates \"oversmoothing\" according to the paper, but classification result is perfect.\n\nDue to the above reasons, the observation and interpretation drawn upon oversmoothing based on NDE in this paper is not valid.\n\n4. Insufficient empirical evaluation. \n- DropEdge and DropMessage can also easily be applied during test time. Although the authors argue that random dropping methods could introduce too much noise during test, the paper does not provide empirical evidence. Please add experiments supporting the claim that using them at test time really reduces model performance.\n- In Figure 1, it seems a GNN with skip connections are perfect remedies for oversmoothing, but it is not discussed in the paper and not included as a baseline method in Fig 3. and 4. Please add it as a baseline and discuss. \n\n----\nReferences\n\nRusch et. al (2022). Graph-coupled oscillator networks. ICLR 2022.\n \nCai and Wang (2020). A note on over-smoothing for graph neural networks. ICML GRL+ workshop.\n\nKeriven (2022). Not too little, not too much: a theoretical analysis of graph (over)smoothing. NeurIPS 2022.\n\nWu et al. (2023). A non-asymptotic analysis of oversmoothing in graph neural networks. ICLR 2023.\n\nRusch et. al (2023). A survey on oversmoothing in graph neural networks. Arxiv."
                },
                "questions": {
                    "value": "1. In Figure 1, it seems that skip connections are perfect remedies for oversmoothing. Why not just go with them?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8797/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8797/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8797/Reviewer_K9Rv"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8797/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698422417942,
            "cdate": 1698422417942,
            "tmdate": 1699637105565,
            "mdate": 1699637105565,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HPAVgQuRi9",
                "forum": "RwhRZojoYw",
                "replyto": "scL1BmLJzt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8797/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8797/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer K9Rv"
                    },
                    "comment": {
                        "value": "Dear Reviewer K9Rv,\n\nWe thank you for your response and important feedback. We are glad you have pointed out the issues and inconsistencies you found. We have made many changes to the paper in response to your feedback.\n\n> Please improve literature review.\n\nWe have revised our literature review of oversmoothing. This has been placed in Section 2.2. Thank you for mentioning the paper by Keriven (2022). Indeed there has been recent work that has tried to analyze the relationship between oversmoothing and generalization from a theoretical perspective. Our work is a contribution from an empirical perspective. One of our conclusions is that oversmoothing is not strictly related to model performance in practice, and minimizing oversmoothing by itself can be undesirable. For example, applying random dropping on the message matrix at test time will reduce oversmoothing according to both MAD and Dirichlet energy \u2013 however this *does not result in a useful model*. We believe that this challenges the prior \u2018consensus\u2019 in the literature that oversmoothing reduction is desirable, giving new insights from an empirical perspective, and also aligns with the theory work by Kerivan (2022) that the truth is simply \u2018reducing oversmoothing is always better\u2019.\n\n> NDE is not a good metric to measure oversmoothing due to at least the three following issues:\n\nWe have **removed the normalized Dirichlet energy** (NDE) from the paper, and have **redone our experiments using both standard Dirichlet energy (DE) and mean average distance (MAD)**. \n* In Section 3.1 we have added a clearer explanation of the advantages and drawbacks of each metric. The figures in our paper have also been updated to reflect this. The empirical results on oversmoothing analysis are actually very similar between DE, MAD (and NDE), and the conclusions of our paper remain the same. \n* We note that while the issues faced by MAD (and NDE) are problematic for a theoretical analysis, for our experiments they don\u2019t seem to create a noticeable difference in practice. Our main use case of these metrics is to compare the relative amount of oversmoothing between different GNNs, which is possible with MAD as it exhibits layerwise exponential convergence for high dimensional node representations. While DE has the theoretically sound property that complete smoothing is equivalent to 0, in practice this does not make a significant difference to our empirical observations.\n\n> Although the authors argue that random dropping methods could introduce too much noise during test, the paper does not provide empirical evidence\n\n* We have now included them in Table 1 the results for using DropEdge and DropMessage at test time \u2013 we apologize for forgetting to include them in the original submission. We have also taken reviewer yTML\u2019s advice to average the inference results over multiple passes to make this baseline stronger.\n\n> seems a GNN with skip connections are perfect remedies for oversmoothing\n\nGNN+skip connection are now in Figures 1, 3 and 4. It certainly does appear like the \u2018perfect remedy\u2019 for oversmoothing like you\u2019ve pointed out \u2013 according to both Dirichlet energy and MAD in fact. This is also the case with enabling DropMessage at test time. What this suggests is that while certain methods that perform well are correlated with having lower oversmoothing, it is questionable whether we can justify one model as being better than another by solely using the oversmoothing reduction as our deciding factor. In the case of simply applying DropMessage at test time, this reduces oversmoothing while being detrimental to model performance. In the recent survey by Rush et al (2023), we can see similar results, in which methods such as PairNorm are highly effective in limiting oversmoothing but are outperformed by many other methods. Rusch et al interpret this as the result of some models sacrificing expressive power to reduce oversmoothing. However, we propose the new perspective that a model which *improves performance* may not be strictly related to oversmoothing at all \u2013 for instance, DropEdge and DropMessage mainly improve performance by providing a data augmentation effect that benefits training. \n\nFinally, the overall objective of our work wasn\u2019t clearly explained in the original submission. Our main contribution is investigating the effect of random dropping approaches on smoothing, rather than trying to propose a superior method of solving oversmoothing. We\u2019ve revised the abstract, the introduction and conclusion to address this. \n\nThank you for taking the time to provide us with valuable feedback \u2013 we hope you can take a look at our revisions and we would be grateful to hear further comments on areas that can be improved.\n\nThank you,\n\nThe authors."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241895828,
                "cdate": 1700241895828,
                "tmdate": 1700241895828,
                "mdate": 1700241895828,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "j3FNoAW0oO",
                "forum": "RwhRZojoYw",
                "replyto": "scL1BmLJzt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8797/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8797/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Request for Feedback Before Rebuttal Period Closure"
                    },
                    "comment": {
                        "value": "Dear Reviewer K9Rv,\n\nWe would like to kindly request your feedback on the revisions made to our paper -- we have thoroughly addressed the concerns raised in your initial review and made significant changes to the paper.\n\nWe would greatly appreciate any additional comments or suggestions you may have before the end of the rebuttal period. We understand the time constraints and appreciate your consideration in providing timely feedback.\n\nThank you for your valuable contribution to improving our work.\n\nSincerely,\n\nThe authors"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700589492351,
                "cdate": 1700589492351,
                "tmdate": 1700589492351,
                "mdate": 1700589492351,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "m6Fm6xlmoL",
                "forum": "RwhRZojoYw",
                "replyto": "HPAVgQuRi9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8797/Reviewer_K9Rv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8797/Reviewer_K9Rv"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up with the authors"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThank you for your response. However, the observation that pure reduction of oversmoothing might sacrifice model performance is not unknown to the community. Despite having done a better literature review during the rebuttal, I feel the paper still needs to go through another round of reframing and reviews to make the contribution more significant. \n\nRegarding the problem of residual connections, Wu et al. (2023) (citation provided above) actually have a theoretical analysis on the tradeoff between the reduction of oversmoothing and the reduction in optimal performance and insights about why that is the case. The analysis is much more concrete than \"the result of some models sacrificing expressive power to reduce oversmoothing.\" The authors might find that to be of interest."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590925165,
                "cdate": 1700590925165,
                "tmdate": 1700590925165,
                "mdate": 1700590925165,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "O3qwcE68GJ",
            "forum": "RwhRZojoYw",
            "replyto": "RwhRZojoYw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8797/Reviewer_xrXy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8797/Reviewer_xrXy"
            ],
            "content": {
                "summary": {
                    "value": "This paper empirically evaluated the influence of DropEdge and DropMessage on the oversmoothing effect of graph neural networks. The findings reveal that random dropping methods are insufficient in mitigating oversmoothing. The authors then propose a non-random dropping approach that learns which element to drop. This method can be used in both training and testing. Empirically, the proposed method alleviates oversoothing and improves performance accuracy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The oversmoothing experiments in Section 3 are very thorough. I particularly appreciate section 3.2, where the authors investigate the importance of randomness to DropEdge (Figure 2).\n- The proposed information bottleneck approach (Section 4.1) seems to be principled and effective."
                },
                "weaknesses": {
                    "value": "The paper relies on a normalized variant of Dirichlet energy to measure oversmoothing. As the authors themselves pointed out in the paper, however, this metric does not correlate with performance accuracy well. On the one hand, I understand that there is no universally agreed metric for evaluating oversmoothing, and I agree that the normalized Dirichlet energy metric used by the authors is a sensible one. On the other hand, I believe that whether a smoothing effect qualifies as an \"oversmoothing\" depends on whether the node features give rise to bad final performance (in an extreme case, if constant node features yield the best results, then it is debatable about whether these constant node features are \"oversmoothed\" or \"appropriately smoothed.\") Hence, to enhance our understanding of how dropping interacts with oversmoothing effects, I recommend the authors to either: \n\n(i) Plot the normalized Dirichlet energy (x-axis) vs. accuracy (y-axis) frontier for models. This could be informative as the normalized Dirichlet energy alone may not fully reveal oversmoothing in node features.\n\n(ii) Use perhaps more than one metric to evaluate oversmoothing. The Dirichlet energy is one such metric, but other metrics exist. For example, another way of evaluating oversmoothing is to use the influence scores of nodes (Xu et al. 2018), among many other ways.\n\n[1] Representation Learning on Graphs with Jumping Knowledge Networks. Keyulu Xu et al. 2018. ICML."
                },
                "questions": {
                    "value": "Does it make sense to compare test-time DropMessage (where we average different outcomes to reduce variance) with Learn2Drop? As the authors pointed out, while DropMessage can stabilize the Dirichlet energy, applying it at test time can introduce high variance in prediction. However, the variance can be reduced with multiple forward passes, each with a different realization of DropMessage. This will introduce some compute overhead, but seems to be a sensible baseline for Learn2Drop to compare."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8797/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698592683276,
            "cdate": 1698592683276,
            "tmdate": 1699637105444,
            "mdate": 1699637105444,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "j4MeSbnqMI",
                "forum": "RwhRZojoYw",
                "replyto": "O3qwcE68GJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8797/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8797/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xrXy"
                    },
                    "comment": {
                        "value": "Dear Reviewer xrXy,\n\nThank you for taking the time to provide us with feedback and suggestions. We agree with your comments  \u2013 oversmoothing is a challenging phenomenon to evaluate. Its relationship with model performance is not as straightforward as one might think \u2013 as suggested by our findings. There are many instances where doing simple yet unhelpful things \u2013 such as dropping messages at inference time \u2013 will apparently solve oversmoothing but fail to improve actual performance. There are also other cases such as with DropEdge, where it may be easy to believe that performance improvement comes from oversmoothing reduction, but in reality may come from elsewhere. \n\n> Use perhaps more than one metric to evaluate oversmoothing. \n\nWe have taken your suggestion of using two different metrics of evaluating oversmoothing \u2013 we removed the use of normalized Dirichlet energy due to many concerns pointed out by the other reviewers, and now all our experiments are conducted using both the standard Dirichlet energy, as well as MAD. While Dirichlet energy has some good theoretical properties, MAD is also commonly used in the literature and less sensitive to arbitrary scaling of the embeddings. Both metrics lead to similar conclusions \u2013 in particular, the analysis of the layer-wise exponential smoothing in section 5.1 is almost identical between Dirichlet energy and MAD. The similarity in the conclusions drawn from both metrics reinforces the soundness of our approach.\n\n> However, the variance can be reduced with multiple forward passes, each with a different realization of DropMessage.\n\nThis is a useful suggestion and would serve as a much better baseline than simply applying random dropping at test time. We have modified the table of accuracies in the results section so that the results for DropEdge and DropMessage at test time are done by averaging the model prediction over 10 inference passes. We do witness a small reduction in variance, but the overall results are still poor. Effectively we are averaging over more inference passes and the mean test accuracies are still low. We believe that this provides a more robust comparison and strengthens the validity of our findings.\n\nWe highly appreciate your insightful feedback and suggestions \u2013 we will be grateful for any further comments. \n\nThank you,\n\nThe authors."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241798651,
                "cdate": 1700241798651,
                "tmdate": 1700241798651,
                "mdate": 1700241798651,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Xdq48CMzH1",
                "forum": "RwhRZojoYw",
                "replyto": "O3qwcE68GJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8797/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8797/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer xrXy,\n\nWe would like to kindly request your feedback on the revisions made to our paper -- we have thoroughly addressed the concerns raised in your initial review and made significant changes to the paper.\n\nWe would greatly appreciate any additional comments or suggestions you may have before the end of the rebuttal period. We understand the time constraints and appreciate your consideration in providing timely feedback.\n\nThank you for your valuable contribution to improving our work.\n\nSincerely,\n\nThe authors"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700589457214,
                "cdate": 1700589457214,
                "tmdate": 1700589457214,
                "mdate": 1700589457214,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "d03c4mP72v",
            "forum": "RwhRZojoYw",
            "replyto": "RwhRZojoYw",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8797/Reviewer_nv43"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8797/Reviewer_nv43"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors empirically show that existing DropEdge and DropMessage operations have limitations, and propose Learn2Drop to mitigate the oversmoothing issue. Specifically, they propose to optimize a mutual information objective using the information bottleneck principle, and conduct experiments on several datasets to evaluate the effectiveness of Learn2Drop."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper studies the oversmoothing issue in GNNs, which is a key issue when applying GNNs.\n- Both analysis and emprical results are provided to show the limitations of two existing works.\n- Open-sourced code helps improve the reproducibility.\n- Empirical experiments are conducted to show the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "- One major concern is from the evaluation part. First, in addition to DropEdge and DropMessage, there are other methods that aim to address the oversmoothing issue. The authors may consider to compare with them. Although the authors mention that the aim of this paper is to isolate and understand the specific impacts of different techniques, it is still encouraged to show to what extent can the proposed method solve the oversmoothing issue compared with the recent, more complex methods. Second, more interpretations related to Table 1 can be provided.\n\n- The authors claim that DropMessage at testing time may introduce a high amount of variance in the model predictions. They may provide more evidence (e.g., some experimental results) on this.\n\n- The writings of the paper can also be improved."
                },
                "questions": {
                    "value": "- Why use Dirichlet energy instead of mean average distance?\n- What does DO mean in Table 1?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8797/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8797/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8797/Reviewer_nv43"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8797/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698759835355,
            "cdate": 1698759835355,
            "tmdate": 1699637105272,
            "mdate": 1699637105272,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QXPmhAeRhb",
                "forum": "RwhRZojoYw",
                "replyto": "d03c4mP72v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8797/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8797/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nv43"
                    },
                    "comment": {
                        "value": "Dear Reviewer nv43,\n\nWe thank you for your response and important feedback! You highlighted an important point - on the comparison of our Learn2Drop with the latest existing approaches for addressing oversmoothing. We would first like to mention that in the original paper, the main motivation of our work wasn\u2019t clearly addressed and it was misleading. We admittedly overemphasized the significance of \u2018reducing oversmoothing\u2019 and would like to highlight that the main novelty of our work is an empirical study and critical look of whether oversmoothing is actually relevant in practice for dropping methods, rather than finding the best way to minimize it. \n\n> The writings of the paper can also be improved.\n\nWe have made **significant changes to many sections of the paper** to make its meaning clearer. In particular, the introduction and conclusion sections have been revised to clarify our contributions**. In summary,\n \n* Our primary contribution is analyzing whether random dropping approaches have an effect on oversmoothing, which we show to be not necessarily true since the methods are applied only during training. This suggests that DropEdge for instance, is fundamentally more of a data augmentation tool.\n* We observe that simply dropping random message elements at test time will result in significant oversmoothing reduction, but translates to poor test accuracy. This suggests that minimizing oversmoothing is not necessarily useful. \n* Learn2Drop was motivated by the question of whether we can perform this random dropping at test time in a more informed manner. We don\u2019t aim to beat existing oversmoothing reduction approaches, but rather focus on bringing the potential benefits of existing random dropping methods to test time. Although we observe that Learn2Drop achieves oversmoothing reduction, **this doesn\u2019t prove that reducing oversmoothing is strictly the reason why it performs better**. It is possible that Learn2Drop\u2019s oversmoothing reduction is just a side effect and not the main reason it performs better. This is what we believe is happening for DropEdge. Learn2Drop serves more as a way to give us insight into the relationship between oversmoothing and model performance, as opposed to solely focusing on reducing oversmoothing.\n\n> First, in addition to DropEdge and DropMessage, there are other methods that aim to address the oversmoothing issue. The authors may consider to compare with them. \n\nFollowing your advice, to make our results more informed, we have added the recent method GraphCon proposed by Rusch (2022) which we treat as a \u2018complex\u2019 method of addressing oversmoothing. This is added into Table 1. Although both our approach and GraphCon can effectively prevent the layer-wise exponential behavior of smoothing, there is a still a performance gap between \u2018optimally dropping at test time using Learn2Drop\u2019 and GraphCon, which further suggests that it is important to see that oversmoothing is perhaps not the only factor when it comes to making a GNN more accurate. However, an important point is that creating a method that outperforms state of the art methods for reducing oversmoothing is not the purpose / goal of this work, and we don\u2019t expect our approach to result in better accuracy than those methods.\n\n> The authors claim that DropMessage at testing time may introduce a high amount of variance in the model predictions.\n\nWe apologize for not having included these results in the original paper \u2013 we have also added results into Table 1 showing the performance of models when applying DropEdge and DropMessage at test time. These results are obtained by averaging multiple inference passes to make it a stronger baseline, as suggested by reviewer yTML.. The results are poor and inconsistent, which is likely why the methods were originally proposed to only be used during training. \n\n> Why use Dirichlet energy instead of mean average distance?\n\nThe main reason for using Dirichlet energy is that it holds the convenient theoretical property that 0 energy is equivalent to complete oversmoothing (all node representations being identical). However, for our empirical comparisons this property does not make a noticeable difference compared to just using MAD. As other reviewers had concerns about the use of normalized Dirichlet energy, we have removed its use from the paper. Our results were rerun while using standard Dirichlet energy and MAD.\n\n>What does DO mean in Table 1?\n\nThat refers to the Dropout method. We have edited that column of the table to make the names clearer.\n\nWe hope our revisions to the paper have addressed your concerns and look forward to hearing your thoughts on them.\n\nThank you,\n\nThe authors."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700241628929,
                "cdate": 1700241628929,
                "tmdate": 1700241628929,
                "mdate": 1700241628929,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hc7cdp5HSn",
                "forum": "RwhRZojoYw",
                "replyto": "d03c4mP72v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8797/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8797/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Request for Feedback Before Rebuttal Period Closure"
                    },
                    "comment": {
                        "value": "Dear Reviewer nv43,\n\nWe would like to kindly request your feedback on the revisions made to our paper -- we have thoroughly addressed the concerns raised in your initial review and made significant changes to the paper.\n\nWe would greatly appreciate any additional comments or suggestions you may have before the end of the rebuttal period. We understand the time constraints and appreciate your consideration in providing timely feedback.\n\nThank you for your valuable contribution to improving our work.\n\nSincerely,\n\nThe authors"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700589386773,
                "cdate": 1700589386773,
                "tmdate": 1700589386773,
                "mdate": 1700589386773,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EwbGYk5ILg",
                "forum": "RwhRZojoYw",
                "replyto": "hc7cdp5HSn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8797/Reviewer_nv43"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8797/Reviewer_nv43"
                ],
                "content": {
                    "title": {
                        "value": "Feedback to the authors"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThanks for your response. My questions are partially addressed. Now the paper has been significantly shifted to an empirical study related to the over-smoothing problem. In this sense, the overall contribution is relatively weak and more theoretical analysis is appreciated (especially considering that there are already related analysis out there)."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8797/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634109838,
                "cdate": 1700634109838,
                "tmdate": 1700634109838,
                "mdate": 1700634109838,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]