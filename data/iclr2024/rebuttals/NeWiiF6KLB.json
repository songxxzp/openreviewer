[
    {
        "title": "Stabilized E(n)-Equivariant Graph Neural Networks-assisted Generative Models"
    },
    {
        "review": {
            "id": "zTZAjZvGdW",
            "forum": "NeWiiF6KLB",
            "replyto": "NeWiiF6KLB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8332/Reviewer_Tc5b"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8332/Reviewer_Tc5b"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a regularizer to alleviate the training instability issue of EGNN in generative modeling. It analyzes the gradient of coordinate updates with respect to input and figures out the sensitive term that results in numerical explosions. Experimental results show that the proposed regularizer can successfully stabilize the ENF and EDM training."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(1) This is the first time delving into the numerical instability issue of the ENF and EDM training. This is a very important challenge and fundamental research question that was overlooked in the previous research. This reviewer agrees with the significance of the proposed challenge;\n\n(2) The writing is basically well-organized, containing both theoretical analysis and experimental analysis, which makes the paper technically solid;"
                },
                "weaknesses": {
                    "value": "(1) This work analyzes the gradient update of a single-layer EGNN but this reviewer does not see an analysis of special reasons leading to unstable ENF and EDM training. Only a single sentence states that \u201cIn both ENF and EDM, the graph node coordinates keep changing during the generative process, and abnormal coordinate updates may occur.\u201d However, the generative process is the inference process while this paper is dealing with the training instability. Therefore, the ENF and EDM training instability is not accurately summarized. Is EGNN on the given training data stable? Is ENF and EDM unstable on the given training data? If the answers to both questions are yes, then this might indicate that there are special reasons resulting in the instability of ENF and EDM training. If ENF and EDM training instability is caused by EGNN training only (no special stuff), then the discussions over the ENF and EDM are redundant and not necessary. In general, this reviewer thinks the preliminary discussions over the training instability are not sufficient. \n\n(2) The proposed regularizer is a sum of gradient calculations, which is extremely hard to compute. The summations are computed over all atom pairs (i, j) and EGNN layers L with at least O(N**2*L) time complexity. Hence, although the proposed regularizer is a very effective approach to restricting the gradient norm, the calculation of the proposed regularizer could not be scalable to either larger particle systems or deep EGNN architectures."
                },
                "questions": {
                    "value": "(1) What is the time complexity of the calculation of the proposed regularizer? How could the proposed approach scale to larger particle systems and deep EGNN architectures?\n\n(2) What are the special reasons for numerical instabilities in ENF and EDM training? Or the instability of ENF and EDM is all caused by EGNN?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8332/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8332/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8332/Reviewer_Tc5b"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8332/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698672425194,
            "cdate": 1698672425194,
            "tmdate": 1699637036165,
            "mdate": 1699637036165,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vcPYMjTRqw",
                "forum": "NeWiiF6KLB",
                "replyto": "zTZAjZvGdW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8332/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8332/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Tc5b"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review and valuable feedback. Since the questions the reviewer raised align with the reviewer\u2019s comments on the weaknesses of our paper, we provide point-by-point responses to the reviewer\u2019s questions.\n\n---\n\n\n**Q1. What is the time complexity of the calculation of the proposed regularizer? How could the proposed approach scale to larger particle systems and deep EGNN architectures?**\n\n**Reply:** The graphs considered in the molecular generation tasks are sparse, and therefore the time complexity of the regularization term is $O(NL)$ with $N$ and $L$ being the number of particles and the depth of EGNN, respectively. \n\nAll summations are taken over the neighborhood of a given graph node, we have fixed all inappropriate notations in the revision.\n\nIn our paper, we have also compared the computational time of the EDM with and without regularization, showing the extra computational time is not very significant. The computational overhead due to the regularization term is much smaller than the baseline computational time.\n\n\n---\n\n**Q2. What are the special reasons for numerical instabilities in ENF and EDM training? Or the instability of ENF and EDM is all caused by EGNN?**\n\n**Reply:** The instability issue of ENF and EDM we study originates from the EGNN model in the training phase, and we have pointed this out in the first paragraph of Section 1.2 of our paper. We focus on ENF and EDM for the following two main reasons:\n\n- Though EGNN is a general E(n)-equivariant graph neural network, one of the most remarkable applications of EGNN is for equivariant generative modeling. If we look at the citations of the EGNN paper, a very large portion are related to equivariant generative modeling.\n\n- Besides stabilizing training ENF and EDM, the proposed regularization scheme brings additional important benefits to ENF and EDM. \nFor ENF, because solving a high-dimensional ODE is required in both training and testing, the proposed regularization can also regularize the Lipschitz constant of the ODE to improve the computational efficiency of ENF. For EDM, the proposed regularization allows EGNN to use an unnormalized coordinate update, and such an unnormalized coordinate update can improve the performance of EDM. \n\nIn the revision, we have further improved the discussion of the instability issues of training ENF and EDM in Section 1.2 and Section 3.\n\nFinally, in the concluding remarks section, we have added the discussion that the training instability in ENF and EDM we study comes from the EGNN itself. Studying the efficacy of the proposed regularization for other EGNN applications can be an interesting future work.\n\n\n---\n\nWe have updated our submission based on the reviewer's feedback, with the revision highlighted in blue. We are happy to address further questions on our paper. Thank you for considering our rebuttal."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8332/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700089824583,
                "cdate": 1700089824583,
                "tmdate": 1700089824583,
                "mdate": 1700089824583,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tucfA1naWG",
                "forum": "NeWiiF6KLB",
                "replyto": "vcPYMjTRqw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8332/Reviewer_Tc5b"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8332/Reviewer_Tc5b"
                ],
                "content": {
                    "title": {
                        "value": "Reply to the author reponse"
                    },
                    "comment": {
                        "value": "Dear Author, \n\nAfter careful reading over the response, we are still concerned about the general consistency of this paper. The most important reason that leads to the numerical instabilities of EGNN when equipped with diffusion and flow is not fully revealed in this work. Currently, only some observations prove the existence of the numerical instability issue. The gradient analysis is actually not very important in this case as its theoretical result follows the intuition. The key part is exploring the special reasons that induce the numerical explosion of EDM and ENF. What particular operations make the EDM and ENF training unstable? It seems this reason is still not clearly stated in the author's rebuttal. Therefore, we think this paper is still inconsistent and we believe this paper can be accepted in the future after rounds of revisions. \n\nBest."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8332/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732196232,
                "cdate": 1700732196232,
                "tmdate": 1700732196232,
                "mdate": 1700732196232,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "q2Zgwm4bg3",
                "forum": "NeWiiF6KLB",
                "replyto": "zTZAjZvGdW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8332/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8332/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe thank the reviewer for the further feedback. The coordinate update operations cause instability in backpropagation, resulting in highly oscillatory or even exploding gradients and making the training unstable. After observing instability empirically in training, it is natural to investigate the gradients - this is the motivation for our theoretical study.\n\nThank you for considering our rebuttal.\n\n\nRegards,\n\nAuthors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8332/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733337418,
                "cdate": 1700733337418,
                "tmdate": 1700734276452,
                "mdate": 1700734276452,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DFefVuELgo",
            "forum": "NeWiiF6KLB",
            "replyto": "NeWiiF6KLB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8332/Reviewer_9sae"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8332/Reviewer_9sae"
            ],
            "content": {
                "summary": {
                    "value": "This paper builds upon E(n)-equivariant graph convolutional networks (EGNN) by normalizing the convolution layer with respect to the node\u2019s positions, and by adding a regularization term to the loss that promotes small gradients during training. The goal is to stabilize the training of graph generative models: normalizing flows and diffusion models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is mostly well written and clear. The regularization and normalization methods are shown in experiments to improve performance."
                },
                "weaknesses": {
                    "value": "The contribution is incremental. The contribution is just dividing by a normalization factor the well known EGCL, and adding a regularization factor to training. These are natural modifications of EGCL, and any other network, and I suspect that many practitioners apply such tricks often. The analysis of stability of backpropagation is partial, and missing important contributing terms to the magnitude of the gradients."
                },
                "questions": {
                    "value": "Page 4: \u201cIn contrast, normalizing coordinates can avoid abnormal coordinate updates from large differences among coordinates of neighboring nodes\u201d - but on the flip side, without the $+1$ regularization in the denominator, it is unstable to small coordinates. But with the $+1$ normalization, close-by nodes contribute a very small difference. How do you then choose the scale of the coordinates for the $+1$ to work well? Why do you use $+1$ and not $+b$ for some $b$ that depends on the characteristic target distance between modes?\n\nPlease explain how you construct a graph from the node locations and features.\n\nProposition 1: In all sums, shouldn\u2019t you sum over the neighborhood, and not the whole graph? $m_{i,j}$ is only defined when $(i,j)$ is an edge.\n\nSection 3.2 Sensitivity Analysis: The normalized EGCL is different from the unnormalized one. Why don\u2019t you compute the derivatives of the normalized version if this is the method you propose? Also, it is strange to directly write the derivative of $\\phi_x$ with respect to $\\|x_i-x_j\\|$. You need to use the chain rule, and first differentiate with respect to $m_{i,j}$.\n\nNotations: you did not define $I_3$.\n\nSection 4: I don\u2019t think that the derivative $\\partial f^{l+1}/\\partial f^l$ is the only main contributor to the size of $\\partial L (f^L)/\\partial \\theta$. To see this, note for example that when partitioning the parameters to the last layer parameters $\\theta^L$ and the previous layers parameters $\\theta^{L-1}$,  $\\partial L (f^L)/\\partial \\theta$ has two components. First, $\\partial L/\\partial f^L \\cdot  \\partial f^L\\partial \\theta^L$, and then $\\partial L/\\partial f^L \\cdot  \\partial f^L\\partial f^{L-1}\\cdot \\partial f^{L-1}\\partial \\theta^{L-1}$. Hence, $ \\partial f^{l}/\\partial \\theta^{l}$ are also important, and these are roughly going to depend on $\\|x_i-x_j\\|^l$ by induction, as each later multiples by a factor of order $\\|x_i-x_j\\|$. I think that this is another main reason you would like to normalize $x_i-x_j$.\n\nThis example is to illustrate that the analysis presented in this paper is partial. There is no systematic analysis of all components that contribute to the gradient. \n\nOne thing that is confusing in the analysis is that it analyzes only the unnormalized layer.  If you want to show that the normalized layer is more stable, you should write down a gradient analysis of it also. And write the full gradient with respect to the model parameters, as these are the gradients in training.\n\n\nExperiments: why are EDMs and ENF not compared against each other and other models in molecule generation in one table? I only see a comparison between EDM and other methods."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8332/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698753310229,
            "cdate": 1698753310229,
            "tmdate": 1699637036054,
            "mdate": 1699637036054,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vqUeIilVvA",
                "forum": "NeWiiF6KLB",
                "replyto": "DFefVuELgo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8332/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8332/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9sae (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review and valuable feedback. We fear that the reviewer ***misunderstood our proposed regularization scheme as simply adding factor 1 to the denominator of the coordinate update formula in EGCL***, i.e. Equation 1, which makes the reviewer feel our contribution is incremental. We would like to stress that our proposed regularization is given by Equation 4 rather than adding factor 1 to the denominator of the coordinate update formula in EGCL. Adding factor 1 to the denominator of the coordinate update formula in EGCL is a default in ENF and EDM, which is not our contribution. In the revision, we have significantly revised the paper to make our contribution more clear. In what follows, we provide point-by-point responses to your comments.\n\n---\n\n**Q1. The contribution is incremental. The contribution is just dividing by a normalization factor the well known EGCL, and adding a regularization factor to training.**\n\n**Reply:** We respectfully disagree that our contribution is incremental. We fear there are some misunderstandings of our contribution. Please allow us to clarify our contribution below:\n\n- First, dividing by a normalization factor, or using the normalized coordinate update is not our contribution, which is a default use in ENF and EDM. Moreover, ***the regularization we proposed is not by adding a factor to the denominator in the normalized coordinate update***. Instead, our proposed regularization scheme is given in Equation 4 in our paper.\n\n- Second, the ***analysis of the instability issues in training ENF and EDM*** Section 3 is our first contribution. In particular, we show that the training instability remains even when using the normalized coordinate update. By performing a sensitivity analysis in Section 3.2, we identify a source of the training instability issue. In particular, the term $\\frac{\\partial {\\bf x}\\_i^{l+1}}{\\partial {\\bf x}\\_j^l}$ is the most sensitive term to the coordinate update in EGNN during the training process.\n\n- Third, in Section 4, based on the expression of $\\frac{\\partial {\\bf x}\\_i^{l+1}}{\\partial {\\bf x}\\_j^l}$, we propose a new regularization term to improve the training of ENF and EDM. ***The new regularization is given by $\\sqrt{\\sum_{i,j\\in\\mathcal{N}(i),l}( \\frac{\\partial \\phi_x({\\bf m}_{ij}^l) }{\\partial  ||{\\bf x}_i^l-{\\bf x}_j^l ||^2  }  )^2 }$, which is novel and highly nontrivial.*** \nBesides stabilizing training ENF and EDM, we further show that ***the proposed regularization can accelerate the training and testing of ENF and improve the performance of EDM***.\nIn Section 5, we numerically verify the efficacy of the proposed regularization and confirm our theoretical results.\n\nWe believe the above contributions are significant.\n\n---\n\n**Q2. The analysis of stability of backpropagation is partial, and missing important contributing terms to the magnitude of the gradients.**\n\n**Reply:** We will address this comment later as a more concrete question on this comment has been raised by the reviewer.\n\n\n---\n\n**Q3. Page 4: \u201cIn contrast, normalizing coordinates can avoid abnormal coordinate updates from large differences among coordinates of neighboring nodes\u201d - but on the flip side, without the +1 regularization in the denominator, it is unstable to small coordinates. But with the +1 normalization, close-by nodes contribute a very small difference. How do you then choose the scale of the coordinates for the +1 to work well? Why do you use +1 and not +b for some b that depends on the characteristic target distance between modes?**\n\n**Reply:** +1 in the denominator is not the regularization scheme we proposed, it was used in the original ENF and EDM models. Again, our proposed regularization is given by Equation 4.\n\n---\n\n**Q4. Please explain how you construct a graph from the node locations and features.**\n\n\n**Reply:** We follow the original ENF paper to construct the graphs for DW4 and LJ13. For QM9, the graph is given in the benchmark datasets; see reference [1] listed below for details. Specifically, the graph is padded with nodes containing zero-valued features until all molecules share the same number of nodes. Then the graph edges are constructed by connecting all nodes with non-zero features. We follow this same graph construction procedure from the baseline ENF and EDM models. The exact code snippet is provided in reference [2] listed below and is executed when the graphs are collated into mini-batches for training.\n\n[1] Anderson et al. Cormorant: Covariant Molecular Neural Networks. NeurIPS 2019.\n\n[2] https://github.com/vgsatorras/en_flows/blob/main/qm9/data/collate.py#L78C4-L83C64\n\n---"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8332/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700089553080,
                "cdate": 1700089553080,
                "tmdate": 1700108127045,
                "mdate": 1700108127045,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lw7CFdaSLZ",
                "forum": "NeWiiF6KLB",
                "replyto": "DFefVuELgo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8332/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8332/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9sae (2/2)"
                    },
                    "comment": {
                        "value": "**Q5. Proposition 1: In all sums, shouldn\u2019t you sum over the neighborhood, and not the whole graph? $m_{ij}$ is only defined when $(i,j)$ is an edge.**\n\n\n**Reply:** Indeed, the sum is over the neighborhood. This has been fixed in the revision. Thank you for pointing this out.\n\n---\n\n**Q6. Section 3.2 Sensitivity Analysis: The normalized EGCL is different from the unnormalized one. Why don\u2019t you compute the derivatives of the normalized version if this is the method you propose? Also, it is strange to directly write the derivative of $\\phi_x$ with respect to $|x_i-x_j|$. You need to use the chain rule, and first differentiate with respect to $m_{ij}$.**\n\n\n**Reply:** We first stress again that the normalized version of EGCL is not what we proposed. \n\nThe notation $\\frac{\\partial \\phi_x }{\\partial ||{\\bf x}_i-{\\bf x}_j||^2}$ is clear while its detailed expression is obtained by using the chain rule. In what follows, we would like to provide basic examples to illustrate this.\n\nLet $w=f(z)$, $z=g(y)$, and $y=h(x)$. Though the exact expression $\\frac{\\partial w}{\\partial x}$ is $\\frac{\\partial w}{ \\partial z}\\cdot \\frac{\\partial z}{ \\partial y}\\cdot \\frac{\\partial y}{ \\partial x}$, in practice, people will simply write $\\frac{\\partial z}{\\partial x}$ rather than always write down the exact expression, which is rather redundant.\n\n\n\n---\n\n**Q7. Notations: you did not define I_3.**\n\n\n**Reply:** Thank you for pointing this out, $I_3$ is the $3\\times 3$ identity matrix. We have clarified this in the revision.\n\n---\n\n**Q8. Section 4: I don\u2019t think that the derivative $\\partial f^{l+1}/\\partial f^l$ is the only main contributor to the size of $\\partial L(f^L)/\\partial \\theta$. To see this, note for example that when partitioning the parameters to the last layer parameters $\\theta^L$ and the previous layers parameters $\\theta^{L-1}$, $\\partial L(f^L)/\\partial \\theta$ has two components. First, $\\partial L/\\partial f^L\\cdot \\partial f^L\\partial \\theta^L$, and then $\\partial L/\\partial f^L\\cdot \\partial f^L\\partial f^{L-1}\\cdot \\partial f^{L-1}\\partial\\theta^{L-1}$. Hence, $\\partial f^l/\\partial \\theta^l$ are also important, and these are roughly going to depend on $|x_i-x_j|^l$ by induction, as each later multiples by a factor of order $|x_i-x_j|$. I think that this is another main reason you would like to normalize $x_i-x_j$. This example is to illustrate that the analysis presented in this paper is partial. There is no systematic analysis of all components that contribute to the gradient.**\n\n\n**Reply:** We respectfully disagree that our analysis is partial. Notice that in Equation 6 of our paper, we have clearly stated that ***$\\theta$ is the weight parameter appearing at the $l$-th layer of EGNN.***\n\nWe would like to point out again that our sensitivity analysis aims to identify the most sensitive term with respect to $||{\\bf x}_i-{\\bf x}_j||$ in the gradient $\\frac{\\partial L}{\\partial \\theta}$. Through this sensitivity analysis, we propose the regularization scheme in Equation 4.\n\n---\n\n**Q9. One thing that is confusing in the analysis is that it analyzes only the unnormalized layer. If you want to show that the normalized layer is more stable, you should write down a gradient analysis of it also. And write the full gradient with respect to the model parameters, as these are the gradients in training.**\n\n\n**Reply:** We believe there is a misunderstanding, we are not aiming to show the normalized layer is more stable than the unnormalized layer. ***What we are showing is that using no matter normalized or unnormalized coordinate updates, training ENF and EDM suffers from the instabilities issue.***\n\nIn our paper, due to the page limit, we only write down the results for the unnormalized coordinate update. For the normalized coordinate update, a similar result can be derived by merging the normalization factor into $\\phi_x({\\bf m}_{ij}^l)$ in Equation 5.\n\n\n---\n\n**Q10. Experiments: why are EDMs and ENF not compared against each other and other models in molecule generation in one table? I only see a comparison between EDM and other methods.**\n\n**Reply:** In our original submission, we separated the results of ENF and EDM into two tables since ENF (normalizing flows) and EDM (diffusion models) are quite different models. In the revision, we have merged the two tables.\n\n---\n\nWe have updated our submission based on the reviewer's feedback, with the revision highlighted in blue. We are happy to address further questions on our paper. Thank you for considering our rebuttal."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8332/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700089682109,
                "cdate": 1700089682109,
                "tmdate": 1700090110894,
                "mdate": 1700090110894,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Hexbr3p4SZ",
                "forum": "NeWiiF6KLB",
                "replyto": "lw7CFdaSLZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8332/Reviewer_9sae"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8332/Reviewer_9sae"
                ],
                "content": {
                    "title": {
                        "value": "An important remaining issue"
                    },
                    "comment": {
                        "value": "Thank you for your response. Some of my concerts were answered, but we still do not agree on the most important issue.\n\nThe issue with the partial analysis regarding backprop still remains.\nYou indeed wrote in (6) that backprop depends on the derivative WRT $\\theta$, but then you did not analyze this derivative. As I explain in my review, this derivative may be a main contributor to the magnitude of the gradient in terms of $|x_i-x_j|$. Since your goal is to find sensitive terms WRT $|x_i-x_j|$, and you did not analyze a term that may be sensitive to $|x_i-x_j|$, the analysis is partial."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8332/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653251478,
                "cdate": 1700653251478,
                "tmdate": 1700653251478,
                "mdate": 1700653251478,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NAEtojXb7M",
                "forum": "NeWiiF6KLB",
                "replyto": "DFefVuELgo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8332/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8332/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further Response to Reviewer 9sae"
                    },
                    "comment": {
                        "value": "Dear Reviewer 9sae,\n\nThank you for your further feedback and we sincerely appreciate the opportunity to further address your concerns. Also, sorry for misunderstanding your question before. In what follows, please allow us to address your concerns on our partial analysis regarding backpropagation. \n\nWe discuss immediately after Equation (6) the reason we focus on $\\frac{\\partial {\\bf f}^{l+1}}{\\partial {\\bf f}^{l}}$. In particular, the gradient $\\frac{\\partial \\mathcal{L}({\\bf f}^{L})}{\\partial \\theta}$ propagates backward through the successive multiplication of $\\frac{\\partial {\\bf f}^{l+1}}{\\partial {\\bf f}^{l}}$. As the layers progress deeper, the successive multiplication of $\\frac{\\partial {\\bf f}^{l+1}}{\\partial {\\bf f}^{l}}$ becomes the predominant term on the overall stability of the gradient. In particular, the instability of $\\frac{\\partial {\\bf f}^{l'+1}}{\\partial {\\bf f}^{l'}}$ may result in the instability of $\\frac{\\partial \\mathcal{L}({\\bf f}^{L})}{\\partial \\theta}$ for all parameters $\\theta$ appearing at the $l$-th layer for any $l'\\geq l$.\n\nConversely, the other components, $\\frac{\\partial {\\bf f}^{l}}{\\partial \\theta}$ and $\\frac{\\partial \\mathcal{L}({\\bf f}^{L})}{\\partial {\\bf f}^{L}}$, remain independent of the other layers. While $\\frac{\\partial \\mathcal{L}({\\bf f}^{L})}{\\partial {\\bf f}^{L}}$ appears in  $\\frac{\\partial \\mathcal{L}({\\bf f}^{L})}{\\partial \\theta}$ for all $\\theta$, it is determined by the choice of loss function $\\mathcal{L}$.\n\nIn light of this, we propose Proposition 1 to delve into the details of $\\frac{\\partial {\\bf f}^{l+1}}{\\partial {\\bf f}^{l}}$, showing that the terms $\\frac{\\partial {\\bf x}_i^{l+1}}{\\partial {\\bf x}_j^{l}}$ -- for $l=0,\\ldots,L-1$ that may appear in the derivative in Equation (6) -- are most sensitive to $||{\\bf x}_i^l-{\\bf x}_j^l ||$. This analysis motivates us to propose the regularization scheme in Equation (4).\n\n\n\nThank you for considering our rebuttal. We are open to addressing any further questions or concerns on our paper."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8332/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687344211,
                "cdate": 1700687344211,
                "tmdate": 1700694077596,
                "mdate": 1700694077596,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "G52rZc8r48",
            "forum": "NeWiiF6KLB",
            "replyto": "NeWiiF6KLB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8332/Reviewer_wjdT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8332/Reviewer_wjdT"
            ],
            "content": {
                "summary": {
                    "value": "The paper highlights that there exists instability in training E(n)-graph neural networks (EGNN), especially maps for positions. The authors aim to stabilize the training of EGNN, especially by regularizations. Note that positional mapping in the EGNN includes the distance multiplication term, which is the distance between the node's position and its neighbor's, and it is critical to the E(n)-equivariance of EGNN.\n\nThe authors first point out that the parameters' gradient is proportional to the gradient of the EGNN outputs wrt the pairwise distance (between a node and its neighbor), and thus, the gradient term is the source of the instability of the EGNN training.\n\nDue to the gradient term, the authors claim that the previous attempt to stabilize the training, i.e., the normalized distance multiplication, is not sufficient, and thus, the authors propose to penalize the norm of the gradient (of EGNN) wrt the pairwise distance.\n\nTo test the hypothesis, the papers compare the three versions of EGNNs (vanilla, normalized distance, and the proposed method) over various benchmark datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I consider that the paper and its results are essential for several reasons:\n\n1. The paper provides a better understanding of a critical problem in training EGNN, i.e., the instability of its training, especially to the potential audience unfamiliar with EGNN and other similar models.\n2. The paper well motivates the proposed method so that readers can understand how each step contributes to the merits of the regularization.\n3. I found that the paper has a well-organized structure that makes it clear to understand the proposed method.\n\nIn addition, I found that the paper has a well-organized structure that makes it clear to understand the proposed method."
                },
                "weaknesses": {
                    "value": "While the proposed method seems well-motivated and interesting, the importance of the proposed method needs further analysis.\n\nFor example, it is unclear why the gradient wrt the pairwise distance is the key source of the instability of the EGNN training, while the authors claim that the instability stems from the gradient. However, the claim is backed only by the sensitivity analysis explained in Section 3.2, which seems close to intuitions in my understanding. I believe that it would be much clearer if the authors were showing that the gradient wrt the pairwise distance is exploded when the training of vanilla EGNN failed."
                },
                "questions": {
                    "value": "In my understanding, when the gradients are unstable during training, one common solution is gradient norm penalty or gradient clipping. How does the proposed method perform against such common techniques?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8332/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8332/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8332/Reviewer_wjdT"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8332/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699037971721,
            "cdate": 1699037971721,
            "tmdate": 1700872301524,
            "mdate": 1700872301524,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "I41kiCOuZ5",
                "forum": "NeWiiF6KLB",
                "replyto": "G52rZc8r48",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8332/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8332/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wjdT"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review and valuable feedback. In what follows, we provide point-by-point responses to your comments.\n\n\n---\n\n**Q1. It would be much clearer if the authors were showing that the gradient wrt the pairwise distance is exploded when the training of vanilla EGNN failed.**\n\n\n**Reply:** Thank you for your suggestion. We have included the plot of epochs vs. the gradient with respect to the pairwise distance, i.e. $\\frac{\\partial L}{\\partial ||{\\bf x}_i-{\\bf x}_j||}$, in Appendix A.1. The numerical results indeed confirm that the gradient with respect to the pairwise distance explodes when the training of vanilla EGNN fails.\n\n---\n\n**Q2. How does the proposed method perform against gradient norm penalty or gradient clipping?**\n\n**Reply:** By default, EDM has used gradient clipping in the experiment on QM9, and we have adopted gradient clipping for the baseline EDM, without regularization, for QM9 experiments. As shown in Section 5.2.2, regularization outperforms gradient penalty.\n\nAs the reviewer suggested, we have further compared our proposed regularization against gradient norm penalty and gradient clipping in Appendix B.3. In particular, we consider training ENF and EDM with gradient clipping or gradient penalty -- using different clipping thresholds and gradient penalty hyperparameters -- on DW4 and compare them against our proposed regularization. The new results further confirm that our proposed regularization outperforms gradient penalty or gradient clipping by a remarkable margin.\n\n---\n\nWe have updated our submission based on the reviewer's feedback, with the revision highlighted in blue. We are happy to address further questions on our paper. Thank you for considering our rebuttal."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8332/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700089297819,
                "cdate": 1700089297819,
                "tmdate": 1700089297819,
                "mdate": 1700089297819,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]