[
    {
        "title": "How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study on Learning with Representations"
    },
    {
        "review": {
            "id": "EvDZvKKegY",
            "forum": "ikwEDva1JZ",
            "replyto": "ikwEDva1JZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4750/Reviewer_z67a"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4750/Reviewer_z67a"
            ],
            "content": {
                "summary": {
                    "value": "This works studies in-context learning in transformers using synthetic data. It extends previous work, by studying composition of a *fixed* non-linear function (L-layer MLP) with a linear function that is learned in-context. This work provides a construction of a transformer that can solve this task, but also demonstrates it empirically on synthetic data. Additionally, the authors provide a mechanistic understanding of the algorithm implemented by a trained transformer."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The results extend the the setup of Garg et al. to study in-context learning with more complex function classes. In particular, transformers can learn a composition of a *fixed* non-linear function with a linear function learnt from context. The authors in Figure 1 provide evidence that a transformer matches the optimal predictor.\n\nThe mechanistic analysis is thorough, and provides compelling evidence for the underlying 3-step mechanism. It is surprising that the mechanism is consistent across multiple training runs and adds to the results of the paper. \n\nThe results also hold when multiple different non-linear representations are used which further strengthens the main claims of the paper. I would have liked to see the results of 4.1.1 more prominently in the main paper, but I understand the authors are constrained by space. \n\nOverall, I think the results would be of interest to the community and the toy setup may be more representative of in-context learning in language models."
                },
                "weaknesses": {
                    "value": "**Why are the constructive proofs important for understanding in-context learning in transformers?**\nI am aware that there are prior works that design transformers that are capable of in-context learning. However, I am not convinced of the importance and significance of these results. Couldn't we also find weights for other architectures (like large MLPs or LSTMs) and argue that they are capable of in-context learning. Is the existence of these model weights informative of what is learnt in practice?\n\n**Choice of non-linear functions.**  I think the authors could be more rigorous in evaluating the non-linear representations used in their setup. In particular, the non-linear representations are L-layer MLPs with the matrices being random orthogonal matrices. Do the results hold for other families of functions and does it fail to work for some other classes of functions? I think it would be helpful to clarify that the results are specific to this setup.\n\n**Is the synthetic setup an accurate toy-model to understand language models?** Like previous work, all the results are on synthetic data. It remains unclear if the toy setup is representative of in-context learning in language models. What kind real world tasks are captured by a composition of a fixed non-linear function and a linear function that is learnt from context?"
                },
                "questions": {
                    "value": "1. Could the authors add more details on how the non-linear functions are created? Can the authors also clarify in the introduction/abstract that the functions are L-layer MLPs?\n\n2. Is it possible to show some of these results on other families of non-linear functions? For example, what happens if the functions are polynomials or exponential functions of the input? Are there scenarios where it fails empirically?\n\n3. What happens if we increase the number of layers used to create the representation from 5 to 15. Does the model start to fail if L=15 (even if transformer has only 11) layers or does it find a good approximation to the non-linear function using just 4-5 layers?\n\n4. Results in appendix E were very interesting! As future work, it would be great to investigate how many different non-linear functions can be learnt. I would also be interested in understanding if in-context learning becomes difficult and if the model sometimes struggles to identify the right non-linear representation.\n\n5. Ruiqi et al. (https://arxiv.org/abs/2306.09927) show that in-context learning fails if the linear functions are selected to be out-of-distribution. Is this also the case here?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4750/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4750/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4750/Reviewer_z67a"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4750/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698691791579,
            "cdate": 1698691791579,
            "tmdate": 1699636457355,
            "mdate": 1699636457355,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "j3RqMHVUhR",
                "forum": "ikwEDva1JZ",
                "replyto": "EvDZvKKegY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4750/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4750/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer z67a"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback. We respond to the specific questions as follows.\n> Why are the constructive proofs important for understanding in-context learning in transformers? I am aware that there are prior works that design transformers that are capable of in-context learning. However, I am not convinced of the importance and significance of these results. Couldn't we also find weights for other architectures (like large MLPs or LSTMs) and argue that they are capable of in-context learning. Is the existence of these model weights informative of what is learnt in practice?\n\nIn our opinion, the importance for the constructive proofs is **not to give a yes/no answer** on \u201cwhether transformers can do this\u201d or whether alternative architectures can do the same. Rather, the constructive proofs can\n* Demonstrate the **efficiency** of transformers for implementing these in-context learning procedures, i.e. only requiring a number of transformer layers and heads. This heavily uses the specific structure of the attention and MLP layers, instead of just invoking some universal approximation result;\n* Suggest concrete mechanisms (computing representation, copying, gradient descent) that can in turn guide the probing experiments.\nAlternative architectures such as LSTM or MLP (on concatenated input) may be able to implement a similar learning procedure, but **much less efficiently**, as it may be difficult to use them to efficiently replicate our mechanisms such as (batch) gradient descent or copying. They may be able to implement other mechanisms efficiently (such as online gradient descent for LSTMs), though.\n\n> Choice of non-linear functions\u2026 I think it would be helpful to clarify that the results are specific to this setup. \n\n> Could the authors add more details on how the non-linear functions are created? Can the authors also clarify in the introduction/abstract that the functions are L-layer MLPs?\n\nWe mentioned \u201cWe instantiate the representation as shallow neural networks (MLPs)\u201d in the paragraph before the list of contributions (Page 2). To clarify, we further highlighted \u201cL-layer MLPs\u201d in the abstract and the list of contributions in our revision. The choice of weight matrices and non-linear functions are described around Eq(4) in Section 4.1. \n\n> Is the synthetic setup an accurate toy-model to understand language models? Like previous work, all the results are on synthetic data. It remains unclear if the toy setup is representative of in-context learning in language models. What kind real world tasks are captured by a composition of a fixed non-linear function and a linear function that is learnt from context?\n\nWhile there is always a gap between such synthetic tasks and real tasks, we believe our fixed representation +  linear function is at least a more realistic model than prior works in this line, which primarily focuses on linear functions only. The fixed representation can model any \u201cprior knowledge\u201d that can be encoded in a representation (e.g. feature extractor), whereas the changing linear function can encode the specific classification task.\nAs an illustrating toy example, consider the in-context learning problem\n\n\"Give me the next word or phrase: \u2018apple->means anger; ocean->means sadness; grass->means happiness; banana->means\u2019\u201d.\n\nThe representation function for this problem would be to map each object to its color (apple -> red), and the linear function would be to map the color to an emotion (red -> anger). The feature map is fixed (relies on prior knowledge about the world) whereas the linear function needs to be learned in context (another such instance could involve a different map). Of course this is a toy example, but we believe its underlying structure (prior knowledge, then learn in context) could be broaderly present in real-world problems."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700517741341,
                "cdate": 1700517741341,
                "tmdate": 1700517741341,
                "mdate": 1700517741341,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "duHUCeQ4j7",
            "forum": "ikwEDva1JZ",
            "replyto": "ikwEDva1JZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4750/Reviewer_R1v9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4750/Reviewer_R1v9"
            ],
            "content": {
                "summary": {
                    "value": "The goal of this paper is to theoretically and empirically understand the mechanism of in-context learning with underlying representations. Specifically, the setting considered is where there is a fixed representation function, chosen to be an MLP, and the ICL problem is to learn ridge regression on these representations. The transformer must learn this fixed representation function during pretraining and a regression hypothesis in-context. The authors theoretically show that it is possible to construct transformers that can perform ridge regression in supervised and linear dynamical system settings on fixed representations. Empirically, the paper verifies that transformers can learn to perform this type of ICL by probing for the emergence of mechanisms and values that should emerge according to theoretical construction."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The paper well-written.\n- The experiments do a good job at validating the claims by probing for the relevant information.\n- The results offer valuable insights into how in-context learning, which is very relevant and timely."
                },
                "weaknesses": {
                    "value": "- Labels in figures and the figure captions can be more clear. For example, items like \"TF_upper+1_layer_TF_embed\" in Figure 4b are not very readable.\n- Section 3 could be significantly condensed by considering theorem 2 as a generalization of theorem 1 instead of presenting them separately.\n- Section 3.1 states that the representation function can be chosen arbitrarily, but Lemma B.3 requires a specific structure and non-linearity to work.\n- Although the paper does a good job of illustrating the claimed mechanism, it does not analyze settings where the mechanism breaks."
                },
                "questions": {
                    "value": "- What happens when the representation function is of a different form? If either the transformer does not have enough layers or the width, is there an approximate representation function learned on which regression is performed, or does the entire mechanism fall apart?\n- How robust is learning of the representation function in settings where the pretraining data contains spurious correlations? Can we say anything about the transformer's ability to compositionally generalize with either the representation function, regression, or both?\n- What is OLS in Fig 1b?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4750/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698823321331,
            "cdate": 1698823321331,
            "tmdate": 1699636457273,
            "mdate": 1699636457273,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iUbEJayOat",
                "forum": "ikwEDva1JZ",
                "replyto": "duHUCeQ4j7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4750/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4750/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer R1v9"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback. We respond to the specific questions as follows.\n\n> Although the paper does a good job of illustrating the claimed mechanism, it does not analyze settings where the mechanism breaks.\n\n> What happens when the representation function is of a different form? If either the transformer does not have enough layers or the width, is there an approximate representation function learned on which regression is performed, or does the entire mechanism fall apart?\n\nIn theory, if the transformer does not have enough layers or width, then it can still implement an approximate algorithm (such as computing an approximate version of the representation function $\\Phi^\\star$, as the reviewer mentioned, combined with fewer gradient descent steps to implement approximate ridge regression).\n\nEmpirically, we conducted some preliminary experiments with the same transformer architecture but more sophisticated representation functions, such as MLPs with higher depth or width. We\u2019ve included these experiments in Appendix F.3 in our revision. Qualitatively, the trained transformer still exhibits similar mechanisms, as shown in the risk and probing curves (Figure 13-15), which gives evidence that they may indeed be implementing approximate versions of our claimed mechanism. Further questions such as what approximate representation function these are implementing could be an interesting question for future work. \n\n\n> How robust is learning of the representation function in settings where the pretraining data contains spurious correlations? Can we say anything about the transformer's ability to compositionally generalize with either the representation function, regression, or both?\n\nBy \u201cspurious correlations\u201d, did the reviewer mean OOD-like scenarios where the test time representation function is different from the training time one in function space, but they happen to be identical on the training data distribution? In that case, we can still guarantee the existence of a transformer that can do well on both the test time representation function and the training data, but we won\u2019t have a statistical guarantee on how training based on empirical risk minimization can actually find this transformer, as the statistical guarantee (e.g. the one sketched in the last paragraph of Section 4.1) only holds in-distribution.\n\n> Section 3.1 states that the representation function can be chosen arbitrarily, but Lemma B.3 requires a specific structure and non-linearity to work.\n\nDid the reviewer mean Section 4.1? Right after \u201cThe representation function $\\Phi^\\star$ can in principle be chosen arbitrarily\u201d in Section 4.1, we stated \u201cAs a canonical and flexible choice for both our theory and experiments, we choose $\\Phi^\\star$ to be a standard L-layer MLP\u201d, followed by Eq(4) which describes the MLP architecture in details (including the choice of non-linearity). Lemma B.3 follows the specifications made here. \n\n> Labels in figures and the figure captions can be more clear. For example, items like \"TF_upper+1_layer_TF_embed\" in Figure 4b are not very readable.\n\nThank you for pointing this out. We have added a short explanation about this in the caption of Figure 4 in our revision.\n\n> What is OLS in Fig 1b?\n\nThe OLS means ordinary least squares (i.e. standard linear regression) on top of the representation function $\\Phi^\\star$."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700517542755,
                "cdate": 1700517542755,
                "tmdate": 1700517542755,
                "mdate": 1700517542755,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KpnsjDr8Ky",
                "forum": "ikwEDva1JZ",
                "replyto": "iUbEJayOat",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4750/Reviewer_R1v9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4750/Reviewer_R1v9"
                ],
                "content": {
                    "title": {
                        "value": "Keeping my score"
                    },
                    "comment": {
                        "value": "Thank you for your responses!\n\nThank you for your additions in Appendix F.3, that is very helpful.\n\n> By \u201cspurious correlations\u201d, did the reviewer mean OOD-like scenarios where the test time representation function is different from the training time one in function space, but they happen to be identical on the training data distribution?\n\nI did not mean that your test time representation function is different. Rather, the data is sampled differently at test time. Right now, your x and z are sampled from a Gaussian prior. What if the covariance in these Gaussian priors is not diagonal? There is often sampling bias in real world data, containing correlations that are not causal. If you train with such correlations present in the data, but show balanced data at test time for ICL, does the model reveal the true representation function?\n\n> Did the reviewer mean Section 4.1?\n\nSorry, yes. I understand that you choose L-layer MLP, but I was questioning the statement \"The representation function can in principle be chosen arbitrarily\". But I see that other representation functions are addressed now in Appendix F.3.\n\nI'm going to keep my score."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700531155361,
                "cdate": 1700531155361,
                "tmdate": 1700531155361,
                "mdate": 1700531155361,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vIxJaCSeE3",
            "forum": "ikwEDva1JZ",
            "replyto": "ikwEDva1JZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4750/Reviewer_67Bi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4750/Reviewer_67Bi"
            ],
            "content": {
                "summary": {
                    "value": "Results illustrating the performance of transformers in ICL tasks that necessitate some degree of representation learning are presented. The theory can be partially validated through probing experiments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Clear and well-written. \n\nThis paper is one of the pioneering efforts to formalize how transformers execute ICL tasks that necessitate a degree of representation learning."
                },
                "weaknesses": {
                    "value": "The theory only encompasses representational results by providing some settings of the parameters in a way that a transformer performs an ICL task. Given the transformer's highly expressive capability, these types of constructions are generally relatively straightforward.\n\nThere's no assurance that these theoretical constructs are truly internalized by the model during the training process. Although probing experiments gave us some confidence that, in specific instances, the theory can predict the model's behavior, these types of experiments generally don't offer robust guarantees. As a result, while the theory is logical and sometimes mirrors empirical events, it could be counterproductive to lean too heavily on these theoretical constructs. It might be necessary to carry out an analysis of training dynamics in order to theoretically determine under which conditions the model actually aligns with the theoretical constructs."
                },
                "questions": {
                    "value": "The reviewer is open to learning about new evidences or analyses which address the points of the \u201cWeaknesses\u201d section above in this review."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4750/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698824825232,
            "cdate": 1698824825232,
            "tmdate": 1699636457188,
            "mdate": 1699636457188,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tuGJ6f1GdZ",
                "forum": "ikwEDva1JZ",
                "replyto": "vIxJaCSeE3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4750/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4750/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 67Bi"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback. We respond to the specific questions as follows.\n> The theory only encompasses representational results by providing some settings of the parameters in a way that a transformer performs an ICL task. Given the transformer's highly expressive capability, these types of constructions are generally relatively straightforward.\n\nWhile transformers are known to be highly expressive approximators of sequence-to-sequence functions, the **efficiency** and the **internal mechanisms** of the transformer still matters a lot. Our transformer constructions in Theorem 1&2 are efficient (transformer has low number of layers and heads). Further, both its high-level structure (such as computing representation + using gradient descent to do linear regression) + lower-level operations such as copying show up experimentally in trained transformers via probing. We believe these make the construction itself much more interesting and reflective of reality than a simple universal approximation type argument.\n\n> There's no assurance that these theoretical constructs are truly internalized by the model during the training process. Although probing experiments gave us some confidence that, in specific instances, the theory can predict the model's behavior, these types of experiments generally don't offer robust guarantees. As a result, while the theory is logical and sometimes mirrors empirical events, it could be counterproductive to lean too heavily on these theoretical constructs. \n\nWe appreciate this thoughtful question. First of all, we agree that theoretical construct is just one way to guide the probing experiments, there may be other ways to formalize hypotheses about the actual mechanism, and we don\u2019t necessarily need to rely solely on the theory. \nNevertheless, we still believe that the probing results on their own are actually a strength of our work, in that we are the first to give evidence of these mechanisms (especially lower-level ones such as copying) for in-context learning on trained transformers. In addition to probing, we also conducted a \u201cpasting\u201d experiment (Figure 4) as a more controlled way to test the linear ICL capability of the upper module of the trained transformer. \nWe believe further probing studies, potentially going beyond the theoretical constructs, is an important direction which we would like to leave as future work.\n\n> It might be necessary to carry out an analysis of training dynamics in order to theoretically determine under which conditions the model actually aligns with the theoretical constructs.\n\nWe agree that the training dynamics is an interesting future direction as well. In our ablations we already had some preliminary experiments on the training trajectory (Appendix F.1, Figure 9) where we found that the representation module and the linear ICL module gets simultaneously learned in the early training stage. Further studies in this direction would be interesting."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700517454946,
                "cdate": 1700517454946,
                "tmdate": 1700517454946,
                "mdate": 1700517454946,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EOa4FAoHaL",
                "forum": "ikwEDva1JZ",
                "replyto": "tuGJ6f1GdZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4750/Reviewer_67Bi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4750/Reviewer_67Bi"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "Thank you for the clarifications! \n\nMy current evaluation is still close to a score of 6 (marginal accept): despite the limitations (that the theory does not guarantee in what settings do the construction can be learned, and the experimental results in this synthetic setting is mostly expected), the theoretical community should still be able to learn from reading this paper because the theory establishes an interesting setting and is well-written."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700519895149,
                "cdate": 1700519895149,
                "tmdate": 1700519895149,
                "mdate": 1700519895149,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xNOEPkibGm",
            "forum": "ikwEDva1JZ",
            "replyto": "ikwEDva1JZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4750/Reviewer_oQN2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4750/Reviewer_oQN2"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on understanding the internal mechanism by which a Transformer model solves an in-context learning task where the label $y$ for an instance $x$ linearly depends on a representation $\\phi^{\\star}(x)$.  A recent line of work has focused on explicitly constructing transformer models that can simulate various learning methods (e.g., gradient descent) on a training objective defined by the in-context labeled examples during a forward pass of the Transformer model. This paper extends this line of work by considering a more general data model where the final label depends on the input instance through a linear function of a representation. The paper provides explicit constructions for Transformer networks that can simulate ridge regression for 1) supervised learning with a representation, and 2) learning dynamical systems with a representation. The explicit constructions first aim to employ the underlying representation map $\\phi^{\\star}$ in the lower layers of the Transformer and then implement gradient descent in the upper layers of the Transformer.\n\nThrough experiments on synthetic datasets, the paper demonstrates that the performance of in-context learning via Transformers closely agrees with the performance of an optimal ridge predictor. Through probing analysis of the Transformer models, the authors show evidence that supports representation mapping following by label prediction aspects of their constructions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The paper successfully extends the recent line of work on showing the feasibility of in-context learning via empirical risk minimization during a forward pass by considering data models where labels depend on the input feature via a representation map.\n2) The paper is well-written and explains the key contributions and techniques clearly.\n3) The empirical results (on synthetic datasets) do indicate the feasibility/presence of the explicit in-context learning mechanism hypothesized in the paper."
                },
                "weaknesses": {
                    "value": "1) Novelty of the technical contributions is limited given prior works of similar flavor that provide the feasibility of empirical risk minimization during forward pass. One of the aspects which authors claim to be novel is that they allow for representation-based learning. However, the underlying assumption is that the representation map is a multi-layer MLP model, which Transformers should be easily able to simulate through its MLP layers. In that sense, the results in not very surprising. \n\n2) The probing analysis is done only on a synthetic setup. It would be nice to get some supporting evidence for the proposed in-context learning mechanism on a real dataset.\n\nMinor issues:\n\n1) The authors may want to formally introduce/discuss the pre-training phase (e.g., Eq (10)) which learns the representation map early in the section on preliminaries. \n\n2) In the paragraph on **In-context learning** in Section 2, $\\mathcal{D}^{(j)}$ and $\\mathbf{w}_{\\star}^{(j)}$ are not defined before their usage."
                },
                "questions": {
                    "value": "1) In general, the transformers are known to be universal approximators. In light of these, could authors comment on the significance of the key contributions of the paper, i.e., learning a representation map before applying gradient-descent in the representation space?\n\n2) Could the authors elaborate on using a **linear model** for their investigation on the upper module via pasting (Figure 4)?\n\n3) Currently the non-linearity in the true representation map is closely tied to the nonlinearity used in the Transformer network. Could the authors comment on generalizing this to broader nonlinearities in the true representation map? How would it increase the required number of layers?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4750/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4750/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4750/Reviewer_oQN2"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4750/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699049538478,
            "cdate": 1699049538478,
            "tmdate": 1700691648908,
            "mdate": 1700691648908,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vL65BEujj3",
                "forum": "ikwEDva1JZ",
                "replyto": "xNOEPkibGm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4750/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4750/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oQN2"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback. We respond to the specific questions as follows.\n\n> Novelty of the technical contributions is limited... However, the underlying assumption is that the representation map is a multi-layer MLP model, which Transformers should be easily able to simulate through its MLP layers. In that sense, the results in not very surprising\u2026 could authors comment on the significance of the key contributions of the paper?\n\nFor the MLP part of the theory, we agree the high-level idea itself is not very surprising, as mentioned. However, instantiating this idea into a full rigorous construction involves several other new ingredients different from existing work. For example, we required an efficient implementation of N parallel gradient descent simultaneously for *predicting at every token* using a decoder transformer (Proposition B.4), whereas similar existing work (Bai et al. 2023, Zhang et al. 2023a) only implements a single gradient descent for predicting at the last token only using an encoder transformer. See the \u201cProof techniques\u201d paragraph after Theorem 1 for more details.\nFurther, another core contribution of this paper lies in using theory to guide empirical mechanistic study in a fine-grained (low-level) way, and indeed validating that the mechanisms identified from the theory show up in trained transformers (Figure 3-5). Along with the findings, we also proposed new techniques such as pasting (Figure 4) which may be of further interest.\n\n> The probing analysis is done only on a synthetic setup. It would be nice to get some supporting evidence for the proposed in-context learning mechanism on a real dataset.\n\t\nWhile our setting is synthetic, the setting (linear function on top of common representation) is arguably more realistic than most existing works in this direction that focus on linear functions. The synthetic setting also allows probing analyses to be done in conjunction with rigorous theoretical studies. Nevertheless, We agree that some probing analyses on e.g. real data or even more realistic synthetic data is an important question, which we believe is out of the scope of this paper but an interesting direction for future work.\n\n\n> In general, the transformers are known to be universal approximators. In light of these, could authors comment on the significance of the key contributions of the paper, i.e., learning a representation map before applying gradient-descent in the representation space?\n\nWhile the high-level plan of \u201clearning representation + applying gradient descent\u201d can be done by universal approximation, and this paradigm is not so surprising given prior work,**how** we instantiate this construction is important and makes a difference. The way we instantiate it in our Theorem 1&2 are efficient (transformer has low number of layers and heads), and lower-level operations such as copying we use in our construction do show up experimentally via probing. We believe these make the construction itself much more interesting and reflective of reality than a simple universal approximation type argument.\n\n> Could the authors comment on generalizing this to broader nonlinearities in the true representation map? How would it increase the required number of layers?\n\nIf the true representation map involves other nonlinearities beyond leaky ReLU, we could approximate such a non-linearity by a linear combination of ReLU functions. Therefore we can construct a similar transformer with potentially more heads within each layer (for approximating scalar nonlinearity using a linear combination of ReLUs), but the same number of layers.\n\n> Could the authors elaborate on using **a linear model for their investigation** on the upper module via pasting (Figure 4)?\n\nIs the question about why we **use a linear model as the data distribution** to investigate the upper module via pasting? Our goal is to test if the upper layers are simply implementing linear regression, or if they are implementing a more complex algorithm like linear regression with feature learning. The linear model is a sensible choice for testing this. In our experiment, we generated data using a linear model and passed it through the upper layers. The results suggest the upper layers are just doing vanilla linear regression. \n\n> In the paragraph on In-context learning in Section 2, $\\mathcal{D}^{(j)}$ and $\\mathbb{w}_{\\star}^{(j)}$\n are not defined before their usage.\n\nThanks for spotting these issues. We\u2019ve updated them accordingly in our revision."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700517362295,
                "cdate": 1700517362295,
                "tmdate": 1700517362295,
                "mdate": 1700517362295,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uNQrbFZLqi",
                "forum": "ikwEDva1JZ",
                "replyto": "vL65BEujj3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4750/Reviewer_oQN2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4750/Reviewer_oQN2"
                ],
                "content": {
                    "title": {
                        "value": "Adjusted the score"
                    },
                    "comment": {
                        "value": "Thank you for your response. \n\nSome of my concerns have been addressed by your response. I have updated my score to 6."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691628605,
                "cdate": 1700691628605,
                "tmdate": 1700691628605,
                "mdate": 1700691628605,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]