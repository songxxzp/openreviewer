[
    {
        "title": "Going Beyond Neural Network Feature Similarity: The Network Feature Complexity and Its Interpretation Using Category Theory"
    },
    {
        "review": {
            "id": "RRXiEgjXmh",
            "forum": "4bSQ3lsfEV",
            "replyto": "4bSQ3lsfEV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2361/Reviewer_Po5f"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2361/Reviewer_Po5f"
            ],
            "content": {
                "summary": {
                    "value": "The paper delves into the feature similarity and further proposes the feature equivalence, and finally feature complexity of a trained neural network. The authors also introduce the well-established math tool of category theory to elegantly estaiblish the theory of the introduced concepts as well as methods. Beyond new theoretical understanding, the authors further devise an iterative algorithm to achieve the computing of feature complexity based on which the functionally equivalent features can be found in a neural network such that pruning of neural networks can be fulfilled."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) an important understanding to the trained neural networks by introducing the new concept of feature equivalence and feature complexity which go beyond the feature similarity.\n2) an effective algorithm for computing the feature complexity and as a side-product can be used to prune the trained networks on the fly without access to the training dataset, neither with any iterations for finetuning which are needed in existing pruning methods.\n3) the authors further draw a few interesting observations which are well interpretable that further consolidates the impact and soundness of this work."
                },
                "weaknesses": {
                    "value": "The paper could be improved in some aspects: 1) due to the density of the new information in this paper, the authors may move some empirical results in plot to Section 1, to make the readers more easily to jump into the main idea and discovery of the work; 2) the related work part can be extendeded. Specifically, please discuss the difference and relation to the recent work in ICML 2023: On the power of foundation models. As far as I know, the paper also intensively uses the category theory tools for interpreting neural networks especially foundations models. Also, as the paper proposes a new pruning mehtod, the related work part need also discuess peer methods; 3) for experiment part, the authors are required to compare peer pruning methods as the main technical approach presented in this paper is a pruning technique. Also the authors shall more comprehensively discuss the pros and cons of their pruning method in the context of network pruning."
                },
                "questions": {
                    "value": "How sensitive of the proposed IFM method to the hyperparameter \\beta? The authors shall discuss it and give some ablation studies if possible."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2361/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698396700091,
            "cdate": 1698396700091,
            "tmdate": 1699636168643,
            "mdate": 1699636168643,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cCgDnYYQSe",
                "forum": "4bSQ3lsfEV",
                "replyto": "RRXiEgjXmh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2361/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2361/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Po5f"
                    },
                    "comment": {
                        "value": "We thank reviewer Po5f for the valuable feedback and suggestions for improvement. We address each point in the comment below.\n\n> **Q1: The authors may move some empirical results in plot to Section 1, to make the readers more easily to jump into the main idea and the discoveries of the work.**\n\nThanks for the suggestion, we have added an illustration in the Fig.1 in the updated paper to help readers understand the main idea more easily.\n\n> **Q2: The related work part can be extendeded. Specifically, please discuss the difference and relation to the recent work in ICML 2023: On the power of foundation models. As far as I know, the paper also intensively uses the category theory tools for interpreting neural networks especially foundations models. Also, as the paper proposes a new pruning mehtod, the related work part need also discuess peer methods.**\n\nWe have extended the related work section in the updated paper, where we discuss and introduce previous works using category theory in machine learning including \"On the power of foundation models\". Due to the space limitation, we add the discussion about pruning methods in Appendix F in the updated paper.\n\n> **Q3: For experiment part, the authors are required to compare peer pruning methods. Also the authors shall more comprehensively discuss the pros and cons of their pruning method in the context of network pruning.**\n\nWe have added the discussion about the difference between our proposed method and other pruning methods as well as the pros and cons in Appendix F in the updated paper. The main difference is that IFM has a different purpose compared with channel pruning methods. The proposed IFM aims at finding the most compact representation without significantly affect the performance while the purpose of channel pruning is better preserving the performance under certain computation budget. The main advantage of IFM is that it does not require access to data while the disadvantage of IFM is that the compression ratio could not be precisely controlled. In the experiment part, we compare our method with another SOTA pruning method INN [1] that also does not require finetuning. Since our proposed method does not require finetuning while most other pruning methods do, it might be unfair to directly compare our results with other pruning methods.\n\n[1] Kirill Solodskikh, Azim Kurbanov, Ruslan Aydarkhanov, Irina Zhelavskaya, Yury Parfenov, Dehua Song, and Stamatios Lefkimmiatis. Integral neural networks. In CVPR, pp. 16113\u201316122. IEEE, 2023.\n\n> **Q4: How sensitive of the proposed IFM method to the hyperparameter \\beta? The authors shall discuss it and give some ablation studies if possible.**\n\nAs introduced in the Appendix D, for all the experiments, we apply grid search on $\\beta$, from 0.01 to 0.2. The empirical results reported in the experiment part (Fig.3 and Fig.4) reports different results with varing $\\beta$. Generally, the \\beta set as 0.05 could achieve good balance between the performance and the compression ratio across the experiments we conducted in this paper. We will add a more comprehensive ablation studies in our paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2361/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699891256413,
                "cdate": 1699891256413,
                "tmdate": 1699891256413,
                "mdate": 1699891256413,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qQP8kgcL3u",
            "forum": "4bSQ3lsfEV",
            "replyto": "4bSQ3lsfEV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2361/Reviewer_R8Xc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2361/Reviewer_R8Xc"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an algorithm for merging of functionally equivalent neurons in a neuron network - neurons that do the same job due to having same input weights and same output weights on the same respective input and output connections.  Empirical evaluation is provided showing that sometimes a substantial pruning of the neurons (or merging of features, as the authors call it) can be achieved for a relatively minor drop in accuracy performance.  Authors propose to use the size of the network after the pruning/merge as the means of quantifying network complexity."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The method is extremely straight forward - compare the distance between concatenated vector of in-going and out-going weights of two neurons in a layer, and if they are below threshold, declare them the same.\n\nEmpirical evaluation seems to show this to be an effective method of pruning neurons in larger networks."
                },
                "weaknesses": {
                    "value": "I don't understand what the point of dragging of the reader through the group theory part of the paper is accomplishing.  It doesn't seem to me like group theory is used to arrive at the proposed feature merging/pruning algorithm.  Permutation of pairs of neurons in a network by swapping their input and output weights (provided they are connected to the same inputs and outputs) is sort of obvious, and in that light, the merging of the features/neurons is quite simple and straight forward.  Defining categories, functors and objects in this setting doesn't seem to give us any extra insight, or produce different tools for how to do the merging.  I find that the theoretical part of the paper has very little to do with the proposed practical aspect, other than perhaps we can name things using group theory terminology.  And if anything, it seems to obfuscate a very straight forward pruning technique that is proposed.  \n\nIn this race to the state of the art accuracy (which I still think we are all in) it seems that any pruning technique that sacrifices even a fraction of accuracy for some gains in computational/training time costs, is not going to be of practical use.\n\nI can't tell if the proposed feature complexity is meaningful in any way.  Does it correlate with generalisation?"
                },
                "questions": {
                    "value": "Aside from my objection to the group theory aspect of the paper, I don't understand what the concept of shape of the features relates to.  Is it the shape of the matrix/tensor representing the features?   \n\nRepeating my question from the previous section - is feature complexity related to generalisation?  And what is actually feature complexity?  Is it the total count of neurons in a network after pruning?  How do we know it means something?    \n\nThe statement \"feature complexity corresponds to the most compact representation of a neural network\" sound like something related to the minimum description length (MDL) or minimum message length (MML) principles.  Any comment how your work relates to those?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2361/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698631617054,
            "cdate": 1698631617054,
            "tmdate": 1699636168562,
            "mdate": 1699636168562,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kfydfXqLUI",
                "forum": "4bSQ3lsfEV",
                "replyto": "qQP8kgcL3u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2361/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2361/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer R8Xc [1/2]"
                    },
                    "comment": {
                        "value": "We thank reviewer R8Xc for the time and effort devoted to the reviewing process. We address each of the questions in the comments below.\n\n> **Q1: The proposed method is straight forward, what is the point of introducing category theory? The theoretical part of the paper has very little to do with the practical part and doesn't seem to provide any extra insights.**\n\nIn the theoretical part of this paper, we define functionally equivalent feature and feature complexity (layer-wise). To facilitate the understanding, we empirically describe them here. Functionally equivalent features mean that, given a feature map of one model, under certain transformations, we could get the output of the model by inferring with another different model that has learned functionally equivalent feature. Take a step further, if we could swap two elements in the feature map while keeping it to be functionally equivalent to the original feature map, it means the two elements could transform to each other, which indicates redundancy in the feature map. Feature complexity is defined as the dimensionality of the most compact representation of the feature without redundancy. \n\n**There are strong connections between the theoretical part and the practical part**: \n\n* With the definition of functionally equivalent features, we theoretically prove that the well-known empirical phenomenon Linear Mode Connectivity (LMC) indicates a special case of two models learning functionally equivalent features.\n\n* We extend the weight matching methods in LMC literature and propose the method IFM to measure feature complexity. Empirical results show that the defined redundancy widely exists in the features learned by neural networks.\n\nIn fact, pruning effect is not the main purpose of the proposed method, but a byproduct resulting from the redundancy in features.\n\n**The reasons for introducing category theory**: To formalize the empirical description above in the language of mathematics, we find category theory an appropriate tool.\n* The definition using the language of category theory is concise and elegant in its expression. Two networks learn functionally equivalent features iff there exist a natural transformation between the corresponding functors (formal definition is in Def. 3.1).\n* With our definition of categories, functors and natural transformations in the deep learning context, we hope the category theory perspective of our definition could inspire future works in more theoretical analyses.\n* Despite the use of the terms in category theory, each of them has a corresponding specific meaning in the training or testing of neural networks. Please refer to Table 1 in the updated paper for details.\n\n**To facilitate the understanding of the theoretical part of the paper, revisions have been made.** We add an overview figure (Fig. 1) to empirically illustrate the concepts we introduce in this paper. We also add a table (Table 1) listing the corresponding specific meaning of each used category theory terms in our definition. The revisions in the updated paper are indicated in blue. \n\n> **Q2: It seems that any pruning technique that sacrifices even a fraction of accuracy for some gains in computational/training time costs, is not going to be of practical use.**\n\nWhile reducing computational cost could be important for scenarios such as edge devices. We would like to clarify that the main purpose of this paper is not neural network pruning. As indicated in the title, we focus on the problem that feature similarity methods sometimes fail to provide sufficient insights (e.g. similar features could also yield different outputs [1,2]). In this paper, we extend the concept of equivalent feature and propose functionally equivalent feature to mitigate the gap between feature similarity and functional similarity (the similarity between outputs)[3]. We further define layer-wise feature complexity corresponding to the dimensionality of the most compact representation of the feature and propose algorithm IFM to measure it. Various insights have been provided (Sections 5.2 and 5.3). Here we list a few insights:\n* Redundancy widely exist in features learned by neural networks.\n* Larger networks have more redundant parameters.\n* Across the layers of a neural network, the feature complexity first increases then decreases.\n\n[1] Frances Ding, Jean-Stanislas Denain, and Jacob Steinhardt. Grounding representation similarity through statistical testing. Advances in Neural Information Processing Systems, 34:1556\u20131568,2021.\n\n[2] Lucas Hayne, Heejung Jung, Abhijit Suresh, and R McKell Carter. Grounding high dimensional representation similarity by comparing decodability and network performance. 2022.\n\n[3] Max Klabunde, Tobias Schumacher, Markus Strohmaier, and Florian Lemmerich. Similarity of neural network models: A survey of functional and representational measures. arXiv preprint arXiv:2305.06329, 2023."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2361/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699889693740,
                "cdate": 1699889693740,
                "tmdate": 1699891009181,
                "mdate": 1699891009181,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mHYp2vjF2y",
                "forum": "4bSQ3lsfEV",
                "replyto": "qQP8kgcL3u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2361/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2361/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer R8Xc [2/2]"
                    },
                    "comment": {
                        "value": "> **Q3: I don't understand what the concept of shape of the features relates to. Is it the shape of the matrix/tensor representing the features?**\n\nYes, it is the shape of the matrix/tensor representing the features. We have added an description in Sec.2.2 to make it more clear.\n\n> **Q4: What is actually feature complexity? Is it the total count of neurons in a network after pruning? How do we know it means something? Does it correlate with generalisation?**\n\nIdeally speaking, feature complexity is the smallest number of neurons in a network (at a certain layer) that one can achieve without affecting the functionality of the network (loss, accuracy, etc.)\n\nWe propose IFM to approximately measure the feature complexity. As we apply grid search on the hyper parameter $\\beta$ in Eq. 9, empirical results (Fig. 3(a) and Fig. 4(a) in the updated paper) approximately show the fraction of redundant parameters. These results suggest that redundancy widely exists in features learned by neural networks.\n\nWe are not sure if the defined feature complexity is correlated with generalization and we leave it for future works. However, empirical results (Fig. 4(b) in the updated paper) indicate that for two networks of the same structure, the performance drops faster for the network with better performance as more features are merged. We conjecture that networks with higher feature complexity would have better generalization performance.\n\n> **Q5: The statement \"feature complexity corresponds to the most compact representation of a neural network\" sound like something related to the minimum description length (MDL) or minimum message length (MML) principles. Any comment how your work relates to those?**\n\nThe feature complexity we defined in this paper aims to reflect the inherent structure of the neural network, which is the result of training data, network structure and training algorithm. Therefore it does not have direct relationship with MDL or MML principle [1] that guides the model selection or model training. However, the proposed feature complexity could be used as a tool to help the application or interpretation of the minimum message length principle. It could be an interesting topic and we leave it for future works.\n\n[1] Wallace, C. S. (Christopher S.), -2004. (2005). Statistical and inductive inference by minimum message length. New York: Springer. ISBN 9780387237954. OCLC 62889003"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2361/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699889777501,
                "cdate": 1699889777501,
                "tmdate": 1699891043334,
                "mdate": 1699891043334,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zUTH0WWK8R",
                "forum": "4bSQ3lsfEV",
                "replyto": "mHYp2vjF2y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2361/Reviewer_R8Xc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2361/Reviewer_R8Xc"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "Thank you for addressing my questions.  The additions you made to the paper definitely improve things, though I am still not quite sure what the implications of this work are and I can't help but feel that, after the grand setup of the category theory, the final algorithm for detecting feature similarity is somewhat trivial - threshold based grid search for Euclidean distance between input and output weights of a neuron.  From the insights, the only one that tells me something new is that across the layers of the network feature complexity first increases then decreases....though then perhaps all it means hat models generally transform input into higher dimensional space before contracting...in which case, perhaps it's not that surprising?  \n\nAnd I am a bit puzzled about the statement on the larger feature complexity potentially correlating with generalisation.  I would guess that model that memorises inputs (as in, say, trained to zero training error with randomly shuffled label data) would have massive feature complexity as compared to model that finds common patterns.\n\nAll in all, I think this is interesting work, that may eventaully lead to substantial insights...but not quite at this stage yet."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2361/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641226887,
                "cdate": 1700641226887,
                "tmdate": 1700641226887,
                "mdate": 1700641226887,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4vOouY4BKE",
            "forum": "4bSQ3lsfEV",
            "replyto": "4bSQ3lsfEV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2361/Reviewer_QeEM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2361/Reviewer_QeEM"
            ],
            "content": {
                "summary": {
                    "value": "The paper attempts to tackle a pertinent issue in neural networks, focusing on understanding and measuring the similarity between features learned by different neural network initializations. The concepts presented seem promising, especially in defining what are termed as functionally equivalent features and the subsequent proposal of an algorithm, Iterative Feature Merging (IFM). However, there are concerns and areas of improvement that need addressing."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The paper addresses a significant issue in neural networks\u2014understanding the behavior and feature representation across different initializations.\n\n* The introduction of the term \"functionally equivalent features\" offers a fresh perspective on understanding neural networks.\n\n* The Iterative Feature Merging (IFM) seems promising as an approach to quantify feature complexity."
                },
                "weaknesses": {
                    "value": "* The term \"functionally equivalent features\" is central to the paper. It would be beneficial to the reader if a simple illustrative example or intuitive explanation accompanied its introduction.\n\n* While the abstract and conclusion provide a concise overview, it remains unclear how rigorous the definitions and proofs, especially related to category theory, are in the main content of the paper. The paper would benefit from a detailed walkthrough of the mathematical formulations and proofs to ensure the robustness of the claims made.\n\n* Given the recent interest in understanding neural network behavior and their feature representations, it is vital to contextualize this work concerning existing literature. How does this work differ or extend previous work on the topic? A comprehensive comparison is essential."
                },
                "questions": {
                    "value": "* The conclusion acknowledges a limitation in testing only for image classification tasks. Why was the scope of experiments limited to image classification tasks, and how would the approach perform on other tasks? It would strengthen the paper to include experiments from a wider range of tasks or at least provide a rationale for why image classification was chosen as the primary focus. \n\n* Can the authors provide a more intuitive or illustrative example of \"functionally equivalent features\" to aid understanding?\n\n* The Iterative Feature Merging (IFM) algorithm is a central piece of this work. How does the Iterative Feature Merging (IFM) algorithm work in detail, and what makes it efficient? A clear and detailed algorithmic procedure, possibly with pseudocode, should be provided for a comprehensive understanding."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2361/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2361/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2361/Reviewer_QeEM"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2361/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698740763595,
            "cdate": 1698740763595,
            "tmdate": 1700737068511,
            "mdate": 1700737068511,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "a5zB9xJgBr",
                "forum": "4bSQ3lsfEV",
                "replyto": "4vOouY4BKE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2361/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2361/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QeEM [1/2]"
                    },
                    "comment": {
                        "value": "We thank reviewer QeEM for the valuable feedback and suggestions for improvement. We address each point in the comment below.\n\n> **Q1: The term \"functionally equivalent features\" is central to the paper. It would be beneficial to the reader if a simple illustrative example or intuitive explanation accompanied its introduction.**\n\nThanks for the suggestion. We have added an intuitive illustration (Fig.1) with intuitive explanation of \"functionally equivalent feature\" and \"feature complexity\" in the updated paper.\n\n> **Q2: The paper would benefit from a detailed walkthrough of the mathematical formulations and proofs to ensure the robustness of the claims made.**\n\nThanks for the suggestion. In our main submission, we provided detailed proofs in Appendix C. We also add a more detailed walkthrough of our formulation with category theory in Appendix C.3 of the updated paper.\n\n> **Q3: It is vital to contextualize this work concerning existing literature. How does this work differ or extend previous work on the topic? A comprehensive comparison is essential.**\n\nThere are two perspectives in the literature investigating similarity between neural networks, feature similarity [1,2,3,4] and functional similarity [5,6,7]. Feature similarity (representational similarity) focuses on the similarity between intermediate features while functional similarity focuses on the similarity between outputs [8]. However, evidences have shown that similar features could yield different output [9]. \n\nIn this paper, we unify the two perspectives and propose functionally equivalent feature. Based on functionally equivalent feature, we further define feature complexity and propose an algorithm IFM to measure the feature complexity. The proposed algorithm is an extension of weight matching methods [10] in linear mode connectivity literature from between two neural networks to within one neural network.\n\nIn response to the suggestion, a more detailed introduction and comparison have been added in Sec. 5 in the updated paper.\n\n[1] Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. SVCCA: singular vector canonical correlation analysis for deep learning dynamics and interpretability. In NIPS, pp. 6076\u20136085, 2017\n\n[2] Alex H. Williams, Erin Kunz, Simon Kornblith, and Scott W. Linderman. Generalized shape metrics on neural representations. In NeurIPS, pp. 4738\u20134750, 2021\n\n[3] Shuai Tang, Wesley J. Maddox, Charlie Dickens, Tom Diethe, and Andreas C. Damianou. Similarity of neural networks with gradients. CoRR, abs/2003.11498, 2020.\n\n[4] Serguei Barannikov, Ilya Trofimov, Nikita Balabin, and Evgeny Burnaev. Representation topology divergence: A method for comparing neural network representations. In ICML, volume 162 pp. 1607\u20131626. PMLR, 2022.\n\n[5] Omid Madani, David Pennock, and Gary Flake. Co-validation: Using model disagreement on unlabeled data to validate classification algorithms. Advances in neural information processing systems, 17, 2004.\n\n[6] Yamini Bansal, Preetum Nakkiran, and Boaz Barak. Revisiting model stitching to compare neural representations. Advances in neural information processing systems, 34:225\u2013236, 2021.\n\n[7] Srinadh Bhojanapalli, Kimberly Wilber, Andreas Veit, Ankit Singh Rawat, Seungyeon Kim, Aditya Menon, and Sanjiv Kumar. On the reproducibility of neural network predictions. arXiv preprint arXiv:2102.03349, 2021.\n\n[8] Max Klabunde, Tobias Schumacher, Markus Strohmaier, and Florian Lemmerich. Similarity of neural network models: A survey of functional and representational measures. arXiv preprint arXiv:2305.06329, 2023.\n\n[9] Frances Ding, Jean-Stanislas Denain, and Jacob Steinhardt. Grounding representation similarity through statistical testing. Advances in Neural Information Processing Systems, 34:1556-1568,2021.\n\n[10] Samuel Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging models modulo permutation symmetries. In ICLR, 2023.\n\n> **Q4: Why was the scope of experiments limited to image classification tasks, and how would the approach perform on other tasks?**\n\nWe conduct experiments on image classification tasks following previous feature similarity literature and pruning literature. In those literature, image classification task is the most widely used and investigated. To align with previous works, our experiments are conducted on most widely used network structures (VGG and ResNet) and benchmarks (CIFAR10 and ImageNet-1K). \n\nOur definition does not have any assumption about the neural network structure and the proposed method does not require access to data. While our focus on image classification tasks aligns with the prevalent literature, the task-agnostic nature of our method suggests its potential applicability to various tasks. Considering the limited time and space, we leave the approach on other tasks for future works."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2361/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699890230351,
                "cdate": 1699890230351,
                "tmdate": 1699890879328,
                "mdate": 1699890879328,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dPqixpBtTn",
                "forum": "4bSQ3lsfEV",
                "replyto": "4vOouY4BKE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2361/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2361/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QeEM [2/2]"
                    },
                    "comment": {
                        "value": "> **Q5: Can the authors provide a more intuitive or illustrative example of \"functionally equivalent features\" to aid understanding?**\n\nYes, an illustrative figure has been added in the updated paper (Fig. 1). More intuitively, if two models learn functionally equivalent features, it means that, under certain transformations, we can replace any layer of one model with the corresponding layer of the other model and get the same output as the original model.\n\n> **Q6: How does the Iterative Feature Merging (IFM) algorithm work in detail, and what makes it efficient? A clear and detailed algorithmic procedure, possibly with pseudocode, should be provided for a more comprehensive understanding.**\n\nIn the main submission, we provided detailed algorithm in Appendix. A. To facilitate the understanding of the proposed method, we have added a flowchart in Fig. 1 in the updated paper which is indicated in blue. In general, the proposed method is efficient because it does not require the access to data and only matches the weights of neural networks."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2361/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699890342698,
                "cdate": 1699890342698,
                "tmdate": 1699890964944,
                "mdate": 1699890964944,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "deXij2YzMj",
                "forum": "4bSQ3lsfEV",
                "replyto": "dPqixpBtTn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2361/Reviewer_QeEM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2361/Reviewer_QeEM"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. The authors have addressed most of my concerns and I'll raise my evaluation to '6: marginally above the acceptance threshold'."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2361/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737038828,
                "cdate": 1700737038828,
                "tmdate": 1700737038828,
                "mdate": 1700737038828,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jLqvFsh5EO",
            "forum": "4bSQ3lsfEV",
            "replyto": "4bSQ3lsfEV",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2361/Reviewer_oxNv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2361/Reviewer_oxNv"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes the concept of neural network feature complexity which is (claimed by the authors) a more formal and comprehensive description of the behavior of the trained neural networks. This idea, along with the technical part including theory, approach and empirical results, to my best knowledge, are basically new in literature. Specifically, the authors develop a quantitative metric to enable the reduction of neural network parameters based on the proposed concept of feature equivalence. In fact, the layer-wise feature complexity is also well supported by a number of empirical studies which also well align with the intuition: e.g. higher feature complexity with the same size of networks can achieve better performance; a larger network tends to learn redundant feaures; equivalents features may correspond to certain low-level semantics. It also well establishes the connection to linear mode connectivity (LMC).\n\nOne other interesting aspect of the paper is introducing the powerful tool of category theory to establish the (clear) theoretical foundation of the work: represent the network structure as a category and a certain neural network as a functor that maps this structure to specific parameters. The authors also give a clear discussion to differentiate their work for using the category theory. This perspective along with its technical derivation and results are new to my best knowledge and well fit with the proposed paradigm for understanding the feteature complexity."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors have well stated the contributions in their paper and my main concerns are mostly from writing and presentation. I think the authors are very familiar to the field of the paper and their presented theoretical results and methods are new, well-motivated and well verified by the experiments (espeically from the pruning perspective)."
                },
                "weaknesses": {
                    "value": "Yet the paper need to be more self-contained when introducing the ideas and background There are a few specific suggestions:\n1) in the abstract, the authors are suggested to emphasize the feature complexity is layer-wise to make it more clear as this is a new concept to my knowledge; they may use the term layer-wise ASAP in the abstract.\n2) it is vague to use the saying: merge of features in the first paragraph. Its exact and clear meaning need be better explained for self-containess. I think it refers to merge the weight to merge the feature; \n3) I am a bit confused with Eq. 4, how \\tau_z^{l+1} is imposed on Z^{l+1}? Is it a dot operation or we shall write it as a function of \\tau_z^{l+1}?\n4) it would also be good to clarify what are the Functionally Equivalent Features in Definitions 3.1, I think they are layer-wise f^{l}(theta_a^{l}) vs. f^{l}(theta_b^{l})?\n5) it a bit abuses the notations? does Z() also equals to f()? Please clarify it.\n6) in the introduction part, the authors may mention the partial order idea to make the complexity concept more tangible to readers, rather than until give the specific definition in Section 3.\n\nsome minor typos:\nIn another words->In another word\nit simply apply no transformation->it simply applies no transformation\nin definition 3.1 Functionally Equivalent Feature->Functionally Equivalent Features\nat l-th layer of network->at the l-th layer of network\nin Theorem 3.3 to a itself->to itself\n\nFinally I suggest the authors consider to put an overview of their work in the introduction part to improve the readability."
                },
                "questions": {
                    "value": "How the proposed pruning comapre with other (SOTA) pruning method? Additional experiments will further enhance the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2361/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698758070397,
            "cdate": 1698758070397,
            "tmdate": 1699636168410,
            "mdate": 1699636168410,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "02IlxPfCOz",
                "forum": "4bSQ3lsfEV",
                "replyto": "jLqvFsh5EO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2361/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2361/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer oxNv"
                    },
                    "comment": {
                        "value": "We thank reviewer QeEM for the valuable feedback and suggestions for improvement. According to the suggestions, we have added an overview figure in the updated paper and fixed the mentioned minor typos. We address each point in the comment below.\n\n> **Q1: In the abstract, the authors are suggested to emphasize the feature complexity is layer-wise to make it more clear as this is a new concept to my knowledge; they may use the term layer-wise ASAP in the abstract.**\n\nThanks for the suggestion, we have revised the introduction (Sec.1) and emphasized that the feature complexity is layer-wise.\n\n> **Q2: it is vague to use the saying: merge of features in the first paragraph. Its exact and clear meaning need be better explained for self-containess.**\n\nWe have revised the introduction section and make it more clear that we merge the weight to merge the feature.\n\n> **Q3: I am a bit confused with Eq. 4, how $\\tau_z^{l+1}$ is imposed on $Z^{l+1}$? Is it a dot operation or we shall write it as a function of $\\tau_z^{l+1}$?**\n\nSorry for the confusion. In Eq. 4, $\\tau_z^{l+1}$ is a function. We followed the category theory literature and omitted parentheses for simplicity. We have added parentheses to avoid confusion.\n\n> **Q4: It would also be good to clarify what are the Functionally Equivalent Features in Definitions 3.1, I think they are layer-wise $f^{l}(\\theta_a^{l})$ vs. $f^{l}(\\theta_b^{l})$?**\n\nThanks for the suggestion. We have added an illustration figure in the updated paper (Fig. 1). However, since the input of $f^{l}$ is determined by the previous $l-1$ layers, the Functionally Equivalent Features in Def. 3.1 is not defined layer-wise but \"model-wise\".\n\n> **Q5: Is it a bit abuses the notations? does Z() also equals to f()? Please clarify it.**\n\n\nIn our notation, $Z^{l}$ represent the composition of the first $l$ layers of the network. We use $Z^{l}$ instead of $f^{l}(f^{l-1}(\\cdots f^{1}()))$ for simplicity.\n\n> **Q6: In the introduction part, the authors may mention the partial order idea to make the complexity concept more tangible to readers, rather than until give the specific definition in Section 3.**\n\nThanks for the suggestion. We have added an illustration and intuitive description of the feature complexity in Fig. 1 in the updated paper to make the complexity concept more tangible.\n\n> **Q7: How the proposed pruning comapre with other (SOTA) pruning method? Additional experiments will further enhance the paper.**\n\nWe have added an detailed discussion of differences between the proposed IFM and other pruning methods in Appendix F in the updated paper. The comparision with another SOTA pruning method INN [1] that also does not require finetuning was provided in the experiment section. Since our proposed method does not require finetuning while most other pruning methods do, it might be unfair to directly compare our results with other pruning methods.\n\n[1] Kirill Solodskikh, Azim Kurbanov, Ruslan Aydarkhanov, Irina Zhelavskaya, Yury Parfenov, Dehua Song, and Stamatios Lefkimmiatis. Integral neural networks. In CVPR, pp. 16113\u201316122. IEEE, 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2361/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699890829676,
                "cdate": 1699890829676,
                "tmdate": 1699890829676,
                "mdate": 1699890829676,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]