[
    {
        "title": "Symbol as Points: Panoptic Symbol Spotting via Point-based Representation"
    },
    {
        "review": {
            "id": "aio8v0FUxr",
            "forum": "aOnUe8ah7j",
            "replyto": "aOnUe8ah7j",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1614/Reviewer_q1T8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1614/Reviewer_q1T8"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new method for symbol segmentation in architectural floorplans. The method is based on representing each graphical primitive as a point with a set of features and thus, relying on Point Transformer for feature extraction. Then, an adaptation of Mask2Former is used to segment and classify the symbols in the floorplan. Some specific components are introduced to adress the specificity of graphical primitives in architectural drawings. Experimental validation is performed by applying the method to a standard floorplan dataset, comparing with state-of-the-art and conducting several ablation studies."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The idea of relying on a point representation of graphical primitives seems novel and makes sense in this context since symbols in architectural drawings are composed of graphical primitives. Then, using the combination of PointTransformer and Mask2Former is also a novel approach in this context that seems suitable for capturing the interaction between graphical primitives for symbol segmentation. \n- Experimental results on a standard dataset report better performance than state-of-the-art methods. A detailed ablation study shows the contribution of the different modules of the proposed framework."
                },
                "weaknesses": {
                    "value": "Perhaps I missunderstood something, but I do not see the motivation of symbol segmentation in CAD drawings. As far as I understand CAD drawings should already contain information about the symbols included in the floorplan and where they are located. \n\nIn the description of the method and the experiments there are several points that are confusing or not well explained:\n- In equation (2) I understand that l_k is the distance between v_1 and v_2. Then, what about circles and ellipses? How is the lengh computed? And for arcs, this definition does not account for the curvature. Two arcs with very different curvature can have the same representation.\n- In equation (3), it is not clear how the neighbourhood M(p_i) is defined. Do adjacent points mean connected primitives? Or primitives inside a certain distance? Which is exactly the difference with A(p_i) defined later in section 3.3 (given that the threshold used in section 3.3 is just one pixel). In this sense, the role of the ACM module is not very clear. \n- It is not clear the motivation of the KNN interpolation described in section 3.5. As far as I understand, since points correspond to graphical primitives, interpolation of neighboring points could lead to losing information of specific primitives and I am not sure that makes sense merging different primitives into a new one. \n- Related to the previous point, It is not clear how it is performed downsamplind and upsampling in the Point Transformer. The same as in the original Point Transformer? \n- In equation (12) it is not clear what is e_i and L(e_i).\n- In the experiments, which is the difference between Semantic and Panoptic Symbol Segmentation? Why in table 1 (semantic segmentation) the evaluation measure is F1? How are F1 and wF1 defined in this context?"
                },
                "questions": {
                    "value": "See above in Weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1614/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698744737509,
            "cdate": 1698744737509,
            "tmdate": 1699636089792,
            "mdate": 1699636089792,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4C1LvjWqMV",
                "forum": "aOnUe8ah7j",
                "replyto": "aio8v0FUxr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1614/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1614/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer q1T8,\n\nThank you very much for your comment and support. We address your concerns as follows.\n\n### 1. Motivation for Symbol Segmentation\n\nWhile tools like AutoCAD enable the rapid creation of  CAD drawing, it typically lack detailed categorization information for each graphical primitive or symbol. The task of  panoptic symbol spotting aims to effectively extract and interpret this information in graphic primitive granularity.  Accurately spotting symbols is crucial for  Building Information Modeling(BIM), as the picture shows here: https://anonymous.4open.science/r/x-BB39/bim.png .\n\n### 2. Clarification on Methodology and Experiments\n\n* **Regarding Equation (2) and Primitive Representation**:  $l_k$ represents the distance between $v_1$ and $v_2$ for linear primitives. For circular primitives like circles and ellipses, $l_k$ is defined as the circumference.  For arcs, $l_k$ means the length of curve, not the distance between $v_1$ and $v_2$. Since all arcs in CAD are quadratic Bezier curves, we can easily compute the length of curves using the python package `svgpathtools`\n* **On Defining Neighborhood $M(p_i)$ in Equation (3):**   In $M(p_i)$, adjacent points means top K nearest primitives which are measured by the distance between the primitive positions (Eq 1) and $p_i$, not locally connected primitives.  $A(p_i)$ means a combination of $M(p_i)$ and $C(p_i)$ which means locally connected primitive points (Eq. 8). The reason that we use a small threshold in Eq. 7 to define the connectivity of two primitives is that for most CAD drawings, the interconnected endpoints of two connected lines are not the same point, but two points that are very close to each other.  We therefore regard lines whose endpoints are close enough as connected. \n* **Motivation Behind KNN Interpolation (Section 3.5):**   This KNN interpolation is used to extract different levels (resolutions) of features in symbol region (i.e, mask region). Although it may lose some information of the symbol in low resolution, but it provides a coarse estimated symbol region at different resolutions, which could be used for pyramid feature extraction as in [R1]. These pyramid of features, including both high-resolution and low-resolution features will be fed into the spotting head for symbol mask/label predicting. Note that we do not only use low-resolution features but different levels of features for final segmentation and label prediction. \n* **Downsampling and Upsampling in Point Transformer:** The downsampling and upsampling processes of point features is shown in https://anonymous.4open.science/r/x-BB39/pool&unpool.png, which is from [R2]. Adjacent point features are downsampled by pooling to get a point feature, which could also be upsampled through unpooling.\n* **Explanation of Equation (12):**  $e_i$ is a graphical entities, $L(e_i)$ is the length of the graphical entitity $e_i$\n* **On Experimental Evaluation:**  (1) Semantic Symbol Segmentation does not distinguish between objects of the same category, while Panoptic Symbol Segmentation does. (2) We follow previous works such as PanCADNet [R3], and GAT-CADNet [R4] to use F1 as metric for fair comparison;  (3) The F1 is the harmonic mean of Precision and Recall, The wF1 is length-weighted F1. Both metrics are well defined in [R3]. We will provide the detail definition in the supplementary material in future.\n\n[R1] Bowen, Cheng, et al. \"Masked-attention mask transformer for universal image segmentation\" CVPR. 2022.\n\n[R2] Xiaoyang Wu, et al. \"Point Transformer V2: Grouped Vector Attention and Partition-based Pooling\" CVPR. 2021.\n\n[R3] Zhiwen, Fan, et al. \"Floorplancad: A large-scale cad drawing dataset for panoptic symbol spotting\" ICCV. 2021.\n\n[R4]Zhaohua, Zheng, et al. \"Gat-cadnet: Graph attention network for panoptic symbol spotting in cad drawings\" CVPR. 2022."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1614/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700103818501,
                "cdate": 1700103818501,
                "tmdate": 1700103818501,
                "mdate": 1700103818501,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Qrw0epvURS",
                "forum": "aOnUe8ah7j",
                "replyto": "4C1LvjWqMV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1614/Reviewer_q1T8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1614/Reviewer_q1T8"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors, thank you for your detailed response, addressing most of my concerns. I do not require further clarifications. I will carefully review your response along with the other reviewer's comments before making my final recommendation."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1614/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643949837,
                "cdate": 1700643949837,
                "tmdate": 1700643949837,
                "mdate": 1700643949837,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3WiyPAEUIL",
            "forum": "aOnUe8ah7j",
            "replyto": "aOnUe8ah7j",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1614/Reviewer_AtGi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1614/Reviewer_AtGi"
            ],
            "content": {
                "summary": {
                    "value": "This paper, titled SymPoint, advocates for representing a symbol as a point and extends the previous methodology to encompass a broader range of symbol properties. The Point Transformer serves as the foundational feature extraction tool. Mask attention and a contrastive connectivity learning mechanism are integrated into the panoptic symbol spotting task, aiming to cultivate rich features that can effectively differentiate between graphic primitives. The PQ performance has been elevated from the previous method to a novel tier, as delineated in the experimental section."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "With the advancements in point cloud processing and the Transformer architecture, the authors suggest leveraging these powerful backbones from other domains and adapting them to address the challenge of panoptic symbol spotting.\nA suite of techniques, encompassing vector graphics representations, Point Transformers, Masked Attention, Contrastive Connection Learning, and KNN Interpolation, has been integrated into the targeted task.\nExperimental outcomes reveal that SymPoint significantly outperforms existing methods, exhibiting a considerable advantage in Semantic Symbol Spotting, Instance Symbol Spotting, and Panoptic Symbol Spotting."
                },
                "weaknesses": {
                    "value": "- A primary concern from the reviewer centers on the paper's predominant reliance on existing methodologies to address the issue. Specifically, in Sec3.1 (From Symbol to Point), many parameterizations echo those found in FloorplanCAD, albeit this paper seeks to enhance the diversity of encoded features. The point-based representation in Sec 3.2 directly employs the Point Transformer, reminiscent of CADTransformer. Both Contrastive Connection Learning (Sec3.4) and KNN Interpolation (Sec3.5) have been thoroughly examined in other scholarly works. While the \"Attention with Connection Module\" presents as novel to the reviewer, it would be beneficial to undertake a comprehensive review to discern if analogous concepts have been previous literature.\n- In Table 4, where the benchmark approach registers a PQ of 73.1, could you detail the design of how this baseline method is formulated?\n- In Table 4, it appears the newly introduced \"ACM\" module inadvertently undermines performance. Could the authors shed light on the causative factors behind this decline?\n- Again, referencing Table 4, the KInter technique emerges as a salient contributor to performance enhancement. Could the authors offer a more clear explanation and visualization? It might also be worthwhile to highlight this module within the methods section.\n- As the proposed framework incorporate a bunch of techniques for a specific application, did you submit the code for reviewing?"
                },
                "questions": {
                    "value": "See the raised concens in Weaknesses section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1614/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698777729440,
            "cdate": 1698777729440,
            "tmdate": 1699636089700,
            "mdate": 1699636089700,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tveixcPxMS",
                "forum": "aOnUe8ah7j",
                "replyto": "3WiyPAEUIL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1614/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1614/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer AtGi,\n\nWe sincerely appreciate your comments and your detailed questions. We address your questions as follows.\n\n###  1. Reliance on Existing Methodologies\n\n* While our approach to primitive features bears a resemblance to the vertex features defined in GAT-CADNet, our method focuses on leveraging the most straightforward and fundamental features in CAD drawings, such as type, length, and clockwise angle. These features, while simple, are highly effective in capturing the essence of individual primitives. Although more complex features could potentially be utilized, we found that these basic attributes yield impressive results, striking a balance between simplicity and effectiveness.\n* Compared to CADTransformer, our method is simple yet effective, marked by three key distinctions:\n  * Our process eliminates the need for rasterization of CAD drawings to images, which is a time-intensive step required for many CAD segmentation methods including CADTransformer.\n  * We rely on simple and direct primitive features, as opposed to CADTransformer's use of HRNet for feature extraction. This approach not only simplifies the process but also enhances the scalability of our method to complex, high-resolution CAD drawings.\n  * Our method employs the Point Transformer for interactions with each graphic primitive, whereas CADTransformer utilizes the Vision Transformer (ViT), which more easily occurs OOM when dealing with high resolution CAD drawings;\n  * Our methodology is designed for end-to-end training without the necessity for any post-processing operations. In contrast, CADTransformer requires the use of a clustering algorithm to derive the final instance results, adding additional steps to the process;\n\n### 2.  Design of Baseline Method\n\nOur baseline is a simple combination of Point Transformer and Mask2Former's transformer decoder with input of point-based representation.  This serves as a foundation for comparing the enhanced performance of SymPoint.\n\n### 3. Performance of the ACM Module\n\nOur original intention in introducing ACM was to utilize these connections between each graphic primitive.  we conduct experiments in SESYD-floorplans dataset that is smaller than floorplanCAD,  ACM can significantly promote performance and accelerate the model convergence.  An convergence curve between without/with ACM is shown here: https://anonymous.4open.science/r/x-BB39/sesyd_val_loss.png, https://anonymous.4open.science/r/x-BB39/sesyd_val_PQ.png, https://anonymous.4open.science/r/x-BB39/sesyd_val_RQ.png, https://anonymous.4open.science/r/x-BB39/sesyd_val_SQ.png,   The quantitative results are shown below,\n\n|              | PQ    | RQ    | SQ    |\n| ------------ | ----- | ----- | ----- |\n| baseline     | 88.23 | 91.61 | 96.01 |\n| baseline+ACM | 91.98 | 95.99 | 95.85 |\n\nBut, floorplanCAD is more complex compared with SESYD-floorplans, and more noisy connections could be introduced. As this figure shows : https://anonymous.4open.science/r/x-BB39/noisy-connection.png . Therefore, we have introduced an additional Contrastive Connection Learning to mitigate the impact of noise connections and more effectively utilize connection information with category consistency.  Ablation study in Table 4 (a) verifies the effectiveness of these modules. \n\n### 4. Explanation and Visualization of KNN interpolation Technique\n\nWhile bilinear interpolation, as utilized in Mask2Former, is tailored for regular data, such as image,  but it is  unsuitable for irregular sparse primitive points. Here, We provided some visualizations of point masks for KNN interpolation and bilinear interpolation as shown in https://anonymous.4open.science/r/x-BB39/vis2-knn_interp-vs-bilinear_interp.png.  Note that these point masks are soft masks (ranging from 0 to 1) predicted by intermediate layers. After downsampling the point mask to 4x and 16x, we can clearly find that KNN interpolation well perverse the original mask information interpolation, while  bilinear interpolation causes a significant information loss, which could harm the final performance.\n\n### 5. Submission of Code for Review\n\nWe appreciate the emphasis on the practical application of our framework.  we promise to release our source code for result reproduction and promote the development of this field."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1614/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700104425830,
                "cdate": 1700104425830,
                "tmdate": 1700104425830,
                "mdate": 1700104425830,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QG70pyNIRX",
            "forum": "aOnUe8ah7j",
            "replyto": "aOnUe8ah7j",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1614/Reviewer_u21w"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1614/Reviewer_u21w"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, a method for symbol spotting from CAD vector graphics (VG), called SymPoint, is proposed. SymPoint treats graphic primitives as a set of 2D points. Two strategies, attention with connection module (ACM) and contrastive connection learning (CCL), are devised to better utilize the local connection information of primitives and enhance their discriminability."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The main idea and technical detailed are clearly presented."
                },
                "weaknesses": {
                    "value": "1. The originality and technical contribution of this work is quite limited. Point Transformer, Mask2Former and InfoNCE are all well-established methods or models.\n2. The potential application range of the proposed method can be narrow (CAD vector graphics), because it is unclear whether the idea and techniques presented in this work can be extend ed to other tasks."
                },
                "questions": {
                    "value": "The authors should explain and verify the originality and technical contribution of the proposed method."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1614/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698831965792,
            "cdate": 1698831965792,
            "tmdate": 1699636089632,
            "mdate": 1699636089632,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8R1CtozFaa",
                "forum": "aOnUe8ah7j",
                "replyto": "QG70pyNIRX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1614/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1614/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer u21w,\n\nWe sincerely appreciate your comments and your detailed questions. We address your concerns as follows.\n\n**1. Regarding Originality and Technical Contribution:**\n\nWhile SymPoint incorporates well-established methods like Point Transformer, Mask2Former, and InfoNCE, our innovation lies in adapting and optimizing these methods for the specific challenge of CAD vector graphics recognition.  **It is worth noting that the combination of Point Transformer and Mask2Former is just our baseline.**\n\nSpecifically, the novelties of our paper lie in several aspects: \n\n* We carefully analyzing the data characteristics of CAD drawings and design novel and effective way of transferring CAD primitive entities into point feature in Section 3.1; \n* We propose Attention with Connection Module (ACM) , which is novel in efficiently capturing the local connection information of primitives and enhancing their discriminability when in conjunction with Contrastive Connection Learning (CCL) module in the context of CAD graphics in Section 3.3 and 3.4 ;\n* We propose KNN Interpolation to downsample  attention mask for masked attention calculation, which shows superior performance compared with the naive bilinear interpolation used in Mask2Former in Section 3.5.  \n\nOur ablation study in Table 4 (a) has demonstrated that these modules effectively promoted the performance of the model. These strategies represent a unique approach not previously explored in this field.  **We achieves 77.3/87.1/ 88.7 (PQ/RQ/SQ) with these novel designs, significantly outperforming the baseline 73.1/83.3/87.7 (PQ/RQ/SQ) . Note that our baseline method has already surpassed many state-of-the-art methods, such as PanCADNet and CADTransformer.**  \n\n**2. On the Potential Application Range:**\n\nIn response to the concern regarding the application range of SymPoint, we acknowledge that our current focus is primarily on CAD vector graphics. However, we believe there is potential for applying our method to a broader range of fields, like **3D modeling[R1]** and **circuit design[R2]**, **sketch segmentation[R3]**  as shown here:  https://anonymous.4open.science/r/x-BB39/application.png. **We have conduct some preliminary experiments on circuit design in the appendix A.2. and achieve state-of-the-art performance on two datasets as shown in Table 7.** Although this beyond the scope of this paper, we see it as a valuable direction for future research.\n\n[R1] Lv, Xiaolei, et al. \"Residential floor plan recognition and reconstruction\" ICCV. 2021.\n\n[R2] Jiang, Xinyang, et al. \"Recognizing vector graphics without rasterization\" NeurIPS. 2021.\n\n[R3]Yang, Lumin , et al. \"Sketchgnn: Semantic sketch segmentation with graph neural networks\" TOG. 2023."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1614/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700104006147,
                "cdate": 1700104006147,
                "tmdate": 1700104006147,
                "mdate": 1700104006147,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]