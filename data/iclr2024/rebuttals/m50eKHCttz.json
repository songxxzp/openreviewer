[
    {
        "title": "Fantastic Gains and Where to Find Them: On the Existence and Prospect of General Knowledge Transfer between Any Pretrained Model"
    },
    {
        "review": {
            "id": "piHByz2CZJ",
            "forum": "m50eKHCttz",
            "replyto": "m50eKHCttz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4151/Reviewer_yxq3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4151/Reviewer_yxq3"
            ],
            "content": {
                "summary": {
                    "value": "The authors provide an empirical study of the ability to transfer complementary knowledge between different pretrained models without performance degradation. This paper analyzes existing approaches in knowledge distillation and find it insufficient, especially in the case of distilling specific information from weaker teacher models. They go on to propose a data partitioning-based method (into regions of desired teacher behavior and desired student behavior retention) to achieve complementary knowledge transfer between the pretrained models considered in this paper."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The paper empirically shows that complementary knowledge exists between a large suite of models defined by different architectures and sizes (and even in weaker models that are less well performant than other stronger models). This complementary knowledge is localized to particular classes (what the authors deem as relative areas of expertise)\n\n2. The authors propose a new data-partitioning approach to transfer complementary knowledge, where data is partitioned by which model has a higher probability of the ground truth class (or chosen simply by maximum probability in an unsupervised case).\n\n3. Extensive experimental results that demonstrate that the proposed distillation approach transfers at a higher rate and transfers complementary knowledge from weaker teacher models.\n\n4. The paper also studies different properties of student models that better allow for knowledge transfer."
                },
                "weaknesses": {
                    "value": "Overall, I think the paper is quite comprehensive. A few points that may be lacking:\n\n1. The results in studying properties of student models is a bit surprising to me. This isn\u2019t a huge weakness, but more exploration of why CNN student models improve with scale and why transformer student models seem to worsen with would strengthen these results.\n\n2. The data partitioning heuristic is reasonable, but some ablations on this approach would be more enlightening. Perhaps in some instances, the student model may be overconfident about particular data points (that either have incorrect labels or are inherently difficult examples to classify), and this data partitioning approach would maintain this overconfidence."
                },
                "questions": {
                    "value": "1. Do you have any intuitions as to why student models that are CNNs exhibit better transfer at scale (while other architectures do not)? (Figure 8 in Supplement)\n\n2. In Table 3, unsupervised DP outperforms supervised DP on several tasks. This seems a bit surprising; in the cases where these methods would be different, your DP approach would be distilling information from the teacher model on instances where the model is both quite confident and incorrect. Do you have an ideas about how this would be beneficial, and does this match your intuitions as to why this method works in general?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4151/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4151/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4151/Reviewer_yxq3"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4151/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698471448428,
            "cdate": 1698471448428,
            "tmdate": 1699636380886,
            "mdate": 1699636380886,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "A47g8WVPKs",
                "forum": "m50eKHCttz",
                "replyto": "piHByz2CZJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4151/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4151/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Review"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their positive and useful review, and address each issue & question raised individually below.\n\n---\n\n__Issue 1__\n> The data partitioning heuristic is reasonable, but some ablations on this approach would be more enlightening. Perhaps in some instances, the student model may be overconfident about particular data points (...), and this data partitioning approach would maintain this overconfidence.\n\nThe KL-Dist.+DP transfer approach __does not introduce additional hyperparameters compared to vanilla KL-Distillation__, with robustness towards e.g., temperature choices (see Supp. A.3) - as such, we believe the most essential method ablations to be covered. In addition, we provide comparisons between supervised and unsupervised partitioning in Section 5.1 and Table 3, which showcase comparable performance but at the cost of increased transfer variance in the unsupervised case.\n\nAs the reviewer correctly points out, the confidence-based partitioning heuristic is imperfect due to issues such as overconfidence, which likely plays into this issue - and while the final results showcase comparable performance, some additional insights could be beneficial. \n\nConsequently, to assess the partitioning of the input data, we calculate the number of positive and negative flip samples from the current training batch that are transferred to the teacher model. We use two metrics, namely \u201cPos. to Teacher\u201d and \u201cNeg. to Teacher\u201d. \nIdeally, the positive flip samples, where the teacher model is correct while the student model is incorrect, should be assigned to the teacher model (i.e., Pos. to Teacher = 1). On the other hand, the negative flip samples, where the student model is correct and the teacher model is incorrect, should not be assigned to the teacher model (i.e., Neg. to Teacher = 0). We evaluated the partitioning heuristic for the teacher and student models presented in Table 6 using our KL-Dist. + DP Transfer approach. \n\nOur analysis shows that, on average, 72% of the positive flip samples and 9% of the negative flip samples were assigned to the teacher model. \nThis means that while the simple confidence-based partitioning heuristic does not perfectly assign the training samples, it does align with the idea of partitioning the training data based on the teachers' and students' areas of expertise. Our findings are consistent with the transfer delta results, where we observed that the KL+DP approach successfully transferred knowledge from the teacher while maintaining the students' pretrained knowledge. Nevertheless, we think the proposed approach would benefit from a more precise partitioning, and we hope to explore this further in future work. We will incorporate the results above in the final version of this paper.\n\n---\n\n__Question 1__\n> Do you have any intuitions as to why student models that are CNNs exhibit better transfer at scale (while other architectures do not)? (Figure 8 in Supplement)\n\nIn general, we observe that student models from all architecture families have a strong tendency to improve with an increase in size, as demonstrated in Figure 5b and Figure 8b. However, it is worth mentioning that it can be challenging to isolate the effect of the student model's size from the impact of other variables, such as the large number of potential model variations within a model family. \nParticularly for CNNs, we found that while transfer receptiveness increases with scale, these models are generally more prone to have their previous knowledge overwritten at reduced capacity levels (see Section 5.1, p.9 and supplementary).\nWe currently attribute this to the stronger visual inductive biases inherently encoded into the model architecture, which limits the structure of knowledge that can be effectively incorporated. This hypothesis is reinforced by the behavior of the other model families. However, a more extensive study could provide further interesting insights, which we will leave to future research to investigate!\n\n---\n\n__Question 2__\n> In Table 3, unsupervised DP outperforms supervised DP on several tasks. This seems a bit surprising (...). Do you have any ideas about how this would be beneficial, and does this match your intuitions as to why this method works in general?\n\nAs noted by the reviewer, knowledge transfer with unsupervised KL+DP performs comparably and, in some cases, better than the supervised variant. This was more likely for stronger and larger teacher models (positive correlation, ~0.25, between teacher model accuracy and the binary event of the unsupervised variant outperforming the supervised one). We can attribute this to the stronger teacher more consistently producing high confidence scores compared to the weaker student, leading to a favorable, closer approximation of the KL distillation transfer performance for high teacher-student performance differences (c.f. Fig. 4b). We will incorporate this information into the final version of this paper."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4151/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699996236139,
                "cdate": 1699996236139,
                "tmdate": 1699996236139,
                "mdate": 1699996236139,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kpUpgfvN9l",
                "forum": "m50eKHCttz",
                "replyto": "A47g8WVPKs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4151/Reviewer_yxq3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4151/Reviewer_yxq3"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer response"
                    },
                    "comment": {
                        "value": "Thanks for the thorough responses! I appreciate the additional results that demonstrate the confidence-based heuristic works quite well (only 9% incorrect assignments). Overall, my questions/concerns are all addressed."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4151/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700433370207,
                "cdate": 1700433370207,
                "tmdate": 1700433370207,
                "mdate": 1700433370207,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "H4fQ2LLjaV",
            "forum": "m50eKHCttz",
            "replyto": "m50eKHCttz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4151/Reviewer_QCZm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4151/Reviewer_QCZm"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies general knowledge distillation (KD), where, given any two models, the student can infer missing information from the teacher model and potentially improve the performance. The authors begin with a comprehensive analysis showing that such complementary knowledge generally exists in any paired models regardless of model capacity or architecture, and existing KD methods cannot leverage the information that student models already carry, i.e., trained students. To this end, the authors propose a continual learning-based extension to the existing KD methods and a data partitioning scheme that, according to the highest prediction probability., simultaneously maintains the useful knowledge of students while learning from the teacher. The extensive experiments conducted on more than 400 models sufficiently verify the effectiveness and provide many insightful analyses."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The problem studied in this paper, i.e., general knowledge distillation, is interesting and practical. It bridges the gap in the existing literature that a trained student might degrade during the knowledge distillation process.\n\n2. The additional analysis also indicates almost every model could benefit from the other teacher models, even if the teachers are weaker than the students. The result and the evaluation methodology may motivate the community for further research.\n\n3. The proposed method is sound yet easy to implement in practice. The authors also consider different scenarios of distillation, including a single teacher and multiple teachers distilled in different ways/orders.\n\n4. The large-scale analysis (over 400 models) validates the claim and provides various insights, such as the properties of student models."
                },
                "weaknesses": {
                    "value": "1. The student models considered in this paper are relatively strong. As the authors claim **general** knowledge distillation, the readers will also be interested in the weaker models or even from scratch. However, the authors only consider powerful architectures, such as transformers or ResNet, in the paper.\n\n2. The proposed method is slightly confusing to me. My understanding is that the proposed data partition is built upon the continual learning method since Figure 4(a) clusters KL-Dist + DP Transfer into continual learning. If so, the contribution of each component is not clear enough to me. Though some experiments, e.g., Figure 4(b), present the partition improves the MCL transfer, the contribution of the partition itself is not investigated in the experiments.\n\n3 (Minor) Consistency of italic type. Some \"KL+DP\" are italics in the text, while some are not."
                },
                "questions": {
                    "value": "1. What would happen if one applies the proposed method to weaker models, e.g., randomly initialized? I guess it will be degenerated to conventional KD methods. A specific section for weaker/smaller models would be interesting to the community of edge device users/researchers."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4151/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834793664,
            "cdate": 1698834793664,
            "tmdate": 1699636380802,
            "mdate": 1699636380802,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PZEwiEid27",
                "forum": "m50eKHCttz",
                "replyto": "H4fQ2LLjaV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4151/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4151/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Review"
                    },
                    "comment": {
                        "value": "We appreciate the useful and positive feedback and have addressed each question raised individually.\n\n---\n\n__Issue 1__\n> The student models considered in this paper are relatively strong. As the authors claim general knowledge distillation, the readers will also be interested in the weaker models or even from scratch. However, the authors only consider powerful architectures, such as transformers or ResNet, in the paper.\n\nWhile the reviewer is correct in that a larger part of the models available within the timm model library may be considered on the stronger end, we want to highlight that timm generally contains all sorts of models of practical interest. This includes smaller and \u201cweaker\u201d ones as well. We deliberately also included such weaker models in our study, as seen in Supp. Table 6 which contains models varying in both model sizes and performances, with performance gaps of up to 15% and tiny transformer models such as PiT-XS with only 11 Million parameters. __For all these models, we find complementary knowledge to clearly exist (see Fig. 1) and complementary knowledge transfer to still be possible__.\n\nFor very weak or even randomly initialized student models, the setting would transform into a regular knowledge distillation problem. As in this scenario, there would be no benefit in retaining the student\u2019s initial performance, our KL+DP approach would not be feasible. \nFor randomly initialized teachers, we show in Section 3, p.4 that no complementary knowledge exists.\n\nHowever, we do agree that an investigation into well-to-moderately performant models of even smaller size or particular edge-device models could be of high relevance. \nAs we find the model capacity to be a driving factor in the amount of complementary knowledge that can be received (see our small model size scaling laws study in Fig. 5b), finding ways to break this trend for edge-device models would make _a particularly interesting direction for future research to build on_! \n\n---\n\n__Issue 2__\n> The proposed method is slightly confusing to me. My understanding is that the proposed data partition is built upon the continual learning method since Figure 4(a) clusters KL-Dist + DP Transfer into continual learning. If so, the contribution of each component is not clear enough to me. Though some experiments, e.g., Figure 4(b), present the partition improves the MCL transfer, the contribution of the partition itself is not investigated in the experiments.\n\nWe thank the reviewer for the question and will first describe our proposed approach in more detail. In particular, for general knowledge transfer, KL-Dist. + DP Transfer effectively extends the standard vanilla KL-Dist. transfer setting. To protect the students' pretrained knowledge during the transfer, we introduce a \u201cstudent-teacher\u201d model (essentially the unaltered, initial pretrained student, see also Fig. 3). \nThis borrows from replay regularization used in continual learning, where we utilize the student-teacher model to replay the students' previous knowledge. \n\nOur data partitioning scheme comes into play for selecting which samples we use for replay from the student's previous knowledge and for determining where to adapt to the teacher context - either leveraging supervision or utilizing confidence estimates for fully unsupervised transfer. Consequently, we consider the KL+DP approach inspired from the perspective of continual learning. \n\nFurthermore, we provide analyses into the partitioning behavior in Section 5.1, p.8, through our comparison between supervised and unsupervised partitioning in Table 3, where we discover comparable general transfer effectiveness, but at the cost of higher transfer variance (see first paragraph p.8). \n\nFinally, KL+DP does not introduce additional hyperparameters compared to vanilla KL-Distillation, with robustness towards e.g., temperature choices (see Supp. A.3) - as such, we believe essential method ablations that allow us to better understand our proposed method to be covered. We do include an additional experiment studying the sample assignments in unsupervised partitioning in our reply to Reviewer yxq3, where we find unsupervised partitioning to assign samples for the majority of cases correctly (Reply 1).\n\nOverall, we will make sure to better highlight the detailed method aspects as described above in the final version of the paper.\n\n---\n\n__Issue 3__\n> Consistency of italic type. Some \"KL+DP\" are italics in the text, while some are not.\n\nWe thank the reviewer for spotting this! We will ensure our notation typesetting to be fully consistent in the final version of the paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4151/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699995979644,
                "cdate": 1699995979644,
                "tmdate": 1699995979644,
                "mdate": 1699995979644,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bEk5zuOMgc",
                "forum": "m50eKHCttz",
                "replyto": "H4fQ2LLjaV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4151/Reviewer_QCZm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4151/Reviewer_QCZm"
                ],
                "content": {
                    "title": {
                        "value": "Response to the authors"
                    },
                    "comment": {
                        "value": "I would like thank the authors for their good work and the response, which has addressed my concerns. I will keep my score and recommend acceptance."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4151/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700576823685,
                "cdate": 1700576823685,
                "tmdate": 1700576843403,
                "mdate": 1700576843403,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1zrM6BHhkF",
            "forum": "m50eKHCttz",
            "replyto": "m50eKHCttz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4151/Reviewer_epvt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4151/Reviewer_epvt"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the phenomenon that different models have complementary information, reflected in their different predictions on each sample. Such complementary information could be due to model architectures, training settings, etc. Then the authors study how to transfer the complementary knowledge from a teacher model to a student model. The authors formulate this as a continual learning problem, which effectively improves upon the knowledge distillation baseline  and achieves better knowledge distillation on diverse models and ImageNet."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper identifies the \"complementary knowledge\" in different neural networks with grounded evidence. I think the claims are reasonable and well-supported.\n\n2. The authors proposed improvement to the general knowledge distillation approaches from the continual learning perspective, including constraining the weight updates and transfer data. Both of the approaches are reasonable and straightforward to apply.\n\n3. The authors conduct experiments on a wide range of models on ImageNet and show improvement with their improved continual knowledge distillation approach."
                },
                "weaknesses": {
                    "value": "1. Present the key terms more clearly. For example, I haven't found the definition of transfer delta $\\Delta_{transf}$, which is an important evaluation metric.\n\n2. I think the proposed regularization, including the constraining of weight updates and transfer data, are some tricks for knowledge distillation. Although I don't work directly in knowledge distillation and I will wait for other expert reviewers to justify the novelty, I think the authors need to clarify more about their motivation or form a more explicit connection with continual learning. I also raised a question in the next section, elaborating my concerns.\n\n3. I suggest the authors add some more experiments to make the arguments more thorough. Specifically, a more detailed breakdown of the accuracy numbers would help. For instance, (a) the percentage of [teacher correct, student wrong] samples are changed to correct answers after the distillation, (b) the percentage of [teacher incorrect, student correct] samples are changed to incorrect labels, to understand if the transfer is indeed complementary."
                },
                "questions": {
                    "value": "1. Explain more about the terms. The readers are likely to have an ambiguous understanding of them, including: transfer delta $\\Delta_{transf}$, \"available complementary knowledge per class,\" and transfer rate.\n\n2. I am wondering if the methods have to reason from the \"continual learning\" perspective. In my opinion, the regularization techniques proposed by the authors seem like generally applicable tricks for knowledge distillation. If any procedure involving multiple steps has to be treated as continual learning, maybe training a neural network with SGD is also a continual learning process? I hope the authors can clarify this motivation better in the paper.\n\n3. See the third weakness above.\n\n4. I suggest the author add an oracle study to strengthen the argument. In the examples (e.g. Table 1), the final improvement seems small in scale. To argue that this is actually challenging, the authors can run several ensembles of the teacher-student model and compare it to the improvement from knowledge transfer. Of course, I welcome other variants of similar analytical studies from the authors."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4151/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4151/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4151/Reviewer_epvt"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4151/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699297296552,
            "cdate": 1699297296552,
            "tmdate": 1699636380727,
            "mdate": 1699636380727,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VKlxwTATS6",
                "forum": "m50eKHCttz",
                "replyto": "1zrM6BHhkF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4151/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4151/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Review (1/3)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable and positive feedback and address each question raised individually.\n\n---\n\n__Question 1__\n> Explain more about the terms. The readers are likely to have an ambiguous understanding of them, including: transfer delta, \"available complementary knowledge per class,\" and transfer rate.\n\nWe apologize for any confusion regarding the key terms mentioned in our paper. To clarify, the transfer delta, as defined in Section 4.1 on page 5, refers to the difference between the student model's accuracy before and after the knowledge transfer. This metric measures the improvement in top-1 accuracy achieved by transferring knowledge from the teacher to the student. \nAdditionally, the term available complementary knowledge per class refers to the number of positive prediction flips within each class, as introduced in Section 3 on Page 4. \n\nFurthermore, in Section 5.1 on Page 8, we define the transfer rate as the student model's improvement in top-1 accuracy relative to the available complementary knowledge. In other words, the transfer rate indicates how much of the complementary knowledge is actually transferred to the student model. \n\nWe understand that these definitions may have been insufficiently highlighted. We will improve and emphasize these for the final version through better visual separation and the above-provided additional explanations.\n\n---\n\n__Question 2__\n> I am wondering if the methods have to reason from the \"continual learning\" perspective. In my opinion, the regularization techniques proposed by the authors seem like generally applicable tricks for knowledge distillation. If any procedure involving multiple steps has to be treated as continual learning, maybe training a neural network with SGD is also a continual learning process? I hope the authors can clarify this motivation better in the paper.\n\nContinual Learning generally comprises scenarios in which a model is exposed to context in a sequential, continual manner (e.g., images and associated class labels presented through a datastream). This means that a model will no longer have access to (and be able to train on) parts of this context after certain time- or training steps. As it continuously trains and learns new things, catastrophic forgetting of previous context knowledge occurs (c.f., e.g. [1,2,3]). This differs from regular SGD-style network training, where the same data can be accessed throughout the training process.\n\nIn our particular case, we tackle the general knowledge transfer problem from the perspective of continual learning because crucial analogies can be drawn between both domains (see Section 4.2, p. 5-6): While transferring knowledge between trained models, we need to avoid catastrophic forgetting of previously learned feature context, while being able to incorporate complementary knowledge.\nThis constraint is also significantly different from standard knowledge distillation, in which the idea and aspects of knowledge retention are not considered (see Section 4.1, p. 5). Consequently, standard knowledge distillation tools are unsuitable for general knowledge transfer between pretrained models, as can be seen experimentally, e.g., in Fig. 4a or 4b.\n\nHowever, we agree that this connection can be carved out more clearly, and we will incorporate the explanation above into the introductory text in Section 4.2.\n\n[1] Kirkpatrick et al. 2017, \u201cOvercoming catastrophic forgetting in neural networks\u201d  \n[2] Zenke et al. 2017, \u201cContinual Learning through Synaptic Intelligence\u201d  \n[3] Buzzega et al. 2020, \u201cDark Experience for Continual Learning: a Strong, Simple Baseline\u201d"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4151/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699995307728,
                "cdate": 1699995307728,
                "tmdate": 1699995307728,
                "mdate": 1699995307728,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vQ5mq1bKYd",
                "forum": "m50eKHCttz",
                "replyto": "1zrM6BHhkF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4151/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4151/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "As the discussion period is closing soon, we would like to thank the reviewer again for their helpful feedback. We hope that our replies, additional results and our updated draft addressed all concerns, and that we better highlighted all the contributions included in this work. Of course, we are happy to continue the discussion in case there are any further questions."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4151/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658672915,
                "cdate": 1700658672915,
                "tmdate": 1700658672915,
                "mdate": 1700658672915,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "k9d69AXIWO",
            "forum": "m50eKHCttz",
            "replyto": "m50eKHCttz",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4151/Reviewer_LBqG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4151/Reviewer_LBqG"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates if it is possible to transfer complementary knowledge from one model to another without performance degradation. To this end, authors propose different heuristics to design how to switch the knowledge transfer between models. Experiments with various pairs of models demonstrate the effectiveness of the model-agnostic knowledge transfer."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The writing is clear and easy to follow.\n\n2. There are consistent performance improvements compared to different types of baselines."
                },
                "weaknesses": {
                    "value": "1. As addressed by the authors, different models are trained with different data augmentations, architectures and optimization techniques. The performance improvements are relatively marginal (e.g., Table 1), especially considering some models are not fully trained.\n\n2. In Table 4, do the authors train all the variations with the same number of training steps? The sequential knowledge transfer may benefit from more training steps."
                },
                "questions": {
                    "value": "See the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4151/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4151/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4151/Reviewer_LBqG"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4151/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699408735416,
            "cdate": 1699408735416,
            "tmdate": 1699636380669,
            "mdate": 1699636380669,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xvy3gJgDeN",
                "forum": "m50eKHCttz",
                "replyto": "k9d69AXIWO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4151/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4151/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Review"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback and address each question raised individually. \n\nBefore doing that, we would just like to highlight that our paper is not just a simple benchmark comparison but rather a fundamental, proof-of-concept study on the prospects of general knowledge transfer (as also highlighted by the other reviewers), for which we not only propose an easy-to-utilize approach but provide the inherent initial motivation and support, alongside analyses on challenges and shortcomings.\n\n---\n\n__Question 1__\n> As addressed by the authors, different models are trained with different data augmentations, architectures and optimization techniques. The performance improvements are relatively marginal (e.g., Table 1), especially considering some models are not fully trained.\n\nOur experiments do involve the use of models __fully pre-trained__ on ImageNet from the open-source model repository timm - these are extensively trained with hyperparameter tuning to optimize performance. \nAt the same time, a gain of even a full percentage point on ImageNet-1k is generally considered significant - which we obtained _without any additional training data or optimization protocol changes and which holds even for already strong pretrained student models_.\n\nThis is particularly relevant as we can achieve positive gains for __nearly all arbitrary model pairings__. In addition, the visualized model pairings were chosen by virtue of coverage of different model type pairings, as opposed to peak transfer performance, in order to show the generality of our proposed general knowledge transfer mechanism.\nHowever, as this work primarily serves as a proof-of-concept of both the existence and possibility of general knowledge transfer, we do agree that certainly further gains can be achieved, which we leave to future research to tackle.\n\n---\n\n__Question 2__\n> In Table 4, do the authors train all the variations with the same number of training steps? The sequential knowledge transfer may benefit from more training steps.\n\nAll experiments, including those in Table 4, use a fixed budget of 20 epochs to transfer teacher knowledge to the student. We have chosen this fixed budget as a practical estimate over which transfer generally converges. However, we have found that this is not the primary factor driving performance gains in the multi-teacher setting. As shown in Fig. 11, the student's performance improves within the first few epochs of transfer with each teacher model and quickly converges after that.\n\nIn the sequential multi-teacher setting, this fixed transfer budget applies to each transfer step, which increases the overall number of transfer steps. \nIn this study, we also tested fixed global numbers of transfer steps equal to the budget in the single-teacher scenario and experimented with increasing the number of transfer steps in the single-teacher setting. However, we found that these changes did not affect the observations reported in our paper.\n\nWe will highlight these additional details, as described above, in the final version of this paper."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4151/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699995082483,
                "cdate": 1699995082483,
                "tmdate": 1699995123100,
                "mdate": 1699995123100,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "js5PxGqmWK",
                "forum": "m50eKHCttz",
                "replyto": "k9d69AXIWO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4151/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4151/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "As the discussion period is closing soon, we would like to thank the reviewer again for their helpful feedback. We hope that our reply and our updated draft addressed all concerns, and that we better highlighted all the contributions included in this work. Of course, we are happy to continue the discussion in case there are any further questions."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4151/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658621062,
                "cdate": 1700658621062,
                "tmdate": 1700658621062,
                "mdate": 1700658621062,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]