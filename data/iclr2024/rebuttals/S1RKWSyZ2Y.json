[
    {
        "title": "Guiding Instruction-based Image Editing via Multimodal Large Language Models"
    },
    {
        "review": {
            "id": "L0XYAyUFY6",
            "forum": "S1RKWSyZ2Y",
            "replyto": "S1RKWSyZ2Y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission485/Reviewer_TXUn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission485/Reviewer_TXUn"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces an image editing method guided by MLLM. This approach can learn from expressive instructions and offer explicit guidance. Comprehensive experiments demonstrate the method's effectiveness."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is clearly written.\n\nThe introduced technique.is novel and interesting.\n\nThe experiments are sufficiently conducted.\n\nThe presented results seems promising."
                },
                "weaknesses": {
                    "value": "While certain aspects of the work might appear less novel, its practical effectiveness compensates for this.\n\nThere are typographical errors. Specifically, \"Methods\" in Tables 1 and 2 should be placed at the top."
                },
                "questions": {
                    "value": "I am curious about the method's potential in handling tasks like:\n\n1. Transferring image texture. Language-Driven Artistic Style Transfer, ECCV 2022.\n\n2. Modifying an object's color. L-CAD: Language-based Colorization with Any-level Descriptions using Diffusion Priors, NeurIPS 2023.\n\n3. Interpreting user-provided emotions. Affective Image Filter: Reflecting Emotions from Text to Images, ICCV 2023.\n\nIncluding comparisons or referencing these studies could further enrich the paper's depth and context."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission485/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission485/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission485/Reviewer_TXUn"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission485/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698002595166,
            "cdate": 1698002595166,
            "tmdate": 1699635975256,
            "mdate": 1699635975256,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TnO2ajyFPh",
                "forum": "S1RKWSyZ2Y",
                "replyto": "L0XYAyUFY6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission485/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission485/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank reviewer TXUn for acknowledging our practical effectiveness.\n\n> **Q) typographical errors**\n\nThanks for the debugging! We have updated all Tables in our revision.\n\n> **Q) transferring texture/color/emotion**\n\nWe attempt to perform such transfers in Sec. A (Transferring Image Texture/Color/Emotion). For texture, we follow CLVA [1] and adopt the style prompt \"*make the whole image as texture [ins]*\". InsPix2Pix can only do limited transfer, but MGIE shows clear visual attributes (e.g., \"*orange*\" or \"*pinkish*\") as well as the complex \"*colorful circular round*\". We perform fine-grained color manipulation, including \"*glasses frame*\" or \"*hair*\". However, the baseline even alters the whole color. For global colorization [2], both InsPix2Pix and our MGIE cannot present appealing results, which indicates the need for fine-tuning. Transferring the emotion is more challenging as the model has to perceive the latent semantics. We are able to illustrate the visual concept of \"*bright day*\" or \"*chaotic and confused*\" as the beach in the early morning or the gloomy street at night. MGIE can also transform from the cozy snowy day into suspenseful and thrilling through \"*nightmare and scared*\". Although exhibiting promising potential, it still requests more profound texture/emotion perception for each specific goal. We leave them as future research for creative visual editing [3].\n\n+ [1] (ECCV'22) Language-Driven Artistic Style Transfer\n+ [2] (NeurIPS'23) L-CAD: Language-based Colorization with Any-level Descriptions using Diffusion Priors\n+ [3] (ICCV'23) Affective Image Filter: Reflecting Emotions from Text to Images"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission485/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700351066984,
                "cdate": 1700351066984,
                "tmdate": 1700351107912,
                "mdate": 1700351107912,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iG3ldOTm3e",
                "forum": "S1RKWSyZ2Y",
                "replyto": "TnO2ajyFPh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission485/Reviewer_TXUn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission485/Reviewer_TXUn"
                ],
                "content": {
                    "comment": {
                        "value": "I extend my thanks to the authors for their diligent efforts. Based on the merits of their work, I am inclined to maintain my rating in favor of acceptance."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission485/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700567368347,
                "cdate": 1700567368347,
                "tmdate": 1700567368347,
                "mdate": 1700567368347,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qTpbcoBQBk",
            "forum": "S1RKWSyZ2Y",
            "replyto": "S1RKWSyZ2Y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission485/Reviewer_RXEx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission485/Reviewer_RXEx"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new instruction-based image editing approach. The Multimodal large language models (MLLMs) are incorporated into the editing, formulating MLLM-Guided Image Editing (MGIE). Different editing operations are utilized for testing, including Photoshop-style modification, global photo optimization, and local editing."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed method can have effective performance on the chosen datasets of evaluation. Especially, a lot of evaluations are conducted with subjective assessment."
                },
                "weaknesses": {
                    "value": "1.\tThe performance is limited by the dataset utilized for training, especially for learning effective instruction and editing performance. Will this approach complete the editing operation which is not appeared in the training data?\n\n2.\tThe template of this paper is wrong, it is still the template of ICLR2023.\n\n3.\tThere is no ablation analysis for the loss functions and their alternatives. \n\n4.\tThe editing effects are not ideal, for example, the woman in the background of Fig. 1 is removed, while there are still some residuary artifacts in the background."
                },
                "questions": {
                    "value": "1.\tHow to balance the loss weights of $\\mathcal{L}_{ins}$ and $\\mathcal{L}_{edit}$ in Eq. 6?\n\n2.\tCan this approach add new objects into the image?\n\n3.\tWhat are the details of the user study\u2019s participants? Like their ages, educations, and the time for taking part in the user study."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The generated image may be harmful to the protection of copyright."
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission485/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698504298543,
            "cdate": 1698504298543,
            "tmdate": 1699635975173,
            "mdate": 1699635975173,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "y7e888aTks",
                "forum": "S1RKWSyZ2Y",
                "replyto": "qTpbcoBQBk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission485/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission485/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank reviewer RXEx for enjoying our proposed method.\n\n> **Q) unseen editing operation**\n\nWe first want to clarify that MGIE is not limited by the training dataset. In IPr2Pr, there is no removal or photo optimization. We have appended such cases in Sec. A (Unseen Editing Operation). InsPix2Pix has failed due to the shortage of training examples. In contrast, our MGIE is able to handle such editing via the visual-aware derivation of MLLM. We can accurately remove \"*the boy in the red shirt*\" or \"*lighten out the yellow tone*\", which demonstrates better generalizability for unseen operations. More qualitative comparisons can be found on our project website (https://mllm-ie.github.io).\n\n> **Q) ICLR template**\n\nThanks for the notification! We have updated the ICLR 2024 template in our revision.\n\n> **Q) ablation study of training loss**\n\nWe have added the study in Sec. A (Ablation Study of Training Loss). There are two training losses, instruction loss $L_\\text{ins}$ and editing loss $L_\\text{edit}$, in our MGIE. $L_\\text{edit}$ is necessary for training to produce the editing result. Without $L_\\text{ins}$, it will derive full but lengthy guidance to lead $L_\\text{edit}$. However, both LGIE and MGIE drop significantly; LGIE even performs worse than the baseline. This underscores the prominence of learning concise expressive instructions, which offer succinct and relevant guidance. Besides, lengthy instructions via the MLLM will incur additional overhead (29.4 vs. ours 9.2), resulting in an inefficient inference.\n\n| Method | Setting | Ma5k | | MagicBrush | | |\n| :-- | :-- | :-- | :-- | :-- | :-- | :-- |\n| | | **SSIM\u2191** | **LPIPS\u2191** | **DINO\u2193** | **CVS\u2191** | **CTS\u2191**\n| InsPix2Pix | | 58.92 | 0.359 | 71.46 | 85.22 | 29.34 |\n| LGIE | - $L_\\text{ins}$ | 57.59 | 0.386 | 70.79 | 83.21 | 28.66 |\n| | + $L_\\text{ins}$ | **64.60** | **0.327** | **80.90** | **88.87** | **30.10** |\n| MGIE | - $L_\\text{ins}$ | 58.18 | 0.365 | 71.50 | 85.19 | 29.11 |\n| | + $L_\\text{ins}$ | **66.25** | **0.298** | **82.22** | **91.14** | **30.40** |\n\n> **Q) editing effects are not ideal**\n\nWe also observe these artifacts. Though the editing results are not perfect, MGIE can effectively remove the target, compared to the failed InsPix2Pix. Perhaps a step-by-step pipeline (e.g., removal followed by inpainting) could deal with such issues.\n\n> **Q) balance between $L_\\text{ins}$ and $L_\\text{edit}$**\n\nAs Eq. 6, the overall training loss is $L_\\text{ins} + 0.5 \\cdot L_\\text{edit}$. We have tried different weights (e.g., 1.0), but the performances are all similar.\n\n> **Q) add new objects**\n\nWe have appended such examples in Sec. A (Adding New Object). MGIE also supports adding new objects that are not present in the input and placing them in reasonable positions. For example, the \"*hat*\" is put on the girl's head, and the \"*river*\" is added along with the grass. More surprisingly, the appended \"*fireworks*\" further makes the beach colorful, which drives the night scene coherent and visually appealing.\n\n> **Q) details of user study**\n\nWe have updated the details in Sec. B (Human Evaluation). We sample 100 examples (25 for each dataset) to conduct our human evaluation. Each task is assigned 3 annotators, who rank across baselines and our MGIE, to avoid potential bias. We require workers to have a 97% approval rate and over 500 approved tasks to ensure quality. The worker is awarded $5 for each task (5 examples) and takes 21 minutes on average to complete."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission485/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700352838197,
                "cdate": 1700352838197,
                "tmdate": 1700352838197,
                "mdate": 1700352838197,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JupnXBOgM2",
            "forum": "S1RKWSyZ2Y",
            "replyto": "S1RKWSyZ2Y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission485/Reviewer_fMJn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission485/Reviewer_fMJn"
            ],
            "content": {
                "summary": {
                    "value": "This submission proposes an approach towards improving the visual output quality of instruction-based image editing. Authors put forward the hypothesis that natural text-based edit instructions, provided by humans, are oft terse and this hinders contemporary learning-based models from successfully capturing and dependably following intended image-edit meanings. \n\nThe crux of the proposed strategy involves learning to map terse input text, representing image edit-instructions, to more expressive (evidently more verbose) output text, in order to provide more explicit image-editing guidance. The work explores the efficacy of leveraging cross-modal capabilities, contemporary language-models, and sensitivity to visual inputs in order to positively influence edit-instruction quality. This multi-modal component is evidenced to be important for text-edit quality and resulting downstream edited images. Quantitative and qualitative evaluation of the proposed strategy, across various metrics and human rankings, are reported in comparison with recent work and a relevant baseline, across four benchmark datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The problems being addressed here are real and are important -- principled solutions and progress towards techniques capable of realising consistently high quality and complex (intended) image edits, that are also of low cost in terms of required human effort, will be of high value to the community and will additionally result in widely applicable and practical end-user benefits.  \n\nThe technical components of the work, the manner in which the various components are concatenated together, appears reasonably straightforward. Explanation of the guided image editing strategy is aided by basic yet clear schematics to aid understanding (Fig. 2, specifically). \n\nThe investigation can be considered reasonably thorough with the inclusion of experimental work covering (i) method efficacy (quant. & qual.), (ii) vision & language hyper-parameter sensitivity, (iii) ablating instruction generation components, (iv) compute. Reported qualitative results are quite compelling and show some good perceptual improvements over baselines. Further results (public anon-web page, supp. materials) are appreciated and provide some aid for apprehensions over method robustness, reliability.\n\nWriting is of a reasonable standard in general. I enjoyed reading the paper."
                },
                "weaknesses": {
                    "value": "Leveraging multi-modal language-models provides an intuitively promising avenue, when striving to follow image-manipulation intentions that are defind by only terse human text instruction. Good evidence is provided that the presented strategy goes some way to tackling the observed short-comings however technical contributions (size, sufficiency) can be regarded as moderate. The overarching system makes use of an array of pre-existing components however the particular implementation that facilitates component concatenations can be regarded as somewhat novel.\n\nFor this venue, an important factor that would strengthen the submission might look to provide additional understanding, discussion on the long-term sustainability of the proposed style of approach. The requirement for intermediary 'on-the-fly' remappings, reconfigurations of text input cannot be regarded as a very elegant or pleasing solution. I explicitly note that this is not grounds for rejection, my point is rather that further thinking, discussion on the fundamental gaps that prevent base models from correctly realising terse (yet human-parseable) instruction would likely prove elucidating and of high value. An empirical avenue here might involve investigating pre- and post-hoc parts-of-speech (POS) distributions, or other statistical evaluation of the edit-text distributions. Is the long-term goal, alternatively, to fashion single models, capable to understand natural (terse) human instruction? The point touches on well-understood (dis-)advantages of modular-component systems c.f. end-to-end.\n\nI would be keen for authors to discuss these points and I am open to modifying my score. \n\nMinor suggestions:\n\n1. Authors may wish to update to an ICLR24 template (c.f. ICLR23) \n\n2. On pp.2: suggest explicitly expand acronym 'IPr2Pr' on first use (presumably Instruction Prompt-2-Prompt ?)\n\n3. Precision of some phrasing could be tightened, towards aiding reading understanding. See example suggestions in following section. \n\n4. The (useful!) model schematic image, found between Figure 3 and Figure 4, is unnumbered. Is this by design ? Suggest 'Baselines' paragraph might serve as suitable caption content."
                },
                "questions": {
                    "value": "* Can the authors commit to a full source code release? The submission opted to stay silent on this issue. Code release would be of clear benefit to the community aiding reproducibility, public probing of method robustness, consistency, range of model abilities. This will undoubtedly increase the value of the contributions. If impossible; web page might be extended to allow public inference-time testing.\n\n* The property of 'elaborate descriptions' is posed as something one wishes to evade in natural commands and yet 'explicit yet detailed guidance' is conversely noted a sought after notion. Do the authors suggest that the former pertains largely to image space, in a similar fashion to e.g. regional masks? Concrete examples of the considered 'elaborate descriptions' may help to clarify this point.\n\n* How is the phrase 'reasonable image editing' to be defined?\n\n* Minor: Small icons (flame, blue cube) tagged to components in the system schematic (Fig.2) presumably represent learnable and frozen model components, respectively. Suggest to make this key explicit."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Authors provide a brief yet reasonable discussion on ethical issues and limitations in their supp. materials. I have no noteworthy outstanding ethical concerns."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission485/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698692057168,
            "cdate": 1698692057168,
            "tmdate": 1699635975096,
            "mdate": 1699635975096,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Kx7rgco5yu",
                "forum": "S1RKWSyZ2Y",
                "replyto": "JupnXBOgM2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission485/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission485/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank reviewer fMJn for appreciating the conducted results and our paper writing.\n\n> **Q) technical contribution**\n\nWe first want to highlight that our deriving human instructions into informative descriptions is a brand-new attempt, which yields surprising enhancements. Though our MGIE is a joint framework with the MLLM and the diffusion model, we want to emphasize that our contribution includes:\n+ an end-to-end pipeline to learn/incorporate explicit yet concise guidance;\n+ an investigation on how to utilize expressive instructions (Table 3);\n+ a comprehensive automatic/human study covering diverse editing aspects.\n\nAll of these are valuable contributions to controllable image editing research.\n\n> **Q) intermediary on-the-fly remappings**\n\nWe believe that deriving expressive instructions is crucial for enhancing image editing. In Table 5, we demonstrate that replacing the CLIP encoder with powerful LLMs only results in limited improvement. Since LLMs are trained unidirectionally (decoder-only), the hidden states will lack sufficient/concrete information unless we derive the subsequent narration. On the other hand, expressive instructions can also reveal how the model modifies the input image, aiding in the interpretation of the editing result.\n\n> **Q) part-of-speech distribution**\n\nWe investigate part-of-speech (POS) distributions of input instructions and our derived expressive instructions in Sec. A (Part-of-Speech Distribution). In general, input instructions involve more nouns but fewer adjectives. In contrast, our expressive instructions can portray concrete edited scenes in detail via more adjectives. The original instructions are also dominated by verbs, which are challenging to perceive. The derivation helps them to be more understandable as adverbs. Moreover, we effectively decrease the number of ambiguous pronouns. More than 68% of pronouns (only 13% in our expressive instructions) are unresolvable in input instructions, where the model can not have explicit goals.\n\n| Instruction | adjective | noun | pronoun | adverb | verb |\n| :-- | :-- | :-- | :-- | :-- | :-- |\n| Input | 8.9 | 53.1 | 9.9 | 0.9 | 27.2 |\n| Expressive | 24.1 | 43.7 | 4.5 | 4.4 | 23.3 |\n\n> **Q) suggestions**\n\nThanks for the notification! We have updated the ICLR 2024 template as well as \"Instruction Prompt-to-Prompt\" for IPr2Pr in our revision. We also add the caption (overview architectures of baselines) to Figure 4. \n\n> **Q) code release**\n\nYes. We will release the code/checkpoints after the review process. We also attach a copy of our codebase in the appendix.\n\n> **Q) elaborate descriptions**\n\nSorry for the misunderstanding. Those previous methods (e.g., Text2Live [1] and Null-Inv [2]) require human-provided descriptions, which are labor-intensive and not intuitive, compared to edit instructions. For example, to manipulate an image of \"*a girl is walking at the beach*\". The user needs to specify \"*a girl with a hat is walking at the beach*\", instead of the more straightforward \"*give her a hat*\". In contrast, our MGIE can automatically derive the concrete guidance \"*the girl will wear a hat and walk at the beach*\", where elaborate descriptions are no longer necessary in this scenario. This can significantly enhance the accessibility of the edit instruction.\n+ [1] (ECCV'22)Text2LIVE: Text-Driven Layered Image and Video Editing\n+ [2] (CVPR'23) Null-text Inversion for Editing Real Images using Guided Diffusion Models\n\n> **Q) reasonable image editing**\n\nWe call it \"*reasonable*\" since our model can follow the reasoning from the MLLM. MGIE leverages the derived expressive instruction to perform related image manipulation, where the guidance from the MLLM can offer a reasonable editing goal through powerful language modeling. For example, \"*make it more healthy*\" for a pizza can be perceived as \"*need more vegetables*\"; \"*add fireworks*\" will alter not only the sky but also the reflection on the ground. We calculate the CLIP-Score between edited results and their input/expressive instructions. Our MGIE achieves better alignment than the baseline and shows an even higher correlation with expressive instructions. This indicates that we indeed follow the MLLM and benefit from its reasoning capability. Looking ahead, one potential direction is to adopt the recent GPT-4V [1] to evaluate the actual reasonableness.\n\n| Method | CLIP-S | EVR | GIER | MA5k | MagicBrush |\n| :-- | :-- | :-- | :-- | :-- | :-- |\n| InsPix2Pix | input instruction | 21.19 | 20.36 | 20.31 | 22.58 |\n| MGIE | input instruction | 23.03 | 21.74 | 21.89 | 24.09 |\n| | expressive instruction | 28.33 | 28.25 | 27.69 | 28.37 |\n\n+ [1] (arXiv'23) The Dawn of LMMs: Preliminary Explorations with GPT-4V(vision)\n\n> **Q) flame/ice icons**\n\nThanks for the nice suggestion! We have added it to the caption of Figure 2."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission485/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700354059541,
                "cdate": 1700354059541,
                "tmdate": 1700354059541,
                "mdate": 1700354059541,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lTYEOJtrY7",
                "forum": "S1RKWSyZ2Y",
                "replyto": "on7wtl0XbX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission485/Reviewer_fMJn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission485/Reviewer_fMJn"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their detailed rebuttal.\n\nIn particular; additional experimental investigations (POS, CLIP-S) and explicit clarifications on terminology ('elaborate descriptions', 'reasonable image editing') are appreciated.\n\nManuscript updates with respect to reviewer BZAL's (valid and somewhat shared) concerns are also noted. Updates related to (i) clear acknowledgement of Koh et al. [1], (ii) multiple random seeds, (iii) description-based baselines, all serve to strengthen the paper.\n\nIn summary I consider reviewer comments well addressed and remain positive about this submission. I lean towards acceptance.\n\n\n1. Koh et al. 'Generating Images with Multimodal Language Models'. NeurIPS 2023"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission485/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588566747,
                "cdate": 1700588566747,
                "tmdate": 1700588566747,
                "mdate": 1700588566747,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xfkinZm6um",
            "forum": "S1RKWSyZ2Y",
            "replyto": "S1RKWSyZ2Y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission485/Reviewer_BZAL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission485/Reviewer_BZAL"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel text-guided image editing method that builds on the instructpix2pix model, which fine-tunes a diffusion model on instruction-image pairs. Authors propose to leverage a multimodal (image + text) large language model to generate more precise and expressive editing instructions, and improve editing ability. The method is trained on the instructpix2pix dataset, and evaluated on 4 datasets, considering different types of edits."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Image editing is a timely and challenging task. Authors demonstrate that they are able to successfully carry out a large set of edit types across multiple datasets. Visual results look very promising, and suggest that the proposed changes can yield noticeable gains over instructpix2pix The evaluation is detailed and the method is analysed from a lot of different angles.\n\nThe idea of replacing the CLIP encoder with a more expressive vision-language model is sound, as providing more detailed instructions can help guide the diffusion model towards the desired output. Authors have made efforts to go beyond simple replacement of the text encoder and added functionality (summarization, adaptation to visual content) that improve performance."
                },
                "weaknesses": {
                    "value": "My main concern with this work is the limited novelty. The crux of the innovation is the use of a multimodal language model instead of a CLIP model in the instructpix2pix setting. The second main innovation is the introduction of [IMG] tokens, which are processed by a transformer head to generate conditioning embedding for a LDM model. This approach is very strongly inspired from Koh et al. 2023, where they train a language model to generate image tokens, which are then transformed via a transformer architecture, and used as conditioning for stable diffusion based generation. The source of inspiration should be credited more clearly (the main reference in the method section simply mentions using a similar feature extractor architecture).\n\nThe presentation of the paper could be improved as well.  The paper is written in a confusing way, and lacks explanation and justifications for design decisions. For example, equation (5) is introduced without any justifications or intuitive explanation, and author do  not explain what they refer to as by score. Similarly, what authors refer to as MLLMs (pre-trained LLMs adapted to take visual inputs as well) is not clearly defined until section 3.1. Multimodal language models can be designed and trained in different ways (e.g. trained with vision-text inputs jointly), and authors should clarify that they refer to a specific type of models. Similarity, the edit head T was not clearly explained, I needed to read the GILL paper (Koh et al.) to understand how these features were generated. Another example is figure 2, which is mentioned at the beginning of chapter 3.2 and shows a MLLM* model without explanation, while this model is only introduced (in a footnote) in a later paragraph. \n\nWhile the evaluation is detailed with experiments carried out on many datasets, state of the art references are limited. The only pre-existing work that authors compare to is instructpix2pix, while the LGIE baseline is an overly poor baseline, which is expected to perform worse than any model with vision-language mappings (e.g. CLIP) in a lot of settings. It does not make a lot of sense to ask a pure language model to hallucinate detailed descriptions of an unseen image. Evaluations on prompt-based editing methods are available on the magic brush dataset, and could provide additional context. For consistency, instructions could easily be converted to a prompt using an LLM."
                },
                "questions": {
                    "value": "- Image editing performance can be influenced by the random seed. Results in tables 1-2 compared to results reported in Zhang et al (MagicBrush) show that there can be a noticeable difference for some metrics (e.g. 70->74 for DINO score on instructpix2pix). Several of these evaluations show performance scores that are relatively close, have authors investigated how consistent these rankings are across multiple seeds? \n\n- Are ground truth images (post edit) available for all datasets? Why were these specific sets of evaluation metric chosen? Why not e.g. measure image quality using FID?\n\n- Authors compare inference times, but do not mention training times. How much more more expensive is it to train this new model compared to pix2pix? This is relevant as noticeable performance gains can be observed when fine-tuning on a specific instruction/edit types."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission485/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698764294063,
            "cdate": 1698764294063,
            "tmdate": 1699635975014,
            "mdate": 1699635975014,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "e4WcYfBB5M",
                "forum": "S1RKWSyZ2Y",
                "replyto": "xfkinZm6um",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission485/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission485/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank reviewer BZAL for the valuable feedback and underscoring our promising quantitative/qualitative study.\n\n> **Q) novelty of MGIE**\n\nThank you for pointing this out! Our MGIE is motivated by GILL [1], and we have made the citation clear in our revision. Despite adopting a similar framework, we are the first to edit an existing image via the MLLM+Diffusion pipeline, rather than creating a new but distinct one. We also want to highlight our deriving human instructions into informative descriptions is a novel attempt at controllable image editing. For the experiments, we investigate the best way to utilize our derived expressive guidance and conduct a comprehensive study of diverse editing aspects, all of which are valuable contributions to this research field.\n\n+ [1] (NeurIPS'23) Generating Images with Multimodal Language Models\n\n> **Q) presentation of the model and experimental setting**\n\nThanks for the constructive feedback! We have appended the detailed experimental setup in Sec. B (Edit Head / Editing Loss). For the MLLM, we first want to clarify that our framework is model-agnostic and compatible with various architectures (as long as they can take visual inputs and provide responses through language modeling). As we are using LLaVA [1] in our implementation, we have made it clearer in Eq. 1. We also added the explanatory footnote of MLLM* to the same page as Figure 2. \n+ [1] (NeurIPS'23) Visual Instruction Tuning\n\n> **Q) description-based baselines**\n\nWe have involved the results of description-based baselines (TEXT2Live [1] and Null Text Inversion [2]) in Sec. A (Comparison to Description-based Baselines).  We leverage GIT [3] to caption the input image as its input description and ChatGPT to merge the edit instruction as the goal description via the prompt \"*Combine two sentences A: [description] and B: [instruction] into a single sentence. The output should be at most similar to sentence A*\". For instance, \"*a girl is walking at the beach*\" and \"*give her a hat*\" will be merged into \"*a girl with a hat is walking at the beach*\". For MagicBrush, we directly apply their released descriptions instead. Text2LIVE and Null-Inv only yield feasible results on the traditional L1 distance but are obviously inferior to our MGIE on semantic-level evaluations (e.g., lower CVS), which supports that they cannot present concrete editing results and carry out goal descriptions well. On the other hand, both count on inference optimization (CLIP alignment and DDIM inversion), which takes more than 200 seconds (vs. ours 9.2 seconds) for each editing task.\n\n| Method | EVR | | | GIER | | | MA5k | | | MagicBrush | | | |\n| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |\n| | **L1\u2193** | **DINO\u2191** | **CVS\u2191** | **L1\u2193** | **SSIM\u2191** | **CVS\u2191** | **L1\u2193** | **SSIM\u2191** | **LPIPS\u2193** | **L1\u2193** | **DINO\u2191** | **CVS\u2191** | **CTS\u2191** |\n| Text2LIVE | 0.169 | 66.19 | 78.22 | **0.126** | 58.32 | 79.32 | 0.165 | 57.62 | 0.342 | **0.071** | **83.35** | 89.71 | 23.59 |\n| Null-Inv | 0.174 | 69.24 | 78.35 | 0.149 | 58.24 | 82.33 | 0.179 | 61.36 | 0.335 | 0.073 | 81.72 | 87.24 | 27.62 |\n| InsPix2Pix | 0.189 | 67.82 | 81.38 | 0.144 | 57.51 | 86.63 | 0.176 | 58.92 | 0.359 | 0.101 | 71.46 | 85.22 | 29.34 |\n| MGIE | **0.163** | **71.49** | **81.73** | 0.135 | **59.24** | **88.59** | **0.133** | **66.25** | **0.298** | 0.082 | 82.22 | **91.14** | **30.40** |\n\n+ [1] (ECCV'22) Text2LIVE: Text-Driven Layered Image and Video Editing\n+ [2] (CVPR'23) Null-text Inversion for Editing Real Images using Guided Diffusion Models\n+ [3] (TMLR'22) GIT: A Generative Image-to-text Transformer for Vision and Language"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission485/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700355281822,
                "cdate": 1700355281822,
                "tmdate": 1700358750354,
                "mdate": 1700358750354,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WxzdNLM3sJ",
                "forum": "S1RKWSyZ2Y",
                "replyto": "xfkinZm6um",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission485/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission485/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> **Q) multiple random seeds**\n\nThanks for the nice suggestion! We have updated the empirical results in our revision, where all evaluations are averaged from 5 random seeds. The statistics variance are shown below. The rankings and observations are still consistent with our previous conclusions.\n\n| Method | EVR | | | GIER | | | MA5k | | | MagicBrush | | | |\n| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |\n| **Zero-shot** | **L1\u2193** | **DINO\u2191** | **CVS\u2191** | **L1\u2193** | **SSIM\u2191** | **CVS\u2191** | **L1\u2193** | **SSIM\u2191** | **LPIPS\u2193** | **L1\u2193** | **DINO\u2191** | **CVS\u2191** | **CTS\u2191** |\n| InsPix2Pix | 0.189\u00b10.013 | 67.82\u00b13.09 | 81.38\u00b13.58 | 0.144\u00b10.007 | 57.51\u00b11.51 | 86.63\u00b11.44 | 0.176\u00b10.005 | 58.92\u00b11.15 | 0.359\u00b10.071 | 0.101\u00b10.009 | 71.46\u00b13.12 | 85.22\u00b13.59 | 29.34\u00b10.30 |\n| LGIE | **0.159**\u00b10.017 | 69.71\u00b12.89 | **82.04**\u00b11.66 | 0.152\u00b10.014 | 56.86\u00b12.37 | 86.99\u00b12.14 | 0.144\u00b10.017 | 64.60\u00b11.88 | 0.327\u00b10.006 | 0.084\u00b10.041 | 80.90\u00b12.07 | 88.87\u00b11.79 | 30.10\u00b10.29 |\n| MGIE | 0.163\u00b10.008 | **71.49**\u00b13.83 | 81.73\u00b12.99 | **0.135**\u00b10.011 | **59.24**\u00b10.72 | **88.59**\u00b10.75 | **0.133**\u00b10.018 | **66.25**\u00b12.41 | **0.298**\u00b10.053 | **0.082**\u00b10.016 | **82.22**\u00b12.70 | **91.14**\u00b14.06 | **30.40**\u00b10.31 |\n\n> **Q) evaluation metrics**\n\nYes. All ground-truth goal images are available in EVR, GIER, MA5k, and MagicBrush. For Photoshop-style modification (EVR and GIER), we adopt semantic-level visual similarity DINO and CVS. Since there are many photo optimization cases in GIER, we also consider SSIM. MA5k aims to adjust the contrast/brightness/saturation, where DINO and CVS cannot clearly tell the difference; Hence we leverage the widely-used style distance LPIPS. As we have the ground-truth description in MagicBrush, we follow the additional CTS to evaluate the alignment between goal captions and edited images.\n\n> **Q) FID score**\n\nWe calculate the FID score between ground-truth and edited images in Sec. A (Evaluating Image Editing via FID). However, the differences are all pretty limited. Since most editing results still resemble the original input images, it is difficult for FID to determine their authenticity. These results indicate that FID is insufficient to compare the quality of image editing.\n\n| Method | Zero-shot | | | | Fine-tuned | | | |\n| :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- | :-- |\n| | **EVR** | **GIER** | **MA5k** | **MagicBrush** | **EVR** | **GIER** | **MA5k** | **MagicBrush** |\n| InsPix2Pix | **6.19** | **5.61** | 5.91 | 5.69 | **5.31** | **5.31** | **5.30** | 5.64 |\n| LGIE | 6.67 | 5.69 | 5.80 | **5.31** | 5.32 | 5.42 | 5.59 | 5.48 |\n| MGIE | 6.45 | 5.64 | **5.48** | 5.61 | 5.53 | 5.59 | 5.41 | **5.42** |\n\n> **Q) training cost**\n\nWe updated our training cost in Sec. B (Training Cost). Our MGIE training requires 26 epochs to converge, and InsPix2Pix has 20 epochs (from their released checkpoint). Both MGIE and InsPix2Pix take a similar 1.6 hours per epoch on our node (8 NVIDIA A100 GPUs), where the overall training can be done in two days."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission485/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700355287894,
                "cdate": 1700355287894,
                "tmdate": 1700358232387,
                "mdate": 1700358232387,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aOLyEYzboK",
                "forum": "S1RKWSyZ2Y",
                "replyto": "Rc0p2YLJjq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission485/Reviewer_BZAL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission485/Reviewer_BZAL"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their rebuttal, thorough efforts to address all reviewers concerns, and improvements to the paper's clarity. Most of my concerns were addressed, and I recommend acceptance of the manuscript."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission485/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700589819035,
                "cdate": 1700589819035,
                "tmdate": 1700589819035,
                "mdate": 1700589819035,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]