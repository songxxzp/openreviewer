[
    {
        "title": "Collaborative Prompt Tuning for Black-Box Vision-Language Models"
    },
    {
        "review": {
            "id": "GdUJ5I2jdh",
            "forum": "twhn5XvOtt",
            "replyto": "twhn5XvOtt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5499/Reviewer_j3Qp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5499/Reviewer_j3Qp"
            ],
            "content": {
                "summary": {
                    "value": "The article introduces a prompt-tuning framework for black-box visual language models. In order to fine-tune the black-box model, the authors propose building two modules. The first module maps low-dimensional vectors to high-dimensional matrices and adjusts the prompt by adding it to the mapped matrix. The second module modifies the logit output of the black-box model using a residual network. For the first module, the authors use the CMA-ES method, which does not require derivatives, for optimization. For the second module, the AdamW method is used for optimization. To address training issues caused by differences in optimization methods, the authors further propose a joint optimization approach. The training process is splitted to several rounds and in each round, the optimization of the second module starts after the first has been optimized. They also introduce KL divergence in the optimization objectives of each module to stabilize the optimization process. The authors conducted few-shot experiments on multiple datasets. The experimental results demonstrate that their method outperforms three baseline methods: zero-shot CLIP, CoOp and BBT, and they also require much less memory for deployment."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "originality: The problem addressed in the article is highly relevant and innovative. It is almost the pioneering work in exploring efficient fine-tuning methods for black-box vision-language models.\n\nquality: The article demonstrates high quality as it conducts extensive experiments on multiple datasets. Abundant ablation experiments also validate the roles of various modules in the framework.\n\nclarity: The description of formulas and methods is fairly clear, although there are some issues such as missing variables that will be pointed out in the questions. \n\nsignificance: The proposed method holds some significance for PEFT, particularly in terms of its memory efficiency."
                },
                "weaknesses": {
                    "value": "originality: The prompt generation module in the proposed framework bears significant similarity to the method mentioned in the paper \"Black-Box Tuning for Language-Model-as-a-Service\". The article also fails to discuss the differences between their approach and BBT in the related work section. Nonetheless, the other modules exhibit some degree of novelty.\n\nclarity: One major concern is how the black-box is defined exactly. The method described in the paper requires modifying the embedding of the prompt and accessing the model's returned logits, which seems inconsistent with the common definition of a black-box VLM.\n\nsignificance: In reality, very few companies providing VLM services openly disclose their embedding models or return logits. Therefore, the practical value of the article's approach for tuning black-box VLMs is questionable."
                },
                "questions": {
                    "value": "1. The core question is why this paper can access the embedding of the prompt and the final returned logits in a black-box setting, which does not match with the reality of model services.\n\n2. In the experimental section, the paper mentions conducting experiments with three different random seeds and averaging the results. Could the authors confirm whether the random seeds are fixed for each method?\n\n3. Why were these 11 datasets chosen for testing? What was the rationale behind their selection?\n\n4. Formula 2 does not include \u03bb, but it is mentioned in the following explanation. Please check it.\n\n5. In Section 3.1, the sentence \"Next, we add the prompts to the initial prompts p0 (e.g., 'a photo of a').\" refers to the prompt in textual form, but the preceding paragraph states that the prompt should be a matrix. It is suggested to improve the consistency in defining the prompt throughout the paper.\n\n6. In Section 3.3, the paper states, \"Fortunately, vision-language models typically have residual connections, and the prediction refinement network also comprises a shortcut connection. Therefore, we can iteratively optimize the prompt generation module and the prediction refinement module.\" The reviewer would like to know why the presence of residual structures in the original VLM facilitates collaborative training. Could you provide some explanations or references supporting this viewpoint?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5499/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698393784870,
            "cdate": 1698393784870,
            "tmdate": 1699636562342,
            "mdate": 1699636562342,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "je3f30bJyy",
            "forum": "twhn5XvOtt",
            "replyto": "twhn5XvOtt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5499/Reviewer_Nv8C"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5499/Reviewer_Nv8C"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the black box optimization problem for pre-trained VL models. With the increasing cost of training, model owners tend to protect their digital assets, making existing fine-tuning methods inapplicable. To address this, the paper proposes a method called CARROT, which allows fine-tuning models without accessing the model parameters, enabling adaptation to downstream tasks. CARROT consists of a prompt generation module and a prediction refinement module. The prompt generation module learns the text prompt, while the prediction refinement module enhances the model's prediction results. Due to the black box nature, CARROT cannot be optimized end-to-end like traditional fine-tuning. Therefore, the paper proposes using CMA-ES and AdamW optimizer to separately optimize the two modules and introduces KL loss as a constraint to ensure consistency between the outputs of the two modules. The paper conducts experiments on the CoOp benchmark, and the results demonstrate a significant improvement over previous methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The black box problem addressed in this paper is important."
                },
                "weaknesses": {
                    "value": "1. This paper lacks a comparison with related methods, such as [1]. As both [1] and this paper apply the CMA-ES algorithm, it is important to clarify their differences. The results of [1] were publicly available on GitHub in April, and it seems that this paper does not have significant advantages over [1] in terms of performance.\n\n2. The ablation experiments are insufficient. For example, in eq (5) and (6), the impact of introducing KL divergence should be demonstrated through ablation experiments. Additionally, the value of $\\lambda$ should be analyzed through experiments.\n\n3. The authors should validate the generalization and versatility of their method on more VL-pretrained models and tasks. For example, they can consider using EVA-CLIP as the base model and include tasks such as text-image retrieval and open-vocabulary segmentation. The current validation only focuses on the CoOp benchmark, which is not sufficient to demonstrate the superiority of their method.\n\n[1.] Yu, Lang, et al. \"Black-box Prompt Tuning for Vision-Language Model as a Service.\""
                },
                "questions": {
                    "value": "No other questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5499/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5499/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5499/Reviewer_Nv8C"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5499/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698737882709,
            "cdate": 1698737882709,
            "tmdate": 1699636562254,
            "mdate": 1699636562254,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "h1aAFePW1g",
            "forum": "twhn5XvOtt",
            "replyto": "twhn5XvOtt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5499/Reviewer_2nQv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5499/Reviewer_2nQv"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed a new method for fine-tuning black-box vision-language models (VLMs) without accessing their parameters. The method is called CARROT and it has three main components:\n\n-A prompt generation module that learns text prompts for the input of the black-box VLM using derivative-free optimization.\n\n-A prediction refinement module that improves the output of the black-box VLM using a residual network and gradient descent.\n\n-A collaborative training algorithm that alternates between optimizing the two modules and uses a prediction-consistent loss to ensure stability.\n\nThe paper claims that CARROT can achieve significant improvements over zero-shot learning and black-box baselines on 15 few-shot classification datasets. It also compares favorably to the white-box method in terms of training speed and memory footprint."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper presents a novel method called CARROT for fine-tuning black-box vision-language models (VLMs) without accessing their parameters. The method consists of three main components: a prompt generation module, a prediction refinement module, and a collaborative training algorithm. The paper claims that CARROT can achieve significant improvements over zero-shot learning and black-box baselines on 15 few-shot classification datasets. It also compares favorably to the white-box method in terms of training speed and memory footprint.\n\nOne of the strengths of the paper is that it addresses an important problem in the field of machine learning, namely how to fine-tune black-box models without access to their parameters. The authors provide a detailed description of the CARROT method and its three main components, which makes it easy to understand and replicate. They also provide extensive experimental results that demonstrate the effectiveness of the method on a variety of few-shot classification tasks.\n\nAnother strength of the paper is that it compares CARROT to both zero-shot learning and black-box baselines, as well as to the white-box method. This allows readers to see how CARROT performs relative to other methods and provides a more complete picture of its strengths and weaknesses. The authors also provide a detailed analysis of the results, which helps readers understand why CARROT performs better than other methods.\n\nOverall, the paper makes a contribution to the field of machine learning by presenting a new method for fine-tuning black-box VLMs that is both effective and efficient. The authors provide extensive experimental results that demonstrate the effectiveness of the method on a variety of few-shot classification tasks, which should be useful for researchers working in this area."
                },
                "weaknesses": {
                    "value": "(1) The proposed CARROT algorithm is composed of three parts, i.e., the black-box PG, the white-box PR, and the Co training paradigm. Since PG is directly borrowed from [1] and PR is not black-box, I believe that Co is the most important contribution in this paper. However, as shown in Table 4, why PR and PG conflict with each other, and why Co resolves such conflict are unclear to me. This is the core part of CARROT. More theoretical or empirical explanations are strongly needed.\n\n(2) I noticed that there is a concurrent paper [2] in this topic (already open-source), which also applies evolutionary algorithms (including CMA-ES) to conduct prompt learning but in either a shallow or a deep manner. Since this paper has no other black-box methods to compare with (BBT in Table 1 and Figure 2 is just PG, a part of CARROT), I strongly suggest the authors make comparisons with [2] in fair experimental settings.\n\n(3) Ablation studies on the subspace dimension in CMA-ES, \\lambda_I, and \\lambda_O are all missing.\n\nMinor flaws\n(4) The font sizes in Figure 1 are inconsistent, and the icon of Co is kind of confusing to me.\n\n(5) The ImageNet acc is 61.33% in Table 1 but becomes 61.17 in Table 2. Please check the reported results.\n\n[1] ICML 2022. Black-Box Tuning for Language-Model-as-a-Service\n\n[2] IJCAI 2023. Black-box Prompt Tuning for Vision-Language Model as a Service"
                },
                "questions": {
                    "value": "Please reply to the five weakness questions during the rebuttal."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5499/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698752250699,
            "cdate": 1698752250699,
            "tmdate": 1699636562154,
            "mdate": 1699636562154,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "o6SwjE8DPe",
            "forum": "twhn5XvOtt",
            "replyto": "twhn5XvOtt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5499/Reviewer_S6pY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5499/Reviewer_S6pY"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a Prompt Tuning method for black-box Vision-Language model (VLM), which designs a better text prompt and refines the output of the VLM. According to the experiments, the few-shot performance of VLM is improved on the classification task, and the proposed collaborative training approach integrates the derivative-free and derivative-based model optimization well."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.\tThe overall writing of the paper is clear, and the tables and figures intuitively designed.\n2.\tFrom the experiments, the proposed CARROT method achieves good improvement on the few-shot classification task."
                },
                "weaknesses": {
                    "value": "1.\tThe paper seems not novel, the proposed Prompt Generation Module is just a direct transfer of the language model's Black-Box Prompt Tuning method [1] to VLM directly, and there is no specific design of the image modality in VLM, which makes the method very incremental.\n2.\tThere has been work [2] on Black-Box's VLM Prompt Tuning and it is specifically designed for both text and image modalities. It also uses CMA-ES optimization and transfers the data to low-dimensional subspace. While this work is not mentioned in this paper, could you please elaborate on the differences between the proposed approach and this paper and clarify your contribution.\n3.\tSec 3.3 in the article is puzzling, especially the second paragraph, it is hard to understand why the residual connection is causally related to iteratively optimizing two modules? \n4.\tWhy include \u03bbI \u2217 KL(YI \u2225 YO) in Eq. 5, and what is the purpose of the Refinement module if one wants YI to converge to YO? It seems more reasonable to keep only the KL divergence calculation in Eq. 6. Please prove the design of this loss function by ablation experiments.\n5.\tThe methods compared in the article are too few, please compare with some sota PT [3,4] methods to further prove the validity of the methodology.\nReference\n[1] Sun, Tianxiang, et al. \"Black-box tuning for language-model-as-a-service.\" International Conference on Machine Learning. PMLR, 2022.\n[2] Yu, Lang, et al. \"Black-box Prompt Tuning for Vision-Language Model as a Service.\" IJCAI, 2023.\n[3] Zhou, Kaiyang, et al. \"Conditional prompt learning for vision-language models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n[4] Sun, Tianxiang, et al. \"BBTv2: towards a gradient-free future with large language models.\" Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022."
                },
                "questions": {
                    "value": "Please refer to the weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5499/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5499/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5499/Reviewer_S6pY"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5499/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698825453291,
            "cdate": 1698825453291,
            "tmdate": 1699636562054,
            "mdate": 1699636562054,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]