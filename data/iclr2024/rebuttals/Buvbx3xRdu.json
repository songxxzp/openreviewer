[
    {
        "title": "VideoClusterNet: Self-Supervised and Adaptive Face Clustering for Videos"
    },
    {
        "review": {
            "id": "4BHKZuTDls",
            "forum": "Buvbx3xRdu",
            "replyto": "Buvbx3xRdu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1494/Reviewer_B82Y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1494/Reviewer_B82Y"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a face clustering algorithm for videos that involves  a self supervised learning process to adapt a generic face ID model to the target video and also a clustering algorithm based on the adapted model. In addition, a new benchmark is proposed for the task of video face clustering. However, while the method of joint face representation adpation and clustering for video has been used in several prior works, such as in the related paper(Sharma et al. (2019), Zhang et al.(2016b)) mentioned by the authors, its novelty is not clearly established. On the other hand, the motivation for proposing the benchmark is not sufficiently explained."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Quite detailed experiments and good performance compared to current methods.\n2. Automatic face clustering is important and practical for many video editing application but lacks sufficient attention in the research community. One of the reasons is the lack of good benchmarks. This work makes a contribution towards addressing this."
                },
                "weaknesses": {
                    "value": "1. Compared to related works that use joint face ID model adaptation and clustering, the uniqueness of this work is not stated clearly. Is the performance increase comes from better face ID model adpatation or just the following clustering method?  If better face ID model, is it because of the teacher-student branch method used in this work or other perspectives?\n\n2. Why we need a new benchmark? The existing ones not challenging enough for practical use? Why? Any quantitative/qualitative comparison among the porposed one and existing ones? The statement should be put clearly in the paper."
                },
                "questions": {
                    "value": "Please refer to the section of weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1494/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698764150373,
            "cdate": 1698764150373,
            "tmdate": 1699636078342,
            "mdate": 1699636078342,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "B6ZuiZRnSP",
                "forum": "Buvbx3xRdu",
                "replyto": "4BHKZuTDls",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1494/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1494/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer B82Y"
                    },
                    "comment": {
                        "value": "We thank the reviewer for providing a thorough review of our paper. This would help us greatly in improving the quality and legibility of our manuscript. Below we address specific questions/concerns raised in the weakness section:  \n\n1. **Clarification on method novelty:** We kindly refer the reviewer to the \u201cJoint Representation and Clustering\u201d paragraph in Section 2 regarding the comparison of our proposed method with closely related methods. We believe that our method\u2019s performance increase is a result of both the self-supervised (SSL) model finetuning and the final clustering modules. Empirical evidence for this fact can be observed in Section 5 (Ablation Analysis). Specifically, Table 3 and Table 4 prove the individual effectiveness of our model finetuning mechanism and clustering algorithm, respectively. Unlike closely related prior work, the model adaptation stage is more effective majorly due to the self-distillation procedure and its sole dependence on positive match pairs. Our final clustering algorithm directly benefits from the model finetuning stage by incorporating the SSL training objective as a distance metric. Such a learned metric helps boost the algorithm\u2019s performance as it efficiently evaluates inter-track distances directly in an embedding space, which is optimized for reduction in the same identity distances. Also, any sub-optimal performance that may be induced due to any user-defined matching thresholds is avoided through the adaptive custom threshold computed for each track. Such a thresholding mechanism helps the clustering algorithm automatically adjust to how effectively the model can match a given track\u2019s identity across a true positive match pair. We will update our paper with this detailed explanation for the reader\u2019s enhanced understanding of our method\u2019s salient novelty.  \n\n2. **Need for new benchmark dataset:** We kindly refer the reviewer to the second last paragraph in Section 1 (Introduction) where we detail a few primary reasons for the need for a comprehensive movie benchmark dataset for video face clustering. To justify it further, the problem we address in the paper, and its application for TV series/movies has seen comparatively lesser interest in the last few decades as opposed to other Computer Vision related tasks. Also, complicated usage and copyrights for TV series/movies meant that providing open source datasets based on them wasn\u2019t feasible or otherwise costly and time-consuming w.r.t. providing annotations. As a result, the majority of related approaches in this domain end up evaluating their method on a very specific TV series/movie dataset while not being able to provide means to access it publicly. A direct consequence is thus not being able to have fair comparisons with prior works, forcing authors to evaluate across a large set of niche datasets. We present our dataset with this view in mind and sincerely hope that it will largely benefit the research community in this niche domain.  \nFinally, we refer the reviewer to our response to the first reviewer (**RNxJ**)\u2019s concern regarding dataset analysis and comparison. We also refer the reviewer to Appendix J, which summarizes the uniqueness and advantages of our proposed dataset over existing ones.  \n\nWe would appreciate it if you could let us know of any further questions/concerns that you may have and if any finer clarification is required for any of our responses listed above. If not, we would greatly appreciate it if the reviewer could consider increasing his/her overall rating."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1494/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700330165071,
                "cdate": 1700330165071,
                "tmdate": 1700497253648,
                "mdate": 1700497253648,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zZtyv5ZnXl",
            "forum": "Buvbx3xRdu",
            "replyto": "Buvbx3xRdu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1494/Reviewer_wXAX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1494/Reviewer_wXAX"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a self-supervised algorithm for clustering face tracks in a video using a generic face ID model. A coarse track matching method is used to extract positive tracks for fine-tuning the face ID model. The fine-tuned model's embedding space is used to evaluate the similarity between face tracks, and an adaptive thresholding mechanism is used for the final clustering step. To evaluate the proposed model, a movie dataset is curated, and the results demonstrate its state-of-the-art performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The annotation effort for the movie dataset is substantial.\n- The method is novel in its elimination of the need for manual selection of a clustering threshold.\n- The pre-processing step includes cutting-edge building blocks such as RetinaFace for face detection and SER-FIQ for face quality assessment.\n- The noticeable efforts to replicate baselines in PyTorch are commendable.\n- Limitations and future works are discussed."
                },
                "weaknesses": {
                    "value": "Missing important details regarding fine-tuning: it is unclear how the data is divided into train/validation sets during the model fine-tuning stage. The self-distillation fine-tuning step proposed requires multiple tracks from different temporal steps to ensure adequate appearance variations. However, the number of tracks required in a video and how it impacts the model is not discussed.\n\nThe authors claim that the positive track pair construction step is independent of any ground truth labels or temporal motion track constraints. This statement is confusing as it contradicts the requirement of ground truth labels.\n\nUnfair comparison with baselines due to advanced pre-processing: the pre-processing techniques used in this paper rely on advanced algorithms such as PySceneDetect (2022), RetinaFace (2020) and SER-FIQ (2020). In contrast, most of the baselines in Table 1 were established before 2020. There is no investigation into how these advanced pre-processing steps affect the clustering performance of the proposed method or the baselines. The observed performance improvement could be attributed to more accurate scene detection or face detection rather than the proposed method itself.\n\nData, training code and baseline re-implementation code are not promised to be open-sourced for reproducibility."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The release of the proposed MovieFaceCluster dataset might breach the copyright of these movies."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1494/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1494/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1494/Reviewer_wXAX"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1494/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698772503648,
            "cdate": 1698772503648,
            "tmdate": 1699636078228,
            "mdate": 1699636078228,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TkXEsCWMfB",
                "forum": "Buvbx3xRdu",
                "replyto": "zZtyv5ZnXl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1494/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1494/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wXAX [Part 1/2]"
                    },
                    "comment": {
                        "value": "We thank the reviewer for taking the time to provide a thorough and detailed review of our paper. This would help greatly improve the quality and legibility of our manuscript. Below we address some specific questions/concerns posed by the reviewer in the weakness section:  \n\n1. **Missing details on model finetuning module:** We would first like to state that the proposed model finetuning module (details in Section 3.4) is fully self-supervised, i.e., completely independent of any manual ground truth labels. This facilitates the selection of all available tracks in a given video for the finetuning process. Also, the model finetuning hyperparameters such as training epochs, batch size, and learning rate (see Appendix A) are kept constant and are agnostic to the training dataset statistics. This removes dependence on a traditional train and validation set based mechanism for performance evaluation. These hyperparameters have been carefully tuned across a wide range of video datasets to ensure convergence of the fine-tuned model. This also helps the model learn all observed natural variations across a given set of dataset face crops. An alternate way to describe this finetuning mechanism is that in our self-supervised setup the training and validation/test set are indeed exactly the same, but where performance validation isn\u2019t essential.  \nAlso, certain image augmentations (detailed in Appendix A.2) are applied during the finetuning process. This ensures that the model observes enhanced facial variations during its training and helps reduce its primary dependence on natural variations present in the dataset. In addition, since the model is finetuned on all face tracks that need to be clustered, it gives the model more flexibility to slightly overfit the training dataset if the track count is low. Both of these factors make the finetuning process fairly agnostic to the number of tracks and the available natural variations within a given video dataset. Table 2 shows empirical evidence of this fact wherein the track count ranges between 119 and 917 across our movie dataset. Our method manages to achieve significant performance improvements over benchmark methods across all the movies despite these track count variations. We hope that these insights help the reviewer better understand the salient novelty and merits of our proposed method. We will update future manuscript versions with this explanation accordingly.  \n\n2. **Positive track pair contribution clarification:** We thank the reviewer for mentioning the ambiguity in one of our listed paper contributions and apologize for the confusion. We would like to provide clarification about the second contribution in the introduction. Our method only relies on positive pairs in the model finetuning/adaptation stage, a unique feature from prior works. Previous methods leverage negative track pairs and incorporate either manual ground truth labels or temporal track constraints to obtain them. We have updated our second contribution accordingly to reflect this clarification.  \n\n3. **Clarification on baseline comparison with advanced pre-processing modules:** For the comparisons with baseline methods listed in Table 1, we used the exact same set of face bounding boxes and motion track information provided by the benchmarked methods for a fair comparison. We skipped the usage of our track pre-processing modules (detailed in Section 3.2) and directly processed the available data through the model finetuning and final clustering modules. Note that this data was made publicly available by (Tapaswi et al. 2019)[1] at this link: https://github.com/makarandtapaswi/BallClustering_ICCV2019. We believe this facilitates a fair evaluation of our self-supervised clustering algorithm w.r.t. baseline methods and eliminates any influence of advanced pre-processing modules, specifically shot and face detection. We kindly refer the reviewer to this fact mentioned in Section 4.1 of our paper. Similarly, for a fair comparison on our dataset in Table 2, we use our track pre-processing module to evaluate both the baselines and our method on the same set of face boxes.  \n\n**Continued in Part 2/2**"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1494/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700329513563,
                "cdate": 1700329513563,
                "tmdate": 1700329513563,
                "mdate": 1700329513563,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SY5erEj0lC",
                "forum": "Buvbx3xRdu",
                "replyto": "zZtyv5ZnXl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1494/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1494/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer wXAX [Part 2/2]"
                    },
                    "comment": {
                        "value": "**Continued from Part 1/2** \n\n4. **Dataset and code availability:** We would like to assure the reviewer that the proposed MovieFaceCluster dataset will be open sourced to the benefit of the wider research community and to encourage further research into this relatively niche domain. As for the concern about movie copyright issues with our dataset release, the reviewer should rest assured that we possess valid internal usage licenses for all the movies listed in our dataset. We will release the dataset in a format such that it doesn\u2019t breach any copyright or license restrictions. In its current state, along with ground truth identity information, we will release the bounding box and motion track annotations referring to each frame in every movie. This would enable the dataset user to obtain the face crops given source media. We hope this effectively addresses the reviewer\u2019s concern about the dataset's legal compliance. We will also work on making face crop data itself available in the next dataset version. We also confidently believe that we have provided thorough implementation details (in Appendix A) to accurately reproduce our proposed method. In addition, we will make available the re-implementation of the baseline code (accurate up to the best of our knowledge) upon the paper's publication.  \n\nWe would appreciate it if you could let us know of any further questions/concerns that you may have and if any finer clarification is required for any of our points listed above. If not, we would greatly appreciate it if the reviewer could consider increasing his/her overall rating.\n\nReferences:  \n[1]: Makarand Tapaswi, Marc T. Law, and Sanja Fidler. Video face clustering with unknown number of clusters. In International Conference on Computer Vision (ICCV), pp. 5027\u20135036. IEEE, 2019."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1494/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700329774738,
                "cdate": 1700329774738,
                "tmdate": 1700497236516,
                "mdate": 1700497236516,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HnTZ7BpJMj",
            "forum": "Buvbx3xRdu",
            "replyto": "Buvbx3xRdu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1494/Reviewer_RNxJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1494/Reviewer_RNxJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a self-supervised video face clustering approach that adapts to challenging variations in facial pose, lighting, and expressions by fine-tuning a generic face ID model to learn robust facial embeddings progressively. Furthermore, it introduces a parameter-free clustering algorithm that automatically clusters facial features based on the fine-tuned model embeddings without the need for user-defined thresholds or initial cluster numbers. In addition, a movie face clustering benchmark dataset MovieFaceCluster is provided to better evaluate the performance of video face clustering algorithms in real-world scenarios."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well-motivated and easy to follow.\n- The proposed method is reasonable and feasible.\n- Compared to the prior work and baseline, the proposed method achieves competitive results."
                },
                "weaknesses": {
                    "value": "- The paper does not provide enough information about the proposed dataset, particularly in terms of presenting its uniqueness and advantages. There is no visualizations or statistical analyses to demonstrate distinctions from existing datasets. Furthermore, the description of dataset annotations is unclear. It remains uncertain whether the term \"Varying Parameter\" mentioned in Figure 1 is associated with dataset annotations.\n- The authors did not compare their dataset with existing movie person identification (PI) datasets, such as MovieNet [1], which includes annotations suitable for PI tasks.\n- While the paper claims not to require pre-defined parameters, fixed values are still set in the quality assessment and coarse track matching modules to generate adaptive thresholds. However, these fixed values lack empirical or theoretical support.\n- Regarding the t-SNE embedding visual comparison on the MovieFaceCluster dataset in Figure 5, the visualizations generated by the proposed method seem to closely approximate the ground truth, which exhibits some anomalous clusters. Authors may explore the limitations of proposed method and present failure cases so as to provide more in-depth insights into video face clustering.\n- Most of the methods used for comparison are outdated, with only one introduced within the last three years.\n- Please ensure the consistency of citation formats. For instance, there is an inconsistency in the citations of Table 1 in Section 4.1, where both 'Table 1' and 'Tab. 1' are used.\n\nRef:\n[1] Huang, Qingqiu, et al. \"MovieNet: A Holistic Dataset for Movie Understanding.\" European Conference on Computer Vision. 2020."
                },
                "questions": {
                    "value": "See the above weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1494/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834234768,
            "cdate": 1698834234768,
            "tmdate": 1699636078110,
            "mdate": 1699636078110,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YJPQjALMAr",
                "forum": "Buvbx3xRdu",
                "replyto": "HnTZ7BpJMj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1494/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1494/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RNxJ [Part 1/2]"
                    },
                    "comment": {
                        "value": "We would first like to thank the reviewer for taking the time to provide a thorough and detailed review of our paper. This would greatly help us improve the quality and legibility of our manuscript. Below we provide some specific comments on questions/concerns pointed out in the weakness section:  \n\n1. **Dataset analysis and comparison:** In addition to the details on our proposed dataset presented in Section 4.3 and Appendix E, we have updated our paper to provide a comprehensive analysis and comparison of our dataset with existing literature in Appendix F. This includes a comparison of some key dataset attributes like unique character and track count, cast racial diversity, and average track face quality score (computed using our face quality estimation module presented in Section 3.6). We also present a detailed histogram comparison of the dataset face quality distribution to give more insights into each dataset\u2019s average quality score. In addition, we provide dataset histogram comparisons across critical attributes, specifically scene lighting and facial blur. We are confident that this revision further strengthens our argument of providing a challenging, more diverse movie face clustering dataset. We sincerely thank the reviewer for his comments on this specific aspect of our paper. Also, in Figure 1 the term \u201cvarying parameter\u201d denotes the dominant image attributes that are significant outliers for a given face crop. It isn\u2019t part of the available dataset annotations and is used simply for enhanced reader understanding. We have added this clarification in the updated paper version in Appendix J.  \n\n2. **Comparison with MovieNet dataset:** Even though MovieNet dataset [1] comprises a wide variety of annotations on a large number of mainstream movies, we would like to kindly point out that its annotations aren\u2019t directly useful for our task of video face clustering. It unfortunately consists of person bounding boxes (covering the entire character's body) and not face bounding boxes, which are essential for our task. Furthermore, complex post processing could be used to detect face bounding boxes in each dataset frame and match them against the ground truth person boxes, in order to associate the identity of each face. For this, the movie frame data would be essential to predict face boxes. To the best of our knowledge, neither the source movie dataset nor the links to the respective movies are available for download from https://movienet.github.io/. Due to these reasons, we strongly believe that a dataset comparison with MovieNet in its current state wouldn\u2019t be fair for our intended task as it doesn\u2019t contain relevant annotations for the same. We would however like to thank the reviewer for mentioning this specific dataset as it could potentially be relevant in a future extension of our work, wherein the video clustering is applied for person bounding boxes rather than just face boxes. We have updated our paper to acknowledge the same in the \u201cLimitations and Future Works\u201d paragraph in Section 5.\n\n3. **Clarification for certain empirical system parameter values:** We would like to highlight here that the track quality estimation module (see Section 3.6) can be viewed as a pre-processing module for our final clustering algorithm (see Section 3.7). It is highly effective in filtering out non-identifiable face tracks that are often present in mainstream media content. Also, the coarse track matching module (see Section 3.5) is used in conjunction with the model finetuning module (see Section 3.4). We do acknowledge that both of the referenced modules contain minor empirically evaluated parameter values. However, we claim in our listed contributions (fourth contribution in the Introduction) that we specifically have a parameter-free video face clustering algorithm. Both of the aforementioned modules are essentially external to our core clustering algorithm. We would like to confidently reiterate that the core clustering algorithm is fully automated in the true sense. We would also like to provide clarification on specific values set for certain parameters in both referenced modules. For Equation 4 in the main paper, backed by empirical evidence, the value of 2.7 was first loosely set by fitting a Gaussian distribution onto the given set of track face quality scores. To select/filter the lower ~1.5% outliers, which are often less than the threshold of (mean - 2.8*std) in a Gaussian distribution, we sampled values in the range of 2.6 to 3.0 and empirically found that 2.7 worked optimally for our test set consisting of a wide range of movies.  **Continued in Part 2/2**"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1494/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700328521705,
                "cdate": 1700328521705,
                "tmdate": 1700328521705,
                "mdate": 1700328521705,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cf3UhmiMOj",
                "forum": "Buvbx3xRdu",
                "replyto": "HnTZ7BpJMj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1494/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1494/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RNxJ [Part 2/2]"
                    },
                    "comment": {
                        "value": "**Continued from Part 1/2**: Regarding the coarse matching module, the motivation to select crop pdf values in the lower 25% range was again based on empirical evidence. To effectively select the true outliers, which would result in a more accurate computation of a given track\u2019s custom match threshold, we experimented with values ranging from 5% to 40%. We found out that setting it to 25% provides effective true positive track matches while avoiding any false positive matches (which is a critical performance parameter) over a wide range of movie track sets. We will update this in the future revised version accordingly.\n\n4. **Anomalous clusters in t-sne visualization:** We believe that the majority of the anomalous clusters visible in Figure 5 for the ground truth visualization are a combination of two main factors. First, there exist certain sub-optimalities in the t-sne embedding method [2] used to find a two-dimensional visualization sub-space. That partly results in the appearance of those outlier clusters due to failure in finding the optimal subspace to have ground truth clusters in a more compact representation. Second, a few of the outlier clusters seen as part of the cluster sets: cluster_1, cluster_17, cluster_5 consist of tracks possessing higher than average difficulty for face clustering. As a result, these tracks appear distant from their respective cluster centers. We would like to highlight here that our method still accurately manages to assign such outlier clusters to their correct ground truth cluster even though they provide a harder face clustering challenge. We kindly refer the reviewer to the \u201cLimitations and Future Works\u201d paragraph in Section 5 of our paper, where we highlight in detail the core limitations of our work and specify future directions to improve on them.  \n\n5. **Literature method comparison:** There are two major selection criteria we used to choose the methods to benchmark our proposed method\u2019s performance in Table 1. First, methods having results reported on The Big Bang Theory (BBT) and Buffy the Vampire Slayer (BVS) datasets were selected as we noticed a major overlap in the use of these two datasets for evaluation in the literature. Certain methods were omitted based on the fact that they only reported results on certain rare TV series datasets that weren\u2019t publicly available for evaluation and/or didn\u2019t have necessary usage rights. Secondly, methods that reported results on publicly available versions of BBT and BVS datasets were selected. For a fair comparison, all methods need to have a constant face track pre-processing module (details in Section 3.2) to ensure that clustering performance is evaluated on the same set of face tracks. Methods that did not satisfy these criterias also, unfortunately, didn\u2019t provide publicly available implementations of their work, which prevented us from re-evaluating these methods on the available input data. For the same reason, we decided to re-implement certain closely related works ourselves for a fair comparison in Table 2. We remark that to the best of our knowledge, Aggarwal et al. (2022)[3] is the only paper published within the last 3 years that we haven\u2019t reported in Table 1. It unfortunately failed to meet the second aforementioned criterion. We sincerely hope that this clarification helps the reviewer provide more insights into our evaluation methods selection criteria shown in Table 1. We would be open to any paper suggestions from the reviewer that may be closely related to our work that we might have missed.  \n  \n6. **Inconsistency in citations:** We would like to thank the reviewer for mentioning this minor issue. We have resolved it in the updated paper. \n\nWe would appreciate it if you could let us know of any further questions/concerns that you may have and if any finer clarification is required for any of the points listed above. If not, we would greatly appreciate it if the reviewer could consider increasing his/her overall rating.\n\nReferences:  \n[1]: Huang, Qingqiu, et al. \"MovieNet: A Holistic Dataset for Movie Understanding.\" European Conference on Computer Vision. 2020.  \n[2]: Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(11), 2008.  \n[3]: Abhinav Aggarwal, Yash Pandya, Lokesh A. Ravindranathan, Laxmi S. Ahire, Manivel Sethu, and Kaustav Nandy. Robust actor recognition in entertainment multimedia at scale. ACM International Conference on Multimedia (ACM MM), pp. 2079\u20132087, 2022"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1494/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700329057644,
                "cdate": 1700329057644,
                "tmdate": 1700497214199,
                "mdate": 1700497214199,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gtKlgIFR5B",
                "forum": "Buvbx3xRdu",
                "replyto": "cf3UhmiMOj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1494/Reviewer_RNxJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1494/Reviewer_RNxJ"
                ],
                "content": {
                    "title": {
                        "value": "Comment"
                    },
                    "comment": {
                        "value": "Thank you for the clarification. My concerns regarding Q2, Q3, and Q4 were adequately addressed. Some concerns remain:\n\n1. Dataset analysis and comparison (A1). The author states in Appendix E: \"The MovieFaceCluster dataset provides challenging face ID tracks within a set of hand-selected mainstream movies. These challenges involve large variations in pose, appearance, illumination, and occlusions.\" Therefore, it would be better to focus visualizations or statistical analyses of the dataset on the variations in key attributes within face ID tracks, rather than solely analyzing faces.\n\n2. Clarification on comparative methods (A5). I agree with the clarification regarding the comparison of methods on the MovieFaceCluster dataset (Table 2). However, there exist concerns regarding the selection criteria for the experimental dataset chosen for the proposed method. Given that the motivation behind the proposed method is to address challenging environmental variations, it is expected to demonstrate superior performance on more other datasets. Consequently, to better demonstrate the effectiveness of the proposed method, the authors should not restrict the dataset selection to BBT and BVS datasets."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1494/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643563249,
                "cdate": 1700643563249,
                "tmdate": 1700643563249,
                "mdate": 1700643563249,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]