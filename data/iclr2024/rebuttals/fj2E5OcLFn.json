[
    {
        "title": "Stochastic Gradient Descent for Gaussian Processes Done Right"
    },
    {
        "review": {
            "id": "OVJ1IC0qzH",
            "forum": "fj2E5OcLFn",
            "replyto": "fj2E5OcLFn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8461/Reviewer_NBgz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8461/Reviewer_NBgz"
            ],
            "content": {
                "summary": {
                    "value": "The authors consider the problem of computing a Gaussian process posterior, specifically its mean and random draws from it. While the naive computation scales cubically in the number of observations, the authors propose a iterative solver with linear cost per iteration. The idea behind this solver is that the expensive quantity in the GP posterior (kernel matrix inverse) can be thought of as a minimiser of a particular regression problem, which can be solved iteratively with gradient-based methods. The authors consider two formulations of such a regression problem (primal and dual), study their convergence properties, as well as discuss randomised gradients computations to achieve linear computational cost. The proposed algorithm is shown to perform competitively on a number of benchmarks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "+ The paper is clearly written and is easy to follow\n+ The differences to the closely related work of Liu et al. (2023) are clearly discussed\n+ I think the results are quite significant for the community. I was especially interested to see that the proposed algorithm performs competitively in comparison to a neural network in Table 2."
                },
                "weaknesses": {
                    "value": "I didn't notice any significant weaknesses."
                },
                "questions": {
                    "value": "- In Fig. 1 you note that the primal gradient makes more progress in K^2-norm while the dual one in K-norm (with the same step size). However, in the left panel of Fig. 1 it seems that for a few iterations in the beginning of optimisation, the primal gradient was also making more progress than dual in the K-norm. Why do you think it is the case?\n\n- The GP hyper-parameters (e.g. observational noise variance, kernel parameters, etc.) are typically estimated by maximising the marginal log-likelihood using gradient-based methods. Do you think it could be possible run the gradient-based hyper-parameters inference jointly with the posterior inference that you discussed in this paper?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8461/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697829471544,
            "cdate": 1697829471544,
            "tmdate": 1699637056024,
            "mdate": 1699637056024,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6nLs8tBNyE",
                "forum": "fj2E5OcLFn",
                "replyto": "OVJ1IC0qzH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8461/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8461/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review, please see our responses to your questions!"
                    },
                    "comment": {
                        "value": "Thank you very much for your review! We are delighted to hear that our results are significant\u2014thank you very much for this comment. Let us address your questions:\n\n---\n\n**1. Early behaviour in optimisation iterations**\n\n> In Fig. 1 you note that the primal gradient makes more progress in K^2-norm while the dual one in K-norm... in the left panel of Fig. 1 it seems that for a few iterations in the beginning of optimisation, the primal gradient was also making more progress than dual in the K-norm.\n\nRegarding Figure 1, specifically the ordering of performance right at the beginning of optimisation: we\u2019ve thought about this, and we genuinely do not know! We think that this behaviour is relatively rare, as it occurs way out of convergence, with relatively few iterations spent in this regime, which should make it unlikely to affect behaviour in practice very much.\n\n**2. Kernel hyperparameter optimisation / Bayesian model selection**\n> The GP hyper-parameters ... are typically estimated by maximising the marginal log-likelihood using gradient-based methods. Do you think it could be possible run the gradient-based hyper-parameters inference jointly with the posterior inference that you discussed in this paper?\n\nOur method can indeed be easily combined with standard approaches to kernel hyperparameter optimisation. **We have added a demonstration of this in Appendix C.7** (we use an approach similar to [Gardner et al. 2018](https://arxiv.org/pdf/1809.11165.pdf)), and include some empirical results to that end where **we match performance of marginal likelihood optimisation with exact linear system solves (see the figure [here](https://anonymous.4open.science/r/SDD-GPs-5486/rebuttal_plots/mll_optim.pdf)).** Thank you for pointing out that this may be interesting to our readers to highlight!"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8461/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700170419906,
                "cdate": 1700170419906,
                "tmdate": 1700170419906,
                "mdate": 1700170419906,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TH2JBDhd36",
                "forum": "fj2E5OcLFn",
                "replyto": "6nLs8tBNyE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8461/Reviewer_NBgz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8461/Reviewer_NBgz"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reply! I will read the interactions with other reviewers and get back to you if I have more questions later."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8461/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700495691956,
                "cdate": 1700495691956,
                "tmdate": 1700495691956,
                "mdate": 1700495691956,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HrEjW464cV",
            "forum": "fj2E5OcLFn",
            "replyto": "fj2E5OcLFn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8461/Reviewer_1x4h"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8461/Reviewer_1x4h"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduced a stochastic dual gradient descent algorithm for kernel ridge regression and sampling. The stochastic dual descent algorithm admits better-conditioned gradients and a faster convergence rate compared to the SGD proposed by Lin et al. (2023). With the selected kernels, experimental results showed competitive performance with a number of SOTA methods on UCI regression/Bayesian optimization/ molecular binding affinity prediction tasks. Overall, the paper is easy to follow and well-written, while technical contributions seem to be below the bar of ICLR."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The strengths are: \n(1) Some fresh insights from the optimization and kernel communities were explored. \n(2) Uniform approximation bound and duality of objectives were both analyzed. \n(3) Different randomized gradients and convergence performance were compared."
                },
                "weaknesses": {
                    "value": "Some suggestions on improving the weakness points are: \n(1) More figures/tables to explicitly show the weakness/instability of the baseline methods are expected. \n(2) Sharing more insights into the algorithm settings, such as the choice of geometric averaging, the effect/influence on the sparsity of the unbiased estimator \\hat(g)(\\alpha), etc, are expected.  \n(3) A theoretical convergence analysis is expected (not only some figures)."
                },
                "questions": {
                    "value": "1. In Figure 1, we can not see the primal gradient descent becomes unstable and diverges for $\\beta n$>0.1. Please show the unstable or compare the evaluated conditional numbers. Under higher step sizes, why does the gradient descent of the primal return $NaN$ (any possible reasons)?\n2. Figure 2 shows the random coordinate estimate with a step size equal to 50. what is the performance on varied step sizes? Can any explanation of the rising part (the blue dashed line) in the middle figure in Figure 2 be given?\n3. What is the step size used to generate the Figure 3? It seems less than 50 and has a competitive rate compared to the random feature estimate shown in Figure 2. Extra clarification and comparison would be better.\n4. How do different batch sizes affect the overall convergence?\n5. It is better to add a test where samples are generated by a non-stationary kernel, to show the ability of the random coordinate estimate. (to distinguish with the random Fourier features)\n6. what is the difference between the $\\beta n$ in the main texts and the $\\beta$ in Algorithm 1?\n7. The green dashed line is missing in Figure 3."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8461/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8461/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8461/Reviewer_1x4h"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8461/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698757083676,
            "cdate": 1698757083676,
            "tmdate": 1700537752886,
            "mdate": 1700537752886,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ka8WmYmf8K",
                "forum": "fj2E5OcLFn",
                "replyto": "HrEjW464cV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8461/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8461/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your feedback, please consider our responses (1/3)"
                    },
                    "comment": {
                        "value": "Thank you for your thorough review. We are glad you think the paper is easy to follow and well-written.\n\nFirst of all we would like to address the overall comment regarding our technical contributions.\n> ...technical contributions seem to be below the bar of ICLR\n\n\nAll applied kernel/GP work we are familiar with, which uses SGD methods, makes suboptimal algorithmic design choices (e.g. [Lin et al. 2023](https://arxiv.org/pdf/2306.11589.pdf) (NeurIPS2023) and [Dai et. al. 2014](https://arxiv.org/pdf/1407.5599.pdf) (NeurIPS2014)). On this basis, we believe that **a simple algorithm that combines recent theoretical advancements in a principled way, and is the first to reliably outperform conjugate gradient-based algorithms used in state-of-the-art GP software packages, can be of value to the ICLR community.** We consider the simplicity of our approach a strength!\n\n\nWe go on to address your remaining concerns and questions.\n\n-----\n\n\n**1. Instability and divergence of baselines.**\n> More figures/tables to explicitly show the weakness/instability of the baseline methods are expected. \n\n**We have added a number of further comparisons where we show failure modes of the baselines we consider**, namely SGD from [Lin et al. 2023](https://arxiv.org/pdf/2306.11589.pdf), CG, and SVGP.\n\n1. In Figures 12 and 13 in Appendix C.6, also available [here](https://anonymous.4open.science/r/SDD-GPs-5486/rebuttal_plots/primal_lr_divergence_pol_10.pdf) and [here](https://anonymous.4open.science/r/SDD-GPs-5486/rebuttal_plots/primal_lr_divergence_pol_20.pdf), you can see how the primal SGD baseline diverges very quickly at even moderately high learning rates.\n2. In Figure 6 in Appendix C.1 ([here](https://anonymous.4open.science/r/SDD-GPs-5486/rebuttal_plots/toy_comparison_rebuttal.pdf)), you can see failure modes for CG (on the infill asymptotics task) and SVGP (large-domain asymptotics task). Our dual SGD method performs robustly in both tasks.\n3. In Figure 11 in Appendix C.5 ([here](https://anonymous.4open.science/r/SDD-GPs-5486/rebuttal_plots/ill_conditioned_pol.pdf)), you can see that CG struggles in ill-conditioned settings, where the dual SGD method is quite robust. \n\n**2. Algorithmic insights**\n> Sharing more insights into the algorithm settings, such as the choice of geometric averaging, the effect/influence on the sparsity of the unbiased estimator \\hat(g)(\\alpha), etc, are expected.\n\nSparsity of the random coordinate estimate plays no role in our recommendation of this gradient estimate. Rather we point it out to help the reader understand what this estimate looks like.\n\nRegarding averaging, **our main point is that iterate averaging may not be necessary for convergence under multiplicative noise (Section 3.3)**, and can be completely done away with. Indeed, arithmetic iterate averaging slows down convergence empirically (see Figure 3).\n\nHowever, we find that geometric averaging, which is much softer a form of averaging (places much more weight on recent iterates), is useful in practice. We thus recommend its use.\n\n**3. Theoretical analysis**\n> A theoretical convergence analysis is expected (not only some figures).\n\n\n\nOur paper is targeted at GP researchers and practitioners. While using SGD for GP inference has recently been found to be very promising ([Lin et al. 2023](https://arxiv.org/pdf/2306.11589.pdf) and [Dai et. al. 2014](https://arxiv.org/pdf/1407.5599.pdf)), **previous work makes a number of suboptimal choices when it comes to the details of the algorithm's formulation. Our contribution is handling these details carefully, highlighting to researchers and practitioners how to make SGD inference even more effective**, particularly on harder problems like the graph kernel comparison.\n\n\nIn the **full-batch (non stochastic) case, the analysis of the algorithm is standard** ([Nesterov 1983](https://www.mathnet.ru/php/archive.phtml?wshow=paper&jrnid=dan&paperid=46009&option_lang=eng)), and it is known that the dual objective will result in faster convergence than its primal counterpart due to conditioning. See, for instance Section 3.7, page 289, of [Bubeck 2015](https://arxiv.org/pdf/1405.4980.pdf), a textbook on convex optimisation. \n\nOur full algorithm makes use of an estimator with multiplicative noise. **The convergence of Stochastic GD under multiplicative noise is currently at the forefront of theoretical research and beyond the scope of our more practical work.** We have extended our discussion of pertinent work in this space in Section 3.4. We hope that our work's demonstration of the empirical performance achievable with SGD under multiplicative noise will motivate research in this area by the optimisation community.\n\n(continued below)"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8461/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700172615804,
                "cdate": 1700172615804,
                "tmdate": 1700173169467,
                "mdate": 1700173169467,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "67XiXaTuD7",
                "forum": "fj2E5OcLFn",
                "replyto": "HrEjW464cV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8461/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8461/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your feedback, please consider our responses (2/3)"
                    },
                    "comment": {
                        "value": "**4. SGD baseline divergence at large step-sizes**\n> In Figure 1, we can not see the primal gradient descent becomes unstable and diverges for >0.1. Please show the unstable or compare the evaluated conditional numbers. Under higher step sizes, why does the gradient descent of the primal return nan (any possible reasons)?\n\n**We include the requested figures showing the divergence of primal SGD with high step-sizes, [here](https://anonymous.4open.science/r/SDD-GPs-5486/rebuttal_plots/primal_lr_divergence_pol_10.pdf) and [here](https://anonymous.4open.science/r/SDD-GPs-5486/rebuttal_plots/primal_lr_divergence_pol_20.pdf), and Appendix C.6. This behaviour quickly exhausts the numerical floating point precision, leading to  overflows and thus NaNs.**\n\nWe did not include these in the original text because the loss curves diverge so quickly that they cannot be viewed simultaneusly with the optimisation trajectories of the non-diverging methods. We run our algorithm for 100k iterations. After only 10 iterations, amplifying oscillations are visible and after 20 iterations an exponential trend is already present.\n\n**5. Varying step-sizes**\n> Figure 2 shows the random coordinate estimate with a step size equal to 50. what is the performance on varied step sizes? \n\n\nWe have conducted **further experiments** under the same settings with **varying step-sizes** for both random coordinate estimators (the one with additive and the one with multiplicative noise). **Please see the corresponding figure [here](https://anonymous.4open.science/r/SDD-GPs-5486/rebuttal_plots/lr_comparison_pol.pdf)** and more details in Appendix C.3.\n\n\nThe performance of the additive noise estimator plateaus earlier as the step-size increases. Smaller step-sizes converge slower but eventually reach better solutions, such that a trade-off is necessary: the step-size is proportional to the speed of convergence but inversely proportional to the final performance. In contrast, the multiplicative noise estimator consistently improves both convergence speed and final performance as the step-size is increased, as long as the step-size is not large enough to cause divergence.\n\n\n\n\n**6. Step size in Figure 3**\n> What is the step size used to generate the Figure 3? \n\nThe step-size used to generate Figure 3 was **$\\beta n = 50$. We added this to the figure caption now**. \n\n**7. Random feature plot line not matching in Figures 2,3 + rising blue line**\n> It seems less than 50 and has a competitive rate compared to the random feature estimate shown in Figure 2. Extra clarification and comparison would be better..\n \n > Can any explanation of the rising part (the blue dashed line) in the middle figure in Figure 2 be given?\n\nThank you for bringing up this source of confusion. Only the dashed light blue line should be nearly identical in both cases. **Both plots use a step-size of 50**, but different batch sizes. However, we have discovered the **lines are different due to the floating point precision** used when running the experiment. Figure 2 was generated with 32-bit floating point, which becomes unstable when the error is very small (~ 1e-7). We have now replaced this with 64-bit floating point, so **this is now fixed. Please see the fixed plot [here](https://anonymous.4open.science/r/SDD-GPs-5486/rebuttal_plots/batch_variants_pol.pdf). To improve clarity, we also added \"random features\" and \"random coordinates\" to the labels in Figure 2.**\n\n(continued below)"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8461/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700172655713,
                "cdate": 1700172655713,
                "tmdate": 1700172969919,
                "mdate": 1700172969919,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OsivASnzLQ",
                "forum": "fj2E5OcLFn",
                "replyto": "HrEjW464cV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8461/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8461/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your feedback, please consider our responses (3/3)"
                    },
                    "comment": {
                        "value": "**8. Effect of batch size**\n> How do different batch sizes affect the overall convergence?\n\n**We conducted additional experiments with varying batch sizes. Please see our figures [here](https://anonymous.4open.science/r/SDD-GPs-5486/rebuttal_plots/batch_size_comparison_pol.pdf) and [here](https://anonymous.4open.science/r/SDD-GPs-5486/rebuttal_plots/batch_size_comparison_pol_per_time.pdf)** and Appendix C.4 for further details.\n\nSmaller batch sizes degrade the final performance of the additive noise estimator because the amount of additive noise is inversely proportional to the batch size. In contrast, the multiplicative noise estimator produces nearly identical optimisation traces with different batch sizes, as long as the batch size is not small enough to cause divergence. Therefore, increasing the batch size consistently improves the final performance of the additive noise estimator while it does not impact the performance of the multiplicative noise estimator, given that the algorithm converges.\n\nIn terms of performance vs time, a larger batch size requires additional compute per step. Thus, the additive noise estimator performance is proportional to the batch size and the multiplicative noise estimator performance per time is inversely proportional to the batch size, as long as the algorithm converges. Therefore, the multiplicative noise estimator should use the smallest batch size which does not cause divergence. Selecting such a batch size a priori is, in general, non-trivial. We have included this discussion in Appendix C4. \n\n\n**9. Non-stationary kernels**\n> It is better to add a test where samples are generated by a non-stationary kernel, to show the ability of the random coordinate estimate. (to distinguish with the random Fourier features)\n\nThank you for the question. **Our optimisation method does not require the existence of closed-form random features: for computing the posterior mean, we can work with any kernel**. We only introduce feature-based gradient estimators due to their use in previous literature ([Lin et al. 2023](https://arxiv.org/pdf/2306.11589.pdf) and [Dai et. al. 2014](https://arxiv.org/pdf/1407.5599.pdf)). We ultimately recommend against their use for gradient estimation due to producing additive rather than multiplicative noise.\n\nWe do use random features for efficiently sampling from the GP prior, which is needed, for instance, in the Bayesian optimisation experiments. We similarly use random hashing for the Tanimoto kernel in the graph experiment. In non-stationary kernels, efficient sampling can often be done on a case-by-case basis, but we prefer not to focus on it as it is orthogonal to our contributions.\n\n**10. $\\beta$ vs. $\\beta n$**\n> what is the difference between the $\\beta$ in the main texts and the $\\beta n$ in Algorithm 1?\n\nThe $\\beta n = \\beta \\times n$ is simply the learning rate $\\beta$ times $n$, the size of the training data set. This formulation allows us to **keep the step-size $\\beta$ times the data set size $n$ constant across data sets**. We mostly use $\\beta n$ throughout our writing, since the algorithm ultimately depends on the respective product. \n\n\n**11. Figure 3 green dashed line**\n\n> The green dashed line is missing in Figure 3.\n\nThank you for pointing this out! **We have updated the caption to reflect that this refers to the olive green line.**\n\n----\n\n**Summarising:** thank you again for your feedback, which has made our experimental evaluation much more comprehensive. Please let us know what you think of the updated results!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8461/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700172860520,
                "cdate": 1700172860520,
                "tmdate": 1700178442495,
                "mdate": 1700178442495,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "T6nH0tCUwz",
                "forum": "fj2E5OcLFn",
                "replyto": "ka8WmYmf8K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8461/Reviewer_1x4h"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8461/Reviewer_1x4h"
                ],
                "content": {
                    "title": {
                        "value": "Many thanks for your reply"
                    },
                    "comment": {
                        "value": "I appreciate the detailed reply which solves most of my questions and concerns. I will raise my score due to the great effort the authors put into this work. The only minor thing I want to point out is perhaps the authors shouldn't disclose their names (through Arxiv) in the review/rebuttal phase."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8461/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700537645851,
                "cdate": 1700537645851,
                "tmdate": 1700537645851,
                "mdate": 1700537645851,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CmR2R4IAIh",
            "forum": "fj2E5OcLFn",
            "replyto": "fj2E5OcLFn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8461/Reviewer_eaDf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8461/Reviewer_eaDf"
            ],
            "content": {
                "summary": {
                    "value": "This paper uses insights drawn from the application of gradient descent in the kernel and optimisation communities to develop a stochastic gradient descent approach for Gaussian processes. In particular this method is useful in regresssion to approximate the posterior mean and to draw samples from the GP posterior. This method, stochastic dual descent, is compared to conjugate gradient, stochastic gradient descent and stochastic variational Gaussian processes on regression benchmarks and molecular binding affinity\nprediction."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This is a well written paper that considers an interesting problem. The use of several benchmarks in the experimental section and comparison with recent work is a plus.\n\nThe justification for use of the dual objective as well as the illustrative example is clear.\n\nThe reason behind the choice of random coordinate estimates is well done."
                },
                "weaknesses": {
                    "value": "It would be useful to emphasise that this work is useful when the Kernel is already known. Comments on whether these methods would be useful in hyperparameter estimation would be useful.\n\nThe claim that the method can be implemented in a few lines of code should be demonstrated. The repo given does not clearly illustrate this using a simple example.\n\nThe paper would benefit from a visualisation comparing samples from a GP using SDD to an exact GP fit to show that the samples lie within the confidence interval."
                },
                "questions": {
                    "value": "What are the implications of limiting the kernel to the form $\\sum_{j=1}^mz_jz_j^T$?\n\nHow does ill conditioning affect the performance of the method?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8461/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8461/Reviewer_eaDf",
                        "ICLR.cc/2024/Conference/Submission8461/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8461/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698777465182,
            "cdate": 1698777465182,
            "tmdate": 1700709520349,
            "mdate": 1700709520349,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TIaE6X9uZN",
                "forum": "fj2E5OcLFn",
                "replyto": "CmR2R4IAIh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8461/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8461/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review, please consider our response below!"
                    },
                    "comment": {
                        "value": "Thank you very much for your time reviewing our paper! We are very happy to hear our work was well-written, and focused on an interesting problem. Let us address the key questions:\n\n----\n**1. Kernel and hyperparameter estimation**.\n\n> It would be useful to emphasise that this work is useful when the Kernel is already known.\n\nThank you for pointing this out: **We have further clarified this in our introduction** with the language \"As a result of the careful choices behind our method, we significantly improve upon the results of [Lin et al. 2023](https://arxiv.org/pdf/2306.11589.pdf) for stochastic gradient descent **in the fixed kernel setting.\".**\n\nOur work solely focuses on improving SGD inference and thus employs the setting of [Lin et al.](https://arxiv.org/pdf/2306.11589.pdf), where the kernel is fixed.\n\n>  Comments on whether these methods would be useful in hyperparameter estimation would be useful.\n\nOur method can indeed be easily combined with standard approaches to kernel hyperparameter optimisation using the usual trick of applying the Hutchinson trace estimator on the marginal likelihood's gradient (see for instance [Gardner et al. 2018](https://arxiv.org/pdf/1809.11165.pdf)). As a proof of concept, **we have added a demonstration of this to Appendix C.7, and include some empirical results to that end where we match performance to standard optimisation of the marginal likelihood (see [here](https://anonymous.4open.science/r/SDD-GPs-5486/rebuttal_plots/mll_optim.pdf)).**\n\n**2. Ease of implementation**\n\n> The claim that the method can be implemented in a few lines of code should be demonstrated.\n\nThank you for your suggestion! **We support our claim that SDD is easy to implement, by including a very simple [Jupyter Notebook](https://anonymous.4open.science/r/SDD-GPs-5486/sdd.ipynb)** that implements our full method in NumPy from scratch and runs it on a toy problem. Without counting data set generation and kernel definition, a basic version of our algorithm can be implemented with only **20 lines of code**. \n\n**3. Visualisation of samples**\n\n> The paper would benefit from a visualisation comparing samples from a GP using SDD to an exact GP fit to show that the samples lie within the confidence interval.\n\nWe agree this would strengthen the paper! **We have added a visual comparison to exact GP, and to our CG and SVGP baselines, on two toy examples considered in Lin et al. 2023. See the figure [here](https://anonymous.4open.science/r/SDD-GPs-5486/rebuttal_plots/toy_comparison_rebuttal.pdf) and Appendix C.1.**\n\n**4. Kernel of the form $\\sum z_j z_j^T$**\n\n> What are the implications of limiting the kernel to the form $\\sum_{j=1}^m z_j z_j^T$?\n\nThank you for drawing us to this potential misunderstanding. We wish to clarify that **our method does not require the kernel to be finite-dimensional.** We only mention this family of approaches in the context of random feature gradient estimators. We study these due to their use in previous literature ([Lin et al. 2023](https://arxiv.org/pdf/2306.11589.pdf) and [Dai et. al. 2014](https://arxiv.org/pdf/1407.5599.pdf)), but ultimately recommend against their use due to producing additive rather than multiplicative noise. To avoid confusion, we have **added a line in Section 3.2**: \"For the sake of exposition (and exposition only, this is not an assumption of our algorithm), we assume that we are in a $m$-dimensional (finite) linear model setting,...\".\n\n**5. Effect of ill-conditioning**\n\n> How does ill conditioning affect the performance of the method?\n\nThank you for your question! \nIn general, methods to solve linear systems are very dependent on the respective condition numbers. However, for Gaussian processes, [Lin et al. 2023](https://arxiv.org/pdf/2306.11589.pdf) show that SGD is significantly less sensitive to conditioning than CG, which can take an arbitrary amount of time to converge (and obtain good results) for sufficiently ill-conditioned systems.\n\nFor SGD, **conditioning determines the maximum step-size which can be used without diverging.** Since the dual gradient presents better conditioning, as it depends on the kernel matrix eigenvalues as opposed to their squares, we can use much larger step-sizes. This can be seen in Figure 1 in the main paper, and in a new figure which we added to the appendix [here](https://anonymous.4open.science/r/SDD-GPs-5486/rebuttal_plots/lr_comparison_pol.pdf).\n\nWe further demonstrate this in Appendix C.5 where we compare both methods on the pol data set while setting the noise standard deviation to 0.001. The figure is also shown [here](https://anonymous.4open.science/r/SDD-GPs-5486/rebuttal_plots/ill_conditioned_pol.pdf)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8461/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700170282845,
                "cdate": 1700170282845,
                "tmdate": 1700171994470,
                "mdate": 1700171994470,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YdBHnkAPEb",
                "forum": "fj2E5OcLFn",
                "replyto": "TIaE6X9uZN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8461/Reviewer_eaDf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8461/Reviewer_eaDf"
                ],
                "content": {
                    "comment": {
                        "value": "The authors have addressed all my concerns in a thorough manner. In addition to the work addressing the concerns of other reviewers, I feel that the paper is greatly improved. I have adjusted my score upwards."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8461/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709452921,
                "cdate": 1700709452921,
                "tmdate": 1700709452921,
                "mdate": 1700709452921,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FmrcoTvImX",
            "forum": "fj2E5OcLFn",
            "replyto": "fj2E5OcLFn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8461/Reviewer_dz2H"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8461/Reviewer_dz2H"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a stochastic dual gradient descent method for optimizing the Gaussian process posterior computation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors present a novel \"dual\" formulation for the Gaussian process regression problem. After studying the condition number of new and old formulations, the authors observe that the \"dual\" formulation allows for the use of larger learning rates, indicating its potential to converge faster. They then propose the stochastic dual gradient descent method, leveraging various optimization techniques based on the \"dual\" formulation, including feature and coordinate sampling (or minibatch) [1], Nesterov's acceleration [2], and Polyak averaging. Notably, the authors introduce a new averaging scheme called geometric averaging.\n\nThe paper is overall well-structured, clear, logically presented, and readable. It contains minimal typos and lacks theoretical flaws. Moreover, the authors conduct sufficient numerical experiments to validate the effectiveness of their proposed optimizer.\n\n[1] \"Sampling from Gaussian Process Posteriors using Stochastic Gradient Descent\"\n[2] Y. Nesterov, \"A method for unconstrained convex minimization problems with a convergence rate of O(1/k^2)\"\n[3] B. T. Polyak, \"New stochastic approximation type procedures,\" Avtomatika i Telemekhanika, 1990."
                },
                "weaknesses": {
                    "value": "The authors do not provide a theoretical justification to verify the convergence of the proposed method. Nevertheless, it is likely that convergence can be ensured under mild conditions, as the optimization techniques employed are standard and well-established in the community and literature.\n\nFrom my perspective, the primary contribution of this paper lies in the introduction of the \"dual\" formulation, as presented on page 4 after Equation (2). This formulation allows for the use of larger step sizes, which suggests the potential for faster convergence. While the remaining studies and techniques are also important, they are somewhat incremental and standard. Consequently, I am uncertain about whether the paper's contribution alone justifies its publication in ICLR. As a result, I have assigned a boundary score and defer to the area chair's judgment for the final decision on acceptance."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8461/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698786898583,
            "cdate": 1698786898583,
            "tmdate": 1699637055617,
            "mdate": 1699637055617,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Lhjdw00yQS",
                "forum": "fj2E5OcLFn",
                "replyto": "FmrcoTvImX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8461/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8461/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your feedback, please consider our response below!"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to read our work and providing feedback. We are delighted to hear that our paper was structured well and presented clearly and logically! Below we address the key concerns:\n\n-----\n**1. Geometric averaging**\n\n> Notably, the authors introduce a new averaging scheme called geometric averaging.\n\nWe want to clarify that we did not introduce geometric averaging, which is widely used in deep learning. **Our contribution is the observation that, for the estimators considered, geometric averaging outperforms arithmetic averaging due to the presence of multiplicative noise**.\nThis choice is unorthodox because almost all previous analysis of SGD uses gradient estimators with additive noise. In that setting, arithmetic averaging (as opposed to geometric) is optimal [(Dieuleveut et. al. 2019)](https://arxiv.org/abs/1602.05419).\n\n**2. Theoretical demonstration of convergence**\n\n> The authors do not provide a theoretical justification to verify the convergence of the proposed method. Nevertheless, it is likely that convergence can be ensured under mild conditions, as the optimization techniques employed are standard and well-established in the community and literature.\n\nOur paper is targeted at GP researchers and practitioners. While using SGD for GP inference has recently been found to be very promising ([Lin et al. 2023](https://arxiv.org/pdf/2306.11589.pdf) and [Dai et. al. 2014](https://arxiv.org/pdf/1407.5599.pdf)), **previous work makes a number of suboptimal choices when it comes to the details of the algorithm's formulation. Our contribution is handling these details carefully, highlighting to researchers and practitioners how to make SGD inference even more effective**, particularly on harder problems like the graph kernel comparison.\n\n\nIn the **full-batch (non stochastic) case, the analysis of the algorithm is standard** ([Nesterov 1983](https://www.mathnet.ru/php/archive.phtml?wshow=paper&jrnid=dan&paperid=46009&option_lang=eng)), and it is known that the dual objective will result in faster convergence than its primal counterpart due to conditioning. See, for instance, Section 3.7, page 289, of [Bubeck 2015](https://arxiv.org/pdf/1405.4980.pdf), a textbook on convex optimisation. \n\nOur full algorithm makes use of an estimator with multiplicative noise. **The convergence of Stochastic GD under multiplicative noise is currently at the forefront of theoretical research and beyond the scope of our more practical work.** We have extended our discussion of pertinent work in this space in Section 3.4. \n\n\n**3. Significance**\n\n>While the remaining studies and techniques are also important, they are somewhat incremental and standard. Consequently, I am uncertain about whether the paper's contribution alone justifies its publication in ICLR.\n\nWe agree that our paper uses insights from the theory literature. Indeed, this is highlighted throughout our text. To our knowledge, these insights have not been studied in a unified way by the kernel/GP community before. Even worse, **all applied kernel/GP work we are familiar with, which use SGD methods, make suboptimal algorithmic design choices**. On this basis we believe that a simple algorithm that combines recent theoretical advancements in a principled way, and  is **the first alternative which reliably outperforms conjugate gradient-based algorithms used in state-of-the-art Gaussian process software packages, can be of value to the ICLR community.**\n\nWe would also like to note that in machine learning, there is a long history of combinations of small improvements eventually leading to breakthrough performance: the most prominent example, arguably, is the development of deep neural networks, which required small tweaks to initialisation, activation functions, architecture, optimisation, and other seemingly incremental factors in order to result in today\u2019s effectiveness."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8461/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700170012145,
                "cdate": 1700170012145,
                "tmdate": 1700178963418,
                "mdate": 1700178963418,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tplsBgYDF5",
                "forum": "fj2E5OcLFn",
                "replyto": "Lhjdw00yQS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8461/Reviewer_dz2H"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8461/Reviewer_dz2H"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' detailed response to my concerns, and I also take note of the feedback provided by the other reviewers and the author's responses. Considering the concerns also raised by other reviewers about the theoretical justification and the perceived limited novelty of this paper due to the combination of previously well-known techniques, I've decided not to revise my score at this moment.\n\nAdditionally, Reviewer WMvZ suggested that the authors might consider including comparisons between the proposed stochastic dual gradient descent and other well-known optimizers like AdaGrad or Adam. While Figure 7 presents a comparison among previous methods, I did not observe a direct comparison with the proposed method. The results for the proposed method with varying setups are depicted in Figure 3; however, a direct comparison of the proposed method's efficiency with the others is crucial, especially considering the authors' assertion of faster convergence as the main contribution."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8461/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700382574129,
                "cdate": 1700382574129,
                "tmdate": 1700382574129,
                "mdate": 1700382574129,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "83CdOka193",
                "forum": "fj2E5OcLFn",
                "replyto": "FmrcoTvImX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8461/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8461/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your comment! We would like to clarify **that \"Nesterov\" in Figure 7 refers to our proposed method**. To prevent further confusion, we will change the corresponding label to \"SDD\". Further, we would like to reiterate that Figure 7 illustrates that our method reaches solutions with **several magnitudes smaller squared $K$- and $K^2$-norms** compared to AdaGrad, RMSprop and Adam."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8461/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700405674347,
                "cdate": 1700405674347,
                "tmdate": 1700582509150,
                "mdate": 1700582509150,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "37ahVnNmx6",
            "forum": "fj2E5OcLFn",
            "replyto": "fj2E5OcLFn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8461/Reviewer_WMvZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8461/Reviewer_WMvZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a stochastic gradient descent method for solving the kernel ridge regression problem. In particular, three aspects are covered: (1) a dual objective that allows a larger learning rate; (2) a stochastic approximation that brings in effective utilization of stochastic gradients; (3) momentum and geometric iterate averaging. By combining these aspects, the algorithm is demonstrated be faster compared to baselines in experiments."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* This paper proposes a new method for the kernel ridge regression problem.\n* Experimental results show that the proposed algorithms can achieve better performance than baselines. When combined with the Gaussian process, the method can also achieve comparable performance to that of graph neural networks."
                },
                "weaknesses": {
                    "value": "* This paper only provides numerical experiments to evaluate the performance of different algorithms. However, it would be good if rigorous theoretical guarantees could be proved, at least for some special cases. Besides, I think the authors stress too much on the algorithm details, which can be deferred to the appendix for a major part of them while trying to leave some room for theoretical analysis.\n* There are many different optimizers for the kernel ridge regression, such as AdaGrad, Adam, etc. The authors should also try these methods in the experiments.\n* The algorithm design is a bit incremental to me, as it looks like a combination of standard existing approaches, which is tuned for the specific tasks. Then, the idea of the algorithm design may be difficult to extend to other tasks.\n* Besides, it is not clear to me whether the variance of stochastic gradient is really a big issue from Figure 2, as the authors do not add the full-gradient version for comparison. If controlling the variance is important, the authors may also need to consider variance-reduce techniques (e.g., SVRG) and add them to the experiments."
                },
                "questions": {
                    "value": "See the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8461/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699202827633,
            "cdate": 1699202827633,
            "tmdate": 1699637055489,
            "mdate": 1699637055489,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jUR6prO1qD",
                "forum": "fj2E5OcLFn",
                "replyto": "37ahVnNmx6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8461/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8461/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your feedback! Please consider our response to your questions (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your review! We are delighted to see the significance of our graph neural network demonstration pointed out in your review! We would also like to highlight that our method reliably outperforms conjugate gradients, which is the standard optimisation method for this problem. We address your key concerns below.\n\n----\n\n**1. Theoretical guarantees**\n\n> \"This paper only provides numerical experiments... would be good if rigorous theoretical guarantees could be proved, at least for some special cases.\"\n\nOur paper is targeted at GP researchers and practitioners. While using SGD for GP inference has recently been found to be very promising ([Lin et al. 2023](https://arxiv.org/pdf/2306.11589.pdf) and [Dai et. al. 2014](https://arxiv.org/pdf/1407.5599.pdf)), **previous work makes a number of suboptimal choices** when it comes to the details of the algorithm's formulation. **Our contribution is handling these details carefully, highlighting to researchers and practitioners how to make SGD inference even more effective**, particularly on harder problems like the graph kernel comparison.\n\nIn the **full-batch (non-stochastic) case, the analysis of the algorithm is standard** ([Nesterov 1983](https://www.mathnet.ru/php/archive.phtml?wshow=paper&jrnid=dan&paperid=46009&option_lang=eng)), and it is known that the dual objective will result in faster convergence than its primal counterpart due to conditioning. See, for instance section 3.7, page 289, of [Bubeck 2015](https://arxiv.org/pdf/1405.4980.pdf), a textbook on convex optimization. \n\nOur full algorithm makes use of an estimator with multiplicative noise. **The convergence of Stochastic GD under multiplicative noise is currently at the forefront of theoretical research and beyond the scope of our more practical work.** We have extended our discussion of pertinent work in this space in Section 3.4. We hope that our work's demonstration of the empirical performance achievable with SGD under multiplicative noise will motivate research in this area by the optimisation community.\n\n**2. Algorithmic details.** \n\n> \"The authors stress too much on the algorithm details. which can be deferred to the appendix for a major part of them while trying to leave some room for theoretical analysis.\"\n\nThank you for this comment. **Our aim was to show how to make the already promising SGD-based learning algorithms for Gaussian processes more effective by giving a careful treatment to algorithmic details. To do so, it is very important to precisely state the algorithm we recommend**, particularly for readers coming from the Gaussian process rather than optimisation community.\n\nAt the same time, we recognize that papers which focus more on the side of theoretical analysis are also valuable. We believe it is best to have both kinds of papers available in the literature, so that readers can more precisely find what they are looking for.\n\n**3. Comparison with other optimisers.** \n\n>  different optimizers such as AdaGrad, Adam, etc. The authors should also try these methods.\n\nWe now include an empirical comparison on the POL dataset of AdaGrad vs RMSprop vs Adam vs our method. **We show that our method compares favourably to these methods by a significant margin: see [linked figure](https://anonymous.4open.science/r/SDD-GPs-5486/rebuttal_plots/optimiser_comparison_pol.pdf) and Appendix C.2.**\n\nThank you for pointing this comparison out. We originally opted to omit these algorithms, as they are usually designed to tackle problems with a non-constant curvature. GP regression has a constant curvature: it yields a quadratic objective where Nesterov-type momentum is theoretically rate-optimal ([Dieuleveut et. al. 2017](https://arxiv.org/abs/1602.05419) and [Jain et. al. 2018](https://arxiv.org/abs/1704.08227)). However, given the prevalence of other optimisation algorithms, we agree that a comparison to them is valuable to the larger community! \n\n(continued in next comment)"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8461/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700169553200,
                "cdate": 1700169553200,
                "tmdate": 1700179729428,
                "mdate": 1700179729428,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]