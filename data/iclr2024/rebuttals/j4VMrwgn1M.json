[
    {
        "title": "Training Graph Transformers via Curriculum-Enhanced Attention Distillation"
    },
    {
        "review": {
            "id": "1e2hJBYx5W",
            "forum": "j4VMrwgn1M",
            "replyto": "j4VMrwgn1M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7112/Reviewer_imQL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7112/Reviewer_imQL"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the challenge of training Graph Transformers for semi-supervised node classification with limited labeled data. The authors propose a curriculum-enhanced attention distillation method that utilizes a Local GT teacher and a Global GT student. By introducing the concepts of in-class and out-of-class, and incorporating out-of-class entropy and top-k pruning, the authors facilitate the student's exploration under the teacher's guidance. Inspired by human learning, the method gradually allows for more out-of-class exploration while maintaining a dynamic balance. Extensive experiments demonstrate the effectiveness of the approach, surpassing state-of-the-art methods on seven public graph benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors identified the limitations of GTs in the task of semi-supervised node classification and effectively addressed them through knowledge distillation, which is a reasonable and effective approach.\n2. The proposed two significant improvements, out-of-class entropy and top-k pruning, can constrain the student\u2019s behavior during out-of-class exploration and enhance its generalization capability.\n3. The experiments demonstrate enhanced performance and improved generalization capability of the method."
                },
                "weaknesses": {
                    "value": "1. My major concern is the motivation of the distillation of GTs. Why not apply distillation on other graph networks to obtain more significant performance, e.g. GCNs? Can the proposed method be applied on other networks?\n2. In Ablation, which components are the most important? Why \"w.o. $\\gamma$\" , \"Global uniform distribution\" bring such damage for the performance?\n3. Why not compare to some general KD methods for classification task, e.g. KD, AT, FitNets, DKD, CRD?\n4. Please provide the details of Teacher and student, including architecture, parameters, etc. If the utilized \"Graph Transformer\" equal to \"Graphormer\" or \"Nodeformer\"? If not, why not conduct distillation on them?"
                },
                "questions": {
                    "value": "Please refer to Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7112/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7112/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7112/Reviewer_imQL"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7112/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698647305820,
            "cdate": 1698647305820,
            "tmdate": 1699636840617,
            "mdate": 1699636840617,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5wYRwBZdXI",
                "forum": "j4VMrwgn1M",
                "replyto": "1e2hJBYx5W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7112/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7112/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer imQL (1/3)"
                    },
                    "comment": {
                        "value": "Thanks for your valuable comments. Below please find our explanation to the listed  weaknesses.\n> **Q1**:My major concern is the motivation of the distillation of GTs. Why not apply distillation on other graph networks to obtain more significant performance, e.g. GCNs? Can the proposed method be applied on other networks?\n\n**A1**:Thank you for the question. \n\nFirst of all, our proposed method can surely be applied on other graph networks, including GCNs. \n\n\nThe motivation behind distilling GTs lies in our aim to enhance the training of Graph Transformer models through knowledge distillation, with the ultimate goal of unleashing the potential of Transformer models in node classification tasks, surpassing the performance of GNN models. Therefore, we choose Transformer models as both teacher and student models, aligning strictly with our motivation. In other words, we do not prefer GNNs to ensure a fair comparison. We aim to show the outstanding potential of Transformer-based or attention-based models enhanced by our proposed distillation framework.\n\n\nWhile initially tailored for Transformers and not specifically intended for GNNs, our method is easily adaptable and extendable to other GNN models, including GCN [1] and SGC [2], with some necessary minor modifications. Specifically, we can generate similarity matrices for non-attention models via their node embeddings. These matrices can be rough approximations of the attention matrices in our knowledge distillation framework.\n\n\nBelow are the results of our preliminary experiments involving distillation from GCN to GGT, GCN to GCN, LGT to GCN, LGT to GGT.\n\n| **Method** | **Cora** | **Citersser** |\n|:--------:|:----------:|:----------:|\n| Vanilla GCN     | 82.06 $\\pm$ 0.69 | 71.6 $\\pm$ 0.32 |\n| Vanilla LGT      | 77.40 $\\pm$ 0.56 | 68.05 $\\pm$ 1.06 |\n| Vanilla GGT      | 54.24 $\\pm$ 3.16 | 50.60 $\\pm$ 3.83 |\n| GCN $\\rightarrow$ GGT      | 82.90 $\\pm$ 0.76 | 72.46 $\\pm$ 0.95 |\n| GCN $\\rightarrow$ GCN      | 82.28 $\\pm$ 0.37 | 72.43 $\\pm$ 0.35  |\n| LGT $\\rightarrow$ GCN      | 82.85 $\\pm$ 0.85 | 73.73 $\\pm$ 0.55 |\n| LGT $\\rightarrow$ GGT      | **84.70 $\\pm$ 0.56** | **76.40 $\\pm$ 0.70** |\n\nThe above results indicate that all four distillation methods can lead to a certain degree of performance improvement in the student model. However, our method achieved the best results on both datasets, significantly outperforming other variants. We attribute this performance gap to the inherent advantages of using Transformer models as both teacher and student models.\n\nAs mentioned and discussed in Section 3.1 of the main text, the internal attention coefficients in Transformer models are automatically and inherently learned, eliminating the need to generate additional relational structures based on embeddings as required in GCNs. This ensures the reliability of attention mechanisms, as the teacher model strictly follows these learned coefficients for aggregation. In contrast, considering other models (such as GCN), the similarity matrix is derived from their node embeddings in a post-hoc manner, and thus, its reliability, interpretability, and quality heavily depend on and are restricted by the embeddings. That's why subsequent construction of relational structures (e.g., similarity matrix) cannot be guaranteed to be equally reliable, interpretable, and high-quality. From this perspective, inherent attention-based models might be more suitable and can benefit more from our distillation framework, just as consistent larger performance gains shown in the above table (average absolutely 2.35\\% improvement for the listed datasets).\n\nWe sincerely thank you for your advice again, and we have included all the discussion in Appendix H of this revision."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7112/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728586936,
                "cdate": 1700728586936,
                "tmdate": 1700728586936,
                "mdate": 1700728586936,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YHeZNNf9MN",
            "forum": "j4VMrwgn1M",
            "replyto": "j4VMrwgn1M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7112/Reviewer_4ymw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7112/Reviewer_4ymw"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel curriculum-enhanced attention distillation approach to enhance the training of Graph Transformers (GTs). This method leverages attention coefficients as knowledge representations for more effective knowledge transfer from a limited graph transformer (LGT) teacher to a general graph transformer (GGT) student. The approach also incorporates out-of-class entropy and top-k pruning to promote active exploration and the surpassing of the teacher's performance by the student. Additionally, curriculum distillation is introduced to manage the attention focus dynamically between in-class and out-of-class learning phases, thereby improving the overall performance. The proposed method has been empirically validated across multiple datasets, demonstrating its efficacy in improving the GTs training process and their generalization capabilities"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well-written and presented.\n- Empirical results are promising."
                },
                "weaknesses": {
                    "value": "- The novelty is limited. Not a big concern, but the paper combines a few well-known methods. \n- The approach is a bit complicated."
                },
                "questions": {
                    "value": "- The loss function is complex with a few components. In Table 4 the authors evaluated their pipeline's performance sensitivity to each of the components. My first question is how sensitive the model is to variation of these hyperparameters (e.g. $\\gamma$, $\\beta$)? Both in case of stability and performance.\n- Can authors please comment on the scalability of this model? Does it scale to bigger datasets? If yes, have they tried running their model on those?\n- Have the authors witnessed a difference in student or teacher behavior in datasets with relatively high homophily values differences?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7112/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698796043176,
            "cdate": 1698796043176,
            "tmdate": 1699636840496,
            "mdate": 1699636840496,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YDSeTykTi7",
                "forum": "j4VMrwgn1M",
                "replyto": "YHeZNNf9MN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7112/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7112/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 4ymw"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful comments. We reply to all your comments below.\n> **Q1**:The loss function is complex with a few components. In Table 4 the authors evaluated their pipeline's performance sensitivity to each of the components. My first question is how sensitive the model is to variation of these hyperparameters (e.g. $\\gamma$, $\\beta$)? Both in case of stability and performance.\n\n\n**A1**:Thank you for your insightful question. We have conducted a thorough sensitivity analysis of hyperparameters in Appendix F.1 of our submission. In Appendix F.1, we investigate the impact of variations in hyperparameters $\\gamma$ and $\\beta$ on the model's stability and performance. Specifically, when keeping $\\gamma$ fixed at an appropriate value, we observe that increasing $\\beta$ leads to a decline in model performance when $\\beta$ exceeds a certain threshold. Conversely, when keeping $\\beta$ fixed at an appropriate value, we observe that increasing $\\gamma$ gradually improves model performance, reaching a plateau after a certain point. Please kindly find a comprehensive analysis in Appendix F.1. We\u2019ll ensure a clear link between Section 4 and Appenxix F.1 in the revision.\n\n\n> **Q2**:Can authors please comment on the scalability of this model? Does it scale to bigger datasets? If yes, have they tried running their model on those?\n\n**A2**:We appreciate the reviewer's insight into the scalability of our method. The scalability of our method depends on the scalability of the chosen student model. We choose the vanilla Transformer [1] as our student model for simplicity. Though the specific chosen student model, i.e. GGT, in our framework has relatively lower scalability, Our method allows for the replacement of the student model effortlessly. For instance, our method can scale to larger graphs by substituting the student model with a more scalable Transformer variant, such as SGFormer [2]. Given the orthogonality and complementarity of our work, we aim to conduct in-depth exploration, investigation, and discussion of these extensions and integrations in future research, accompanied by comprehensive experimental studies. We believe that by incorporating certain methods and techniques from previous research, we can effortlessly enhance scalability and expect further performance improvements.\n\n> **Q3**:Have the authors witnessed a difference in student or teacher behavior in datasets with relatively high homophily values differences?\n\n**A3**:Compared to homogeneous graphs, node classification tasks on heterogeneous graphs have always been considered more challenging, as evident from the performance of traditional GNN models on heterogeneous graphs, as shown in Table 2. However, the framework proposed in this paper demonstrates consistent performance improvements on both homogeneous and heterogeneous graphs. The experimental results in Table 2 indicate that our method consistently outperforms the second-best method on homogeneous and heterogeneous graphs. Upon comparing the performance of teacher and student models, as shown in Table 3, we observe a more significant improvement in heterogeneous graphs. This could be attributed to the synergy between our proposed method and the Transformer model, where the teacher model provides valuable hints to the student model. This allows the student model to explore more beneficial interactions among non-neighbor nodes (i.e., discovering those distant yet potentially beneficial interactions), thus leading to a substantial enhancement in performance.\n\n**Reference**\n\n[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n\n[2] Qitian Wu, Wentao Zhao, Chenxiao Yang, Hengrui Zhang, Fan Nie, Haitian Jiang, Yatao Bian, and Junchi Yan. Simplifying and empowering transformers for large-graph representations. arXiv preprint arXiv:2306.10759, 2023."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7112/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725922143,
                "cdate": 1700725922143,
                "tmdate": 1700725922143,
                "mdate": 1700725922143,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "W02s5xCJDY",
            "forum": "j4VMrwgn1M",
            "replyto": "j4VMrwgn1M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7112/Reviewer_qbbX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7112/Reviewer_qbbX"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel curriculum-enhanced attention distillation method to improve the training of graph transformers for semi-supervised node classification. The key ideas and contributions are: \n\n- Proposes using attention coefficients from a Local Graph Transformer (LGT) teacher to guide an untrained Global Graph Transformer (GGT) student via layer-to-layer attention distillation. - Introduces the concepts of in-class and out-of-class attention. Proposes two techniques - out-of-class entropy and top-k pruning - to encourage active out-of-class exploration by the student. \n\n- Inspired by curriculum learning, proposes curriculum distillation where the teacher gradually allows more out-of-class exploration via dynamic scheduling of distillation loss weights. \n\n- Achieves significant performance gains over strong baselines on seven benchmark graph datasets, demonstrating the ability to improve training and generalization of graph transformers.\n\nOverall, the work makes multiple innovations in adapting knowledge distillation for graph transformers in a semi-supervised node classification setting. The curriculum-based attention distillation framework is intuitive and achieves strong empirical results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-  The work presents a novel perspective of applying attention-based knowledge distillation for graph transformers. Using the teacher's attention maps to guide the student is an original idea. The curriculum distillation framework that balances in-class and out-of-class attention over time is also a creative approach.\n\n-  The methodology is technically sound, with clear algorithm descriptions, reasonable design choices, and rigorous empirical evaluation. The curriculum scheduling strategies are grounded in educational learning principles. The improvements for out-of-class attention demonstrate thoughtful analysis.\n\n-  The paper is well-written and easy to follow. The motivations are clearly explained, and the methodology sections provide sufficient details. Figures aid understanding of the core concepts like curriculum scheduling."
                },
                "weaknesses": {
                    "value": "- The proposed method relies on selecting the right teacher-student pair, but the criteria for these choices are not fully analyzed. How does performance vary for different teacher-student combinations?\n\n- While outperforming baselines, the absolute performance gaps are sometimes small (~1-2%). The gains may not justify the added complexity in some applications.\n\n- Curriculum scheduling adds hyperparameters like epoch boundaries. The impact of these hyperparameters could be studied more systematically. In addition, I highly recommend that authors use auto-distillation (KD-Zero: Evolving Knowledge Distiller for Any Teacher-Student Pairs (NeurIPS-2023, Automated Knowledge Distillation via Monte Carlo Tree Search (ICCV2023)) to optimize different parameter options.\n\n\n- Attention distillation for other graph model families besides transformers could also be promising but is not explored. It is essential to incorporate a thorough discussion of relevant KD-related studies, including Self-Regulated Feature Learning via Teacher-free Feature Distillation (ECCV2022), NORM: Knowledge Distillation via N-to-One Representation Matching (ICLR2023), Shadow Knowledge Distillation: Bridging Offline and Online Knowledge Transfer (NIPS2022), DisWOT: Student Architecture Search for Distillation Without Training (CVPR2023) . This discussion will help position the proposed approach within the existing literature, establish connections, and provide valuable insights for potential comparisons."
                },
                "questions": {
                    "value": "see Weaknesses\n\n\n-------------------------------------------\n\n\nThe author's response addressed my concerns well, so I'm improving my score to acceptance, thanks!"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "no"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7112/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7112/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7112/Reviewer_qbbX"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7112/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699101676247,
            "cdate": 1699101676247,
            "tmdate": 1700732494984,
            "mdate": 1700732494984,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HECVur1cjb",
                "forum": "j4VMrwgn1M",
                "replyto": "W02s5xCJDY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7112/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7112/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer qbbX (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your review and suggestions. We reply to all your comments below.\n> **Q1**:The proposed method relies on selecting the right teacher-student pair, but the criteria for these choices are not fully analyzed. How does performance vary for different teacher-student combinations?\n\n**A1**:Thank you for raising a thoughtful question regarding the selection of the teacher-student pair. The motivation behind our work is to leverage a teacher model with decent performance and resistance to overfitting to guide a student model with significant potential but susceptibility to overfitting, enhancing the training of the student model. Given this criterion, there are multiple suitable teacher-student pairs to choose from. In our submission, we propose a generic framework, preferring simple and widely understood architectures to showcase the method's potential. The selected LGT and GGT models are fundamental Transformer encoders, emphasizing that outstanding performance can be achieved with the most basic attention layers. We also have conducted preliminary explorations of different teacher and student model combinations in the ablation studies section (Section 4.4). From the experimental results, it is clear that our method outperforms three variants: \"GGT $\\rightarrow$ GGT\", \"Local neighborhood uniform distribution\", and \"Global uniform distribution\", affirming the soundness and rationality of our design. \n\n> **Q2**:While outperforming baselines, the absolute performance gaps are sometimes small (~1-2\\%). The gains may not justify the added complexity in some applications.\n\n**A2**:Thank you for the insightful question. While, on certain datasets, the absolute performance improvement of our method over baselines may not be substantial, our method consistently improves performance across all seven datasets, with an average absolute improvement of 4.9\\%. Notably, we achieve a significant gain of 7.74\\% on the Texas dataset, as detailed in Section 4.2. Different from traditional knowledge distillation methods that mainly focus on model compression, our method presents a novel and interesting motivation, i.e., enabling the student model to surpass the performance of the teacher model (See detailed discussions in Section 4.3 in the main text for a specific distillation comparison), which is definitely ignored by prior works. Additionally, our framework's adaptability and scalability are noteworthy, relying on a proposed loss function that is easy to implement in practice.\n\n> **Q3**:Curriculum scheduling adds hyperparameters like epoch boundaries. The impact of these hyperparameters could be studied more systematically. In addition, I highly recommend that authors use auto-distillation (KD-Zero: Evolving Knowledge Distiller for Any Teacher-Student Pairs (NeurIPS-2023, Automated Knowledge Distillation via Monte Carlo Tree Search (ICCV2023)) to optimize different parameter options.\n\n**A3**:Thank you for the valuable feedback. We find your advice quite helpful and beneficial for our work. While we have discussed some important hyperparameters in Curriculum scheduling in Appendix F.1, we acknowledge that our analysis may not have been exhaustive. We appreciate the suggestion to employ auto-distillation techniques such as KD-Zero [1] and Automated Knowledge Distillation via Monte Carlo Tree Search [2], which could provide a more systematic exploration of hyperparameter options. We will thoroughly explore and incorporate these extensions into our future work. We sincerely appreciate your guidance, and we believe your advice will significantly contribute to refining our research in upcoming studies."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7112/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725475212,
                "cdate": 1700725475212,
                "tmdate": 1700725475212,
                "mdate": 1700725475212,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MLysE9RIg9",
                "forum": "j4VMrwgn1M",
                "replyto": "W02s5xCJDY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7112/Reviewer_qbbX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7112/Reviewer_qbbX"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response."
                    },
                    "comment": {
                        "value": "The author's response addressed my concerns well, so I'm improving my score to acceptance, thanks!"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7112/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732518493,
                "cdate": 1700732518493,
                "tmdate": 1700732518493,
                "mdate": 1700732518493,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hHtVKAkAXD",
            "forum": "j4VMrwgn1M",
            "replyto": "j4VMrwgn1M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7112/Reviewer_pnsY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7112/Reviewer_pnsY"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a curriculum-enhanced attention distillation method that involves utilizing a Local Graph Transformer as the teacher model and a Global Graph Transformer as the student model. They also introduce the concepts of \u201cin-class\u201d and \u201cout-of-class\u201d regions and introduce out-of-class entropy and top-k pruning. Experiments are conducted on 7 node classification benchmarks, which show the performance benefits of the proposed method."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The paper proposes an effective method of training Graph Transformers. The method is verified in multiple node classification benchmarks. The distillation comparative studies show that the method generally outperforms both the teacher and student models. The paper is well-written with justified motivations and reasonable solutions."
                },
                "weaknesses": {
                    "value": "Some of the implementation details are unclear. Please see the questions listed below."
                },
                "questions": {
                    "value": "Q1: The distillation losses are calculated by distances between the teacher and student attention coefficients, which are averaged over graph nodes and attention layers, as demonstrated by Equation (7). Does the averaging calculation smooth out the signal?\n\nQ2: What is the exact distance metric $d$ do you use?\n\nQ3: For the top-k pruning, what is the exact $k$ do you use? Does it vary significantly between datasets? For example Pubmed has 19,717 nodes while Cornell and Texas only have 183 nodes, does the k values different? \n\nQ4: What are the specific LGT and GGT models do you use in the experiments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7112/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7112/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7112/Reviewer_pnsY"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7112/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699167603571,
            "cdate": 1699167603571,
            "tmdate": 1699636840273,
            "mdate": 1699636840273,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "l7jZdgwKuU",
                "forum": "j4VMrwgn1M",
                "replyto": "hHtVKAkAXD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7112/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7112/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer pnsY"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful comments and suggestions. Please kindly find our detailed response to each raised question as below.\n> **Q1**:The distillation losses are calculated by distances between the teacher and student attention coefficients, which are averaged over graph nodes and attention layers, as demonstrated by Equation (7). Does the averaging calculation smooth out the signal?\n\n**A1**:Thank you for the in-depth question. Indeed, the averaging calculation is designated to smooth out the signal during knowledge distillation. This process effectively mitigates noise or fluctuations in individual attention coefficients, providing a stable and representative signal for guiding the learning of the student model. Importantly, knowledge distillation is a holistic process, transferring the knowledge from the teacher model to the student model on a global scale. By averaging attention coefficients across all nodes and layers, our method captures essential patterns and relationships globally, significantly enhancing the student model's performance.\n\n> **Q2**:What is the exact distance metric do you use?\n\n**A2**: In our experiments, we use cross-entropy as the distance metric, which is equivalent to the Kullback-Leibler (KL) divergence. We opt for cross-entropy mainly for its simplicity and widespread use in the machine learning community as a common metric for probability distributions. We have emphasized this point in Appendix E.2 of the revision to enhance the paper's clarity. \n\nIn addition, most distance metrics are applicable in our experiments, such as Euclidean distance. We notice that different distance metrics may impact our distillation method. An appropriate distance metric has the potential to facilitate better knowledge transfer and improve student training. However, we feel that a more in-depth exploration of this aspect goes beyond the scope of this paper and is orthogonal to our current focus. We plan to explore and extend our framework in future work to delve into this aspect more thoroughly.\n\n> **Q3**:For the top-k pruning, what is the exact $k$ do you use? Does it vary significantly between datasets? For example Pubmed has 19,717 nodes while Cornell and Texas only have 183 nodes, does the $k$ values different?\n\n**A3**:Thank you for raising a crucial question. For top-k pruning, we customize the selection of $k$ values based on dataset size. Smaller datasets like Cornell and Texas utilize $k$ values such as 5,10,15, and 20, while larger datasets have a broader but still constrained range, typically from 10 to 50. Considering that different datasets require different $k$ values, the specific $k$ values are determined through grid search, ensuring the most suitable $k$ for each dataset. For the Pubmed dataset, we set $k=20$, while for the Cornell and Texas datasets, $k=10$. In our submission, we have already included the hyperparameter search space in Appendix E.2. We acknowledge that we overlooked detailing the specific hyperparameter configurations for each dataset in the paper. In the revision, we have provided detailed information regarding the hyperparameter configurations for each dataset in Appendix E.2. We appreciate your guidance and understand the importance of this clarification for reproducibility. These additions will enhance the overall quality of our study. Once again, we express our gratitude for your professional advice.\n\n> **Q4**:What are the specific LGT and GGT models do you use in the experiments?\n\n**A4**:Thank you for addressing a vital question. For the LGT model, we implement it based on the GT model [1], which computes attention by considering only the first-order neighborhood (including the node itself) for each node. As for the GGT model, we adopt the vanilla Transformer architecture [2]. In particular, the implementation of the Transformer layer in our GGT model closely follows that of the vanilla Transformer model. In our model selection, we lean towards straightforward frameworks to demonstrate the potential of our method. The chosen LGT and GGT models are based on the fundamental Transformer encoder, emphasizing the effectiveness achievable with the native attention layer. This choice aligns with our motivation: employing a teacher model with decent performance and stability to guide a student model with significant potential but prone to overfitting. This represents a paradigm distinct from traditional knowledge distillation. We also explore various teacher-student model combinations to validate the rationale and effectiveness of our choices in Section 4.4 of the main text.\n\n**Reference**\n\n[1] Vijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs. arXiv preprint arXiv:2012.09699, 2020.\n\n[2] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7112/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700724964180,
                "cdate": 1700724964180,
                "tmdate": 1700724964180,
                "mdate": 1700724964180,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]