[
    {
        "title": "Local Search GFlowNets"
    },
    {
        "review": {
            "id": "DN3NBQldAd",
            "forum": "6cFcw1Rxww",
            "replyto": "6cFcw1Rxww",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission918/Reviewer_4fxw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission918/Reviewer_4fxw"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes an improvement to the GFlowNet training procedure that biases samples towards the high-reward terminal states. The idea is to train a back-tracking model that would go back from the completed flow trajectory in DAG, and then use the forward model to sample the removed part again.\nThe method boasts impressive performance on biological sequence design when combined with different objectives for training GFlowNets, and achieves the best performance with Trajectory Balance method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The presentation is clear\n- The idea is simple and easy to implement\n- The evaluation is comprehensive and showcases the strength of the idea"
                },
                "weaknesses": {
                    "value": "- The proposed method adds some computational overhead"
                },
                "questions": {
                    "value": "- How is the number of steps K picked for backtracking? If it's fixed, is there a way to pick it automatically?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission918/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698858781245,
            "cdate": 1698858781245,
            "tmdate": 1699636018803,
            "mdate": 1699636018803,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fFSzeMhxMP",
                "forum": "6cFcw1Rxww",
                "replyto": "DN3NBQldAd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission918/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission918/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for providing valuable feedback.\n\n---\n\n**W1: About computational overhead**\n\nLS-GFN does not introduce any additional computational overhead. Below are the wall clock times for each training round in the QM9 task.\n\n|  | $I$ |$M$ | Wall clock time per round|\n| -------- | -------- | -------- | -------- |\n| TB   | 0     | 32     |   1.93 \u00b1 0.13 seconds  | \n| TB + LS-GFN    | 7     | 4     |  1.91 \u00b1 0.07 seconds |\n\nThe wall clock time is evaluated with a single RTX 3090 GPU and Intel(R) Xeon(R) Gold 5317 CPU @ 3.00GHz and 256 GB RAM.\n\n\n\n\nThis restriction is in place to ensure that the sample complexity remains consistent with that of the baseline GFN methods.\n\nThe calculation of sampling complexity follows this formula: $(I+1) \\times M$, where $I$ signifies the number of local search iterations, and $M$ denotes the batch size per training round.\n\n| | $I$ |$M$ | Sampling Complexity per Training Round |\n| -------- |-------- |-------- |-------- |\n| GFN        | 0     | 32     | $1 \\times 32 = 32$     |\n| LS-GFN         | 7     | 4     |$(7+1) \\times 4 = 32$      |\n\n---\n\n**Q1: How is the number of steps K picked for backtracking? If it's fixed, is there a way to pick it automatically?**\n\nWe used $K=\\lfloor (L+1) / 2\\rfloor$, where $L$ is length of trajecotry. We also provided a hyperparameter analysis in the original manuscript in Appendix B.5. As you mentioned, we can automatically pick $K$ to adaptively balance the wideness of the local search region, which could be a great avenue for future work."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission918/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699957499703,
                "cdate": 1699957499703,
                "tmdate": 1699957529466,
                "mdate": 1699957529466,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JXChBRaaIB",
            "forum": "6cFcw1Rxww",
            "replyto": "6cFcw1Rxww",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission918/Reviewer_xEGb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission918/Reviewer_xEGb"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes Local Search GFlowNet (LS-GFN) for training GFlowNets with local search to enhance the training effectiveness. The proposed algorithm explores the local neighborhood through destruction and reconstruction guided by backward and forward policies, respectively. Extensive experiments demonstrate significant performance improvements in several biochemical tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper introduces an algorithm which combines inter-mode global exploration with intra-mode local exploration in GFlowNets training.\n2. The paper is well-written and easy to understand. The proposed algorithm and experimental setup are clearly described."
                },
                "weaknesses": {
                    "value": "1. The paper does not declare the sampling complexity of the proposed method. The local search may require more sampling, which can lead to an unfair comparison. \n2. The limitations of the proposed algorithm and potential drawbacks are not discussed in detail."
                },
                "questions": {
                    "value": "1. What about the sampling complexity of local search? Can you conduct experiments under the same sampling complexity or compare your algorithm with an upper bound sampling times?\n2. How to decide the local search interaction I? What about the results with different I?\n3. As PRT is adopted in LS-GFN training, is PER used for RL methods in te experiments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission918/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission918/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission918/Reviewer_xEGb"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission918/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699084772046,
            "cdate": 1699084772046,
            "tmdate": 1700887579973,
            "mdate": 1700887579973,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gfMfm7cXmN",
                "forum": "6cFcw1Rxww",
                "replyto": "JXChBRaaIB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission918/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission918/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the valuable feedback. \n\n---\n\n**W1, Q1: The paper does not declare the sampling complexity of the proposed method. The local search may require more sampling, which can lead to an unfair comparison.**\n\nThank you for highlighting this for clarification. All our experiments were conducted under a fair setting, as we used exactly the same number of samples across all experiments. Note this is already outlined in Section 5.3.\n\nThe calculation of sampling complexity follows this formula: $(I+1) \\times M$, where $I$ stands for the number of local search iterations, and $M$ denotes the batch size per training round. \n\n\n\n| | $I$ |$M$ | Sampling Complexity per Training Rounds |\n| --------  | -------- |-------- |-------- |\n| GFN      | 0     | 32     | $1 \\times 32 = 32$     |\n| LS-GFN        | 7     | 4     |$(7+1) \\times 4 = 32$      |\n\n---\n\n**W2: The limitations of the proposed algorithm and potential drawbacks are not discussed in detail.**\n\n\nA limitation of LS-GFN lies in the potential impact of the quality of the backward policy on its performance, particularly when the acceptance rate of the local search becomes excessively low. One immediate remedy is to introduce an exploratory element into the backward policy, utilizing techniques like $\\epsilon$-greedy or even employing a uniform distribution to foster exploration within the local search.\n\nA promising avenue for future research could involve fine-tuning backward policy to enhance the acceptance rate of the local search. We plan to incorporate this as a topic in our paper's \"Limitations and Further Work\" after we get one additional page for the final paper (we now put this into the appendix), and we appreciate your valuable suggestion.\n\n---\n\n**Q2: How to decide the local search interaction I? What about the results with different I?**\n\nWe have conducted an ablation study on the hyperparameter $I$, as now detailed in Appendix B.3. \n\nIt's worth noting that across various hyperparameter candidates, namely $I \\in \\{1, 3, 7, 15, 31\\}$, LS-GFN ($I > 0$) consistently outperforms GFN ($I = 0$). This observation suggests that choosing the appropriate hyperparameter $I$ for LS-GFN is relatively straightforward, as it consistently yields superior results.\n\n---\n\n**Q3: As PRT is adopted in LS-GFN training, is PER used for RL methods in te experiments?**\n\nTaking into consideration your feedback, we implemented reward-prioritized experience replay training in conjunction with the off-policy RL baseline, SQL. It's worth noting that PPO and A2C are on-policy RL methods in which replay training is not directly utilized. Here are new experiment results that compare replay-trained SQL and LS-GFN:\n\n\n\n| | Number of Modes | TopK reward |\n| -------- | -------- | -------- |\n| SQL    | 232 \u00b1 8     | 0.56 \u00b1 0.01     |\n|   SQL (replay trained)   | 235 \u00b1 6     | 0.56 \u00b1 0.01     |\n| TB + LS-GFN   | 793 \u00b1 4     | 0.67 \u00b1 0.00     |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission918/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699957447509,
                "cdate": 1699957447509,
                "tmdate": 1699958611828,
                "mdate": 1699958611828,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jazpW7vg2h",
            "forum": "6cFcw1Rxww",
            "replyto": "6cFcw1Rxww",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission918/Reviewer_wUkH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission918/Reviewer_wUkH"
            ],
            "content": {
                "summary": {
                    "value": "- This paper introduces local-search generative flow networks (LS-GFN), a method to improve exploitation in generative flow networks (GFNs).\n- The authors argue that, while GFNs explore effectively, their learning is hampered by exploitation \u2014 that is, they sample candidate objects that are mostly good but fail to obtain the highest rewards.\n- To address this limitation, they propose to refine candidate objects by backtracking\n- They validate LS-GFNs on real-world biochemical testbeds, where LS-GFNs significantly outperforms alternatives.\n- The paper also includes a well-written introduction to GFNs (Section 3) which covers all the necessary background for the paper."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Novelty: the method is novel as it cleverly takes advantage of both the probabilistic forward and backward policy of GFNs.\n- Significance: the proposed methods exhibits excellent empirical results and drastically outperforms the other baselines. I wish there were more of them, but more on that below.\n- Clarity: the paper is well written and provides an exceptionally clear and concise introduction to generative flow networks. Moreover the method is quite simple (I think I could replicate the authors\u2019 results) so it\u2019s likely to be adopted by the community."
                },
                "weaknesses": {
                    "value": "- While the method itself is simple and well presented, it remains a little unclear the relative importance of different components. For example, the exposition focuses on the trajectory balanced objective \u2014 does this mean LS-GFNs don\u2019t work with different training objectives? The paper also mentions building on top of Shen et al., 2023 which introduces prioritized replay training (PRT) to GFNs, but this method doesn\u2019t appear as a baseline \u2014 so how much of the improvements are due to PRT and how much is due to the local search?\n- In addition to the hyper-parameter $I$ (number of revisions with local searches), I wished the number of backtracking steps and the acceptance rate were also studied \u2014 how much tuning did they require in order to get these good results?\n- Wall-clock time is never mentioned \u2014 how much overhead does the refining process of LS-GFNs incur? How about in terms of sampled states (instead of training rounds)?\n- One baseline I wished were included: given a first trajectory, resample the last K steps and only keeping the best candidate. This is akin to beam search in LLMs or go-explore in RL. Other nice-to-have baselines include top-p and top-k sampling."
                },
                "questions": {
                    "value": "- Why call it \u201cdestroy\u201d  since the original trajectory isn\u2019t always discarded? A more intuitive name could be \u201cbacktrack\u201d, \u201crewind\u201d, or anything that doesn\u2019t suggest destruction.\n- The top plots in Figure 5 are strange: it looks like some curves go beyond 100% accuracy. Could you either fix them so we can still see the curves on the plot, or explain what is happening?\n- How can LS-GFNs recover from a biased backward policy? In other words, assume the forward policy is fine but the backward policy always backtracks to states which yield the same (high reward) candidate objects \u2014 how can LS-GFNs overcome this lack of exploration?\n- Please confirm that lines 7 and 8 in Algorithm 1 aren\u2019t swapped. If they aren\u2019t (which partially addresses my question above), wouldn\u2019t swapping them and extending $\\mathcal{D}$ with $\\tau_m$ further improve exploitation? Maybe this should be added as an ablation as well."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission918/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699245600183,
            "cdate": 1699245600183,
            "tmdate": 1699636018654,
            "mdate": 1699636018654,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "W7Nd0zXKbH",
                "forum": "6cFcw1Rxww",
                "replyto": "jazpW7vg2h",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission918/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission918/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your valuable comments. \n\n---\n\n**W1-1: While the method itself is simple and well presented, it remains a little unclear the relative importance of different components. For example, the exposition focuses on the trajectory-balanced objective \u2014 does this mean LS-GFNs don\u2019t work with different training objectives?**\n\n\nLS-GFN is a versatile technique that can be effectively employed for a wide range of GFN objectives. We have already successfully applied LS-GFN to various GFN objectives, including Sub-trajectory balance (SubTB), detailed balance (DB), and maximum entropy GFN (MaxEnt), as demonstrated in Table 1 and Table 2 in our original manuscript.\n\n\n---\n\n**W1-2: The paper also mentions building on top of Shen et al., 2023 which introduces prioritized replay training (PRT) to GFNs, but this method doesn\u2019t appear as a baseline \u2014 so how much of the improvements are due to PRT and how much is due to the local search?**\n\nFor a fair comparison, we consistently use \"PRT\" for training and \"SSR\" for parameterization in all GFN baselines. Additionally, for direct comparison purposes, we assessed our approach against Shen et al., 2023, denoted as \"GTB\" in Figure 5 and Figure 6.\n\nIt's important to note that Shen et al., 2023 introduced three distinct techniques: \"PRT\" (pertaining to dataset sampling), \"SSR\" (concerning GFN parameterization), and \"GTB\" (associated with the guided trajectory balance objective).\n\n---\n\n**W2: In addition to the hyper-parameter \n (number of revisions with local searches), I wished the number of backtracking steps and the acceptance rate were also studied \u2014 how much tuning did they require in order to get these good results?**\n \nWe've already conducted experiments on hyperparameter $I$ in Appendix B.3, as well as explored the acceptance rate in Appendix B.6.\n\nIt's worth noting that across various hyperparameter candidates, namely $I \\in \\{1, 3, 7, 15, 31\\}$, LS-GFN ($I > 0$) consistently outperforms GFN ($I = 0$). This observation suggests that choosing the appropriate hyperparameter $I$ for LS-GFN is relatively straightforward, as it consistently yields superior results.\n\n---\n\n**W3: Wall-clock time is never mentioned \u2014 how much overhead does the refining process of LS-GFNs incur? How about in terms of sampled states (instead of training rounds)?**\n\n\nThere's negligible additional wall clock time overhead.\n\nHere are the wall clock times for each training round in the QM9 task. Wall clock time is computed as the mean of three independent seeds. To remove the bias from warmup, we compute the wall clock time per round after 10 initial training rounds. There is no meaningful wall time difference between the two methods: \n\n|  | $I$ |$M$ | Wall clock time per round|\n| -------- | -------- | -------- | -------- |\n| TB   | 0     | 32     |   1.93 \u00b1 0.13 seconds  | \n| TB + LS-GFN    | 7     | 4     |  1.91 \u00b1 0.07 seconds | \n\n\nThe wall clock time is evaluated with a single RTX 3090 GPU and Intel(R) Xeon(R) Gold 5317 CPU @ 3.00GHz and 256 GB RAM.\n\n\nThe wall clock time remains consistent because we always use the same number of samples per training round ($M \\times (I+1) = 32$, $M$: batch size, $I$: number of local searches) for every method across all experiments. This choice is particularly crucial considering that evaluating the reward per sample is a significant bottleneck (in some applications, one could spend a day for reward computation), especially in real-world scenarios like bio and chemical discovery tasks. By keeping the sample complexity uniform, we ensure that the computational overhead remains stable."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission918/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699956840633,
                "cdate": 1699956840633,
                "tmdate": 1699957681960,
                "mdate": 1699957681960,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bv214AXd7d",
                "forum": "6cFcw1Rxww",
                "replyto": "jazpW7vg2h",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission918/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission918/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Q1: Why call it \u201cdestroy\u201d since the original trajectory isn\u2019t always discarded? A more intuitive name could be \u201cbacktrack,\u201d \u201crewind,\u201d or anything that doesn\u2019t suggest destruction.**\n\nWe are in agreement on this matter. We have updated our manuscript to incorporate the term \"backtrack\" instead of \"destroy.\"\n\n---\n\n**Q2: The top plots in Figure 5 are strange: it looks like some curves go beyond 100% accuracy. Could you either fix them so we can still see the curves on the plot or explain what is happening?**\n\nPlease note that Figure 5 is plotted accurately in accordance with the baseline from the paper by Shen et al., 2023, where we have set our accuracy metrics following their guidelines:\n\n\n$\\text{Acc}(p(x;\\theta)) = 100 \\times \\text{min}(\\frac{E_{p(x;\\theta)}[R(x)]}{E_{p^{*}(x)}[R(x)]},1)$\n\n\nThis provides an upper bound of 100% accuracy. In response to your suggestion, we have included a graph illustrating accuracy beyond the 100% mark in Appendix B.7.\n\n---\n\n**Q3: How can LS-GFNs recover from a biased backward policy? In other words, assume the forward policy is fine but the backward policy always backtracks to states that yield the same (high reward) candidate objects \u2014 how can LS-GFNs overcome this lack of exploration?**\n\nBiased backward policies can indeed pose challenges. In such scenarios, we can employ an $\\epsilon$-greedy method to introduce exploratory back-sampling into the biased backward policy. It's worth noting that LS-GFN performs exceptionally well in MaxEnt GFN, even when the backward policy is set to a uniform distribution (i.e., $\\epsilon = 1$).\n\nBuilding upon your valuable feedback, one potential avenue for future research could involve fine-tuning the backward policy to maximize the local search acceptance rate. This approach would aim to ensure that the backward policy identifies diverse intermediate states that lead to improved rewards without exhibiting a bias towards yielding the same candidate objects repeatedly. We appreciate your insightful comment.\n\n---\n\n**Q4: Please confirm that lines 7 and 8 in Algorithm 1 aren\u2019t swapped. If they aren\u2019t (which partially addresses my question above), wouldn\u2019t swapping them and extending them further improve exploitation? Maybe this should be added as an ablation as well.**\n \nNo swapping is involved here. We update every sample to the dataset, irrespective of whether it is accepted or not. The rationale behind this approach is that every sample where the reward is calculated is considered valuable. During the refinement phase, our primary objective is to explore the local region in order to discover high-reward samples rather than determining whether the found sample should be used in training.\n\nIn the training process, we employ PRT to sample trajectories in proportion to their rewards from the dataset. Consequently, accepted samples have a higher likelihood of being included during training.\n\nRegarding your suggestion to swap lines 7 and 8, it would entail updating the data only with accepted samples. However, as you pointed out, while this could enhance exploitation, it might excessively hinder exploration. To shed more light on this, here are the new results of our ablation study:\n\n\n\n| Column 1 | Number of modes| TopK reward |\n| -------- | -------- | -------- |\n| Swapped version     | 754 \u00b1 6    | 0.66 \u00b1 0.00    |\n| Original LS-GFN    | 793 \u00b1 4     | 0.67 \u00b1 0.00    |\n\nBased on the above results, the original LS-GFN gives higher performances than the swapped version. \n\n---\n\n**References**\n\nMax W Shen, Emmanuel Bengio, Ehsan Hajiramezanali, Andreas Loukas, Kyunghyun Cho, and\nTommaso Biancalani. Towards understanding and improving GFlowNet training. In International\nConference on Machine Learning, pp. 30956\u201330975. PMLR, 2023"
                    },
                    "title": {
                        "value": "Responses to the questions"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission918/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699957015088,
                "cdate": 1699957015088,
                "tmdate": 1699957752155,
                "mdate": 1699957752155,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]