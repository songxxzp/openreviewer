[
    {
        "title": "Efficient Sharpness-Aware Minimization for Molecular Graph Transformer Models"
    },
    {
        "review": {
            "id": "8DZ37Unp5y",
            "forum": "Od39h4XQ3Y",
            "replyto": "Od39h4XQ3Y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission658/Reviewer_1va4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission658/Reviewer_1va4"
            ],
            "content": {
                "summary": {
                    "value": "This paper designs a computation efficient variant of sharpness aware minimization (SAM) for molecular graph transformers."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "S1. This paper observes that flat minima generalize better for molecular graph transformers. This extends the boundary of sharpness awareness to a new domain.\n\nS2. To overcome the computational burden, this paper leverages domain specific knowledge to design GraphSAM. Supporting theories are derived to explain the reasons behind design.\n\nS3. GraphSAM improves throughput with comparable performance with SAM. Compared to other variants of computation efficient SAM, GraphSAM performs better in the considered setting."
                },
                "weaknesses": {
                    "value": "W1. Can the authors elaborate more on equation (3)? In particular, are there any specific reasons for choosing $\\epsilon_t$ as the moving average of $w_t/\\|w_t \\|$? What will happen if $\\epsilon_t$ becomes moving average of $w_t$?\n\nW2. What are the specific reasons behind the choice of StepLR for the $\\rho$-schedulers? Does other approaches, such as cosine schedulers or inverse square root schedulers help?"
                },
                "questions": {
                    "value": "Q1. Table 1 does not fit into the paper properly.\n\nQ2. Some of the expressions are unnecessarily complicated. For example, in equation (2) and (3), $sign(\\epsilon_t)|\\epsilon_t|$ can be replaced by $\\epsilon_t$.\n\nQ3. Missing references on SAM variants. \n\n[1] Du, Jiawei, Daquan Zhou, Jiashi Feng, Vincent YF Tan, and Joey Tianyi Zhou. \"Sharpness-Aware Training for Free.\" arXiv preprint arXiv:2205.14083 (2022).\n\n[2] Li, Bingcong, and Georgios B. Giannakis. \"Enhancing Sharpness-Aware Optimization Through Variance Suppression.\" arXiv preprint arXiv:2309.15639 (2023)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission658/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697957194451,
            "cdate": 1697957194451,
            "tmdate": 1699635993351,
            "mdate": 1699635993351,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wHIq1hChgV",
                "forum": "Od39h4XQ3Y",
                "replyto": "8DZ37Unp5y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission658/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission658/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate a lot for your careful review, and would like to provide responses to your mentioned questions and weaknesses one by one.\n\n###**[W1 - What will happen if $\u03b5_{t+1}$ becomes moving average of $w_{t}$? \u2014 It can cause gradient explosion.]**\n\nThank you for your insightful observation. Utilizing $|w_{t}|$ without normalization could make $\u03b5_{t+1}$ grow excessively large, which might trigger a gradient explosion. Such an occurrence would critically disrupt the training process, rendering it not only ineffective but also at risk of divergence.\nIn Eq. (3), the choice to use $w_{t}/|w_{t}|$ for updating $\u03b5_{t+1}$ is deliberate and crucial for the stability and effectiveness of the update process. This formulation ensures that the update direction of $\u03b5_{t+1}$ is determined by $w_{t}$ while its magnitude is controlled by the factor (1-\u03b2). By normalizing  $w_{t}$ with its magnitude $|w_{t}|$ , we prevent the scale of $w_{t}$ from disproportionately influencing the update of $\u03b5_{t+1}$. If we were to use $w_{t}$ directly for updating $\u03b5_{t+1}$, the absence of normalization would lead to significant issues. Our experiments have demonstrated that $|w_{t}|$ >> $|\u03b5_{t}|$, further underscoring the necessity of this normalization step.\n\n###**[W2 - Does Cosine schedulers or inverse square root schedulers help? \u2014 They have some effect, but StepLR is the best.]**\n\n\nThank you for your question regarding the choice of the StepLR scheduler for the \u03c1-schedulers in our model. The decision to use StepLR was primarily guided by its simplicity and effectiveness in controlling the learning rate in discrete steps. This allows for a straightforward reduction of the \u03c1 size at predetermined epochs, which we found to be beneficial for the steady convergence to a flat region of our model's parameters. In response to your suggestion, we have added the following two sets of experiments:\n\nCosineLR: $\u03c1_{new}$=$\\frac{\u03c1_{initial}}{2}*(1+cos(\\frac{epoch\u00b7\u03c0}{EPOCH}))$\n\nInverseSquareLR: $\u03c1_{new}$=$\\frac{\u03c1_{initial}}{\\sqrt{max(epoch,k)}}$\n\nwhere epoch is the current training epoch, EPOCH is total training epoch, and k=1, $\u03c1_{initial}$ = 0.05.\n\n|GROVER+GraphSAM | BBBP(ROC-AUC\u2191) | Tox21 (ROC-AUC\u2191)| Sider (ROC-AUC\u2191)| Clintox(ROC-AUC\u2191)|ESOL(RMSE\u2193) | Lip(RMSE\u2193)|\n|   :----: |   :---: |    :---: |  :---: |    :---: |    :---: |    :---: |\n| Original model| 0.917 | 0.822| 0.649 |0.853|0.639 | 0.671|\n| +StepLR| **0.928** | **0.846**| **0.665** |**0.866**|**0.625** | **0.654**|\n| +CosineLR | 0.921 | 0.828| 0.651|0.856| 0.633|0.661\n| +InverseSquareLR | 0.924 | 0.836 |0.654|0.858|0.628|0.665\n\nWe acknowledge that other approaches, such as cosine schedulers or inverse square root schedulers, could also be effective.  While these alternative schedulers present intriguing possibilities, our initial experiments with StepLR showed the most promising results, which led to its selection for the current study. However, we recognize the value in exploring these alternative scheduling methods in future work, as they may offer different advantages in specific training contexts or with certain types of datasets.\n\n\n###**[Q1 - Table 1 does not fit into the paper properly. \u2014 Fixed, and thank you for the close read!]**\n\n\n###**[Q2 - $sign(\u03b5_{t})|\u03b5_{t}|$ can be replaced by $\u03b5_{t}$! \u2014 Yes, you are right and we fixed it.]**\n\n###**[Q3 - Missing references on SAM variants. \u2014 We have added them.]**\n\nThank you for highlighting the omission of references related to SAM variants in our manuscript. In our revised manuscript, we will include and discuss the two SAM variants [1, 2] in the related work.\n\n**References:**\n\n[1] Du, Jiawei, Daquan Zhou, Jiashi Feng, Vincent YF Tan, and Joey Tianyi Zhou. \"Sharpness-Aware Training for Free.\" arXiv preprint arXiv:2205.14083 (2022).\n\n[2] Li, Bingcong, and Georgios B. Giannakis. \"Enhancing Sharpness-Aware Optimization Through Variance Suppression.\" arXiv preprint arXiv:2309.15639 (2023)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission658/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700543193763,
                "cdate": 1700543193763,
                "tmdate": 1700543193763,
                "mdate": 1700543193763,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UMiLyK5F8c",
            "forum": "Od39h4XQ3Y",
            "replyto": "Od39h4XQ3Y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission658/Reviewer_mSQh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission658/Reviewer_mSQh"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes GraphSAM to reduce the training cost of sharpness-aware minimization (SAM) and improve the generalization performance of graph transformer models. GraphSAM uses the updating gradient of the previous step to approximate the perturbation gradient at the intermediate steps smoothly and theoretically proves that the loss landscape of GraphSAM is limited to a small range centered on the expected loss of SAM. Extensive experiments on six datasets with different tasks demonstrate the effectiveness and efficiency of GraphSAM."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- **Motivation is clear**: SAM is a powerful optimizer but doubles the computational cost compared with the base optimizer. Thus, **improving its efficiency** (the focus of this paper) is an important problem\n- GraphSAM improves the efficiency of SAM by **approximating the perturbation gradient** $\\epsilon_{t+1}$ as the moving average of $\\epsilon_t$ and $\\omega_{t}$ based on two observations: (i) $\\epsilon_t$ is close to $\\epsilon_{t+1}$ (Figure 3(a)) and (ii) $\\epsilon_{t+1}$ is close to $\\omega_t$ (Figure 3(c))\n- By approximating the perturbation gradient, GraphSAM almost **does not need to compute the perturbation gradient** (only once per epoch). Thus, GraphSAM is efficient\n- Experimental results on the graph dataset show that GraphSAM is comparable with SAM (Tables 1 and 2) and is more efficient (Table 2)"
                },
                "weaknesses": {
                    "value": "- **writing**:\n  - The colors in Figure 1 are inconsistent with the observations: \"compared to the base optimizer of Adam (the blue bar)\", but the blue is LookSAM? also, SAM (cyan)?\n  - Figure 1, it is better to include the proposed GraphSAM as well\n  - Related works: \"most of the work still ignores the fact of SAM\u2019s double overhead (Damian et al., 2021; Kwon et al., 2021;Wang et al., 2022).\" I think some methods have attempted to mitigate this issue, e.g., AE-SAM, SS-SAM, better to discuss them here\n  - Section 3, \"SAM consumes double overhead due to the extra computation of perturbation gradient compared with the base optimizer.\" existing methods (AE-SAM, SS-SAM) have tried to mitigate this issue, better to discuss them here, and why they cannot be used for graph datasets, this is also the motivation for the proposed GraphSAM\n  - Figure 3, sub-caption for each subfigure\n  - Table 1: also compare with GROVER+AE-SAM/SS-SAM\n  - \"AE-SAM and RST periodically compute the update gradient by different strategies\": AE-SAM is an adaptive strategy, not a periodic strategy\n- it seems that the proposed GraphSAM is general, not limited to graph transformers, and can also be used in computer vision. Thus, it is better to conduct some experiments on computer vision.\n- ablation study for hyperparameters $\\gamma, \\lambda, \\beta$\n- Theorem 2, based on the proof, the proportion ratio depends on the $\\theta$ (Eq(7)), it is trivial. if we assume the loss is Lipschitz, then the constant can be independent of $\\theta$."
                },
                "questions": {
                    "value": "see the questions in weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission658/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698296073352,
            "cdate": 1698296073352,
            "tmdate": 1699635993279,
            "mdate": 1699635993279,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZlBWkMb9R5",
                "forum": "Od39h4XQ3Y",
                "replyto": "UMiLyK5F8c",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission658/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission658/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 1/2"
                    },
                    "comment": {
                        "value": "Thank you for the many valuable comments on writing, we have also been corrected! Notably, the SS-SAM you mention is the same paper as the RST compared in our paper.\n\n###**[W1.1 - The colors in Fig.1 are inconsistent \u2014 Fixed, and thank you for the close read! ]**\n\n###**[W1.2 - Add GraphSAM in Fig.1 \u2014 We have added GraphSAM in Fig.1.]**\n\n###**[W1.3 and W1.4 - Discuss existing efficient methods, e.g., AE-SAM and SS-SAM. \u2014 We have discussted and compared them in Appendix.]**\n\nIndeed, we have provided a detailed comparison of these efficient SAM versions, including AE-SAM and SS-SAM(RST), in Appendix A.4 and A.5. We specifically discuss why these methods are less suited for molecular graph datasets and how GraphSAM is uniquely designed to address the challenges inherent to such data.\n\nFor AE-SAM, the trend of its squared stochastic gradient norms in CV domain is not consistent with the molecule data. This leads to a decrease in its sensitivity, which is not as good as its performance in CV, but works better than other methods.\n\nFor SS-SAM, the advantage is efficiency; but the disadvantage is obvious: random decisions about whether or not to update the gradient can lead to performance reduction. Its performance in CV literature is also poor.\n\n###**[W1.5 - sub-caption for each subfigure. \u2014 We put the sub-caption for each subfigure uniformly into the description.]**\n\n###**[W1.6 - Table 1: also compare with GROVER+AE-SAM/SS-SAM. \u2014 We have compared with CoMPT and Grover (+ every efficient SAM) in detail in Table 2 and Table 7.]**\n\n###**[W1.7 - Description of AE-SAM is incorrect. \u2014 Fixed, and thank you for the close read!]**\n\n\n###**[W2 - GraphSAM can also be used in CV? \u2014 Yes, but it does not perform ideally.]**\n\nSorry, GraphSAM doesn't perform well in the CV domain. It is experimentally found that GraphSAM is a good approximation for graph transformers rather than in CV domain. Although our proposed method is designed based on the empirical observations, actually, Observations 1 and 2 are presented only in the graph transformers, while they do not appear in those of CV domain [1]. In CV, the model's perturbation gradient variations are often consistent with updating gradients, which is not in line with the phenomenons on the molecular graphs.\n\nTo further address the concern and provide holistic discussion, we add the following experiments to compare the performance of lookSAM [1], AE-SAM and GraphSAM in the CV and molecule datasets, respectively. LookSAM and GraphSAM are efficient SAMs designed based on the gradient varification patterns of models in their respective domains, and AE-SAM is an adaptively-updating efficient method accoding to the comparison evaluation between gradient norm and a pre-defined threshold.\n\n|ResNet-18 (CV) | CIFAR-10 | CIFAR-100|\n|   :----: |   :---: |    :---: |  \n| +SGD| 95.41 | 78.21|\n| +SAM| **96.53** | 80.18|\n| +LookSAM | 96.28 | 79.91|\n| +AE-SAM |96.49 | **80.31**|\n| +GraphSAM | 95.86 | 78.69 |\n\n\n|CoMPT (Graph) | Tox21 | Sider| ClinTox|\n|   :----: |   :---: |    :---: |   :---: |\n| +Adam| 82.81 | 62.17| 91.41|\n| +SAM| 83.96 | 64.33| 92.73|\n| +LookSAM | 82.55| 62.54| 91.64|\n| +AE-SAM | 83.33| 63.16| 91.92|\n| +GraphSAM | **84.11** | **64.58** |**93.78** |\n\n\nIt is observed LookSAM has better results on the image datasets instead on the molecular graphs investigated in this work. On the contrary, our GraphSAM works on the molecules but leads to the worst performances on the image benchmarks. That is because model gradient variations are diverse accross the different domains. It is challenging to transfer the efficient SAMs designed based on the specific gradient patterns. AE-SAM achieves an acceptable performance on the molecular graphs since the perturbation gradient norms of graph transformers are monitored to inform the necessarity of gradient re-computation. But it is not as good as GraphSAM where we accurately fit the perturbation gradients at each step. In summary, GraphSAM is optimized particularly for the graph transformers based on the empirical observations of gradient variations.\n\n**References:**\n\n[1] Yong Liu, Siqi Mai, Xiangning Chen, Cho-Jui Hsieh, Yang You. Towards efficient and scalable sharpness-aware minimization. CVPR 2022."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission658/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700542925040,
                "cdate": 1700542925040,
                "tmdate": 1700543051624,
                "mdate": 1700543051624,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QwAx1VYbUd",
            "forum": "Od39h4XQ3Y",
            "replyto": "Od39h4XQ3Y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission658/Reviewer_sT1K"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission658/Reviewer_sT1K"
            ],
            "content": {
                "summary": {
                    "value": "The study introduces Sharpness-aware minimization (SAM) in computer vision, a technique effective in eliminating sharp local minima in training trajectories and countering generalization degradation. However, its dual gradient computations during optimization increases time costs. To address this, a novel algorithm named GraphSAM is proposed. It lowers the training expenses of SAM while enhancing the generalization performance of graph transformer models. The approach is underpinned by two main strategies: gradient approximation and loss landscape approximation. Empirical tests across six datasets validate GraphSAM's superiority, particularly in refining the model update process. Anonymized code is also provided for further reference."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) The paper is exceptionally coherent in its writing. The content is presented seamlessly, and the expression is clear.\n\n(2) This paper is both conceptually and technically innovative. It uniquely reutilizes the updating gradient from the previous step, approximating the perturbation gradient in an efficient manner.\n\n(3) The experimental design of this study is well-conceived. It systematically validates the representational capabilities of GraphSAM. Additionally, the appendix offers an extensive set of supplementary experiments."
                },
                "weaknesses": {
                    "value": "(1) In Observation 1, the author notes the differing changes of the perturbation gradient and the updating gradient throughout the training process. I wonder, is this a general phenomenon? Could more case studies be provided to illustrate this further?\n\n(2) The results presented in Table 1 seem to show limited improvement on certain datasets. I suggest the author consider incorporating an efficiency aspect into the table. Efficiency is a crucial contribution, yet it hasn't been emphasized adequately in the experimental section, which seems inconsistent."
                },
                "questions": {
                    "value": "see weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission658/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission658/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission658/Reviewer_sT1K"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission658/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698325310072,
            "cdate": 1698325310072,
            "tmdate": 1699635993212,
            "mdate": 1699635993212,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FbcKQI6mlu",
                "forum": "Od39h4XQ3Y",
                "replyto": "QwAx1VYbUd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission658/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission658/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate a lot for your careful review, and would like to provide responses to your mentioned questions and weaknesses one by one.\n\n\n###[**W1 -  Is the differing changes of gradient a general phenomenon? \u2014 Yes, it is.]**\n\nThe observation of differing changes in the perturbation gradient and the updating gradient throughout the training process is indeed a general phenomenon. To further substantiate this, we have enriched the Figure 8 in Appendix 8 with additional case studies illustrating these variations. They are consistent with the conclusions in Observation 1.\n\n\n###[**W2 -  Efficiency is a crucial contribution, add experiments to illustrate \u2014 Agreed, we have compared the efficiency in Table 2 and Table 7.]**\n\nWe have included comprehensive efficiency comparisons in Table 2 of the main paper and Table 7 of Appendix A.5. These tables specifically highlight the efficiency aspects of GraphSAM, offering a balanced view of both its performance and computational efficiency."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission658/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700542830039,
                "cdate": 1700542830039,
                "tmdate": 1700542830039,
                "mdate": 1700542830039,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "psYtMk8JtI",
            "forum": "Od39h4XQ3Y",
            "replyto": "Od39h4XQ3Y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission658/Reviewer_QvPb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission658/Reviewer_QvPb"
            ],
            "content": {
                "summary": {
                    "value": "The authors study Sharpness Aware Minimization (SAM), an effective training objective in the image domain to avoid \"sharp minima\", in the context of graph transformers for molecular data. The proposed efficient SAM-alternative, termed GraphSAM, empirically performs on par with SAM while being more efficient."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. More efficient approach for SAM (roughly 30%-50% faster training)\n1. GraphSAM consistently improves the performance of the base model\n1. Approach is ablated and rich explanations are provided\n1. The paper is well organized and it is easy to follow."
                },
                "weaknesses": {
                    "value": "1. The relation to graph machine learning is a bit obscure: is GraphSAM only a good approximation for graph transformers? After all, GraphSAM does not rely on anything graph-related besides the observations made with SAN + graph transformers. Although it is fine to focus on molecular data, the paper would benefit significantly from a holistic discussion (and some rudimentary experiments).\n1. The theoretical statements could be better motivated and more decisively embedded into the story. It remains a bit vague what the actual implications are. For example, the conclusion of Theorem 1 is speculative \"by replacing the inner maximum of LG(\u03b8 + \u03b5\u02c6S) with the maximum of LG(\u03b8 + \u03b5\u02c6G), we tend to smooth the worse neighborhood loss in the loss landscape. In other words, if one could minimize the upper bound given by LG(\u03b8 + \u03b5\u02c6G), the final loss landscape is as smooth as that obtained from SAM.\" Alternatively, the wording may be improved. It is not clear why optimizing the upper bound necessarily results in a \"loss landscape is as smooth as that obtained from SAM\".\n1. Proof of Theorem 1 relies on empirical observations and (from my perspective) strong assumptions. However, the authors do sufficiently not discuss this in the main part and, thus, the current presentation is misleading. In other words, these assumptions, etc. should be made explicit in a rather prominent way in the main part (e.g. \"and after ||\u03c90 ||2 experimental analysis, the updating gradient \u03c9 \u226b \u03b5 \" is not stated clearly in main part). I think it would be better to drop the term \"Theorem\" and rather give some intuitive, mathematically motivated explanation.\n\nMinor:\n1. Missing space in the first line of page 2\n1. The bold highlighting in Table 1 is counter-intuitive. Perhaps add an additional marker to highlight the best model per task."
                },
                "questions": {
                    "value": "1. Is GraphSAM only a good approximation for graph transformers? How is GraphSAM working, e.g., in the image domain?\n1. Are there any observations of how the behavior of the graph transformer changes if trained with GraphSAM? For example, do the attention scores then align better with the graph connectivity, or do they become smoother (higher entropy)?\n1. Is it possible to compare GraphSAM with other efficient SAM derivates besides SAM-k (e.g., see related work section)?\n1. Is The proportionality in Theorem 2 not merely caused by the first-order Taylor approximation (Eq 6 in A1.1)? How do the authors know that the linear term is a sufficient approximation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission658/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission658/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission658/Reviewer_QvPb"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission658/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834819434,
            "cdate": 1698834819434,
            "tmdate": 1699635993071,
            "mdate": 1699635993071,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "amoQepA3oV",
                "forum": "Od39h4XQ3Y",
                "replyto": "psYtMk8JtI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission658/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission658/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 1/2"
                    },
                    "comment": {
                        "value": "We appreciate a lot for your careful review, and would like to provide responses to your mentioned questions and weaknesses one by one.\n\n###**[W1 & Q1 - Is GraphSAM only a good approximation for graph transformers? How is GraphSAM working, e.g., in the image domain? \u2014 Yes, GraphSAM only works good at molecular graph domain.]**\n\n\nIt is experimentally found that GraphSAM is a good approximation for graph transformers rather than in CV domain. Although our proposed method is designed based on the empirical observations, actually, Observations 1 and 2 are presented only in the graph transformers, while they do not appear in those of CV domain [1]. In CV, the model's perturbation gradient variations are often consistent with updating gradients, which is not in line with the phenomenons on the molecular graphs.\n\nTo further address the concern and provide holistic discussion, we add the following experiments to compare the performance of lookSAM [1], AE-SAM and GraphSAM in the CV and molecule datasets, respectively. LookSAM and GraphSAM are efficient SAMs designed based on the gradient varification patterns of models in their respective domains, and AE-SAM is an adaptively-updating efficient method accoding to the comparison evaluation between gradient norm and a pre-defined threshold.\n\n|ResNet-18 (CV) | CIFAR-10 | CIFAR-100|\n|   :----: |   :---: |    :---: |  \n| +SGD| 95.41 | 78.21|\n| +SAM| **96.53** | 80.18|\n| +LookSAM | 96.28 | 79.91|\n| +AE-SAM | 96.49 | **80.31**|\n| +GraphSAM | 95.86 | 78.69 |\n\n\n|CoMPT (Graph) | Tox21 | Sider| ClinTox|\n|   :----: |   :---: |    :---: |   :---: |\n| +Adam| 82.81 | 62.17| 91.41|\n| +SAM| 83.96 | 64.33| 92.73|\n| +LookSAM | 82.55| 62.54| 91.64|\n| +AE-SAM | 83.33| 63.16| 91.92|\n| +GraphSAM | **84.11** | **64.58** |**93.78** |\n\n\nIt is observed LookSAM has better results on the image datasets instead on the molecular graphs investigated in this work. On the contrary, our GraphSAM works on the molecules but leads to the worst performances on the image benchmarks. That is because model gradient variations are diverse accross the different domains. It is challenging to transfer the efficient SAMs designed based on the specific gradient patterns. AE-SAM achieves an acceptable performance on the molecular graphs since the perturbation gradient norms of graph transformers are monitored to inform the necessarity of gradient re-computation. But it is not as good as GraphSAM where we accurately fit the perturbation gradients at each step. In summary, GraphSAM is optimized particularly for the graph transformers based on the empirical observations of gradient variations.\n\n\n###**[W2 - The conclustion of Theorem 1 is not clear. \u2014 Will add to the following new explanations.]**\n\nThank you for your constructive feedback regarding the theoretical aspects of our paper. We hope the following revisement can mitigate your concerns.\nThe revised sentence is: \"Recalling the min-max optimization problem of SAM in Eq. (1), if we replace the inner maximum objective from $L_{G}(\u03b8 + \u03b5^{S})$ to $L_{G}(\u03b8 + \u03b5^{G})$, the graph transformer is motivated to smooth a worse neighborhood loss in the loss landscape. In other words, the proposed GraphSAM aims to minimize a rougher neighborhood loss, whose value is theoretically larger than that of SAM, and obtain a smoother landscape associated with the desired generalization.\"\n\n###**[W3 - Proof of Theorem 1 relies on empirical observations and strong assumptions. \u2014 Agreed, we have now changed it.]**\n\nThanks for your suggestions, and we have uniformly changed 'Theorem' to 'Conjecture'. Conjecture is a statement or conclusion based on insufficient evidence, rather than through rigorous proof. We rewrite Theorem 1 to include the experimental assumption that supports our conclusion.\n\nThe new  Conjecture 1 is:\nLet $\u03f5\u0302_{S}$ and $\u03f5\u0302_{G}$ \ndenote the perturbation weights of SAM and GraphSAM, respectively, \nwhere we ignore the subscript of t for the simple representation. \nSuppose that  \u03c9/|\u03c9| >>  \u03f5 ,as  empirically  discussed in Observation 1, and \n$|\u03f5\u0302_{S}|$ < $|\u03f5\u0302_{G}|$ \nfor \u03c1> 0, \ndesignating $\u03f5\u0302_{S}$ as the ground-truth. We have:\n\nL_{G}(\u03b8+$\u03f5\u0302_{S}$)\n \u2264 \n L_{G}(\u03b8+$\u03f5\u0302_{G}$)\n\n**References:**\n\n[1] Yong Liu, Siqi Mai, Xiangning Chen, Cho-Jui Hsieh, Yang You. Towards efficient and scalable sharpness-aware minimization. CVPR 2022."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission658/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700539917396,
                "cdate": 1700539917396,
                "tmdate": 1700543084685,
                "mdate": 1700543084685,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DYPeP9dikG",
                "forum": "Od39h4XQ3Y",
                "replyto": "psYtMk8JtI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission658/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission658/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response 2/2"
                    },
                    "comment": {
                        "value": "###**[M1 - Missing space \u2014 Fixed, and thank you for the close read! ]**\n\n###[**M2 - Counter-intuitive bold highlighting \u2014 Sure, we have changed it]**\n\nIn the revised manuscript, we maintain the bold highlighting for the best optimizer of graph transformer per task, but also add 'underline' as an additional marker to further emphasize the top-performing model in each category.\n\n\n###[**Q2 - How the behavior of the graph transformer changes if trained with GraphSAM? \u2014 When the model trained with GraphSAM, nodes' attention score matrix average entropy of same molecular graph is higher.]**\n\nThis is a wonderful suggestion. Regarding the attention matrix over BBBP dataset, we observe that models optimized with Adam, GraphSAM, and SAM obtain average entropies of 2.4130, 2.7239, and 2.8382, respectively. That means the application of SAM approaches can smooth the attention scores to avoid the overfitting. We have added the attention heat map analysis in appendix. In addition, Figure 5 at main paper shows that incorporating GraphSAM leads to smoother training and testing loss curves and reduces the gap between them. This smoother convergence suggests that GraphSAM helps learning a more robust and stable representation, which indicates a more evenly distributed attention across the graph's features.\n\n\n\n###[**Q3 -  Compare GraphSAM with other efficient SAM derivates? \u2014 Yes, we have compared with them, and GraphSAM is both efficient and high performing.]**\n\nWe have compared with other efficient SAMs (e.g., LookSAM, AE-SAM, and RST) in Table 2 of the main paper and Table 7 of Appendix A.5. We observe GraphSAM is both efficient and high-performaning on molecular graphs, while the other efficient SAMs deliver poor generalization results. The detials can be checked in paper.\n\n###[**Q4 -  Is The proportionality in Theorem 2 not merely caused by the first-order Taylor approximation? How do the authors know that the linear term is a sufficient approximation? \u2014 Yes, we leverage the first-order Taylor approximation, but it is accurate enouhgh.]**\n\nThe proportionality in Theorem 2 is indeed analyzed mainly according to the first-order Taylor approximation. We are aware that although the higher-order terms could offer a more nuanced view, the first-order term often dominates in practical scenarios, making it a sufficient approximation for the purpose of our analysis. Take the second-order Taylor expansion as an example:\n\n$L_{G}$(\u03b8+\u03f5\u0302) \u2248$L_G(\u03b8)$ + \u03f5\u0302$\\nabla_\\theta$ $L_G(\u03b8)$ +$ \\frac{1}{2} \u03f5\u0302^{2}H(\u03b8)$,\n\nwhere $\\mathbf{H}(\\theta)$ is a Hessian matrix (i.e., a matrix of second-order derivatives) of the function with respect to $\\theta$. $\\hat{\\epsilon}$ is a mapped perturbation gradient obtained by Eq. (2) and $||\\hat{\\epsilon}||_{2}$ is pretty small, which is often at the scale $10^{-3}$. Therefore the second-order expansion term  as well as the laters can be omitted. Taylor's first-order expansion can be sufficient approximation."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission658/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700539955426,
                "cdate": 1700539955426,
                "tmdate": 1700542754255,
                "mdate": 1700542754255,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BMKHApYWgd",
                "forum": "Od39h4XQ3Y",
                "replyto": "DYPeP9dikG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission658/Reviewer_QvPb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission658/Reviewer_QvPb"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up"
                    },
                    "comment": {
                        "value": "I thank the authors for the careful and elaborate response.\n\nMy points have been addressed. The authors, though, could elaborate more in the main part why GraphSAM is only a good approximation for molecular tasks."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission658/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731211538,
                "cdate": 1700731211538,
                "tmdate": 1700731211538,
                "mdate": 1700731211538,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CWQ9MG5nCf",
                "forum": "Od39h4XQ3Y",
                "replyto": "psYtMk8JtI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission658/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission658/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer QvPb,\n\nWe sincerely appreciate your thoughtful feedback. In response to your suggestion, we elaborate in Observation 1 of the main text on why GraphSAM is only a good approximation specifically for molecular tasks. Meanwhile, we compare GraphSAM with other efficient SAMs designed for the CV domain and analyze their limitations in the molecular graph domain in detail as shown in Appendix A.4.\n\nBest regards,\n\nThe Authors of \u2019Efficient Sharpness-Aware Minimization for Molecular Graph Transformer Models\u2018"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission658/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740016515,
                "cdate": 1700740016515,
                "tmdate": 1700740029653,
                "mdate": 1700740029653,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]