[
    {
        "title": "LDINet: Latent Decomposition and Interpolation for Single Image FMO Deblatting"
    },
    {
        "review": {
            "id": "2KztatVdwJ",
            "forum": "fagTLzHFBs",
            "replyto": "fagTLzHFBs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5383/Reviewer_JWwD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5383/Reviewer_JWwD"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes LDiNet, a novel network for FMO deblatting. The authors propose Decomposition-Interpolation Block (DIB) to break down feature maps into discrete time indexed parts and interpolate them accordingly to obtain the target frame. The authors identify the non-triviality of affine transformation in the latent space. To address this, they decompose each part of the latent representation into a scalar field and a gradient field and train an AffNet to estimate the affine transformation in the feature space. The authors also introduce several objectives to optimize the proposed network. The experimental results demonstrate the superior performance of LDiNet compared to existing methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The idea of interpolation by affine transformation in latent space to diversify the representations of different time indices is interesting.\n2. The authors propose a novel technique to handle the non-linearity of affine transformation in the latent space. The observation that the convolution results can be decomposed into scalar fields and gradient fields is non-trivial.\n3. The authors provide detailed training settings and comprehensive experimental results, including analysis of the effect of the parameters and some visualization results."
                },
                "weaknesses": {
                    "value": "1. The contributions are somehow incremental. The encoder-decoder framework and most of the training losses mentioned in this article have already been proposed in DeFMO. \n2. Although interpolating in hidden space seems to have some intuitive benefits, and the proposed network achieve empirically improvements, the authors do not seem to provide ablation study to verify the improvement of DIB module. The authors should conduct further experiments to explore the impact of the DIB module and add some understanding experiments if possible to illustrate how DIB \"exploits the intrinsic structure of the latent space\" and thus benefiting the deblatting. \n3. I have some doubts about the performance of the proposed method. Though there are only three datasets evaluated in this paper, there are no improvements of PSNR and SSIM on TbD. The improvement of PSNR on TbD-3D also seems marginal. It might be more convincing if the authors ran multiple random seeds and provided the mean and standard deviation or provide results on more datasets."
                },
                "questions": {
                    "value": "1. I'm puzzled as to why the weights in the formula at the bottom of page 4 (which has no formula label) are set this way and why they serve the purpose of \"fully leverage the information from both the two neighboring parts.\"\n2. I would like to know the training cost of the proposed method compared to DeFMO if possible.\n3. Could you explain the motivation for choosing the direction with a smaller relative error rate when determining the trajectory direction?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5383/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5383/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5383/Reviewer_JWwD"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5383/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698293634823,
            "cdate": 1698293634823,
            "tmdate": 1699636544554,
            "mdate": 1699636544554,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jT4B80O757",
                "forum": "fagTLzHFBs",
                "replyto": "2KztatVdwJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5383/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5383/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses for reviewer JWwD"
                    },
                    "comment": {
                        "value": "Thank you for the valuable comments. The following are our responses to the comments.\n\n**Q1: why the weights in the formula at the bottom of page 4 (which has no formula label) are set this way and why they serve the purpose of \"fully leverage the information from both the two neighboring parts.**\n\nA1: The weights in this formula are set in such way to guarantee that $Q_{\\tau}$ is identity to $P_{\\tau}$ when $\\tau = t$ or $\\tau=t\u2019$. In the case that $\\tau \\in (t, t\u2019)$, a larger weight is given to the transformed result from the closer latent part and a smaller one to the other transformed result. Since the target latent frame $Q_{\\tau}$ is a combination of two transformed results from two neighboring latent parts, it would fully leverage the information from both sides.\n\n**Q2: The training cost of the proposed method compared to DeFMO.**\n\nA2: For the training dataset used in Section 3, it costs about 1.5 days and 2.5 days for the training of our method (60 epochs) and DeFMO (50 epochs) respectively on 8 Nvidia A5000 GPUs.\n\n**Q3: The motivation for choosing the direction with a smaller relative error rate when determining the trajectory direction.**\n\nA3: For the direction selection, we actually follow DeFMO to leverage such a strategy for determining the appropriate trajectory direction. The difference is that our method only calculates the relative error rate on the masks. Since in the generated results, only the masked part of the appearance is what we need, it can be considered that the reconstruction of the mask needs to be focused on.\n\n**Q4: The contributions are somehow incremental. The encoder-decoder framework and most of the training losses mentioned in this article have already been proposed in DeFMO.**\n\nA4: Although our method is built with a similar encoder-decoder framework and loss functions, we want to emphasize that the novelty of our method mainly lies in the decomposition-interpolation paradigm, which allows naturally introducing the required time indexes via interpolation operation. And the well-designed AffNet helps perform feature alignment between different frames during the interpolation. Further, the splitting scheme of the scalar-like and gradient-like feature maps could further improve the alignment effect as the processing way of scalar and gradient fields under the affine coordinate transformations is different. This helps generate a more accurate latent frame at the target time index for decoding. These designs can well distinguish our method from DeFMO. We will add such explanations in the revision.\n\n**Q5: More ablation studies and experiments for the DIB module.**\n\nA5: We have provided an ablation study to compare the performance between settings of linear interpolation and affine interpolation in the DIB module. Please refer to the common response part.\nIn addition, we also provide the visualization of their output images in the supplemental material for better understanding.\n\n**Q6: Some doubts about the randomness on the performance of the proposed method.**\n\nA6: Please refer to the common responses part."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5383/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700307298917,
                "cdate": 1700307298917,
                "tmdate": 1700307298917,
                "mdate": 1700307298917,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WtiYfORT00",
            "forum": "fagTLzHFBs",
            "replyto": "fagTLzHFBs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5383/Reviewer_Gmw4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5383/Reviewer_Gmw4"
            ],
            "content": {
                "summary": {
                    "value": "This paper solves a temporal super-resolution task to deblur images of fast-moving rigid objects in static scenes. The method is heavily built on top of the DeFMO method, with several improvements that lead to better qualitative and quantitative scores. Most importantly, the authors encode the input and produce several latent feature maps, which correspond to different time stamps of the underlying motion. Then, these feature maps are interpolated using a new Decomposition-Interpolation Block. Finally, the object is then decoded and rendered at the required time index. The method is trained on a synthetic dataset, but it shows good generalization capabilities since it is evaluated on three real-world datasets. The design choices are evaluated in an extensive ablation study."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed method builds on top of DeFMO and combines in an elegant way this data-driven approach with an idea of piece-wise linear appearance changes from TbD-3D. \n- The whole decomposition-interpolation block seems to be well-designed and implemented. The idea of splitting the latent space into time-indexed pieces and then into scalar and gradient fields seems interesting, even though it requires more motivation and analysis.\n- The method is extensively evaluated on three real-world datasets. Moreover, many ablation studies are performed, which highlight most of the design choices.\n- The paper is mostly well-written and well-structured."
                },
                "weaknesses": {
                    "value": "### **AffNet, scalar and gradient fields**\nI do not fully understand why there is a need to split the latent space into scalar and gradient fields. I acknowledge that Table 4 shows that treating everything as scalar (the straightforward case) leads to slightly lower scores. However, I'd like to see an ablation where the feature maps are simply interpolated by linear interpolation. Wouldn't automatic backpropagation (e.g. in PyTorch) solve this? In this case, there is no need for AffNet. P_t's are simply interpolated and become Q_t, without any overhead from AffNet. This is a very important ablation.\n\nOn page 4, it says that P''(x) is equal to the gradient of the scalar field S_t, but the scalar field was already defined as P'(x). Is P'(x) = S(x)?\n\n### **Changes w.r.t. DeFMO**\nMany parts and loss functions are reused from the DeFMO method. However, some have been modified. For instance, the reconstruction loss compares generated images (I_t) concatenated with the background instead of comparing appearances (F_t), as in DeFMO. Is this change important? Does it bring improvement?\n\n### **Equation (6)**\nThe sum that denotes the number of pixels within the FMO blur should rather be written as $\\sum_D \\sum_{\\tau} (M_{\\tau} > 0)$. Otherwise, I don't understand how it can normalize the losses. Moreover, why is this normalization different for the first (without $>0$) and the second term (with $>0$)?\n\n### **Feature consistency loss**\nFeature consistency loss (9) penalizes the differences between adjacent latent feature maps. It means that it's minimized when all latent maps are the same, which shouldn't be the case if the goal is to capture a moving object. This is similar to the time-consistency loss in DeFMO, which at least contains normalized cross-correlation that allows for some movement. In contrast, the feature consistency loss prefers only identical feature maps.\n\n### **Table 2 (arch)**\nArch. (bi-branched) ablation is not clear. The paper says that it provides improvement by separating the estimation of the appearance and the mask. Does it mean that otherwise, they are not separated? What does it even mean? This is very confusing. \n\nIn general, I'd like to see more ablations on the architecture side, e.g. does it make sense to introduce AffNet at all? How is it dependent on pre-training?\n\n### **Experimental results**\nIt's not clear how many times was each ablation/experiment run before reporting the results. Is it run only once? In general, it's always good to run many times and provide mean/std values of each score. For now, it's not clear how much the improvement is contributed by randomness. For example, in Table 3, when the number of parts is set to 20, the scores go down compared to 16. Is it expected?\n\n### **Typos**\nThere are many typos in the paper:\n- Abstract: with backgrounds -> with background\n- Abstract: the feature maps of the long blur is -> are\n- Abstract: experiments are conducted and has shown -> have shown\n- Intro (p1): which is accordance -> is in accordance\n- Conclusion: feature maps is -> feature maps are\n\n\n### **Other comments**\n- \"Conventional methods mostly recover a clear image at the median of the motion\": this is not true, I'd even say they recover a clear image at an arbitrary location of the motion.\n- This method is not compared against SfB, which is fine since SfB is used more like a post-processing on top of DeFMO, meaning that the proposed method can actually be used to get better results from SfB. However, it would be really nice to see SfB results if this new method is used for silhouette estimation. I believe SfB would perform even better."
                },
                "questions": {
                    "value": "- Are AffNet and separating gradient fields really necessary? \n- It would be helpful to add SfB results if sub-frame silhouettes are used from the proposed method as input.\n- Please see the Weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5383/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5383/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5383/Reviewer_Gmw4"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5383/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698709052285,
            "cdate": 1698709052285,
            "tmdate": 1699636544444,
            "mdate": 1699636544444,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "80jCpz8QUY",
                "forum": "fagTLzHFBs",
                "replyto": "WtiYfORT00",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5383/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5383/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses for reviewer Gmw4"
                    },
                    "comment": {
                        "value": "Thank you for the valuable comments. The following are our responses to the comments.\n\n**Q1: Are AffNet and separating gradient fields really necessary?**\n\nA1: Please refer to the common responses part.\n\n**Q2: It would be helpful to add SfB results if sub-frame silhouettes are used from the proposed method as input.**\n\nA2: We have provided the results for SfB with silhouettes generated by LDINet as follows:\n| Method |Falling TIoU| Falling PSNR|Falling SSIM| TbD-3D TIoU | TbD-3D PSNR | TbD-3D SSIM | TbD TIoU |TbD PSNR| TbD SSIM |\n| :-: | :-: | :-: | :-: | :-: |:-: |:-: |:-: |:-: |:-: |\n|  SfB (ours)  | 0.658(0.012) | 25.348(0.421) | 0.730(0.004) | 0.843(0.003) | 24.172(0.096) | 0.651(0.001) | 0.601(0.000) | 24.130(0.062) | 0.609(0.001) |\n|SfB (DeFMO)| 0.650(0.001) | 24.982(0.096) | 0.732(0.003) | 0.864(0.000) | 23.982(0.027) | 0.640(0.002) | 0.561(0.001) | 24.164(0.030) | 0.592(0.002) |\n\nSince time is limited, we only report the 8-step results with two prototypes and set the learning rate to 0.03 for stable optimization in SfB. And the std is provided in parentheses for 3 times experiments. We will add this comparison into the revision.\n\n**Q3: An ablation where the feature maps are simply interpolated by linear interpolation.**\n\nA3: Please refer to the common response part.\n\n**Q4: On page 4, it says that $P\u2019\u2019(x)$ is equal to the gradient of the scalar field $S(x)$, but the scalar field was already defined as $P\u2019(x)$. Is $P\u2019(x) = S(x)$?**\n\nA4: $P\u2019(x)$ is not $S(x)$. Due to the separating of scalar-like and gradient-like feature maps, $P\u2019\u2019(x)$ and $P\u2019(x)$ are generated from different channels of the same latent part. Thus, though $S(x)$ is the scalar field to produce $P\u2019\u2019(x)$, there actually exists differences between $P\u2019(x)$ and $S(x)$.\n\n**Q5: Loss changes w.r.t. DeFMO.**\n\nA5: Please refer to the common responses part.\n\n**Q6: For the Equation 6, (1) about the sum term that denotes the number of pixels within the FMO blur, and (2) why is this normalization different for the first (without $>0$) and the second term (with $>0$)?**\n\nA6: Since $I$ denotes the blur image which is the average of a series of sharp images at the given time indexes within the exposure time, the formula reviewer provided could count the same pixel multiple times because of its appearance in masks of different time indexes. In our formula, we use a pixel-wise indicator function $[\\cdot >0]$ to set each pixel that appears in any mask to 1 which avoids the multiple-counting.\n\nOn the other hand, $>0$ is part of the pixel-wise indicator function $[\\cdot >0]$. The first normalization term is without the function because the mask $M_\\tau$ is a binary map whose value is either 0 or 1 for different pixels and thus it could be directly summed. While the second normalization needs the function to convert the summation over time indexes $\\sum_{\\tau} M_{\\tau}$ to a binary map to further count the size of the blurred area.\n\n**Q7: The Arch. (bi-branched) ablation in Table 2 and the dependence of pretraining.**\n\nA7: In the decoder, we separate the estimation of the appearance and the mask with two different branches as in Figure 1. In particular, this separation happens after the first two Resblocks of the decoder. Otherwise, they are estimated with one shared branch setup. We will add more explanations to the revision.\nIn addition, for the dependence of pretraining, we provide the following ablation study to show its advantage:\n| Method |Falling TIoU| Falling PSNR|Falling SSIM| TbD-3D TIoU | TbD-3D PSNR | TbD-3D SSIM | TbD TIoU |TbD PSNR| TbD SSIM |\n| :-: | :-: | :-: | :-: | :-: |:-: |:-: |:-: |:-: |:-: |\n|DeFMO | 0.684| 26.83 |0.753 |0.879 |26.23| 0.699 | 0.550 | 25.57 | 0.602|\n|w/ pretrain| 0.686(0.007) | 28.087(0.006) | 0.771 (0.001) | 0.906(0.002) | 26.503(0.136) | 0.707(0.003) | 0.616(0.004) | 25.242(0.122) | 0.626(0.008) |\n| w/o pretrain| 0.697 |27.706 | 0.766 | 0.906 | 26.490 | 0.706 | 0.615 | 25.702 | 0.626 |\n\nSince time is limited, we only report the one-run results for w/o pretraining here and we will further complement the results.\n\n**Q8: About the randomness of the experimental results.**\n\nA8: Please refer to the common responses part.\n\n**Q9: About the trend change in Table 3.**\n\nA9: For \u201cIn Table 3, it is seen that the score goes down if the number of parts is set to 20.\u201d, this is expected as the final results would rely more on the quality of the direct output of the encoder when the number of parts increases and the affine transformation would not produce effects as the space for interpolation is gradually reduced.\n\n**Q10: About the typos and some expressions.**\n\nA10: Thanks a lot, and we will correct the typos and revise our paper more carefully. For the description of \"Conventional methods mostly recover a clear image at the median of the motion\", such expression is only to illustrate the common problem setup in the existing literatures. We will revise accordingly to avoid confusion."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5383/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700307164350,
                "cdate": 1700307164350,
                "tmdate": 1700307164350,
                "mdate": 1700307164350,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hJKBors0xw",
            "forum": "fagTLzHFBs",
            "replyto": "fagTLzHFBs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5383/Reviewer_3qEi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5383/Reviewer_3qEi"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles the problem of FMO deblurring. The goal of this paper is to separate the object from the background and recover the appearance of the foreground object for each time stamp during the exposure time. In particular, the proposed method decomposes the latent space into a few latent segments. A pair of latents could be related by the predicted affine transformations from a pair of time stamps. And the latent at any time stamp could be interpolated from the disentangled latents and it could be decoded to reconstruct the mask and image at time t. The method is evaluated on FMO deblatting benchmark which shows promising results, outperforming existing methods on two out of three existing datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The idea is promising in recovering the appearance of the FMO and has potential to reconstruct the sharp image of FMO at any time during the exposure time.\n+ The decomposition of the latent into a set of latent segments which are responsible different latent frames. The interpolation network could lead to the reconstruction of the latent at any time within the exposure time."
                },
                "weaknesses": {
                    "value": "1)\tConvolution is defined as a linear combination of the signals within a spatial window in an image. However, it is not clear to the reviewer why we need to further decompose the convolution as a linear combination of summation and series of directional derivatives. What is the benefit of this further decomposition? It seems that the Taylor expansion of the latent is applied at time t to approximate the latent at any time t? It was not clearly motivated in the paper.\n2)\tAffine transformation. The paper mainly describes what has been implemented to approximate the affine transformation via prediction of two affine transformations (one is computed from I_t to I_t\u2019 and the other one is computed from I_t\u2019 to I_t). Would this be redundant as one transformation should be the inverse of the other?\n3)\tEq.10 is not correct. It should be the Frobenius Norm for computing the relative distance between two matrices not the standard MSE. \n4)\tThere is no guarantee that the predicted affine transformation is invertible. While losses are introduced to guide the learning process, no hard constraint is enforced in the framework.\n5)\tAll the equations should be written properly. D is referred to as the domain for the pixel coordinate. However, no index ever appears in the loss function for Eq. (5,6,7,8).\n6)\tNo supplementary is provided in the submission. Could the authors show the recovery of the appearance of the FMO in a video as the proposed method can reconstruct the image at any time within the exposure time? The qualitative results in Figure 6 in the appendix cannot demonstrate the effectiveness of the proposed method. The masks recovered by the proposed method and DeFMO look the same."
                },
                "questions": {
                    "value": "-Please addressed the concerns mentioned above in the weakness section. \nOverall, the idea is promising. The only concern from the reviewers is about the motivation of representing convolution as the scalar and directional derivative field. The equations could be improved. The quantitative results are not impressive. It would be great to show more qualitative results."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5383/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698739073686,
            "cdate": 1698739073686,
            "tmdate": 1699636544339,
            "mdate": 1699636544339,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aEQyZHJiQa",
                "forum": "fagTLzHFBs",
                "replyto": "hJKBors0xw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5383/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5383/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses for reviewer 3qEi"
                    },
                    "comment": {
                        "value": "Thank you for the valuable comments. The following are our responses to the comments.\n\n**Q1: Why we need to further decompose the convolution as a linear combination of summation and series of directional derivatives.**\n\nA1: The main reason is to help improve the alignment effect by the affine transformation. In particular, by decomposing the convolution as a linear combination of summation and series of directional derivatives, the resulting feature maps are correspondingly split into scalar-like and gradient-like feature maps. Further, the scalar-like and gradient-like feature maps have different processing ways under the affine coordinate transformation. Thus, different ways of transformation operations are enforced on these two kinds of feature maps to fully exploit their particular characteristics, and thus better alignment effect could be achieved, which helps improve the embedding quality for any time index and obtain better overall performance.\n\n**Q2: About the affine transformation. The paper mainly describes what has been implemented to approximate the affine transformation via prediction of two affine transformations. Would this be redundant as one transformation should be the inverse of the other?**\n\nA2: We predict the two affine transformations mainly for the consideration of the numerical stability of the model. Empirically, using the inverse operation directly has the risk of numerical instability, especially in the first several epochs in the training. Such a design of modeling the transformation from both directions also provides some flexibility when either one happens to be not precise.\n\n**Q3: The Equation 10 is not correct.**\n\nA3: Thanks for pointing this out. We will correct it by using the Frobenius Norm for computing the relative distance between two matrices.\n\n**Q4: There is no guarantee that the predicted affine transformation is invertible.**\n\nA4: For the invertibility, based on the experimental statistics, the id loss in Equation 10 is less than 1e-4 during the training process, which empirically shows that the predicted affine transformation approximates to be invertible. We will add such analysis in the revision.\n\n**Q5: All the equations should be written properly.**\n\nA5: Thanks for pointing them out. We will revise the Equations 5, 6, 7, and 8 accordingly.\n\n**Q6: About the qualitative results.**\n\nA6: In the supplemental material, we have provided the compared results for the recovery of the appearance of the FMO in the form of videos."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5383/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700306844588,
                "cdate": 1700306844588,
                "tmdate": 1700306844588,
                "mdate": 1700306844588,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bIr81sLYPp",
            "forum": "fagTLzHFBs",
            "replyto": "fagTLzHFBs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5383/Reviewer_wbgD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5383/Reviewer_wbgD"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a network to interpolate deblurred images with respective object alpha matte for single image with fast moving object. In the proposed network, the features are affine transformed to generate the features in arbitrary time steps. Also, 'scalar fields' and 'gradient fields' are considered in the network. The provided experimental results show the proposed method performs better than other methods. This paper is poorly written and should not be accepted in its current form."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The provided experimental results show the proposed method performs better than other methods."
                },
                "weaknesses": {
                    "value": "1. The novelty is quite limited. \n2. The authors fail to provide evidence of why the proposed method works. Also, the presentation of this paper is too poor to be fully understood. For example:\n(1) More details should be added to discuss the difference between the proposed network and the prior work DeFMO and describe why the proposed method is better.\n(2) What are scalar-like and gradient-like feature maps? How does the network get them? Why are they so important? It also lacks an ablation study to validate it.\n(3) The detailed process from eq. 3 to eq. 5 is not clear which makes it hard to understand. Also, how this process is presented in Fig. 2 is also not clear."
                },
                "questions": {
                    "value": "See weaknesses for details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5383/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5383/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5383/Reviewer_wbgD"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5383/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698843862055,
            "cdate": 1698843862055,
            "tmdate": 1699636544224,
            "mdate": 1699636544224,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Zg9wIREfeH",
                "forum": "fagTLzHFBs",
                "replyto": "bIr81sLYPp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5383/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5383/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses for reviewer wbgD"
                    },
                    "comment": {
                        "value": "Thank you for the valuable comments. The following are our responses to the comments.\n\n**Q1: The novelty is limited.**\n\nA1: Please refer to the common responses part.\n\n**Q2: Fail to provide evidence of why the proposed method works.**\n\nA2: Thanks for your suggestion. We would reorganize the paper and provide more required explanations for the important components of the proposed method as is pointed out. To further validate the effectiveness of our work, we would add more ablation/experimental studies. For example, (1) We would compare with the results by linear interpolation to show the benefits of the affine transformation in our interpolation method, (2) We would provide an ablation setting to show the importance of the pre-training, and (3) We would provide more qualitative results to show the benefits of the decomposition-interpolation paradigm.\n\n**Q3: The presentation of this paper is too poor to be fully understood. (1) More details about the difference between the proposed network and the prior work DeFMO. (2) More details about the implementations for scalar-like and gradient-like feature maps. (3) More details for the process from eq. 3 to eq. 5.**\n\nA3: We will revise the paper accordingly.\n\n(1)\tThe main difference between our method and DeFMO lies in the way of constructing latent embedding for different time indexes. DeFMO concatenates the target time index with shared latent feature maps as the corresponding embedding for the subsequent decoding. Instead, our method first decomposes the feature maps in latent space into several latent parts and then generates the required corresponding embedding for any time index via an interpolation method with affine transformation. This helps greatly improve the flexibility of generating diverse outputs for different time indexes. Further, in the decoder part, DeFMO generates the mask and appearance of the object with only one branch, while our method generates them with two separate branches as they have different value distributions. More details about the difference will be added in the revision.\n\n(2) For the scalar-like and gradient-like feature maps, such design mainly considers the characteristics of convolution operation under the affine coordinate transformations to improve the interpolation quality. In particular, on the one hand, the convolution operation could be seen as a linear combination of summation and series of directional derivatives, where the directional derivatives are linear projections of the gradient fields. On the other hand, the processing way of scalar and gradient fields under the affine coordinate transformations is different. Thus, it is very beneficial to represent the feature maps output by the convolution operation as a combination of scalar-like and gradient-like feature maps. Such a well-designed splitting scheme helps improve the alignment effect by the affine transformation.\nIn practice, our model learns the scalar-like and gradient-like feature maps with a pre-training stage. In the pre-training, we first obtain the latent parts for the original input image. Then we split the channels of latent parts into the scalar-like and gradient-like categories where each scalar-like feature map has a single channel and each gradient-like feature map has two channels as the two components of the gradient field, which are processed by an estimated affine transformation by AffNet. Further, the resulting feature maps are forced to be consistent with the corresponding ones for a distorted input version which is produced by applying a pre-set affine transformation to the FMO blur contained in the original input image. Meanwhile, in this process, the estimated affine transformation is also encouraged to be similar to the pre-set one. The related ablation study is provided in Table 4 to validate the importance of generating two kinds of feature maps instead of only one kind. More details will be added in the revision accordingly.\n\n(3) In particular, Equation 3 and Equation 4 formulate the supposed transformations for the scalar-like and gradient-like feature maps, respectively. In Equation at the bottom of page 4, a function $\\Phi$ is first defined to split the input feature map into the scalar-like and gradient-like feature maps, which are then processed with the transformations in Equation 3 and 4 correspondingly. The two respective results are concatenated as the output. Therefore, the Equation 3 and 4 provide the important component for the operation of the Equation at the bottom of page 4. For Figure 2, since it is defined that the latent feature map $P_t$ is the concatenation of the scalar part $P_t\u2019$ and the gradient part $P_t\u2019\u2019$, only the process of the Equation at the bottom of page 4 is explicitly presented in Figure 2(a), where the function $\\Phi$ takes as input ($P_t$, $A_{\\tau\\rightarrow t}$) and ($P_{t\u2019}$, $A_{\\tau\\rightarrow t\u2019}$) for generating $Q_{\\tau}$. More explanations will be added in the revision."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5383/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700306326194,
                "cdate": 1700306326194,
                "tmdate": 1700306557108,
                "mdate": 1700306557108,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]