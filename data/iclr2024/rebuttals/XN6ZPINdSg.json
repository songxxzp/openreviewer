[
    {
        "title": "COLEP: Certifiably Robust Learning-Reasoning Conformal Prediction via Probabilistic Circuits"
    },
    {
        "review": {
            "id": "oJh6LLL4zX",
            "forum": "XN6ZPINdSg",
            "replyto": "XN6ZPINdSg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7604/Reviewer_nQJc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7604/Reviewer_nQJc"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose conformal prediction sets that are robust to adversarial perturbations -- they maintain the nominal coverage for any perturbed input in the threat model. The main idea is to leverage a learning-reasoning component which can improve the worst-case bounds on the predicted conditional class probabilities. Constructing the prediction sets using the worst-case scores derived from the worst-case (corrected) class probabilities yields adversarially robust coverage."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "In terms of originality, the approach can be seen as the clever combination of two main ideas:\n- improved worst-case bounds on the class probabilities using a reasoning component\n- using worst-case bounds on the conformity scores to get robust sets \n\nBoth ideas have been explored in the literature, but their combination leads to improved results. Specifically, more efficient (smaller) sets compared to RSCP while having the same guarantee, and larger worst-case lower bound.\n\nThe technical and theorethical components of the paper are sufficiently detailed and rigorous. Theorem 3 can be seen as a generalization of Theorem 2 by Gendler et al. The analysis in Section 6, while making stronger assumptions, is interesting and shows when we can expect improvement.\n\nThe exprimental results are expected since the derived bounds are tighter.\n\nThe contribution is significant, as is the improvement over the previous SOTA."
                },
                "weaknesses": {
                    "value": "While the authors do consider the finite-sample error induced by the finite size of the calibration set, they fail to account for the finite-sample error due to the Monte-Carlo sampling (they use 10000 samples) when estimating the expectations under the randomized smoothing framework. Therefore, the resulting sets are only asymptotically valid.\n\nAs pointed out by a concurent ICLR submission (https://openreview.net/forum?id=BWAhEjXjeG) RSCP also suffers from the same issue and it is non-trivial to correct for this. The same issue is also discussed among the reviewers and the RSCP authors on their respective openreview page (https://openreview.net/forum?id=9L1BsI4wP1H). Naively, one would have to apply a union bound over all examples in the calibration set and the test example such that each of the randomized smoothing expectations hold simultaneously. There is also a subtler alternative solution. When correcting RSCP for this error the resulting sets end up returning all labels, i.e. they are useless. It is not clear to which degree COLEP suffers from the same issue. In any case, the \"fix\" proposed by the concurent submission is orthogonal and can be applied to COLEP as well. Still, this finite-sample issue needs to be addressed in the paper, especially given that the goal is to produce a sound certificate. \n\nThe second weakness is in the PGD attack. The authors state \"For fair comparisons, we apply PGD attack (Madry et al., 2018) with the same parameters on CP, RSCP, and COLEP\". However, it is unclear whether the PGD attack is adaptive, i.e. it takes the learning-reasoning component into account. An adaptive attacker can in fact know that a reasoning component is used, and thus can try to perturb the input such that both the class and the concept probabilities are suitably changed as to fool the entire pipeline. While this is not critical, not using adaptive attacks makes the conclusions drawn from Figure 3 less reliable."
                },
                "questions": {
                    "value": "1. How does COLEP perform when accounting for finite-sample errors when estimating randomized smoothing expectations?\n2. In section 3 you have \"assume that the data samples are drawn i.i.d. (thereby exchangeably)\". Is the i.i.d. assumptions necessary for the learning-reasonig component, or can this be relaxed to just exchangeability as with vanilla CP?\n3. Is the PGD attack adaptive, i.e. takes the reasoning component into account?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7604/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7604/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7604/Reviewer_nQJc"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7604/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698751487616,
            "cdate": 1698751487616,
            "tmdate": 1700685035557,
            "mdate": 1700685035557,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IPM6nM5pFH",
                "forum": "XN6ZPINdSg",
                "replyto": "oJh6LLL4zX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer nQJc [1/2]"
                    },
                    "comment": {
                        "value": "> Q1: Consideration of finite-sample error induced by Monte-Carlo sampling in randomized smoothing. How does COLEP perform when accounting for finite-sample errors when estimating randomized smoothing expectations?\n\nWe thank you for bringing up this interesting point! We added the following discussions as well as theoretical and empirical results considering the finite-sample errors of randomized smoothing in the revised manuscript.\n\nRSCP+ [1] points out the neglect of finite-sample errors of Monte-Carlo sampling by randomized smoothing in RSCP [2]. A naive solution is to apply union bound over all calibration samples, but it leads to a loose prediction set as shown in RSCP+.  Therefore, RSCP+ proposes a subtle solution by **viewing the empirical mean $\\hat{S}\\_{\\text{RS}}$ as the non-conformity scores** and thus only need to bound the finite-sample error of the empirical mean $\\hat{S}\\_{\\text{RS}}$ on the test sample $(x\\_{n+1},y\\_{n+1})$ (Corollary 2 in [1]). Specifically, they apply Hoeffding\u2019s inequality and Empirical Bernstein\u2019s inequality to bound the finite-sample error of empirical mean $\\hat{S}\\_{\\text{RS}}$ compared to the true mean $S\\_{\\text{RS}}$ of the test sample $(x\\_{n+1},y\\_{n+1})$ (Theorem C.6 in [1]). In this way, RSCP+ does not need to apply union over calibration samples and one concentration bound over the test sample is sufficient.\n\nSince COLEP applies randomized smoothing to the model output in the learning component instead of the end-to-end non-conformity scores, we need to deal with the finite-sample error in a slightly different way, but the general spirit aligns with RSCP+.\nCOLEP applies randomized smoothing to get the bound of output probabilities of the main model and knowledge models, and then propagates the bound to the probability bound after the reasoning component (Theorem 1) and the bound of non-conformity scores (Theorem 2).\nTherefore, to consider the finite-sample errors, we need to define a **$\\beta$-confidence worst-case non-conformity score** based on Equation (12) in Theorem 2. Concretely, in Equation (12), we replace the original probability upper bound $\\mathbb{U}\\_{\\delta}[\\cdot]$ with a $\\beta$-confidence version $\\mathbb{U}\\_{\\beta,\\delta}[\\cdot]$, denoting the upper bound of non-conformity scores with confidence $1-\\beta$, and similarly for the lower bound. In this way, during the proof of Theorem 2, we need to apply the union bound in the derivation from Equation (53) to Equation (54) by considering the event probability $\\mathbb{P}[\\hat{\\pi}\\_j^{\\text{COLEP}}(\\tilde{X}\\_{n+1}) \\le \\mathbb{U}\\_{\\beta,\\delta}[\\hat{\\pi}\\_j^{\\text{COLEP}}(X\\_{n+1})] ] \\ge 1 - 2\\beta$. Similarly, we consider the event related to the lower bound in the derivation from Equation (57) to Equation (58). Finally, to counter the effect of finite-sample error and maintain a $1-\\alpha$ coverage, we need to compute the $1-\\alpha+2\\beta$ quantile in Equation (11) in Theorem 2. We provided the rigorous theorem statement and proofs in **Theorem 6 in Appendix F.3**. We separated the theoretical result with finite-sample errors since Theorem 2 also allows for deterministic certification methods such as bound propagation methods which do not lead to finite-sample errors. We added related discussions and references in the remarks of Theorem 2.\n\nWe also empirically considered the finite-sample errors induced by randomized smoothing expectation approximation and provided the updated results in Figure 2 and Figure 3 in the revised manuscript. We consider the finite-sample errors of RSCP following RSCP+ (Corollary 2 in [1]) and the finite-sample errors of COLEP following Theorem 6 in Appendix F.3. We performed 100k Monte-Carlo sampling for a smaller error in practice and provided all the details in the experiment section of the revised manuscript. We generally observe an error of $[0.002,0.005]$ induced by finite samples of randomized smoothing. One can further reduce the finite-sample errors by increasing the Monte-Carlo sample sizes or applying better concentration bounds. From the updated results in Figure 2 and Figure 3, we can still conclude that COLEP with the reasoning component demonstrates better robustness and prediction efficiency than SOTA baseline RSCP.\n\n*[1] Provably Robust Conformal Prediction with Improved Efficiency, ICLR 2024 submission (https://openreview.net/forum?id=BWAhEjXjeG)*\n\n*[2] Gendler, Asaf, et al. \"Adversarially robust conformal prediction.\" ICLR 2021.*"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700297911260,
                "cdate": 1700297911260,
                "tmdate": 1700297911260,
                "mdate": 1700297911260,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NpTOTuKt8M",
                "forum": "XN6ZPINdSg",
                "replyto": "oJh6LLL4zX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer nQJc [2/2]"
                    },
                    "comment": {
                        "value": "> Q2: Clarification of adaptive PGD attack against the learning-reasoning pipeline.\n\nWe thank you for the valuable comment. We added a clear clarification that we consider an adaptive PGD attack against the complete learning-reasoning pipeline in Section 7. Formally, the attack process can be formulated as the optimization problem: $\\max\\_{\\|\\eta\\|\\_2 \\le \\delta} \\ell\\_\\text{CE}(\\hat{\\pi}^{\\text{COLEP}}(x),y)$, where $\\ell\\_\\text{CE}$ is the cross-entropy loss and $\\hat{\\pi}^{\\text{COLEP}}(x)$ is the class probabilities given sample $x$ computed by Equation (6) which takes the reasoning component into consideration.\n\n> Q3: Are the i.i.d. assumptions necessary for the learning-reasoning component, or can this be relaxed to just exchangeability as with vanilla CP?\n\nWe thank you for the valuable questions. In Section 3 of the revised manuscript, we made it clear that we **only need the exchangeability assumptions** as vanilla CP. Specifically, we only assume the clean test sample is drawn **exchangeably** to the calibration samples and the adversarial sample is within bounded perturbations from the clean test sample."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700298006621,
                "cdate": 1700298006621,
                "tmdate": 1700298006621,
                "mdate": 1700298006621,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wlkxiY1GTW",
                "forum": "XN6ZPINdSg",
                "replyto": "oJh6LLL4zX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7604/Reviewer_nQJc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7604/Reviewer_nQJc"
                ],
                "content": {
                    "title": {
                        "value": "Reply"
                    },
                    "comment": {
                        "value": "Thank you for the comprehensive reply!\n\nI have one follow question. If I understood correctly Figure 3 in the updated version considers the finite-sample errors for COLEP similar to RSCP+. However, for the second bar do you show RSCP (without error correction) or do you show RSCP+ (with error correction)? I'm asking since [1] reported very high set size for RSCP+. \n\nIt might be also interesting to show COLEP and COLEP+ (or however the corrected version is called) side by side."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700608429559,
                "cdate": 1700608429559,
                "tmdate": 1700608468427,
                "mdate": 1700608468427,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gUzO4o4r72",
                "forum": "XN6ZPINdSg",
                "replyto": "j75ocZFkE0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7604/Reviewer_nQJc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7604/Reviewer_nQJc"
                ],
                "content": {
                    "title": {
                        "value": "Follow up"
                    },
                    "comment": {
                        "value": "Thank you, that clears things up. I'm assuming the results in Table 5 are for certified coverage. What value of $\\delta$?"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641697829,
                "cdate": 1700641697829,
                "tmdate": 1700641697829,
                "mdate": 1700641697829,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UYNrUpTI2Y",
                "forum": "XN6ZPINdSg",
                "replyto": "oJh6LLL4zX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up Discussions with Reviewer nQJc"
                    },
                    "comment": {
                        "value": "Thank you for the comment! We evaluate the marginal coverage and average set size under PGD attack with perturbation bound $\\delta=0.25$ (same setting as Figure 3). We added the specification in the caption of Table 5 in the updated version."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660963175,
                "cdate": 1700660963175,
                "tmdate": 1700661174503,
                "mdate": 1700661174503,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1dKrjgqYer",
                "forum": "XN6ZPINdSg",
                "replyto": "UYNrUpTI2Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7604/Reviewer_nQJc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7604/Reviewer_nQJc"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up"
                    },
                    "comment": {
                        "value": "Thank you for the clarification. How does the average set size look like for the *certifed* sets with $\\delta=0.25$?"
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665377381,
                "cdate": 1700665377381,
                "tmdate": 1700665377381,
                "mdate": 1700665377381,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Feai0y0SJK",
                "forum": "XN6ZPINdSg",
                "replyto": "JOOIY5cuTu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7604/Reviewer_nQJc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7604/Reviewer_nQJc"
                ],
                "content": {
                    "title": {
                        "value": "Follow up"
                    },
                    "comment": {
                        "value": "Does that mean for the result in table 5 you start with a clean x, then find a perturbed x' with PGD, and then compute the worst case sets for a ball of size $\\delta$ around x' either with theorem 2 or theorem 6?"
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669974162,
                "cdate": 1700669974162,
                "tmdate": 1700669974162,
                "mdate": 1700669974162,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uohEQ5fJYu",
                "forum": "XN6ZPINdSg",
                "replyto": "BBXFkbrHtS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7604/Reviewer_nQJc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7604/Reviewer_nQJc"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up"
                    },
                    "comment": {
                        "value": "Thank you very much for the clarification. It makes sense.\n\nOne final question. If I understand correctly:\n- Your certificate plugs in the standard score for for test example and the quantile is computed on the worst-case scores for each calibration point\n- While the certificate of [1] and implicitly of RSCP plug in the worst-case score for the test example and the standard score for each callibration point \n\nThis seems to be an interesting difference in where the worst-case bound is being used. Can you in principle also plug in your (tighter) worst-case bound for the test point and use standard scores for the calibration point, resulting in a different certificate?\n\n[1] Provably Robust Conformal Prediction with Improved Efficiency, ICLR 2024 submission (https://openreview.net/forum?id=BWAhEjXjeG)"
                    }
                },
                "number": 30,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700676174607,
                "cdate": 1700676174607,
                "tmdate": 1700676174607,
                "mdate": 1700676174607,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lQjKSD5KR6",
                "forum": "XN6ZPINdSg",
                "replyto": "oJh6LLL4zX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7604/Reviewer_nQJc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7604/Reviewer_nQJc"
                ],
                "content": {
                    "title": {
                        "value": "Thank you"
                    },
                    "comment": {
                        "value": "Thank you for the engaging discussion and the detailed reply.\n\nI agree with your comment. One minor point is that since $\\frac{\\epsilon}{\\delta}$ is fixed it can be interepreted either as the first line or the second line of certification.\n\nWhile the first line might be indeed more reasonable. I think that the second line also make sense. The reason is that if the perturbation ball is symmetric, then a ball around the perturbed $x'$ must contain the clean $x$ (of course the ball around the clean $x$ is different, but the two balls overlap). \n\nNow when certifying classification, if everything inside the ball has the same prediction then the prediction for $x'$ coincides with the prediction for $x$. We can conclude this without knowing the clean $x$. The same applies for the conformal set if we use the worst-case score, since the worst-case score in a ball around x' is by definition larger than the score of the clean x."
                    }
                },
                "number": 32,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700680695243,
                "cdate": 1700680695243,
                "tmdate": 1700680768630,
                "mdate": 1700680768630,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rEamaUWaPp",
                "forum": "XN6ZPINdSg",
                "replyto": "oJh6LLL4zX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up Discussions with Reviewer nQJc"
                    },
                    "comment": {
                        "value": "Thank you for the quite valuable response!\n\nProviding general certification for the second line based on the symmetry between $x$ and $x'$ is indeed an interesting point! Specifically, given perturbed $x'$ during test time, we can compute the bounds of output logits in the $\\delta$-ball, which will also cover the clean $x$. Since the logit bounds also hold for clean $x$, we can compute the worst-case scores based on the bounds and finally construct the prediction set with the clean quantile value. \n\nWe agree that the two lines of certification are sound and essentially parallel. It is fundamentally due to the exchangeability of clean samples, which leads to consideration of worst-case bounds either during calibration or inference. One practical advantage of the first line of certification might be that the computation of worst-case bounds requires more runtime, and thus, computing it during the offline calibration process might be more desirable than during the online inference process. In terms of the tightness of the certification, we currently do not have a straightforward intuition of which one is dominantly better. Since we mainly focus on demonstrating the effectiveness of logical reasoning in this paper, we would like to leave the interesting exploration to future work.\n\nWe added discussions of alternative certification methods in the revised version in Appendix A. Thank you again for the valuable comments and points!"
                    }
                },
                "number": 33,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684876742,
                "cdate": 1700684876742,
                "tmdate": 1700684987986,
                "mdate": 1700684987986,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OznvZN8Uli",
                "forum": "XN6ZPINdSg",
                "replyto": "rEamaUWaPp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7604/Reviewer_nQJc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7604/Reviewer_nQJc"
                ],
                "content": {
                    "title": {
                        "value": "Increased score"
                    },
                    "comment": {
                        "value": "Thank you for the interesting discussion. I've increased my score to 8."
                    }
                },
                "number": 34,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685073308,
                "cdate": 1700685073308,
                "tmdate": 1700685073308,
                "mdate": 1700685073308,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0yEpC0S4cn",
            "forum": "XN6ZPINdSg",
            "replyto": "XN6ZPINdSg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7604/Reviewer_qYpF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7604/Reviewer_qYpF"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces COLEP a certifiably robust learning-reasoning conformal prediction framework. Authors leverage probabilistic circuits for efficient reasoning and provide robustness certification. They also provide marginal coverage guarantees using conformal prediction methods. Finally, by performing several experiments, they highlight the relevance of their proposal."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "First, I would like to make it clear that this paper is not in my area of expertise. That is why I had some difficulty understanding it. However,\n\n1) the contributions of the paper seem to be significant.\n\n2) experience suggests that COLEP is competitive with previous work."
                },
                "weaknesses": {
                    "value": "The paper is very dense and difficult to read.\n\nCitations are not appropriate. For example, in the related work on conformal prediction, the first sentence mentions articles from 2021, and the seminal articles, notably by Vovk and others, are not cited.\n\nThere is no \"limitation\" in the conclusion. This section should be added.\n\nMinor:\n\n\"with the guarantee of marginal prediction coverage:...\" the inclusion should be a $\\in$."
                },
                "questions": {
                    "value": "Why is COLEP better suited to conformal prediction than already existing methods?\n\nWhat does \"certified conformal prediction\" mean? Is this different from saying that we have a marginal coverage guarantee?\n\nThe paper claims that it achieved \"a certified coverage of COLEP\" but, for example in Figure 2, the coverage obtained with COLEP is well below 0.9. Can you explain this in more detail?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7604/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698835542300,
            "cdate": 1698835542300,
            "tmdate": 1699636922568,
            "mdate": 1699636922568,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wK3lbTmBAy",
                "forum": "XN6ZPINdSg",
                "replyto": "0yEpC0S4cn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer qYpF [1/2]"
                    },
                    "comment": {
                        "value": "> Q1: Density notations and difficulty of reading.\n\nUpon your feedback, we made the following improvements to the readability of the notations and analysis.\n\n1.  We include more comprehensive and structured notation review tables in Appendix K for a better understanding of the analysis of COLEP. Specifically, we provide an overview of notations related to model definitions in Table 5, notations related to conformal prediction in Table 6, notations related to certification of Section 5 in Table 7, and notations related to analysis of Section 6 in Table 8. All the used notations in COLEP are currently provided in the notation tables in a structured manner, which is emphasized at the beginning of Section 4. In this way, readers can look up the meanings of notations easily and quickly via the tables.\n\n2. We also improved the overall certification flow in Figure 4 in Appendix E. We restructured the flow and included more illustrations to improve the readability. We also added a brief overview description of the certification flow for a better understanding. The certification setting is that the inference time adversaries can add bounded perturbations to the test sample and violate the data exchangeability assumption, compromising the guaranteed coverage. The certification generally achieves the following three goals. (1) We can preserve the guaranteed coverage using a prediction set that takes the perturbation bound into account (in Theorem 2), achieved by computing the probability bound of models before the reasoning component (by randomized smoothing) and the bound after the reasoning component (by Theorem 1). (2) We prove the worst-case coverage (a lower bound) if we use the standard prediction set as before (in Theorem 3). (3) We theoretically show that COLEP can achieve better prediction coverage (in Theorem 4) and prediction accuracy (in Theorem 5) than a data-driven model without the reasoning component. \n\n3. We follow the suggestion to separate important definitions from the paragraphs for ease of backtracking the original definitions. Concretely, we separate the definitions of $\\hat{\\pi}_j(x)$ and $\\hat{\\pi}^{(l)}(x)$ in Section 4.1 and the definition of $F(\\mu)$ in Section 4.2.\n\nWe believe that with the improvements, the readers can follow the presented analysis more easily.\n\n\n> Q2: Citation issues.\n\nWe thank you for the valuable comments. We added the citations of seminal articles in the related work section.\n\n> Q3: Limitations section.\n\nWe included the discussions on limitations in Appendix A and added references to it in Section 8. To summarize, a possible limitation of COLEP may lie in the computational costs induced by pretraining knowledge models. However, the problem can be partially relieved by training knowledge models with parallelism. Moreover, we only need to pretrain them once and then can encode different types of knowledge rules based on domain knowledge. The training cost is valuable since we theoretically and empirically show that more knowledge models benefit the robustness of COLEP a lot for conformal prediction.\nAnother possible limitation may lie in the access to knowledge rules.\nFor the datasets that release hierarchical information (e.g., ImageNet, CIFAR-100, Visual Genome), we can easily adapt the information to implication rules used in COLEP. We can also seek knowledge graphs with related label semantics for rule design. These approaches may be effective in assisting practitioners in designing knowledge rules, but the process is not fully automatic, and human efforts are still needed. Overall, for any knowledge system, one naturally needs domain experts to design the knowledge rules specific to that application. There is probably no universal strategy on how to aggregate knowledge for any arbitrary application, and rather application-specific constructions are needed. This is where the power as well as limitation of our framework comes from.\n\n> Q4: Notation fix in the preliminaries.\n\nWe thank you for pointing it out. We fixed the notation in Section 3 in the revised manuscript."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700297354735,
                "cdate": 1700297354735,
                "tmdate": 1700297354735,
                "mdate": 1700297354735,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "flcGeP56An",
                "forum": "XN6ZPINdSg",
                "replyto": "0yEpC0S4cn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer qYpF [2/2]"
                    },
                    "comment": {
                        "value": "> Q5: Why is COLEP better suited to conformal prediction than already existing methods?\n\nTo the best of our knowledge, RSCP [1] is the most relevant existing method proposed for the problem of adversarially robust conformal prediction.\nRSCP provides the robustness certification of conformal prediction for a single model where they treat the mapping from input to the non-conformity score as a black-box function and apply randomized smoothing to achieve the certification. COLEP differs from RSCP [1] in all these aspects. Namely,\n\nAt the model level: RSCP considers a standard single model, while COLEP follows a two-stage learning-reasoning pipeline where the learning component employs several models, and then integrates a separate reasoning component with PCs to it. We study this complex pipeline and theoretically prove that its robustness is better than a robust single model (RSCP) in terms of prediction coverage and prediction accuracy.\n\nAt the certification level: the direct application of randomized smoothing as in RSCP does not yield a tight end-to-end robustness certification for COLEP. As such, we\n\n a) use robustness certification approaches to derive bounds of the learning component,\n\n b) consider the structural properties of PCs and provide efficient robustness certification of the PC component and\n\n c) translate the bound of output probabilities of COLEP to the bound of non-conformity scores to achieve the end-to-end certification.\n\n*[1] Gendler, Asaf, et al. \"Adversarially robust conformal prediction.\" International Conference on Learning Representations. 2021.*\n\n\n> Q6: What does \"certified conformal prediction\" mean? Is this different from saying that we have a marginal coverage guarantee?\n\nCertified conformal prediction presents the worst-case coverage guarantee under bounded input perturbations. Marginal coverage guarantee presents the lower bound of prediction coverage with the assumption of exchangeability, indicating that the test samples are clean and drawn from the same distribution as the calibration samples. In contrast, certified conformal prediction presents the lower bound of prediction coverage **with bounded input perturbations on the clean test sample**. The additional input perturbations can violate the exchangeability assumption and break the original marginal coverage guarantee. Based on the problem, COLEP presents a certification of the worst-case coverage guarantees considering the adversarial perturbations during the inference time and improves the robustness with the power of logical reasoning. We made it more clear in the revised manuscript.\n\n> Q7: Explanations of why the certified coverage of COLEP is below $0.9$ in Figure 2.\n\nWe evaluate the certified coverage (i.e., the lower bound of prediction coverage under bounded perturbations) in Figure 2. Since $0.9$ is the nominal coverage level without perturbations during the test time, the certified coverage considering perturbations should be lower than the nominal coverage level $0.9$ and can be rigorously computed by our certification, as shown in Figure 2. In Figure 2, the comparison of the certified coverage to the baseline (RSCP) demonstrates the robustness of COLEP with logical reasoning and the tightness of our certification in Theorem 3. \n\nFurthermore, we can also take the test-time perturbations into consideration during the calibration process to ensure the certified coverage remains at the specified level $0.9$. We evaluate the results in this scenario in Figure 3. The results show that the coverage of COLEP is always above the nominal coverage $0.9$ and achieves better efficiency than RSCP."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700297534743,
                "cdate": 1700297534743,
                "tmdate": 1700297534743,
                "mdate": 1700297534743,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OaPHd822kL",
                "forum": "XN6ZPINdSg",
                "replyto": "flcGeP56An",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7604/Reviewer_qYpF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7604/Reviewer_qYpF"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for their detailed response and maintain my score in favor of accepting the article."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642477803,
                "cdate": 1700642477803,
                "tmdate": 1700642477803,
                "mdate": 1700642477803,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qYuuPIGYo4",
            "forum": "XN6ZPINdSg",
            "replyto": "XN6ZPINdSg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7604/Reviewer_GJ8Z"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7604/Reviewer_GJ8Z"
            ],
            "content": {
                "summary": {
                    "value": "The paper offers a new construction for a conformal prediction pipeline that offers guaranteed coverage both in the exchangeable and in the adversarial perturbations case, and generally should offer improved adaptivity and set size whenever domain-specific learning-reasoning rules are provided. The main new component is the integration of probabilistic circuits into the construction of a nonconformity score of the APS (Romano et al) type, where an underlying model's class probabilities output is postprocessed according to a collection of reasoning rules, which help correct each class's predicted probability before plugging it into the APS score. The benefits of the reasoning components are investigated theoretically, by means of a theorem stating that under some assumptions marginal coverage will be strictly better with than without these components, as well as experimentally over three datasets and compared to two benchmark conformal method (one with adversarially robust guarantees)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper essentially constructs a novel type of score for conformal prediction; rather than the simple softmax layer-based constructions of some recent nonconformity scores like APS and RAPS, the idea here is to take such a softmax output from e.g. a neural net and, using domain specific learning-reasoning rules, postprocess each individual class's predicted probability to become appropriately corrected, and only then use them in an APS score-like fashion.\n\nThese corrections imply both the robustness properties (at least to l2 perturbations), as well as offer (subject to mild assumptions) theoretically guaranteed improvement over just using the underlying model's softmax outputs.  It is also more generally easy (as the authors show) to propagate estimation uncertainty through the learning-reasoning pipeline. This type of provable guarantees is, to my knowledge, novel and has not come up in the conformal literature before.\n\nMoreover, the proposed technique is more flexible than standard conformal prediction, in that it can provide guarantees not just over all classes, but also for each class separately (at its own coverage threshold).\n\nThese above technical innovations, as well as the practical potential of learning-reasoning components to improve domain-specific performance of conformal prediction, lead me to conclude that the proposed paper would be a novel and interesting addition to the conformal literature."
                },
                "weaknesses": {
                    "value": "1. Empirical evaluation is quite limited, which is quite unfortunate given that including more settings beyond datasets as simple as CIFAR-10, as well as more benchmarks, would likely let the guarantees afforded by the learning-reasoning setup shine even more: both in terms of the ability to construct rules specific to each considered setting, as well as the resulting adaptivity compared to other conformal scores/methods.\n\nRelevantly, I was quite confused when the authors repeatedly call the APS score of Romano et al (2020) 'SOTA'. It is certainly one of the existing reasonable conformal approaches to use, but by no means state of the art when it comes to adaptivity and especially set size. For instance, its generalization and improvement, the RAPS nonconformity score introduced in \"Uncertainty Sets for Image Classifiers using Conformal Prediction\", may still be considered close to the SOTA frontier for many domains --- and it would be very pertinent to include it, along with further improved conformal methods developed thereafter. Also, CIFAR 10 not quite sounding like the natural domain to apply potentially highly complex learning-reasoning boosts, it'd be wise to show the performance of the framework on Imagenet; as a byproduct, it would help me convince myself of the tractability of useful learning-reasoning in more challenging settings. In any case, having just three relatively simple datasets and only two comparison methods, Romano et al and Gendler et al, appears insufficient to fully explore the relative advantages of learning-reasoning.\n\n2. The main other weakness to me is the overbearing nature of the notation used, and the overall presentation. It took me many hours to internalize the type of the guarantees that learning-reasoning components lead to, and even the notation itself. Moreover, especially the multi-subscript-superscript notation involved in both theorem statements and proofs related to the learning-reasoning component was very hard to parse and even read. The notational review in the last appendix was a nice touch, but only marginally helpful as it only listed some of the relevant notation; Diagram in Figure 4, describing the overall certification flow, also felt confusing and didn't really help, making me resort to understanding text only. Instead of structured definitions of the main notations and concepts (using appropriate latex environments), everything was crammed into the paragraphs, thus diluting the structure of the presentation and making it hard to backtrack to the context in which a piece of notation was originally defined."
                },
                "questions": {
                    "value": "1. The experimental section needs strengthening; please see above for suggestions/guidelines on directions.\n2. In discussing experimental results, it is of especial interest to discuss which learning-reasoning components were used in each domain. Other details are quite standard and common in the conformal literature, but this one is novel and thus should be propagated to the main part rather than left in the appendix. The current running example is the stop sign example, but it would be very useful to have a walk-through of how the rules were implemented in actual experiments.\n3. In the same vein, right now the experimental part only features plots of standard metrics (coverage, set size, ..) Meanwhile, I would like to see a visualization of what the learning-reasoning components do to the predictions of the underlying model --- e.g. which rules turned out to correct the initial estimates by the largest amount, etc.\n4. Notation needs to be significantly improved and clarified to achieve readability."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7604/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7604/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7604/Reviewer_GJ8Z"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7604/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698837324028,
            "cdate": 1698837324028,
            "tmdate": 1699636922393,
            "mdate": 1699636922393,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "79v9MLFhwg",
                "forum": "XN6ZPINdSg",
                "replyto": "qYuuPIGYo4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer GJ8Z [1/3]"
                    },
                    "comment": {
                        "value": "> Q1: Limited empirical evaluations.\n\nWe thank you for your valuable comments and suggestions on the evaluation part. We made the following updates to clarify and strengthen the evaluation of COLEP.\n\n1. We avoided the confusion induced by improper namings in Section 7. APS score is indeed not the SOTA method, and thus, we called it a baseline of conformal prediction and only called RSCP [1] as the SOTA approach in adversarially robust conformal prediction in the revised version.\n\n2. We followed the suggestion to compare with more baselines of various conformal prediction scores to show the robustness of COLEP. Specifically, we further evaluate the RAPS score [2], which improves the APS score by adding a regularization term regarding the ranking of the ground truth label. We also evaluate the recent clustered conformal prediction (CCP) [3], which computes the quantile and performs calibration at the cluster level. We compare COLEP with these baselines on AwA2 under PGD attack. The results in the following Table 1 show that (1) conformal prediction with vanilla conformity scores APS, RAPS, and CCP leads to a broken coverage in the adversary setting, while adversarially robust conformal prediction method RSCP and COLEP still maintains the valid coverage, and (2) compared to RSCP, COLEP demonstrates a better tradeoff between the coverage and prediction efficiency.\n\n3. We added the following clarifications and more evaluations to demonstrate the scalability of COLEP. Besides the relatively small-scale but standard benchmark CIFAR-10 and GTSRB, we also conducted evaluations on AwA2 [4] in different settings in Figure 2 and Figure 3 in Section 7. AwA2 dataset contains 37,322 images of 50 different animal classes, which are extracted from the ImageNet dataset and with a high resolution of $256 \\times 256$. The dataset also has 85 attribute labels (concept labels) for each animal class and is suitable for evaluations of knowledge-intensive tasks. In total, we construct 187 implication rules in total with 4 types of knowledge encoded in 4 PCs separately. Evaluations on AwA2 are already the largest scale in the related literature on learning-reasoning [5] and neuro-symbolic [6]. The scalability of COLEP comes from the usage of probabilistic circuits with linear inference time with respect to the graph size instead of the intractable Markov logic network with NP-complete time complexity used by prior works [5,6]. To consolidate the scalability to AwA2, we further added more evaluations with multiple perturbation bounds in the following Table 1. The results together with results in Figure 2 and Figure 3 on AwA2 demonstrate the scalability and robustness of COLEP to achieve an impressive prediction coverage and efficiency in the adversary setting. \n\nWe also admit that it is challenging for evaluations on the full ImageNet with 1000 classes due to the expensive costs of training thousands of concept models and the lack of knowledge rules at the current stage, which is also discussed as one limitation in Appendix A. However, we deem that the principles and analysis in COLEP shed light on the effectiveness of the reasoning component to achieve a more robust conformal prediction. It is also interesting for future work to leverage the CLIP model for concept detection and the large language models for the design of knowledge rules, which can fully unleash the power of COLEP on more large-scale evaluations.\n\nTable 1. Marginal coverage / average set size of multiple conformal prediction baselines, SOTA method RSCP, and COLEP under PGD Attack and AutoAttack with $\\ell_2$ bounded perturbations $0.25$ and $0.50$ on AwA2. The nominal level of coverage is $0.9$.\n\n|  | APS | RAPS | CCP | RSCP | COLEP |\n| --- | ----------- | ----------- | ----------- | ----------- | ----------- | \n| $\\delta=0.25$ | 0.8325 / 3.22 | 0.8363 / 3.20 | 0.8427 / 3.53 | 0.945 / 6.41 | 0.962 / 4.74 |\n| $\\delta=0.50$ | 0.7829 / 2.83 | 0.7821 / 2.89 | 0.7894 / 2.93 | 0.932 / 5.83 | 0.942 / 4.42 |\n\n*[1] Gendler, Asaf, et al. \"Adversarially robust conformal prediction.\" ICLR 2021.*\n\n*[2] Angelopoulos, Anastasios, et al. \"Uncertainty sets for image classifiers using conformal prediction.\" ICLR 2021.*\n\n*[3] Ding, Tiffany, et al. \"Class-Conditional Conformal Prediction With Many Classes.\" NeurIPS 2023.*\n\n*[4] Xian, Yongqin, et al. \"Zero-shot learning\u2014a comprehensive evaluation of the good, the bad and the ugly.\" TPAMI 2018.*\n\n*[5] Yang, Zhuolin. Improving certified robustness via statistical learning with logical reasoning. NeurIPS 2022.*\n\n*[6] Wu, Tailin, et al. \"Zeroc: A neuro-symbolic model for zero-shot concept recognition and acquisition at inference time.\" NeurIPS 2022.*"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700296587257,
                "cdate": 1700296587257,
                "tmdate": 1700296587257,
                "mdate": 1700296587257,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "haNHHB5N8Y",
                "forum": "XN6ZPINdSg",
                "replyto": "qYuuPIGYo4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer GJ8Z [2/3]"
                    },
                    "comment": {
                        "value": "> Q2: The notations are complex for readers.\n\nWe thank you for your valuable comments and suggestions. We make the following updates to enhance the reading of the paper.\n\n1.  We include more comprehensive and structured notation review tables in Appendix K for a better understanding of the analysis of COLEP. Specifically, we provide an overview of notations related to model definitions in Table 5, notations related to conformal prediction in Table 6, notations related to certification of Section 5 in Table 7, and notations related to analysis of Section 6 in Table 8. All the used notations in COLEP are currently provided in the notation tables in a structured manner, which is emphasized at the beginning of Section 4. In this way, readers can look up the meanings of notations easily and quickly via the tables.\n\n2. We also improved the overall certification flow in Figure 4 in Appendix E. We restructured the flow and included more illustrations to improve the readability. We also added a brief overview description of the certification flow for a better understanding. The certification setting is that the inference time adversaries can add bounded perturbations to the test sample and violate the data exchangeability assumption, compromising the guaranteed coverage. The certification generally achieves the following three goals. (1) We can preserve the guaranteed coverage using a prediction set that takes the perturbation bound into account (in Theorem 2), achieved by computing the probability bound of models before the reasoning component (by randomized smoothing) and the bound after the reasoning component (by Theorem 1). (2) We prove the worst-case coverage (a lower bound) if we use the standard prediction set as before (in Theorem 3). (3) We theoretically show that COLEP can achieve better prediction coverage (in Theorem 4) and prediction accuracy (in Theorem 5) than a data-driven model without the reasoning component. \n\n3. We follow the suggestion to separate important definitions from the paragraphs for ease of backtracking the original definitions. Concretely, we separate the definitions of $\\hat{\\pi}_j(x)$ and $\\hat{\\pi}^{(l)}(x)$ in Section 4.1 and the definition of $F(\\mu)$ in Section 4.2.\n\nWe believe that these enhancements will make it simpler for readers to understand the provided analysis."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700296719581,
                "cdate": 1700296719581,
                "tmdate": 1700296719581,
                "mdate": 1700296719581,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VrqvwSNtn7",
                "forum": "XN6ZPINdSg",
                "replyto": "qYuuPIGYo4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer GJ8Z [3/3]"
                    },
                    "comment": {
                        "value": "> Q3: More details of reasoning component construction in the experiment part in the main text.\n\nWe thank you for the valuable suggestion, In summary, we have 3 PCs and 28 knowledge rules in GTSRB, 3 PCs and 30 knowledge rules in CIFAR-10, and 4 PCs and 187 knowledge rules in AwA2. Based on the domain knowledge in each dataset, we encode a set of implication rules into PCs. In each PC, we encode a type of implication knowledge with disjoint attributes. In GTSRB, we have a PC of shape knowledge (e.g., \u201coctagon'', \u201csquare''), a PC of boundary color knowledge (e.g., \u201cred boundary'', \u201cblack boundary''), and a PC of content knowledge (e.g., \u201cdigit 50'', \u201cTurn Left''). In CIFAR-10, we have a PC of category knowledge (e.g., \u201canimal'', \u201ctransportation''), a PC of space knowledge (e.g., \u201cin the sky'', \u201cin the water'', \u201con the ground''), and a PC of feature knowledge (e.g., \u201chas legs'', \u201chas wheels''). In AwA2, we have a PC of superclass knowledge (e.g., \u201cprocyonid'', \u201cbig cat''), a PC of predation knowledge (e.g., \u201cfish'', \u201cmeat'', \u201cplant''), a PC of appearance (e.g., \u201cbig ears'', \u201csmall eyes''), and a PC of fur color knowledge (e.g., \u201cgreen'', \u201cgray'', \u201cwhite'').\n\nFor example, in the PC of shape knowledge in GTSRB, we can encode the implication rules related to the shape knowledge such as $\\text{IsStopSign} \\implies \\text{IsOctagon}$ (stop signs are octagon) with weight $w_1$, $\\text{IsSpeedLimit} \\implies \\text{IsSquare}$ (speed limit signs are square) with weight $w_2$, and $\\text{IsTurnLeft} \\implies \\text{IsRound}$ (turn left signs are round) with weight $w_3$. To implement the COLEP learning-reasoning framework, we follow the steps to encode the rules into PCs. \n\n- (1) We learn the main model and knowledge models implemented with DNNs to estimate a preliminary probability for $N_c$ class labels and $L$ concept labels (e.g., $p(\\\\text{IsSpeedLimit}=1)=\\\\hat{\\\\pi}\\_{sp}$, $p\\(\\text{IsSquare}=1)=\\\\hat{\\\\pi}\\_{sq}$). \n\n- (2) We compute the factor value $F(\\mu)$ for any possible assignment $\\mu \\in \\\\{0,1\\\\}^{N_c+L}$ as $F(\\mu) = \\exp \\{\\sum\\_{h=1}^H w\\_h \\mathbb{I}\\_{[\\mu \\sim K_h]}\\}$ (e.g.,$ F([1,1,1,1,1,1])=e^{w_1+w_2+w_3}$ since the assignment satisfy all three knowledge rules above). \n\n- (3) We construct a three-layer PC as shown in Figure 1: (a) the bottom layer consists of leaf nodes representing Bernoulli distributions of all class and concept labels parameterized by the preliminary class probabilities, and the constant factor values of assignments (e.g., $B(\\hat{\\pi}\\_{sp}),B(1-\\hat{\\pi}\\_{sp}),B(\\hat{\\pi}\\_{sq}),B(1-\\hat{\\pi}\\_{sq}),...,F([1,1,1,1,1,0]),F([1,1,1,1,1,1]),...$), (b) the second layer consists of product nodes computing the likelihood of each assignment by multiplying the likelihood of the instantiations of Bernoulli variables and the correction factor values (e.g., $\\hat{\\pi}\\_{sp} \\times \\hat{\\pi}\\_{sq} \\times \u2026 \\times F([1,1,1,1,1,1])$), and (c) the top layer is a sum node computing the summation of the products. \n\n- (4) We linearly combine multiple PCs that encode different types of knowledge rules (e.g., 3 PCs in GTSRB: PCs of shape/color/content knowledge).\n\nWe included the statistics of knowledge rules and details of PC implementation in Section 7 and Appendix J.1 in the revised manuscript.\n\n> Q4: Visualization of the functionality of the reasoning component.\n\nWe thank you for the suggestion of adding visualization of the contribution of different knowledge rules. Specifically in GTSRB, we consider the contribution of the shape knowledge rules, color knowledge rules, and content knowledge rules. In CIFAR-10, we consider the contribution of the category knowledge rules, space knowledge rules, and feature knowledge rules. More details of the meanings and examples of knowledge rules are provided in response to Q3 and Appendix J.1. We provided the visualization of contributions of different types of knowledge rules in Figure 5 in Appendix J.2. The results show that the rules related to the color knowledge in GTSRB and space knowledge in CIFAR-10 benefits the certified coverage more, but collectively applying all the well-designed knowledge rules lead to even better coverage. The effectiveness of these types of knowledge is attributed to a higher utility of the knowledge models (i.e., the accuracy of concept detection). Concretely, the feature of these more effective concepts is relatively easy to learn and can be more accurately detected (e.g., color concepts are more easily learned than shape concepts). The correlation between the knowledge model utility and effectiveness of COLEP is also theoretically analyzed and observed in Section 6."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700297169294,
                "cdate": 1700297169294,
                "tmdate": 1700297169294,
                "mdate": 1700297169294,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uXFFbhrjG1",
                "forum": "XN6ZPINdSg",
                "replyto": "VrqvwSNtn7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7604/Reviewer_GJ8Z"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7604/Reviewer_GJ8Z"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Authors"
                    },
                    "comment": {
                        "value": "Thank you for a very detailed answer to my questions, as well as for making extensive edits to the paper with the aim of making empirical evaluation more extensive and --- importantly --- enhancing readability. Thanks to the above explanations of the mechanics of knowledge rules in the provided examples, I now think I better understand how this new type of conformal prediction can really incorporate side/prior knowledge about a task. Thank you for the clarifications here, as well as for improving the exposition in the Appendices.\n\nOn the one hand, new experiments (both those addressing my questions on comparisons to true SOTA as well as on more challenging datasets, as well as those in response to other reviewers' requests to test the method's robustness in the presence of additional adversarial attacks) appear to show that COLEP achieves both competitive efficiency and coverage as compared to well-performing prior conformal methods, as well as good robustness in several settings. That is good. Also, I agree with the authors' point that it becomes more challenging to construct knowledge rules etc. for these larger and more unwieldy datasets, and that CLIP and the like may be of good help there --- I am perfectly happy with this being a nice direction for future research, and will not insist on including such further explorations in the paper.\n\nOn the other hand, readability of the manuscript has also improved due to extensive efforts on the authors' part (adding intuition behind main theorems/results, doing a more extensive recap of all notation, as well as longer/somewhat clearer explanations of the circuitry that is being used). That being said, the text and the formulas are still incredibly densely presented, and the manuscript in my opinion remains very challenging to read for a conformal prediction researcher. I would highly recommend to seriously contemplate ways in which the amount of notation could be significantly reduced (!). As well as, the paper still does not use proper Definition environments etc. to introduce all necessary notions/concepts. Of course, I understand the challenges of needing a lot of notation to rigorously express results that amalgamate several components (i.e. conformal, PCs, etc.) each with their own traditional notation, as well as having limited space to properly lay out all concepts in the intro sections, but I believe that making such further readability improvements will significantly expand the audience for the paper.\n\nIn sum, having considered both other reviews and the responses, I appreciate the directions in which the paper has been updated, and once again thank the authors for their significant effort to improve the manuscript. I will thus keep my positive score and evaluation of the paper --- but encourage the authors to keep improving exposition (e.g. along the above lines), which still prevents me from raising my score."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632183990,
                "cdate": 1700632183990,
                "tmdate": 1700632183990,
                "mdate": 1700632183990,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZKrrda0K4F",
                "forum": "XN6ZPINdSg",
                "replyto": "qYuuPIGYo4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up discussion with Reviewer GJ8Z"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the valuable suggestions and positive feedback!! We will definitely follow the reviewer\u2019s suggestion to further improve our paper further and add additional results and related analysis based on our discussion. Please let us know if you have other suggestions, and we hope our paper will lead to an interesting new direction to integrate data-driven learning with knowledge-enabled logical reasoning for the community."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637024749,
                "cdate": 1700637024749,
                "tmdate": 1700661356522,
                "mdate": 1700661356522,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ElAWKNaPAF",
            "forum": "XN6ZPINdSg",
            "replyto": "XN6ZPINdSg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7604/Reviewer_MM5a"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7604/Reviewer_MM5a"
            ],
            "content": {
                "summary": {
                    "value": "The authors applied conformal prediction to a conformity score incorporating logic reasoning based on prior knowledge graphs and demonstrated that the resulting prediction intervals achieved desired coverage while being narrower compared to RSCP on three real data sets under l2 norm perturbations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors adapted conformal prediction to a new scenario where a reliable knowledge graph is available and showed empirically that incorporating this side information can improve the efficiency of conformal predictions."
                },
                "weaknesses": {
                    "value": "See questions."
                },
                "questions": {
                    "value": "1. Is the F(mu) prefixed function with H rules? Is mu observed only for the training? (the knowledge graph encodes the relationship between, e.g.,  \"stop\" and shape info, but the goal is to predict if it is a stop sign).  I am confused about how pi_j(x) is calculated when mu is unknown.\n\n2. It seems that the gain of COLEP originates from utilizing the prior knowledge graph through F(u) and u: The knowledge graphs incorporated are side information with both the graph relationship and u remained true against attacks. How robust does the method perform against contaminated graphs/u?\n\n3. Is it easy to calculate max|\u03b7|2\u2264\u03b4 \u02c6 \u03c0j (x + \u03b7) and min|\u03b7|2\u2264\u03b4 \u02c6 \u03c0j (x + \u03b7) for general probability assignment function pi_j(.)? The two main Theorems are from a brute-forth search of the worst-case scenario under perturbations, which are intuitively correct but it seems more important to show the feasibility of achieving this for general functions.\n\n4. How are the weights w chosen in F(mu)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7604/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7604/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7604/Reviewer_MM5a"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7604/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698985482807,
            "cdate": 1698985482807,
            "tmdate": 1700695890828,
            "mdate": 1700695890828,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TNG0rDNC13",
                "forum": "XN6ZPINdSg",
                "replyto": "ElAWKNaPAF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer MM5a [1/2]"
                    },
                    "comment": {
                        "value": "> Q1: More clarifications about assignment $\\mu$, factor function value $F(\\mu)$, and the computation of $\\pi^{\\text{COLEP}}_j(x)$ accordingly.\n\nWe included the following clarification of definitions in Section 4.2. $F(\\mu)$ is indeed a fixed factor function once $H$ knowledge rules and weights are determined. Note that $\\mu$ is an **index variable** of the summation process in Equation (6) (instead of observation), and thus we do not need to observe the values of $\\mu$. For a particular index vector $\\mu$ in the summation process, $F(\\mu)$ computes the summation of the weights of satisfied knowledge rules, denoting how well the index vector $\\mu$ aligns with the specified knowledge rules. \n\nLet\u2019s consider the illustrative example of stop sign classification model with an auxiliary knowledge model of octagon shape classification in Section 4.2. The index variable $\\mu \\in \\\\{0,1\\\\}^2$ denotes a possible assignment of Bernoulli random variables $\\text{IsStopSign}$ and $\\text{IsOctagon}$. \nTherefore, we have that $F(\\mu=[0,0])=F(\\mu=[0,1])=F(\\mu=[1,1])=e^w$ and $F(\\mu=[1,0])=e^0$ since only the assignment $\\mu=[1,0]$ (i.e., $\\text{IsStopSign}=1, \\text{IsOctagon}=0$) violates the knowledge rule $\\text{IsStopSign} \\rightarrow \\text{IsOctagon}$ with weight $w$. Then according to Equation (6), we can compute $\\pi_j^{\\text{COLEP}}(x)$ (i.e., $p(\\text{IsStopSign}=1)$) by marginalizing the Bernoulli variable $\\text{IsOctagon}$ as follows:\n$$ \\pi_j^{\\text{COLEP}}(x) = \\dfrac{p(\\text{IsStopSign}=1)p(\\text{IsOctagon}=0)F(\\mu=[1,0]) +  p(\\text{IsStopSign}=1)p(\\text{IsOctagon}=1)F(\\mu=[1,1])}{p(\\text{IsStopSign}=1)p(\\text{IsOctagon}=0)F(\\mu=[1,0]) + p(\\text{IsStopSign}=1)p(\\text{IsOctagon}=1)F(\\mu=[1,1]) + p(\\text{IsStopSign}=0)p(\\text{IsOctagon}=0)F(\\mu=[0,0]) + p(\\text{IsStopSign}=0)p(\\text{IsOctagon}=1)F(\\mu=[0,1])} $$, where $p(\\cdot)$ is the likelihood estimate of the main model and knowledge models without the reasoning component.\n\n\n> Q2: Discussion of the robustness of COLEP with contaminated knowledge graph.\n\nWe thank you for the interesting question! It is interesting to consider cases where the knowledge graph is contaminated by adversarial attackers or contains noises and misinformation by the neglect of designers. According to the response to Q1, $\\mu$ is an index variable instead of observations, and thus, the robustness of COLEP mainly originates from the knowledge rules via factor function $F(\\cdot)$ denoting how well each assignment conforms to the specified knowledge rules. Since the knowledge rules and weights are specified by practitioners and fixed during inferences, the PCs encoding the knowledge rules together with the main model and knowledge models (learning + reasoning component in Figure 1) can be viewed as a complete model. According to the adversarial attack settings in the literature[1,2], the model weights can be well protected and not manipulated by attackers in practical cases, and thus, the PC graph (i.e., source of side information) which is a part of the complete model can not be easily manipulated by adversarial attackers.\n\nOn the other hand, it is an interesting question to discuss the case when the knowledge rules may be contaminated and contain misinformation due to the neglect of designers. We can view the problem of checking knowledge rules and correcting misinformation as a pre-processing step before the employment of COLEP. The parallel module of knowledge checking and correction is explored by a line of research [3,4,5], which basically detects and corrects false logical relations via semantic embeddings and consistency-checking techniques. It is also interesting to leverage more advanced large language models to check and correct human-designed knowledge rules. We deem that the process of knowledge-checking is independent and parallel to COLEP, which may take additional efforts before the deployment of COLEP but significantly benefits in achieving a much more robust system based on our analysis and evaluations. We added the related discussions in Appendix A. \n\n\n*[1] Goodfellow, Ian J., Jonathon Shlens, and Christian Szegedy. \"Explaining and harnessing adversarial examples.\" arXiv preprint arXiv:1412.6572 (2014).*\n\n*[2] Gendler, Asaf, et al. \"Adversarially robust conformal prediction.\" International Conference on Learning Representations. 2021.*\n\n*[3] Melo, Andr\u00e9, and Heiko Paulheim. \"An approach to correction of erroneous links in knowledge graphs.\" CEUR Workshop Proceedings. Vol. 2065. RWTH Aachen, 2017.*\n\n*[4] Caminhas, Daniel D. \"Detecting and correcting typing errors in open-domain knowledge graphs using semantic representation of entities.\" (2019).*\n\n*[5] Chen, Jiaoyan, et al. \"Correcting knowledge base assertions.\" Proceedings of the Web Conference 2020.*"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700295671458,
                "cdate": 1700295671458,
                "tmdate": 1700295671458,
                "mdate": 1700295671458,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6hHx0hMt5s",
                "forum": "XN6ZPINdSg",
                "replyto": "ElAWKNaPAF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer MM5a [2/2]"
                    },
                    "comment": {
                        "value": "> Q3: Computation of the bound of class probabilities for more general functions.\n\nAs illustrated in Section 5.1, the computation of the bound of class probabilities consists of two steps: \n- (1) bound computation of class probabilities after the learning component (i.e., $\\min\\_{\\| \\eta \\|_2 \\le \\delta} \\pi\\_j(x+\\eta)$ and $\\max\\_{\\| \\eta \\|_2 \\le \\delta} \\pi\\_j(x+\\eta)$)\n- (2) bound computation of class probabilities after the reasoning component (i.e., $\\min\\_{\\|\\eta\\|\\_2 \\le \\delta} \\pi\\_j^{\\text{COLEP}}(x+\\eta)$ and $\\max\\_{\\|\\eta\\|\\_2 \\le \\delta} \\pi\\_j^{\\text{COLEP}}(x+\\eta)$). \n\nWe can compute the bound in step (1) for any general functions $\\pi_j$ with randomized smoothing. We can compute the bound in step (2) based on the results of step (1) according to Theorem 1, which holds for general probabilistic circuits that can encode arbitrary logic rules. Therefore, COLEP allows for bound computation for general deep neural networks in the learning component and general probabilistic circuits in the reasoning component.\n\n> Q4: Selection of weight $w$ in $F(\\mu)$.\n\nWe thank you for the interesting question. The weight $w$ of logical rules is specified based on the utility of the corresponding knowledge rule. In Section 6, we theoretically show that the robustness of COLEP correlates with the utility of knowledge rules regarding the uniqueness of the rule, defined in Equation (17). Specifically, a universal knowledge rule which generally holds for almost all class labels is of low quality and does not benefit the robustness and should be assigned with a low weight $w$. For example, if all the traffic signs in the dataset are of octagon shape, then the implication rule $\\text{IsStopSign} \\implies \\text{IsOctagon}$ should be useless and assigned with a low weight $w$, which also aligns with the intuition. \n\nIn the empirical evaluations, since we already designed knowledge rules of high quality (i.e., good uniqueness) as shown in Section 7, we do not need to spend great efforts selecting weights for each knowledge rule individually. We assume that all the knowledge rules have the same weight $w$ and select a proper weight $w$ for all the knowledge rules via grid search. The results in the following Table 1 inspire us to fix $w$ as $1.5$, and the evaluations demonstrate that it is sufficient to achieve good robustness in different settings. One can also model the weight selection problem as a combinatorial optimization, which is non-trivial but can better unleash the power of COLEP. We leave a more intelligent and automatic approach of knowledge weight selection for future work. We included the discussions in Section 7 and Appendix J.\n\nTable 1. Certified coverage of COLEP on GTSRB with different weights $w$ of knowledge rules under perturbation bound $\\delta=0.25$.\n\n| w | 0.0 | 0.5 | 1.0 | 1.5 | 2.0 | 3.0 | 5.0 | 10.0 |\n| --- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- |\n| Certified Coverage | 0.816 | 0.832 | 0.837 | **0.842**  | 0.840 | 0.823 | 0.829 | 0.827 |"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700296362806,
                "cdate": 1700296362806,
                "tmdate": 1700297234431,
                "mdate": 1700297234431,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LVfcKLlrBJ",
                "forum": "XN6ZPINdSg",
                "replyto": "ElAWKNaPAF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up Discussions"
                    },
                    "comment": {
                        "value": "We thank you again for the valuable suggestions and comments! Since it is close to the end of the discussion period, we wonder whether our responses address your concerns. We are happy to take any further questions or concerns."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700607359124,
                "cdate": 1700607359124,
                "tmdate": 1700607372874,
                "mdate": 1700607372874,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9iWKACLoN0",
                "forum": "XN6ZPINdSg",
                "replyto": "TNG0rDNC13",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7604/Reviewer_MM5a"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7604/Reviewer_MM5a"
                ],
                "content": {
                    "comment": {
                        "value": "[Q1] Thank you for the explanation-- that makes sense. \n\n[Q2] Related, another possibility is that a relationship is partially true, e.g., IsStopSign->IsOctogan is true with a probability of 70%. This is not a good example, but it can happen in many settings. Discussion of such a knowledge graph that might not be universally true could be interesting and related to the choice of w.\n\nMinor comments: (1) I suggest changing the example after e.g., on page 3, to match [IsStopSign, IsOctagon] the figure and the probability vector after it."
                    }
                },
                "number": 36,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700687824669,
                "cdate": 1700687824669,
                "tmdate": 1700687824669,
                "mdate": 1700687824669,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2TxDUgK738",
                "forum": "XN6ZPINdSg",
                "replyto": "6hHx0hMt5s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7604/Reviewer_MM5a"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7604/Reviewer_MM5a"
                ],
                "content": {
                    "comment": {
                        "value": "Q3. I think my question is the computation of the min and max values because it requires looping through eta. Could the authors explain more about what you have done in their experiments to guarantee, e.g., the min_{|\\eta|_2 < \\delta}\\pi_j(x+\\eta), has been achieved?\n\nQ4. I would suggest the authors have some guidelines for how practitioners choose w themselves without double dipping their test samples. It seems important to automatically address the choice of w as a future work."
                    }
                },
                "number": 37,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688387391,
                "cdate": 1700688387391,
                "tmdate": 1700688387391,
                "mdate": 1700688387391,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Aau7t2jwmL",
                "forum": "XN6ZPINdSg",
                "replyto": "ElAWKNaPAF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up Discussions with Reviewer MM5a"
                    },
                    "comment": {
                        "value": "We thank you for the valuable feedback!\n\n> Q2 & Q4: More discussions about partially true knowledge rules and the guidelines of selections of rule weights. \n\nWe thank you for the interesting and valuable question! If a knowledge rule is true with a lower probability, then the associated weight rule should be smaller, and vice versa. In the example, the probability is with respect to the data distribution, which means that $70\\\\%$ of data satisfies the rule \u201cIsStopSign->IsOctogan\u201d. Besides, other distribution-dependent factors such as the portions of class labels also affect the importance of corresponding rules. If one class label is pretty rare in the data distribution, then the knowledge rules associated with it clearly should be assigned a small weight, and vice versa. In summary, the importance of knowledge rules depends on the data distribution, no matter whether we select it via grid search or optimize it in a more subtle way. Therefore, we deem that a general guideline for an effective and efficient selection of rule weights is to perform a grid search on a held-out validation set, which is also essential for standard model training. However, we do not think the effectiveness of COLEP is quite sensitive to the selection of weights. From the theoretical analysis and evaluations, we can expect benefits from the knowledge rules as long as we have a positive weight $w$ for the rules.  We included related discussions in Appendix A in the revision.\n\n> Minor comments: Changing the example on page 3.\n\nWe changed the example on Page 3 for better alignment in the revised version.\n\n> Q3. More details about how to compute the probability bound (e.g., $\\min\\_{\\\\|\\\\eta\\\\|\\_2 < \\delta}\\pi_j(x+\\eta)$).\n\nWe directly apply randomized smoothing [1] to compute the bound of model output in the learning component (i.e., $\\min\\_{ \\\\|\\\\eta\\\\|_2 < \\delta}\\\\pi\\_j(x+\\\\eta)$, $\\max\\_{ \\\\|\\\\eta\\\\|_2 < \\delta}\\\\pi\\_j(x+\\\\eta)$) in experiments. Following the standard procedure in [1], we (a) randomly sample Gaussian noises $\\eta$ from $\\mathcal{N}(0,\\sigma^2)$ for 100k times, (b) compute the empirical mean of the output of the smoothed model $\\hat{\\mathbb{E}}[\\\\pi\\_j(x+\\\\eta)]$, (c) compute the lower bound and upper bound as $\\min\\_{ \\\\|\\\\eta\\\\|_2 < \\delta}\\\\pi\\_j(x+\\\\eta) = \\Phi(\\Phi^{-1}(\\hat{\\mathbb{E}}[\\\\pi\\_j(x+\\\\eta)]) - \\delta/\\sigma)$, $\\max\\_{ \\\\|\\\\eta\\\\|_2 < \\delta}\\\\pi\\_j(x+\\\\eta)=\\Phi(\\Phi^{-1}(\\hat{\\mathbb{E}}[\\\\pi\\_j(x+\\\\eta)]) + \\delta/\\sigma)$, where $\\Phi(\\cdot)$ is the standard Gaussian CDF and $\\delta$ is the perturbation bound. We also provided related details in Appendix C.3 in our paper.\n\n*[1] Cohen, Jeremy, Elan Rosenfeld, and Zico Kolter. \"Certified adversarial robustness via randomized smoothing.\" ICML 2019.*"
                    }
                },
                "number": 38,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692796708,
                "cdate": 1700692796708,
                "tmdate": 1700692977385,
                "mdate": 1700692977385,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BZsI0RyD7f",
                "forum": "XN6ZPINdSg",
                "replyto": "Aau7t2jwmL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7604/Reviewer_MM5a"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7604/Reviewer_MM5a"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for clarifying Q3, I was not aware of this line of work previously. I have raised my score."
                    }
                },
                "number": 40,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694081975,
                "cdate": 1700694081975,
                "tmdate": 1700694081975,
                "mdate": 1700694081975,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CnUdC79aFk",
            "forum": "XN6ZPINdSg",
            "replyto": "XN6ZPINdSg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7604/Reviewer_TP2m"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7604/Reviewer_TP2m"
            ],
            "content": {
                "summary": {
                    "value": "COLEP is a learning-reasoning framework for conformal prediction that provides improved certified prediction coverages under adversarial perturbations. The framework is composed of a learning/sensing component followed by a knowledge-enabled logical reasoning component. The learning component consists of several deep learning models while the reasoning component consists of one or more probabilistic circuits. Alongside the main classification task, the deep learning models are used to predict and estimate the probabilities of other concepts in the input e.g. shape, color etc. The PCs in the reasoning component encode domain knowledge specified as propositional rules over the class and concept variables and helps to ensure robustness against $\\ell_2$ bounded adversarial perturbations to the input variables. The experiments show that COLEP achieves higher prediction coverages as well as smaller prediction set sizes compared to other SOTA methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper presents a novel idea to improve certified robustness of conformal prediction using deep learning models. To the best of my knowledge, this is the first work on using tractable probabilistic circuits to improve the coverage of conformal prediction.\n   \nA positive side of the work is its theoretical analyses. The authors have derived certified coverage for COLEP under both $ell_2$ and finite calibration sample size. And then show that COLEP received higher certified coverage compared to a single model. \n\nExperimental results are also promising."
                },
                "weaknesses": {
                    "value": "The manuscript is strong overall, but there are areas where it could benefit from further elaboration and additional empirical support. \n\n1. The reasoning part seems to be missing significant details to understand how the PCs are working together to reduce the error produced by the main classifier. The paper makes a strong assumption that the reader will be familiar with semantics of PC structures. There should be some brief introduction to these models. \n\n2. The leaf weights (factor weights) are assumed to be prespecified and then there are also the Bernoulli parameters estimated by the neural networks.  It is unclear how these estimated parameters, predictions (classes and concepts) and user defined knowledge rules are combined together to make robust decisions. The coefficients of the component PCs $\\beta_k$ should be more clearly specified in the main manuscript.  \n\n3. The theorems in the main paper need a more detailed discussion. Having only a short paragraph for each theorem doesn't fully explain the complexities and nuances involved, which doesn't give the paper's theoretical contributions their due.\n\n4. To make the empirical evaluation stronger, the authors could consider using more datasets or testing against different types of attacks. This will help confirm that COLEP is robust and works well in various situations."
                },
                "questions": {
                    "value": "1. How many PCs were considered in each of the experiments? Can the authors give an idea on the complexity of the PCs? \n\n2. Can the scope of the evaluation be extended to include the performance of COLEP against  other forms of adversarial attacks? \n\n3. Why wasn't the effect of varying the parameter $\\alpha$ and $\\rho$ on COLEP's performance analyzed?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7604/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7604/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7604/Reviewer_TP2m"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7604/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699287797872,
            "cdate": 1699287797872,
            "tmdate": 1700692822945,
            "mdate": 1700692822945,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mQB3ouGTM9",
                "forum": "XN6ZPINdSg",
                "replyto": "CnUdC79aFk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer TP2m [1/3]"
                    },
                    "comment": {
                        "value": "> Q1: Provide more details about how the PCs can help reduce the error of the main classifier. Specifically, add clarifications of how the prespecified factor weights and estimated likelihood (classes and concepts) and user-defined knowledge rules are combined together to make robust decisions.\n\nWe thank you for the valuable comment. We included the following details about the PC structure and its benefits to the robustness in Section 4.2 in the revised manuscript.\n\nPC defines a joint distribution over a set of random variables (e.g., $\\text{IsStopSign}$, $\\text{IsOctagon}$) and computes the joint probability of them in the graph efficiently by performing forwarding passes. We can encode the knowledge rules with a 3-layer PC as shown in Figure 1: (1) the bottom layer consists of leaf nodes representing Bernoulli variables; (2) the second layer consists of product nodes computing the joint probability of an assignment by multiplying the likelihood of individual Bernoulli variables and the factor values (e.g., $\\hat{\\pi}_{s} \\times \\hat{\\pi}^{o} \\times F([1,1])$); and (c) the top layer is a sum node computing the summation of the products. With the PC structure, one can efficiently compute the marginal probabilities in Equation (6) with two forwarding passes. The details and proofs are shown in Appendix C.1.\n\nThe reasoning component can correct the output probability with encoded knowledge rules and improve the robustness of the prediction. Consider the following concrete example. Suppose that the adversarial example of a speed limit sign misleads the main classifier to detect it as a stop sign with a large probability (e.g., $\\hat{\\pi}_s=0.9$), but the octagon classifier is not misled and correctly predicts that the speed limit sign does not have an octagon shape (e.g., $\\hat{\\pi}_o=0.0$). Then according the computation as Equation (6), the corrected probability of detecting a stop sign given the attacked speed limit image is $\\hat{\\pi}^{\\text{COLEP}}_s=0.9/(0.1e^w+0.9)$, which is down-weighted from the original wrong prediction $\\hat{\\pi}_s=0.9$ as we have a positive logical rule weight $w>0$. Therefore, the reasoning component can correct the wrong output probabilities with the knowledge rules and lead to a more robust prediction framework. Besides the intuition reflected by the simple example, we also theoretically show that the reasoning component benefits in achieving better prediction coverage and accuracy compared to a single DNN without the reasoning component.\n\n\n> Q2: Add clear specification of PC coefficient $\\beta_r$ in the main manuscript.\n\nThank you for the suggestion! We incorporated more specifications for PC coefficients $\\beta_r$ in the main manuscript in Section 4.2. Concretely, we can formulate the corrected conditional class probability $\\pi_j^{\\text{COLEP}}$ with multiple PCs as $\\pi_j^{\\text{COLEP}}(x) = \\sum_{r \\in [R]} \\beta_{r} \\pi_j^{(r)}(x)$, where $\\pi_j^{(r)}$ is the output class probabilities of the $r$-th PC. The core of this formulation is the mixture model involving a latent variable $r_{\\text{PC}}$ representing the PCs. In short, we can write $\\pi_j^{(r)}(x)$ as $\\pi_j^{(r)}(x)=\\mathbb{P}[Y=y|X=x, r_{\\text{PC}}=r]$, and $\\pi_j(x)$ as the marginalized probability over the latent variable $r_{\\text{PC}}$ as $\\pi_j(x)=\\sum_{r \\in [R]} \\mathbb{P}[r_{\\text{PC}}=r] \\cdot \\pi_j^{(r)}(x)$. Hence, the coefficient $\\beta_{r}$ for the $r$-th PC are determined by $\\mathbb{P}[r_{\\text{PC}}=r]$. Although we lack direct knowledge of this probability, we can estimate it using the data by examining how frequently each PC $r$ correctly predicts the outcome across the given examples, similarly as in the estimation of prior class probabilities for Naive Bayes classifiers."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700295133482,
                "cdate": 1700295133482,
                "tmdate": 1700295133482,
                "mdate": 1700295133482,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RsDUWdZPeI",
                "forum": "XN6ZPINdSg",
                "replyto": "CnUdC79aFk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer TP2m [2/3]"
                    },
                    "comment": {
                        "value": "> Q3: Add more discussions about the theorems in the main paper.\n\nWe thank you for the valuable suggestion. We added more details in the remarks of theorems in the revised manuscript. Concretely, we made the following additions: \n\n- updated details of how we can get the probability bounds of the learning component and how we leverage them for computations of the bounds after the reasoning component in remarks of Theorem 1 \n\n- details of the meanings of different terms such as certified prediction set and worst-case non-conformity score and the corresponding references to the equations in the remark of Theorem 2\n\n- illustrations of the technical difference between Theorem 3 and Theorem 2 in the remark of Theorem 3 for a better understanding\n\n- discussions of the physical meanings of the quantities affecting the bounds such as the utility of models $T_{j,\\mathcal{D}}^{(r)},Z_{j,\\mathcal{D}}^{(r)}$ and the utility of knowledge rules $U_j^{(r)}$ in the remark of Theorem 4 and the practical insights implied by the results\n\n- discussions of the influence of the utility of models $T_{j,\\mathcal{D}}^{(r)},Z_{j,\\mathcal{D}}^{(r)}$ and the utility of knowledge rules $U_j^{(r)}$ on the prediction accuracy of COLEP and the practical implications in the remark of Theorem 5\n\n> Q4: Add more empirical evaluations of COLEP. Specifically, evaluate the performance of COLEP against other forms of adversarial attacks.\n\nWe thank you for the valuable suggestions. To evaluate the robustness of COLEP under various types of attacks, we further consider AutoAttack [1], which uses an ensemble of four diverse attacks to reliably evaluate the robustness: (1) APGD-CE attack, (2) APGD-DLR attack, (3) FAB attack, and (4) square attack. APGD-CE and APGD-DLR attacks are step-size-free versions of PGD attacks with cross-entropy loss and DLR loss, respectively. FAB attack optimizes the adversarial sample by minimizing the norm of the adversarial perturbations. Square attack is a query-efficient black-box attack. We evaluate the prediction coverage and averaged set size of COLEP and SOTA method RSCP with $\\ell_2$ bounded perturbation of $0.25$ on GTSRB, CIFAR-10, and AwA2. The desired coverage is $1-\\alpha=0.9$ in the evaluation. The results in the following Table 1 show that under diverse and stronger adversarial attacks, COLEP still achieves higher marginal coverage than the nominal level $0.9$ and also demonstrates a better prediction efficiency (i.e., smaller size of prediction sets) than RSCP. We incorporated the results in Appendix J.2 and referred to it in Section 7 in the main text.\n\nTable 1. Marginal coverage / average set size of RSCP and COLEP under AutoAttack [1] with $\\ell_2$ bounded perturbation of $0.25$ on GTSRB, CIFAR-10, and AwA2.\n\n| | GTSRB | CIFAR-10 | AwA2 |\n| --- | ----------- | ----------- | -- |\n| RSCP | 0.9682 / 2.36 | 0.9102 / 3.56 | 0.9230 / 5.98  |\n| COLEP | **0.9702** / **2.13** | **0.9232** / **2.55** | **0.9520** / **4.57** |\n\n*[1] Croce, Francesco, and Matthias Hein. \"Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks.\" International conference on machine learning. PMLR, 2020.*"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700295171990,
                "cdate": 1700295171990,
                "tmdate": 1700295171990,
                "mdate": 1700295171990,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BRUAKDcmJH",
                "forum": "XN6ZPINdSg",
                "replyto": "CnUdC79aFk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer TP2m [3/3]"
                    },
                    "comment": {
                        "value": "> Q5: Include more details about the number and complexity of PCs in the experiment part.\n\nWe thank you for the valuable question. In summary, we have 3 PCs and 28 knowledge rules in GTSRB, 3 PCs and 30 knowledge rules in CIFAR-10, and 4 PCs and 187 knowledge rules in AwA2. Based on the domain knowledge in each dataset, we encode a set of implication rules into PCs. In each PC, we encode a type of implication knowledge with disjoint attributes. In GTSRB, we have a PC of shape knowledge (e.g., \u201coctagon'', \u201csquare''), a PC of boundary color knowledge (e.g., \u201cred boundary'', \u201cblack boundary''), and a PC of content knowledge (e.g., \u201cdigit 50'', \u201cTurn Left''). In CIFAR-10, we have a PC of category knowledge (e.g., \u201canimal'', \u201ctransportation''), a PC of space knowledge (e.g., \u201cin the sky'', \u201cin the water'', \u201con the ground''), and a PC of feature knowledge (e.g., \u201chas legs'', \u201chas wheels''). In AwA2, we have a PC of superclass knowledge (e.g., \u201cprocyonid'', \u201cbig cat''), a PC of predation knowledge (e.g., \u201cfish'', \u201cmeat'', \u201cplant''), a PC of appearance (e.g., \u201cbig ears'', \u201csmall eyes''), and a PC of fur color knowledge (e.g., \u201cgreen'', \u201cgray'', \u201cwhite''). For example, in the PC of shape knowledge in GTSRB, we can encode the implication rules related to the shape knowledge such as $\\text{IsStopSign} \\implies \\text{IsOctagon}$ (stop signs are octagon), $\\text{IsSpeedLimit} \\implies \\text{IsSquare}$ (speed limit signs are square), and  $\\text{IsTurnLeft} \\implies \\text{IsRound}$ (turn left signs are round). We included the statistics of PCs and knowledge rules in Section 7 and more details in Appendix J.1 in the revised manuscript.\n\n> Q6: Add evaluations with variations of parameters $\\alpha$ and $\\delta$.\n\nWe thank you for the valuable suggestions. We further evaluate COLEP with multiple nominal levels of coverage $1-\\alpha$ and with various perturbation bounds $\\delta$. We provide the results with multiple coverage levels $1-\\alpha$ in the following Table 2, and the results with multiple perturbation bounds $\\delta$ in Table 3. The results demonstrate that for different parameters of $\\alpha$ and $\\delta$, (1) COLEP always achieves a higher marginal coverage than the nominal level, indicating the validity of the certification in COLEP, and (2) COLEP shows a better trade-off between the prediction coverage and prediction efficiency (i.e., average set size) than the SOTA baseline RSCP. We incorporated the results in Appendix J.2 and referred to it in Section 7 in the main text.\n\nTable 2. Marginal coverage / average set size of RSCP and COLEP under PGD Attack with multiple desired coverage levels $1-\\alpha$ with $\\ell_2$ bounded perturbation of $0.25$ on GTSRB.\n\n|  | $1-\\alpha=0.85$ | $1-\\alpha=0.9$ | $1-\\alpha=0.95$ |\n| --- | ----------- | ----------- | -- |\n| RSCP | 0.9029 / 1.63 | 0.9682 / 2.36 | 0.9829 / 2.96  |\n| COLEP | **0.9053** / **1.34** | **0.9702** / **2.13** | **0.9859** / **2.53** |\n\nTable 3. Marginal coverage / average set size of RSCP and COLEP under PGD Attack with multiple $\\ell_2$ bounded perturbations on GTSRB. The nominal level of coverage is $0.9$.\n\n| | $\\delta=0.125$ | $\\delta=0.25$ | $\\delta=0.5$ |\n| --- | ----------- | ----------- | -- |\n| RSCP | 0.9320 / *1.34* | 0.9682 / 2.36 | 0.9712 / 2.78  |\n| COLEP | **0.9891** / 1.47 | **0.9702** / **2.13** | **0.9780** / **2.69** |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700295199156,
                "cdate": 1700295199156,
                "tmdate": 1700295199156,
                "mdate": 1700295199156,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8v5KWlQEpl",
                "forum": "XN6ZPINdSg",
                "replyto": "CnUdC79aFk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7604/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up Discussions"
                    },
                    "comment": {
                        "value": "We thank you again for the valuable suggestions and comments! Since it is close to the end of the discussion period, we wonder whether our responses address your concerns. We are happy to take any further questions or concerns."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700607312718,
                "cdate": 1700607312718,
                "tmdate": 1700607386849,
                "mdate": 1700607386849,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8DA3gd3mS1",
                "forum": "XN6ZPINdSg",
                "replyto": "8v5KWlQEpl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7604/Reviewer_TP2m"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7604/Reviewer_TP2m"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their detailed responses and I have no further questions. At this point, I would like to increase the score for the paper."
                    }
                },
                "number": 39,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7604/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700693140536,
                "cdate": 1700693140536,
                "tmdate": 1700693140536,
                "mdate": 1700693140536,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]