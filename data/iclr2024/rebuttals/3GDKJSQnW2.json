[
    {
        "title": "Pivotal Prompt Tuning for Video Dynamic Editing"
    },
    {
        "review": {
            "id": "fOLtonJKJh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6930/Reviewer_q1QA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6930/Reviewer_q1QA"
            ],
            "forum": "3GDKJSQnW2",
            "replyto": "3GDKJSQnW2",
            "content": {
                "summary": {
                    "value": "This paper targets video editing, especially for the non-rigid, motion editing in the video. Two novel techniques are proposed.\n\n(1) prompt pivoting tuning using a masked target prompt\n\n(2) Spatial-temporal focusing which fuse the cross-attention map of pivotal tuning and editing\n\nThis method is compared with tune-a-video, text2video-zero in many quantitative and qualitative metrics"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Provide many quantitative results in Figures 4,7 of the main paper, Table 1,2, Figure 11 of the appendix"
                },
                "weaknesses": {
                    "value": "My largest concern is how the results if viewed in a dense video format like mp4 or gif. For results in sparse discrete sequences in Figures, 6, 7, and 8, the difference in two consecutive is quite large.\n\nAnother point is the \u201cdynamic motion\u201d in the paper can be better defined. Currently, the concept of \u201cdynamic motion\u201d is closer to the domain of \u201cnew pose\u201d, including \u201cjumps\u201d, \u201cdance\u201d, and \u201cjump\u201d. If it is a pose editing problem, a fair starting point can be a pose-driven controlnet or a text-to-pose model. If the paper targets a larger open-domain \u201cmotion\u201d, like camera motion, fluid, and liquid motion. The better starting point is the motion prior in a pre-trained video model, because fundamentally, pre trained stable diffusion does not have knowledge about open-domain \u201cmotion\u201d."
                },
                "questions": {
                    "value": "Hope the video or long sequence results (including mp4, gif or PNG frames) can be further provided for evaluation.\n\nConsidering the time limit of rebuttal, I understand evaluation on open-domain motion can be difficult and that is not expected."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6930/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6930/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6930/Reviewer_q1QA"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6930/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697420893924,
            "cdate": 1697420893924,
            "tmdate": 1699636807827,
            "mdate": 1699636807827,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZlT6IsRakL",
                "forum": "3GDKJSQnW2",
                "replyto": "fOLtonJKJh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6930/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6930/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer q1QA, We address your comments and questions below.\n\n**[Q1] My largest concern is how the results if viewed in a dense video format like mp4 or gif. Hope the video or long sequence results (including mp4, gif, or PNG frames) can be further provided for evaluation.**\n\n**[A1]** Yes, we update supplementary materials with videos. Please refer to this.\n\n**[Q2] Another point is the \u201cdynamic motion\u201d in the paper can be better defined. Currently, the concept of \u201cdynamic motion\u201d is closer to the domain of \u201cnew pose\u201d, including \u201cjumps\u201d and \u201cdance\u201d. If it is a pose editing problem, a fair starting point can be a pose-driven controlnet or a text-to-pose model. If the paper targets a larger open-domain \u201cmotion\u201d, like camera motion, fluid, and liquid motion. The better starting point is the motion prior in a pre-trained video model, because fundamentally, pre-trained stable diffusion does not have knowledge about open-domain \u201cmotion\u201d.**\n\n**[A2]** Yes, thank you for the attention to this matter. The dynamic motion targeted in our method is also closer to the domain of new pose. We will make this clear in the Abstract and Introduction of the main paper. (i.e., The revision is marked in blue)"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6930/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700072623705,
                "cdate": 1700072623705,
                "tmdate": 1700073222101,
                "mdate": 1700073222101,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JdbQAFaQid",
                "forum": "3GDKJSQnW2",
                "replyto": "ZlT6IsRakL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6930/Reviewer_q1QA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6930/Reviewer_q1QA"
                ],
                "content": {
                    "title": {
                        "value": "Comment by Reviewer"
                    },
                    "comment": {
                        "value": "Thanks for providing the video results.\nI feel your result is quite close to Tune-a-video baselines, which is a little outdated.\nMy suggestion for improvement is that\n(1) Since you are doing few-shot video editing, choosing a strong prior is important. I recommend the video diffusion model or Controlnet.\n(2) The Spatial-Temporal Focusing can also be replaced with a more advanced module like FateZero.\n\nI would keep my previous rating"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6930/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700351935120,
                "cdate": 1700351935120,
                "tmdate": 1700351935120,
                "mdate": 1700351935120,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SL97e5ZqYw",
                "forum": "3GDKJSQnW2",
                "replyto": "fOLtonJKJh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6930/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6930/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**[Q1] I feel your result is quite close to Tune-a-video baselines, which is a little outdated.**\n\n**[A1]** With all due respect, it is absolutely false to say that our results are close to Tune-A-Video. The Tune-A-Video does not have the capability to perform non-rigid editing. It will not be able to change the motion of the object. Our method performs non-rigid editing that includes motion variation of an object, while Tune-A-Video or any other cannot. Non-rigid editing is the paper's motivation, and this paper is the first to attempt it. \n\n**[Q2] Since you are doing few-shot video editing, choosing a strong prior is important. I recommend the video diffusion model or Controlnet.**\n\n**[A2]** We are using the video diffusion model proposed by Jonathan Ho, et al, 'Video Diffusion Models\u2019\u2014this model is described in Section 4.1 (paragraph of `video tuning\u2019). Our model also incorporates a strong prior with Stable Diffusion. It extends the 2D pre-trained diffusion model (i.e., Stable Diffusion) to 3D by enhancing the 2D convolution and attention layers to 3D. Furthermore, the ControlNet is not appropriate for what we want to do. Its generation is based on rigid editing given rigid guidance (e.g., canny edge, hough line). In fact, it is everything contrary to what the proposed algorithm is trying to achieve. \n\n**[Q3] The Spatial-Temporal Focusing can also be replaced with a more advanced module like FateZero.**\n\n**[A3]** Our method covers a more general module of attention control than FateZero catered for video and performs the dynamic motion variation that we are trying to achieve. The module used in FateZero is the cross-attention control module in the work proposed by Hertz, Amir, et al, \u2018Prompt-to-Prompt\u2019 formulated as ( $x = \\alpha_w \\odot x + \\beta_w \\odot y$ ). Our proposed cross-attention control (i.e., Spatial-Temporal Focusing) ( $x^{i} = \\gamma^{i} (\\alpha_w \\odot x^{i} ) + (1\u2212\\gamma^{i})(\\beta_w \\odot y^{i})$ - given in page 7 of the manuscript) allows the capability to perform frame-level control by leveraging temporal weight $\\gamma^{i}$ for each $i$-th frame, which enhances temporal dynamic editing along the frames.\n\n($\\alpha_w, \\beta_w$ are coefficients for blending, $x$ is the cross attention map for editing and $y$ is the cross attention map for tuning process. $\\odot$ is element-wise multiplication with broadcasting.)"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6930/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700389040990,
                "cdate": 1700389040990,
                "tmdate": 1700389104264,
                "mdate": 1700389104264,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eOpKDQLEsi",
                "forum": "3GDKJSQnW2",
                "replyto": "fOLtonJKJh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6930/Reviewer_q1QA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6930/Reviewer_q1QA"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Reviwers"
                    },
                    "comment": {
                        "value": "Thanks for pointing out the difference.\nI get the difference between your method with Tune-a-video, and the controller.\n\nAnyway, I would like to share the results of Tune-a-Video + ControlNet here.\nFirst, tune the stable diffusion model on the input video as\nhttps://tuneavideo.github.io/assets/data/man-basketball.gif \nThen, they prepare the skeleton of the new motion which can be different from the input video motion \nhttps://tuneavideo.github.io/assets/results/tuneavideo/pose/man-basketball/dancing/pose.gif\nhttps://tuneavideo.github.io/assets/results/tuneavideo/pose/man-basketball/running/pose.gif\nAs we can see, the final result is also a ``non-rigid editing'' here:\nhttps://tuneavideo.github.io/assets/results/tuneavideo/pose/man-basketball/dancing/video.gif\nhttps://tuneavideo.github.io/assets/results/tuneavideo/pose/man-basketball/running/video.gif"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6930/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700434285750,
                "cdate": 1700434285750,
                "tmdate": 1700434328527,
                "mdate": 1700434328527,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "b75EldwqB9",
                "forum": "3GDKJSQnW2",
                "replyto": "fOLtonJKJh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6930/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6930/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for sharing the video. Upon viewing the video and double checking with the GitHub of Tune-a-video, it's evident that this is not an example of non-rigid editing. Instead, the dancing video appears to have been previously prepared using a skeleton-based video followed by rigid editing based style transfer."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6930/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700488300157,
                "cdate": 1700488300157,
                "tmdate": 1700492771195,
                "mdate": 1700492771195,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GQNOvhyvCo",
            "forum": "3GDKJSQnW2",
            "replyto": "3GDKJSQnW2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6930/Reviewer_9TJK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6930/Reviewer_9TJK"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed a Pivotal Dynamic Editing (PDEdit) framework for text-based video editing task.  PDEdit allows non-rigid edits to a general video using the target text. PDEdit is versatile and effective for editing various types of videos. Extensive experimental results reveal the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. A versatile video editing method that allows non-rigid editing is proposed.\n\n2. The motivation is clear, and experimental results reveal the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "1. The concept of \"non-rigid edit\" is difficult to understand, existing methods (e.g., Tune-A-Video) can edit the input video so that the motion is inconsistent with the original video, isn't this non-rigid editing?\n\n2. The proposed PDEdit involves many hyperparameters to achieve good results, and requires additional Charades-STA dataset, which makes the practical application process full of challenges.\n\n3. Since the prompt corresponding to the original video is not used, if the target prompt has a low correlation with the original video, will this cause the first tuning stage to fail?\n\n4. Some \"rigid edit\" methods such as Gen-1, VideoComposer and Control-A-Video, which incorporate structure guidance from input videos to generate videos, should also be discussed in the related work.\n\n5. Lack of comparison with existing open-source editing methods such as Pix2video and Video-p2p.\n\n6. Authors should include edited sample videos in supplementary materials or on anonymous websites for easy comparison."
                },
                "questions": {
                    "value": "See Weaknesses for more details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6930/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698396638796,
            "cdate": 1698396638796,
            "tmdate": 1699636807697,
            "mdate": 1699636807697,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2LoRTaFZmz",
                "forum": "3GDKJSQnW2",
                "replyto": "GQNOvhyvCo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6930/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6930/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 9TJK, We address your comments and questions below.\n\n**[Q1] The concept of \"non-rigid edit\" is difficult to understand, existing methods (e.g., Tune-A-Video) can edit the input video so that the motion is inconsistent with the original video, isn't this non-rigid editing?**\n\n**[A1]** Non-rigid editing involves synthesizing the shape of content to align with a specified target prompt. In contrast, rigid editing focuses on altering the style of content while preserving its shape. Existing methods have performed rigid-editing such as inpainting. As shown in Figure 2 of the main paper and Appendix B, our studies found that current editing systems (e.g., Tune-A-Video) lack the capability to perform non-rigid editing such as motion editing.\n\n**[Q2] The proposed PDEdit involves many hyperparameters to achieve good results and requires an additional Charades-STA dataset, which makes the practical application process full of challenges.**\n\n**[A2]** PDEdit takes a single hyperparameter about deterministic score s*, where Charades-STA dataset is used for investigating the optimal value of s*. In an effort to alleviate the reliance on additional datasets, Figure 11 in the Appendix conducts a sensitivity analysis on s*, establishing a robust operating range between 20 and 22. This ensures the proper functioning of the system without necessitating additional datasets for the reproduction of s*.\n\n**[Q3] Since the prompt corresponding to the original video is not used, if the target prompt has a low correlation with the original video, will this cause the first tuning stage to fail?**\n\n**[A3]** To identify this, we perform an experiment on the video in Figure 6 of the main paper (i.e., a video of a woman walking). We prepared two target prompts as (1) 'astronaut on the moon' and (2) 'goldfish in the water' The results are updated in Figure 17 of the Appendix. If a target prompt describes a situation that can be applicable to the given video to a certain degree, it is mirrored accordingly. However, if the prompt describes an entirely different scenario, the model switches to generating video based on the text rather than editing. Thank you for the attention to this matter. We will further elaborate on these findings.\n\n**[Q4] Some \"rigid edit\" methods such as Gen-1, VideoComposer, and Control-A-Video, which incorporate structure guidance from input videos to generate videos, should also be discussed in the related work.**\n\n**[A4]** Yes we add them in the revision, please check our updated version. (i.e., The revision is marked in blue)\n\n**[Q5] Lack of comparison with existing open-source editing methods such as Pix2video and Video-p2p.**\n\n**[A5]** We include the reuslts of Video-P2P and Pix2Video into our current validation in Table 1. We assess the non-rigid/rigid editing capabilities of current editing systems with respect to textual alignment, frame consistency, and fidelity to the input video. Textual alignment measures the average CLIP score between the target prompt and the edited video frame. For frame consistency, we consider both the average CLIP score among sequential frames and the FVD between input and output videos. To evaluate fidelity to the input videos, we mask the edited regions in the edited videos and measure PSNR, LPIPS, and SSIM scores. PDEdit exhibits superior performance compared to previous editing systems, particularly demonstrating significant improvements in non-rigid editing. We also update these in Table 1 of the Appendix.\n\n**[Q6] Authors should include edited sample videos in supplementary materials or on anonymous websites for easy comparison.**\n\n**[A6]** Yes, we update supplementary materials with videos. Please refer to it.\n\n***Table Specifications***\n\n**Textual Alignment**: Clip(text-video)\n\n**Temporal Consistency**: Clip(image-image), FVD\n\n**Fidelity**: PSNR, LPIPS, SSIM on the unedited area (i.e., applying mask on edited area)\n\n**A/B**: non-rigid edit/rigid edit\n\n**Table 1**\n\n||Clip(text-video) \u2191|Clip(image-image) \u2191|FVD \u2193|PSNR \u2191|LPIPS \u2193|SSIM \u2191|\n|:---|:---:|:---:|:---:|:---:|:---:|:---:|\n|PDEdit|26.1/27.9|93.1/94.2|2831/2521|19.75/21.24|0.3672/0.3121|0.7135/0.7821|\n|TAV|16.4/26.0|89.8/92.6|3403/2720|14.62/17.46|0.5776/0.4452|0.5425/0.6271|\n|T2V-Zero|13.7/24.9|86.1/87.4|4052/4235|9.31/11.73|0.5902/0.5732|0.4090/0.4264|\n|Video-P2P|15.1/25.8|91.6/93.4|3261/2683|16.17/18.46|0.4502/0.3951|0.5832/0.7183|\n|Pix2Video|15.9/25.8|90.4/91.8|3131/2704|16.09/18.31|0.4964/0.4211|0.5609/0.7293|"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6930/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700072572260,
                "cdate": 1700072572260,
                "tmdate": 1700073040395,
                "mdate": 1700073040395,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZNE4rWgkb0",
            "forum": "3GDKJSQnW2",
            "replyto": "3GDKJSQnW2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6930/Reviewer_UHtg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6930/Reviewer_UHtg"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on spatial-temporal non-rigid video editing, a relatively underexplored area in video editing tasks. The proposed method, Pivotal Dynamic Editing (PDEdit), distinguishes itself by not requiring original video captions but editing videos based on suggested prompt pivoting. The method demonstrates a degree of generality."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Unlike previous video editing approaches, which mainly focus on simple edits like appearance and style, this paper pioneers motion editing, showing a forward shift in research focus.\n- The method proposed here is innovative, deviating significantly from existing pipelines.\n- The writing is clear and relatively easy to follow."
                },
                "weaknesses": {
                    "value": "- The visual results presented in the paper are subpar. While the difficulty of the task is understood, the demonstrated demos are unsatisfactory in their current state.\n- The experimental section is weak, lacking explicit numerical comparisons in the main text. The experiments are limited to only 20 videos, which is a small sample size. The comparison metrics lack persuasiveness.\n- Although supplementary materials include editing demos, the videos appear to lack continuity, and there seems to be a limitation in supporting video editing across different resolutions."
                },
                "questions": {
                    "value": "See above. \n- I am uncertain about how many frames of video the author's method supports editing. It appears to be limited to very short 8-frame videos, which have limited practical significance.\n- Stacking images as a presentation method is not ideal; I would prefer actual demos in GIF or video format.\n- Despite these shortcomings, I recognize the contribution of the authors to this underexplored area. Their method might inspire future research. However, at the current stage, I can only give a borderline score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "no"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6930/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698410953172,
            "cdate": 1698410953172,
            "tmdate": 1699636807549,
            "mdate": 1699636807549,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nDL42AaJDT",
                "forum": "3GDKJSQnW2",
                "replyto": "ZNE4rWgkb0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6930/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6930/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer UHtg, We address your comments and questions below.\n\n**[Q1,3] The visual results presented in the paper are subpar. While the difficulty of the task is understood, the demonstrated demos are unsatisfactory in their current state. Although supplementary materials include editing demos, the videos appear to lack continuity, and there seems to be a limitation in supporting video editing across different resolutions.**\n\n**[A1,3]** To make this clear, we updated supplementary material with videos. Please refer to it.\n\n**[Q2] The experimental section is weak, lacking explicit numerical comparisons in the main text. The experiments are limited to only 20 videos, which is a small sample size. The comparison metrics lack persuasiveness.**\n\n**[A2]** We conduct experiments involving numerical comparisons, as detailed in Table 1 of the Appendix. Our evaluation encompasses textual alignment (CLIP), frame consistency (CLIP, FVD), and fidelity to the input video (PSNR, LPIPS, SSIM). Additionally, in Table 2 of the Appendix, we extend our validation to include more samples from other video datasets, (i.e., UCF101, WebVid-10M). We will consider to incorporate these into the main paper.\n\n**[Q4] I am uncertain about how many frames of video the author's method supports editing. It appears to be limited to very short 8-frame videos, which have limited practical significance.**\n\n**[A4]** Considering our GPU resource (i.e., A100 GPU), our experiments are conducted on video sequences spanning 24 to 40 frames. Our proposed method does not rely on the length of the video.\n\n**[Q5] Stacking images as a presentation method is not ideal; I would prefer actual demos in GIF or video format.**\n\n**[A5]** To make this clear, we add supplementary results with video. Please refer to the updated materials.\n\n**[Q6] Despite these shortcomings, I recognize the contribution of the authors to this underexplored area. Their method might inspire future research. However, at the current stage, I can only give a borderline score.**\n\n**[A6]** Thank you for the recognition. Of course, we can not do all of it. But, to the best of our knowledge, PDEdit represents a pioneering effort in tackling non-rigid editing, particularly in the realm of motion editing. We aspire for our work to make meaningful contributions to these research domains. We ensure to release our code and project."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6930/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700072447117,
                "cdate": 1700072447117,
                "tmdate": 1700072447117,
                "mdate": 1700072447117,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rc1J7LiRY4",
            "forum": "3GDKJSQnW2",
            "replyto": "3GDKJSQnW2",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6930/Reviewer_1N5p"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6930/Reviewer_1N5p"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a method for text-based video dynamic editing, which involves making non-rigid spatial-temporal alterations.  They propose pivotal prompt tuning for the system to tune a video with target text prompt and temporal dynamic editing to apply motion changes in spatial and temporal domain."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper proposes a novel approach to text-based video dynamic editing\n2. The video frames shown in the paper demonstrate edited motion similar to the text-prompt inputs."
                },
                "weaknesses": {
                    "value": "1. The absence of accompanying videos in a video editing paper makes it challenging to assess the temporal consistency of the edited frames.\n2. The quality of the editing falls short of expectations, with significant color discrepancies and noticeable object alterations post-editing. It seems like the unedited area is not preserved after editing for the applications outside of style transfer.\n3. The quantitative evaluation should also evaluate how this editing method preserves the unedited area. For example, include PSNR, SSIM or LPIPS for the unedited background area for motion editing."
                },
                "questions": {
                    "value": "1. In distributional pivoting, do you need to tune the parameter for s* for different videos?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Privacy, security and safety"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Video motion editing could be abused for illegal behavior that should be concerned."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6930/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6930/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6930/Reviewer_1N5p"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6930/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698876362596,
            "cdate": 1698876362596,
            "tmdate": 1699636807423,
            "mdate": 1699636807423,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "c0vkfKobDx",
                "forum": "3GDKJSQnW2",
                "replyto": "rc1J7LiRY4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6930/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6930/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 1N5p, We address your comments and questions below.\n\n**[Q1] The absence of accompanying videos in a video editing paper makes it challenging to assess the temporal consistency of the edited frames.**\n\n**[A1]** We uploaded a supplementary material containing the videos. \n\n**[Q2] The quality of the editing falls short of expectations, with significant color discrepancies and noticeable object alterations post-editing. It seems like the unedited area is not preserved after editing for the applications outside of style transfer.**\n\n**[A2]** There is a variance in the quality of editing results due to the text-scene bias of the pre-trained diffusion model (e.g., Stable Diffusion). For example, as shown in Figure 16 about Limitation in the Appendix, we edit a walking astronaut\u2019s motion as skiing, and the results show the man skiing on a snowy scene. This suggests that the editing about skiing exhibits a bias towards scenes involving skiing on snow. Consequently, the quality of editing depends on whether the required editing by the target prompt is bias-aligned or bias-conflicted.\n\n**[Q3] The quantitative evaluation should also evaluate how this editing method preserves the unedited area. For example, include PSNR, SSIM, or LPIPS for the unedited background area for motion editing.**\n\n**[A3]** Yes we did. In Table 1 of the Appendix, we evaluated PSNR, SSIM, and LPIPS for the unedited area by applying the mask to the edited region.\n\n**[Q4] In distributional pivoting, do you need to tune the parameter for** s* **for different videos?**\n\n**[A4]** Yes, the s* is a trainable parameter determined under the training video dataset. In Figure 11 of the Appendix, our investigation delves into the sensitivity analysis of \"s*.\" The findings reveal a stable working region defined by 20 < s* < 22. This implies we can leverage this specific range for optimal score selection without training videos.\n\n**[Q5] Video motion editing could be abused for illegal behavior, and that should be a concern.**\n\n**[A5]** Within our main paper's ethical statements, we acknowledge and align with concerns regarding the illicit use of video editing. Consequently, we explore viable solutions, such as forensic analysis and digital watermarking, to mitigate these ethical issues effectively."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6930/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700072398628,
                "cdate": 1700072398628,
                "tmdate": 1700390712359,
                "mdate": 1700390712359,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "A87PSIyZ7m",
                "forum": "3GDKJSQnW2",
                "replyto": "c0vkfKobDx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6930/Reviewer_1N5p"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6930/Reviewer_1N5p"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the video provided. From the video, I cannot find consistent editing even for unedited areas. Although the proposed method uses stable diffusion which makes the editing inconsistent, there are some other works of video generation that produce consistent background, for example, Emu video. Therefore, I'll keep my original rating"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6930/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641393229,
                "cdate": 1700641393229,
                "tmdate": 1700641393229,
                "mdate": 1700641393229,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]