[
    {
        "title": "Hard View Selection for Contrastive Learning"
    },
    {
        "review": {
            "id": "MjGrQUbLL5",
            "forum": "ioBIT7gLBm",
            "replyto": "ioBIT7gLBm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission294/Reviewer_zn9X"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission294/Reviewer_zn9X"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces Hard View Selection (HVS), aiming to improve the effectiveness of pretraining in contrastive learning scenarios. By selecting the \"hardest\" views during training, HVS pushes the model to learn more robust features. The authors claim compatibility with several popular contrastive learning methods, like SimSiam, DINO, and SimCLR, and demonstrate its effectiveness using ImageNet for pretraining."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Novel Approach: The paper introduces a novel concept of \"Hard View Selection (HVS),\" which aims to improve the efficacy of pretraining in contrastive learning settings.\n- Compatibility: One of the significant advantages of HVS is its compatibility with a variety of existing contrastive learning methods like SimSiam, DINO, and SimCLR. This makes it easily adaptable in various existing pipelines.\n- Simplicity: The method is described as being simple to integrate, requiring only the ability to compute sample-wise losses. This lowers the bar for adoption and experimentation.\n- Focus on Challenging Samples: By focusing on the \"hardest\" views during training, the paper takes an interesting approach to make the model focus on challenging aspects of the data, potentially leading to more robust feature learning."
                },
                "weaknesses": {
                    "value": "- Computational Cost: The method involves selecting the \"hardest\" views by running forward passes for multiple view pairs, which could increase the computational cost of training, especially for large-scale datasets or complex models.\n- Lack of Theoretical Analysis: The paper seems to focus on empirical validation but doesn't provide a theoretical foundation for why the \"hard view selection\" approach should work, which could limit its scientific rigor.\n- Unclear Impact on Convergence: Introducing harder samples could potentially slow down the convergence of the training process"
                },
                "questions": {
                    "value": "Could you elaborate on the computational overhead introduced by the Hard View Selection (HVS) method, especially when dealing with large-scale datasets or complex models?\n\nCould you provide more details on the complexities involved in computing sample-wise losses, particularly in the context of different neural network architectures?\n\nDoes the introduction of \"harder\" samples through HVS have any noticeable impact on the convergence speed or stability of the training process?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission294/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission294/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission294/Reviewer_zn9X"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission294/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698781027176,
            "cdate": 1698781027176,
            "tmdate": 1699635955621,
            "mdate": 1699635955621,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bSwhvpgLLn",
                "forum": "ioBIT7gLBm",
                "replyto": "MjGrQUbLL5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission294/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission294/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Lack of Theoretical Analysis: The paper seems to focus on empirical validation but doesn't provide a theoretical foundation for why the \"hard view selection\" approach should work, which could limit its scientific rigor.**\n\nThank you for your feedback. We also thought about ways to incorporate a theoretical foundation and validation to our manuscript. Given the nature of our study however, we found it challenging to find an easy enough framework where math/theory could be applied, especially given the dynamics and assumptions of the various baselines HVS is applied to in our manuscript. Note also that, to our best knowledge, little to no theory exists for the considered baselines in this paper. We appreciate your point on enhancing scientific rigor and would be grateful for any suggestions or insights you might have on how to integrate theoretical analysis more effectively into our work. \n\n**Unclear Impact on Convergence: Introducing harder samples could potentially slow down the convergence of the training process**\n\nAcross all our experiments, we fortunately observed no slow-down in convergence during training. More broadly speaking, the training loss pattern remained identical to the baseline training. The only difference to the baseline trainings we were able to observe is that the training loss is slightly higher (see Fig. 7 in the supplementary material for an example of the training loss with/without HVS). This might encourage better generalization. \n\n**Could you elaborate on the computational overhead introduced by the Hard View Selection (HVS) method, especially when dealing with large-scale datasets or complex models?**\n\nThank you for communicating this concern. We have addressed this point in the common comment to all reviewers, and we hope that the additional information provided adequately addressed your inquiry. If you have any further questions regarding the computational overhead, please feel free to let us know.\n\n**Could you provide more details on the complexities involved in computing sample-wise losses, particularly in the context of different neural network architectures?**\n\nSince most contrastive/non-contrastive learning approaches preserve the sample-wise losses within a batch in their objective function, applying HVS works out of the box. In the rare cases where approaches (or architectures) collapse the batch (or sample) dimension, HVS cannot be applied out of the box. One example where this is the case is Barlow Twins [1]. Here, the authors choose an objective function where the cross-correlation matrix between embedding outputs is computed to minimize the distance to the identity cross-correlation matrix. During the computation of the cross-correlation, the batch-dimension is collapsed by matrix multiplication that results in loss of any sample-wise information. To still apply HVS here, one potential approach is to explore the construction of sub-matrices from the full cross-correlation matrix. These sub-matrices could be designed to correspond to individual samples, allowing for the recovery of sample-specific information. It's worth noting that, as of now, we haven't conducted tests to validate the effectiveness of this approach.\n\n[1] https://arxiv.org/abs/2103.03230\n\n**Does the introduction of \"harder\" samples through HVS have any noticeable impact on the convergence speed or stability of the training process?**\n\nWhen designing the HVS approach, we also thought of the potential risk of negative impacts with regard to convergence speed. However, empirically, we realized that HVS works extremely stably and has not impacted convergence speed across any of our experiments. Moreover, and as mentioned in the paper, it is also extremely robust to the baseline hyperparameters and does not require adaptation of these.\n\nWe express our gratitude once more for the insightful feedback provided."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission294/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699796231217,
                "cdate": 1699796231217,
                "tmdate": 1699796231217,
                "mdate": 1699796231217,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Vzvr3UhyZG",
            "forum": "ioBIT7gLBm",
            "replyto": "ioBIT7gLBm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission294/Reviewer_PWAF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission294/Reviewer_PWAF"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an approach to improve self-supervised contrastive learning methods (computer vision) that aim to learn features invariant to different views (i.e. augmentation) of an input image. The paper introduces a simple (and easy to implement) algorithm \u201cHard View Selection (HSV)\u201d for selecting the view pair used in SSL contrastive training. The goal of HSV algorithm is to select relatively hard views that improve the model's performance. HSV comprises of making forward calls with a set of randomly selected views, then making forward calls to pick out the view pair with highest loss for backpropagation step. The paper hypothesizes that selecting the view pair that are dependent on the current state of the model parameters yields the most benefits rather than using a fixed/random/learned adversary policy. The benefit of the proposed method is to empirically improve the SSL feature representation so that it could be show improvement for other downstream tasks (classification, detection, etc.). The paper shows that HSV is compatible with other contrastive methods like DINO, SimSiam and SimCLR."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper explores a strategy (HSV) for selecting the views (image augmentation pairs in contrastive learning) that leads to performance improvement on downstream tasks.\u2028HSV is a simple and easy to implement view-selection strategy that achieves improvement on ImageNet linear classification accuracy (order of 0.55% - 1.93%). The paper provides detailed ablation experiments to find an alternative view-sampling strategy (both learning-based eg. STN and statistics-based approach). Ablation experiments regarding the initial number of view pairs that determine the hardness of the selected view pair is also provided.(higher initial pool of view pairs would lead to more adversarial view selection for CL training). The paper is well-written and easy to follow. The paper provides code for reproducibility."
                },
                "weaknesses": {
                    "value": "It would be helpful for the reader to get a better understanding of the following questions/suggestions:\n\n1. A detailed analysis of the HVS overhead in terms of running time/ wall clock/ FLOPs/ throughput along with memory requirement for each the baselines DINO (2x overhead), SimSiam(1.55x overhead), SimCLR.  Preferably in a format of a table.\n\n2. Extending on the above point, it seems since the overhead is added due to HSV, a more fair choice of x-axis in table 1 (and other tables) should be training time instead of epochs, as we see in table 1, that algorithm's performance increases with longer epoch training. For example assuming DINO + HSV is twice slower than DINO, it would be great to see a performance comparison between DINO+HSV at epoch 100 with DINO at epoch 200 (similarity with other baselines).\n \n3. It might be good to see some qualitative results on how good the learned features are with HSV strategy. For example, adding some attention maps as done in DINO paper, to get an idea if the attention maps also improves using HVS.\n\n4. As HSV is proposed as a general view sampling strategy it might be helpful to see some results on other relevant baselines like SimCLR_v2, DINO_v2, Swav. If some of the recent methods require huge amounts of compute or dataset, showing some results on ImageNet with lower training epochs would give the reader a better idea about the efficacy of the hard view selection strategy.\n\n5. It could be a good idea to relate and mention in the sub-section of \u201cTaking a Closer Look at the Intersection over Union\u201d with the idea of \u201clocal-to-global\u201d correspondences (as introduced in SimCLR :  Global and local views/Adjacent views and DINO : global and student view). Qualitatively, looking at Fig2, it does look like a HSV selected view pair comprises of global view (larger area crop) and student view (small area crop)."
                },
                "questions": {
                    "value": "Some questions/suggestions: \n\n1. How many seeds are used for Table 1 average computation. \u201cRef: averaged over multiple seeds unless otherwise mentioned\u201d\n\n2. It is stated in the paper that SimSiam+HSV has 1.55x overhead and DINO+HSV has 2x overhead. Since DINO uses much more views (10 views leading to 128 max view pairs for forward call) than SimSiam (2 views that leads to 4 view pairs for forward call). What is the time taken for doing 1 DINO forward call and 1 backward call as the reviewer was expecting the DINO+HSV to be slower than 2x.\n\n3. The reviewer was wondering about the combination of HSV with random view pair selection in order to reduce the computational  overhead. Maybe having 50% random and 50% HSV samples for training would be nice ablation experiment to shed light on the requirement of HSV adversarial pair selection. If this works without decreasing the performance with a large margin, then it would be interesting to see some ablation on tuning this percentage of HSV samples in training."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission294/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission294/Reviewer_PWAF",
                        "ICLR.cc/2024/Conference/Submission294/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission294/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698824546316,
            "cdate": 1698824546316,
            "tmdate": 1700640800400,
            "mdate": 1700640800400,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qgcX75BL1q",
                "forum": "ioBIT7gLBm",
                "replyto": "Vzvr3UhyZG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission294/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission294/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**[...] Ablation experiments regarding the initial number of view pairs that determine the hardness of the selected view pair is also provided.(higher initial pool of view pairs would lead to more adversarial view selection for CL training). The paper is well-written and easy to follow. The paper provides code for reproducibility.**\n\nWe thank the reviewer for their constructive feedback and for acknowledging our efforts to make our work transparent and reproducible. \n\n\n**A detailed analysis of the HVS overhead in terms of running time/ wall clock/ FLOPs/ throughput along with memory requirement for each the baselines DINO (2x overhead), SimSiam(1.55x overhead), SimCLR. Preferably in a format of a table. [...] Extending on the above point, it seems since the overhead is added due to HSV, a more fair choice of x-axis in table 1 (and other tables) should be training time instead of epochs, as we see in table 1, that algorithm's performance increases with longer epoch training. For example assuming DINO + HSV is twice slower than DINO, it would be great to see a performance comparison between DINO+HSV at epoch 100 with DINO at epoch 200 (similarity with other baselines).**\n\nWe have addressed the concern on computation overhead in the common comment to all reviewers. Please let us know if your concerns persist so that we can further discuss them with you.\n\n\n**It might be good to see some qualitative results on how good the learned features are with HSV strategy. For example, adding some attention maps as done in DINO paper, to get an idea if the attention maps also improves using HVS.**\n\nThis is an interesting idea, which indeed may lead to insights as to what type of features HVS learns. We are working on this and will report back to the reviewer here once we have these figures.\n\n**As HSV is proposed as a general view sampling strategy it might be helpful to see some results on other relevant baselines like SimCLR_v2, DINO_v2, Swav. If some of the recent methods require huge amounts of compute or dataset, showing some results on ImageNet with lower training epochs would give the reader a better idea about the efficacy of the hard view selection strategy.**\n\nWe thought carefully and long about the selection of baselines for our paper. Indeed, there exist other baselines as well. For DINOv2, we kindly note that it was not pretrained on ImageNet-1k but on the unreleased LVD-142M dataset and so there would be no way to add HVS and compare against it fairly. For SimCLRv2, we note that only a simplified version has been open-sourced by the authors [1]. For example, the public code of v2 does incorporate contrastive learning which is used in v1. In order for us to show that our method works with the original (contrastive) v1 loss, we decided against using v2. In the case of Swav, we notice that it comes from the same authors as DINO which is the more recent and better-performing method that, similar to Swav, also incorporates the multi-crop technique. \n\nRegarding the reviewer\u2019s comment on including a more recent baseline, we hope we were able to address it by adding the popular iBOT baseline (see general comment to all reviewers above). We believe this result again underpins the general applicability of HVS since it also works well with a MIM objective.\n\n[1] https://github.com/google-research/simclr/issues/115\n\n**It could be a good idea to relate and mention in the sub-section of \u201cTaking a Closer Look at the Intersection over Union\u201d with the idea of \u201clocal-to-global\u201d correspondences (as introduced in SimCLR [...]**\n\nWe thank the reviewer once again for this valuable feedback point. Indeed, this is a rich addition to the existing description and builds a useful analogy to SimCLR as well as DINO through the local-to-global correspondences and multi-crop settings. We have added this to the end of the mentioned subsection in the paper.\n\n**How many seeds are used for Table 1 average computation. \u201cRef: averaged over multiple seeds unless otherwise mentioned\u201d**\n\nThank you for pointing this out. Our results are averages of 3 seeds (except for iBOT where we currently average over 2 seeds). We have updated the mentioned sentence in the paper to be more specific.\n\n**What is the time taken for doing 1 DINO forward call and 1 backward call as the reviewer was expecting the DINO+HSV to be slower than 2x.**\n\nDid we answer your question with the common comment to all reviewers and with the additional method-specific speed information provided there? Please do let us know if your concerns persist so that we can provide the missing information."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission294/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699796126676,
                "cdate": 1699796126676,
                "tmdate": 1699796126676,
                "mdate": 1699796126676,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3mvUe9CkTY",
                "forum": "ioBIT7gLBm",
                "replyto": "Vzvr3UhyZG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission294/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission294/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Attention Map Results"
                    },
                    "comment": {
                        "value": "Dear reviewer,\nWe wanted to follow up on your suggestion to create attention maps as a way to qualitatively study the potential effects and differences of the HVS learning method and their resulting features when compared to the baseline. We now added an overview of the generated attention maps under Section \u201cJ Attention Maps\u201d in our supplementary material (last 3 pages), where we contrast 9 attention maps from a DINO ViT-S/16 100-epoch model for each HVS and the baseline.\n\nAll input images are from the ImageNet-1k validation split. The color code used depicts strong attention in yellow and weak attention in green and attentions from the HVS model are shown in the top row and attentions from the baseline in the bottom row, respectively. To summarize our take-away from this study, we do not see apparent strong differences between HVS and the baseline. What we occasionally notice is that the HVS models seem to capture the shape of some subtle/indistinct objects better (e.g. the creek/river in the valley in Fig. 13, the lizard in Fig. 14, the speaker in Fig. 15) or focus slightly more on the context and background (Fig. 17, Fig. 18, and Fig. 19). However, we note that these characteristics also often seem subtle. Would the reviewer agree?"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission294/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699985969699,
                "cdate": 1699985969699,
                "tmdate": 1699985969699,
                "mdate": 1699985969699,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ATYbE9RL8b",
                "forum": "ioBIT7gLBm",
                "replyto": "3mvUe9CkTY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission294/Reviewer_PWAF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission294/Reviewer_PWAF"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "Thank you for providing detailed explanation for the questions. It is encouraging to see the improvement after \"normalizing Table 1 for wallclock time and applying the 50% resolution reduction\" and it would helpful for the readers to add this \"normalized table1\" result to the main paper (or appendix by adding a reference for this table in main paper). Accordingly, I have increased my score to 6.\n\nYeah I agree with the authors that the attention maps does look quite similar (with or without HVS)."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission294/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700640783708,
                "cdate": 1700640783708,
                "tmdate": 1700640783708,
                "mdate": 1700640783708,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "INQ8xtWEfP",
            "forum": "ioBIT7gLBm",
            "replyto": "ioBIT7gLBm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission294/Reviewer_seyJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission294/Reviewer_seyJ"
            ],
            "content": {
                "summary": {
                    "value": "In pair-based self-supervised representation learning methods (e.g. SimCLR), positive pairs are constructed using data augmentation. \nThis paper proposes to produce many random positive pairs (e.g. 4) for any query image and compute loss over the \"hardest\" one.\nIn a way, it is a hard positive (or view) selection method.\nIt is shown that this technique can be utilized for various methods such as DINO, SimSiam and SimCLR, yielding consistent improvements on ImageNet classification and several transfer learning scenarios."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The main idea behind the paper is to make the training of pair-based self-supervised methods harder. \nThis is done by producing several random positive pairs and backpropagating gradients through the hardest one.\nIt is shown that this strategy picks pairs that overlap less, hence models learn better representations on ImageNet-1K after being trained the same amount of \"epochs\".\nBetter representations mean consistent improvements on ImageNet-1K classification and various transfer learning experiments including classifcation, detection and segmentation.\n\nOverall, the paper is easy to read."
                },
                "weaknesses": {
                    "value": "I have two main concerns, listed below.\n\n1) As shown in Figure-3, positive pairs used for computing loss overlap less thanks to the proposed method, and this facilitates better representations. Tian 2020b already shows this phenomenon, i.e. there is a sweat spot in the IoU ratio which leads to the \"optimal\" performance. It is not clear what more this paper offers. One distinct angle in this paper is the fact that multiple positive pairs are utilized during training (although loss is computed over only 1 pair in the end). I wonder what would happen if loss was computed over all crops, by simple averaging or weighted averaging (depending on the difficulty of a pair).\n\n2) The computational overhead introduced by the proposed method (\"about a factor of 1.55\u00d7 for SimSiam\") is a bit unfair for the baselines. I wonder if this extra compute time can be used in favor of other models too, e.g. by training models longer. Cause longer training schedules often bring substantial gains for self-supervised methods. Also, it should be noted that the proposed method has seen more samples (due to encoding multiple pairs) which already impacted for instance batch-norm statistics (although loss is not backpropagated over unused pairs). Then it would be nice to see baselines processed 4x more samples."
                },
                "questions": {
                    "value": "I would like the authors to address the concerns I raised in the weaknesses part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission294/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698872496817,
            "cdate": 1698872496817,
            "tmdate": 1699635955458,
            "mdate": 1699635955458,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pE9I4Waun4",
                "forum": "ioBIT7gLBm",
                "replyto": "INQ8xtWEfP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission294/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission294/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**As shown in Figure-3, positive pairs used for computing loss overlap less thanks to the proposed method, and this facilitates better representations. Tian 2020b already shows this phenomenon, i.e. there is a sweat spot in the IoU ratio which leads to the \"optimal\" performance. It is not clear what more this paper offers.**\n\nWe thank the reviewer for pointing out the relevance of Tian2020b. Indeed, we agree with the reviewer that in hindsight we should have contrasted our paper in greater detail with Tian2020b and regret to not have done that in the initial version of the paper. Since also Reviewer #a1az has brought up this point, we are copy-pasting our response here, which we believe discusses the similarities and differences to Tian2020b comprehensively:\n\nFirst, Tian2020b leverages the information of views to characterize good views for a given task based on the mutual information principle and use this framework to discover sweet spots for data augmentation hyperparameters. These data augmentation hyperparameters are widely adopted by SSL approaches and our baselines. Additionally, Tian2020b use their insights about good views to then adversarially learn flow-based models that generate novel color spaces for the small STL-10 dataset. After the views have been learned offline, they then perform standard contrastive learning on these generated views. \n\nThis approach is different to HVS on multiple fronts: with HVS we propose a method that can be integrated *online* into CL and non-CL pipelines without the use of a potentially instable adversarial learning objective. Our method is dependent on the model state and on individual samples since it is integrated online into the learning procedure, which is different to Tian2020b since their approach is not entangled with the model state. \n\nWhile Tian2020b ablates also based on distance metrics in their study, they do not include the IoU metric. Moreover, another central difference is that we use the distance metric as a tool to better understand the selection of HVS and to discover that model-state and image-dependence is essential, where Tian2020b uses it as a motivation for their framework. Furthermore, our ablation studies include experiments that quantify the difficulty of predicting good/hard pairs, which Tian2020b does not have. In summary, we view Tian2020b as complementary to our work, particularly in the context of their derived data augmentation policies and believe that HVS could even be combined with the findings of Tian2020b. However, it's important to underscore the limited overlap in methodology and experimental design between the two approaches. HVS in the end could be seen as an early work that shows that for good views, we need to derive better sampling methods that can dynamically adapt to the actual state of training.\n\nWe have now added the expanded discussion on Tian2020b from above to the Introduction and Related Work sections of our paper. \n\n**[...] One distinct angle in this paper is the fact that multiple positive pairs are utilized during training (although loss is computed over only 1 pair in the end). I wonder what would happen if loss was computed over all crops, by simple averaging or weighted averaging (depending on the difficulty of a pair).**\n\nThank you for this idea on averaging the losses. Please allow us to make sure that the method was correctly understood by the reviewer. We emphasize that the loss is computed for all pairs individually but only backpropagated for one pair (namely the one with the highest loss). Let us make sure we understood your suggestion correctly: instead of forwarding all view pairs and discarding all except the pair with the highest loss, you are suggesting to keep all pair losses and compute an average across all pair losses or smart averaging (by weighing the pairs with some difficulty score)? Assuming we understood your suggestion, we believe it would significantly ramp up the computation cost of the method since it would effectively multiply the batch size (since all activations for all pairs would have to be kept in memory for the backward pass). While applying HVS with a weighted loss that amplifies the gradient updates proportional to some difficulty measure sounds very exciting, but since we have no experience in this regard we lack the intuition about the consequences of such a modification with regard to training stability. Please let us know if we misunderstood your suggestion so that we can engage in a follow-up discussion."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission294/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699795963305,
                "cdate": 1699795963305,
                "tmdate": 1699795963305,
                "mdate": 1699795963305,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8S7NfMpYfL",
                "forum": "ioBIT7gLBm",
                "replyto": "INQ8xtWEfP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission294/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission294/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**The computational overhead introduced by the proposed method (\"about a factor of 1.55\u00d7 for SimSiam\") is a bit unfair for the baselines. I wonder if this extra compute time can be used in favor of other models too, e.g. by training models longer. Cause longer training schedules often bring substantial gains for self-supervised methods. Also, it should be noted that the proposed method has seen more samples (due to encoding multiple pairs) which already impacted for instance batch-norm statistics (although loss is not backpropagated over unused pairs). Then it would be nice to see baselines processed 4x more samples.**\n\nThank you for communicating this critique with us. We have addressed the concern on computation overhead in the common comment to all reviewers. Please let us know if your concerns persist so that we can further discuss them with you.\n\nRegarding the concern about the models having seen more samples, we kindly disagree with the reviewer. First, the DINO and iBOT ViT-S models do not use batch norm but layer norm layers which do not keep track of running statistics and those experiments still show the same improvements. Moreover, even when batch norm layers are used, the batch norm learnable parameters are not affected by the HVS forward passes since we disable gradients for the selection step and hence, the BN parameters are not updated during the selection. Note also, that the running mean and variance buffers are only used for inference, see the pytorch documentation [1] which states:\n\n\u201c*Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation.*\u201d\n\nDuring training, we keep our model in model.train() state so the running mean and variance are not used. Consequently, the HVS models do not have an advantage over the baselines in terms of having seen more samples.\n\n[1] https://pytorch.org/docs/master/generated/torch.nn.BatchNorm2d.html?highlight=batchnorm2d#torch.nn.BatchNorm2d\n\n\nWe express our gratitude once more for the constructive feedback provided. Aiming to have effectively addressed your concerns, we kindly inquire whether there is an opportunity to increase your score. Alternatively, we welcome any additional points for improvement that would allow for a score increase."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission294/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699795991753,
                "cdate": 1699795991753,
                "tmdate": 1699796013082,
                "mdate": 1699796013082,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jesK5ZY9IU",
            "forum": "ioBIT7gLBm",
            "replyto": "ioBIT7gLBm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission294/Reviewer_a1az"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission294/Reviewer_a1az"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel approach called Hard View Selection (HVS) to enhance self-supervised methods by improving the selection of image views during training. HVS effectively increases the task difficulty during pretraining and achieves competitive or better results compared to the conventional baseline, improving accuracy on ImageNet and transfer tasks across various methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The method is simple to understand and sound\n- Augmentations are crucial in self-supervised learning. However, they are either hardcoded or grid-searched. It is good to see that there is research in the direction of picking augmentations automatically\n- The authors try many methods and validate with many downstream tasks"
                },
                "weaknesses": {
                    "value": "- The title says \u201ccontrastive\u201d but most of the experiments use non-contrastive methods like simsiam and dino. This is confusing.\n- In addition to the point above, it actually seems that the proposed method is better defined for non-contrastive losses. In fact, for contrastive losses (which contrast the attraction of the positives with the repulsion of the negatives) the method is not guaranteed to find the hardest possible set of positives and negatives. This would require exploring all the possible combinations of negatives, which is intractable. A better idea would be to use all the negatives from all the views at the denominator. In that case, since the denominator works as a max at low temperatures, you would be \u201cguaranteed\u201d to have the hardest negative. This is not described and discussed in the paper except one sentence at the end of 3.3.\n- In algorithm 1, the authors report that they first organize the views into pairs and then forward each pair. This entails forwarding every view multiple times (one view can be in more than one pair). This is confusing and does not make sense at all. It would be better to first forward the views separately and then compare them 2 by 2. Can the authors please explain why they first create the pairs and then forward?\n- Why not just make all comparisons and backprop all of them? Why is this not ablated?\n- Figure 4 shows that the manual min IoU policy works for SimSiam but not for DINO. Maybe it\u2019s just that multi-crop does not play well with min IoU. I am not fully convinced by this ablation.\n- Figure 5 shows that performance decreases over time, which might be due to overfitting (or might not). In any case, cifar100 is not a good benchmark to make a point about longer training.\n- Throughout all the tables the performance improvements are small. Once factored in the ~2x slowdown, there seems to be almost no difference. This is not great, also considering that the proposed method probably uses a lot more memory than the vanilla method, and it is also more complex to implement. In practice, I would rather train longer with a well-known vanilla method than using the proposed method.\n- In addition, there exist more efficient ways to learn representations nowadays (e.g. CLIP). There are many papers that use multiple views for CLIP as well. Why not testing them?\n- There are many other papers that treat this topic. They were not discussed. A few examples:\nThere are many other papers that treat this topic. They were not compared with. A few examples: \"On the Importance of Asymmetry for Siamese Representation Learning\", \"What Makes for Good Views for Contrastive Learning?\"."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission294/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699303874504,
            "cdate": 1699303874504,
            "tmdate": 1699635955349,
            "mdate": 1699635955349,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GmeAve5fIk",
                "forum": "ioBIT7gLBm",
                "replyto": "jesK5ZY9IU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission294/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission294/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**The title says \u201ccontrastive\u201d but most of the experiments use non-contrastive methods like simsiam and dino. This is confusing.**\n\nThank you for pointing this out. We agree with you that the title is indeed misleading and may suggest that HVS is applicable to only contrastive learning. We changed the title to \u201cHard View Selection for Self-Supervised Learning\u201d. Do you agree that title is better suitable?\n\n**In addition to the point above, it actually seems that the proposed method is better defined for non-contrastive losses. In fact, for contrastive losses (which contrast the attraction of the positives with the repulsion of the negatives) the method is not guaranteed to find the hardest possible set of positives and negatives. This would require exploring all the possible combinations of negatives, which is intractable. A better idea would be to use all the negatives from all the views at the denominator. In that case, since the denominator works as a max at low temperatures, you would be \u201cguaranteed\u201d to have the hardest negative. This is not described and discussed in the paper except one sentence at the end of 3.3.**\n\nWe agree with the reviewer that this point is described vaguely and requires more clarity in the paper. We describe this in greater detail in the Appendix under \u201cI.1 SimCLR\u201d. If we understood the reviewer right, yes, that is correct and we updated I.1 with the following description for more clarity:\n\n\u201c*For each iteration, we evaluate all possible view pairs and contrast each view against every other example in the mini-batch. Intuitively, the pair that yields the highest loss is selected, which is the pair that at the same time minimizes the numerator and maximizes the denominator in the above equation. In other words, the hardest pair is the one, that has the lowest similarity with another augmented view of itself and the lowest dissimilarity with all other examples.*\u201d\n\nDid this address your concern and did we understand you correctly? We hope we were able to give some clarification and are happy to discuss further suggestions.\n\n**In algorithm 1, the authors report that they first organize the views into pairs and then forward each pair. This entails forwarding every view multiple times (one view can be in more than one pair). This is confusing and does not make sense at all. It would be better to first forward the views separately and then compare them 2 by 2. Can the authors please explain why they first create the pairs and then forward?**\n\nThanks, you are right. In practice, we indeed only forward each view once to generate its embedding before we create the pairs over the embeddings. We fully agree with the reviewer that Algorithm 1 does not reflect this properly. For didactic reasons, we thought that it would be more understandable to a reader if the view pairs were first generated. However, after considering the reviewer\u2019s feedback, we agree it would be better to stay closer to the actual (more efficient) implementation and updated Algorithm 1 accordingly. Thank you for pointing this out.\n\n**Why not just make all comparisons and backprop all of them? Why is this not ablated?**\n\nWhile we agree with the reviewer that the result of this experiment would certainly be interesting and valuable, its execution would require significantly larger compute resources because instead of selecting one pair from many and only computing the gradients for this pair, one would now have to compute and hold all gradients for all pairs which would drive up the memory and speed costs."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission294/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699795727812,
                "cdate": 1699795727812,
                "tmdate": 1699795727812,
                "mdate": 1699795727812,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]