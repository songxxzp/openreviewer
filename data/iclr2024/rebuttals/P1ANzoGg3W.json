[
    {
        "title": "H2O-SDF: Two-phase Learning for 3D Indoor Reconstruction using Object Surface Fields"
    },
    {
        "review": {
            "id": "w1bRSTXEJa",
            "forum": "P1ANzoGg3W",
            "replyto": "P1ANzoGg3W",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4361/Reviewer_ZNfu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4361/Reviewer_ZNfu"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes follow-up method to the SDF-based NeRF-like methods for indoor reconstruction, with a focus to improve geometry on objects in the second phase of optimization. The main novelty is the introduction of the auxiliary representation of Object Surface Field (OSF), which is activated on object surfaces. OSF can learned with 2D supervision of instance segmentation, as well as a loss in 3D which jointly constrain the SDF field and OSF field, bringing about zeros of SDF around object surfaces, leading to improved reconstruction on detailed high-frequency object parts. The method is evaluated against baseline SDF-based NeRF-like methods, on scenes including ScanNet and Replica."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "[1] The proposal of using instance segmentation as additional input to the pipeline, as well as designing effective supervision signals with input segmentation. \n\nDespite it is not new to use additional signals to the task (e.g. monocular normal and depth supervision for 2D-3D consistency, sparse points to supervise local SDF values, and using semantics and planar assumptions improve geometry of layouts), the paper is one of the first to demonstrate the usage of instance segmentation to improve fine geometry. More importantly, the paper does so in a non-trivial way, by introducing OSF to explicitly evolve object surfaces using a 2D loss between the input segmentation and OSF as well as a 3D loss between OSF and SDF.\n\n[2] Illustration of the relationship of OSF and SDF (and the gradients), and the use of the OSF to drive SDF.\n\nThe paper provides informative illustration in Fig. 4 and related text on the relationship of OSF and SDF and how does the optimization of OSF loss drive SDF towards zero point around surfaces. The illustration using examples and 1D figures is clear and supports the motivation of the design of OSF.\n\n[3] Extensive evaluation of the proposed method against baseline methods, and on more than one datasets."
                },
                "weaknesses": {
                    "value": "[1] Clarification on OSF. Despite the good illustration of OSF as mentioned above, extra clarification is urgently needed to explain the motivation of the mathematical form of the 3D OSF loss (Equ. 2), and details in Fig. 4.\n\nSpecifically, despite Fig. 4 explains how the gradients of the 3 loss drives SDF to form a zero points around surfaces, and paper does not provide intuitive explanation on (a) why the various terms of the loss in Equ. 2 are designed as they are, (b) how \\gamma controls the steepness of the function, how it matters and how $\\gamma$ is picked (better with illustrations similar to Fig. 4). Additionally, it is not clear that, between Fig. 4 (a) and (c), why different d(x) lead to identical $\\sigma_\\gamma(x)$. Without clarifying the issues it is difficult to understand why OSF and the losses are designed the way they are, despite being proven effective.\n\n[2] Demonstration of applying the proposed OSF and losses general SDF-like NeRF-based methods. The proposed OSF and losses should theoretically be applicable to all of the baselines methods as simple drop-in, but somehow the paper decides to compare against its own vanilla baseline. Is it possible to apply to other existing methods to better showcase the general nature of the proposed method, and how effective will it be?\n\n[3] Limited scenes to evaluate. The main evaluation is done on ScanNet, with limited qualitative results on Replica. However ScanNet is known to have image quality issues. Why is the method not evaluated and compared on alternative datasets including Replica, Tanks & Temples, etc, as is done in other papers like MonoSDF? Without the additional evaluation, it is difficult to decide the generalization of the method across various indoor scenes.\n\n[4] Additional comparison. One less important thing to add is, potential comparison with I^2-SDF. Despite I^2-SDF is based on additional geometry supervision signals, the goal is aligned with the proposed method, and it showcases similar improvement in fine detailed objects. It would be beneficial to add comparison to I^2-SDF to inspire the discussion on optimal strategy to improve reconstruction of high frequency signals in indoor geometry."
                },
                "questions": {
                    "value": "Please see the Weakness section for questions to address."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4361/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698714989729,
            "cdate": 1698714989729,
            "tmdate": 1699636408113,
            "mdate": 1699636408113,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "srNTLntXnY",
                "forum": "P1ANzoGg3W",
                "replyto": "w1bRSTXEJa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4361/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4361/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZNfu (Part 1/2)"
                    },
                    "comment": {
                        "value": "We'd like to thank the reviewer for their time in making a detailed review with constructive feedback. We have numbered the weaknesses arisen as (Wx) and the questions arisen as (Qx) to ease the follow-up process where needed. \n\n(W1) *Clarification on OSF. Despite the good illustration of OSF as mentioned above, extra clarification is urgently needed to explain the motivation of the mathematical form of the 3D OSF loss (Equ. 2), and details in Fig. 4.\nSpecifically, despite Fig. 4 explains how the gradients of the 3 loss drives SDF to form a zero points around surfaces, and paper does not provide intuitive explanation on (a) why the various terms of the loss in Equ. 2 are designed as they are, (b) how \\gamma controls the steepness of the function, how it matters and how is picked (better with illustrations similar to Fig. 4). Additionally, it is not clear that, between Fig. 4 (a) and (c), why different d(x) lead to identical. Without clarifying the issues it is difficult to understand why OSF and the losses are designed the way they are, despite being proven effective.*\n\n(W_A1) Acknowledging the reviewer's feedback, we have prepared in the common response section a more detailed explanations of OSF and its corresponding loss functions.  \n\nAddressing the reviewer's specific questions, \n\n(a) Equation 2 incorporates a loss term with dual components: one targeting the room layout and another for objects with intricate details. The room layout component tends to converge quickly due to a \"smoothness bias\". The object component, rich in high-frequency details, evolves differently. The design rationale behind Equation 2 is elaborated in the subsequent paragraphs following its introduction. The object region's loss term initially appears as depicted in Figure 4 (a). However, as training progresses, our Object Surface Field (OSF) evolves into the form shown in Figure 4 (e). Section 3.2, 'Mutual Induction of OSF and SDF', mathematically expounds this transition, focusing on the influence of the gradients of 3d_osf_loss on both OSF and the Signed Distance Function (SDF).\n\n(b) The 3d_osf_loss in our framework is engineered to modulate the OSF from 0 to 1, correlating with the SDF's transition from positive to negative values. The parameter gamma governs the rate at which OSF's value alters. Through experimental evaluations with varying gamma values, we observed that a lower gamma, indicating a gentler slope, enables the OSF to learn a more expansive region on the object. Conversely, a higher gamma, resulting in a steeper slope, restricts the OSF to a narrower region, potentially undermining its effectiveness in guiding the SDF. Based on these experiments, we have optimally selected a gamma value of 1.\n\n(c) Figure 4 (a) illustrates the scenario in which a well-trained SDF directs the OSF to accurately learn the surface field, particularly in general cases such as bulky furniture. On the other hand, Figure 4 (c) shows a situation where the OSF is effectively trained, possibly with additional inputs like an object mask, but the SDF has not yet accurately represented thinner structures. These examples are intended to demonstrate the reciprocal influence of OSF and SDF in varied training contexts.\n\nAcknowledging the reviewer's feedback, we will provide extra clarification to better explain the motivation of the mathematical form of the 3D OSF loss and other related concepts in the final paper submission. \n\n(W2) \n*Demonstration of applying the proposed OSF and losses general SDF-like NeRF-based methods. The proposed OSF and losses should theoretically be applicable to all of the baselines methods as simple drop-in, but somehow the paper decides to compare against its own vanilla baseline. Is it possible to apply to other existing methods to better showcase the general nature of the proposed method, and how effective will it be?*\n\n(W_A2) The reviewer's observation is valid as our Object Surface Field (OSF) can be seamless integrated with NeuS-based methods. However, we opted to incorporate our OSF into our enhanced baseline model, particularly after the initial stage of Holistic Surface Learning. This design choice is a strategic improvement over traditional baseline models like NeuRIS. While NeuRIS implements a normal prior and selectively utilizes normals based on their reliability, our Holistic Surface Learning goes a step further. It dynamically adjusts the utilization of both color and normal priors, contingent on the confidence level of the normal information. Consequently, we applied our Object Surface Learning to the refined Signed Distance Function (SDF) emerging from Holistic Surface Learning, rather than directly integrating it into the basic baseline models. This methodological choice was aimed at leveraging the advanced features of our enhanced model for more effective learning and integration of the OSF."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4361/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700455787196,
                "cdate": 1700455787196,
                "tmdate": 1700617165624,
                "mdate": 1700617165624,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qr13yOjDv6",
            "forum": "P1ANzoGg3W",
            "replyto": "P1ANzoGg3W",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4361/Reviewer_rsDk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4361/Reviewer_rsDk"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a neural 3D indoor reconstruction framework to reconstruct 3D mesh of indoor scenes with a volume rendering framework. The key motivation of this paper is to decouple the learning of the layout and object with two stages. In the first stage, the layout of the scene is trained with an uncertainty-aware rendering loss function on both color and normal prediction. In the second stage, a new term named Object surface field (OSF) is introduced to measure the object occupancy of a 3D point, and authors demonstrate how SDF will facilitate SDF with the presented mutual induction. Extensive experiments on ScanNet have showcased the effectiveness of the proposed framework over different state-of-the-art (SOTA) methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(1) The motivation to decouple the learning of layout and object into two stage is straightforward and clear. The layout contains planar areas and the objects may have more high-frequency signals, thus may have different pace of convergence.\n\n(2) The introduction of OSF is novel, and how the OSF can be transformed back to SDF and assist its representation is technically sound.\n\n(3) Experiments on ScanNet have shown the advantages of proposed components of the method."
                },
                "weaknesses": {
                    "value": "(1) The major concern for me is that of the technical impact of this work is limited by introducing a normal estimation network [1] which is also trained on ScanNet, to provide pseudo groundtruth normal and uncertainty during training. This cannot ensure fairness among baseline comparison and highly constraints the generalizability of the proposed method onto different benchmarks. A fair setting would be replace this network with another model or method which is pretrained on other datasets, or alternatively, test this method onto other indoor datasets such as 7-Scenes. This will significantly improve the fairness and technical impact of this work.\n\n(2) In the supplementary material, authors present that they apply the OSF-based Filtering during reconstruction. I am curious about where does the major improvement of OSF comes from, either the proposed osf loss or the filtering. Authors are expected to conduct ablation study about this to make the contribution more convincing.\n\n(3) Minor: The presentation can be further improved, and there exists noticeable typos in the submission such as Table 2."
                },
                "questions": {
                    "value": "I appreciate the motivation of the design of this paper, however the use of a model seen on the same dataset limits the value of the proposed method. I would consider to improve my rating if my concerns listed in the weaknesses part can be well addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4361/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698816938506,
            "cdate": 1698816938506,
            "tmdate": 1699636408011,
            "mdate": 1699636408011,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IcIGNEkVqH",
                "forum": "P1ANzoGg3W",
                "replyto": "qr13yOjDv6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4361/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4361/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We'd like to thank the reviewer for their time in making a detailed review with constructive feedback. We have numbered the weaknesses arisen as (Wx) and the questions arisen as (Qx) to ease the follow-up process where needed. \n\n(W1) *The major concern for me is that of the technical impact of this work is limited by introducing a normal estimation network [1] which is also trained on ScanNet, to provide pseudo groundtruth normal and uncertainty during training. This cannot ensure fairness among baseline comparison and highly constraints the generalizability of the proposed method onto different benchmarks. A fair setting would be replace this network with another model or method which is pretrained on other datasets, or alternatively, test this method onto other indoor datasets such as 7-Scenes. This will significantly improve the fairness and technical impact of this work.*\n\n(W_A1) We value the reviewer's insightful feedback. The concern regarding the use of a normal estimation network trained on ScanNet and its implications for the fairness of balinese comparisons is indeed a valid point. We thoroughly considered this aspect prior to our submission but we still opted to proceed along the path of using ScanNet based pre-trained model, noting that numerous recent studies in indoor scene reconstruction using neural implicit representations have adopted similar approaches, leveraging various pretrained models as foundational models. For example, ManhattanSDF incorporates a segmentation model trained using ScanNet, and NeuRIS, akin to our approach, employs the same normal estimation model, also trained on ScanNet, but excludes test scenes from its dataset. Due to constraints in paper length, we were unable to include all experimental results at full extent that substituted our network with a different model or method trained on alternative datasets, or to extend testing of our method to other indoor datasets as suggested by the reviewer.\n\n**Table 1: Comparison results for Replica**\n\n| Model    | Accu.\u2193 | Comp.\u2193 | Prec.\u2191 | Recall\u2191 | F-score\u2191 |\n|----------|--------|--------|--------|---------|----------|\n| MonoSDF  | 0.020  | 0.017  | 0.951  | 0.923   | 0.934    |\n| H2O-SDF  | 0.016  | 0.025  | 0.983  | 0.935   | 0.957    |\n\n\n**Table 2: Comparison results for 7-Scenes**\n\n| Model         | Accu.\u2193 | Comp.\u2193 | Prec.\u2191 | Recall\u2191 | F-score\u2191 |\n|---------------|--------|--------|--------|---------|----------|\n| ManhattanSDF  | 0.112  | 0.132  | 0.351  | 0.326   | 0.336    |\n| NeuRIS        | 0.133  | 0.132  | 0.405  | 0.424   | 0.410    |\n| H2O-SDF       | 0.128  | 0.129  | 0.416  | 0.469   | 0.447    |\n\nAcknowledging the reviewer\u2019s valid concern, we are now presenting additional experiments using the Replica and 7-scenes datasets. In our first experiment, detailed in Table 1, we utilized an Omnidata-based surface normal estimation to test our model with a surface normal estimation model pretrained on a different dataset. This directly responds to the reviewer's suggestion of replacing our ScanNet-based network. In our second experiment, shown in Table 2, we applied our model to the 7-scenes dataset using ScanNet-based surface normal estimation, responding to the suggestion by the reviewer to test our method on other indoor datasets. \n\nThe results, as illustrated in Tables 1 and 2, \nhighlight our method's robust performance and its ability to generalize effectively, irrespective of the pre-trained model used for normal estimation or the specific domain of application. In the final version of our paper, we intend to include these additional experimental results either in the main body or in an appendix.\n\n(W2) *In the supplementary material, authors present that they .... more convincing.*\n\n(W_A2) In the supplementary material, the Object Mesh Extraction via OSF-based Filtering serves merely as an example to demonstrate the potential applications of our trained OSF. Note that this filtering process is not a core component of our proposed method. Hence, we do not anticipate a notable enhancement in performance during reconstruction through OSF-based filtering. To avoid any confusion for the reader, we will explicitly clarify this distinction in the final version of the paper.\n\n(W3) *Minor: .... Table 2.*\n\n(W_A3)\nWe will enhance the readability of our paper by correcting all typos, including those in Table 2.* and elsewhere in the submission.\n\n(Q1) *I appreciate the motivation of the design of this paper, however the use of a model seen on the same dataset limits the value of the proposed method. I would consider to improve my rating if my concerns listed in the weaknesses part can be well addressed.*\n\n(Q_A1) We hope that our response to Weakness 1 (W_A1) adequately addresses the concern raised by the reviewer. We would be grateful if the reviewer could reconsider and, if deemed appropriate, improve the rating based on these amendments."
                    },
                    "title": {
                        "value": "Response to Reviewer rsDK"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4361/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700362191115,
                "cdate": 1700362191115,
                "tmdate": 1700455785037,
                "mdate": 1700455785037,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0bgFJ3cWE7",
            "forum": "P1ANzoGg3W",
            "replyto": "P1ANzoGg3W",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4361/Reviewer_ypMg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4361/Reviewer_ypMg"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a two-phase learning approach named H2O-SDF that combines both holistic surface learning and object surface learning, for 3D reconstruction in indoor environments. \n\nThe main contributions are: 1) a two-phase learning framework that balances between the reconstruction of global room geometry and local object details. 2) Introduction of Object Surface Field (OSF), a new concept designed to address the vanishing gradient problem suffered by SDF, which hinders the reconstruction of high-frequency details. The authors also introduce an OSF-guided sampling strategy to prioritize object surfaces in the sampling process."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper tackle an important issue in the field of 3D indoor scene reconstruction \u2014 the difficulty of preserving the overall geometry while capturing intricate object details. It introduces a two-phase learning approach, which has not been explored before. \n2. The OSF concept is new and shows promising results in handling the inherent vanishing gradient issue in the learning process.\n3. It is an interesting idea to use normal uncertainty as a guidance to re-weight normal and color loss, to adaptively moderate normal and color losses in both low-texture and texture-rich regions.\n4. The submission appears to be well-organized with its ideas clearly articulated.\n5. Experimental evaluations, together with ablation studies, confirm the effectiveness of H2O-SDF. The results show that the proposed solution outperforms existing state-of-the-art methods."
                },
                "weaknesses": {
                    "value": "1. The explanation and exposition of some key, novel concepts, such as OSF, L2D_OSF, L3D_OSF, could be more thorough. There is insufficient mathematical detail on the OSF guided sampling strategy (although there is graphical illustration in the appendix A2, the explanation seems to be mostly a repetition of the main body). Strengths of the proposed formulation could be better appreciated by providing more detailed explanations and mathematical insights. \n\n2. Running time: The paper does not provide specific details about the computational complexity or running time of the approach, for both training and inference.  It only states that all experiments were conducted on a single NVIDIA RTX 3090Ti GPU.\n\n3. Comparison with more diverse data (this is more of a suggestion): While the paper compares favorably to state-of-the-art methods on the ScanNet dataset, it would strengthen the paper to include a broader range of data under different conditions, such as different indoor layout complexities, object variations etc."
                },
                "questions": {
                    "value": "1. It would be interesting to find out to what extent this method relies on pre-trained models and priors, which might limit its application in environments where such models are not easily available.\n\n2. Running time/computational complexity of the proposed method. Please refer to point 2 under Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4361/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4361/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4361/Reviewer_ypMg"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4361/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698826774823,
            "cdate": 1698826774823,
            "tmdate": 1699636407907,
            "mdate": 1699636407907,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "f8FneLYFnn",
                "forum": "P1ANzoGg3W",
                "replyto": "0bgFJ3cWE7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4361/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4361/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ypMg"
                    },
                    "comment": {
                        "value": "We'd like to thank the reviewer for their time in making a detailed review with constructive feedback. We have numbered the questions arisen as (Qx) to ease the follow-up process where needed. \n\n(Q1) *It would be interesting to find out to what extent this method relies on pre-trained models and priors, which might limit its application in environments where such models are not easily available.*\n\n(Q\\_A1)\nWe agree with the reviewer's insight that delving into the extent to which our method relies on pre-trained models and prior knowledge is indeed important. As our model draws cues for indoor scenes from readily available pre-trained models, we have confidence in its potential for broad generalizability across a range of generic and diverse indoor scenes. This assertion is further supported by our empirical findings, which include experiments conducted on additional datasets such as 7-scenes and Replica. We have elaborated on these results in the common comments section, and we cordially invite the reviewer to refer this section for a more comprehensive understanding. Furthermore, we intend to incorporate these findings into the final version of our paper for submission.\n\n(Q2) *Running time/computational complexity of the proposed method. Please refer to point 2 under Weaknesses.*\n\n(Q\\_A2)\n\nBelow, we present the average training and inference times obtained from our experiments on the 12 ScanNet scenes:\n\n|  Time                  | ManhattanSDF | NeuRIS | MonoSDF | H2O-SDF |\n|--------------------|--------------|--------|---------|---------|\n| Training (hours)    | 5.2     | 4.5     | 21.5    | 4.5     |\n| Inference (seconds) | 30      | 21      | 35        | 21       |\n\nWe plan to include these results in the Appendix of our final paper submission."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4361/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700455646277,
                "cdate": 1700455646277,
                "tmdate": 1700455646277,
                "mdate": 1700455646277,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mfpayFMCeT",
            "forum": "P1ANzoGg3W",
            "replyto": "P1ANzoGg3W",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4361/Reviewer_93pm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4361/Reviewer_93pm"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a two-phase framework (H2O-SDF) for 3D indoor scene reconstruction. In particular, the proposed method adopts a two-stage method, which consists of one-stage reconstruction for the scene layout followed by a second-stage reconstruction of the objects using NERF. The key contribution is to introduce the concept of the object surface field. The 2D and 3D object surface losses are introduced to estimate the SDF for fine object surface details. The experiments are conducted on ScanNet and show superior results compared with existing methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The method reconstruct the layout and the object separately and achieves very good reconstruction on the details of the objects.\n+ The introduced OSF captures the occupancy of the surface of the 3D object.\n+ The introduced two losses let the SDF captured more surface details."
                },
                "weaknesses": {
                    "value": "-\t2D object surface loss. Could it be explained as the loss between the rendered object masks and the ground truth masks? It would be great to make it clear that the proposed method actually requires object annotations.\n-\tIt would be great to explain OSF with more details. Based on the description in the paper, it is quite similar to the absolute gradient field of the occupancy values. In particular, Eq. 3 actually enforces the OSF to have large values on the object defined by the 3D points.  In addition, 3D points provide strong prior on the details of the shapes. It would be great to provide the ablations study of using the point cloud with MVS images or not.\n-\tExperiments on ablations studies. It is not clear to the reviewer what model A, B, C are. It would be great to provide detailed explanations about those models.\n-\tFor the second stage, it would be great to ablate whether all the losses have contributed to the final results. The proposed method adopts more accurate point cloud obtained from MVS images compared with monocular depth estimated from a single image. Those factors should be ablated to demonstrate the performance benefits from OSF and the sampling strategy not from the prior data"
                },
                "questions": {
                    "value": "- It would be great to elaborate more on the insight of OSF and also compared with existing density functions parameterised with the SDF values. \n- The ablations studies are missing. Please demonstrate all the losses introduced in stage two all contributes to the improvement of the reconstruction. In addition the proposed method leverages the point cloud obtained from MVS. It would be great to show how these priors can influence the final performance."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4361/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699629225851,
            "cdate": 1699629225851,
            "tmdate": 1699636407830,
            "mdate": 1699636407830,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "N9hpcACdt5",
                "forum": "P1ANzoGg3W",
                "replyto": "mfpayFMCeT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4361/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4361/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 93pm (Part 1/3)"
                    },
                    "comment": {
                        "value": "We'd like to thank the reviewer for their time in making a detailed review with constructive feedback. We have numbered the weaknesses arisen as (Wx) and the questions arisen as (Qx) to ease the follow-up process where needed. \n\n(W1) *2D object surface loss. Could it be explained as the loss between the rendered object masks and the ground truth masks? It would be great to make it clear that the proposed method actually requires object annotations.*\n\n(W_A1) \nThank you for highlighting the lack of clarity in our explanation regarding the non-requirement of object annotations for the 2D object mask. In Section 3.2, Line 6 of \"2D Object Surface Loss\", we mentioned, \"The 2D object mask is determined by a process which can be easily executed using a pre-trained model (Lambert et al., 2020)\". However, we realize this could lead to confusion about whether annotations are necessary during the training of the segmentation model. In our final paper submission, we will explicitly state in Section 4.1, \"Implementation Details\", that \"The 2D object mask is obtained using the MSeg segmentation method, employing its officially provided pre-trained model\".\n\nThe concept of 2D object surface loss is essentially the disparity between the volume-rendered result of the object surface field and the 2D object label (pseudo-ground truth), which is derived from a pre-trained segmentation model, MSeg. This approach enables us to acquire pseudo-ground truth labels without using object annotations during the training process that relies on this loss.\n\n(W2) *It would be great to explain OSF with more details. Based on the description in the paper, it is quite similar to the absolute gradient field of the occupancy values. In particular, Eq. 3 actually enforces the OSF to have large values on the object defined by the 3D points. In addition, 3D points provide strong prior on the details of the shapes. It would be great to provide the ablations study of using the point cloud with MVS images or not.*\n\n(W_A2)\nPlease refer to the common response section a full detailed explanations of OSF and its corresponding loss functions. \n\nQuoting what is already mentioned in the common response section:\n\n***\"OSF essentially quantifies the likelihood (ranging from 0 to 1) of the object surface OSF(x) at each spatial point, enhancing the SDF\u2019s ability to better capture fine geometric details and high-frequency surfaces, while ensuring a smooth room layout\"***\n\nIt implies that there are clear differences between OSF and absolute gradient field. That is, the Object Surface Field is bounded between 0 and 1, indicating the probability of an object's surface presence, whereas the absolute gradient field's values can range from 0 to infinity. Furthermore, our Object Surface Field is not merely the gradient field of an imperfectly trained Signed Distance Function (SDF) using color and normal information. Instead, it serves as an independent representation, enhancing the SDF by indicating regions it has yet to encapsulate, thereby acting as a crucial 3D geometric cue.\n\nMoreover, incorporating refine loss (ref_loss), which leverages the point cloud obtained from Multi-View Stereo (MVS), improves the model's capability in capturing intricate shapes. However, it is essential to emphasize that ref_loss plays a complementary role. Our model demonstrates state-of-the-art performance independently of the ref_loss component. The below experimental results support this claim.\n\n| Methods                   | Acc\u2193   | Comp\u2193  | Prec\u2191  | Recall\u2191 | F-score\u2191 |\n|---------------------------|--------|--------|--------|---------|----------|\n| NeuS                      | 0.179  | 0.208  | 0.313  | 0.275   | 0.291    |\n| ManhattanSDF              | 0.053  | 0.056  | 0.715  | 0.664   | 0.688    |\n| NeuRIS                    | 0.052  | 0.050  | 0.713  | 0.677   | 0.690    |\n| MonoSDF                   | 0.035  | 0.048  | 0.799  | 0.681   | 0.733    |\n| HelixSurf                 | 0.038  | 0.044  | 0.786  | 0.727   | 0.755    |\n|---------------------------|--------|--------|--------|---------|----------|\n| H2O-SDF w/o ref_loss   | 0.034  | 0.038  | 0.824  | 0.757   | 0.789    |\n| H2O-SDF                   | 0.032  | 0.0373 | 0.834  | 0.769   | 0.799    |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4361/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700455297819,
                "cdate": 1700455297819,
                "tmdate": 1700615438281,
                "mdate": 1700615438281,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sCQQwc1e61",
                "forum": "P1ANzoGg3W",
                "replyto": "mfpayFMCeT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4361/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4361/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 93pm (Part 2/3)"
                    },
                    "comment": {
                        "value": "(W3) *Experiments on ablations studies. It is not clear to the reviewer what model A, B, C are. It would be great to provide detailed explanations about those models.*\n\n(W_A3) We admit that we employed some unclear notations. In the final paper submission, we plan to employ more enhanced notations in the ablation study part. That is, here is a revised summary:\n\nIn our ablation study, we compare various model configurations: Model A, Model B, Model C, and H2O-SDF. Model A, our baseline, is an adaptation of NeuS with the integration of normal loss. Model B represents a scenario where the model undergoes training solely through our first stage, Holistic Surface Learning (NeuS + Normal + re-weighting scheme), right up to the final epoch, but does not include the Object Surface Learning phase. Model C and H2O-SDF, on the other hand, adopt a two-phase learning process. In this approach, Object Surface Learning (OSL) is initiated after the completion of Holistic Surface Learning (HSL). Specifically, Model C explores the impact of conducting OSL without employing the OSF-guided sampling strategy (OGS).\n\n(W4) *For the second stage, it would be great to ablate whether all the losses have contributed to the final results. The proposed method adopts more accurate point cloud obtained from MVS images compared with monocular depth estimated from a single image. Those factors should be ablated to demonstrate the performance benefits from OSF and the sampling strategy not from the prior data*\n\n(W_A4) We report our ablation study results for each loss used during Object Surface Learning (i.e. 2nd stage of our method). The notations used in the table are as follows. \n\n- HSL: Holistic Surface Learning, i.e., color and normal reweighting scheme. \n- 2D: 2D object surface loss.\n- 3D: 3D object surface loss.\n- ref: refinement loss.\n- OGS: Object Surface Field guided sampling strategy.\n\n| Model                                 | Acc\u2193  | Comp\u2193 | Prec\u2191 | Recall\u2191 | F-score\u2191 |\n|---------------------------------------|-------|-------|-------|---------|----------|\n| Neus + HSF                            | 0.041 | 0.041 | 0.802 | 0.751   | 0.776    |\n| Neus + HSF + 2D                       | 0.040 | 0.041 | 0.807 | 0.755   | 0.779    |\n| Neus + HSF + 2D + 3D                  | 0.040 | 0.041 | 0.809 | 0.760   | 0.784    |\n| Neus + HSF + 2D + 3D + ref            | 0.039 | 0.041 | 0.817 | 0.762   | 0.787    |\n| Neus + HSF + 2D + 3D + ref + OGS      | 0.037 | 0.039 | 0.830 | 0.773   | 0.800    |\n\nConsidering the page limit constraint, we will include this table in the Appendix of our final paper submission to clearly demonstrate the effectiveness of each loss."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4361/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700455433032,
                "cdate": 1700455433032,
                "tmdate": 1700456099425,
                "mdate": 1700456099425,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]