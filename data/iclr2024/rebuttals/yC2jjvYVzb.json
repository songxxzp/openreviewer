[
    {
        "title": "Confidence-Based Model Selection: When to Take Shortcuts in Spurious Settings"
    },
    {
        "review": {
            "id": "6uGlOXESJ2",
            "forum": "yC2jjvYVzb",
            "replyto": "yC2jjvYVzb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3999/Reviewer_niM7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3999/Reviewer_niM7"
            ],
            "content": {
                "summary": {
                    "value": "This paper concentrates on introducing a training method known as COnfidence-baSed MOdel Selection (COSMOS). The paper presents the COSMOS framework, which utilizes model confidence to adaptively select among models with varying strengths for distinct subpopulations. COSMOS does not necessitate target labels or group annotations, making it suitable for situations where obtaining such information is challenging. Nevertheless, this approach lacks adequate experimental analysis and falls short in the interpretability of its algorithm design."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper presents a novel framework, COSMOS, which tackles the issue of distributional shift by selectively using suitable classifiers based on model confidence. COSMOS adaptively chooses models depending on their appropriateness for various inputs, taking into account both shortcut and invariant classifiers. The proposed approach does not depend on target labels or group annotations, making it applicable in situations where such information is inaccessible or challenging to obtain.\n\n- This paper has a clear and rational motivation that advocates for treating shortcut and invariant classifiers equally, with both being experts in different regions of the input space.\n\n- This paper provides a comprehensive formal definition of the problem, including the problem setting and formal intuition.\n\n- To a certain degree, the algorithm in this paper demonstrates experimental results that the model can maintain satisfactory performance for majority groups while enhancing the performance of minority groups."
                },
                "weaknesses": {
                    "value": "- The COSMOS framework assumes that test data is provided in a batch format, with multiple inputs available at once for model selection. However, in real-world situations, particularly in medical diagnosis where subpopulation shifts are common, test data may be received in a streaming manner, processing one sample at a time.\n\n- The algorithm's design lacks interpretability. The analysis did not take into account the relationship between the algorithm's design and the use of shortcut and invariant features, nor did it explain why different classifiers can use various combinations of these two features instead of relying on the same shortcut features.\n\n- COSMOS' performance depends on the abilities of the base classifiers. If the base classifiers are similar, COSMOS may not offer significant improvements.\n\n- Another drawback related to numerous base classifiers is the need to train multiple base classifiers, each with potentially different architectures or training backbones. This increases the complexity and computational cost of the training process, as each base classifier must be trained and calibrated individually. Managing and optimizing multiple training pipelines can be difficult, particularly when working with large-scale datasets or complex models. In comparison to many existing methods that only require one base encoder (e.g., see [1, 2] and benchmarking methods in [3]), COSMOS displays increased training complexity.\n\n- As the paper focuses exclusively on spurious correlations as a type of subpopulation shift, it neglects the wider variety of subpopulation shift types found in the literature. According to [3], subpopulation shifts can take many forms, such as attribute imbalance and class imbalance. Real-world datasets often exhibit multiple types of shifts at the same time, and the paper does not discuss how COSMOS would perform in these situations. As a result, the paper's limited scope undermines its generalizability and applicability to real-world datasets that may display different types of subpopulation shifts.\n\n- The paper does not offer a comprehensive comparison with current state-of-the-art methods, making it challenging to evaluate COSMOS' relative performance and advantages compared to other techniques.\n\n- Although the paper proposes considering metrics beyond worst-group accuracy (WGA), it only evaluates regret and does not acknowledge the tradeoffs between other essential metrics and their interactions. Recent research on subpopulation shifts [3, 4] has shown that metrics such as calibration error (ECE) or worst-case precision may conflict with WGA. As a result, it is crucial to carefully consider the limitations and potential trade-offs of alternative metrics when assessing the performance of the proposed COSMOS framework. How does COSMOS perform on those metrics?\n\n- The ablation experiment is insufficient. The authors did not examine whether this advantage is due to the presence of TS. Moreover, if random selection or other selection methods are used among K classifiers, it is unclear whether the results will differ. It remains uncertain whether the advantage of the results is due to the integration of multiple classifiers.\n\n\n\n_[1] Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. ICLR 2020._\n\n_[2] On feature learning in the presence of spurious correlations. NeurIPS 2022._\n\n_[3] Change is Hard: A Closer Look at Subpopulation Shift. ICML 2023._\n\n_[4] Simple data balancing achieves competitive worstgroup-accuracy. 2022._"
                },
                "questions": {
                    "value": "Please refer to the Weaknesses. In addition to the points raised above, I have the another question it seems the k should be k^i = \\frac{D^i_T}{N}, since i denotes multiple target test sets."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3999/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698131859635,
            "cdate": 1698131859635,
            "tmdate": 1699636361903,
            "mdate": 1699636361903,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "s4PGXL05z7",
                "forum": "yC2jjvYVzb",
                "replyto": "6uGlOXESJ2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3999/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3999/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your review. We address your comments below. \n\n> The COSMOS framework assumes that test data is provided in a batch format, with multiple inputs available at once for model selection. However, in real-world situations, particularly in medical diagnosis where subpopulation shifts are common, test data may be received in a streaming manner, processing one sample at a time.\n\nIf the test data arrives in a streaming fashion, we can still apply COSMOS to each individual point that arrives, and we have a method, COSMOS (input-dep), that does this. Although we do not get the benefit of clustering, this method still outperforms the individual base classifiers.\n\nWe note that assuming unlabeled test data in batch form is a realistic and powerful source of information, and is a common assumption in settings with distribution shifts, such as OOD generalization [1,2,3], test-time adaptation [4,5], and OOD detection [6,7]. There are a number of realistic settings where it is easy to collect unlabeled data from the target distribution whenever a machine learning model is deployed, for almost no additional cost [7,8], or to retrieve relevant unlabeled data from a large dataset based on similarity [9].\n\n[1] Zhang, Linfeng, et al. \"Be your own teacher: Improve the performance of convolutional neural networks via self distillation.\" ICCV 2019\n\n[2] Xie, Qizhe, et al. \"Self-training with noisy student improves imagenet classification.\" CVPR 2020\n\n[3] Sohn, Kihyuk, et al. \"Fixmatch: Simplifying semi-supervised learning with consistency and confidence.\" NeurIPS 2020\n\n[4] Wang, Dequan, et al. \"Tent: Fully test-time adaptation by entropy minimization.\" ICLR 2021\n\n[5] Zhang, Marvin, Sergey Levine, and Chelsea Finn. \"Memo: Test time robustness via adaptation and augmentation.\" NeurIPS 2022\n\n[6] Katz-Samuels, Julian, et al. \"Training ood detectors in their natural habitats.\" ICML 2022\n\n[7] Tifrea, Alexandru, Eric Stavarache, and Fanny Yang. \"Semi-supervised novelty detection using ensembles with regularized disagreement.\" Uncertainty in Artificial Intelligence. PMLR, 2022.\n\n[8] Sagawa, Shiori, et al. \"Extending the WILDS benchmark for unsupervised adaptation.\" ICLR 2022\n\n[9] Schuhmann, Christoph, et al. \"Laion-5b: An open large-scale dataset for training next generation image-text models.\" NeurIPS 2022\n\n\n> The algorithm's design lacks interpretability. The analysis did not take into account the relationship between the algorithm's design and the use of shortcut and invariant features, nor did it explain why different classifiers can use various combinations of these two features instead of relying on the same shortcut features.\n\nThe intuition behind why our method utilizes both shortcut and invariant features is described in Section 4.1. In summary, the inference problem can be formulated as smoothed entropy minimization, which motivates our use of confidence in choosing which classifier to use. Shortcut and invariant features naturally have different confidences on different inputs as a result of how they are trained and this is also reflected in how they have different accuracies on different groups of inputs (as shown in Table 1). Thus, by leveraging these different confidences, our method is able to utilize different combinations of both shortcut and invariant features.\n\n\n> COSMOS' performance depends on the abilities of the base classifiers. If the base classifiers are similar, COSMOS may not offer significant improvements.\n\nCOSMOS assumes access to a diverse set of base classifiers, and the framework benefits from diversity in the prediction strategies of the base models. Such diverse models are often available in the distribution shift settings we consider. In the extreme worst case of all models being equal, COSMOS does as well as the single model.\n\n> Another drawback related to numerous base classifiers is the need to train multiple base classifiers, each with potentially different architectures or training backbones. This increases the complexity and computational cost of the training process, as each base classifier must be trained and calibrated individually...\n\nWe do use different base classifiers, which may require additional compute over training a single model, but we find that this is worth it for the additional improvement in performance that we get from maintaining high majority and minority group accuracies. Furthermore, other methods like Ensembling do also train and use multiple models and we find in our experiments that we are able to leverage the models more effectively for better performance."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3999/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700207312746,
                "cdate": 1700207312746,
                "tmdate": 1700207312746,
                "mdate": 1700207312746,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FSZc1TIyKS",
                "forum": "yC2jjvYVzb",
                "replyto": "ibiMTmBgxm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3999/Reviewer_niM7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3999/Reviewer_niM7"
                ],
                "content": {
                    "title": {
                        "value": "Metric"
                    },
                    "comment": {
                        "value": "Thank you for your rebuttal, the calibration metric ECE please refer to [A].\n\n[A] Guo, Chuan, et al. \"On calibration of modern neural networks.\" International conference on machine learning. PMLR, 2017."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3999/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700485793052,
                "cdate": 1700485793052,
                "tmdate": 1700485793052,
                "mdate": 1700485793052,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eKX5vKJhKd",
            "forum": "yC2jjvYVzb",
            "replyto": "yC2jjvYVzb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3999/Reviewer_RGsk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3999/Reviewer_RGsk"
            ],
            "content": {
                "summary": {
                    "value": "The paper discusses the challenges and solutions in machine learning related to feature learning, model robustness, and calibration. It highlights the importance of identifying shortcut features, which are often ignored in favor of robust predictors. The authors propose a technique called COnfidence-baSed MOdel Selection (COSMOS) that uses model confidence to guide model selection without the need for target labels or group annotations. They show that COSMOS outperforms other methods on datasets with distributional shift. Additionally, the paper introduces a fewshot recalibration approach to improve model calibration for specific data slices, demonstrating its effectiveness in various downstream tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The paper introduces a unique approach (COSMOS) for model selection based on model confidence, which does not rely on target labels or group annotations, addressing a common challenge in machine learning.\n2) The paper demonstrates that COSMOS performs better than other model aggregation methods on datasets with distributional shift, achieving lower regret across subpopulations.\n3) The approach is general to be applied to a wide range of models."
                },
                "weaknesses": {
                    "value": "1) There exists gap between the formal intuition and practical approach. Some assumptions are strict to stand in practice. The rationality of theories and the gap between theories and methods need to be addressed. Otherwise, we have no way of knowing the scope of the method.\n2) More methods, as well as some SOTA , should be considered in experiment."
                },
                "questions": {
                    "value": "nan"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "nan"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3999/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3999/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3999/Reviewer_RGsk"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3999/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698749714017,
            "cdate": 1698749714017,
            "tmdate": 1699636361817,
            "mdate": 1699636361817,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WJo9Eze55d",
                "forum": "yC2jjvYVzb",
                "replyto": "eKX5vKJhKd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3999/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3999/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your review. We'd like to ask for some clarification on the weaknesses you indicated.\n\n>There exists gap between the formal intuition and practical approach. Some assumptions are strict to stand in practice. The rationality of theories and the gap between theories and methods need to be addressed. Otherwise, we have no way of knowing the scope of the method.\n\nWe apologize for any potential confusion here. Can you explain what you find to be the gap between our theory and method so that we can clarify this? \n\n> More methods, as well as some SOTA, should be considered in experiment.\n\nWe already compare to state-of-the-art classifiers and methods that use multiple classifiers (e.g. ensembling variants). If you have any recommendations for other specific methods that would be informative to compare against, we would be happy to consider including them.\n\nWe kindly ask you to expand on your concerns if they remain, and to reevaluate your score if they are resolved."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3999/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700207572682,
                "cdate": 1700207572682,
                "tmdate": 1700207572682,
                "mdate": 1700207572682,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UsGw3XaiSq",
            "forum": "yC2jjvYVzb",
            "replyto": "yC2jjvYVzb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3999/Reviewer_4daw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3999/Reviewer_4daw"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of confidence-based model selection in an effort to address the issue of distribution shifts in the testing phase by equally considering invariant and shortcut features. Given multiple base classifiers trained on the source dataset, the COSMOS algorithm is proposed that (1) first clusters test examples in K clusters, and (2) then uses a confidence score to select one out of the base classifiers to perform classification of the examples for each cluster. The performance of the proposed algorithm is evaluated on 4 datasets  and compared with methods that use only invariant features, only shortcut features and ensemble methods."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "+ The problem of addressing distribution shifts in the testing dataset is addressed when spurious correlations are present.\n+ The idea of using different models for different inputs is very neat.\n+ The proposed algorithm is simple and very intuitive and has a nice formal intuition.\n+ The performance of the proposed algorithm is validated using 4 datasets, illustrating its superior performance compared to existing works.\n+ The proposed algorithm improves classification performance in real-world scenarios that are prevalent with distribution shifts and spurious correlations.\n+ The paper is in general well-written and makes it easy for the reader to understand both the problem statement and the solution."
                },
                "weaknesses": {
                    "value": "- I believe that the solution presented in the paper relates also to the problem of dynamic or instance-wise classifier selection, where the goal is to select the best classifier to use during testing for each test example. The related work section does not seem to include any relevant work in this area. Some example references follow:\n\n (1) R. M. Cruz, R. Sabourin, and G. D. Cavalcanti, Dynamic classifier selection: Recent advances and perspectives, Information Fusion, vol. 41, pp. 195\u2013216, 2018.\n\n(2) M. Sellmann and T. Shah, Cost-sensitive hierarchical clustering for dynamic classifier selection, arXiv preprint arXiv:2012.09608, 2020.\n\n(3) R. M. O. Cruz, L. G. Hafemann, R. Sabourin, and G. D. C. Cavalcanti, Deslib: A dynamic ensemble selection library in python, Journal of Machine Learning Research, vol. 21, no. 8, pp. 1\u20135, 2020.\n\n(4) S. P. Ekanayake, D. Zois and C. Chelmis, Sequential Datum-Wise Joint Feature Selection and Classification in the Presence of External Classifier, IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Rhodes Island, Greece, 2023, pp. 1-5, doi: 10.1109/ICASSP49357.2023.10097057."
                },
                "questions": {
                    "value": "(1) It would be great if the authors discuss how the proposed method differs from the problem of dynamic or instance-wise classifier selection, and depending on the relevance, they will consider extending their related work section accordingly.\n\n(2) I am a little bit confused about the relationship between the index variable of the subpopulation and the label. Initially, I thought that the test set could be split into subpopulations based on the possible values of the labels. However, as I continued reading, it seems that subpopulations are not necessarily constructed based on the possible values of the labels. In this case, what is the meaning of subgroups and how do you justify this?\n\n(3) Can you explain what is the meaning of the invariance assumptions in Sec. 3?\n\nMinor:\n(a) I believe there is a small typo in notation. Namely, shouldn't p_{T_i} be p_{T^i} in Sec. 3 or am I confused?\n(b) In pg. 5, dist(.) should be properly defined as a divergence measure.\n(c) The statistics of the datasets (e.g., number of instances, features, etc) are not reported."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3999/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698774749871,
            "cdate": 1698774749871,
            "tmdate": 1699636361746,
            "mdate": 1699636361746,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HLowz81ScU",
                "forum": "yC2jjvYVzb",
                "replyto": "UsGw3XaiSq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3999/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3999/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your thoughtful review and appreciate your acknowledgment of many strengths of our work. If you have any remaining questions, please let us know.\n\n> It would be great if the authors discuss how the proposed method differs from the problem of dynamic or instance-wise classifier selection, and depending on the relevance, they will consider extending their related work section accordingly.\n\nThank you for bringing this up and the links to references. We have added these and some discussion to the related work section accordingly. Our method is an example of dynamic selection (DS) from multiple classifiers; we propose a distinct selection criteria, combining both clustering and confidence strategies in order to enhance performance on subpopulation shifts.\n\n> I am a little bit confused about the relationship between the index variable of the subpopulation and the label. Initially, I thought that the test set could be split into subpopulations based on the possible values of the labels. However, as I continued reading, it seems that subpopulations are not necessarily constructed based on the possible values of the labels. In this case, what is the meaning of subgroups and how do you justify this?\n\nThanks for bringing this point up, and we apologize for any confusion. In subpopulation shifts that occur as a result of spurious correlations, we follow prior literature, e.g. Group DRO, JTT, etc., and consider different combinations of (spurious attribute, label) as a group. For example, for the Waterbirds dataset, where background type spuriously correlates with the label of bird type, we consider 4 groups total, 2 of them majority: (land background, landbird), (water background, waterbird), and 2 of them minority (water background, landbird), (land background, waterbird). \n\n> Can you explain what is the meaning of the invariance assumptions in Sec. 3?\n\nWe apologize for any confusion here. The invariance assumptions simply define the scope of the distribution shifts that we consider. We are specifically interested in subpopulation shifts where the main difference between the source distribution and target distributions is the proportion of subpopulations and other aspects between the distributions remain the same. \n\n> Minor: (a) I believe there is a small typo in notation. Namely, shouldn't p_{T_i} be p_{T^i} in Sec. 3 or am I confused? (b) In pg. 5, dist(.) should be properly defined as a divergence measure. (c) The statistics of the datasets (e.g., number of instances, features, etc) are not reported.\n\nThank you for your careful reading of our paper. We have fixed these points in the revised pdf."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3999/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700207794036,
                "cdate": 1700207794036,
                "tmdate": 1700207794036,
                "mdate": 1700207794036,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "g3CvzExwCC",
                "forum": "yC2jjvYVzb",
                "replyto": "HLowz81ScU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3999/Reviewer_4daw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3999/Reviewer_4daw"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for clarifying my concerns. Now that I have gained a better understanding of the invariance assumption, I am thinking that the proposed approach must related to research focusing on imbalanced datasets since you are changing the proportion for subpopulations. In that sense, I think it is wise to discuss how the proposed approach differs from prior work in the area."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3999/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588980473,
                "cdate": 1700588980473,
                "tmdate": 1700588980473,
                "mdate": 1700588980473,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]