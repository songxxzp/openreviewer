[
    {
        "title": "Efficient Certification of Physics-Informed Neural Networks"
    },
    {
        "review": {
            "id": "diEYGxvQXa",
            "forum": "JSlTXa6WE6",
            "replyto": "JSlTXa6WE6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5770/Reviewer_5KvE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5770/Reviewer_5KvE"
            ],
            "content": {
                "summary": {
                    "value": "Propose certification for PINN, i.e., guarantees on the worst-case residual error of a PINN, which can be used for prediction of PINN's performance for trustworthy PINN."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well-motivated as the theoretical guarantee is always a huge concern for PINN whose theory is not transparent, making it less practical than traditional finite element methods.\n\nThe theory is clearly proved together with supporting experiments."
                },
                "weaknesses": {
                    "value": "The authors only provide theory for 1st and 2nd order derivates in PDEs. However, high-order PDEs are common, such as KdV, Kuramoto-Sivashinsky, and Boussinesq-Burger PDEs.\n\nI don't think the extension to general high-order PDEs is simple, although I noticed the author thought it was trivial, because the expression of high-order derivative is extremely complex, which may affect the tightness of the certification. How does the PDE order affect the tightness of your certification?\n\nIs this framework still valid when PINN encounters propagation failure? See https://arxiv.org/abs/2203.07404 Figure 2. The losses are very small but the error is huge.\n\nTo be more specific, there exists a case where PINN converges to a low residual loss but with a huge relative test l2 error, due to the propagation failure, or we can regard it as PINN converges to some spurious local minima corresponding to the trivial solution. In that case, the PINN model will be smooth and low loss. How can your theory deal with this case?\n\nBesides, although the author shows by a figure that the residual loss correlates with the test l2 relative error well, I would like to emphasize that residual loss only provides a loose upper bound for the test l2 relative error. In other words, low residual loss cannot guarantee low test error.\n\nThus, I suggest the author test their theoretical framework in some tough cases where PINN encounters propagation failure."
                },
                "questions": {
                    "value": "Can this theoretical framework be extended to high-order nonlinear PDEs? High-order PDEs are common, such as KdV, Kuramoto-Sivashinsky, and Boussinesq-Burger PDEs. How does PDE order affect the tightness of your certification?\n\nIs this framework still valid when PINN encounters propagation failure? See https://arxiv.org/abs/2203.07404 Figure 2. The losses are very small but the error is huge."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5770/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698409032145,
            "cdate": 1698409032145,
            "tmdate": 1699636606187,
            "mdate": 1699636606187,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kYIUHrnK2C",
                "forum": "JSlTXa6WE6",
                "replyto": "diEYGxvQXa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5770/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5770/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response to Reviewer 5KvE"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the time spent on the paper as well as for their comments.\n\n**(W1) On extending $\\partial$-CROWN to certify higher-order PINNs.** We partially agree with the reviewer in that extending $\\partial$-CROWN to higher-order derivatives following the more efficient, hybrid scheme that we introduced for the first and second derivatives could be laborious as the derivations of higher-order derivatives become more complicated as a result of the chain rule. However, it is theoretically possible to achieve it by following a similar process. \n\nFollowing the reasoning from Raissi et al., (2019), we can view higher-order derivatives as a depth-wise extension of the original network (as portrayed also in Figure 1 of our paper). As widely observed by the network verification community, deeper networks are more difficult to verify as bound propagation tends to become looser in incomplete verifiers (Gowal et al., 2018; Wang et al., 2018; Shi et al., 2020; Xu et al., 2020a). We mitigate this problem by introducing Greedy Input Branching to obtain tight bounds on the PINNs used in a more efficient way than other verification methods (see Appendix C). For higher-order PINNs it is likely that one will need (i) tighter relaxations of the nonlinearities of the networks, and (ii) more efficient branching methods that allow us to compensate for the tightness loss in deeper networks.\n\nWe will modify the last paragraph of Section 5.1 to clarify these points.\n\n**(W2) On low residual errors and high solution errors.** We thank the authors for the reference to Wang et al. (2022). Regarding the point on Figure 2 the reviewer makes, we would point out that while the residual loss values are of the order of $10^{-2}$, using the causal method the authors of the paper are able to achieve residual loss values below $10^{-5}$ (Figure 4 of the paper), which is what is typically expected in PINN training. This supports the observations we make in Section 6.2. And note also that these values are taken at fixed points instead of taking the maximum across the full domain, which might be significantly higher than at the optimized points.\n\nWe partially agree with the reviewer, in that a low maximal residual error does not guarantee a low maximal $\\ell_2$ error due to the nonlinearity of the PDEs. We only claim that certified bounds on the boundary, initial conditions and residual error can be interpreted as tolerances in numerical solvers for the PDE \u2013 this is explicitly mentioned in Section 6.2.\n\nThe reviewer suggests that we \u201ctest [our] theoretical framework in some tough cases where PINN encounters propagation failure.\u201d In practice this is what we do in the case of Allen-Cahn\u2019s and the Diffusion-Sorption equations, where the residual errors are so large that we are effectively identifying these PINNs have not been trained properly. We analyze this result in more detail in Appendix C.\n\nWe hope these answers are useful to the reviewer in reconsidering the score attributed, and look forward to continuing the discussion.\n\nAdditional references:\n- Wang, Sifan, Shyam Sankaran, and Paris Perdikaris. \"Respecting causality is all you need for training physics-informed neural networks.\" arXiv preprint arXiv:2203.07404 (2022)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5770/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699795302198,
                "cdate": 1699795302198,
                "tmdate": 1699795302198,
                "mdate": 1699795302198,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "JAJT3ZodmG",
            "forum": "JSlTXa6WE6",
            "replyto": "JSlTXa6WE6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5770/Reviewer_2eyr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5770/Reviewer_2eyr"
            ],
            "content": {
                "summary": {
                    "value": "The paper provides a definition of global correctness conditions for PINNs that produces the solutions of PINNs. The authors propose CROWN based certification framework to bound first and second derivatives $\\mu_\\theta$ as well as the $f_\\theta$. \n\nWhile CROWN provides a theoretical bound, given the approximations used throughout the bounding process, such bound is still less tight to bound the actual $h$. The authors utilize a similar idea to BaB, and apply greedy input branching algorithm to the bounding process.\n\nThe authors have provided experiments to verify that (1) $\\partial$- CROWN are more tight compared to empirical errors computed with a large number of samples (2) residual-based certificates and the commonly reported solution errors are highly correlated. They have also provided an ablation study to check the importance of the branching algorithm."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) The paper provides a good contribution to the research community. The authors are building the correctness conditions for PINNs over the input domain, which can improve the robustness and trustworthiness of PINN when applying it to the scientific domains.\n\n(2) While CROWN is not new, the authors make novel contributions to apply CROWN to solving PINN's correctness conditions. \n\n(3) In Section 5, the authors have provided detailed theoretical foundations for how to bound the correctness conditions. The proofs in appendix are long but easy to read.  The authors have also provided a theoretical analysis of the running time for $\\partial$-CROWM algorithm.\n\n(4) The authors have provided an improved version with greedy input branching, similar to the idea of original branch-and-bound (BaB) process."
                },
                "weaknesses": {
                    "value": "(1) The major concern that reviewer has is that the certification procedure is the on PINN residual error, $f$, but not on the estimation to the true PDE solution. While the authors have provided Exp 2 and Appendix C to demonstrate that there is a strong relationship between the residual error and the PDE solution, it is only conducted on PDEs with a clear PDE solution. More experiments should be conducted with various PDE equations (with multiple solutions, with trivial solutions, etc.) to find out how bounding $f$ is effective. Especially for the recent papers that discuss the failure modes of PINN[1], how would bounding the residual errors help, both theoretically and empirically?\n\n(2) Figure 2 looks meaningless. The Figure 2(a)-2(d) only shows the visualization of the time evolution of $u$, versus the residual\nerrors for three different PDE equations. However, it is hard to observe any clear patterns of the residual errors, and the authors only briefly mention the residual errors without explaining any scientific insights from the Figure. The reviewer thinks it is better to put the Figure into Appendix, and instead briefly mention Appendix A and B in the main text.\n\n(3) Table 1 shows that  $\\partial$-CROWN approaches the empirical bounds obtained using Monte Carlo sampling while providing the guarantee that no point within the domain breaks those bounds. The reviewer think it will be nice to document some time it takes for MC sampling versus the $\\partial$-CROWN (at least for residual error), to demonstrate its efficiency.\n\n(4) Figure 1 is occupying too much spaces while the words on the upper right corner are hard to read. Maybe the authors should consider providing a more concise illustration that occupies less space. \n\n(5) In Section 6.3, the authors should consider also including the ablation study on $N_b$ into the main text. \n\n(6) In Section 5.3, the authors mention that \"exploring the areas... via sampling (SAMPLE, line 3)\", is there a typo and should be line 5 instead of line 3? \n\n\n[1]: Krishnapriyan, Aditi S., Amir Gholami, Shandian Zhe, Robert M. Kirby, and Michael W. Mahoney. 2021. \"Characterizing possible failure modes in physics-informed neural networks.\" arXiv preprint arXiv:2109.01050."
                },
                "questions": {
                    "value": "(1) In the appendix A, the authors have mentioned to use Physics-informed Adversarial Training to reduce the errors of PINN and demonstrate that $\\partial$- CROWN can also bound the residual errors of PIAT. However, there are been various ways of reduce the errors of PINN, for example, scheduled training [1] or building a more complex architecture with transformer [2]. The reviewer is wondering if the $\\partial$-CROWN can adapt to NN training that involves more complex loss functions, or more complex architectures?\n\n(2) Similar to Question 1 in Weakness, how could $\\partial$-CROWN be used in solving the failures of PINN? If the residual error is large (or being stagnant for a long time), does that imply the PINN is failing to provide a good solution?\n\n(3) Have the authors study the relation between $\\partial$-CROWN's time complexity to the size of the PINN's neural network?\n\n(4) How does $\\partial$-CROWn behave when the amount of training data is low? Intuitively, the reviewer believes that the amount of training data needs to be evenly spreading across the domain to allow greedy input branching algorithm to work. But what happens when the amount of training data is not evenly spreading?\n\n \n[1]: Krishnapriyan, Aditi S., Amir Gholami, Shandian Zhe, Robert M. Kirby, and Michael W. Mahoney. 2021. \"Characterizing possible failure modes in physics-informed neural networks.\" arXiv preprint arXiv:2109.01050.\n\n[2]: Zhao, Zhiyuan, Xueying Ding, and B. Aditya Prakash. 2023. \"PINNsFormer: A Transformer-Based Framework For Physics-Informed Neural Networks.\" arXiv preprint arXiv:2307.11833."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5770/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5770/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5770/Reviewer_2eyr"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5770/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698681652904,
            "cdate": 1698681652904,
            "tmdate": 1699636606078,
            "mdate": 1699636606078,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YNyvuAenfg",
                "forum": "JSlTXa6WE6",
                "replyto": "JAJT3ZodmG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5770/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5770/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response to Reviewer 2eyr"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the time spent on the paper as well as for their comments.\n\n**(W1) On the relation of residual and solution error.** This is an important point. We believe the analysis in Section 6.2 and Appendix C indicates a strong correlation between the two types of errors, but there is more to be done in this direction in future work. \n\n**(W2) On Figure 2.** The main purpose of Figure 2 is to give an intuition to readers regarding the landscape of the solution and residual error of each of the PDEs studied. However, we agree with the reviewer that the figure would likely be better suited for the appendix, and will move it there in the final version of the paper.\n\n**(W3) On running time of MC sampling and $\\partial$-CROWN.** In general, Monte Carlo sampling is much faster than $\\partial$-CROWN for the number of samples we have considered in Table 1 (e.g., inference on $10^6$ samples takes approximately $0.21s$ for Burgers\u2019 equation), scaling linearly with the number of samples for higher values. However, as discussed in the paper, MC sampling only provides a lower bound for the value of the error, whereas $\\partial$-CROWN provides a certified upper bound. We will add the runtime information to Table 1.\n\n**(W4) On Figure 1.** We thank the reviewer for the feedback and will consider a few ways of making the figure more compact.\n\n**(W5) Including N_b ablation in the main text.** Given the plan to move Figure 2 to the Appendix as per (W2) of this rebuttal, we will move the ablation on N_b to the main text in the final version of the manuscript.\n\n**(W6) Typo in Algorithm 1.** We thank the reviewer for pointing out that typo which will be corrected in the final version of the paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5770/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699795095458,
                "cdate": 1699795095458,
                "tmdate": 1699795095458,
                "mdate": 1699795095458,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xlSzjllSyB",
                "forum": "JSlTXa6WE6",
                "replyto": "GbP2e2L6BO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5770/Reviewer_2eyr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5770/Reviewer_2eyr"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Authors"
                    },
                    "comment": {
                        "value": "Thank you for the detailed explanations. However, I am not entirely sure that the authors have answered (W1) On the relation of residual and solution error. It seems that Section 6.2 and Appendix C indicates a strong correlation between the two types of errors, but the paper lacks a theoretical justification to how the two errors are correlated. \n\nMore work should be done towards learning the failure of PINNs and how PINNs could avoid trivial solutions and failure modes in solving the PDEs. The method only provides the post-hoc manner insight, which is less valuable. \n\nTherefore, I would keep my current scores."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5770/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700604367545,
                "cdate": 1700604367545,
                "tmdate": 1700604367545,
                "mdate": 1700604367545,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZYo5v4B09h",
            "forum": "JSlTXa6WE6",
            "replyto": "JSlTXa6WE6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5770/Reviewer_uoUZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5770/Reviewer_uoUZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper tries to establish correctness certification for PINN models via the proposed $\\partial-$CROWN post-training framework. Under several assumptions, the authors prove two theorems showing that the upper and lower bound of the solution can be computed in $\\mathcal O(L)$ time. The authors experimentally demonstrate the tightness of their bound via a few small PDE examples, showing that they can achieve excellent approximation (compared with MC samples) much more efficiently."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Undoubtedly, bounding the error of ML solutions *globally*, as the authors try to tackle, is important and necessary for ML methods to be practically applicable. I am happy to see theoretical and empirical research on this avenue.\n- The improved Greedy Input Branching intuitively makes sense to me, although its direction comparison with the non-optimized algorithm seems to be lacking.\n- This paper is well-written, and the organization and flow are clear. I also pretty much appreciate the combination of theoretical proof and empirical demonstration."
                },
                "weaknesses": {
                    "value": "While I like this paper, there are several non-negligible problems that have to be addressed before the paper reaches the bar of acceptance.\n\n- First of all, I am not sure why the authors naturally assume that the ground truth can be represented by a FNN. For instance, even a single electron wave function decays exponentially, which an FNN cannot represent. Since the goal here is to bound the global error, I do not think the gap between the best FNN versus the actual ground truth can be ignored. Similarly, assumption 1 limits the applicability of your theorems - what happens if I add some exponential layers, or what if I use other architecture?\n- Your theorems only prove the existence of bounds, while your algorithm is still greedy. If I understand correctly, this actually means that: the approximation error (say the numbers in Table 1) is not *guaranteed* to be an upper bound of the global error, right? If so, despite the acceleration you achieve, how can you demonstrate that in a large system, your error approximation is *accurate*?\n- I do think the paper should include more complex tasks, as PINN is known to be less accurate in large systems (or long simulation time intervals). Current experiments, while sufficient as a proof of concept, are insufficient to demonstrate the proposed algorithm can indeed provide a better approximation. For instance, you can try multi-body Schrodinger equations, like [1], or HJB equations [2]. These are of large scales and non-linear, making it challenging (IMO) to your method.\n\n[1] Forward Laplacian: A New Computational Framework for Neural Network-based Variational Monte Carlo\n\n[2]  Is $L^2$ Physics-Informed Loss Always Suitable for Training Physics-Informed Neural Network?"
                },
                "questions": {
                    "value": "- From my understanding, speed is one of your strengths. Can you more comprehensively compare with previous methods to demonstrate how fast your method is? A table or figure will be ideal.\n- I think the paper could be benefited by including more takeaways, especially for the theorems. I can sort of imagine how this theorem is proved, but it would be great if the authors could give a proof sketch and discuss how each assumption and condition works.\n- notation layer $k$: try to use $l$ since the total layer number is $L$."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5770/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5770/Reviewer_uoUZ",
                        "ICLR.cc/2024/Conference/Submission5770/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5770/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698726439758,
            "cdate": 1698726439758,
            "tmdate": 1700623265412,
            "mdate": 1700623265412,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1oJjRVdQ7t",
                "forum": "JSlTXa6WE6",
                "replyto": "ZYo5v4B09h",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5770/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5770/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response to Reviewer uoUZ"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the time spent on the paper as well as for their comments.\n\n**(W1) On the assumptions and applicability of $\\partial$-CROWN.** Regarding the reviewer\u2019s first point on the representation capacities of feedforward networks, we note that these networks are, in theory, universal approximators in the infinite width limit, so they should be able to represent arbitrary solution functions given an appropriate choice of activation. However, we agree with the reviewer in that it might not be an optimal architecture choice for some PINNs (which is likely the reason behind at least some training failures). Despite this, it is widely used in the previous literature, and hence why we chose to focus on these types of networks.\n\nNote that some previous works providing guarantees for PINNs such as Shin et al., (2020), Wang et al., (2022b), or Mishra and Molinaro (2022) only provide them for specific families of PINNs, whereas our work is more general since it can be applied to all PINNs represented by FNNs.\n\n**(W2) On the greedy nature of our method.** Our method provides a ***guaranteed upper bound*** on the global errors over a domain \u2013 so the upper bounds provided in Table 1 are guaranteed bounds. We introduce greedy input branching in Section 5.3. to improve the tightness of the bounds obtained using $\\partial$-CROWN in the full PDE domain by splitting it into subdomains. Since the union of all the subdomains corresponds to the full PDE domain, taking the worst-case bound of each of the subdomains gives us a bound on the error in the full domain. As such, the bounds obtained via greedy input branching are **guaranteed** to be globally satisfied over the domain - there is no \"approximation\" that breaks the bound. More details on the procedure of greedy input branching is available in Appendix H.\n\n**(W3) On further experiments.** The PINNs in the experiments section were chosen primarily because they are well established in previous literature in the field, from the classic Burgers\u2019 and Schr\u00f6dinger\u2019s equation in Raissi et al. (2019a) to more recent and difficult ones to learn in the Allen-Cahn\u2019s equation from Monaco and Apiletti (2023) and the Diffusion-Sorption from Takamoto et al. (2022). The HJB equation from [2] is modeled as an MLP, so our framework could be applied to this problem, but as the reviewer mentions the scale of the network would be challenging for $\\partial$-CROWN as it is over $1,500\\times$ bigger than the networks considered. This drawback of our method is explicitly mentioned in the Section 7 of the paper, and we hope future work will help scale verification methods to those larger networks.\n\n**(Q1) On the efficiency of our method.** To the best of our knowledge, $\\partial$-CROWN is the first framework designed to bound the errors of general PINNs. However, for completeness of analysis, in Appendix B we extend Interval Bound Propagation (IBP) (Gowal et al., 2018; Mirman et al., 2018) \u2013 known for its simplicity and trading-off bound tightness for speed \u2013 to this setting. The results are presented in Table 3 of our paper (Appendix B). It presents the performance of IBP and $\\partial$-CROWN on the initial, boundary and residual errors for a fixed runtime limit in Burgers\u2019 equation. This is a fair comparison which takes into account the runtime/tightness trade-off of the two methods. We observe that $\\partial$-CROWN is significantly more efficient than IBP, achieving bounds that are $165 - 1,566\\times$ tighter than the baseline in the same total runtime.\n\n**(Q2) On takeaways for Theorem 1 and 2.** While the expressions for the linear lower and upper bounds are quite efficient to compute in practice, we found it difficult to include them in the main text without excessive notation. However, we agree that the dependencies of the linear functions are not explicit in the theorem, and that a proof sketch should be added in the following paragraph. We will make these modifications in the final version of the paper.\n\n**(Q3) On notation.** We thank the reviewer for pointing this out, and we will make the suggested adjustment in the final version of the paper.\n\nWe hope these answers are useful to the reviewer in reconsidering the score attributed, and look forward to continuing the discussion."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5770/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699794830767,
                "cdate": 1699794830767,
                "tmdate": 1699794830767,
                "mdate": 1699794830767,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bLEm6Prxcb",
                "forum": "JSlTXa6WE6",
                "replyto": "1oJjRVdQ7t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5770/Reviewer_uoUZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5770/Reviewer_uoUZ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply.\n\n- **(W1) On the assumptions and applicability of CROWN.** I don't think the universality expression explanation is convincing enough for not considering a wider range of network architectures. In fact, despite the universality, it's CNN (not FNN) that works well in images, and it's Transformer (not FNN) that works well in languages. I understand that most previous works consider even limited structures, but what's the challenge that prevents your current methods from being applied to other architectures? Is it because some of the mathematical techniques you use work only on linear (or piecewise linear) models? That's the reason why I want to learn more about the proof sketch. More explanation about why there is a limitation on the application range is important, and I encourage the authors to add this.\n\n- **(W2) On the greedy nature of our method.** Thanks. That makes sense.\n\nI am happy to increase my rating if the authors can address the (W1) issue, either by extending to a more general architecture, or by explicitly pointing out what limits your method."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5770/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700523258171,
                "cdate": 1700523258171,
                "tmdate": 1700523258171,
                "mdate": 1700523258171,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5S0P7vPKr4",
                "forum": "JSlTXa6WE6",
                "replyto": "8auBVeIKxX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5770/Reviewer_uoUZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5770/Reviewer_uoUZ"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for adding this to the paper. I will increase my rating to 6."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5770/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700623251258,
                "cdate": 1700623251258,
                "tmdate": 1700623251258,
                "mdate": 1700623251258,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LTSydZDE33",
            "forum": "JSlTXa6WE6",
            "replyto": "JSlTXa6WE6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5770/Reviewer_3Mdo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5770/Reviewer_3Mdo"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to solve the question of \u201cefficient certification\u201d of PINNs which \u2013 as the authors state \u2013 is the question of trying to quickly estimate bounds on the supremum (over domain points) error made by the neural surrogate for the residuals. The authors give experiments to suggest that their methods give upperbounds almost as good as the estimations made via random sampling of domain points."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The claims that have been proven in this paper do indeed seem a bit surprising \u2013 that upto second order of derivatives for the neural net, these functions can be linearly bounded bothways."
                },
                "weaknesses": {
                    "value": "Prima facie the paper is very badly written \u2013 to the point of being very much devoid of motivations. Till one gets to the experiments, it's entirely unclear what is the \u201ccertification\u201d question that is even being solved. In fact even after reading everything in the main paper, it's not clear at all as to why the random point sampling way is not enough to implement a check of the inequalities as required in Definition 1. What exactly is being gained by this method (as demonstrated in the third column of Table 1) as opposed to the results in the first two columns?  \n\nEven the run-time reported to obtain some of these numbers in the third column of Table 1 seems to be in millions of seconds. \nThat is more than a day! So this method isn't hinged on an argument of speed either.  \n\nSecondly, I am not able to ascertain from the given descriptions if the presence of these linear two-way bounds is a special case for the first two derivatives of a neural net or does this happen for all derivatives. Is the method $\\partial-$CROWN hinged on the validity of these linear bounds? In that case, why is the method of interest to the general PINN formalism where arbitrarily high orders of derivatives can potentially occur.  \n\nThirdly, the theorems are very badly stated. It's not clear from their statements as to whether or not there is an explicit expression for these linear bounds and if yes (which it seems to be from the appendix!) then what information is required to be able to compute them.  \n\nFourthly, the choice of the PDE examples for testing are very skewed. If one is doing experiments on multiple PDEs then it would have been better to include PDEs that have more than 2 variables. It does not help to have all the examples be stuck to two variable PDEs!  \n\nAlso, the Schrodinger PDE example looks a bit strange. Which natural Hamiltonian function even leads to this? This seems to be coming from a system with a potential energy which is \u201c$-|u|^2$\u201d - and that is never a natural setting \u2013 to the best of my knowledge of quantum theory!"
                },
                "questions": {
                    "value": "It would be great if the authors can give a precise answer to the first two weaknesses pointed out above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5770/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698782955930,
            "cdate": 1698782955930,
            "tmdate": 1699636605838,
            "mdate": 1699636605838,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GhJzSvgoEI",
                "forum": "JSlTXa6WE6",
                "replyto": "LTSydZDE33",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5770/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5770/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response to Reviewer 3Mdo"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the time spent on the paper as well as for their comments.\n\n**(W1) On the motivation of PINN certification and efficiency.** We stress in the introduction that the main goal of this paper is to provide a formal guarantee for *every* input within the feasible domain of the PINN. While most previous works report computed mean and/or max $\\ell_2$ solution error computed over a finite number of points, this cannot be relied on in practice as they might not sample domain points where the error is large. We showcase this in the Diffusion-Sorption example in Table 1, where the residual bound estimate using $10^4$ MC samples is $1.1\\times 10^{-3}$, while the certified upper bound is obtained at $21.34$. **Through sampling we are only able to obtain lower bound estimates on the error** which, as we can see from Table 1, can be quite distant from the actual error at specific domain points (visible on the same equation when we increase the number of samples from $10^4$ to $10^6$). This highlights the need for **certified upper bounds, which we provide with $\\partial$-CROWN**.\n\nAs far as we are aware, our framework is the first one that can be used to certify arbitrary PINNs with solutions modeled by fully connected neural networks. And while runtime is still a drawback of our framework, in Appendix B we perform an efficiency comparison with an adaptation of Interval Bound Propagation (IBP) (Gowal et al., 2018; Mirman et al., 2018) \u2013 known for its simplicity and trading-off bound tightness for speed \u2013 to this setting. We compare the performance of IBP and $\\partial$-CROWN on the initial, boundary and residual errors for a fixed runtime limit in Burgers\u2019 equation. This is a fair comparison which takes into account the runtime/tightness trade-off of the two methods. We observe that $\\partial$-CROWN is significantly more efficient than IBP, achieving bounds that are $165 - 1,566\\times$ tighter than the baseline in the same total runtime. \n\n**(W2) On the linear bounds and higher-order derivatives.** As we note at the beginning of page 6 of the manuscript, $\\partial$-CROWN can be extended to higher-order derivatives in a similar fashion to the process used for deriving the second-order bounds. As such, it is ***not a special case of second-order PINNs***, and it can be applied to higher-order ones too. The linear bounds are obtained by relaxing the nonlinearities using linear overapproximations, so they can be applied to any function defined over a continuous domain. \n\n**(W3) On theorem notation.** The bounds obtained in Theorem 1 and 2 can be computed from the weights of the solution network $u_\\theta$ (that is $W^{(k)}$) and the bounds provided by Assumption 1 ($\\mathbf{A}^{(k),L}$, $\\mathbf{a}^{(k),L}$, $\\mathbf{A}^{(k),U}$, $\\mathbf{a}^{(k),U}$, $y^{(k), L}$ and $y^{(k), U}$), using the recursive definition provided in Appendix E. While these expressions are quite efficient to compute in practice, we found it difficult to include them in the main text without excessive notation. However, we will change Theorem 1 and Theorem 2 to clarify the dependencies of the linear bounds.\n\n**(W4) On the choice of PINNs.** The PINNs in the experiments section were chosen primarily because they are well established in previous literature in the field, from the classic Burgers\u2019 and Schr\u00f6dinger\u2019s equation in Raissi et al. (2019a) to the more recent and difficult ones to learn in the Allen-Cahn\u2019s equation from Monaco and Apiletti (2023) and the Diffusion-Sorption from Takamoto et al. (2022). Another important factor in the choice was the availability of either the code or trained models from all of these previous works. While we considered other suitable higher dimensional PINNs, such as several of the Navier-Stokes equations from Jin et al., (2021), or the Gray-Scott system from Giampaolo et al., (2022), neither training code nor the pre-trained models were released that allow us to apply $\\partial$-CROWN.\n\nWe hope these answers are clear and useful to the reviewer in reconsidering the score attributed, and look forward to continuing the discussion.\n\nAdditional references:\n- Giampaolo, Fabio, et al. \"Physics-informed neural networks approach for 1D and 2D Gray-Scott systems.\" Advanced Modeling and Simulation in Engineering Sciences 9.1 (2022): 1-17.\n- Jin, Xiaowei, et al. \"NSFnets (Navier-Stokes flow nets): Physics-informed neural networks for the incompressible Navier-Stokes equations.\" Journal of Computational Physics 426 (2021): 109951."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5770/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699794612117,
                "cdate": 1699794612117,
                "tmdate": 1699794612117,
                "mdate": 1699794612117,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]