[
    {
        "title": "GATE: How to Keep Out Intrusive Neighbors"
    },
    {
        "review": {
            "id": "1grCOr0puH",
            "forum": "AP779Zy70y",
            "replyto": "AP779Zy70y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8254/Reviewer_J31R"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8254/Reviewer_J31R"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce GATE, an variant of Graph Attention Networks (GAT) which they claim alleviates the over-smoothing problem found in GNNs. The claim is that the proposed method can close the \"gate\" of neighborhood aggregation to irrelevant features. Experimental results on both synthetic and real world data are shown to back the claims."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The contributions of the paper are clearly mentioned. \n\nThe flow of the paper makes it easy to read."
                },
                "weaknesses": {
                    "value": "A pictorial representation of the GATE mechanism would really add to the description. If it is relatively straight forward to add, please consider it. It is not absolutely necessary but may help with readers' intuition."
                },
                "questions": {
                    "value": "(Table 1) Although GATE helps with the numbers for the case with more layers, typically one wouldn't use a lot of layers for datasets such as the toy one used here. And for the single layer case, GATE was not necessary to get a near 100 score. \n\n(Table 2) Was the experiment run just once? Instead of reporting the maximum accuracy at some epoch, it would be better to look at mean accuracy over a number of runs. The latter has more information."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8254/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8254/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8254/Reviewer_J31R"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8254/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698713603039,
            "cdate": 1698713603039,
            "tmdate": 1700329345987,
            "mdate": 1700329345987,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "49HCiMp3Qw",
                "forum": "AP779Zy70y",
                "replyto": "1grCOr0puH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8254/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8254/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer J31R"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback and insightful questions, which we answer below.\n\n- We agree with the reviewer's suggestion that a pictorial representation would help with a reader's intuition. Unfortunately, due to space constraints, it is not easy to add one to the paper. \n\n\n- As the reviewer has rightly pointed out, GATE was not necessary to get a near 100 score in the single-layer case. We highlight the importance of this observation in paragraph 6 on page 5. The key idea of the experiment is not to discover how many layers are needed for the toy case example, but rather to empirically demonstrate our theoretical insights in Section 4. Our reasoning for why GAT is unable to switch off neighborhood aggregation is related to norm constraints on the parameters imposed by an inherent conservation law in the network. For a single-layer network, the norms are not constrained and hence GAT is able to switch off neighborhood aggregation. In contrast, the 2-layer (or more) network is bound by norm constraints that prevent GAT from achieving the desired behavior. This reinforces our theoretical insight. In contrast, GATE can retain its performance for deeper models as well by switching off neighborhood aggregation in all layers.\n\n\n- All our experiments were run multiple times.  As the motivation behind designing a learning task via a synthetic dataset was to study how well GAT and GATE learn the task at hand, we decided to stick to analyzing one run of each model in detail. We compare the test accuracy of the model when it has achieved its maximum train accuracy (usually 100\\% except for cases marked with asterisk) to the maximum test accuracy achieved by the model (and when). This provides an insight into whether the model is simply overfitting to the train data or really learning the task. We report the mean accuracy in Table 8 in Appendix C.4. Due to space constraints, both tables can not be added to the main text. If the reviewers suggest that the detailed analysis be moved to the appendix, it can be replaced by the aggregate results across multiple runs in the main paper."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700290190121,
                "cdate": 1700290190121,
                "tmdate": 1700290190121,
                "mdate": 1700290190121,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "owxULJYhzi",
                "forum": "AP779Zy70y",
                "replyto": "49HCiMp3Qw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8254/Reviewer_J31R"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8254/Reviewer_J31R"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response."
                    },
                    "comment": {
                        "value": "I have no further questions. I have updated the score to 6."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700329332485,
                "cdate": 1700329332485,
                "tmdate": 1700329332485,
                "mdate": 1700329332485,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CYK5KsdLec",
            "forum": "AP779Zy70y",
            "replyto": "AP779Zy70y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8254/Reviewer_8362"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8254/Reviewer_8362"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents GATE, an extension of GAT tackling the over-smoothing issue. Speficially, the authors first show that GAT cannot effectively switching off the aggregation from irrelevant neighbors, and then modify the GAT architecture by assigning separate attention parameters for the node and the neighborhood. Experimental evaluations show the effectiveness of GATE, especially in heterophilic graphs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is generally well-written and easy to follow.\n- Theoretical analysis of the proposed method is provided.\n- Extensive experiments on both synthetic and real datasets are conducted to evaluate the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "- The theoretical analysis is restricted to the weight-sharing version. The gap between the weight-sharing version and the original version is unclear.\n\n- Only GAT is used as the baseline in the experiments. Although the authors stated that the main focus of this paper is to show the limitations of neighborhood aggregation in GATs, it would still be meaningful to show how well does the proposed method address the over-smoothing issue in GNNs.\n\n- Some of the figures are difficult to interpret. For example, the 20%ile - 90%ile lines in Figure 4 are diffucult to distinguish. Also, what does this figure tells?"
                },
                "questions": {
                    "value": "- Why did the authors report the min loss and max acc in Table 1?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8254/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8254/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8254/Reviewer_8362"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8254/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807857193,
            "cdate": 1698807857193,
            "tmdate": 1699637026484,
            "mdate": 1699637026484,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XBy6l8tIUx",
                "forum": "AP779Zy70y",
                "replyto": "CYK5KsdLec",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8254/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8254/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8362"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback and insightful questions, which we answer below.\n\n- While our derivations that lead to Insight 4.2 only consider the weight-sharing case, a similar principle also applies to GAT without weight-sharing. Experimental results in Table 1 demonstrate that GATs, both with and without weight sharing, are unable to switch off neighborhood aggregation, as they struggle to assign large magnitudes to the parameters $a$. Contrary to this, GATE, both with and without weight sharing, is able to switch off neighborhood aggregation. We have presented results on real-world datasets for both GATE and GATE$_S$ in Figure 9 and Table 6 (deferred to the appendix due to space limitation and observation of similar results) and also briefly discussed the trade-off between parameter sharing and no parameter sharing in light of training time and generalization accuracy.\n\n- We have now added a comparison of GATE with FAGCN and GraphSAGE on synthetic datasets in Appendix C.5 (Tables 9 and 10) and discuss their ability to switch off neighborhood aggregation. To the best of our knowledge, current techniques to alleviate over-smoothing in GNNs are based on regularization such as dropout and sampling, or architectural elements such as skip connections from a previous layer to the next (or from the input layer to every other layer (FAGCN), or from every layer to the last layer (GCNII)),  and concatenation of a node's own features with aggregated neighbors (GraphSAGE). These techniques are complementary and thus could be combined with GATE. Note that GATE also maintains a strong performance for deeper networks that would usually suffer from over-smoothing.\n\n- The lines from 20\\%ile - 90\\%ile can be distinguished by their order (bottom to top) for example in the plot for Layer 1 in the bottom right plot of Figure 4. When they are indistinguishable, for example in layers 3-4 of the same plot, it implies that they are (almost) equal (and hence do not need to be differentiated.) Section 5.2 discusses in detail the results in Figure 4 in the context of the effect of depth and how GATE offers interpretable neighborhood aggregation. However, upon the reviewer's identification of the difficulty of interpreting Figure 4 in isolation, we have added a summary of its insights from Section 5.2 to the caption of Figure 4.\n\n- Table 1 does not report the min. loss but rather the test accuracy at the epoch of min loss (most of these cases correspond to 100\\% train accuracy except the entries marked with an asterisk). Because the reported results are for synthetic data designed specifically to distinguish the behavior of GAT and GATE, we believe it is important to capture the full behavior of both architectures by ensuring that even though GAT can memorize the data to achieve 100\\% train accuracy, it fails to generalize, unlike GATE. The epoch information helps us to verify that despite being allowed to train for longer, GAT is unable to effectively learn the task, whereas GATE generalizes perfectly almost at the same time when it achieves 100\\% training accuracy. We have added a sentence that explains this reasoning in more detail."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700290488339,
                "cdate": 1700290488339,
                "tmdate": 1700290488339,
                "mdate": 1700290488339,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fGLWNqMxHA",
                "forum": "AP779Zy70y",
                "replyto": "XBy6l8tIUx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8254/Reviewer_8362"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8254/Reviewer_8362"
                ],
                "content": {
                    "title": {
                        "value": "Feedback to the authors"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThanks for your response. My questions are partially addressed. Given that there is sitll a gap in the theoretical analysis, I will keep my score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634407388,
                "cdate": 1700634407388,
                "tmdate": 1700634407388,
                "mdate": 1700634407388,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7ihQZ84egG",
                "forum": "AP779Zy70y",
                "replyto": "CYK5KsdLec",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8254/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8254/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Added theory for non-weight sharing case"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their response. According to their request, we have added the theoretical analysis of the non-weight-sharing case in Appendix A.1, which completes our theoretical investigation and leaves no gap.\nThe flow of arguments is identical to the weight-sharing case. Yet, the analysis involves the distinction of $6$ cases, which we now discuss all in detail. \n\nWe would be happy to answer any further questions upon request."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663848062,
                "cdate": 1700663848062,
                "tmdate": 1700665629648,
                "mdate": 1700665629648,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "J1em8iyfuv",
            "forum": "AP779Zy70y",
            "replyto": "AP779Zy70y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8254/Reviewer_nehk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8254/Reviewer_nehk"
            ],
            "content": {
                "summary": {
                    "value": "The paper discusses the limitations of Graph Attention Networks (GATs) in terms of their inability to switch off unnecessary neighbors. To address this limitation, the authors propose GATE, a GAT extension that alleviates over-smoothing, enjoys the expressiveness from deeper layers, and often outperforms GATs on real-world datasets. In addition, the paper evaluates and deeply analyzes the behavior of GATE by creating synthetic datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well-written and easy to follow. The proposed method is well-motivated and well-described. The GNN operator, which aggregates only relevant neighbors, is a very important component in learning graph representation. The implementation is simple but effective in real-world heterophilous benchmarks."
                },
                "weaknesses": {
                    "value": "- If I understand correctly, there is a logical flaw in theoretical results. Can the authors explain the theoretical result more (conservation of norms \u2192 small \u2018relative\u2019 contributions of attention)? Why is \u2018switching off neighborhood aggregation\u2019 related to [\u03b1ij/\u03b1ii << 1] rather than [\u03b1ij << 1]? Regardless of the self-loop, if there are large attention values in the other neighbors than j, wouldn't \u03b1ij be small? The separation of attention parameters for self-loops and neighbors clearly affects [\u03b1ij/\u03b1ii << 1] but may not for [\u03b1ij << 1].\n- Most experiments are conducted on synthetic datasets and experiments on other real-world benchmarks are required (Lim, Derek, et al. and Platonov, Oleg, et al.). Synthetic datasets are useful lenses to analyze the model behavior but they are over-simplied versions of real-world datasets. However, real-world datasets used in the paper are small-scale and known to have various pitfalls (See Platonov, Oleg, et al.)\n  - Lim, Derek, et al. \"Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods.\" Advances in Neural Information Processing Systems 34 (2021): 20887-20902.\n  - Platonov, Oleg, et al. \"A critical look at the evaluation of GNNs under heterophily: Are we really making progress?.\" The Eleventh International Conference on Learning Representations. 2023.\n- There are existing GNNs that use different aggregators on self and neighbor nodes. Although there is no attention in model names, it is believed that they are able to perform GATE's switching off neighbors. Here are examples below. There should be qualitative and quantitative comparisons with them.\n  - GraphSAGE: Hamilton, Will, Zhitao Ying, and Jure Leskovec. \"Inductive representation learning on large graphs.\" Advances in neural information processing systems 30 (2017).\n  - FAGCN: Bo, Deyu, et al. \"Beyond low-frequency information in graph convolutional networks.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 5. 2021.\n  - Residual Gated Graph ConvNets: Bresson, Xavier, and Thomas Laurent. \"Residual gated graph convnets.\" arXiv preprint arXiv:1711.07553 (2017).\n- There are some missing references related to this paper\u2019s motivation and methods:\n  - Wang, Guangtao, et al. \"Improving graph attention networks with large margin-based constraints.\" arXiv preprint arXiv:1910.11945 (2019).\n  - Kim, Dongkwan, and Alice Oh. \"How to Find Your Friendly Neighborhood: Graph Attention Design with Self-Supervision.\" International Conference on Learning Representations. 2021.\n  - Fountoulakis, Kimon, et al. \"Graph attention retrospective.\" Journal of Machine Learning Research 24.246 (2023): 1-52.\n  - Lee, Soo Yong, et al. \"Towards Deep Attention in Graph Neural Networks: Problems and Remedies.\" ICML (2023)."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8254/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698840709174,
            "cdate": 1698840709174,
            "tmdate": 1699637026364,
            "mdate": 1699637026364,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AKJ1qT6Wu3",
                "forum": "AP779Zy70y",
                "replyto": "J1em8iyfuv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8254/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8254/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nehk"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback and insightful questions, which we answer below.\n- We would be happy to provide further explanations of the theoretical result. \n    - Firstly, note that $\\alpha_{ij} << 1$ would not necessarily be sufficient to make the contribution of neighbor $j$ insignificant. For instance, nodes with a high degree $\\alpha_{ij} << 1$ could hold for all $j$ and the node $i$ so that each neighbor, as well as node $i$, would still similarly impact the features of $i$. To make $j$ insignificant relative to the contribution of the node's contribution, we actually need $\\alpha_{ij}/\\alpha_{ii} < < 1$. Also note that the analysis of relative $\\alpha_{ij}/\\alpha_{ii}$ is simpler, as the normalization constant cancels out. This fact elucidates further why we need to study $\\alpha_{ij}/\\alpha_{ii}$ and not $\\alpha_{ij}$: even if the attention parameter in another neighbor was large, this would affect the normalization constant and thus potentially also weight down $\\alpha_{ii}$. Furthermore, to switch off all neighborhood aggregation, we require $\\alpha_{ij}/\\alpha_{ii} < < 1$ for all neighbors $j$. \n     - Next, we provide an intuition of how the limited expressiveness of attention results from the derived conservation law. Theorem 4.1 induces the corollary that $\\left\\lVert\\mathbf{W}^{l}\\right\\rVert^2 = \\left\\lVert\\mathbf{W}^{l+1}\\right\\rVert^2 + \\left\\lVert\\mathbf{a}^{l}\\right\\rVert^2 + c$ during gradient flow.  We show that to make the contribution of all neighbors $j$ insignificant, we actually need $\\left\\lVert\\mathbf{a}^{l}\\right\\rVert^2$ to be large.  The above conservation law implies that $\\left\\lVert\\mathbf{W}^{l}\\right\\rVert^2$ would also need to be large and $\\left\\lVert\\mathbf{W}^{l+1}\\right\\rVert^2$ relatively small. However, due to the structure of gradients in Equation (6) on page 4, this would require large `relative' gradients $\\Delta \\mathbf{W}^{l+1} = \\nabla \\mathbf{W}^{l+1}/ \\mathbf{W}^{l}$ of parameters in $\\mathbf{W}^{l+1}$ and small relative gradients of parameters in $\\mathbf{W}^{l}$, for which the model would need to enter a less trainable regime and thus could not converge to a meaningful model.\n- We present additional results on the larger and more recent five datasets introduced in the paper `A critical look at the evaluation of GNNs under heterophily' proposed by the reviewer in Table 6 in Appendix C.1, as well as two large-scale OGB datasets: Arxiv and Product. For most of these datasets, we observe a substantial improvement of GATE over GAT.\n- We would like to emphasize that our aim is to provide insights into the attention mechanism of GATs and understand its limitations. While it should be able to flexibly assign importance to neighbors and the node itself without the need for concatenated representation or explicit skip connections of the raw features to every layer, it is currently unable to do so in practice. In order to verify our identification of trainability issues, we modify the GAT architecture to enable the trainability of attention parameters which control the trade-off between node features and structural information. Other architectures could potentially switch off neighborhood aggregation, as we show next. However, they are less flexible in assigning different importance to neighbors, suffer from over-smoothing, or come at the cost of an increased parameter count by increasing the size of the hidden dimensions (e.g. via a concatenation operation). Concretely, we have newly added a comparison of GATE with FAGCN and GraphSAGE on synthetic datasets in Appendix C.5 (Tables 9 and 10) and discuss their ability to switch off neighborhood aggregation as follows.\n     - **GraphSAGE**  uses the concatenation operation to combine the node's own representation with the aggregated neighborhood representation. Therefore, it is usually (but not always) able to switch off the neighborhood aggregation for the synthetic datasets designed for the self-sufficient learning task. Mostly, GATE performs better on the neighbor-dependent task, in particular for deeper models, where the performance of GraphSAGE drops likely due to over-smoothing. \n     - **FAGCN** requires a slightly more detailed analysis. Hence, due to space limitations, we make the complete discussion in Appendix C.5 on page 21. The key point is that FAGCN, as we observe, is unable to switch off neighborhood aggregation, particularly for deeper models. \n- The first two papers discuss training attention in GNNs with additional supervision. We have discussed the paper \"How to Find Your Friendly Neighborhoods\" as superGAT in Figure 10 in Appendix C.5 but have missed the citation. The last two papers suggested by the reviewer are important recent works that highlight the interest of the GNN community in understanding and addressing the limitations of graph attention. We thank the reviewer for pointing us towards further relevant literature and have added them all to our related work section."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700292107451,
                "cdate": 1700292107451,
                "tmdate": 1700292107451,
                "mdate": 1700292107451,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rHlb8nQDjr",
                "forum": "AP779Zy70y",
                "replyto": "AKJ1qT6Wu3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8254/Reviewer_nehk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8254/Reviewer_nehk"
                ],
                "content": {
                    "comment": {
                        "value": "Dear authors. Thank you for your detailed response.\n\n- I have acknowledged (1) additional results on the larger and more recent five datasets (2) newly added a comparison of GATE with FAGCN and GraphSAGE (3) added relevant literature all to related work. They resolved my concerns related to each bullet.\n- I am still reading the first bullet, and will continue the discussion about it."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700554448097,
                "cdate": 1700554448097,
                "tmdate": 1700554448097,
                "mdate": 1700554448097,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2rSRddYpZh",
                "forum": "AP779Zy70y",
                "replyto": "J1em8iyfuv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8254/Reviewer_nehk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8254/Reviewer_nehk"
                ],
                "content": {
                    "comment": {
                        "value": "> Firstly, note that $\\alpha_{ij} << 1$ would not necessarily be sufficient to make the contribution of neighbor j insignificant. For instance, nodes with a high degree $\\alpha_{ij} << 1$ could hold for all j and the node i so that each neighbor, as well as node i, would still similarly impact the features of i.\n\nI do not think it is important for an insignificant node to have a similar impact with other neighbors. What really matters is how much an insignificant node contributes to the overall neighborhoods (including self-loops). Using an example of a high-degree node i, even if insignificant j impacts similar to other neighbors, its contribution will be very small since $\\alpha_{ij} << 1$. That is, the insignificant node j will have little effect on the features of node i.\n\n> Also note that the analysis of relative $\\alpha_{ij}/\\alpha_{ii}$ is simpler, as the normalization constant cancels out.\n\nNevertheless, now I understand that using relative contribution $\\alpha_{ij}/\\alpha_{ii}$ makes an analytical interpretation simple. \n\n> even if the attention parameter in another neighbor was large, this would affect the normalization constant and thus potentially also weight down $\\alpha_{ii}$. Furthermore, to switch off all neighborhood aggregation, we require $\\alpha_{ij}/\\alpha_{ii} < < 1$ for all neighbors j.\n\nA small $\\alpha_{ii}$ can be a problem when a large portion of neighbors is insignificant. In this case, we might need to switch off all neighborhood aggregation. However, if so, we should ask a basic but important question: why do we use a neighborhood aggregation rather than just MLP on the node itself? I recommend the authors discuss this question and compare the results of MLP with GATE."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700572574278,
                "cdate": 1700572574278,
                "tmdate": 1700572602089,
                "mdate": 1700572602089,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "s2AYnKJSQK",
            "forum": "AP779Zy70y",
            "replyto": "AP779Zy70y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8254/Reviewer_NiY6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8254/Reviewer_NiY6"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an extension of graph attention network called GATE to make the attention mechanism in the original GAT more powerful. The authors stand on the assumption that attention mechanisms in GAT should be able to assign heterophilic nodes low weights while homophilous nodes high weights. Through experiments and analyses, the authors discover that original GAT fails to achieve that and accordingly propose GATE, which theoretical can switching off redundant neighbors. Through experiments on datasets where raw node features contains sufficient information to infer class labels, the authors show that GATE can successfully turn off information passed from other neighbors."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The research problem is interesting and the proposed method is simple yet effective. \n\n2. The proposed GATE has sufficient theoretical guarantee."
                },
                "weaknesses": {
                    "value": "1. I am a little bit worrying about the motivation of this work. In the introduction, the authors claim that an effective attention mechanism should be able to block messages passed from heterophilic neighbors and constitute the remainder of this paper following this claim. However, the scenario described is only one demonstration of a successful attention mechanism. I believe that as long as the attention mechanism can project node of the same class into the similar regions on the decision manifold , it is a good attention mechanism. I think examples from this paper could be good showcases of my point [1]. \n\n2. The experiments are conducted over synthetic use cases or graphs designed for heterophily. I am wondering how's GATE's performance over benchmarks with public splits (e.g., Cora, Citeseer, and Pubmed with fixed 20 nodes per class, and arxiv and product with ogb splits).\n\n3. I would recommend the authors to add a section at the end of section 3 to quickly summarize the key merits discussed in section 4 in case some readers don't want to read too many theoretical materials. I think in order to understand how GATE works requires a full understanding of section 4, which could be improved. \n\n[1] Yao Ma, et al., Is Homophily a Necessity for Graph Neural Networks? ICLR'22"
                },
                "questions": {
                    "value": "Please refer to the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8254/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699338640564,
            "cdate": 1699338640564,
            "tmdate": 1699637026251,
            "mdate": 1699637026251,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LqxYgC6tKG",
                "forum": "AP779Zy70y",
                "replyto": "s2AYnKJSQK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8254/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8254/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NiY6"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback and insightful comments, which we address below.\n\n- The suggested paper provides an insightful discussion on 'good' and 'bad' heterophily that we have included in our literature discussion.  In similar terminology, as the reviewer has correctly pointed out, the synthetic datasets for the self-sufficient task in Section 5.1 exemplify one use case of 'bad' heterophily.  However, they were designed to verify our identification of the cause behind the limited expressivity of the attention mechanism in GATs. Our main contribution is the theoretical insight into the reasoning behind the inability of GATs to switch off neighborhood aggregation, which is rooted in norm constraints imposed by the inherent conservation law.  To verify the correctness of this insight, we have designed GATE to address the identified root cause of the problem. Similar reasoning can also be applied to nodes in the neighborhood itself (by comparing $\\alpha_{uv}$ and $\\alpha_{u'v}$ for $\\{u',v\\}\\in \\mathbb{N}(v)$ instead of comparing $\\alpha_{uv}$ and $\\alpha_{vv}$.  We would like to reiterate that our claim is that an effective attention mechanism should be able to adapt neighborhood aggregation according to the task at hand, and the presence of 'good' and 'bad' heterophily are also task-dependent.  As we show in this work, the intended capacity of the present attention mechanism to express the importance of neighboring nodes (considering a node to be a neighbor of itself by self-loops generally introduced in GATs) is impeded by trainability issues that we have verified by alleviating the trainability issue of attention parameters for one particular neighbor (the node itself). Therefore, although GATE does not eliminate the problem completely, the identification of the problem in this work serves as a stepping stone that will allow us to address the problem more generally and enable more effective attention mechanisms for GNNs in the future. \n\n- Results on Cora and Citeseer can be found in Table 3 on page 9 with fixed 20 nodes per class. In addition, we present results in Table 6 in Appendix C.1 on more datasets: OGB-arxiv, OGB-product, and larger more recent heterophilic datasets. GATE achieves a substantial improvement over GAT for most of these larger datasets, notably, for OGB-Arxiv and OGB-Product.\n\n- We thank the reviewer for their suggestion and have incorporated their feedback in the updated document.  Unfortunately, due to space limitations, we could only include a very brief two-line overview. However, we would like to encourage readers to draw their attention to Section 4, as it discusses the main contribution of our work. Detailed theoretical insights are deferred to Appendix A."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700292418969,
                "cdate": 1700292418969,
                "tmdate": 1700292418969,
                "mdate": 1700292418969,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PEGxSmOdUK",
            "forum": "AP779Zy70y",
            "replyto": "AP779Zy70y",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8254/Reviewer_sCet"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8254/Reviewer_sCet"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a GAT extension named GATE, which enables GATs to switch off task-irrelevant neighborhood aggregation, alleviate over-smoothing and benefit from higher depth. Experiments on real-world datasets and theoretical analyses demonstrate the model's good performance."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is well-written and easy to follow. The organization of this paper is good.\n2. The experiments are comprehensive and the theoretical analyses are solid, which makes the good performance of GATE convincing. \n3. It is an interesting and novel idea to switch off task-irrelevant neighborhood aggregation for GATs."
                },
                "weaknesses": {
                    "value": "1. It seems that GATE switches off task-irrelevant neighborhood by separating the parameters $a_t$ of the target nodes from $a_s$ of the source nodes. By this, the effect of target nodes is decreased by tuning $a_t$. The question is, when a neighborhood is switched off, all nodes in this neighborhood including those that are helpful to the task are synchronously switched off. Will this be harmful to the model's performance?\n2. GATE is able to reduce the performance drop caused by increased depth, would it be more beneficial to introduce more hops of neighbors by using deeper GATE or it is better to use a shallower one?\n3. The texts in the legends of figures are not very clear."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8254/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699356990479,
            "cdate": 1699356990479,
            "tmdate": 1699637026136,
            "mdate": 1699637026136,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "d6kRZd0nWN",
                "forum": "AP779Zy70y",
                "replyto": "PEGxSmOdUK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8254/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8254/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sCet"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the constructive feedback and insightful questions, which we answer below.\n\n- While the architecture is able to completely switch off the contribution of every neighbor if that is adequate, it does not have to do this. On synthetic tasks in which neighbors do not contain any information about a node's label, the ability of GATE to switch off neighborhood aggregation improves its performance. On many real-world data sets, however, the contributions of neighbors are not synchronously switched off. We demonstrate this by comparing the distributions of $\\alpha_{uv}$ where $u\\in \\mathbb{N}(v) \\backslash v$ in Fig. 9 in Appendix C.3 for GATE that shows a skewed distribution of $\\alpha_{uv}$ over all edges where not all neighbors are switched off. In principle, the self-attention mechanism and neighborhood aggregation in GATE are no different than in GAT and neighboring nodes may still be assigned different levels of importance. The advantage of GATE is that it can distinguish between node and neighbor features and weigh them more flexibly against each other. This fact explains its often superior performance to GAT, especially for heterophilic tasks.\n\n- Ideally, the architecture should be flexible enough to determine the more beneficial case among the two and this is the direction we aim to pursue with GATE as this is an interesting dilemma that manifests itself in most graph representation learning tasks and is a much broader research problem. To the best of our knowledge, so far in the literature, the number of layers of GNNs has been treated as a hyper-parameter and deeper models have been enabled by explicit skip connections and identity mappings. While these techniques are complementary, the ability of GATE to switch off neighborhood aggregation could potentially lead to the network simulating perceptron behavior in certain layers, which may then be able to learn an identity or more complex deeper non-linear transformations of nodes multiple hops away which may then again be aggregated at further depth. We discuss this briefly in the paper in Section 5.2 (paragraph: interpretable neighborhood aggregation) in light of Fig. 4. The bottom line is that this decision is task-dependent and we observe in Table 6 in Appendix C.1 that for larger graphs, deeper GATE models lead to an improvement by possibly gaining information from a larger radius in the neighborhood and/or by learning more complex features of neighbors and the node itself.\n\n- We thank the reviewer for the feedback and would be happy to improve the legends of our figures. It would be really helpful if the reviewer could be slightly more specific with an example of a figure and whether it's a readability problem or the text in the legend is ambiguous. This would allow us to make a more satisfactory improvement."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8254/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700292631943,
                "cdate": 1700292631943,
                "tmdate": 1700292631943,
                "mdate": 1700292631943,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]