[
    {
        "title": "LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers"
    },
    {
        "review": {
            "id": "1GqVHv9i19",
            "forum": "kC5i5X9xrn",
            "replyto": "kC5i5X9xrn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1216/Reviewer_xpDg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1216/Reviewer_xpDg"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce LightSeq, a distributed training approach for long-context LLMs.\nLightSeq uses sequence parallelism over multiple GPUs, i.e. splitting query, key and value along the sequence dimension. Their approach DistAttn then computes the attention matrix chunk wise. \nTheir approach improve performance for large context lengths compared to parallelization over attention heads such as Megatron-LM as the degree of parallelization is not limited by the number of heads. \nBy overlapping communication and computation, improving load-balancing for unidirectional models, and adjusting checkpointing for flash-attention, LightSeq achieves a speedup between 0.77 and 2.01 over Megatron-LM when training Llama-7B based models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The research topic is of importance and the presented approach appears useful. Parallelization is essential in training LLMs and parallelization over the sequence length can be important with the ever-increasing context lengths, as is also demonstrated by the experiments in this paper.\n- The paper is well written and easy to follow. It clearly communicates the motivation and contributions and gives an easy-to-understand explanation of the introduced approaches.\n- The approach seems overall well-grounded and optimized, employing several techniques to increase performance and reduce idle time.\n- The experimental results demonstrate a significant speedup over Megatron-LM, a widely used parallelization strategy for transformer training.\n- The authors provide source-code, thereby supporting reproducibility of their approach"
                },
                "weaknesses": {
                    "value": "One issue is the assumption of causal learning tasks and thus the limitation of the approach to uni-directional Transformer architectures. The approach does not translate to bi-directional models, such as BERT, where long sequences are becoming a problem just as well. \n\nMy main concern however is the limited experimental evaluation, which focusses solely on comparing to Megatron-LM and only on Llama model variants, even though the authors mention other sequence-parallel approaches . \nFor one giving only the speedup over a single other parallel approach (Table 1) limits the general comparability:\n- For one including the speedup over sequential BP would be helpful to judge how well both of these approaches scale.\n- A comparison to Li et al., which the authors mention, would be highly necessary. The authors state that this approach is not optimized for unidirectional, long-context training, so the benefit of LightSeq over this approach for models such as Llama would be valuable.\n- And while the authors argue that pipeline parallelism is limited by its unbalanced memory consumption, it would be good to back this up with experimental results. A comparison to TeraPipe [1], which employs token-level pipelining, would also be interesting.\n\nFurthermore, the evaluated model architectures seem highly tailored to the specific aspects of LightSeq, which again does not demonstrate any generalization.\n- The 33H case seems rather constructed and unlikely in practice (why would you chose such a number specifically?). It is a valid limitation of parallelization over the heads but should not be that big a problem in actual use, reducing the realistic speedup to ~1.5x (which is still substantial). Also, the title of the paragraph does not really fit the content as it never gives any support why one would want to use an arbitrary numbers of heads.\n- In that regard the suggestion of \"scaling\" Megatron by adding dummy heads seems unnecessary, the authors even discuss how inefficient this would be. I don't believe it is necessary to discuss this first \"option\" at all.\n\n\nFurther issues\n- Quite a lot of detail on the algorithmic implementation of DistAttention, e.g. the communication scheme, is lacking. One has to dig through the source code to get a glimps of what is going on\n- In contrast to the other sections, section 4 seems unpolished and is often hard to understand \n- no links between different sections, some Figures are never referred to, inconsistent punctuation when referring to figures and tables. This could all be solved by using a package like cleverref or autoref.\n- the difference between the different models in the model setup is not described clearly enough, GQA should be cited and explained a bit better, the 33H model is only clear from context, and it is not clear how exactly the 16H-2H models are scaled \"properly\"\n- The example analysis comparing MHA and GQA in Section 4.1 is a bit hard to follow and it would probably help to give the actual runtimes and a table or figure.\n- in Section 4.3 when discussing the effect of load balancing, a diagram illustrating the work assigned to each time step would help visualizing the difference between the schedules. It would also help to remind the reader again, that the balanced schedule completes the 36 work units in 5 time steps, thus resulting in a maximum speedup of 7.2, instead of having the reader backtrack to understand where the 7.2 came from.\n- Section 4.4 seems a bit odd, it is titled \"Discussion\" but contains a mix of future work, introducing a very brief comparison to DeepSpeed Ulysses which is never mentioned before or after, and reiterating on the drawbacks of pipeline parallelism without given much new information that had not already been stated in Section 4.2."
                },
                "questions": {
                    "value": "- Your approach for DistAttn seems to me very similar to what FlashAttention2 does to parallelise on a single GPU. Can you please elaborate a bit more on the algorithmic differences between the two?\n- LightSeq is especially advantageous when the sequences are long and there are few heads or the number of heads is not well divisible by the number of GPUs. Would it be possible/make sense to use tensor-parallelism (in addition to the sequence-parallelism) in the DistAttn to retain the advantages of Megatron-LM and also scale well in scenarios with shorter sequences and many heads?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1216/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698833017418,
            "cdate": 1698833017418,
            "tmdate": 1699636048035,
            "mdate": 1699636048035,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZmhgCGqXlh",
                "forum": "kC5i5X9xrn",
                "replyto": "1GqVHv9i19",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1216/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1216/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer xpDg (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful comments! We would like to answer your questions and address your concerns in this response:\n\n**Q1:** Your approach for DistAttn seems to me very similar to what FlashAttention2 does to parallelize on a single GPU. Can you please elaborate a bit more on the algorithmic differences between the two?\n\n**A1:** In the distributed setting, unbalanced computation and the cross-GPU communication overhead become two challenges that do not exist in the single-GPU case. Therefore, we propose and implement a load-balancing scheduling and an asynchronous communication schema to overcome this. DistAttn without these two optimizations can be understood as a distributed version of FlashAttention2. We extended the FlashAttention2 kernel to support multi-GPU computation by sharding the outer loop onto different devices and communicating the keys and values upon need.\n\n**Q2:** LightSeq is especially advantageous when the sequences are long and there are few heads or the number of heads is not well divisible by the number of GPUs. Would it be possible/make sense to use tensor-parallelism (in addition to the sequence-parallelism) in the DistAttn to retain the advantages of Megatron-LM and also scale well in scenarios with shorter sequences and many heads?\n\n**A2:** This is a great suggestion that we do recommend in practice to replace (instead of combining at the same time) TP with DistAttn at long sequences as DistAttn requires less communication and supports scaling beyond the number of heads. \n\n**Q3:** The 33H case seems rather constructed and unlikely in practice (why would you choose such a number specifically?). It is a valid limitation of parallelization over the heads but should not be that big a problem in actual use, reducing the realistic speedup to ~1.5x (which is still substantial)\n\n**A3:** Yes, we designed the 33-head model to illustrate the advantages of keeping the distributed training design agnostic to the model configuration. We picked the number 33 for this illustration which may sound unusual. However, we would like to note that it is a common case in practice to have a non-power-of-2 number of heads as we discussed in the second paragraph of the introduction. For example, GPT-2-XL has 25 attention heads, GPT-2 has 12 attention heads, Llama-33B and its fine-tuned versions (e.g., Tulu-30B) have 52 attention heads, Whisper-large has 20 attention heads, and Falcon-7B has 71 attention heads. We would also note that our design can facilitate future model architectures as we discussed in the second paragraph of the introduction: \n> Moreover, many works have shown that the future Transformer architecture design may have even fewer attention heads. For example, Bian et al. (2021) demonstrate that Transformers with a single head outperforms its multi-head counterparts, representing a challenging scenario for solutions like Megatron-LM.\n\n**Q4:** One issue is the assumption of causal learning tasks and thus the limitation of the approach to uni-directional Transformer architectures. The approach does not translate to bi-directional models, such as BERT, where long sequences are becoming a problem just as well.\n\n**A4**: We thank the reviewer for pointing this out. We focus on the optimization for the GPT-based model (specifically, the Llama models) as it\u2019s the most popular model architecture in recent days. The evaluation of bi-directional models is out-of-scope in this paper. However, we respectfully disagree that our approach is therefore only limited to uni-directional Transformer architectures. The DistAttn can be easily applied to bi-directional models by simply disabling the load-balancing scheduler. The load-balancing optimization is proposed to address the bubble caused by causal modeling. But there\u2019s no bubble when handling bi-directional attention, in which case the vanilla design of DistAttn w/o load-balancing scheduling will be good enough."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700443496431,
                "cdate": 1700443496431,
                "tmdate": 1700443496431,
                "mdate": 1700443496431,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vwqYiNIw33",
                "forum": "kC5i5X9xrn",
                "replyto": "1GqVHv9i19",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1216/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1216/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you and let us know any further comments"
                    },
                    "comment": {
                        "value": "Dear reviewer xpDg,\n\nWe are really encouraged by your helpful and positive feedback, and sincerely appreciate your efforts. We hope our answer, updated experiments and the corresponding manuscript makes the paper clearly and stronger. Please let us know if you have any further comments, and we are more than happy to address them.\n\nBest,\n\nThe authors"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536075671,
                "cdate": 1700536075671,
                "tmdate": 1700536075671,
                "mdate": 1700536075671,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bMw9e88ja7",
            "forum": "kC5i5X9xrn",
            "replyto": "kC5i5X9xrn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1216/Reviewer_AsB4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1216/Reviewer_AsB4"
            ],
            "content": {
                "summary": {
                    "value": "This work, LIGHTSEQ, proposes a sequence parallelism prototype for long-context transformer\ntraining. Three key elements are introduced: 1) distributed attention with load balancing for causal language\nmodelings, 2) distributed attention with overlapped communication with computation, and 3) a re-materialization-aware checkpointing strategy. This work's experiments on 16 A100s show decent speedup over Megatron-LM on Llama models as well as enabled longer sequences."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "+. Proposed a new loading balancing schedule dedicated to distributed attention by shifting busy workers' q/k/v compute to idle workers.\n\n+. Developed a system optimization of overlapping remote q/k/v with local compute by using two CUDA streams.\n\n+. Improved huggingface's gradient checkpointing by leveraging FlashAttention's recompute feature in backward compute.\n\n+. Has open-source code implementation\n\n+. Evaluation on real clusters"
                },
                "weaknesses": {
                    "value": "-. **Overstatement**: \n\n> \"we propose partitioning solely the input tokens (i.e., sequence parallelism)\" \n\nwhy sequence parallelism is proposed in this work, instead of enhanced by this work?\n\n> \"We present a solution that is agnostic to the model architecture\"\n\nwhy sequence parallelism by default is NOT agnostic to model architecture (i.e., like data parallel, just replicate the layer across devices)?\n\n> Figure 4\n\nBy default, FlashAttention uses recompute during backward pass. How does this become a contribution of this work?\n\n> Figure 1 (left)\n\nBy default, using sequence parallelism for the attention layer should be like this: replicate FFN and projection, and communication (k, v) for attention op. why did this scheme become a proposal for this work?\n\n-. **Unclear load-balanced scheduling**: \n\n> Figure 1 (right)\n\nBesides shifting worker 6,7,8's compute on (kv1, kv2, kv3) to worker 1,2,3, can we also shift worker 6,7,8's compute on (kv6, kv7, kv8)?\n\nIs there a way to optimally find the best compute workload to shift?\n\nHow does the DistAttn & load balancing work for attention op with sliding local windows?\n\nHow does the DistAttn & load balancing work for global windows (i.e., each token attends to all tokens)?\n\nHow does the DistAtten & load balancing work for MQA models?\n\n-. **Unclear Evaluation Setup**: \n\n> Figure 3\n\nIs it measured in a distributed setting or on a single GPU?\n\n-. **Intertwined with Other Work**\n\n> LIGHTSEQ + FSDP\n\nFSDP replies a large batch/sequence size to amortize the weight AllGather overhead; how will an enlarged sequence length affect LIGHTSEQ's performance?\n\nDoes this work use FSDP for cross-node and LIGHTSEQ for intra-node communication?\n\nHow about we use only LIGHTSEQ for cross-node and intra-node communication?\n\nCan LIGHTSEQ scale up to more nodes, without leveraging FSDP?\n\n-. **Limited improvement**: \n\nFor sequence length < 8K, LightSEQ is slower than DeepSpeed-Ulysses by up to 0.5x.\n\nIt seems that this work is only evaluated on Llama models.\n\nWhy sequence length at 128K show 1.44x overlapped cased vs 1.x no communication in Figure 5?"
                },
                "questions": {
                    "value": "*. See above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1216/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1216/Reviewer_AsB4",
                        "ICLR.cc/2024/Conference/Submission1216/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1216/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698915905353,
            "cdate": 1698915905353,
            "tmdate": 1700704959234,
            "mdate": 1700704959234,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YjLsmDWJU0",
                "forum": "kC5i5X9xrn",
                "replyto": "bMw9e88ja7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1216/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1216/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer AsB4 (Part 1)"
                    },
                    "comment": {
                        "value": "**Q1:** Overstatement on proposing sequence parallelism, \u201cwhy sequence parallelism is proposed in this work, instead of enhanced by this work?\u201d\n\n**A1:**  We didn\u2019t claim we are the first on sequence parallelism. In the related work, we have clearly stated that [1] is among the first to propose sequence parallelism. We also discussed [2] as another previous work.\n\n[1] Shenggui Li, Fuzhao Xue, Yongbin Li, and Yang You. Sequence parallelism: Making 4D parallelism possible.\n[2] Vijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. Reducing activation recomputation in large transformer models. Proceedings of Machine Learning and Systems, 5, 2023.\n\n**Q2:** Overstatement on flash attention \u201cBy default, FlashAttention uses recompute during backward pass. How does this become a contribution of this work?\u201d\n\n**A2:** We did not claim it is our contribution. Our contribution is a new checkpointing strategy that utilizes this recomputation to avoid another recomputation at the HuggingFace gradient checkpointing level, not the recomputation inside the FlashAttention kernel.\n\n**Q3:** The optimal way to shift compute: \u201cBesides shifting\u2026 kv8.\u201d\n\n**A3:** Thanks for this important and interesting question. The criteria of optimality is to evenly assign workload to different workers. Assume there are P workers, then the number of units of work is (1+2+...+P) = (1+P) * P / 2. As long as the scheduling finishes in ceil[(1+P) / 2] time steps, it is optimal in terms of compute balancing. Using 8 workers as examples, if the scheduling finishes in ceil[(1+8)/2] = 5 time steps, it is optimal. While there are certainly other schedules to achieve this, we provide one of these.\n\n**Q4:** How does the DistAtten & load balancing work for MQA models?\n\n**A4:** DIstAttn distributes the sequence dimension, and the system does not change whether it is a single head (MQA) or multi-heads (MHA). \n\n**Q5:** Is Figure 3 measured in a distributed setting or on a single GPU?\n\n**A5:** We thank the reviewer for this question. It is measured in an A100 40GB. We have updated the manuscript to reflect this.\n\n**Q6:** How does DistAttn and load balance works for sliding local windows or global windows\n\n**A6:** We thank the reviewer for this important comment. The focus of current work is on exact attention, which is also described in the related work section. Incorporating LightSeq with a sparse attention pattern is a natural next step. In this answer, we give an initial idea on how to extend LightSeq with local and global windows.\n\nFor local sliding windows, the workload is naturally (near) balanced, regardless of single directional or bidirectional attention. Thus, simply disregarding the attention logic to non-local workers suffices. For instance, in exact attention, worker 7 needs to compute attention to all other workers. If the sliding window has a number of tokens equal to that of one worker, then worker 7 only needs to attend to itself and tokens in worker 6. In other words, it only needs to fetch key and value from worker 6, and compute attention. In terms of implementation change, it just needs to change the end condition of the for loop (from looping worker 1 - worker 7 to looping only from worker 6 - worker 7).\n\nGlobal windows can be interpreted in two ways. We\u2019re not sure which one corresponds to the reviewer\u2019s question so we answer for both. The first interpretation is one special design of the sparse attention (e.g., global attention in Longformer), where there are a certain number of global tokens that all later tokens need to attend to, which are used to capture the global information. To adapt DistAttn to this, one easy way is to keep a replica of all the global tokens in each worker, which is simple and practical as otherwise, the global tokens will need to be all-gathered at each time step. One can also split the global tokens evenly onto all workers and use all-gather upon computation to further reduce the memory requirement. The second interpretation is the bidirectional modeling in the full attention situation (e.g., BERT). In this situation, the users can simply disable the load-balancing schema and use the vanilla DistAttn as illustrated in Figure 6 of the updated manuscript. We introduce the load-balancing only to handle the bubble issue caused by the causal modeling, so there\u2019s no need to apply it if it\u2019s bidirectional attention as there is no computation bubble in this case."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700444851979,
                "cdate": 1700444851979,
                "tmdate": 1700444851979,
                "mdate": 1700444851979,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vDHz1lajsA",
                "forum": "kC5i5X9xrn",
                "replyto": "bMw9e88ja7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1216/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1216/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We are more than happy to address your further comments"
                    },
                    "comment": {
                        "value": "Dear reviewer AsB4,\n\nWe would like to appreciate again on your precious time and helpful comments. We hope our response and updated paper has adequately addressed your concerns. And we are more than happy to address any of your further comments.\n\nBest,\n\nThe authors"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700529186325,
                "cdate": 1700529186325,
                "tmdate": 1700529186325,
                "mdate": 1700529186325,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gcrwV6HZV0",
                "forum": "kC5i5X9xrn",
                "replyto": "1TaRjQIwdu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1216/Reviewer_AsB4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1216/Reviewer_AsB4"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the great effort of the authors on the rebuttal and clarifications. The rebuttal has addressed some of my concerns, but still missed evaluations on non-LLAMA models and better performance over DeepSpeed-Ulysses. So I slightly prefer to hold my score. Thanks again for the authors."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705255155,
                "cdate": 1700705255155,
                "tmdate": 1700705255155,
                "mdate": 1700705255155,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "90KXpsebOi",
            "forum": "kC5i5X9xrn",
            "replyto": "kC5i5X9xrn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1216/Reviewer_ZAxH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1216/Reviewer_ZAxH"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a technique, DISTATTN, to speed up training of causal transformer models on clusters of GPUs and embodies them in a framework called LightSeq.  The LightSeq framework, based upon FlashAttention-2, shows $2\\times$ speedup over an updated Megatron-LM (MLSys 2023).    \n\nDISTATTN addresses the load imbalance that occurs when splitting up the input sequence among worker GPUs while training a causal language model.  Naively, splitting up the input sequence across GPUs causes later worker GPUs to perform more computation as they need to attend to more of the input sequence so perform more computations.  The idea in DISTATTN is to have some of the earlier workers perform computations that would normally be performed by later workers.   An additional optimization applied is to shift the point of checkpointing to avoid some recompilations in the backward pass.\n\nAnalytical analysis shows a reduction from $10dN$ to $3dN$ in total communication volume versus Megatron-LM (2023) making the approach a bit more suitable for clusters without high bandwidth (and expensive) interconnects."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The results show a significant reduction in training time.\n\nThe general ideas are clear -- use load balancing and overlap communication and computation. \n\nReducing communication volume should enable model parallel training on cheaper hardware."
                },
                "weaknesses": {
                    "value": "The concept of load balancing and overlapping communication and computation in parallel systems is not novel on their own even if the details here might be.  \n\nDetails of the specific load balancing technique could be explained more clearly.\n\nWhile the paper shows longer sequence lengths can be supported there were no accuracy or quality results demonstrating how much improvement the longer sequence length translated to."
                },
                "questions": {
                    "value": "For right side of Figure 1, are all circles independent computations?  Otherwise it would be unclear why bubble size is only 4 after balancing.  For the rightmost figure it is unclear which computation have been moved to worker 1-3  for the upper triangle versus before balancing.   Perhaps the color scheme could be used to indicate this.   \n\nI found it unclear the relationship between Figure 1 and 2 unclear.  In Figure 1 it looks like worker 7 should be computing attn(q7,k1v1) in the first timestep, but in Figure 2 in the first timestep what is shown is attn(q7,k7v7).  Similarly, the communication pattern in Figure 2 is a bit unclear.  Is there a simple formula for what attn query/key/value combination is computed on a worker in a given timestep and what values are  communicated from/to a given worker on a given timestep? \n\nOn Page 5 it is mentioned that the evaluation uses variously 2 A100 DGXs and an 2x8xA100 in house platform without infiniband.  However, as both of these have 2x8 GPUs, it is unclear which system the measurements are for in Table 1.   I would expect given the reduction in communication volume that a \"cheaper\" cluster without NVLINK might give decent results and was interested to see results for a comparison vs. DGX, but it seems like they are missing from the paper and supplemental.\n\nWhat is the speedup versus FlashAttention-2?  If I understood correctly for LightSeq you started with the FlashAttention-2 framework, so I would expect to see speedups with respect to that system too.  \n\nThe longer sequence lengths explored in Table 2 and 3 should yield better accuracy / quality on the tasks the networks are applied to, but  there appeared to be no results demonstrating this."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1216/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1216/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1216/Reviewer_ZAxH"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1216/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699133880311,
            "cdate": 1699133880311,
            "tmdate": 1699636047882,
            "mdate": 1699636047882,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IwcPaBIgNv",
                "forum": "kC5i5X9xrn",
                "replyto": "90KXpsebOi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1216/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1216/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer ZAxH"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful and helpful feedback! We would like to address your questions in the below response.\n\n**Q1**: The concept of load balancing and overlapping communication and computation in parallel systems is not novel on their own even if the details here might be.\n\n**A1**:  We agree with the reviewers that load balancing and overlapping communication has always been important topics in parallel systems. However, we want to point out LightSeq is the first work that discusses how to design and implement these two techniques in the space of sequence parallelism. \n\n**Q2**: Details of the specific load balancing technique could be explained more clearly.\n\n**A2**: We thank the reviewer for raising this clarification question. We added a section in the appendix (Appendix C) to illustrate the load-balancing design. Table 6 shows the computation and communication pattern before balancing, while Table 7 shows the pattern after load-balancing. We will also answer your question in the next paragraph and please feel free to let us know if there is still anything unclear to you. \n\nEach circle is a unit of computation. Circles in the same color mean that they are computed in the same time step.  For instance, the rightmost and bottommost circle means that at time step 1 (t1), worker 8 is executing attn(q8, k8, v8). Similarly, green color denotes computations that happen at the second time step (t2). At t2, worker 1 is executing attn(q8, k1, v1). We have updated the figure and the caption to communicate the idea better.\n\n**Q3**: The relationship between Figure 1 and Figure 2 is unclear - \u201cI found it \u2026 on a given timestep\u201d?\n\n**A3**: Following up on the previous answer, at the first step (blue color), worker 7 is computing the circle corresponding to q7 and kv7, i.e. attn(q7, k7, v7). \n\nYes, we would like to provide a formula here. (Before rescheduling) the computation a worker w at time t is $attn(q_w, k_{(w-t+1)}, v_{(w-t+1)})$, i.e. in a backward manner - worker 7 computes attn(q7, k7, v7) on time 1, and attn(q7, k6, v6) on time 2. After rescheduling, if a worker w helps other workers (e.g. worker 1, 2, 3 in Figure 1), it is computing $attn(q_{(w+P-t+1)}, k_w, v_w)$, i.e. also in a backward manner - worker 1 computes attn(q8, k1, v1) on time 2, attn(q7, k2, v2) on time 3. For workers who do not help others, the computation remains the same (but the maximal time step gets reduced because of load balancing). \n\nFor communication, the worker pre-fetches the values needed for the next time step. Before rescheduling, a worker w at time t is computing $attn(q_w, k_{(w-t+1)}, v_{(w-t+1)})$. Thus, it will prefetch $k_{(w-t+1)}, v_{(w-t+1)}$ at time (t-1). In other words, at time t, worker w prefetches $k_{(w-t)}$ and $v_{(w-t)}$. As an example, worker 7 computes attn(q7,k6, v6) in time 2, so it prefetches k6 and v6 in time 1. With rescheduling, the logic is very similar. If a worker w helps other workers, it will compute $attn(q_{(w+P-t+1)}, k_w, v_w)$ at time t, and thus prefetches $q_{(w+P-t+1)}$ at time t-1. Additionally, it sends the output o back when it is ready. For workers who do not help others, the prefetching logic for key and value is the same. \n\n**Q4**: The configuration of Table 1: \u201cOn page 5 it is mentioned that\u2026 and supplementary\u201d.\n\n**A4**: Table 1 is using the second cluster setup (2 DGX boxes). We found in the previous We have updated the manuscript to indicate that the default setup is the 2 DGX boxes, and thanks again for the helpful feedback. \n\nTo test the performance of inter-node training, we mainly report results on DGX boxes with Infiniband. This is because the current implementation of the P2P component is not yet highly optimized (as we brought up in the discussion section),). We are actively working on the engineering side to improve this. To be more specific, in higher bandwidth inter-node training settings, our communication overlapping component results in a very noticeable speedup, and we thus use the 2 DGX boxes as the default testbed. We have also updated the discussion of the manuscript to make our implementation roadmap clearer. \n\n**Q5**: Speedup against FlashAttention: \u201cWhat is the speedup.. to that system too\u201d.\n\n**A5**: LightSeq and Flash attention are orthogonal work, one is distributed and the other is on a single GPU. Conceptually, LightSeq reduces to Flash attention when P=1. When scaling to P=8, we have observed 7.5x speedup compared to flash attention (Figure 5 Left), a near-linear scaling.\n\n**Q6**: No accuracy report for longer sequences: \u201cThe longer \u2026 demonstrating this\u201d.\n\n**A6**: In this work, we focus on the system/infrastructure side of long-context training. However, the gain of longer sequences training has been validated in several previous works, e.g. in [1], [2].\n\n[1] FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\n\n[2] Longformer: The Long-Document Transformer"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700445673313,
                "cdate": 1700445673313,
                "tmdate": 1700587623943,
                "mdate": 1700587623943,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2LVMf6hUXZ",
                "forum": "kC5i5X9xrn",
                "replyto": "90KXpsebOi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1216/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1216/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you and please let us know any further comments!"
                    },
                    "comment": {
                        "value": "Dear Reviewer ZAxH,\n\nWe are deeply encouraged by your helpful and positive feedback, and sincerely appreciate your efforts. We hope our answer and correspondingly updated manuscript makes the paper clearly and stronger. Please let us know if you have any further comments, and we are more than happy to address them.\n\nBest,\n\nThe authors"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700535950760,
                "cdate": 1700535950760,
                "tmdate": 1700535950760,
                "mdate": 1700535950760,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0rD5pOYqHD",
            "forum": "kC5i5X9xrn",
            "replyto": "kC5i5X9xrn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1216/Reviewer_Fg9E"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1216/Reviewer_Fg9E"
            ],
            "content": {
                "summary": {
                    "value": "LightSeq introduces a novel approach for training large language models (LLMs) with extended context lengths, overcoming the limitations of previous model-parallel systems like Megatron-LM. Unlike Megatron-LM, which is limited by the number of attention heads and incurs high communication volumes, LightSeq efficiently partitions over the sequence dimension. This makes it adaptable to various model architectures and significantly reduces communication needs by up to 4.7\u00d7, while also overlapping communication with computation. In comprehensive experiments, including on the Llama-7B model, LightSeq demonstrates up to 2.01\u00d7 speedup in end-to-end training and supports up to 8\u00d7 longer sequence lengths for models with fewer heads compared to Megatron-LM."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Efficient training of long-sequence models is an important goal.\n* Demonstrates runtime speedup over Megatron-LM for long sequence lengths."
                },
                "weaknesses": {
                    "value": "* Leveraging sequence-level parallelism in transformer training (Section 3.1) is not novel (e.g., https://arxiv.org/abs/2105.13120).\n* Overlapping computation and communication (Section 3.2) appears to be a fairly standard technique and may not be as effective on more recent GPUs like the H100, which have significantly higher compute capabilities than the A100, not to mention optimizations such as transformer engine, tensor memory accelerator, etc.\n* The configurations for parallelism are questionable."
                },
                "questions": {
                    "value": "Thank you for submitting to ICLR 2024. Leveraging sequence-level parallelism is an interesting direction to scale both sequence length and training efficiency. However, I have the following questions:\n\n* This is not the first work to leverage sequence-level parallelism for training transformer models. For example, how does the proposed work compare, both qualitatively and quantitatively, to [this 4D sequence parallelism work](https://arxiv.org/abs/2105.13120)?\n* The parallelism configurations are unclear. What is the batch size used for training? If the batch size is high enough, wouldn't a combination of tensor/data/pipeline parallelism provide sufficient parallelism to achieve high utilization of GPUs, even without utilizing sequence parallelism?\n* It is somewhat odd to take results from different clusters for different experiments (e.g., the results in Table 2 were drawn from a small memory configuration without InfiniBand). Is there a convincing reason for this?\n* In Table 3, the authors used a batch size of 1, which does not seem to be a reasonable configuration for training. Can you elaborate on this choice?\n* Regarding the explanation of pipeline parallelism, the authors attribute the low performance to \u201chigh memory pressure in the first stage.\u201d Couldn't this be due to poor partitioning between stages?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1216/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699627999689,
            "cdate": 1699627999689,
            "tmdate": 1699636047805,
            "mdate": 1699636047805,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IpXapHBCPE",
                "forum": "kC5i5X9xrn",
                "replyto": "0rD5pOYqHD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1216/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1216/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer Fg9E"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the insightful and helpful feedback! We would like to address your questions in the below response.\n\n**Q1**: How does the proposed work compare, both qualitatively and quantitatively, to this 4D sequence parallelism work?\n\n**A1**: We discussed 4D sequence parallelism in the related work section:  \n\n>\u201c[1] is among the first to parallelize along the sequence dimension. However, it is not optimized for the computational pattern of causal language modeling and is incompatible with memory-efficient attention, which is crucial to long-context LLM training. \u201d\n\nIn other words, the 4D sequence parallelism work is the first to propose this concept but it\u2019s not optimized for modern LLM training and, thus is unable to support very long sequences efficiently. Quantitatively, LightSeq supports at least 8x longer sequences than this work, and achieves 4.45x - 5.64x speedup. We have added this experiment results and further analysis in the updated paper (appendix: Comparison with Ring Self-Attention). We provided the main experiment numbers below.\n\n### Maximal sequence length on Llama-7B on the DGX (A100-80GB) cluster\n |           | 1 Node (8 GPUs) | 2 Nodes (16 GPUs) |\n|-----------|-----------------|-------------------|\n| RSA       | 32K             | 64K               |\n| LightSeq   | > 256K          | > 512K            |\n\n### Per iteration time comparison with RSA (seconds) on the DGX clusters\n\n|           | 1 Node (32K sequence length) | 2 Nodes (64K sequence length) |\n|-----------|--------------|---------------|\n| RSA       | 14.10        | 30.49         |\n| LightSeq   | 2.50         | 6.85          |\n| Speedup   | 5.64x        | 4.45x         |\n\n\n**Q2**: The parallelism configurations are unclear. What is the batch size used for training? If the batch size is high enough, wouldn't a combination of tensor/data/pipeline parallelism provide sufficient parallelism to achieve high utilization of GPUs, even without utilizing sequence parallelism? In Table 3, the authors used a batch size of 1, which does not seem to be a reasonable configuration for training. Can you elaborate on this choice?\n\n**A2**: \nWhen combined with tensor/data/pipeline parallelism, LightSeq sequence parallelism can be thought of as a better parallelism approach *replacing TP* to support long context. To see this, we note the LLM training memory utilization can be decomposed into two parts. The first part is related to the model parameters, i.e., memory used to store parameters, gradients, and optimizer states. The second part is related to the activation. It is worth noting that the memory for activation dominates when increasing the context length and the batch size only affects the memory for activation. Given this premise, we know that, firstly, data parallelism wouldn\u2019t reduce either part, so we could consider it as orthogonal and only discuss the DP=1 case. Secondly, in Table 2, we show that the combination of model parallelism and pipeline parallelism handles shorter sequence lengths in a variety of scenarios, even when batch_size=1. This indicates that our sequence parallelism can support a very long context, as it would avoid TP+PP running out of memory.. In addition, Table 2 shows that using TP alone can effectively support the same length as LightSeq, but it requires more communication and takes longer than LightSeq as shown in Table 1. Increasing the batch size has the same effect on TP and LightSeq sequence parallelism (LightSeq is used with FSDP in experiments). Therefore, LightSeq should be a better choice than TP in supporting long context lengths.\n\n**Q3**: Regarding the explanation of pipeline parallelism, the authors attribute the low performance to \u201chigh memory pressure in the first stage.\u201d Couldn't this be due to poor partitioning between stages?\n\n**A3**: **We believe optimizing pipeline partitioning may not solve the problem**. We follow the standard practice in Megatron-LM to evenly partition the layers onto different devices and utilize micro-batches to reduce the bubble size. This partitioning strategy prefers to have an even workload across stages. If we partition it into uneven stages to balance the memory, it will cause unbalanced computation (stragglers), leading to inefficiency. \n\n[1] Shenggui Li, Fuzhao Xue, Yongbin Li, and Yang You. Sequence parallelism: Making 4d parallelism possible. arXiv preprint arXiv:2105.13120, 2021."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700443703847,
                "cdate": 1700443703847,
                "tmdate": 1700587453599,
                "mdate": 1700587453599,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1SqTwLFJlj",
                "forum": "kC5i5X9xrn",
                "replyto": "0rD5pOYqHD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1216/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1216/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We are more than happy to address your further comments"
                    },
                    "comment": {
                        "value": "Dear reviewer Fg9E,\n\nOnce again we sincerely thank your precious time and helpful comments. We hope our response and updated paper has adequately addressed your concerns. And we are more than happy to address any of your further comments.\n\nBest,\n\nThe authors"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1216/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700529051874,
                "cdate": 1700529051874,
                "tmdate": 1700529051874,
                "mdate": 1700529051874,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]