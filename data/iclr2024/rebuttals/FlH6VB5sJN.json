[
    {
        "title": "A Parallel Multi-compartment Spiking Neuron For Multi-scale Sequential Modeling"
    },
    {
        "review": {
            "id": "VOEkBhI7wr",
            "forum": "FlH6VB5sJN",
            "replyto": "FlH6VB5sJN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5407/Reviewer_Chwq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5407/Reviewer_Chwq"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a new architecture for sequence modelling, based on multicompartment neuron dynamics. They then develop a method for parallel training of these systems and apply them to several benchmark tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The experimental results seem very strong, significantly outperforming previous methods. In addition, the figures are clear and help the reader with understanding the content. Lastly, parallelizing the training of such spiking multi-compartment models in the temporal dimensions is novel (to my knowledge) and can be potentially impactful."
                },
                "weaknesses": {
                    "value": "**Soundness**\n\nOne of the main contributions of the paper, the parallel implementation of the algorithm, seems to hinge on the fact that they set beta_{n, n-1}. What is the effect of this on the neuronal dynamics? Can this model still be considered biophysically realistic?\n\n**Clarity**\n\nMany parts of the paper are presented in an overly convoluted way. I believe that this paper would largely benefit from moving math that is not essential to understanding the context of the paper to the appendix, for example equations 17, 18, 19.\n\nIn addition, it would be valuable if the authors attributed a biophysical meaning to their learnable parameters. \n\nFinally, I fail to understand where equation 14 comes from (although I am not exactly from the field, maybe it is clear to other reviewers).\n\n**Novelty**\n\nI believe that the claims on novelty for the multicompartment model are a bit over-stated. At the very least should the authors cite some of the (decades of) work on multicompartment modelling and its numerical implementations (starting from Hines 1984). The voltage update equations (apart from the non-linear reset) are identical to those papers, and I think this should be clarified."
                },
                "questions": {
                    "value": "See questions in the \"weaknesses\" section.\n\nPage 6: I do not understand this sentence: `we force vs to reset to a level below the firing threshold, which bears closer resemblance with biological observations.` Why does this have closer resemblance to biological observations? Also closer than what other mechanism?\n\nLastly, the authors claim (even in the abstract) that their work is motivated by a cable model of cable model of hippocampus pyramidal neurons. Given that the model they eventually use is a 5-compartment cable without any particular dynamics, this choice of model seems extremely specific. What exactly is the reason to claim that the model is in any way \"hippocampal\" or even \"pyramidal\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5407/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5407/Reviewer_Chwq",
                        "ICLR.cc/2024/Conference/Submission5407/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5407/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698528307863,
            "cdate": 1698528307863,
            "tmdate": 1700396496895,
            "mdate": 1700396496895,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3f75om3So9",
                "forum": "FlH6VB5sJN",
                "replyto": "VOEkBhI7wr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5407/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5407/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Chwq (1/2)"
                    },
                    "comment": {
                        "value": "> **Soundness 1:** One of the main contributions of the paper, the parallel implementation of the algorithm, seems to hinge on the fact that they set beta_{n, n-1}. What is the effect of this on the neuronal dynamics? Can this model still be considered biophysically realistic?\n\n**Response:** Thanks for your insightful comment. First of all, we would like to clarify that this work lies in the field of neuromorphic computing, which aims to design brain-inspired neural architecture that can solve real-world pattern recognition tasks with improved energy efficiency and effectiveness. This differs from the studies in computational neuroscience that aim to simulate and study the dynamics of biologically realistic spiking neuronal systems. Secondly, as you have rightly pointed out, we set $\\beta_{n, n-1}$ to 0 in our model as it is a necessary setting to enable model parallelization across the temporal dimension. From the neuronal dynamics perspective, this operation eliminates the feedback effect from the $n^{th}$ compartment to the ${n-1}^{th}$ compartments. However,  its impact should not be viewed in isolation. During the training process, all other compartments adapt and evolve,  effectively compensating for the effects of this abstraction. As clearly demonstrated in Figure 3 and Appendix A.6, the learned dynamics within each compartment exhibit rich and varied multi-scale temporal patterns. These dynamics confirm the effectiveness of our proposed multi-compartment neuron model in capturing complex temporal dependencies across various time scales as their biological counterparts.\n\n&nbsp;\n\n> **Clarity 1:** Many parts of the paper are presented in an overly convoluted way. I believe that this paper would largely benefit from moving math that is not essential to understanding the context of the paper to the appendix, for example equations 17, 18, 19.\n\n**Response:** Thanks for your helpful suggestion. We will move Equations 18 and 19 to Appendix, and thoroughly improve the clarity of the paper in our revised manuscript.\n\n&nbsp;\n\n> **Clarity 2:** In addition, it would be valuable if the authors attributed a biophysical meaning to their learnable parameters.\n\n**Response:** We really appreciate your suggestions. The learnable parameters in our network can be categorized into two parts. The first part is the weight matrix $\\mathcal{W}$ between network layers, which can be interpreted as synaptic strength between neurons. The second part consists of the parameters within the proposed neuron models. More specifically, as defined in Equation 3, $\\beta$ can be considered as the coupling strength between different compartments, while $\\tau$ is the membrane capacitance. These neuronal parameters are trained jointly with weight parameters in our SNNs. To improve the clarity, we will supplement the biophysical meaning of these parameters in our revised manuscripts to provide a more intuitive understanding of our formulation.\n\n&nbsp;\n\n> **Clarity 3 & Question 1:** Finally, I fail to understand where equation 14 comes from (although I am not exactly from the field, maybe it is clear to other reviewers). Page 6: I do not understand this sentence: we force vs to reset to a level below the firing threshold, which bears closer resemblance with biological observations. Why does this have closer resemblance to biological observations? Also closer than what other mechanism?\n\n**Response:** Thank you for raising concerns on this point. In Equations 13 and 14, we have described the neuronal dynamics of the last compartment that takes charge of spike generation. Inspired by the repolarization process of biological neurons after each spike generation, the reset-by-subtraction scheme [1] has been commonly used in SNNs to reset the membrane potential after neuron firing. In particular, the membrane potential ($v_s$) is reset by subtracting the value of the threshold ($\\theta$) from it. In contrast, our model modifies this mechanism to enable parallelization for the output compartment. This is achieved by subtracting the value of $ \\theta \\lfloor v_s/\\theta \\rfloor$ from $v_s$ as given in Equation 14. This operation ensures the membrane potential has been reset to a level that is below the firing threshold as well as close to the resting potential ($0$). \nLet me provide an example to illustrate the difference between these two reset schemes. For instance, assume $v_s[t]=10.5$, $\\theta=1$, and the resting potential = 0. Following the reset-by-subtraction scheme, $v_s[t+1]$ will be 9.5 after resetting, which is still much higher than the threshold. In contrast, following our reset scheme, $v_s[t+1]$ will be 0.5, which is closer to the resting potential. Therefore, our model more closely resembles the repolarization process observed in biological neurons, which reset the membrane potential to a value close to its resting potential."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700064248758,
                "cdate": 1700064248758,
                "tmdate": 1700064248758,
                "mdate": 1700064248758,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4fFvdkvYTG",
                "forum": "FlH6VB5sJN",
                "replyto": "nLVllBXS8s",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5407/Reviewer_Chwq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5407/Reviewer_Chwq"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response!"
                    },
                    "comment": {
                        "value": "Thank you for the thorough responses. It clarifies all concerns apart from the last one: why is the model in any way \"hippocampus\" or \"pyramidal\"?\n\nYour response gives two reasons, namely that (1) they have a crucial role for memory formation and (2) you borrow ideas from multicompartment models of hippocampus pyramidal studies. \n\nI am not convinced by this:\n(1) They do indeed play a role in memory formation, but why is this so crucial for your work? E.g. cortical pyramidal cells are extremely important for sensory processing. Why is memory formation closer to the tasks in your paper than sensory processing?\n\n(2) which ideas do you borrow that only relate to \"hippocampus pyramidal studies\"? Multicompartment models have been built for an extremely diverse set of neuron types and it unclear to me why relate less to your work than multicompartment hippocampus pyramidal studies."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700302782466,
                "cdate": 1700302782466,
                "tmdate": 1700302782466,
                "mdate": 1700302782466,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ESWEOOdpJE",
                "forum": "FlH6VB5sJN",
                "replyto": "sZIXzUVndL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5407/Reviewer_Chwq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5407/Reviewer_Chwq"
                ],
                "content": {
                    "title": {
                        "value": "Thank you!"
                    },
                    "comment": {
                        "value": "Thank you for the response and proposed modifications! This clarifies my concerns and I have increased my score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700396467613,
                "cdate": 1700396467613,
                "tmdate": 1700396467613,
                "mdate": 1700396467613,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "u4bzJQWiGh",
            "forum": "FlH6VB5sJN",
            "replyto": "FlH6VB5sJN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5407/Reviewer_kRbB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5407/Reviewer_kRbB"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes the parallel multi-compartment spiking neuron (PMSN). The PMSN has n compartments, and only the last compartment can fire and reset. Thus, the neuronal dynamics of the first (n - 1) compartments can be paralleled easily. For the last compartment, the soft reset and the floor function are combined, which parallelizes the neuronal dynamics with reset. The performance and energy estimation are validated on temporal datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Compared with the previous work PSN (Fang et al., 2023) which removes the neuronal reset, the PMSN can be parallelized with the neuronal reset. The ablation experiments show that the removal of neuronal reset decreases task performance.\n\n2. The experiment results on sequential CIFAR are high, showing the advantage of the PMSN."
                },
                "weaknesses": {
                    "value": "In section 5.3, only the speeds of serial and parallel implementation of the PMSN are compared. \n\nThe accuracy of the ImageNet dataset, which is an important benchmark for deep SNNs, is not reported."
                },
                "questions": {
                    "value": "Can the authors provide a speed comparison between the PSN and the PMSN?\n\nWhat are the theoretical FLOPs and memory consumption of the PMSN? I suggest that the authors provide a comparison between these two neurons. It would be better if a comparison between the memory consumption during training of an SNN with two neurons is provided."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5407/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698645479262,
            "cdate": 1698645479262,
            "tmdate": 1699636548209,
            "mdate": 1699636548209,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "K9Eqj4U61j",
                "forum": "FlH6VB5sJN",
                "replyto": "u4bzJQWiGh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5407/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5407/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kRbB (1/2)"
                    },
                    "comment": {
                        "value": "> **Weakness 1 & Question 1, 2:** In section 5.3, only the speeds of serial and parallel implementation of the PMSN are compared. Can the authors provide a speed comparison between the PSN and the PMSN? What are the theoretical FLOPs and memory consumption of the PMSN? I suggest that the authors provide a comparison between these two neurons. It would be better if a comparison between the memory consumption during training of an SNN with two neurons is provided.\n\n**Response:** Thanks a lot for raising concerns about the insufficient comparison of speed, computational cost, and memory consumption between PSN and PMSN. We have already conducted both empirical and theoretical studies on the computational cost of PMSN and PSN models. Our theoretical analysis, detailed in Appendix A.5, breaks down the theoretical computational cost into two primary components: the number of Multiply-Accumulate (MAC) operations and the number of cheap Accumulate (AC) operations. In our analysis, the FLOPs correspond to the number of MAC operations. Consider a scenario with  $m$ neurons across a time window of length $t$. The theoretical FLOPs for PSN and PMSN (5-compartment) are calculated as $mt^2$, and $32mt$, respectively. This indicates that as the time window lengthens, the PSN model requires increasingly more FLOPs compared to the PMSN model.\n\nAs per your suggestion, we have further conducted a comparative analysis focusing on memory consumption and training/inference speed using the following three benchmarks. In all experiments, identical training configurations and network architecture are used. The results are provided as follows: \n| Dataset  | S-MNIST (T=784) | Sequential CIFAR10 (T=32) | Sequential CIFAR10 (T=1024) |\n| ----------------- | --------------- | ------------------------- | --------------------------- |\n| **Training time (seconds/epoch)** ||||\n| PMSN | 35.3235 | 73.2060  | 61.7562|\n| PSN | 21.1776 | 49.0240 | 34.0634 |\n| **Testing time (seconds/epoch)**  ||||\n| PMSN | 2.6903 | 6.4175 | 4.7766 |\n| PSN | 1.4773 | 4.1030  | 2.4646  |\n| **Memory consumption (GB)**  ||||\n| PMSN | 1.4385 | 3.1369  | 9.4000|\n| PSN | 0.6130 | 1.7618  | 9.0833 |\n| **Accuracy (%)**  ||||\n| PMSN |  99.40 | 90.97 | 82.14 |\n| PSN |  97.90 | 88.45 | 55.24 | \n\nAs expected, the PMSN model requires more time and memory than the PSN model across all datasets. This increased demand is primarily attributed to a larger number of neuronal compartments being used (i.e., five in PMSN v.s. one in PSN). However, the increase in actual time and memory consumption is relatively modest, generally less than twice that of the PSN model in all experiments. This highlights the effectiveness of our proposed parallelization method. Furthermore, we believe this additional computational cost is worthwhile when considering the significantly enhanced sequential modeling capacity as demonstrated on the Sequential Cifar10 and Cifar100 datasets. \n \nTo further strengthen our manuscript, we will include these new results and discussions in our revised manuscript to provide a more comprehensive comparison between the two models."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700060786289,
                "cdate": 1700060786289,
                "tmdate": 1700060786289,
                "mdate": 1700060786289,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NwIbl42S7U",
                "forum": "FlH6VB5sJN",
                "replyto": "yJe9zKqVBF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5407/Reviewer_kRbB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5407/Reviewer_kRbB"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response. I have more questions."
                    },
                    "comment": {
                        "value": "1. The PMSN has lower FLOPs than the PSN, but its training speed is slower. Can you explain the reason?\n\n2. The PMSN consumes more memory during training. What is the number of parameters of the PMSN? \n\n3. I do not fully agree with your response about ImageNet. Although ImageNet is a static dataset, the training of SNNs on this dataset usually uses T > 1. If we use the Real-Time Recurrent Learning (RTRL) method to train SNNs on ImageNet, the accuracy also drops. We can infer that temporal credit assignment and temporal dependency modeling are also required during the training on the ImageNet dataset.\n\nMoreover, the under-fitting problem is the main issue of training SNNs on the ImageNet dataset, while the over-fitting problem is the main issue on other datasets, such as the neuromorphic CIFAR10-DVS and SHD datasets. This is also the reason why the ImageNet dataset has become a well-acknowledged benchmark in deep learning.\n\nWith 8x 3090 GPUs, dozens of epochs can be trained during the rebuttal period. Although the SNNs will not be trained well, the accuracy of the early epochs can also be reported and compared (between the PMSN and the PSN)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700152707949,
                "cdate": 1700152707949,
                "tmdate": 1700152707949,
                "mdate": 1700152707949,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "M3NgLlK99P",
            "forum": "FlH6VB5sJN",
            "replyto": "FlH6VB5sJN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5407/Reviewer_9iDX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5407/Reviewer_9iDX"
            ],
            "content": {
                "summary": {
                    "value": "The authors develop a new multi-compartment spiking neural model,\nwhich is can be efficiently computed on GPUs, because 1) feed-forward\narchitecture is assumed, so that generated spikes at one time do not\neffect the membrane potential in future and 2) the spiking activity is\ndecoupled from the membrane potential of all but the last compartment.\nThe authors show how the equations can be efficiently integrated and\ncompute on GPUs (as membrane potential evolution is just a linear\nfilter of the inputs). They compare the results on benchmarks\nrequiring long temporal integration of information, where typical SNNs\n(in particular feed-forward SNNs based on simple\nleaky-integrate-and-fire neurons) struggle, and show that the richer\ncompartmental dynamics indeed can capture long term information."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Overall, it is a solid paper that suggests a new neuron model based on\ncompartments for SNNs trained with SGD, and shows in detail how to efficiently implement the\nsimulation in forward, backward, and gradient update. The idea of\ndendritic computing is not new, however, it is largely restricted to\nbiological spiking networks instead of SNNs.\n\n* Since the added dynamics is at the neuron-level (instead of synapse\nlevel), and its efficient implementation, the additional computational\ncost for simulating (or deploying on neuromorphic hardware) is\nmanageable."
                },
                "weaknesses": {
                    "value": "* The\nimplementation seems to require feed-forward structure as well as\npositive inputs, which seems quite restrictive (in particular the\nlatter).\n\n* The neuron model does not seem to\nimprove the benchmark results dramatically over other approaches in\nall tasks (ie recurrent RNNs are similar for S-MNIST) and it thus\nremains open how useful the model will be."
                },
                "questions": {
                    "value": "* I don't follow why one can assume that $I(t)$ is always positive for\nthe ``parallel implementation of the output compartment with\nreset''. In typical SNNs, the input current is given by $I(t)=WS(t-1)$\n(without a synapse model) and the weights are positive or negative\nreal numbers, so in general, the inputs *can* be negative. Isn't that\nassumed here? However, if negative inputs are allowed, then the floor\nof the accumulated input with the threshold $\\theta$ (in Eq 15) will\nnot result in the number of spikes over the time period. Maybe I am\nmissing something here. If only positive currents are allowed, it\nshould be discussed as it seems to be a major restriction. If so it\nwould be interesting to know what the impact of this step would be (on\nthe simulation speed) if not done in parallel to allow for negative\ncurrents.\n\n* It is not clearly stated whether the flux parameters and decay rates\nof and between the compartments are learned with SGD?\n\n* In the equations (eg Eq. 3) the neural indices are written as\nsuper-script ($v^{i}$) which is easily confused with power. Better to\nuse subscripts or write $v^{(i)}$ to avoid confusion."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5407/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5407/Reviewer_9iDX",
                        "ICLR.cc/2024/Conference/Submission5407/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5407/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698772193910,
            "cdate": 1698772193910,
            "tmdate": 1700378238279,
            "mdate": 1700378238279,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wqlj5KdVOU",
                "forum": "FlH6VB5sJN",
                "replyto": "M3NgLlK99P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5407/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5407/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9iDX (1/2)"
                    },
                    "comment": {
                        "value": "> **Weakness 1 & Question 1:** The implementation seems to require feed-forward structure as well as positive inputs, which seems quite restrictive (in particular the latter). I don't follow why one can assume that $I(t)$ is always positive for the ``parallel implementation of the output compartment with reset''. \n\n**Response:** We greatly appreciate your time in reviewing our manuscript and providing constructive feedback. As you have rightly pointed out, we did assume that $I_h[t]$ to be non-negative in Equation 15. This is a compromise to enable the parallelization of the reset mechanism, and we provide a detailed proof of Equation 15 based on this assumption in Appendix A.4.2.\nHowever, it's worth mentioning that this assumption only applies to the last compartment. It does not require the input to our PMSN model, namely $I(t)$, to be always positive. In fact, the negative input current to other compartments (i.e., 1 to n-1) in Equation 5  is allowed. As specified below the Equation 3, its value is formulated as $I^l(t)=\\mathcal{W}^l S^{l-1}(t)$. Therefore, we **don\u2019t need to impose any constraint on the input signal $I(t)$ of the PMSN model**, but only apply a clamp function on the intermediate states (output of Equation 12) before it is fed into Equation 15. We will highlight this point in our revised manuscript to avoid confusion.\n\nIn the following, we would like to discuss the **impact of removing this assumption**. Firstly, according to our analysis, if negative inputs are allowed for the output compartment, the state update of the last compartment can only be done in a serial manner, while other compartments can still be computed in parallel. Following your suggestion, we have conducted an ablation study to quantify its impact on the simulation speed. The time elapse (seconds/epoch) for PMSN and last-compartment-serial PMSN are 62 vs 158 on sequential CIFAR-10 (T=32), and 73 vs 5182 on sequential CIFAR-10 (T=1024) tasks. These exploded simulation time underscore the necessity of the proposed parallel solution. Interestingly, we also observed that the last-compartment-serial PMSN model, which allows negative input for the last compartment, performs worse than the PMSN model. It exhibits a 3.98% and 4.5% accuracy drop on sequential CIFAR-10 and sequential CIFAR-100 (T=32) tasks, respectively. We suspect that negative input can result in negative membrane potentials that adversely affect the gradient flow. This is because when the membrane potential is far from the firing threshold, the gradient g\u2019[t] computed from the surrogate function will become zero. This issue hinders the effective gradient propagation, therefore, leads to poorer classification accuracies than our proposed one. \n\n&nbsp;\n\n> **Weakness 2:** The neuron model does not seem to improve the benchmark results dramatically over other approaches in all tasks (ie recurrent RNNs are similar for S-MNIST) and it thus remains open how useful the model will be.\n\n**Response:**  We appreciate your insightful comment. While it is true that the performance enhancement of our model over recurrent SNNs may not seem dramatic in the context of the S-MNIST task, it\u2019s worth noting that this task is relatively easy and models can perform well with limited memory capacity. The existing methods have already reached a very high accuracy and the margin for improvement is limited. Specifically, the state-of-the-art (SOTA) model TC-LIF with an RSNN architecture achieves an error rate of just 0.8%. Nevertheless, our model managed to reduce this error rate to 0.47% further.\n\nWe understand that the S-MNIST dataset alone may not fully demonstrate the potential benefits of our proposed model. Thus, to further illustrate the enhanced sequential modeling capabilities of our proposed model, we have extended our comparisons to include more challenging sequential tasks. In these scenarios, our model consistently outperforms other SOTA models by significant margins. For instance, in the SHD speech command recognition task, our model achieves an accuracy improvement of 1.8% and 5.3% over the SOTA Adaptive Axonal Delay (feedforward) and TC-LIF (recurrent) model respectively, while maintaining a comparable number of parameters. Moreover, in the Sequential CIFAR10 (T=32, 1024) and Sequential CIFAR100 (T=32) tasks, our model surpasses the accuracy of other SOTA models by 2.52%, 11.91%, and 3.87%, respectively. It is worth noting that, the accuracy improvements increase along with the increasing complexity of the task.\n\nWe believe these results underscore the potential of our model in handling more complex sequential modeling tasks, and we look forward to exploring its capabilities further in future research."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700059516640,
                "cdate": 1700059516640,
                "tmdate": 1700059516640,
                "mdate": 1700059516640,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TpNR8df69Y",
                "forum": "FlH6VB5sJN",
                "replyto": "M3NgLlK99P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5407/Reviewer_9iDX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5407/Reviewer_9iDX"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the detailed response and clarification. Given that the assumptions of a positive current only applies to the last compartment and not to the others and has limited  (or even detrimental) effect on accuracy, I will revise my score to 6."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700378188524,
                "cdate": 1700378188524,
                "tmdate": 1700378501332,
                "mdate": 1700378501332,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kLTWZdoBjs",
            "forum": "FlH6VB5sJN",
            "replyto": "FlH6VB5sJN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5407/Reviewer_QBv1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5407/Reviewer_QBv1"
            ],
            "content": {
                "summary": {
                    "value": "The authors define a multi-compartment spiking neuron model that is able to process signals of \"high temporal complexity\".  Importantly, this model accounts for the spike reset mechanism.  The Parallel Multi-Compartment Spiking Neuron is inspired by hippocampal pyramidal neurons and admits a formulation that facilitates parallel training on GPUs.  The authors demonstrate the model's advantages in terms of performance and complexity."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The model and the approximations/trade-offs made are well-presented and clear.  Capturing complex temporal sequences is also a useful property.  A GPU accelerated model is of course an advantage in the modern day with the ubiquity of such hardware."
                },
                "weaknesses": {
                    "value": "The model is very specific, without any particular justification.  When one uses spiking networks, it is usually (in my experience) because one wants to model a spiking neuron system, but then choices of compartments are no longer arbitrary.  This is really a question of motivation for the paper that seems missing to me.  What problem are the authors solving?  Why does one need a spiking neuron model? Or is this particular spiking neuron model related to one in common use in circumstances unfamiliar to me?  If it is already related to a model in common use, then having a fast implementation is of some importance.\n\nA related issue is that the models used for comparisons seem very limited (if one is only concerned with whether the model is a spiking model).  Many groups have been training spiking neural networks for a long time (Zenke & Ganguli 2018 is but one example).  If the multi-compartment model itself is not of particular importance, but I needed to train a spiking model, I could just as well use a network model from one of these groups.  Does the PMCS have advantages over those models?"
                },
                "questions": {
                    "value": "The main question I have (from above) is why this particular model?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5407/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5407/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5407/Reviewer_QBv1"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5407/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698805309482,
            "cdate": 1698805309482,
            "tmdate": 1699636548009,
            "mdate": 1699636548009,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "69eihbMYQD",
                "forum": "FlH6VB5sJN",
                "replyto": "kLTWZdoBjs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5407/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5407/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer QBv1"
                    },
                    "comment": {
                        "value": "**Response:**  We appreciate your comments and would like to address your concerns with our following responses:\n\n&nbsp;\n\n> **Weakness 1:**  The model is very specific, without any particular justification. When one uses spiking networks, it is usually (in my experience) because one wants to model a spiking neuron system, but then choices of compartments are no longer arbitrary.\n\n**Background \u2013** Different from the studies in computational neuroscience that aim to simulate and study the dynamics of biologically realistic spiking neuronal systems, this work lies in the field of neuromorphic computing. In particular, we aim to design brain-inspired neural architecture that can solve real-world pattern recognition tasks with improved energy efficiency and effectiveness. To this end, simplified phenomenological spiking neuron models are typically used (e.g., Leaky Integrate-and-Fire (LIF) model), which allow SNNs to be implemented in silico with extremely low energy cost. In this context, the choice of compartments in our model is also driven by these goals. We trade off its computational efficiency and efficacy in sequential modeling tasks, making the model well-suited for processing complex temporal signals with low-energy requirements.\n\n&nbsp;\n\n> **Weakness 2:** This is really a question of motivation for the paper that seems missing to me. What problem are the authors solving? Why does one need a spiking neuron model? Or is this particular spiking neuron model related to one in common use in circumstances unfamiliar to me?\n\n**Research Gap \u2013** While SNNs have achieved substantial performance improvements for tasks with limited temporal dynamics, such as image classification, their performance in handling tasks with long sequence lengths and rich temporal dynamics (e.g., audio signal processing) remains limited. This is attributed to the limited memory capacity and neuronal dynamics offered by the commonly used LIF model, which is also the one used in **(Zenke & Ganguli 2018)**. This oversimplified model, while computationally efficient, exhibits limited capacity in representing the rich neuronal dynamics of biological neurons. To bridge this gap, we design a memory-enhanced multi-compartment spiking neuron model in this work. Our model specifically addresses these limitations by leveraging the interplay among neuron compartments. \n\n&nbsp;\n\n> **Weakness 3:** If it is already related to a model in common use, then having a fast implementation is of some importance.\n\n**Novelty -** Our model draws inspiration from the multi-compartment model used to describe biological neurons. As mentioned by Reviewer 9iDX *``This model is not new, however, it is largely restricted to biological spiking networks instead of SNNs\u2019\u2019.* In the context of SNNs, our proposed model is the **first** to incorporate more than two compartments, demonstrating superior performance in sequential modeling. Furthermore, we introduce a GPU-accelerated solution to enhance the simulation speed of the proposed model. This is also the **first** multi-compartment model in SNNs that supports parallel training on GPUs. \n\n&nbsp;\n\n> **Weakness 4:** A related issue is that the models used for comparisons seem very limited (if one is only concerned with whether the model is a spiking model). Many groups have been training spiking neural networks for a long time (Zenke & Ganguli 2018 is but one example).\n\n**Outstanding Sequential Modelling Capacity -** In our experiment section, we aim to provide a holistic view of the existing SNN research by including all the state-of-the-art memory-enhanced spiking neuron models that have been studied earlier, including the LIF model that had been used in **(Zenke & Ganguli 2018)**. Our analysis suggests that the proposed multi-compartment model can not only be trained in parallel, but also excel in modeling multi-scale temporal dependencies, thereby bridging the performance gap between biological neuron and spiking neuron models in terms of the sequential modeling capacity.\n\n&nbsp;\n\n> **Weakness 5:** If the multi-compartment model itself is not of particular importance, but I needed to train a spiking model, I could just as well use a network model from one of these groups. Does the PMCS have advantages over those models?\n\n**Practicality \u2013** As discussed above, if you're seeking to train an SNN to process temporal signals with long sequence length and rich temporal dynamics (e.g., in scenarios like speech processing), our model presents an excellent choice due to its enhanced sequential modeling capacity and rapid training speed."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700057624357,
                "cdate": 1700057624357,
                "tmdate": 1700057624357,
                "mdate": 1700057624357,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TMo3LadgbJ",
            "forum": "FlH6VB5sJN",
            "replyto": "FlH6VB5sJN",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5407/Reviewer_cmkV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5407/Reviewer_cmkV"
            ],
            "content": {
                "summary": {
                    "value": "Simplified spiking neuron models, such as LIF, have trouble with multi-scale sequence modeling, or learning tasks involving learning complex temporal dynamics. While others have developed methods to account for this, training speed and performance is still relatively subpar. In this paper, the authors utilize biological inspiration from hippocampus pyramidal neurons and their multi-compartmental modeling to design a new generalized multi-compartment model which allows for arbitrary numbers of compartments. They also introduce a parallel implementation of this model for faster training on GPU accelerated hardware while accounting for the reset mechanism, unlike previous works."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) The new multi-compartment model performs better than other memory-enhanced models and single compartment models on sequence modeling tasks with relatively similar parameter counts on standard benchmarks.\n2) The topic of better exploiting temporal dynamics during SNN training is important for their development.\n3) The parallel implementation, especially accounting for reset, is an important and useful contribution as SNNs are generally slow to train and have yet to be parallelized effectively."
                },
                "weaknesses": {
                    "value": "1) Regarding Figure 4: while the parallel model was compared to its serial implementation in terms of speed up, what is the ratio/training time difference between PMSN and PSN? If PSN is faster, since it is only a single compartment model, how significant is this difference? I am not referring to the computational cost but instead training acceleration only.\n2) Is the parallel implementation/methodology that incorporates the reset mechanism general enough to be applied to single compartment models?"
                },
                "questions": {
                    "value": "Please see above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5407/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698878474359,
            "cdate": 1698878474359,
            "tmdate": 1699636547925,
            "mdate": 1699636547925,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "k97lvHTkQF",
                "forum": "FlH6VB5sJN",
                "replyto": "TMo3LadgbJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5407/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5407/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer cmkV"
                    },
                    "comment": {
                        "value": "> **Weakness 1:** Regarding Figure 4: while the parallel model was compared to its serial implementation in terms of speed up, what is the ratio/training time difference between PMSN and PSN? If PSN is faster, since it is only a single compartment model, how significant is this difference? I am not referring to the computational cost but instead training acceleration only.\n\n**Response:** Thanks for your insightful comment. Following your suggestions, we have conducted a comparative analysis of the training speed between the PMSN and PSN models. This involves measuring each model's training time across various tasks with different time window lengths, with identical training configurations and network architectures for a fair and direct comparison. The results, including time elapsed (seconds/epoch) and classification accuracy (%), are as follows:\n\n| Dataset | S-MNIST (T=784) | Sequential CIFAR10 (T=32) | Sequential CIFAR10 (T=1024) | \n| ------- | --------------- | ------------------ |-------------------- | \n| Speed (PMSN) | 35.3235 | 61.7562 | 73.2060 | \n| Accuracy (PMSN) | 99.40% | 90.97% | 82.14% | \n| Speed (PSN) | 21.1776 |  34.0634 | 49.0240 |\n| Accuracy (PSN) | 97.90% | 88.45% | 55.24% | \n\nThe results clearly show that the PMSN model, with more complex and sophisticated multi-compartment dynamics (five compartments in PMSN vs. one in PSN), trains slightly slower than the PSN model. However, the difference in training speed is not significant (with a ratio of less than 2 across all the experiments), demonstrating the efficiency of our parallelization approach. Furthermore, this minor trade-off in speed is outweighed by the superior sequential modeling capabilities of PMSN, particularly evident in its performance on the Sequential CIFAR-10 dataset. This underlines the value of the PMSN model in applications where enhanced temporal signal processing capacity is critical.\n\n&nbsp;\n\n> **Weakness 2:** Is the parallel implementation/methodology that incorporates the reset mechanism general enough to be applied to single compartment models?\n\n**Response:** We appreciate your insightful question. This reset mechanism can indeed be seamlessly transferred to the single-compartment Integrate-and-Fire (IF) model, thereby enabling parallelizable training for this model. The generalization is straightforward and involves a simple modification. Specifically, it requires replacing the input of Equation 13, denoted as $I_h[t]$, with the input of the single-compartment model. For instance, the replaced input can be denoted as $I^l[t]=\\mathcal{W}^l S^{l-1}[t]$, which adheres to a common practice of single-compartment models regardless of the dynamics of synaptic decay. Consequently, Equations 13 and 14 effectively formulate a parallel IF model that incorporates our reset mechanism.\n\n&nbsp;\n\nWe sincerely hope that our response has addressed your concerns. We will include these additional results in the revised manuscript and Appendix."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5407/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700056147343,
                "cdate": 1700056147343,
                "tmdate": 1700056147343,
                "mdate": 1700056147343,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]