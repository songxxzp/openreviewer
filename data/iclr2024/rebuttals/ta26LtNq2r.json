[
    {
        "title": "Learning to Reject for Balanced Error and Beyond"
    },
    {
        "review": {
            "id": "0631RSpvPM",
            "forum": "ta26LtNq2r",
            "replyto": "ta26LtNq2r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6368/Reviewer_91tX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6368/Reviewer_91tX"
            ],
            "content": {
                "summary": {
                    "value": "This paper works on learning to reject (L2R) under long-tailed scenarios. They find that Chow's rule is suboptimal for this setting and propose a new approach with theoretical guarantees. Extensive experiments on benchmark datasets validate the effectiveness of the proposed approach."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is very well written. Even those unfamiliar with L2R can easily understand the setting.\n- The setting is novel and meaningful, since the long-tailed setting is quite common but rarely studied in the context of L2R.\n- The proposed method is novel and interesting, and the theoretical results are not trivial."
                },
                "weaknesses": {
                    "value": "- If I understand correctly, the solution of the optimization problem in equation (9) seems to be heuristic. Is $\\mu$ not optimized, but selected from a predefined set? I am afraid that such solutions may not lead to an optimal solution. Will the authors provide some theoretical or empirical analysis of this problem and prove that the solution can be close to optimal?\n\n- What is the candidate value of $\\mu$? The authors say that they propose to do a grid search over $\\mu$, but what's the search space? If the search space is very large, the speed of the algorithm may be slow.\n\n- In the experimental section, only two compared methods are used. I am not sure if more compared methods are needed to validate the effectiveness of the proposal."
                },
                "questions": {
                    "value": "Please see the \"Weaknesses\" section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6368/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6368/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6368/Reviewer_91tX"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6368/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698206424562,
            "cdate": 1698206424562,
            "tmdate": 1700636808612,
            "mdate": 1700636808612,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "b6v9HYined",
                "forum": "ta26LtNq2r",
                "replyto": "0631RSpvPM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6368/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6368/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 91tX"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their encouraging comments and questions.\n\n> What is the candidate value of $\\mu$? The authors say that they propose to do a grid search over $\\mu$, but what's the search space? If the search space is very large, the speed of the algorithm may be slow. \n\nThe reviewer is correct that we apply a grid search to choose $\\mu$. However, the search space we use is very small, as detailed below:\n\n- First, using the re-parametrization trick described in Appendix C.1, for $K$ groups, we only need to search over $K-1$ multiplier parameters. For our experiments, we consider settings with two groups (head and tail), requiring us to tune **only one** multiplier parameter.\n\n- Second, even for this single parameter, we find sufficient to use a **small search space**. In fact, as noted in Appendix D.3, we pick the multiplier from the small range {1, 6, 11}. \n\n- Third, recall that ours is a post-hoc approach that constructs a simple rejection rule from a **pre-trained base model**. When varying the multiplier parameter, we are merely changing the rejection rule in Equation 15 without re-training the base model. Therefore carrying out a search over multipliers can be implemented efficiently when $K$ is small.\n\n> If I understand correctly, the solution of the optimization problem in equation (9) seems to be heuristic. Is not optimized, but selected from a predefined set? I am afraid that such solutions may not lead to an optimal solution. Will the authors provide some theoretical or empirical analysis of this problem and prove that the solution can be close to optimal? \n\nTo confirm that it suffices to use a small search space for the multipliers, we repeated the CIFAR-100 experiments by picking the multiplier from a fine-grained grid {0.25, 0.5, 0.75, \u2026, 11}. We additionally replaced our heuristic procedure in Section 4.2 with a grid search over group prior $\\alpha$, choosing the head-to-tail ratio from the range {0, 0.004, 0.008, \u2026, 0.996, 1.0}. We pick the parameter *combination* that yields the lowest balanced error on a held-out validation set\n\nWe show below the **balanced error**  on the test and validation samples, with *lower* being *better* (the numbers are slightly different from the paper because they are evaluated on different validation-test splits):\n\\begin{aligned}\n&~~~~\\text{Test} &\\text{Validation} \\\\\\\\\n\\text{Chow's rule}~~~~ &~~~~ 0.512 \\pm 0.004 &0.498 \\pm 0.011 \\\\\\\\\n\\text{Plug-in [Balanced, heuristic]}~~~~ &~~~~  0.291 \\pm 0.008 &0.282 \\pm 0.007\\\\\\\\\n\\text{Plug-in [Balanced, grid search]}~~~~ &~~~~ 0.284 \\pm 0.007 &0.264 \\pm 0.008\n\\end{aligned}\n\nAlthough the grid search provides gains on the validation set, on the test set, our heuristic approach to picking the multipliers and group priors yields metrics that are only slightly worse than grid-search. Moreover, the differences are within standard errors. \n\n> In the experimental section, only two compared methods are used. I am not sure if more compared methods are needed to validate the effectiveness of the proposal.\n\nTo the best of our knowledge, there are **no prior L2R methods that can handle general (non-decomposable) evaluation metrics**. We therefore compare our approach against **representative** methods that minimize the standard classification error.\n\nThe reason we do not pick an exhaustive list of methods from the L2R literature is because most of these methods **share the same optimal solution** (Equation 2), which as discussed in Section 3 (Remark 1), is notably different from the optimal solution for the balanced or worst-group errors.\n\nWe believe the two baselines we compare against represent **two broad categories** of L2R approaches, namely post-hoc methods (represented by Chow\u2019s rule), and loss-based methods (represented by by the CSS baseline of Mozannar & Sontag (2020))."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6368/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700618352232,
                "cdate": 1700618352232,
                "tmdate": 1700634125163,
                "mdate": 1700634125163,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sH6e456aH2",
                "forum": "ta26LtNq2r",
                "replyto": "b6v9HYined",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6368/Reviewer_91tX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6368/Reviewer_91tX"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply! My concern is resolved and I am willing to increase my score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6368/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636791387,
                "cdate": 1700636791387,
                "tmdate": 1700636791387,
                "mdate": 1700636791387,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sjadiBHyEg",
            "forum": "ta26LtNq2r",
            "replyto": "ta26LtNq2r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6368/Reviewer_RZqs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6368/Reviewer_RZqs"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the problem of classification with rejection when data distribution is long-tailed. In this case, metrics considering the data distribution such as balanced error is more appropriate than traditional misclassification error. For the case when learning a classifier and a separate rejector, authors derive corresponding Bayes optimal solutions for the problem setting, showing many existing methods are not optimal for metrics such as balanced error. Authors then propose a modification method based on the derived Bayes optimal solution, and empirically show its superiority."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- The paper considers an important problem setting combination, learning with rejection and class-imbalanced distribution.\n- The analysis is rigorously conducted with high quality and significance.\n- The paper is overall well structured and well written. It is easy and joyful to follow.\n- The paper provide appropriate literature reivew for related work."
                },
                "weaknesses": {
                    "value": "Limit of the proposed framwork is not properly addressed. For example, are there any popular metrics not covered?"
                },
                "questions": {
                    "value": "This paper considers the learning setting where a classifer and a separate rejector function exist. There is also another framework learning solely one classifier and later uses its output distribution over labels to conduct abstention. Is it impossible for this approach to derive Baye optimal solutions, or it is simply not the scope of the paper?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6368/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698502374472,
            "cdate": 1698502374472,
            "tmdate": 1699636703746,
            "mdate": 1699636703746,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2f80lfVJx2",
                "forum": "ta26LtNq2r",
                "replyto": "sjadiBHyEg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6368/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6368/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RZqs"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the encouraging comments.\n\n> are there any popular metrics not covered?\n\nWe are currently able to minimize the **balanced error** and the **worst-group error**. Using techniques similar to Yang et al. (2020), our approach can be extended to handle metrics of the form  $\\psi\\left(e_1(h, r), \\ldots, e_K(h, r)\\right)$ in Secton 5, provided $\\psi$ is either *convex* or *fractional-linear*. Metrics which do not fall under this family cannot be directly handle by our framework.   \n\n> This paper considers the learning setting where a classifer and a separate rejector function exist. There is also another framework learning solely one classifier and later uses its output distribution over labels to conduct abstention. Is it impossible for this approach to derive Baye optimal solutions, or it is simply not the scope of the paper?\n\nThanks for the question. If we understand, the reviewer is referring to settings where the rejector $r(x)$ is strongly coupled with the underlying classifier $h(x)$ or scorer $f(x)$, e.g., is given by the margin or softmax probability of the predicted label.\n\nOne can in fact strengthen Equation 2 to derive the optimal choice of $r$ given a **fixed $h$**.  Our main results (e.g., Theorem 2) can similarly be strengthened, and cover the case where one does not jointly optimise over both $h$ and $r$. For a fixed classifier $h$, the optimal rejector in Theorem 2 will take the form:\n\n$$\nr\\^\\*(x) = 1 \\~\\~\\\\iff\\~\\~ \n\\frac{1}{\\alpha^*_{[h(x)]}} \\cdot \\eta_{h(x)}(x)\n<\n\\sum_{y' \\in [L]}  \\left( \\frac{1}{\\alpha^*_{ [y'] }}  - \\mu^*_{ [y'] } \\right) \\cdot \\eta_{y'}(x) - c\n$$\n\nOf course, if one does not choose $r$ in an optimal manner, this could lead to a globally sub-optimal solution. We can nonetheless add a discussion of this point."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6368/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700617883403,
                "cdate": 1700617883403,
                "tmdate": 1700617883403,
                "mdate": 1700617883403,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gpbxSXFs5b",
                "forum": "ta26LtNq2r",
                "replyto": "2f80lfVJx2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6368/Reviewer_RZqs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6368/Reviewer_RZqs"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clear response.\n\nMy question was on the case of optimising strongly coupled $h(x)$ and $r(x)$ at the same time. I think this can be somehow covered by the main results of the manuscript. It is also nice to see the optimal rejector for a fixed $h(x)$, which may benefit some post hoc situations."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6368/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700620476638,
                "cdate": 1700620476638,
                "tmdate": 1700620476638,
                "mdate": 1700620476638,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ab5TD8JPoW",
                "forum": "ta26LtNq2r",
                "replyto": "sjadiBHyEg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6368/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6368/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to follow-up question"
                    },
                    "comment": {
                        "value": "This is indeed an interesting question! The reviewer is correct that our results can be extended to handle joint optimization of the classifier and rejector. \n\nOne possible approach is to formulate the Lagrangian for the constrained optimization problem in Equation (9), and replace the indicators in the resulting Lagrangian objective with a surrogate loss function. We may consider the joint losses proposed recently by Mozannar & Sontag (2020) or Verma & Nalisnick (2022) as candidate surrogate losses. The resulting optimization would involve a maximization over the Lagrange multipliers and a minimization over the classifier-rejector parameters.\n\nWe will be happy to include a discussion on this in the paper and mention it as a part of our future work."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6368/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621657850,
                "cdate": 1700621657850,
                "tmdate": 1700621670310,
                "mdate": 1700621670310,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1U7AkgOt2i",
                "forum": "ta26LtNq2r",
                "replyto": "Ab5TD8JPoW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6368/Reviewer_RZqs"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6368/Reviewer_RZqs"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your prompt response. I am happy to be helpful on improving the manuscript."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6368/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621848364,
                "cdate": 1700621848364,
                "tmdate": 1700621848364,
                "mdate": 1700621848364,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "B2uiISOBvH",
            "forum": "ta26LtNq2r",
            "replyto": "ta26LtNq2r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6368/Reviewer_iFdp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6368/Reviewer_iFdp"
            ],
            "content": {
                "summary": {
                    "value": "This paper derives and examines the Bayes optimal solution to minimizing the balanced error metric when a reject option is available (rejection is assumed to have a fixed cost). This problem is much more challenging than learning to reject for plain error because, as the authors show, the rejection thresholds for the various classes are coupled. Moreover, no reduction to cost sensitive learning with fixed per-class misclassification costs exist, since these costs change depending on the fraction of examples in each class not rejected.\n\nGiven the challenges, the authors present an approximate scheme for minimizing balanced-error with a reject option, and they compare its effectiveness to recent methods for learning to reject on 3 image classification datasets. The proposed method out-performs or matches the compared-to methods across all datasets.\n\nAdditionally, the authors formulate the Bayes optimal solution for minimum balanced error rate with rejection to one for any metric that is a function of the per-class error rates. This allows for minimizing the maximum per-class error rate, and the G-mean metric; as well as for balancing error rates across an arbitrary partition of classes -- e.g., balancing the error rate across the set of frequent classes and the tail classes."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "This is the first paper to formulate the Bayes optimal solution for minimum balanced error classification with a rejection option, and to provide a means of optimizing it. The problem itself is rather significant since there are high stakes scenarios in which it's desirable for a classifier to perform equally well across groups, and for a rejection or abstention option to be available so that cases can be escalated to humans for more careful consideration.\n\nThe experiments reasonably demonstrate the method's effectiveness, and the authors generalize the form of the given solution to metrics other than balanced accuracy."
                },
                "weaknesses": {
                    "value": "The major weakness of this paper is that the proposed optimization scheme is only approximate and an analysis of the gap is not given. Is it possible to say how much grid search over the space of lagrange multiplers is needed to find a good solution? Also, the proposed optimization scheme finds a fixed point of the alpha parameters for given values of lagrange multipliers. But is this fixed point unique? And if not, are they all equally good? These questions are not addressed in the paper."
                },
                "questions": {
                    "value": "1) Is it possible to say how much grid search over the space of lagrange multiplers is needed to find a good solution?\n\n2) the proposed optimization scheme finds a fixed point of the alpha parameters for given values of lagrange multipliers. But is this fixed point unique, and if not, are they all equally good?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6368/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698737824902,
            "cdate": 1698737824902,
            "tmdate": 1699636703624,
            "mdate": 1699636703624,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PTnmdAKpXK",
                "forum": "ta26LtNq2r",
                "replyto": "B2uiISOBvH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6368/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6368/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer iFdp"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the encouraging comments and questions.\n\n> Is it possible to say how much grid search over the space of lagrange multiplers is needed to find a good solution? \n\nUsing the re-parametrization trick described in Appendix C.1, for $K$ groups, we only need to search over $K-1$ multiplier parameters. For our experiments, we consider settings with two groups (head and tail), requiring us to tune **only one** multiplier parameter. In our experiments, we find it sufficient to pick this parameter from a small set  {1, 6, 11}. We tried a grid search over a fine-grained range but found the gains to be small. Please feel free to also see response to Reviewer 91tX.\n\n> the proposed optimization scheme finds a fixed point of the alpha parameters for given values of lagrange multipliers. But is this fixed point unique, and if not, are they all equally good?\n\nThe proposed optimization scheme was mainly intended as a heuristic to pick the group prior for the plug-in estimator. To confirm its efficacy, we repeated the CIFAR-100 experiments replacing our heuristic optimization scheme for picking $\\alpha$ with a **fine-grained grid search**.  Specifically, we chose the head-to-tail ratio from the range  {0, 0.004, 0.008, \u2026, 0.996, 1.0}\nand the multiplier from the wider range {0.25, 0.5, ..., 11} to yield the minimum balanced error on a held-out validation set. \n\nWe show below the **balanced error**  on the test and validation samples, with *lower* being *better* (the numbers are slightly different from the paper because they are evaluated on different validation-test splits):\n\\begin{aligned}\n&~~~~\\text{Test} &\\text{Validation} \\\\\\\\\n\\text{Chow's rule}~~~~ &~~~~ 0.512 \\pm 0.004 &0.498 \\pm 0.011 \\\\\\\\\n\\text{Plug-in [Balanced, heuristic]}~~~~ &~~~~  0.291 \\pm 0.008 &0.282 \\pm 0.007\\\\\\\\\n\\text{Plug-in [Balanced, grid search]}~~~~ &~~~~ 0.284 \\pm 0.007 &0.264 \\pm 0.008\n\\end{aligned}\n\nAlthough the grid search provides gains on the validation set, on the test set, our heuristic approach yields metrics that are only slightly worse than grid-search. Moreover, the differences are within standard errors."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6368/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700617289478,
                "cdate": 1700617289478,
                "tmdate": 1700634150456,
                "mdate": 1700634150456,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FGWorNDuw4",
                "forum": "ta26LtNq2r",
                "replyto": "PTnmdAKpXK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6368/Reviewer_iFdp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6368/Reviewer_iFdp"
                ],
                "content": {
                    "title": {
                        "value": "Reply to authors response"
                    },
                    "comment": {
                        "value": "I have read the authors' response to my review.\n\nThank you for running the expanded grid search to further evaluate the power iteration-like heuristic. Adding these results to the appendix would be helpful.\n\nI will keep my review as is."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6368/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722171889,
                "cdate": 1700722171889,
                "tmdate": 1700722171889,
                "mdate": 1700722171889,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "a88oEOQJMp",
            "forum": "ta26LtNq2r",
            "replyto": "ta26LtNq2r",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6368/Reviewer_RJqn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6368/Reviewer_RJqn"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript considers the problem of learning optimal classifiers with reject option, i.e., there's a certain budget, and a fixed cost, for refraining from making any predictions on test instances. The problem of 'learning to reject' has been studied before, but mostly in the setting when the performance metric of interest is classification accuracy (or 0-1 loss). In this work, the authors consider more general and practical performance measures, balanced error and worst-group error in particular. The authors derive Bayes optimal classifiers (and rejectors) for the general measures, and provide a practical algorithm for estimating them with samples. \n\nTwo key contributions: 1) Clear formulations of the learning to reject problem, various metrics of interest, and deriving Bayes optimal forms for the different metrics. While general performance measures have been considered before in different supervised learning settings, deriving general results for classifiers with reject option is novel as far as I can tell. 2) Algorithm for estimating the classifiers and rejectors in practice for general performance measures, and supportive empirical results on standard datasets.\n\nOverall, I find the paper to be sufficiently interesting to any ML audience, with technical depth and contributions par for the venue. I've some questions for the authors listed below, but the paper is a clear accept for me."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "As I mentioned in my summary the paper makes key contributions, and each of the contributions involves good technical rigor and presentation which I very much appreciate: 1) Clear formulations of the learning to reject problem, various metrics of interest, and deriving Bayes optimal forms for the different metrics. While general performance measures have been considered before in different supervised learning settings, deriving general results for classifiers with reject option is novel as far as I can tell. 2) Algorithm for estimating the classifiers and rejectors in practice for general performance measures, and supportive empirical results on standard datasets.\n\nThe paper is well-written, ideas flow coherently, related results are placed well in context, and it was a pleasure to read!"
                },
                "weaknesses": {
                    "value": "These are more nit picks than weaknesses.\n1. Some intuitive justification for the Bayes optimal forms would have been useful. As it stands, the material is somewhat dense, and for a reader outside the area, some of these results might be confusing. For instance, I would love to know if there's an intuitive explanation for the 'discount' factor $-\\mu^*_{[y']}$ in (2). \n2. I found it a bit of a leap to go from deterministic classifiers in Theorem 2 to stochastic classifiers in Theorem 5. Balanced error does involve weights that are distribution dependent, while general $\\psi$ metrics studied in Theorem 5 I believe are functions that do not depend on the distribution in any way other than via the arguments $e_j(h,r)$'s. So where is the stochasticity arising from? Also, it would be good to draw corollaries where for some simple forms of \\psi functions, you indeed get deterministic classifiers and rejectors (when $h^{(1)} = h^{(2)}$ holds, etc.)"
                },
                "questions": {
                    "value": "I would like the authors to address the questions in the 'weaknessess' section in their rebuttal."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6368/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6368/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6368/Reviewer_RJqn"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6368/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698941026957,
            "cdate": 1698941026957,
            "tmdate": 1699636703487,
            "mdate": 1699636703487,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "INNOg5xpuf",
                "forum": "ta26LtNq2r",
                "replyto": "a88oEOQJMp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6368/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6368/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RJqn"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the encouraging and detailed comments.\n\n> Some intuitive justification for the Bayes optimal forms would have been useful. For instance, I would love to know if there's an intuitive explanation for the 'discount' factor\n\nThanks for the suggestion. To build intuition for the Bayes optimal rejector, let\u2019s consider a simpler risk with a fixed cost of $\\beta_y$ for errors on class y:\n\n$\\min_{h, r}~    \\sum_i \\beta_i \\cdot \\mathbb{P} \\left( y = i, h(x) \\ne y, r(x) = 0 \\right)  + c \\cdot \\mathbb{P}\\left( r( x ) = 1 \\right)$\n\nThe Bayes-optimal rejector for this risk is of the form:\n\n$$r\\^\\*(x) = 1 \\~\\~\\\\iff\\~\\~ \\\\max\\_y \\\\beta\\_y \\\\cdot \\\\eta_y(x) < \\\\sum\\_i \\\\beta\\_i \\\\cdot \\\\eta\\_i(x) - c~~~~~~(i)$$\n\nOne may consider the term $\\max_y \\beta_y  \\cdot \\eta_y(x)$ as a measure of classifier confidence, and the right-hand side as an instance dependent threshold. When $\\beta_y = 1, \\forall y$, Equation (i) is the same as Chow\u2019s rule: $\\~\\~r\\^\\*(x) = 1 \\~\\~\\\\iff\\~\\~ \\\\max\\_y \\\\eta_y(x) < 1 - c$.\n\nNote that the rejector $r^*$ is unconstrained in what rejection rate it has on individual classes. Suppose we now additionally constraint the rejector to satisfy a particular rejection rate for each class:\n\n$$\\\\min\\_{h, \\~r}\\~    \\sum_i \\beta_i \\cdot \\mathbb{P} \\left( y = i, h(x) \\ne y, r(x) = 0 \\right)  + c \\cdot \\mathbb{P}\\left( r( x ) = 1 \\right)$$\n\n$$\\\\text\\{s.t.\\}\\~\\~ \\\\mathbb\\{P\\}\\\\left( r( x ) = 1, y = i \\\\right) = B_i, \\\\forall i,$$\n\nfor budgets $B_1, \\ldots, B_L$.\n\nIn this case, the optimal rejector (under additional distributional assumptions) takes the form:\n$$r\\^\\*(x) = 1 \\~\\~\\\\iff\\~\\~ \\\\max\\_y \\beta_y \\cdot \\eta_y(x) < \\sum_i (\\beta_i - \\mu_i) \\cdot \\eta_i(x) - c~~~~~~(ii),$$\nwhere the discount factors $\\mu_i$s ensure that the rejector satisfies the budget constraints. The optimal rejector again uses the term $\\max_y \\beta_y \\cdot \\eta_y(x)$ to measure confidence, but differs in how it thresholds the confidence measure. For example, in the case of binary labels, the rejector can be seen as applying a different constant threshold for each predicted class (see e.g. Corollary 3).\n\nWe will be happy to add this discussion to the paper.\n\n> I found it a bit of a leap to go from deterministic classifiers in Theorem 2 to stochastic classifiers in Theorem 5.\n\nWe would first like to clarify that the balanced error is a **special case** of the formulation in Equation 16 for $\\psi(z_1, \\ldots, z_K) = \\frac{1}{K} \\sum_k z_k$. In this case, $\\psi$ happens to be a simple **linear** function, and the optimal solution is deterministic. When $\\psi$ is a general **non-linear** function (e.g. the worst-group error), the optimal solution admits a stochastic form. \n\nThe technical details of how we arrive at a stochastic solution are provided in Appendix B. The key point is that in order to solve Equation 16, we formulate an equivalent optimization problem over the **space of confusion matrices** $\\mathcal{C}$ (see Appendix B.1 and Equation 28). In turn, attaining the optimal confusion matrix requires the use of randomization over $(h, r)$ pairs.\n\nWhen $\\psi$ is linear, as is the case with the balanced error, the optimal solution is a **boundary point** in $\\mathcal{C}$, and therefore a deterministic $(h, r)$ pair. When $\\psi$ is non-linear, the optimal solution can be an **interior point**, and therefore require randomizing between two $(h, r)$ pairs.\n\nWe will be happy to provide more details when transitioning from deterministic to stochastic rejectors.\n\n> Also, it would be good to draw corollaries where for some simple forms of \\psi functions, you indeed get deterministic classifiers and rejectors\n\nThanks for the suggestion. When $\\psi(z_1, \\ldots, z_K) = \\sum_k \\beta_k z_k$ is a linear function with coefficients $\\beta_k \\in \\mathbb{R}$, the optimal solution is the deterministic pair $(h, r)$ given in Theorem 9 in the appendix. In this case, indeed $h^{(1)} = h^{(2)}$ and $r^{(1)} = r^{(2)}$. We will add a note on this in the paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6368/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700616000494,
                "cdate": 1700616000494,
                "tmdate": 1700618621530,
                "mdate": 1700618621530,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VXIBG1Ej1L",
                "forum": "ta26LtNq2r",
                "replyto": "INNOg5xpuf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6368/Reviewer_RJqn"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6368/Reviewer_RJqn"
                ],
                "content": {
                    "title": {
                        "value": "Thanks!"
                    },
                    "comment": {
                        "value": "Thank you for the clarifications. It would be helpful to include some of these in the main text.\nI really like this work!"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6368/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700636042303,
                "cdate": 1700636042303,
                "tmdate": 1700636042303,
                "mdate": 1700636042303,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]