[
    {
        "title": "Large Language Models Cannot Self-Correct Reasoning Yet"
    },
    {
        "review": {
            "id": "r40GN1Wttt",
            "forum": "IkmD3fKBPQ",
            "replyto": "IkmD3fKBPQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2828/Reviewer_oT3c"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2828/Reviewer_oT3c"
            ],
            "content": {
                "summary": {
                    "value": "Large Language Models (LLMs) have been increasingly capable. However, they still make many mistakes. Recent work has explored the idea of \u201cself-correction\u201d where LLMs refine their responses based on feedback to their previous outputs. This paper critically examines the role and efficacy of self-correction within LLMs. Central to the investigation is the definition of intrinsic self-correction, whereby an LLM attempts to correct its initial responses based solely on its inherent capabilities, with no external feedback. The paper finds that LLMs struggle to self-correct their responses for reasoning tasks without external feedback, and at times, their performance might even degrade post self-correction."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. Contrary to prior results, the paper finds the self-correct methods in prior research such as Kim et al. (2023); Shinn et al. (2023) make use of oracle labels to guide the self-correction process. \n\n2. For self-correction through multi-agent debate (Du et al., 2023; Liang et al., 2023) to improve reasoning where multiple instances of an LLM critique each other\u2019s responses, the paper's results reveal that its efficacy is no better than self-consistency when considering an equivalent number of responses, highlighting the limitations of such an approach."
                },
                "weaknesses": {
                    "value": "1. Intrinsic self-correction, defined as the model endeavors to rectify its initial responses based solely on its inherent capabilities without the crutch of external feedback, is not very clear to me. Does recall examples from parameter knowledge (see paper below) considered intrinsic self-correction? \nLarge Language Models as Analogical Reasoners\nhttps://arxiv.org/pdf/2310.01714\n\n2. It is not clear which prior papers have the various problems exposed. It would be very helpful to put them in a table. Furthermore, please provide details on where prior methods rely on oracle labels, specific problems on poorly constructed pre-prompts, specific benchmark results that are wrong. For example, I do not seem to locate which part Shinn et al. (2023) have the problems."
                },
                "questions": {
                    "value": "The paper exposes an intriguing problem in prior work on self-correction of LLMs that shows, to the contrary that LLMs can not self-correct reasoning yet.\n\nHowever, it is still not clear how we should think about all the techniques to improve reasoning without external feedback. Does breakdown a problem into sub-problem and do step-wise verification paired with self-consistency considered self-correction? It would be very helpful to put all these techniques into perspective."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2828/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698554889229,
            "cdate": 1698554889229,
            "tmdate": 1699636226491,
            "mdate": 1699636226491,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TYi3eU7iVG",
                "forum": "IkmD3fKBPQ",
                "replyto": "r40GN1Wttt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2828/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2828/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer oT3c"
                    },
                    "comment": {
                        "value": "Thank you for your positive review and the valuable points raised. We would like to address each of your questions.\n\n>**1. Intrinsic Self-Correction and Analogical Prompting:**\n\nThank you for your question. Analogical prompting, which involves recalling relevant problems and solutions from parameter knowledge, is a form of pre-hoc prompting. In contrast, self-correction represents a type of post-hoc prompting (see Section 4 for more details). These methods differ fundamentally; hence, analogical prompting is not directly related to self-correction.\n\n\n>**2. \u201cIt is not clear which prior papers have the various problems exposed\u2026\u201d**\n\nThank you for your suggestion. The table below summarizes the issues of prior methods as you suggested, and we added this table to Appendix C in our revision.\n\n| Method                                              | Issue                                      |\n|-----------------------------------------------------|--------------------------------------------|\n| RCI (Kim et al., 2023); Reflexion (Shinn et al., 2023) | Use of oracle labels                        |\n| Multi-Agent Debate (Du et al., 2023)                | Unfair comparison to self-consistency       |\n| Self-Refine (Madaan et al., 2023)                   | Suboptimal pre-hoc prompt design            |\n\nRegarding the use of oracle labels, in Appendix C.2 of Kim et al. (2023), they stated that \u201cwe use the correct label to decide when to stop the RCI loop.\u201d For Shinn et al. (2023), please refer to Section 4.2 of their paper: \u201cwe use exact match answer grading using the environment to give a binary success signal to the agent,\u201d which is also revealed in their official implementation.\n\n>**3. Question: \u201cHowever, it is still not clear how we should think about all the techniques to improve reasoning without external feedback\u2026\u201d**\n\nThank you for your suggestion. As discussed in our Section 5 \u201cEmploying self-consistency as a method of self-verification\u201d, breaking down a problem into sub-problems and conducting step-wise verification paired with self-consistency can be helpful.\n\nAnother promising future direction is exploring how to enable LLMs to interact with and learn from external feedback (e.g., interacting with the environment), though this falls outside the scope of \"intrinsic self-correction\" discussed in our paper.\n\nWe thank you for recognizing the contribution of our paper to the field. Your feedback will significantly enhance the clarity and comprehensiveness of our research."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232266815,
                "cdate": 1700232266815,
                "tmdate": 1700232266815,
                "mdate": 1700232266815,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3vvEHgZfcp",
            "forum": "IkmD3fKBPQ",
            "replyto": "IkmD3fKBPQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2828/Reviewer_N1pE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2828/Reviewer_N1pE"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies *intrinsic* self-correctness in LLMs in the context of reasoning, where no external feedback is provided to the language model i.e., just simply asking the model to detect a mistake in its output and fix it. Through experiments over three reasoning tasks (GSM8K, Commonsense QA, and HotpotQA), the paper does the following. First, it argues that what is currently referred to in the literature as \"self-correctness\" where external feedback is provided (e.g., whether the final answer is correct) is not practical and we should focus on settings where we do not know the answer. Second, self-consistency is a strong baseline and similar approaches such as multi-agent debate methods should be considered an instance of voting rather than self-correction methods. Third, to assess whether LLMs actually have the capacity for self-correction, the authors emphasize the importance of designing a good initial (pre-hoc) prompt that performs well as opposed to using a suboptimal initial prompt, where providing additional information in the feedback prompt can be useful.  Overall, the paper argues that current LLMs fall short when prompted to self-correct their reasoning and when no additional information is provided in the feedback prompt."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper studies an important direction that is now taking over the LLM scene and brings a fresh perspective on how good SoTA LLMs are at detecting their own errors. \n* Focusing on *intrinsic self-correction* is much needed in the current \"sea\" of self-correction papers.\n* The experimental design is sound. I liked the random guessing baseline with Commonsense QA.  \n* I have to say I enjoyed reading the paper: the flow is natural, the writing is good, and most of the arguments are intuitive and make sense. \n\nOverall the community would certainly benefit from this paper gaining wider visibility."
                },
                "weaknesses": {
                    "value": "* I find the explanation in section 3.2.1\u2014why post-hoc prompting can lead the model to go from a correct to an incorrect answer\u2014unsatisfying. We know that the feedback prompt is changing the model output somehow. The question is *why?* I suggest providing more intuition here.\n* The paper discusses the issue but does not provide any hint at a potential solution. I understand this is not the point of the paper but hinting at potential directions to improve intrinsic self-correctness could make the paper even more valuable. \n* The authors focus only on ChatGPT and GPT-4. I think the community could benefit from seeing that the results discussed generalize to other open-source LLMs such as LLaMA"
                },
                "questions": {
                    "value": "* Have you tried combining Multi-agent debating with self-consistency? \n* How much effort did you invest into finding the self-correct prompt you used \"Review your previous answer and find problems with your answer\"? Can't different variations of this same prompt lead to different results?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2828/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2828/Reviewer_N1pE",
                        "ICLR.cc/2024/Conference/Submission2828/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2828/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698721212735,
            "cdate": 1698721212735,
            "tmdate": 1700550040272,
            "mdate": 1700550040272,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2CU1s8XIUt",
                "forum": "IkmD3fKBPQ",
                "replyto": "3vvEHgZfcp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2828/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2828/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer N1pE"
                    },
                    "comment": {
                        "value": "We thank the reviewer for highlighting the importance and soundness of our study, as well as the presentation of our paper. We greatly appreciate your thorough review and constructive feedback. We are pleased to address the points you raised.\n\n>**1. Explanation of Post-Hoc Prompting Leading to Incorrect Answers:**\n\nThank you for your feedback. The fundamental issue is that LLMs cannot properly judge the correctness of their reasoning. So the feedback prompt introduces a new context that may bias the model away from its original correct answer. We have added the explanation in the revision.\n\n>**2. Potential Directions for Improving Intrinsic Self-Correction:**\n\nThank you for your suggestion. We have provided some suggestions for future research and practical application in Section 5. For intrinsic self-correction, it can still be useful for aligning responses with certain preferences. Regarding self-correcting reasoning, one suggestion involves leveraging self-consistency for self-verification. Additionally, exploring ways to enable LLMs to interact with and learn from external feedback (e.g., interacting with the environment) would be a potential solution and a promising future direction.\n\n>**3. Generalization to Other LLMs:**\n\nThank you for your suggestion. We conducted additional experiments with GPT-4-Turbo (gpt-4-1106-preview in the table) and Llama-2-70B. The results consistently show that the models\u2019 performance decreases after self-correction. We added the results and full error analysis in Appendix B.2 of our revision. \n\n|                          |                      | # calls | GSM8K | CommonSenseQA |\n|--------------------------|----------------------|-------|-------|---------------|\n| gpt-4-1106-preview       | Standard Prompting   | 1     | 91.5  | 84.0          |\n|                          | Self-Correct (round 1) | 3   | 88.0  | 81.5          |\n|                          | Self-Correct (round 2) | 5   | 90.0  | 83.0          |\n| Llama-2-70b-chat-hf      | Standard Prompting   | 1     | 62.0  | 64.0          |\n|                          | Self-Correct (round 1) | 3   | 43.5  | 37.5          |\n|                          | Self-Correct (round 2) | 5   | 36.5  | 36.5          |\n\n>**4. Question A: Combining Multi-Agent Debating with Self-Consistency**\n\nYes. When we run the multi-agent debate method, we take the majority vote of the final responses of all three agents (following the setting of the original paper). We also try taking the majority vote of all responses (including the previous responses) of the agents, but do not observe performance improvement.\n\n\n>**5. Question B: Effort Invested in Finding the Self-Correct Prompt:** \n\nThe prompt \"Review your previous answer and find problems with your answer\" originates from Kim et al., (2023). We adhered to their prompt to replicate their experiments. We have also made efforts in designing different prompts for testing:\n- In Appendix B.1 of the initial draft, we test different feedback prompts, such as \u201cVerify whether your answer is correct, and provide an explanation.\u201d\n- In Appendix B.2 of the revised draft, we add a prompt suggested by other reviewers.\n- The prompts used in previous works (Kim et al., 2023; Shinn et al., 2023; Du et al., 2023; Madaan et al., 2023) are different.We adhere to the prompts from the source papers to replicate their experiments. So our prompts in these experiments are also different.\n\nAll the results consistently show that the models\u2019 performance on reasoning does not improve and even decreases after self-correction."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232179251,
                "cdate": 1700232179251,
                "tmdate": 1700232179251,
                "mdate": 1700232179251,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DFtNcvk87n",
                "forum": "IkmD3fKBPQ",
                "replyto": "2CU1s8XIUt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2828/Reviewer_N1pE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2828/Reviewer_N1pE"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for the additional experiments and results. I have decided to raise my score from 6 to 8."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700550027112,
                "cdate": 1700550027112,
                "tmdate": 1700550027112,
                "mdate": 1700550027112,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "b2CsLfx0dr",
            "forum": "IkmD3fKBPQ",
            "replyto": "IkmD3fKBPQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2828/Reviewer_No3p"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2828/Reviewer_No3p"
            ],
            "content": {
                "summary": {
                    "value": "This work investigates prior reports that LLMs can self-correct their own responses when prompted to do so. A critical distinction is made between self-correction with and without external feedback (termed intrinsic self-correction). The results show that given the same self-correction prompt used in prior work on particular benchmark datasets, LLMs often do not succeed in self-correcting their responses without external feedback."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Self-correction of today\u2019s LLMs is a highly significant topic. The paper clearly points out the crucial distinction between self-correction with and without feedback, and sheds light on the latter case (intrinsic self-correction). The usage of oracle labels to terminate self-critique is also examined.\n\nThe paper\u2019s organization and writing quality is uniformly high, making it a pleasure to read."
                },
                "weaknesses": {
                    "value": "The most serious weakness of the paper is its misleading title, which baldly asserts a claim unsupported by the analysis and results. The words \u201ccannot\u201d and \u201cyet\u201d imply that even today\u2019s most capable LLMs (GPT-4) obtain zero benefit from self-correction in nearly all cases. The abstract quickly tones down the claim by saying \u201cour research indicates that LLMs struggle to self-correct their responses without external feedback\u201d, but even that statement goes beyond what is actually demonstrated by the experiments. A more properly measured title for this work would be \u201cReexamining the Ability of Large Language Models to Self-Correct\u201d. This is more like statements that appear later in the paper:  \u201cwe provide insights into the nuances of LLMs\u2019 self-correction capabilities\u201d, and \u201ctheir self-correction capabilities, particularly in reasoning, are still nascent.\u201d\n\nThe second major problem is the work\u2019s heavy reliance on a single, loaded prompt:\n\u201cReview your previous answer and find problems with your answer\u201d\n\nPractitioners in the rapidly moving field of prompt engineering recognize this as a highly leading prompt, essentially telling the LLM that problems do exist in the previous answer. This typically causes the LLM to find problems that aren\u2019t actually present. As the paper says at one point:  \u201ccareful consideration of prompt design is essential\u201d.\n\nHere\u2019s a longer list of important factors used routinely in prompt engineering that would be needed for a proper study of LLM self-correction:\n\n- Focus on GPT-4, since its capabilities are known to be significantly greater than those of GPT-3.5. This is reflected in Table 3. \n\n- Include diagrams like those in Figure 1 for GPT-4, not just for GPT-3.5.\n\n- Evaluate a set of reasonable, unbiased self-correction prompts. For instance: \u201cAssume that this answer could be either correct or incorrect. Review the answer carefully and report any serious problems you find.\u201d\n\n- Focus on the zero-temperature setting, since that\u2019s far less prone to spurious hallucination than 1. \n\nIn discussing the option of trying other self-correction prompts (\u201cSuch a search essentially leverages feedback from humans or training examples.\u201d), the paper conflates feedback received on a per-problem basis (as when using an oracle), with feedback received from the results of multiple self-correction prompts across entire datasets. The former does indeed go beyond the definition of intrinsic self-correction, but the latter is merely hard work. \n\nThe paper says that \u201cOur main objective is to encourage a more critical examination of self-correction experiments.\u201d But readers expect this paper to be a critical examination of that nature, not just a call for critical examination.\n\nThe paper also states that \u201cin the reasoning tasks studied in this paper, we did not observe any improvement through self-correction.\u201d That\u2019s true enough, but as pointed out above, the study was not carried far enough to shed much light on the general question of how reliably LLMs can self-correct.\n\n**Post-rebuttal Comments**\n\nI commend the authors for performing the additional experiments reported in Appendix B.2, using GPT-4 with an unbiased feedback prompt and zero temperature. These results on these two datasets are interesting, so I have raised my assessment of the paper\u2019s contribution from 2 to 3. \n\nI still view these limited results as insufficient to support the broad claim made by the paper\u2019s title:  \u201cLarge Language Models Cannot Self-Correct Reasoning Yet\u201d. The authors argue that the title\u2019s claim is restricted to \u201creasoning\u201d tasks, but this is not much of a restriction at all. \n\nRegarding other feedback prompts used in the experiments, the ones in Appendix B.1 only apply to GPT-3.5, which is widely known to not be good at self-critique. The authors imply that the feedback prompts from prior works are different, but Appendix A shows only the single, loaded prompt: \u201cReview your previous answer and find problems with your answer\u201d\n\nFor these reasons, I still view the work as fundamentally unsound, in the sense that the limited findings do not justify the headline-grabbing title of the paper.\n\n**Additional Post-rebuttal Comments**\n\n\n*we do not fully understand the argument that \"The authors argue that the title\u2019s claim is restricted to \u201creasoning\u201d tasks, but this is not much of a restriction at all.\"*\n\n\nLLMs are strong at memorization, but struggle with many kinds of reasoning. So while self-critique can be applied to both memorization and reasoning problems, application to reasoning is of greater interest and is being intensely studied. For this reason, restricting the consideration of self-critique to reasoning is not much of a restriction at all. \n\n\n\n*we will change the title to **\"Reexamining the Ability of Large Language Models to Self-Correct Reasoning\"** in the final version.*\n\n\nThis title would be in line with the experiments. So on the assumption that this will indeed be the title, I'm raising my rating from 3 to 6."
                },
                "questions": {
                    "value": "Section 3.2 says \u201cIntuitive Explanation. If the model is well-aligned and paired with a thoughtfully designed initial prompt, the initial response should already be optimal\u201d  But why should this be intuitive or expected? Wouldn\u2019t similar reasoning conclude that the value of chain-of-thought prompting is itself unintuitive?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2828/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2828/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2828/Reviewer_No3p"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2828/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698956171633,
            "cdate": 1698956171633,
            "tmdate": 1700533350404,
            "mdate": 1700533350404,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5NiRq3ATEJ",
                "forum": "IkmD3fKBPQ",
                "replyto": "b2CsLfx0dr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2828/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2828/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer No3p [1/2]"
                    },
                    "comment": {
                        "value": "We thank the reviewer for highlighting the importance of our study and commending the quality of our writing. We appreciate the insightful and detailed feedback provided. We would like to address each of your concerns.\n\n>**1. Title and Claim Adjustment:**\n\nThank you for your feedback. Based on the ICLR policy, we are not able to change the paper title during the rebuttal phase, but we will consider your suggestion in the final version. Nevertheless, we wish to clarify that our title does not mean self-correction does not work for all tasks; rather, as our original title includes the word \u201creasoning\u201d, our work focuses on self-correction for reasoning. The title reflects our finding that current LLMs cannot yet enhance reasoning performance through self-correction. We have also adjusted our claims in the paper to make them more specific to reasoning.\n\n>**2. Test on Various Prompts:**\n\nA primary question raised by the reviewer concerns the reliance on a single, loaded prompt. However, this is not true. **In our paper, both in the main text and Appendix B, we have tested multiple prompts**.\n\nFirst, as this paper functions partially as a critique paper, we initially follow the prompts and models used in prior studies, specifically testing three different sets of prompts from Kim et al., 2023, Shinn et al., 2023, and Du et al., 2023.\n\n**Furthermore, we also test unbiased self-correction prompts in Appendix B.1 of the initial version of our paper, such as 'Verify whether your answer is correct, and provide an explanation.' In Appendix B.2 of the revised manuscript, we conduct additional tests on GPT-4-Turbo and Llama-2-70b using the prompt suggested by the reviewer: 'Assume that this answer could be either correct or incorrect. Review the answer carefully and report any serious problems you find.' The results consistently show that the models\u2019 performance decreases after self-correction.** We welcome the reviewer to check the results.\n\n\n>**3. Use of Zero-Temperature Setting:**\n\nAs our paper partially serves as a critique paper, our choice of temperature setting adheres to previous works. We conducted additional experiments detailed in Appendix B.2, employing the settings suggested by the reviewer (e.g., zero-temperature, the prompt suggested by the reviewer). We added the results and full error analysis in Appendix B.2 of our revision.\n\n|                          |                      | # calls | GSM8K | CommonSenseQA |\n|--------------------------|----------------------|-------|-------|---------------|\n| gpt-4-1106-preview       | Standard Prompting   | 1     | 91.5  | 84.0          |\n|                          | Self-Correct (round 1) | 3   | 88.0  | 81.5          |\n|                          | Self-Correct (round 2) | 5   | 90.0  | 83.0          |\n| Llama-2-70b-chat-hf      | Standard Prompting   | 1     | 62.0  | 64.0          |\n|                          | Self-Correct (round 1) | 3   | 43.5  | 37.5          |\n|                          | Self-Correct (round 2) | 5   | 36.5  | 36.5          |\n\n\n>**4. \u201cFocus on GPT-4. Include diagrams like those in Figure 1 for GPT-4, not just for GPT-3.5.\u201d**\n\nThere are two primary reasons we test on GPT-3.5 (while also providing results for GPT-4):\n\n(1) Alignment with previous studies: As our paper partially serves as a critique of the studies by Kim et al., 2023; Shinn et al., 2023; and Du et al., 2023, which focus on GPT-3.5, it is imperative that we follow their settings to accurately reproduce their results.\n\n(2) Cost considerations: The cost associated with GPT-4 is substantial. For instance, conducting a single full test on GSM8K with self-correction using GPT-4 incurs a cost of approximately $200 (in total, it will cost thousands of dollars). To manage these costs, we opt to randomly sample 200 examples for our tests in our initial draft.\n\nWe appreciate your suggestion and have included diagrams for GPT-4 in both Figure 1 and Appendix B.2."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700231937291,
                "cdate": 1700231937291,
                "tmdate": 1700256815641,
                "mdate": 1700256815641,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lFNw64M7M0",
                "forum": "IkmD3fKBPQ",
                "replyto": "b2CsLfx0dr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2828/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2828/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer No3p [2/2]"
                    },
                    "comment": {
                        "value": ">**5. Our study focuses on the question \"Can LLMs self-correct *reasoning* intrinsically?\" by examining various prompts and previous studies, not on the general question of how reliably LLMs can self-correct for all tasks:**\n\nAs discussed above, in Appendix B, we have tested with different (unbiased) self-correction prompts. We have also tested three different sets of prompts from Kim et al., 2023; Shinn et al., 2023; and Du et al., 2023. All the results consistently show that the models\u2019 performance on reasoning does not improve and even decreases after self-correction, supporting the statement that LLMs struggle to self-correct reasoning intrinsically.\n\nMoreover, as mentioned in the title and our response 1 above, the focus of our paper is on self-correction for *reasoning*, not on general self-correction for all tasks. While we do discuss self-correction for other tasks a bit, our aim is to provide readers with a more balanced and comprehensive understanding of self-correction. Additionally, we are open to conducting any further experiments (considering the cost and time) that the reviewers might deem necessary before the revision deadline.\n\n\n>**6. Questions on Section 3.2:**\n\nThe reviewer appears to have truncated our statement into an incomplete sentence. The complete sentence is, \u201cIf the model is well-aligned and paired with a thoughtfully designed initial prompt, the initial response should already be optimal *relative to the prompt and the specific decoding algorithm*.\u201d The critical missing part, 'the response is optimal relative to the given prompt and the specific decoding algorithm', significantly alters the meaning when omitted.\n\nThe effectiveness of 'Chain-of-Thought Prompting' lies in its provision of better guidance for the model to solve the problem. Therefore, the model's response is optimal to the improved prompt (w/ CoT); it surpasses the response generated without CoT, which is optimal to a less effective prompt (w/o CoT). This contrasts significantly with the intrinsic self-correction we discuss here, where the feedback prompt (without usage of oracle labels) does not provide any helpful benefits for reasoning and may even bias the model to generate a worse response.\n\nWe thank you for your constructive critique and guidance. The suggested modifications certainly enhance our paper."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700232056586,
                "cdate": 1700232056586,
                "tmdate": 1700265234075,
                "mdate": 1700265234075,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pNtK6ZAy7p",
                "forum": "IkmD3fKBPQ",
                "replyto": "b2CsLfx0dr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2828/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2828/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up Response to Reviewer No3p"
                    },
                    "comment": {
                        "value": "Thank you for reviewing our response. We would like to follow up and further address your concerns.\n\nFirst, we present additional results for GPT-4-Turbo using different feedback prompts. These results are summarized in the table below, consistently demonstrating a decline in performance after self-correction. We have included these results in Table 8 of Appendix B.2.\n\n|                               | # calls | GSM8K | CommonSenseQA |\n|-------------------------------|---------|-------|---------------|\n| _Feedback Prompt_: Assume that this answer could be either correct or incorrect. Review the answer carefully and report any serious problems you find. |         |       |               |\n| Standard Prompting            | 1       | 91.5  | 84.0          |\n| Self-Correct (round 1)        | 3       | 88.0  | 81.5          |\n| Self-Correct (round 2)        | 5       | 90.0  | 83.0          |\n|                                   |         |       |               |\n| _Feedback Prompt_: Review your previous answer and determine whether it's correct. If wrong, find the problems with your answer. |         |       |               |\n| Standard Prompting            | 1       | 91.5  | 84.0          |\n| Self-Correct (round 1)        | 3       | 90.0  | 74.5          |\n| Self-Correct (round 2)        | 5       | 90.0  | 81.0          |\n|                                   |         |       |               |\n| _Feedback Prompt_: Verify whether your answer is correct, and provide an explanation. |         |       |               |\n| Standard Prompting            | 1       | 91.5  | 84.0          |\n| Self-Correct (round 1)        | 3       | 91.0  | 81.5          |\n| Self-Correct (round 2)        | 5       | 91.0  | 83.5          |\n\n\nRegarding the feedback prompts from previous studies: In Section 3.1.1 Prompts, we state, \"we mostly adhere to the prompts from the source papers. For GSM8K and CommonSenseQA, we integrate format instructions into the prompts of Kim et al. (2023) to facilitate a more precise automatic evaluation (detailed prompts can be found in Appendix A). For HotpotQA, we use the same prompt as Shinn et al. (2023).\" In Section 3.3, we note, \"For an unbiased implementation, we use the exact same prompt as Du et al. (2023) and replicate their experiment with the gpt-3.5-turbo-0301 model, incorporating 3 agents and 2 rounds of debate.\" The reviewer may refer to the previous papers or their implementations for the prompts, which are identical to those used in our experiments.\n\nRegarding the title, we thank you for your suggestion. However, we do not fully understand the argument that \"The authors argue that the title\u2019s claim is restricted to \u201creasoning\u201d tasks, but this is not much of a restriction at all.\" We would appreciate it if the reviewer could further clarify. If you feel that this is the main weakness, we will change the title to \"Reexamining the Ability of Large Language Models to Self-Correct Reasoning\" in the final version. Additionally, we have softened the statements in the paper, for example, changing 'cannot' to 'struggle to'.\n\nWe hope the additional results and the responses above address your concerns. Please do not hesitate to contact us for any further questions or clarifications. Additionally, we are open to conducting any further experiments (considering the cost and time) that the reviewers might deem necessary before the revision deadline."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700511645240,
                "cdate": 1700511645240,
                "tmdate": 1700512378983,
                "mdate": 1700512378983,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Quhrtq7F6W",
                "forum": "IkmD3fKBPQ",
                "replyto": "b2CsLfx0dr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2828/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2828/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your update. We are happy that our response addressed your concern, and we thank you again for your constructive feedback."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700585034721,
                "cdate": 1700585034721,
                "tmdate": 1700585034721,
                "mdate": 1700585034721,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TUaMWym9fv",
            "forum": "IkmD3fKBPQ",
            "replyto": "IkmD3fKBPQ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2828/Reviewer_yEtd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2828/Reviewer_yEtd"
            ],
            "content": {
                "summary": {
                    "value": "LLMs apparently have the ability to self-correct, as evidenced by previous publications. In the current article, which acts partly as a survey, the authors investigate how well LLMs can actually self-correct intrinsically and report nuanced, mixed results in terms of LLMs ability to perform such a feat."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper tackles a very important topic, has a good literature review section, and uses well-known and trusted datasets to investigate self-correction abilities. In particular, I appreciated the distinction between intrinsic self-correction and self-correction that leverages information from humans or training examples."
                },
                "weaknesses": {
                    "value": "- Only a small set of questions (200) is used on GPT4, the remaining ones apply only to ChatGPT  \n- In terms of reasoning, there are much more challenging datasets out there\n- I found the presentation somewhat confusing since there wasn't a clear description of their methodology (e.g., were all self-correction prompts formulated as the examples in Figure 2, or did variations exist?).\n- Also, the wording is unfortunately often trendy rather than clear: From the conclusion: \"while LLMs represent a groundbreaking step forward in the realm of AI and language generation, their self-correction capabilities, particularly in reasoning, are still nascent\". \"Nascent\" can mean anything from the self-correction capabilities being \"not present\", \"weak\" \"promising\" etc. Please try to use more specific wording.\n- The paper is confusing to read, because there are somewhat contradictory statements: In the beginning the authors emphasize that they want to investigate *intrinsic* self-correction abilities. Then, on page 3, they state: \"With this in mind, we center our investigation on a pivotal query: Can large language models self-correct their reasoning?\" which seems to me to not imply intrinsic self-correction, but any type of self-correction. This unclarity seems to pervade the remainder to the paper.      \nE.g., in section 7 the authors state \" Some existing literature may inadvertently contribute to this confusion, either by\nrelegating crucial details about label usage to less prominent sections or by failing to clarify that their\ndesigned self-correction strategies actually incorporate external feedback. Our intention in this paper is to amplify these concerns and offer a comprehensive overview of the state of \u201cself-correction\u201d in LLMs\".      \nThis implies that the current paper was more like a survey, rather than establishing new results.    \nFurthermore, the title suggests that self-correction does not work, although the paper actually does not really argue this and in section 7 the authors remark \"The title, \u201cLarge Language Models Cannot Self-Correct Reasoning Yet\u201d, is not an outright dismissal of self-correction techniques\". Generally, it would be good to use a title that is completely representative of the paper. (Perhaps add \"intrinsic\" to the title?)"
                },
                "questions": {
                    "value": "- I don't understand the second paragraph from section \"3.1.3 REFLECTION\". How does this guessing approach work? It seems that cannot be used for non-multiple-choice type answers, like in the case of the GSM8K dataset?          \nI cannot follow your footnote: \"For GSM8K, a similar random baseline might not exist, but the underlying rationale remains the same. Additionally, we can design a baseline, for example, by generating a random number each time. After a significant number of rounds, it may reach the correct answer, but such a kind of improvement is apparently not meaningful. A more direct justification is: If we already know the answer, why do we need to do this?\"      \n- \"Per the discussions in Section 3.1.3, since the idea that LLMs can self-correct their reasoning is not supported by the evidence so far, we turn our focus to the results in the intrinsic self-correction.\" I wasn't quite able to follow why section 3.1.3 shows this; Table 1 actually seems to show that self-reasoning is well-supported by evidence?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2828/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2828/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2828/Reviewer_yEtd"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2828/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699472036161,
            "cdate": 1699472036161,
            "tmdate": 1699636226257,
            "mdate": 1699636226257,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UWm0ifRB5J",
                "forum": "IkmD3fKBPQ",
                "replyto": "TUaMWym9fv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2828/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2828/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer yEtd [1/2]"
                    },
                    "comment": {
                        "value": "We thank the reviewer for recognizing the importance of our study. We appreciate your comprehensive review of our paper and would like to address each of your questions.\n\n>**1. Use of GPT-4 and Selection of Datasets**\n\nThere are two primary reasons we test on GPT-3.5 (while also providing results for GPT-4):\n\n(1) Alignment with previous studies: As our paper partially serves as a critique of the studies by Kim et al., 2023; Shinn et al., 2023; and Du et al., 2023, which focus on GPT-3.5, it is imperative that we follow their settings (e.g., testing on GPT-3.5) to accurately reproduce their results on the well-established benchmarks used in their papers.\n\n(2) Cost considerations: The cost associated with GPT-4 is substantial. For instance, conducting a single full test on GSM8K with self-correction using GPT-4 incurs a cost of approximately $200 (in total, it will cost thousands of dollars). To manage the costs, we opt to randomly sample 200 examples for our tests on GPT-4.\n\nA point to mention is that we do consider running tests on the full test set when the cost is reasonable. For instance, Du et al. (2023) tested only 100 examples on GSM8K with GPT-3.5, whereas we chose to run the full test set to make the results more reliable.\n\nIn Appendix B.2 of our revision, we conducted additional experiments on more examples with GPT-4-Turbo and Llama-2, employing the settings suggested by other reviewers.\n\n|                          |                      | # calls | GSM8K | CommonSenseQA |\n|--------------------------|----------------------|-------|-------|---------------|\n| gpt-4-1106-preview       | Standard Prompting   | 1     | 91.5  | 84.0          |\n|                          | Self-Correct (round 1) | 3   | 88.0  | 81.5          |\n|                          | Self-Correct (round 2) | 5   | 90.0  | 83.0          |\n| Llama-2-70b-chat-hf      | Standard Prompting   | 1     | 62.0  | 64.0          |\n|                          | Self-Correct (round 1) | 3   | 43.5  | 37.5          |\n|                          | Self-Correct (round 2) | 5   | 36.5  | 36.5          |\n\n\n>**2. \u201cwere all self-correction prompts formulated as the examples in Figure 2, or did variations exist?\u201d**\n\nWe test on various prompts:\n- In Section 3.1.1 Prompts, we mentioned that \u201cwe mostly adhere to the prompts from the source papers. For GSM8K and CommonSenseQA, we integrate format instructions into the prompts of Kim et al. (2023) to facilitate a more precise automatic evaluation (detailed prompts can be found in Appendix A). For HotpotQA, we use the same prompt as Shinn et al. (2023).\u201d The prompts used in Du et al. (2023) and Madaan et al. (2023) are also different.\n- In Appendix B.1 of the initial draft, we test on different feedback prompts, such as \u201cVerify whether your answer is correct, and provide an explanation.\u201d\n- In Appendix B.2 of the revised draft, we add a prompt suggested by other reviewers.\n\n>**3. Presentation and Wording**\n\nThank you for your feedback. Note that at the end of Section 2 (page 2), we specified, \u201cFor brevity, unless explicitly stated otherwise (e.g., self-correction with oracle feedback), all references to \u2018self-correction\u2019 in the remainder of this paper pertain to intrinsic self-correction.\u201d Thus, the question \u201cCan large language models self-correct their reasoning?\u201d on page 3 refers to intrinsic self-correction.\n\nWe appreciate your suggestion on the wording. The term \u201cnascent\u201d was intended to convey that LLMs' self-correction abilities, particularly in reasoning, are not yet fully effective. We have updated the conclusion section and incorporated more specific wording in the revision.\n\n>**4. \u201cThis implies that the current paper was more like a survey...\u201d**\n\nWe would like to clarify that our paper critiques and analyzes LLMs\u2019 ability to self-correct reasoning. We present new results showing that LLM self-correction without oracle labels underperforms baselines without self-correction:\n- Critique: We reexamine previous works and find that 1) previous works use oracle labels to perform self-correction, which is not practical; 2) multi-agent debate is not superior to self-consistency; 3) there is an issue with employing a well-crafted post-hoc prompt to guide the model in \u201cself-correcting\u201d a response generated through a poorly constructed pre-hoc prompt. \n- Analysis: We demonstrate that current LLMs struggle to self-correct reasoning intrinsically through comprehensive analysis and offer insights and suggestions for future work.\n\n>**5. Paper Title**\n\nThank you for your feedback. Based on the ICLR policy, we are not able to change the title during the rebuttal phase, but we will consider your suggestion in the final version. Nevertheless, we wish to clarify that our title doesn\u2019t mean self-correction does not work for all tasks; rather, our work focuses on self-correction for reasoning. The title reflects our finding that current LLMs cannot yet enhance reasoning performance through intrinsic self-correction."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700231597873,
                "cdate": 1700231597873,
                "tmdate": 1700231597873,
                "mdate": 1700231597873,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3HqJccZQcL",
                "forum": "IkmD3fKBPQ",
                "replyto": "TUaMWym9fv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2828/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2828/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Response to Reviewer yEtd [2/2]"
                    },
                    "comment": {
                        "value": ">**6. Question A: Explanation of random guessing baseline**\n\nThis section presents a case study to demonstrate that improvements achieved using oracle labels (a setting used in previous work) do not necessarily reflect true self-correction ability. Specifically, we illustrate the effectiveness of a simple guessing strategy employing oracle labels in multiple-choice settings.\n\nFor example, consider a question with the correct label (C). If the model initially predicts the answer as (A), we can determine the response's correctness using labels: since (A) \u2260 (C), the random baseline is employed to randomly select a new answer from the remaining options (B-E). If the sampled option is (D), and (D) \u2260 (C), the random baseline samples again from the remaining options (B, C, E). If the next sampled option is (C), which equals (C), we then obtain the correct answer and stop. In a 5-choice question format like CommonSenseQA, this baseline can always achieve 100% accuracy within four rounds of self-correction.\n\nFor non-multiple-choice datasets like GSM8K (which can be regarded as questions with an extensive range of numerical choices), we mention the possibility of designing a baseline by generating random numbers in the footnote. Through a significant number of attempts, it may still reach the correct answer and achieve performance improvement. However, this type of improvement is not meaningful. In the revision, we remove the footnote to avoid confusion.\n\n>**7. Question B: \u201cI wasn't quite able to follow why section 3.1.3 shows this; Table 1 actually seems to show that self-reasoning is well-supported by evidence?\u201d** \n\nAs explained above, the results in Table 1 use oracle labels to guide self-correction, which is impractical and differs from the intrinsic self-correction setup. Therefore, they cannot support the claim that LLMs can self-correct reasoning (intrinsically). We have adjusted the writing to enhance clarity."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700231678080,
                "cdate": 1700231678080,
                "tmdate": 1700231678080,
                "mdate": 1700231678080,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BNiXNkZ6wy",
                "forum": "IkmD3fKBPQ",
                "replyto": "TUaMWym9fv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2828/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2828/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer yEtd, we hope that our paper revision and the above responses address your concerns. Please let us know your thoughts, and we are more than happy to answer any further questions."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549176273,
                "cdate": 1700549176273,
                "tmdate": 1700549176273,
                "mdate": 1700549176273,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ehZTA1PCgB",
                "forum": "IkmD3fKBPQ",
                "replyto": "BNiXNkZ6wy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2828/Reviewer_yEtd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2828/Reviewer_yEtd"
                ],
                "content": {
                    "comment": {
                        "value": "I commend the authors for improving their paper. Its readability has significantly increased. I found the cost aspect that the authors mentioned above striking - this would be an important, concrete piece of information to add to the paper that motivates the small dataset strongly.      \n\nI believe the paper currently mentions somewhat vaguely only \"costs\"; I think adding concrete numbers (e.g., for the camera-ready version, if the paper is accepted), as they did above, relating to how much various runs cost would be 1) very interesting for the readers and 2) clearly show how this analysis could be scaled and what financial resources that would take, which would inform readers how easy/hard it is in financial terms to replicate these results.   \nFew papers do that, so adding this information could be a selling point of this paper.\n\nRegarding the score, I will maintain it."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2828/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734385077,
                "cdate": 1700734385077,
                "tmdate": 1700734385077,
                "mdate": 1700734385077,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]