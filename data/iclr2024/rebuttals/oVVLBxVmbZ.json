[
    {
        "title": "Fast Conditional Intervention in Algorithmic Recourse with Reinforcement Learning"
    },
    {
        "review": {
            "id": "3CDBxNZ1m3",
            "forum": "oVVLBxVmbZ",
            "replyto": "oVVLBxVmbZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9061/Reviewer_a8gS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9061/Reviewer_a8gS"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an RL-based method for the recourse generation problem. The paper incorporates causal graphs of input features to calculate a new cost for conditional intervention called Intervention Cost. The experiments conducted on synthetic and real-world datasets show a better performance than baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is easy to read and follow.\n- The construction of the Intervention Cost is sound and highly motivated. \n- The proposed Markov Decision Process is well-defined and reasonable."
                },
                "weaknesses": {
                    "value": "- The paper assumes that all subjects share the same prior causal graph. However, in reality, each individual typically possesses a distinct causal graph. To address this concern, De Toni et al. (2022) [2] propose a solution. They initially establish a fixed causal graph and then iteratively learn the subject-specific cost function. Subsequently, they seek an appropriate sequence of interventions.\n- The author omits the description of the reinforcement learning algorithm used to solve the MDP and its parameters. \n- The way the author handles the noisy graphs (incompleteness of the casual graph) is unclear.\n- The learning curve of rewards, objectives, and metrics should be reported. The evaluation can be improved by comparing the proposed method and baselines on more datasets. \n- In Section 3.2.3, the authors state that architectural corrections can alleviate the instability of the PASVG(0). However, there is no justification or ablation study for this claim."
                },
                "questions": {
                    "value": "- In section 3.2.2, when finding the longest path length between $X_i$ and $X_k$, what is the edge weight between two vertices of the graph?  Does the algorithm find the longest path on the casual graph?\n- The reward function and the objective function in Section 3.2.2 are not related to each other, making me confused about interpreting their role in the training.\n\n**References**\n\n[1] Sahil Verma, Varich Boonsanong, Minh Hoang, Keegan E. Hines, John P. Dickerson, and Chirag Shah. Counterfactual explanations and algorithmic recourses for machine learning: A review, 2020.\n\n[2] Giovanni De Toni, Paolo Viappiani, Bruno Lepri, and Andrea Passerini. Generating personalized counterfactual interventions for algorithmic recourse by eliciting user preferences, 2022."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9061/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9061/Reviewer_a8gS",
                        "ICLR.cc/2024/Conference/Submission9061/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9061/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698510238734,
            "cdate": 1698510238734,
            "tmdate": 1700671920083,
            "mdate": 1700671920083,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "etXM045H8u",
                "forum": "oVVLBxVmbZ",
                "replyto": "3CDBxNZ1m3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9061/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9061/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your precious comments. Hopefully the followings will address your concerns.\\\n\\\n**Response to weakness 1** \\\nWe agree that the idea from De Toni et al. is interesting. However, we would also like to highlight the difference and connection between these two works. Different from De Toni et al. who propose to incorporate user preferences, we focus on improving recourse by considering causality and thus do not require human-in-the-loop. It is true that our model does not personalize recourse; however, we find no apparent conflict of introducing the cost function proposed by De Toni et al. to our model. Specially, given that the two works both consider uncertainty between features, it may be possible to find a probabilistic approach combining the merits (i.e., conditional intervention and personalization) concerning distinct issues.\\\n\\\n**Response to weakness 2** \\\nThe specification of MDP is in Section 3.2.2, and the objective function of the reinforcement learning algorithm, REINFORCE (Williams, 1992), is shown in Eq. 15. We also revise the paper to make the description more clear. The newly added paragraph \u201cIntuitively, when observing state $s^t$ (current feature values), if the action $a^t$ corresponds to a positive cumulative reward compared to baseline $(R^t-b^t)$, we should increase the log probability of choosing $a^t$\u201d is in Section 3.2.3. Finally, we would like to emphasize that the objective function is only for the classifier to output the target class. The total cost we optimize is $L_{agent}$, in which the parameters account for the degree of exploration ($\\beta$) and the strength of intervention cost ($\\eta$).\\\n\\\n**Response to weakness 3** \\\nIn our work, incompleteness refers to the unobserved variables and we propose to model the incompleteness via variance as discussed in Section 1. Let us say we have a simple causal graph A \u2192 B where there is no unobserved variable. In this case, the estimated variance of $P(B | A)$ shall be near 0 and our model will then choose to intervene A when we need a different value of B. On the contrary, if there exists an unobserved variable C being a parent of B, our model would discover a higher variance in $P(B|A)$ and therefore conclude the existence of C. Our model would then propose to intervene B directly, revealing the fact that changing A might not be an effective means to affect B. \\\n\\\n**Response to weakness 4** \\\nWe now report the learning curves in Appendix B.2. \\\n\\\n**Response to weakness 5** \\\nWe now report the performance with and without our trick in Appendix C. The performance gaps are small in general; however, we found using gumbel-softmax can occasionally cause not-a-number errors on Sangiovese dataset and thus require more careful tuning. \\\n\\\n**Answer to Q1** \\\nYes, the algorithm finds the longest path (LP) on the causal graph. The weight of each edge is set to 1. Consider a causal graph with two paths, A->B->C and A->E->F->C. Supposed at one step, the RL agent chose to intervene upon feature A, then firstly B, E (LP(A,B)=LP(A,E)=1) should update to new values because of the treatment effect from A. Secondly, F should update (LP(A,F)=2) because of the treatment effect of E.  Finally C should update (LP(A,F)=3) because of the treatment effects from B and F.   \\\n\\\n**Answer to Q2** \\\nWe are sorry for the confusion.The reward we adopt is the cumulative reward $R^t$ rather than reward $r^t$, which appears following eq. 12. This is because the action at step t will have effects on all the steps after t. Therefore, in the objective function (eq. 15), the probability of choosing action $a^t$ is updated according to the cumulative reward $R^t$."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9061/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590968535,
                "cdate": 1700590968535,
                "tmdate": 1700590968535,
                "mdate": 1700590968535,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FIlbQA4TFw",
                "forum": "oVVLBxVmbZ",
                "replyto": "etXM045H8u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9061/Reviewer_a8gS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9061/Reviewer_a8gS"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the author's response. The answer has clarified lots of aspects of the paper. I have raised the score from 3 to 5."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9061/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671900334,
                "cdate": 1700671900334,
                "tmdate": 1700671900334,
                "mdate": 1700671900334,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tEPGMMSM9T",
            "forum": "oVVLBxVmbZ",
            "replyto": "oVVLBxVmbZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9061/Reviewer_J2Mk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9061/Reviewer_J2Mk"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes to use RL agent for helping design more efficient and accuracy intervention strategies for explanations.  By the desgined architecture with the so-called interventional cost as loss functions, the method shows some advantage over existing ones on some datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The use of RL is interesting.\n2. The experiments concerning interventions are convincing."
                },
                "weaknesses": {
                    "value": "1. Some theoretical properties need more justifications.\n2. The efficiency of training needs more evaluations."
                },
                "questions": {
                    "value": "1. About Fig 2. Is this graph representative? It seems the only confounder is U_0, and other Us can be considered as additive noise. Why this graph is used as an example for experiments?\n2. About the theoretical aspects of \"incomplete SCM\". Is there any theoretical justification of how \"incomplete\" your method works? Or under some quantification of missing nodes, can you show some error bounds or something like that?\n3. About the RL part. Is there anything related to the choice of reward, policy that have impacts on the final experimental outcomes?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9061/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698630483303,
            "cdate": 1698630483303,
            "tmdate": 1699637140818,
            "mdate": 1699637140818,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pIJ083CV81",
                "forum": "oVVLBxVmbZ",
                "replyto": "tEPGMMSM9T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9061/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9061/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your precious comments. Hopefully the followings will address your concerns.\\\n\\\n**Response to weakness** \\\n Our model and FastAR require training, and the training time of the 2 models are 10 - 50 minutes and 2 hours on Intel Xeon cpu E5-2650-v4. We leave the details to Appendix B.3. \\\n\\\n**Answer to Q1** \\\n In this work, we assume the existence of unobserved variables in the causal graph and thus design Figure 2 as an example for illustrating the idea. Also, we generate synthetic data via nonlinear equations (Appendix B.2) for the experiments in Section 4. Notably, we assume the unobserved variables do not cause spurious correlations. Therefore, we design $U_0$ pointing to one endogenous feature and $U_1$ on the path of observed features as the examples. \\\n\\\n**Answer to Q2** \\\nWe quantify the incompleteness or uncertainty via estimated variances (numerical) and entropy (categorical). We assume that higher variance or entropy is a stronger signal of missing causes of an endogenous feature. The proposed Intervention cost (IC) is a generalization of the causal proximity $\\sum_v \\|f_v(pa_v\u2019) - x_v\u2019\\| \\ v\\in endo$ from Mahanjan et al. [1]. When all the endogenous features $X_{vs}$ share the same constant conditional variance (given parent feature values $pa_v\u2019$), IC degrades to the causal proximity from Mahajan et al. IC should bound the endogenous features $X_{vs}$ in the region given by eq. 7. The error (deviation from eq. 7) depends on the difficulty of changing the given classifier output, which is our primary goal. Usually the more difficult it is, there can be more deviation from eq. 7 for the endogenous features since it requires more and larger interventions on them. \\\n\\\n**Answer to Q3** \\\nThere are two terms in the reward (eq. 12), one for changing the classifier output; the other for controlling the proximity. The parameter $\\lambda$ in eq.12 decides the degree of controlling the proximity. There is usually a tradeoff between validity and proximity since some difficult cases require larger interventions to change the classifier output.\\\n\\\nAs for the policy, we find it better to model the distribution of a numerical feature with a two-component GMM than with a simple gaussian distribution. For example, for a simple logistic regression model $f(x)=\\frac{1}{1+e^{-x^2}}$, when $x=0$, increasing or decreasing $x$ both increases the probability of a positive output. A learnable GMM is able to preserve the possibility of the two actions and degrade to a single gaussian distribution when facing simple cases. \n \\\n\\\n[1] Divyat Mahajan, Chenhao Tan, and Amit Sharma. Preserving causal constraints in counterfactual explanations for machine learning classifiers. 2019."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9061/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590272282,
                "cdate": 1700590272282,
                "tmdate": 1700590272282,
                "mdate": 1700590272282,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4jm5WmZuLA",
                "forum": "oVVLBxVmbZ",
                "replyto": "pIJ083CV81",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9061/Reviewer_J2Mk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9061/Reviewer_J2Mk"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the comments"
                    },
                    "comment": {
                        "value": "Thank you for the Rebuttal. I still think the paper contains several novel ideas but the limitation from theoretical aspect is also present, and remain unchanged of my score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9061/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656152721,
                "cdate": 1700656152721,
                "tmdate": 1700656152721,
                "mdate": 1700656152721,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "F87XF6L4qQ",
            "forum": "oVVLBxVmbZ",
            "replyto": "oVVLBxVmbZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9061/Reviewer_vwWM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9061/Reviewer_vwWM"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the problem of finding realistic and causally-grounded counterfactual explanations. They propose a reinforcement learning (RL)-based approach with conditional interventions. The proposed intervention method has theoretical properties, e.g., it considers both feature dependencies leveraging the SCM. For the RL strategy, computational complexity is provided. Experiments are performed on synthetic and real datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper brings together counterfactual fairness, causality, and reinforcement learning. \nThe strategy tries out several interventions using reinforcement learning to identify a realistic recourse given an SCM. It is mathematically interesting.\n\nThe challenge arises since at each stage the RL agent has to decide which feature to intervene and also with what value. To address this challenge, the RL agent will leverage a structural causal model. Then, it would perform conditional interventions, i.e., interventions conditioned on the parents of that feature. Ultimately, the goal is to obtain a counterfactual that will respect the SCM and also be as close to the original point as possible in fewer steps than the number of features changed. Additionally, they require the number of interventions T to be less than p which is the number of actionable features.\n\nThey have included relevant baselines in their experiments, and show time benefits."
                },
                "weaknesses": {
                    "value": "One limitation is that the SCM may not always be available. \n\nThe scenario of incomplete causal graphs as mentioned in the abstract was not very clear to me. What is the assumption here?\n\nThe experiments directly seem to use the causal discovery method of another paper. Is this done for the proposed method as well?\n\nI also wonder if RL is a bit of an overkill for this problem since the number of features (p) is often quite small. It is often desirable to intervene on fewer features. For instance, the experiments drop the feature Capital Gain since intervening only on that one feature suffices for recourse. Also, what about exploration? Could the authors strengthen the motivation behind this approach? \n\nAnd also, how is the time being calculated in the experiments? It seems to be only the inference time. What about preprocessing time? Could the authors discuss/elaborate on the preprocessing time of various methods?\n\nThe experiment section does not provide enough details on how the causal graph was generated for the real-world datasets and if that causal graph is reliable.\n\nUltimately, human evaluations might also be necessary at some point to compare different methods."
                },
                "questions": {
                    "value": "Already discussed in weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9061/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9061/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9061/Reviewer_vwWM"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9061/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698903537746,
            "cdate": 1698903537746,
            "tmdate": 1699637140704,
            "mdate": 1699637140704,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "l5arfHgaYv",
                "forum": "oVVLBxVmbZ",
                "replyto": "F87XF6L4qQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9061/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9061/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your precious comments. Hopefully the followings will address your concerns.\\\n\\\n**One limitation is that the SCM may not always be available.**\\\nWe cannot agree more. This is the reason why this work chooses to relax the requirement of needing a complete SCMs. We assume that a partially observed SCM can be obtained much easier in the real-world situation. After all, as discussed by Karimi et al. [1], recourse without SCMs cannot be guaranteed unless more assumptions are introduced.\\\n\\\n**The scenario of incomplete causal graphs as mentioned in the abstract was not very clear to me. What is the assumption here?** \\\nThe assumed incompleteness is the unobserved variable set. As shown in Figure 1, we assume an endogenous variable can be influenced by unobserved variables in the graph. A partially observed casual graph is more realistic and easier to obtain.\\\n\\\n**The experiments directly seem to use the causal discovery method of another paper. Is this done for the proposed method as well?**\\\nWe do not use causal discovery methods in all experiments except for OrdCE on the synthetic dataset. Since OrdCE is specifically designed for linear SCMs, we employ its built-in causal discovery method to obtain an approximated SCM. For the Sangiovese and Adult datasets where SCMs are linear, we let OrdCE directly use the SCMs. \\\n\\\n**I also wonder if RL is a bit of an overkill for this problem since the number of features (p) is often quite small. It is often desirable to intervene on fewer features. For instance, the experiments drop the feature Capital Gain since intervening only on that one feature suffices for recourse. Also, what about exploration? Could the authors strengthen the motivation behind this approach?** \\\nWe believe one of the challenges of suggesting ideal interventions is considering the causations between features. Take figure 1 for example, considering the consequence of changing a variable (i.e., height) helps us improve recourse (i.e., +2 instead of +8 kg). We argue that handling such dependency can be challenging when more features are involved and we thus need an approach flexible and powerful enough to obtain ideal outcomes. \\\n\\\nAnother advantage of our RL approach lies in the efficiency. Optimization-based methods (e.g. DiCE [2]) solve an optimization problem for every instance. The proposed RL-based method, on the other hand,  performs fast inference for new instances after training.\\\n\\\n**And also, how is the time being calculated in the experiments? It seems to be only the inference time. What about preprocessing time? Could the authors discuss/elaborate on the preprocessing time of various methods?**\\\nWe finish preprocessing before inference. Since we apply the same preprocessing (normalization) to data for all methods, we did not include preprocessing time for comparison. Also, as reported in Appendix B.3,  our model and FastAR require training, and the training time of the 2 models are 10 - 50 minutes and 2 hours on Intel Xeon cpu E5-2650-v4.\\\n\\\n**The experiment section does not provide enough details on how the causal graph was generated for the real-world datasets and if that causal graph is reliable.** \\\nWe are sorry for the missing details. The sources of the causal graphs are as follows.\\\nSangiovese: https://www.bnlearn.com/bnrepository/clgaussian-small.html#sangiovese \\\nAdult: We use the pre-processed dataset released by the authors of FastAR [3] (https://github.com/vsahil/FastAR-RL-for-generating-AR). \\\nSynthetic: We report the structure and equations in Appendix B.2 for examination.\\\n\\\n**Ultimately, human evaluations might also be necessary at some point to compare different methods.**\\\nIndeed. We thank and agree with the reviewer for this suggestion.\n\n [1] Amir-Hossein Karimi, Bodo Julius von K\u00fcgelgen, Bernhard Sch\u00f6lkopf, and Isabel Valera. Algorithmic recourse under imperfect causal knowledge: a probabilistic approach. 2020.\\\n[2] Ramaravind K. Mothilal, Amit Sharma, and Chenhao Tan. Explaining machine learning classifiers through diverse counterfactual explanations. 2020.\\\n[3] Sahil Verma, Keegan Hines, and John P. Dickerson. Amortized generation of sequential algorithmic recourses for black-box models. 2022."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9061/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700589391895,
                "cdate": 1700589391895,
                "tmdate": 1700589391895,
                "mdate": 1700589391895,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ULqzr7Kad5",
            "forum": "oVVLBxVmbZ",
            "replyto": "oVVLBxVmbZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9061/Reviewer_1q46"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9061/Reviewer_1q46"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an efficient RL-based approach with the idea of conditional intervention, with the goal of handling noisy and/or incomplete graphs, as well as efficient performance of inference for black-box classifier. The experimental results show the efficiency of the proposed method on both synthetic and real datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper tackles an important problem in algorithmic recourse, which is causal sequential recourse, using the technique from reinforcement learning that works in a boarder setting compared to the previous paper."
                },
                "weaknesses": {
                    "value": "One weakness of the paper is the assumptions are pretty strong-- it feels like a lot of assumptions (e.g., the formulation of intervention cost) are made for mathematical convenience rather than for accurate modeling. In addition, the writing and structure of the paper can be improved; for example, it is still unclear to me how CIR is especially superior to existing methods in preserving causality and how the method handles incomplete graph cases. Answering the questions in the Questions section might help make some clarifications."
                },
                "questions": {
                    "value": "1. The paper mentions that \"The less it is determined by their parents, the more _space_ we can intervene.\" Could you explain more why that's the case? in particular, what does \"space\" mean? And why do we want to primarily intervene in higher uncertainty endogenous features? \n\n2. Does the size of the action space grow exponentially as a function of the feature space? If so, how does the algorithm handle this?\n\n3. Intuitively, what is the benefit of conditional intervention compared to traditional intervention? \n\n\nTypo:\n\n1. At the bottom of page 5, \"...$X_k$ is intervened upon is calculated by..\""
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9061/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699432535053,
            "cdate": 1699432535053,
            "tmdate": 1699637140587,
            "mdate": 1699637140587,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "S4uZhie1gM",
                "forum": "oVVLBxVmbZ",
                "replyto": "ULqzr7Kad5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9061/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9061/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your precious comments.\\\n\\\n**Response to weakness** \\\nSince we optimize the agent with reinforcement learning, it is in fact rather easy to optimize a complex conditional density for an endogenous feature in the design of the intervention cost. However, estimating accurate density conditioning on multiple parents is difficult due to data sparsity. Causal proximity from Mahajan et al. [1] is another extreme, in which only the first moment (conditional mean) is estimated. Our intervention cost (IC) estimates the first and second moment (conditional variance), which is an understandable statistic that describes uncertainty. Also the range constructed by conditional mean and variance is guaranteed to cover the actual distribution to some extent by Chebyshev\u2019s inequaltiy. \\\n\\\n**Answer to Q1** \\\nHigh uncertainty of an endogenous variable given its parents implies there exists an influential factor that is not observed. Namely, the parents are not sufficient to decide the value of the endogenous variable. Let us assume there are two variables A and B with a relation A \u2192 B where A fully decides B. If B needs to be higher to change the classifier output, the ideal suggestion from an AR method would be intervening A instead of B, as it is the only way to control B. On the contrary, if B is highly uncertain given A, there may exist a hidden factor C having relation C \u2192 B for intervention. As C is not observed, changing B through C is simplified to changing B in our AR method output. The \u201cspace\u201d in this case is the uncertainty of B given A. In this work, we model the uncertainty via variance of $P(B|A)$ introduced in Eq.6 and 8. \\\n\\\n**Answer to Q2** \\\nIt is true that the action space of discrete features can grow exponentially. To the best of our knowledge, our competitors all face the same challenge while our model has an advantage of inference speed as shown in the experiments. \\\n\\\n**Answer to Q3** \\\nThe main difference is that conditional intervention considers effects from the parents. \nConsider a teenager of 155cm and 70kg to make the basketball team. In the traditional intervention, height and weight are treated independently. It can result in the intervention of +8 kg to make the team, as 70+8kg seems to be a reasonable weight for a person, if the height is not considered. On the other hand, conditional intervention cares about the fact that (70+8) kg is too much for a 155cm person. It could seek for another option \u201c+2cm and + 4kg\u201d while considering both factors together. \\\n\\\n[1] Divyat Mahajan, Chenhao Tan, and Amit Sharma. Preserving causal constraints in counterfactual explanations for machine learning classifiers. 2019."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9061/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588247047,
                "cdate": 1700588247047,
                "tmdate": 1700591044586,
                "mdate": 1700591044586,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "m69rRGboEC",
                "forum": "oVVLBxVmbZ",
                "replyto": "S4uZhie1gM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9061/Reviewer_1q46"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9061/Reviewer_1q46"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your response"
                    },
                    "comment": {
                        "value": "I've carefully read the response from the author, and my evaluation remains the same."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9061/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700589108292,
                "cdate": 1700589108292,
                "tmdate": 1700589108292,
                "mdate": 1700589108292,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]