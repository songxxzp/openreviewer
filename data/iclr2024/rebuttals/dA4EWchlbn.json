[
    {
        "title": "Advancing the Adversarial Robustness of Neural Networks from the Data Perspective"
    },
    {
        "review": {
            "id": "edn6zRd76i",
            "forum": "dA4EWchlbn",
            "replyto": "dA4EWchlbn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3684/Reviewer_pfh7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3684/Reviewer_pfh7"
            ],
            "content": {
                "summary": {
                    "value": "The authors establish a connection between the curvature of the data manifold, as perceived by a model during training, and the model\u2019s adversarial robustness. They provide empirical evidence showing that neural networks gain adversarial robustness more slowly in less robust regions of the data manifold."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- A novel perspective on adversarial robustness through the lens of data robustness (curvature of data manifold)\n- Claims are backed up by empirical results\n- Generally good writing"
                },
                "weaknesses": {
                    "value": "The most critical issue is the lack of comparison with prior works. It is well-known in the literature that not all data points are equally susceptible to adversarial attack, and this has motivated the design of various variants of adversarial training ([1,2,3,4,5,6]) to (adaptively) focus on a subset of training samples. The authors made a similar observation \"it appears beneficial to emphasize the least robust elements during training\", but seemed to be completely unaware of this line of research. Without proper discussion and comparison with prior works, it is hard to fairly position and evaluate this work in the vast literature. \n\nReferences\n\n[1] Cai, Qi-Zhi, Chang Liu, and Dawn Song. \"Curriculum adversarial training.\" Proceedings of the 27th International Joint Conference on Artificial Intelligence. 2018.\n\n[2] Ding, Gavin Weiguang, et al. \"MMA Training: Direct Input Space Margin Maximization through Adversarial Training.\" International Conference on Learning Representations. 2019.\n\n[3] Wang, Yisen, et al. \"On the Convergence and Robustness of Adversarial Training.\" International Conference on Machine Learning. PMLR, 2019.\n\n[4] Zhang, Jingfeng, et al. \"Geometry-aware Instance-reweighted Adversarial Training.\" International Conference on Learning Representations. 2020.\n\n[5] Zeng, Huimin, et al. \"Are adversarial examples created equal? A learnable weighted minimax risk for robustness under non-uniform attacks.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 12. 2021.\n\n[6] Xu, Yuancheng, et al. \"Exploring and Exploiting Decision Boundary Dynamics for Adversarial Robustness.\" The Eleventh International Conference on Learning Representations. 2022.\n\n[7] Xiao, Jiancong, et al. \"Understanding adversarial robustness against on-manifold adversarial examples.\" arXiv preprint arXiv:2210.00430 (2022)."
                },
                "questions": {
                    "value": "One of the mainstream hypotheses regarding adversarial examples is the off-manifold assumption ([7]): \"Clean data lies in a low-dimensional manifold. Even though the adversarial examples are close to the clean data, they lie off the underlying data manifold.\" I would like to understand how this hypothesis is related to your findings (focusing on data manifold with high curvature is helpful in adversarial training)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3684/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3684/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3684/Reviewer_pfh7"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3684/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698545724224,
            "cdate": 1698545724224,
            "tmdate": 1699636325099,
            "mdate": 1699636325099,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "feoJHiT3ic",
                "forum": "dA4EWchlbn",
                "replyto": "edn6zRd76i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3684/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3684/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Official Review of Submission3684 by Reviewer pfh7"
                    },
                    "comment": {
                        "value": "## *We thank reviewer \"pfh7\" for the criticism and the recommended literature.*\n\n---\n---\n---\n\n## ---*First, we address the mentioned weaknesses regarding the lack of comparison with prior works to delineate our contribution.*---  \n---\nThere is indeed a vast corpus of literature regarding adversarial machine learning in general and adversarial training in particular. In order to (re)position our work with respect to the listed papers, we have revised our exposition to include the previous advancements and clarify which \u201cgap\u201d we intend to close with our contribution. More precisely, we focus on \u201cwhy\u201c adversarial training shows a weaker effect in regions of non-robust data which cannot be explained entirely by small margins between differently labelled points.\n\nTo stress this point, we added to the updated version that the minimal $l_\\infty$ distance between two non-robust CIFAR-10 images ($\\approx0.2118$) is more than six times as large as the maximum perturbation radius ($\\approx0.0314$) which, geometrically, leaves more than enough space for a robust decision boundary. However, it is not found consistently in these regions, as seen in Fig. 1.\n\nOur primary goal and contribution in this work is to provide a new perspective on this phenomenon which builds on a deeper geometric concept, namely curvature (as perceived by a model during training). Whereas one part of this contribution is the theoretical framework connecting the robustness of data to the perceived curvature, our empirical results show that this perspective is meaningful in practice.\nRegarding the adversarial training experiments, this view of perceived curvature can explain why focusing on a negligible number of non-robust points during training leads to an increased robust generalisation accuracy beyond the expected local improvements. Indeed, as we clarify in the updated version, the focus on the least robust points entails a better understanding of the pixel distances (the local improvement) and a better understanding of semantic distances (judging by the improved performance on unseen data). Together, this leads to a better understanding of curvature.\n\nWhereas we do improve upon the current state-of-the-art approach from Wang et al. (2023) in a cost-neutral way, the main contribution of our work is a deeper understanding of representation learning. Our focus is not to provide a new method of adversarial training (which is beyond the scope of this work) but to motivate why developing methods based on this deeper geometric insight is promising in the first place.\n\n---\n---\n---\n## ---*Regarding the question.*---\n---\nFirst, if shifting points away from the true data manifold alone would impede a model's performance, we would expect a much worse accuracy on data augmented with, for example, slight uniformly distributed noise. (It is mathematically impossible to generate noise tangent to the manifold without knowing its shape.) However, as shown recently (Xiao et al. (2022); Chen et al. (2023)), it is possible to generate (unrestricted) semantic adversarial examples which are not only close but on the data manifold.\n\nWhat the view of a model perceiving curvature allows is to combine both types of vulnerabilities described above. Adversarial noise normal to the data manifold is detrimental even when the model's internal representation of the data concurs with the true representation. On the other hand, adversarial variations of the data may align with the data manifold if the model's learned representation of the latter is distorted in the first place. Our proposed view of a model perceiving curvature can explain how and where these distorted representations appear and why, for example, adversarial training shows a weaker effect in the least robust regions.\n\nIntuitively, in the regions of perceived high curvature, the training algorithm forces a model to connect a binary \"semantic\" change (labels are either different or not) to a relatively small change in terms of pixels (as is the case in the least robust regions). This may lead to the model adapting by learning spurious correlations to account for this perceived imbalance between semantic and pixel variation, ultimately making the model more susceptible to (adversarial) noise. This can explain why models can even be susceptible to noise perturbations tangent to the data manifold (because they are not aligned with the model's internal representation of the same)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3684/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700602084822,
                "cdate": 1700602084822,
                "tmdate": 1700602084822,
                "mdate": 1700602084822,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GL3Byw656H",
            "forum": "dA4EWchlbn",
            "replyto": "dA4EWchlbn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3684/Reviewer_vUSY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3684/Reviewer_vUSY"
            ],
            "content": {
                "summary": {
                    "value": "The paper leverages concepts from metric geometry to understand how neural networks perceives the geometric property, particularly curvature, of the data manifold. To be more specific, it argues that data that more susceptible to adversarial perturbations are connected to regions in the data manifold with high curvature and vice versa. The paper proposes to use Eq 1 to quantitatively measure such curvature information as perceived by the model. The empirical studies in this paper are based on CIFAR10 and CIFAR100. A series experiments are performed to verify the proposed connection between curvature and model robustness. Building on these findings, the paper propose a learning rate strategy that increases the adversarial robustness model against $\\ell_2$ and $\\ell_\\infty$-norm bounded adversarial perturbations generated using AutoAttack in a white-box setting."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**Originality**:\nThe paper proposes a very novel (to my knowledge) and unique perspective to understand the adversarial robustness of neural networks. The proposed concept is also quite intuitive: certain data points are inherently more susceptible to adversarial attacks due to specific properties they possess.\n\n**Quality**:\nI am not an expert in manifold learning nor in metric geometry, so this limit my ability to properly assess the technical details of Section 3.  However, on the empirical side, the paper tries to provide several experiments to validate their hypothesis regarding the connection between model's robustness and its perception on the curvature of data manifold.\n\n**Significance**:\nThe proposed data perspective provides an interesting direction on which future methods can be designed to better address the adversarial robustness problem."
                },
                "weaknesses": {
                    "value": "The paper has two main weaknesses.\n\n1. The clarity of the presentation and the quality of interpretation regarding the empirical observations could benefit from further refinement to enhance understanding.\n2. The improvement in adversarial robustness is very marginal."
                },
                "questions": {
                    "value": "**Figure 1**:\nI suppose this is on CIFAR10. How are the most/east robust elements defined?\nThe claim at the bottom of page 1 is essentially that by training for another 2000 epochs, the most robust training data (at epoch 400) become 1% more robust (at epoch 2400); and the least robust training data (at epoch 400) becomes 7.5% more robust (at epoch 2400). Is that correct? However, are we tracking the same training data? Is \"eval_adversarial_acc\" the validation accuracy? \n\nAlso, it seems that the author uses the term \"validation set\" as a parts of the training set, which is odd.\n\nI did not understand \"For CIFAR-100, we disregarded the 40 least robust elements because of distorted robustness values due to differently labelled duplicate images\". What are \"differently labelled duplicate images\"? What happens to the result if we include them?\n\n\n**Figure 2**:\nMy understanding is that the absolute sensitivity is computed using Eq1. How are the relative sensitivity computed? \nIn the analysis of Figure2, it is said that \"The generated data, in particular, admit much shorter tails, indicating more robust elements overall. \" However, arent the curves of  1m-1 and 1m-2 quite similar to cifar-10/100/100s? Also, why does the pseudo-labbeled data have very different sensitivity? \n\n**Figure 3**: \nThe interpretation of the results are not clear. Do we know what the differences in the mechanisms of MDS and RTE are that leads to this inverted pattern? It would be very helpful to interpret the result if there is some brief explanation on what those methods are and why they are used.\n\n**General suggestions on all figures**:\nPlease consider using subcaptions to increase the clarity of the results.\n\n**Exploration experiments**:\nIn the setup column of Table2, just to clarify: minus v, \"-v\" means removing v from the training set, and (v) means all the validation accuracy is based on v. Is this correct?\nAre the most sensitive data points computed based on another pre-trained model?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3684/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3684/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3684/Reviewer_vUSY"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3684/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698983043916,
            "cdate": 1698983043916,
            "tmdate": 1699636324985,
            "mdate": 1699636324985,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yw4MKwZSMB",
                "forum": "dA4EWchlbn",
                "replyto": "GL3Byw656H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3684/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3684/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Official Review of Submission3684 by Reviewer vUSY"
                    },
                    "comment": {
                        "value": "## *We thank reviewer \"vUSY\" for the criticism and the suggestions to improve our work.*\n---\n---\n---\n## ---*First, we address the mentioned weaknesses individually.*---  \n---\n> *The clarity of the presentation and the quality of interpretation regarding the empirical observations could benefit from further refinement to enhance understanding.*\n\nTo clarify the presentation of empirical results, we rephrased and refined Section 4 to make it less convoluted and more accessible. We focused on more intuitive descriptions, especially for readers who are not necessarily familiar with the topics.\n\n> *The improvement in adversarial robustness is very marginal.*\n\nIn absolute terms, an improvement of 0.2 percentage points seems small but, as stated in the paper, amounts to the same increase of extending the training by 800 epochs, that is, one-third of the entire training (based on results from Wang et al. (2023)). Importantly, this benefit is achieved using identical computational resources.\nHowever, the primary goal of these experiments is to show that focusing on a negligible number of points from non-robust regions leads to an increased robust generalisation accuracy beyond the expected local improvements. In other words, a better understanding of the pixel distances between points is paired with a better understanding of semantic distances (hence the improved performance on unseen data).\n\n\n---\n---\n---\n## *Regarding the questions.*\n---\n\n\n\n> *Figure 1: I suppose this is on CIFAR10. How are the most/east robust elements defined?*\n\n\nThe robustness of a data point is the minimal distance to a differently labelled data point. We added an explicit definition in Section 3 to avoid confusion. However, the absolute robustness (or, reciprocally, sensitivity) value is of secondary importance. It is the order of points when sorted according to their robustness (or sensitivity) values which lets us connect these concepts to the perceived curvature of a model. For this reason, we always refer to the \"least and most robust points\" instead of \"points with a robustness of <x> or smaller/larger than <x>\". To make the concept more accessible to readers, we changed the introduction by taking a more intuitive approach to explaining data robustness. \n\n\n\n\n> *The claim at the bottom of page 1 is essentially that by training for another 2000 epochs, the most robust training data (at epoch 400) become 1% more robust (at epoch 2400); and the least robust training data (at epoch 400) becomes 7.5% more robust (at epoch 2400). Is that correct?* \n\nCorrect.\n\n\n> *However, are we tracking the same training data? Is \"eval_adversarial_acc\" the validation accuracy?*\n\n\nThe \"eval_adversarial_acc\" traces the adversarial robustness against a PGD-40 attack with maximum perturbation radius 8/255 and step size 2/255 on the 1024 least and most robust elements, respectively. We added more information to the introduction to clarify this.\nHowever, these subsets are also fixed parts of the training data in every epoch, allowing us to enforce a focus on these points during adversarial training. This example demonstrates that the effect of adversarial training is much weaker in non-robust regions compared to robust regions.\nWe modified the exposition of this example by (i) removing two plots and (ii) changing its explanation, where we chose a more natural way to introduce data robustness beforehand.\n\n\n\n\n\n\n\n\n\n\n\n> *Also, it seems that the author uses the term \"validation set\" as a parts of the training set, which is odd.*\n\nTo avoid confusion, we changed the nomenclature at several points.\n\n\n\n\n\n\n\n> *I did not understand \"For CIFAR-100, we disregarded the 40 least robust elements because of distorted robustness values due to differently labelled duplicate images\". What are \"differently labelled duplicate images\"? What happens to the result if we include them?*\n\nAs we found out, the CIFAR-100 set contains identical images with different labels. For such data points, the sensitivity is not defined due to the vanishing quotient (comp. Equation 1). Based on the implementation, the sensitivity computation for these points would either return \u201cNan\u201ds or extremely large values (as for us), both of which are inadmissible and distort the sensitivity distributions.  \n\n\n\n\n\n\n> *Figure 2: My understanding is that the absolute sensitivity is computed using Eq1. How are the relative sensitivity computed?*\n\nThe relative sensitivity distributions are empirical probability distributions over the absolute sensitivity values (similar to histograms). However, since these plots confuse readers (and add barely any meaningful information), we removed them."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3684/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700601411126,
                "cdate": 1700601411126,
                "tmdate": 1700601411126,
                "mdate": 1700601411126,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dDxeQ6LhXu",
                "forum": "dA4EWchlbn",
                "replyto": "xBRVOplcNw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3684/Reviewer_vUSY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3684/Reviewer_vUSY"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I will go through the revised paper shortly and then go through your responses. \nMeanwhile, I have a quick question. How statistically significant is the 0.2% improvement? Is it an average over multiple runs? Given this modest improvement, particularly, it is important to report other statistics such as the total number of runs, the standard deviation between the runs, etc."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3684/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679657613,
                "cdate": 1700679657613,
                "tmdate": 1700679657613,
                "mdate": 1700679657613,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ujVDHwfPzM",
            "forum": "dA4EWchlbn",
            "replyto": "dA4EWchlbn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3684/Reviewer_8FHc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3684/Reviewer_8FHc"
            ],
            "content": {
                "summary": {
                    "value": "The authors investigate the robustness of neural networks undergoing training via gradient descent through the lens of the geometry of the data. The authors analyze the dynamic of robustness by proposing a measure of \u201cperceived curvature\u201d. Essentially, the perceived curvature resembles the local Lipschitz constant exhibited by the neural network, modified so that the predictions are mapped to the discrete set of labels. Algorithmically, the authors analysis implies that by emphasizing the least-robust elements of the training set, modest gains in adversarial test error can be achieved. \n\nThe authors perform exploratory experiments by showing some correlation between the perceived manifold curvature and robustness as well as visualizations depicting the most and least robust examples and data sensitivity.\n\nWhile the paper is interesting and the experiments are reasonably comprehensive, I do not think this paper offers particularly new or deep insight into the nature of adversarial robustness, beyond what has been explored by prior work. These issues coupled with the quality of the writing and composition make me inclined to reject, although I am open to changing my score."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Interesting application of diffusion models to investigate the adversarial robustness of a neural network for certain examples via a notion of perceived curvature\n- Comprehensive visualizations, plots to demonstrate the relationship between per-sample-robustness, sensitivity, and margin"
                },
                "weaknesses": {
                    "value": "**Contribution / significance**\n\nThe basic observation made by the paper regarding the relationship between robustness, sensitivity, and sample importance during training is interesting, but well-known. To strengthen the contribution and significance of the work, the authors should clarify the contribution of their analysis in the context of the previous work, or demonstrate some actionable insights- e.g. an algorithm that exhibits superior adversarial robustness relative to existing techniques.\n\n**Missing relevant work**\n\nThere is some missing existing work that should be cited that explores the emphasis of certain vulnerable examples in the training set to enhance clean and robust test-set performance. E.g. reweighting methods such as [1, 2], subsampling methods such as [3],  and others that I do not list (e.g. on applications of diffusion models to the adverarial robustness context). \n\n[1] Zhang et al., Geometry-aware Instance-reweighted Adversarial Training, ICLR 2021\n\n[3] Wang et al., Probabilistic Margins for Instance Reweighting in Adversarial Training, NeurIPS 2021\n\n[3] Zhang et al., Attacks Which Do Not Kill Training Make Adversarial Learning Stronger, ICML 2020\n\n**Writing and composition**\n\nThe writing could use some work. Several seemingly important statements are made, but I found it difficult to parse the english. For example, the following are examples:\n\n_However, we argue that the labels evoke the impression of disconnectedness, which a model then tries to account for when remodelling the perceived decision boundaries during training._\n\n_Although the skewed label distribution of s (comp. Figure 5) should come as a disadvantage, one may, a priori, argue for the converse\u2026_\n\nI also could not understand the idea of figure 1. The preceding text states that they intend to illustrate some previous claim, but the previous claim seems to be about data sensitivity and curvature, while figure 1 details the adversarial robustness / robustness gap for models trained for different numbers of epochs with a certain overlap between the training and validation sets. The experiment seems very complicated and difficult to understand compared to the claim that \n\n_It appears beneficial to emphasise the least robust elements during training to help a model gain robustness in regions where it struggles the most._"
                },
                "questions": {
                    "value": "- Could the authors clarify their contribution in the context of existing methods / analysis (e.g. by providing some explanation for the efficacy of existing methods to enhance robustness)?\n- One claim is that _diffusion model connects regions of non-robust data to more prominent semantic changes, which we take as the model accounting for a more significant perceived curvature._ Can this be made more precise?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3684/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699032920213,
            "cdate": 1699032920213,
            "tmdate": 1699636324903,
            "mdate": 1699636324903,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "R8YFqHspAH",
                "forum": "dA4EWchlbn",
                "replyto": "ujVDHwfPzM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3684/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3684/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Official Review of Submission3684 by Reviewer 8FHc"
                    },
                    "comment": {
                        "value": "## *We thank reviewer \"8FHc\" for the criticism and their literature recommendations.*\n---\n---\n---\n## ---*First, we address the mentioned weaknesses individually.*--- \n---\n**Contribution / significance**\n\nTo explain and delineate our contribution from existing work and to show the significance of the proposed link between data robustness and the perceived curvature of a model, we reframed our exposition in the updated version.\n\nAs mentioned by the reviewer, it is known that nearby differently labelled points (= non-robust points) enforce small margins, which naturally impair adversarial training in such regions (as showcased in the first example). However, while \"less space\" may explain the weaker effect of adversarial training in such non-robust regions, it fails as an explanation if \"less space\" is \"still more than enough space\". To strengthen our argument, we included the following point: the minimal $l_\\infty$ distance between two non-robust CIFAR-10 images ($\\approx0.2118$) is more than six times as large as the maximum perturbation radius ($\\approx0.0314$) which, geometrically, leaves more than enough space for a robust decision boundary. Nevertheless, a robust decision boundary is not found during training for a significant portion of the non-robust points (comp. Fig. 1). As our primary contribution in this work, we provide a new perspective for this phenomenon to explain why these non-robust points are \"difficult\" for any model and why adversarial training may not lead to a robust decision boundary, even if we know it exists.\n\nWhereas this motivates new adversarial training methods, the development of such a method is beyond the scope of this work. Instead, we provide theoretical and empirical evidence to show that this curvature-based perspective is meaningful in practice and can motivate new methods of adversarial training in the future. We made significant changes in the introduction and Section 4 to clarify this point.\n\n**Missing relevant work**\n\nWe included the listed works to explain what phenomenon we focus on in this work (which cannot be explained by traditional margin-based optimisation strategies; see explanation above). We highlight that our primary contribution is a new and deeper geometry-based perspective to explain why adversarial training shows a weaker effect in certain regions. We clarified this point in the updated version.\n\n\n**Writing and composition**\n\nWe reframed several sentences in the updated version to make the content less convoluted and clarified some essential aspects to make them more accessible. To this end, we redesigned the example in the introduction as well.\n\n---\n---\n---\n## ---*Regarding the questions.*--- \n---\n> *Could the authors clarify their contribution in the context of existing methods / analysis (e.g. by providing some explanation for the efficacy of existing methods to enhance robustness)?*\n\nOur primary contribution is an explanation for the weaker effect of adversarial training in non-robust regions, which small margins between differently labelled points cannot entirely account for. This explanation, building on connections between a model\u2019s perception of curvature and the robustness of data, can motivate new methods of adversarial training in the future. In this work, we aim to provide theoretical and empirical evidence to demonstrate that this connection exists and is meaningful in addition to the development of new methods (which is beyond the scope of this paper). \n\n\n> *One claim is that diffusion model connects regions of non-robust data to more prominent semantic changes, which we take as the model accounting for a more significant perceived curvature. Can this be made more precise?*\n\nWe rephrased the explanation in the paper to clarify what we mean. In essence, we use the diffusion model to generate new images from the most and least robust regions. As we demonstrate, the semantic differences between the original and generated images are much smaller for the most robust elements than the least robust ones. Hence, the semantic distance (or manifold distance $\\tilde{d}$) is larger for the latter. By comparing the corresponding $l_2$ and $l_\\infty$ distances between original images and clones, we notice that the converse holds for the pixel distance. More precisely, the pixel distance ($d$) between the original and generated images is more significant for the most robust elements compared to the least robust elements.\n\nBy comparing the semantic and pixel distances in the form of the Finsler-Haantjes curvature, we see that the diffusion model connects regions of lower/higher curvature with regions of robust/non-robust data. Here, we refer to regions instead of single points, as curvature and robustness (or, reciprocally, sensitivity) are local concepts (comp. Theorem 1)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3684/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700599873023,
                "cdate": 1700599873023,
                "tmdate": 1700599873023,
                "mdate": 1700599873023,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0gJxHKG9sl",
            "forum": "dA4EWchlbn",
            "replyto": "dA4EWchlbn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3684/Reviewer_yQLb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3684/Reviewer_yQLb"
            ],
            "content": {
                "summary": {
                    "value": "The paper is on adversarial robustness and proposes that models are particularly vulnerable in regions where the data manifold would be perceived as highly curved by the model. Some theoretical developments are proposed to support that. Experiments are conducted to demonstrate that by oversampling data samples in curved areas and using them to generate new artificial samples for training would improve robustness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Tackle a significant question on understanding better the input space of deep models and the corresponding robustness to adversarial attacks.\n- Support claims through elaborated theoretical developments."
                },
                "weaknesses": {
                    "value": "- The paper and overall presentation is very difficult to follow. Although the authors seem to know very well their topic, the communication is lacking and a reader not in that specific field gets lost quite quickly.\n- The notion of curvature on the manifold is really unclear to me and not very well explained in the paper. But it appears in the end we are looking at distance between samples, the notion of curvature is there to support theoretical developments that are not directly translated in practice.\n- The technical aspects of the experiments section are not very clear nor clearly explained. I guess that the reader should look at some of the referenced papers like Karras et al. (2022) and Wang et al. (2023), but still I would like to get more background and specific details to better understand what is done in the experiments. It is quite unclear to me that the details provided would make results reproducibility easy.\n- It is difficult to figure out what exactly the experimental results are providing as support to the conclusion. The differences in Table 2 between the results is very small, and as such not very convincing that the proposal is of any meaningful effects for improving robustness.\n- Overall, the experiments are not very well explained and presented and the results are very difficult to interpret. I have a hard time making sense of all this."
                },
                "questions": {
                    "value": "Looking at equation 1, if we assume that $d(p_i,p_j)$ is an Euclidean distance and that $\\|y(p_i)-y(p_j)\\|$ is basically equal to zero or one when using $y(p)$ as one hot vector over the classes, it means that in practice, the proposal consists of looking 1/distance to the nearest sample from a different class from the current one. Is this correct? That\u2019s what was used for the experiments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns with this paper."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3684/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699241233038,
            "cdate": 1699241233038,
            "tmdate": 1699636324825,
            "mdate": 1699636324825,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hgo0pbbL7J",
                "forum": "dA4EWchlbn",
                "replyto": "0gJxHKG9sl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3684/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3684/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Official Review of Submission3684 by Reviewer yQLb"
                    },
                    "comment": {
                        "value": "## *We thank reviewer \"yQLb\" for the criticism.*\n\n---\n---\n---\n\n## ---*First, we address the mentioned weaknesses regarding the exposition and clarity of our paper.*---  \n---\nIn order to make the content of our paper more accessible, we have rewritten the introduction. The primary goal was (i) to make abstract concepts such as data robustness and the data manifold's curvature more accessible and (ii) to clarify what our contribution in this context is. We removed two plots in Figure 1 and added a schematic to clarify what we mean by the different distances in feature space and along the data manifold (as a preliminary for curvature).\n\nTo show why perceived curvature is meaningful, we outlined how it can explain the weaker performance of adversarial training in regions of non-robust data where robust decision boundaries exist but are not found during training.\n\nTo clarify how our experimental evidence supports our theoretic view (of a model perceiving curvature differently in robust and non-robust regions), we have also rewritten Section 4. One goal was to convey an intuition for why we observe a diffusion model generating semantically very different images in the least robust regions but not in the most robust regions. Overall, we explained the experimental setups in a less convoluted way to avoid confusion.\n\nFinally, the gains in adversarial robustness appear small but are indeed significant in this area, where we refer to the work of **Wang et al. (2023)**. However, the surprising fact we stressed in the updated version is that focusing on a negligible number of points from non-robust regions leads to an increased robust generalisation accuracy beyond the expected local improvements. In other words, a better understanding of the pixel distances between points is paired with a better understanding of semantic distances (hence the improved performance on unseen data).\n\n---\n---\n---\n\n## ---*Regarding the question.*---\n---\nYes. This \u201csensitivity\u201d information was used in the experiments. However, as shown theoretically, we are not interested in the absolute sensitivity value but the value compared to the entire sensitivity spectrum. In this way, sensitivity has only meaning in relative terms. This is why we always refer to the \u201cmost and least sensitive or robust points\u201d instead of the \u201cpoints with a sensitivity above/below x\u201d. Ultimately, this allows us to connect the primitive measure of sensitivity to the elaborate concept of curvature.\n\n---\n---\n---\n\n## ---*References*--- \n---\nZekai Wang, Tianyu Pang, Chao Du, Min Lin, Weiwei Liu, and Shuicheng Yan. Better diffusion models\nfurther improve adversarial training. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara\nEngelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International\nConference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp.\n36246\u201336263. PMLR, 23\u201329 Jul 2023."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3684/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700598826428,
                "cdate": 1700598826428,
                "tmdate": 1700599900808,
                "mdate": 1700599900808,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]