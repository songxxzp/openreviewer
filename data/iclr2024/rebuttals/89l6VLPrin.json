[
    {
        "title": "Graph layouts and graph contrastive learning via neighbour embeddings"
    },
    {
        "review": {
            "id": "L9IazmQM4L",
            "forum": "89l6VLPrin",
            "replyto": "89l6VLPrin",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5711/Reviewer_JHfU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5711/Reviewer_JHfU"
            ],
            "content": {
                "summary": {
                    "value": "First, this paper introduces graph t-SNE for two-dimensional graph drawing, and shows that the resulting layouts outperform all existing algorithms in terms of local structure preservation, as measured by kNN classification accuracy. Second, the work introduces graph contrastive neighbor embedding (graph CNE), which uses a fully connected neural network to transform graph node features into an embedding space by optimizing the contrastive InfoNCE objective."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The experimental performance is good.\n\n2. This method is easy to implement and experimentally efficient."
                },
                "weaknesses": {
                    "value": "1. This work uses MLP to transform node features and gives reasons for not using GCN. If possible, I think it would be helpful to include experiments that use GCN to transform node features.\n\n2. This work seems to apply only at the node level but not at the graph level."
                },
                "questions": {
                    "value": "Is the proposed method applicable at the graph level?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5711/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5711/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5711/Reviewer_JHfU"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5711/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698761896951,
            "cdate": 1698761896951,
            "tmdate": 1699636597499,
            "mdate": 1699636597499,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QO9v0XEhRh",
                "forum": "89l6VLPrin",
                "replyto": "L9IazmQM4L",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5711/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5711/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "> The experimental performance is good. This method is easy to implement and experimentally efficient.\n\nWe thank the Reviewer for their positive evaluation!\n\nIndeed, our graph t-SNE beats SOTA results by a large margin, and our graph CNE beats SOTA on MLP-based graph contrastive learning. Also, we  believe that the connection between graph layouts and graph contrastive learning is a novel and important insight of our paper. \n\n> This work uses MLP to transform node features and gives reasons for not using GCN. If possible, I think it would be helpful to include experiments that use GCN to transform node features.\n\nWe did not use GCNs because all existing GCL papers use GCNs, so we thought it is sufficient to refer to them. For example, Local-GCL is the most similar method to ours but uses GCN. Conveniently, Local-GCL authors also report the results using MLP architecture (quoted in our Table 2), and our Graph CNE outperforms them on **all** datasets.\n\nAs we explain in our Discussion, we think that for node-level graph learning, GCNs are not very appropriate, as they cannot process one node at a time. GCNs can be very suitable for graph-level contrastive learning, but this is conceptually  a very different task requiring different datasets (not one graph at a time, but a collection of graphs, such as e.g. molecular stuctures).\n\nWe have now found another existing paper that reports MLP-based results (Guo et al., NeurIPS 2023, run experiments on GRACE with MLP: https://arxiv.org/abs/2311.02687), and added it to Table 2. Strikingly, Graph CNE outperforms both reported MLP-based GCL results on **all** datasets.\n\n\n> This work seems to apply only at the node level but not at the graph level. [...] Is the proposed method applicable at the graph level?\n\nNot really. Of course one can average the node-level representation across all nodes and thus obtain a graph-level representation. But we do not think that our method is a reasonable approach for that. To be honest, we do not consider this is a limitation: graph-level contrastive learning is a very different task requiring different datasets (not one graph at a time, but a collection of graphs, such as e.g. molecular structures) and different methods.\n\nIn this work we focused on node-level learning, both in 2D (graph layouts) and in high-D (graph contrastive learning).\n\nIf there any further concerns, we will be happy to clarify!"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700167778050,
                "cdate": 1700167778050,
                "tmdate": 1700167801633,
                "mdate": 1700167801633,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sxM3AEUWSz",
                "forum": "89l6VLPrin",
                "replyto": "QO9v0XEhRh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5711/Reviewer_JHfU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5711/Reviewer_JHfU"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the detailed response provided by the authors. I will maintain the original score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662864935,
                "cdate": 1700662864935,
                "tmdate": 1700662864935,
                "mdate": 1700662864935,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "10y5XzwZcB",
            "forum": "89l6VLPrin",
            "replyto": "89l6VLPrin",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5711/Reviewer_WEvC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5711/Reviewer_WEvC"
            ],
            "content": {
                "summary": {
                    "value": "The authors aim to achieve both graph layout and node-level graph contrastive learning by neighbor embedding methods. They simplify $t$-SNE and CNE by using graph adjacency matrix as their proposed neighbor embedding methods. Experiments with 3 metrics on 6 public datasets are conducted to demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The results on graph layouts look promising.\n\n2. The proposed graph CNE does not rely on the entire graph as input."
                },
                "weaknesses": {
                    "value": "Please kindly correct me if I misunderstood something.\n\n**Majors:**\n\n1. In Section 4, the authors say \"we split all nodes into a training (2/3 of all nodes) and a test set (1/3 of all nodes).\" Is this setting applied to all datasets? How to choose nodes for training? Why not use $K$-fold splits?\n\n2. In Section 6, Paragraph 2, the authors say \"we used the cosine distance and the Gaussian similarity kernel for $d$=128.\" Does it make sense to evaluate the models by $k$NN recall/accuracy, which is based on Euclidean distance?\n\n3. In Section 6, Paragraph 3, the authors say \"The number of epochs was set to 100.\" I think the number of epochs should be decided by the convergence of a model. The authors also say \"running CNE for more epochs and/or with higher $m$ could likely yield better results\" and \"be due to insufficient optimization length and/or $m$ value\" in the following paragraphs. Does this mean the models are not well-trained?\n\n4. In Section 3.2, the authors use CNE to denote the framework of [1], and the proposed contrastive learning model is named graph CNE. However, the CNE in Figure 4 and Table 2 seems to denote the proposed method, which is confusing.\n\n        [1] Damrich, Sebastian, et al. \"From t-SNE to UMAP with contrastive learning.\" The Eleventh International Conference on Learning Representations. 2022.\n\n5. In Table 2, are the baseline methods using the same experiment settings (for example, train/val/test split) as the proposed method?\n\n6. The authors claim that \"graph CNE performed comparably to the state-of-the-art graph contrastive learning algorithms.\" However, in Table 2, Local-GCL looks better. Please provide more evidence (for example, $t$-test) to support this sentence.\n\n7. Why not try GNN architecture if the authors want to compare the proposed graph CNE with other GCL methods?\n\n**Minors:**\n\n8. Why is Figure 1 shown on Page 1 but cited on Page 7?\n\n9. How are the hyperparameters decided?\n\n10. \".. where running CNE for more epochs and/or with higher $m$ could likely yield better results.\" Is the $m$ explained somewhere in this paper?"
                },
                "questions": {
                    "value": "Please see the weaknesses part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5711/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5711/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5711/Reviewer_WEvC"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5711/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698806813576,
            "cdate": 1698806813576,
            "tmdate": 1700583868994,
            "mdate": 1700583868994,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "roL5Ai68mC",
                "forum": "89l6VLPrin",
                "replyto": "10y5XzwZcB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5711/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5711/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response, part 1"
                    },
                    "comment": {
                        "value": "> The results on graph layouts look promising. The proposed graph CNE does not rely on the entire graph as input.\n\nWe thank the Reviewer for this assessment!\n\nJust to clarify what we think our main contributions are:\n\n* We are the first to point out and explore conceptual connection between graph layouts and node-level graph contrastive learning, tying together two distinct fields.\n* We suggest 2D graph contrastive learning (graph CNE with $d=2$) as a novel graph learning task as a \"missing link\" between graph layouts and standard graph contrastive learning. \n* Our graph t-SNE beats SOTA on graph layouts by a large margin (!).\n* Our graph CNE beats SOTA on MLP-based graph contrastive learning.\n\nThe Reviewer scored the \"novelty\" aspect as 1, but we believe the our work contains a whole lot of novel results: both conceptual insights and SOTA achievements!\n\n> In Section 4, the authors say \"we split all nodes into a training (2/3 of all nodes) and a test set (1/3 of all nodes).\" Is this setting applied to all datasets? How to choose nodes for training? Why not use K-fold splits?\n\nYes, we used the same 2:1 splitting on all datasets. The set of training nodes was selected randomly. We could indeed use K-fold cross-validation, but used a fixed train/test split for simplicity and because it is standard in the field. The graphs that we used are large enough (number of nodes $n>2000$) so that the sampling error here is small anyway.\n\nWe do not see this as a limitation, but if the reviewer disagrees, we are happy to provide some K-fold classification results for comparison.\n\n> In Section 6, Paragraph 2, the authors say \"we used the cosine distance and the Gaussian similarity kernel for d=128.\" Does it make sense to evaluate the models by NN recall/accuracy, which is based on Euclidean distance?\n\nThis is an excellent question! Indeed, for $d=128$ embedding we also tried using cosine-metric kNN graph for measuring recall/accuracy, but found that it did not noticeably affect the results. So for simplicity we only reported Euclidean kNN evaluation. We have now inserted a clarification about it into the text.\n\n> In Section 6, Paragraph 3, the authors say \"The number of epochs was set to 100.\" I think the number of epochs should be decided by the convergence of a model. The authors also say \"running CNE for more epochs and/or with higher could likely yield better results\" and \"be due to insufficient optimization length and/or m value\" in the following paragraphs. Does this mean the models are not well-trained?\n\nThank you for bringing this up. Since the original submissions, we have done additional experiments, and by now believe that 100 epochs is sufficient for convergence on all our datasets. We removed all statements that you quoted here.\n\n> In Section 3.2, the authors use CNE to denote the framework of [1], and the proposed contrastive learning model is named graph CNE. However, the CNE in Figure 4 and Table 2 seems to denote the proposed method, which is confusing.\n\nThank you! We changed Figure 4 and Table 2 to always say \"graph CNE\".\n\n> In Table 2, are the baseline methods using the same experiment settings (for example, train/val/test split) as the proposed method?\n\nWe took the values reported in Table 2 from the literature (as indicated in the caption). The train/test splits there could be different. However, given that the graphs are large and that the train sets are selected at random, that should not play a major role. In fact, the variability due to network re-training (which is what is shown in Table 2 as $\\pm$ intervals) is likely larger than the variability due to the test set selection. We have now clarified where exactly the quoted values are coming from."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700167654044,
                "cdate": 1700167654044,
                "tmdate": 1700167654044,
                "mdate": 1700167654044,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vLv0ZIajq7",
                "forum": "89l6VLPrin",
                "replyto": "10y5XzwZcB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5711/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5711/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response, part 2"
                    },
                    "comment": {
                        "value": "> The authors claim that \"graph CNE performed comparably to the state-of-the-art graph contrastive learning algorithms.\" However, in Table 2, Local-GCL looks better. Please provide more evidence (for example, t-test) to support this sentence.\n\nIndeed, in Table 2 some GCL methods outperform our Graph CNE, but only by a small margin, and on one of the datasets (PUB), Graph CNE is actually the best. This is what we mean by \"comparably\": not that it performs the best, but it performs close to the best.\n\nCrucially, as we explain in the paper, the more relevant comparison is to the GCL methods when they also use MLP architecture. We have now found another existing paper that reports MLP-based results (Guo et al., NeurIPS 2023, run experiments on GRACE with MLP: https://arxiv.org/abs/2311.02687), and added it to Table 2. Strikingly, Graph CNE outperforms both reported MLP-based GCL results on **all** datasets.\n\n> Why not try GNN architecture if the authors want to compare the proposed graph CNE with other GCL methods?\n\nThis is a good question. We did not use GCNs because all existing GCL papers use GCNs, so we thought it is sufficient to refer to them. Local-GCL is the most similar method to ours but uses GCN. Conveniently, Local-GCL authors also report the results using MLP architecture (quoted in our Table 2), and our Graph CNE outperforms them on all datasets.\n\nAs we explain in our Discussion, we think that for node-level graph learning, GCNs are not very appropriate, as they cannot process one node at a time. For graph-level learning, GCNs can be very appropriate, but that is another story.\n\n> Why is Figure 1 shown on Page 1 but cited on Page 7?\n\nWe added a reference to Figure 1 on Page 2, and also in some other places throughout the text. Thank you.\n\n> How are the hyperparameters decided?\n\nFor graph t-SNE, we used all default hyperparameters of openTSNE.\n\nFor graph CNE, we mostly used default hyperparameters of CNE, but increased the number of epochs and the number of negative samples to ensure convergence and improve the InfoNCE approximation (as per Damrich et al. 2023). We also adapted the batch size to ensure convergence, based on our pilot experiments.\n\nWe added some clarifications to the text. \n\n> \".. where running CNE for more epochs and/or with higher m could likely yield better results.\" Is the m explained somewhere in this paper?\n\nYes, the $m$ value is explained in Section 3.2 (see Equation 4), but we now removed this sentence because we now believe that $m=100$ that we used was sufficiently high.\n\nIf there any further concerns, we will be happy to clarify!"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700167680902,
                "cdate": 1700167680902,
                "tmdate": 1700167820018,
                "mdate": 1700167820018,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wP3YtInzbV",
                "forum": "89l6VLPrin",
                "replyto": "vLv0ZIajq7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5711/Reviewer_WEvC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5711/Reviewer_WEvC"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Rebuttle"
                    },
                    "comment": {
                        "value": "I appreciate the authors' efforts in providing a detailed response and an updated submission, which addresses some of my concerns, so I upgraded my rating to weak reject. However, I still have major concerns in the contrastive learning experiment settings (Q1, Q5, and Q7) as follows:\n\nIt is not fair to compare the proposed method with the baselines if they are not done under the same experiment settings.\n\n**For training/val/test splits**, the proposed method \"split all nodes into a training (2/3 of all nodes) and a test set (1/3 of all nodes),\" but the baseline methods are not. The Citeseer/Cora/Pubmed datasets have public splits, and the APH/ACO datasets are split as 1:1:8 by some of the baselines, which is significantly different from the proposed method.\n\n**For architecture**, the proposed method adopts MLP and many other baselines use GCN. It is good if the proposed method can outperform others, however, it does not. I suggest the authors explore the GNN architecture to showcase the superior performance of the proposed method under similar settings compared to others, and then elucidate the trade-off issues associated with employing MLP."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700583842939,
                "cdate": 1700583842939,
                "tmdate": 1700583842939,
                "mdate": 1700583842939,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4W2km3vei8",
            "forum": "89l6VLPrin",
            "replyto": "89l6VLPrin",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5711/Reviewer_pX5U"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5711/Reviewer_pX5U"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new algorithm for graph layouts, graph t-SNE, and a new algorithm for contrastive learning algorithm, graph\nCNE, and draw a connection between the two algorithms."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper uses several real-world graph datasets to demonstrate the superiority of the proposed methods.\n2. The exploration of the connection between graph layouts and graph contrastive learning is a relatively under-explored yet important topic given the abundance of graph data nowadays and the need to visualize and process graph data."
                },
                "weaknesses": {
                    "value": "1. Paper writing can be made better. For example, for Equation 3, there is no description of the meaning of y. The definitions of parametric and non-parametric embeddings should be clearly described earlier in the paper.\n2. As the authors mention, it is suspected that \"in GCL algorithms employing GCNs, it is the GCN that does the heavy lifting, and not the specifics of the GCL algorithm.\" However, there is no experimental setup where the authors verify this hypothesis, which is a pity."
                },
                "questions": {
                    "value": "1. Does graph t-SNE use the graph node features? This is asked because for Graph CNE, the paper mentions the reduction of dimensionality from D (input feature dimension) to d (2 or 128), and to my understanding, all the methods for graph layouts use only the structure/topology of the graph. If this is the case, I am still unsure if kNN classification accuracy is the right metric, since the metric quantifies local class separation, yet the class label for each node is unobserved by the layout methods. If this is the case, please justify the adoption of such a metric for comparing different graph layout algorithms. The concern is, what if for a real-world dataset, the class labels of nodes are less related or unrelated to the structure/topology of the graph? Then wouldn't all the layout methods show low accuracy? Fundamentally, the question is about the justification of using this metric to evaluate layout algorithms."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5711/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699158364879,
            "cdate": 1699158364879,
            "tmdate": 1699636597215,
            "mdate": 1699636597215,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LAuTBdYck2",
                "forum": "89l6VLPrin",
                "replyto": "4W2km3vei8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5711/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5711/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "> The exploration of the connection between graph layouts and graph contrastive learning is a relatively under-explored yet important topic given the abundance of graph data nowadays and the need to visualize and process graph data.\n\nWe thank the reviewer for this assessment!\n\nIndeed, we also believe that this connection is conceptually very important, and our paper is the first to point it out and explore. Along the way, we beat SOTA on graph layouts, beat SOTA on MLP-based graph contrastive learning, and suggest 2D graph contrastive learning (graph CNE with $d=2$) as a novel graph learning task and a \"missing link\" between graph layouts and standard graph contrastive learning. \n\n> Paper writing can be made better. For example, for Equation 3, there is no description of the meaning of y. The definitions of parametric and non-parametric embeddings should be clearly described earlier in the paper.\n\nThank you for these suggestions! We added the definition of $\\mathbf y_i$ vectors into Section 3.1 and added a definition of parameteric/non-parametric embeddings into Section 3.2. We also carefully went over the entire paper and inserted clarifications and cross-references. Please do let us know if you find any other specific places that should be clarified!\n\n> As the authors mention, it is suspected that \"in GCL algorithms employing GCNs, it is the GCN that does the heavy lifting, and not the specifics of the GCL algorithm.\" However, there is no experimental setup where the authors verify this hypothesis, which is a pity.\n\nIndeed, we do not provide experimental evidence for this, but in fact, another paper (not ours) has just been accepted to NeurIPS and published online: Guo et al. 2023 Architecture matters: Uncovering implicit mechanisms in graph contrastive learning (https://arxiv.org/abs/2311.02687). This paper explicitly studies the effect that GCN architecture has in GCL and indeed shows that it does the heavy lifting in pulling connected nodes together. We have therefore reformulated this paragraph and cited this paper.\n\n> Does graph t-SNE use the graph node features? \n\nNo it does not. The Reviewer's understanding is correct.\n\n> If this is the case, I am still unsure if kNN classification accuracy is the right metric, since the metric quantifies local class separation, yet the class label for each node is unobserved by the layout methods. If this is the case, please justify the adoption of such a metric for comparing different graph layout algorithms. The concern is, what if for a real-world dataset, the class labels of nodes are less related or unrelated to the structure/topology of the graph? Then wouldn't all the layout methods show low accuracy? Fundamentally, the question is about the justification of using this metric to evaluate layout algorithms.\n\nThis is an excellent question. Indeed, the standard approach in the graph layout literature is to use kNN preservation metrics, such as the kNN recall that we use in Figure 3a. We also consider it our main metric for graph layouts (and note that our graph t-SNE outperformed all other methods on all considered datasets, according to this metric).\n\nWe additionally showed kNN accuracy in Figure 3b because it is similar to the metric always used in node-level graph contrastive learning (linear classification accuracy). Given that one of the aims of the paper is to establish the conceptual connection between graph layouts and graph contrastive learning, we think it is useful to show this metric here. But again, we agree with the Reviewer that the kNN recall metric is more important here.\n\nWe have clarified this in the text.\n\nIf there any further concerns, we will be happy to clarify!"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700167460216,
                "cdate": 1700167460216,
                "tmdate": 1700167460216,
                "mdate": 1700167460216,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HzfpovGbuG",
                "forum": "89l6VLPrin",
                "replyto": "4W2km3vei8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5711/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5711/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear reviewer pX5U, thanks again for your feedback. We hope our edits have resolved your concerns! If there are any remaining concerns, we would still have 2 hours to respond."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5711/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733334229,
                "cdate": 1700733334229,
                "tmdate": 1700733334229,
                "mdate": 1700733334229,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]