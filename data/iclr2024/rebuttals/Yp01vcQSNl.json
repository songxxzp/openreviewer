[
    {
        "title": "DIRECTIONALITY IN GRAPH TRANSFORMERS"
    },
    {
        "review": {
            "id": "Ml1w96kC9r",
            "forum": "Yp01vcQSNl",
            "replyto": "Yp01vcQSNl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8547/Reviewer_A6eP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8547/Reviewer_A6eP"
            ],
            "content": {
                "summary": {
                    "value": "Summary: This paper focuses on directionality of edges in graphs and introduces a new graph transformer architecture called DiGT that explicitly models edge directionality in directed graphs. The key ideas are: i) Dual node representations to capture source and target roles; ii) Directionality incorporated via asymmetric attention and dual query/key matrices, and iii) Localization via k-hop neighborhood attention.\n\nContributions:\n- Proposes DiGT, a transformer that uses dual node encodings and directional attention to capture directionality.\n- Introduces strategies to make other graph transformers directional via asymmetric/dual attention.\n- New directional graph datasets where direction correlates with labels.\n- Shows superior performance over GNNs and graph transformers on directional benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Strong Points:\n- Dual node encodings in DiGT elegantly capture directionality throughout the model.\n- Directional attention neatly exploits query/key asymmetry for modeling direction.\n- DiGT significantly outperforms baselines on directional graphs.\n- Quantifies dataset directionality via entropy measure.\n- New directional graph datasets enable better evaluation, although it may be biased in the context of this work."
                },
                "weaknesses": {
                    "value": "Weaknesses and Questions:\n- While dual encodings are powerful, they double model size. Could this be optimized?\n- Although computational limitation of the proposed architecutre is discussed briefly, the runtime complexity is quadratic in number of nodes like vanilla transformers. If sparsity helps, how would the complexity differ for DiGT compared to other non-directional graph transformers?\n- Are there other ways to quantify directionality of graphs besides SCC entropy?\n- A closely related work \"Edge Directionality Improves Learning on Heterophilic Graphs\", Rossi et al., 2023, is not discussed in this work which would be important and provides (i) necessary context on novelty (ii) applicability of DirGNN on vanilla GTs."
                },
                "questions": {
                    "value": "included together with weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8547/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698472714208,
            "cdate": 1698472714208,
            "tmdate": 1699637069317,
            "mdate": 1699637069317,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vMzFGToNnw",
                "forum": "Yp01vcQSNl",
                "replyto": "Ml1w96kC9r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8547/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8547/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback. Below are our responses to the issues you've highlighted:\n\nW1. We provide the answer to this question in our general response. As noted, our model does not use more parameters, since we adjust the choices to keep it aligned with the competitive baselines.\n\nW2. The complexity of graph transformers typically depends on their structural design. Standard graph transformers, such as Vanilla Graph Transformer and DiGT, assume that all nodes are interconnected, resulting in a complexity that is quadratic relative to the number of nodes. In contrast, graph transformers employing sparse attention focus only on neighboring nodes and ignore non-existent edges. This approach reduces both computational time and memory requirements and works like graph neural networks. We will explore sparse attention techniques in the future. \n\nW3. There are no definitive measures to characterize the importance of directionality. One option is to use various directed graph metrics like distribution of in and out-degrees, directed cycles, etc. Another option is to use some spectral measures. We used the SCC entropy that combines some aspects of the directed metrics, and we also study the empirical effect of random edge flipping. We believe this is an interesting open question for the graph learning community.\n\nW4. Thank you for suggesting the Dir-GNN work, which is a directed graph neural network that focuses on incoming and outgoing edges. However, DiGT assumes that all the nodes are connected since our model is a graph-level transformer model. Dir-GNN was first published on May 17, 2023, making it concurrent to our research. We will thoroughly explore it in our future studies."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700627609335,
                "cdate": 1700627609335,
                "tmdate": 1700627609335,
                "mdate": 1700627609335,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5TmyrgD0gP",
                "forum": "Yp01vcQSNl",
                "replyto": "vMzFGToNnw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8547/Reviewer_A6eP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8547/Reviewer_A6eP"
                ],
                "content": {
                    "title": {
                        "value": "Response to rebuttal"
                    },
                    "comment": {
                        "value": "Dear authors, thank you for your answers to my questions on parameters, complexity and directionality metric. Based on the merits of the paper, I retain my score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670175632,
                "cdate": 1700670175632,
                "tmdate": 1700670175632,
                "mdate": 1700670175632,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sMDMt3Tm2z",
            "forum": "Yp01vcQSNl",
            "replyto": "Yp01vcQSNl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8547/Reviewer_yoox"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8547/Reviewer_yoox"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes Directed Graph Transformer (DiGT), a global self-attention transformer specialized in encoding directed networks via dual node embeddings for source and target representations, learnable implicit adjacency information via directed attention, and k-hop neighborhood localization. For experimentation, the paper first explores the directionality of existing directed graph classification datasets by performing a random flip test, from which it finds that directionality is not a crucial factor in most datasets. Due to such limitation, the paper synthesizes two novel datasets, FlowGraph and Twitter, where the graph label is explicitly related to the edge direction pattern. Experiments on these datasets show that DiGT attains best performance across various message-passing GNN and graph Transformer baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- [S1] Developing a Transformer architecture for learning directed graphs is a fairly underexplored topic, yet there is a clear demand in the community due to datasets where directionality occurs naturally.\n- [S2] The overall methodology behind DiGT is well-written with great detail."
                },
                "weaknesses": {
                    "value": "- [W1] **There are some questionable design choices in DiGT that seemingly contradict with the overall direction, yet are missing additional explanations.** \n  - In particular, the end of Section 2 mentions how directed message-passing GNNs \"suffer from convolutional inductive bias... restricted to only the given neighborhood structure\", yet DiGT uses attention that is localized to the k-hop neighborhood. Doesn't this essentially downplay the advantage GTs have over message-passing GNNs? \n  - Also, the end of Section 3 mentions that \"DiGT uses dual node embeddings in all layers\", but Equations (9) and (10) imply that dual embeddings do not remain dual throughout the encoder, but are instead merged together into a single embedding $\\mathbf{Y} \\in \\mathbb{R}^{n \\times d_p}$, then separated via linear layers $L_{VS}$ and $L_{VT}$ once every layer. Why is this the case?\n- [W2] **The experimental setup is unconvincing.** \n  - The paper claims that directionality does not play a significant role in existing datasets, proceeds by proposing synthetic datasets, and then performs graph classification on those networks instead. However, the first observation in Table 1 is not really comprehensive (experiments are only shown with certain model-dataset pairs), and the numbers do not necessarily overlap in standard deviation, which then leads to the question of \"Are these results truly indicative of directionality not playing a significant role in these datasets?\".\n  - The pipeline used to generate the Twitter datasets also seems problematic, due to how the label of each graph is chosen. Specifically, Twitter5 has 5 labels, each corresponding to the perturbation rate in [0, 25, 50, 75, 100]% used to rewire or reverse each edge in the original ego-network. Then for the experiments in Table 1, randomly flipping 50% of the edges in a Twitter5 graph labeled 1 (originally perturbed by 25%) would return a graph that is equivalent to a graph labeled 3 (originally perturbed by 75%) with no edge-flipping. In essence, it is unclear whether the drop in performance shown in Table 1 is simply due to having noisy labels, rather than the dataset exhibiting significance on directionality."
                },
                "questions": {
                    "value": "- [Q1] **Details on number of parameters.** Could the authors clarify the exact number of parameters used for each model in Table 2? Due to its dual attention mechanism, I suspect DiGT would use more parameters compared to other models if using the same number of layers. Having the model sizes alongside performance metrics would help clarify whether the performance gains are due to the proposed mechanisms, and not due to having more parameters.\n- [Q2] **Missing results for Malnet-tiny of Tables 2 and 3.** Are these all blank due to out-of-memory issues? If so, it would be better to fill them in with OOM."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8547/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698587327679,
            "cdate": 1698587327679,
            "tmdate": 1699637069203,
            "mdate": 1699637069203,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Bxt1hiZLtZ",
                "forum": "Yp01vcQSNl",
                "replyto": "sMDMt3Tm2z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8547/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8547/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We want to thank you for your thoughtful ideas. Here is our response to the concerns you raised regarding potential weaknesses:\n\nW1. We choose to retain the k-hop constraint, since it helps somewhat, as shown in Table 11 in the appendix. We replicate the last two rows of that table here:\n\n| Model              | Flow2 | Flow3 | Flow6 | Twitter3 | Twitter5 |\n|--------------------|-------|-------|-------|----------|----------|\n|DiGT-DA (no k-hops) | 96.52 | 74.33 | 46.22 | 91.31    | 85.46    |\n|DiGT                | 97.42 | 74.55 | 46.80 | 91.67    | 85.94    |\n\nAs we can see the use of k-hops confers a slight benefit. But, one critical difference from GCNs should be noted. The GCNs are always constrained by the direct neighbors which add a very strict inductive bias. On the other hand, when we used k-hops, we treat the entire set of k-hop neighbors as the context, i.e., any node can directly communicate with any other node *within* the k-hops, which is a much less restrictive \"bias\", and lets us tradeoff the global attention with somewhat localized attention. \n\nWhen we say that we use dual encodings throughout, it does not imply that there is no exchange between these source and target embeddings. In fact, it is clearly of benefit to allow information to flow from one to the other, enabling them to mutually exchange information.\nWe do not want to  reduce the system to a mere duplication of single-encoding attention, so it is not clear whether the reviewer is suggesting that keeping them independent is a better choice? \n\nW2. We refer the reviewer to Tables 6 and 7 in the appendix, that already does a comprehensive comparison of all the flipping experiments to show the importance of directionality.  \n\nIn these experiments, we implement a strategy of randomly shuffling the dataset at each training step. This approach means that the dataset appears different in each of the 100 epochs during model training. As a result, a slight decrease in model performance is anticipated, given that the model encounters a 'new' dataset each time. However, for datasets like MNIST, CIFAR10, and Ogbg-Code2, we observed only about a 1% drop in model accuracy. This finding suggests that directional information is considerably less significant, and potentially negligible, compared to the node or edge features. \n\nSince the study of flipping edges is an independent experiment, we establish a standard pipeline applicable to all datasets. In the case of the Twitter dataset we first generate Twitter datasets for all the experiments. Then, for the random flip study, we randomly flip the edges for each step. The changes in the accuracy highlight the impact of edge directionality in this dataset, though there may also be an effect of class label confusion. Please note that, whatever may be the final effect, DiGT displays superior performance compared to all other (undirected) models, which leads further credence to the important role of directionality in the Twitter dataset.\n\n\nQ1. We provide the answer to this question in our general response. As noted, our model does not use more parameters, since we adjust the choices to keep it aligned with the competitive baselines.\n\nQ2. Thank you for your suggestions. We will mark \"OOM\" on these tables."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700627588407,
                "cdate": 1700627588407,
                "tmdate": 1700629352945,
                "mdate": 1700629352945,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nrwVNmD8dz",
                "forum": "Yp01vcQSNl",
                "replyto": "sMDMt3Tm2z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8547/Reviewer_yoox"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8547/Reviewer_yoox"
                ],
                "content": {
                    "title": {
                        "value": "Response to Author Rebuttal by Reviewer yoox"
                    },
                    "comment": {
                        "value": "Thank you authors for your commitment into providing additional clarifications. Many of my concerns have been addressed, but I still have two concerns remaining:\n  - **Interactions between dual embeddings.** I agree that exchanging information across source and target embeddings would help in performance. However, due to commutativity of vector addition, adding the two embeddings and processing the result through two separate linear layers to obtain dual embeddings for the next layer (Equations 9 and 10) loses information on the ordering of the two embeddings, and thus we can no longer tell which embedding corresponds to the source or target. One simple way to exchange information while preserving the source/target ordering would be to fuse the embeddings via  $S' = S+ L_{VS}(T)$ (similar to feature fusion in [A]), instead of $S' = L_{VS}(S+T)$ which is used in the presented work. Did I miss something that makes this commutativity a nonissue?\n  - **Experiments under Randomized Directionality.** My previous concern was that Table 1 only shows results from certain model-dataset pairs, and does not seem conclusive to say that \"directionality does not play significant role in these datasets.\" While the authors have referred to Tables 6 and 7 in response, I am guessing the authors meant Tables 3 and 4 since Tables 6 and 7 pertain to another experiment. Nonetheless, these results still do not cover all model-dataset settings and the numbers still are not really convincing.\n\nBecause of these remaining concerns, I will retain my score.\n\n[A] Zhu et al., Graph Geometry Interaction Learning. NeurIPS 2020."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700719803666,
                "cdate": 1700719803666,
                "tmdate": 1700719803666,
                "mdate": 1700719803666,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9FG47UFvrd",
            "forum": "Yp01vcQSNl",
            "replyto": "Yp01vcQSNl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8547/Reviewer_r1B3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8547/Reviewer_r1B3"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a directional graph transformer that utilizes dual encodings to represent the different roles of source or target of each connected node pair. The dual encodings are acquired through the utilization of latent adjacency information, which is extracted using the directional attention module localized with k-hop neighborhood information. Additionally, the paper introduces alternative methods for incorporating directionality within the Transformer architecture. In the experimental study, the paper examines the role of directionality in current datasets, and proposes two new directional graph datasets. By conducting a comparison on directional graph datasets, the authors demonstrate that their approach achieves state-of-the-art results."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The work introduces a method that does not fully rely on the explicit directed graph structure, which allows the central node to receive extra information from non-neighbors. At the same time, the positional encodings and the k-hop localization ensure that the attention scores do not deviate too much from the original data structure.\n\n2. The proposed model outperforms GT and GNN alternatives on five reported datasets. It also surpasses the other directional GTs on three out of five datasets. \n\n3. The paper provides two new datasets that offer new benchmarks to evaluate directional graph modeling."
                },
                "weaknesses": {
                    "value": "1. How the learnt attention scores are related to the original graph structure or the edge directions is not fully revealed (probably can be done by visualization and comparison).\n\n2. Some designs are not well justified by either theoretical or empirical evidence. For example, the design of the positional encoding. No illustration, for instance, unidirectional counterparts or theoretical justification, has been provided about why the design is suitable for the directional situation.\n\n3. The computational complexity of the proposed model appears high. It may have scalability issues when applied to large datasets."
                },
                "questions": {
                    "value": "1. Would it be possible to include some node-level experiments to further examine the capability of the model? There are a few commonly used benchmark datasets with directed graphs for node level tasks, e.g., Actor. Are such graph datasets too large for the proposed model to handle?\n \n2. Based on the recent literature, [1] also presents a positional encoding design tailored for directed graph Transformers. It would be valuable to see a comparative evaluation of your proposed model with this work.\n \n[1] Geisler, Simon, Yujia Li, Daniel J. Mankowitz, Ali Taylan Cemgil, Stephan G\u00fcnnemann, and Cosmin Paduraru. \"Transformers meet directed graphs.\" In International Conference on Machine Learning, pp. 11144-11172. PMLR, 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8547/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698648025603,
            "cdate": 1698648025603,
            "tmdate": 1699637069074,
            "mdate": 1699637069074,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NznZc2xoqb",
                "forum": "Yp01vcQSNl",
                "replyto": "9FG47UFvrd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8547/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8547/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your positive feedback. Here are our answers to the points raised:\n\nW1. Visualizing the attention scores would be interesting, something we can explore for the future.\n\nW2. We used SVD-based positional encodings (PEs) since the left and right singular values/vectors can be used very naturally for directed graphs, unlike the Graph Laplacian, which is symmetric. For a comparison with Magnetic Laplacian PEs, see Q2 below.\n\nW3. We have included the number of parameters used by our model and others in the general response above. Thus, our method is no more and no less scalable than other (dense) transformer based models. \n\nQ1. We ran our model on the WebKB and the Telegram datasets. WebKB contains Cornell, Wisconsin, and Texas datasets. The table below reports on the accuracy of DiGT vs other baselines; we see clear improvements using DiGT (we do not include std-dev for clarity, but we will include them in our revised paper). Nevertheless, transformer-based models work only for medium sized datasets; for large number of nodes all (dense) attention methods will have scalability issues.\n\n\n| Datasets    | Cornell | Wisconsin | Texas | Telegram |\n|-------------|---------|-----------|-------|----------|\n| DGCN [ii]   | 65.13   | 71.08     | 64.90 | 85.77    |\n| DiGCN [i]   | 45.68   | 50.54     | 53.73 | 65.96    |\n| DiGCNIB [i] | 41.08   | 50.81     | 56.47 | 56.35    |\n| EGT         | 71.43   | 73.59     | 81.18 | 80.00    |\n| DiGT        | 71.62   | 74.05     | 83.73 | 90.38    |\n\n[i] Z. Tong, Y. Liang, C. Sun, X. Li, D. Rosenblum, and A. Lim. Digraph inception convolutional networks. Advances in Neural Information Processing Systems, 33, 2020.\n\n[ii] Z. Tong, Y. Liang, C. Sun, D. S. Rosenblum, and A. Lim. Directed graph convolutional network. arXiv preprint arXiv:2004.13970, 202\n\nQ2. Thanks for suggesting the use of the Magnetic Laplacian PEs, which are indeed even better than SVD based positional encodings, as shown in the table below (we will include these new results in Table 2 in our updated paper; we do not show the std-dev values below for clarity, but we will include those as well):\n\n\n| Datasets | Flow2 | Flow3 | Flow6 | Twitter3 | Twitter5 |\n|----------|-------|-------|-------|----------|----------|\n| DiGT-SVD | 97.42 | 74.55 | 46.80 | 91.67    | 85.94    |\n| DiGT-Mag | 97.00 | 74.92 | 47.49 | 93.34    | 86.61    |\nHere DiGT-Mag is with the magnetic Laplacian position encodings."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700627549211,
                "cdate": 1700627549211,
                "tmdate": 1700629499417,
                "mdate": 1700629499417,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QxOtOIFEdh",
                "forum": "Yp01vcQSNl",
                "replyto": "NznZc2xoqb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8547/Reviewer_r1B3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8547/Reviewer_r1B3"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "Unfortunately, most of my concerns are not addressed (detailed below). I will keep the current rating.\n\nW1: Visualization of attention scores is yet to be provided. It is important to provide some experimental results to prove that DiGT does learn some directional information within graphs.\n\nW2: The relationship between the left/right singular values/vectors and the direction of edges is not revealed.\n\nW3&Q1: The response somehow reveals the scalability issue of DiGT. The WebKB datasets are quite small in node-level tasks. For example, Cornell only contains 183 nodes. The suggested dataset, Actor, is also considered as a small graph in node-level tasks, which contains 7600 nodes. However, the authors did not provide experimental results on this dataset.\n\nQ2: Magnetic Laplacian position encodings seem to outperform DiGT-SVD."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637805416,
                "cdate": 1700637805416,
                "tmdate": 1700637805416,
                "mdate": 1700637805416,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uzEm6onQYn",
            "forum": "Yp01vcQSNl",
            "replyto": "Yp01vcQSNl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8547/Reviewer_1Uzt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8547/Reviewer_1Uzt"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Directed Graph Transformer (DiGT), a novel graph transformer architecture that effectively addresses the challenge of analyzing directed graphs. DiGT incorporates edge direction and graph connectivity as integral elements within the Transformer framework, enabling it to dynamically learn dual node encodings that capture the directionality of edges. Experimental results demonstrate that DiGT significantly outperforms state-of-the-art graph neural networks and graph transformers in directed graph classification tasks, particularly when edge directionality is a crucial aspect of the data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) Exploring the utilization of graph transformers to encode the directed information within graphs is a promising avenue of research.\n2) DiGT, as an approach that leverages a graph transformer to encode directedness, shows promise, and the experimental results provide evidence of its effectiveness to a certain extent."
                },
                "weaknesses": {
                    "value": "1) The three modules of DiGT are all based on heuristic methods, lacking some theoretical explanations and insights.\n2) DiGT contains many linear layers and learnable parameters, making it quite complex. While the author briefly describes the complexity of DiGT in Appendix F, I recommend conducting a more detailed theoretical analysis and empirical validation.\n3) In the experiments, the author mentions abandoning the use of some available datasets because they think the directionality of these datasets is unimportant. I don't entirely agree with this viewpoint. I think that even if directionality is not crucial, if a model can encode directionality, it should still yield some benefits.\n4) Some important baselines were not compared in the experiments. For example, the method proposed in the paper \"Transformers Meet Directed Graphs [1]\" achieved the SOTA result on Ogbg-Code2.\n\n[1] Geisler, Simon, et al. \"Transformers meet directed graphs.\"\u00a0_International Conference on Machine Learning_. PMLR, 2023."
                },
                "questions": {
                    "value": "Please refer to the aforementioned weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8547/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698731098397,
            "cdate": 1698731098397,
            "tmdate": 1699637068952,
            "mdate": 1699637068952,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xzarhbIhbe",
                "forum": "Yp01vcQSNl",
                "replyto": "uzEm6onQYn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8547/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8547/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We want to thank you for your thoughtful comments. Below are our responses:\n\nW1. Our DiGT model is inspired by the well known HITS approach for directed graphs using notion of authority and hub scores (which, are eigenvectors of the $A^T A$ and $A A^T$ matrices). Our key innovation is the use of source/target vectors (embeddings) with a learnable adjacency matrix in the HITS approach, and thus it has a solid theoretical foundation.\n\nW2. We not only suggest a novel learnable formulation for the directed embeddings, but we also study alternative formulations for incorporating directionality, which we study empirically. Thus, our work is not just a set of ad-hoc methods, but rather a systematic study of how direction can be added effectively within transformer models.\n\nW3. We disagree. If a dataset has no directionality, then how or why will a directional transformer help? For example, if a graph is undirected, then its adjacency matrix A, will be symmetric, and the notion of hub or authority collapse -- $A^TA$ and $A A^T$ will be the same. \n\nW4. Please note that directionality is not significant in the original ogb-code2 dataset, as we have shown in Table 1. Furthermore, please note that Geisler et al preprocess and modify the original graph to get competitive results, but we consider the original graph, which has little directionality."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700627478431,
                "cdate": 1700627478431,
                "tmdate": 1700627478431,
                "mdate": 1700627478431,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JEjKBcuQ53",
                "forum": "Yp01vcQSNl",
                "replyto": "xzarhbIhbe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8547/Reviewer_1Uzt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8547/Reviewer_1Uzt"
                ],
                "content": {
                    "title": {
                        "value": "Re"
                    },
                    "comment": {
                        "value": "Thank you for the response. After reading the rebuttal and other reviews, I will keep my score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643483737,
                "cdate": 1700643483737,
                "tmdate": 1700643483737,
                "mdate": 1700643483737,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]