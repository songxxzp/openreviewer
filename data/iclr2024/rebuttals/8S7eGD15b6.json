[
    {
        "title": "Subspace Grid-sweep: ML Defense Evaluation via Constrained Brute-force Search"
    },
    {
        "review": {
            "id": "i408zYjx2h",
            "forum": "8S7eGD15b6",
            "replyto": "8S7eGD15b6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7742/Reviewer_2Hxt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7742/Reviewer_2Hxt"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new defense evaluation tool called Subspace Grid-sweep. This tool uses deterministic inference to assess adversarial robustness more effectively, revealing vulnerabilities in a previously published defense. The paper also suggests that randomness may not be necessary for defense robustness."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-written, and I comprehended it easily.\n2. The motivation of this paper seems good and well-founded, as initially, researchers believed that randomness enhanced defense robustness. However, with random defenses being overcome, it remains unclear whether randomness is beneficial or merely complicates defense evaluation.\n3. The proposal Subspace Grid-sweep is simple and intuitive."
                },
                "weaknesses": {
                    "value": "1. The only difference between deterministic defense and random defense is merely the choice between randomly selecting once and randomly selecting multiple times. In fact, random selection even just once still qualifies as random defense. This may not necessarily imply that defense does not require robustness.\n2. The idea of subspace grid sweep lacks novelty."
                },
                "questions": {
                    "value": "1.Why do the results in Figure 2 and Figure 7(a) show differences?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7742/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7742/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7742/Reviewer_2Hxt"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7742/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698675732085,
            "cdate": 1698675732085,
            "tmdate": 1699636945326,
            "mdate": 1699636945326,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "r3Ccxg559F",
                "forum": "8S7eGD15b6",
                "replyto": "i408zYjx2h",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7742/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7742/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the reviewer for considering our work well-written, well-motivated, and easy to understand. We respond to and ask clarification on the noted reviewer concerns and questions below, and hope this makes the strengths and usefulness of our paper more apparent. Please let us know any other information we can provide.\n\n> The only difference between deterministic defense and random defense is merely the choice between randomly selecting once and randomly selecting multiple times. In fact, random selection even just once still qualifies as random defense. This may not necessarily imply that defense does not require robustness.\n\n\nWe categorize a defense as deterministic if it always gives the same output given the same input. If we use the same random seed for each inference in a randomized defense that depends on that seed, then we achieve this effect. \n\nOur reasoning behind this categorization is that one of the difficulties with evaluating randomized defenses is that for each inference, including gradient computation, you get a random variable for the gradients and the output that could be different given the same input. This makes it difficult to do fine-grained optimization when you have randomness in the mix. Also, black-box attacks are hindered, along with other issues discussed in Section 6. Modifying a defense to instead always have the same output given the same input effectively removes these issues, and makes the defense deterministic for our purposes.\n\nWe are happy to include some discussion in the paper on why we refer to using the same seed for every inference as making the defense deterministic, and why it suits our purposes.\n\nAs for the semantics on if using the same random seed for each inference counts as \u201cremoving randomness\u201d, we are also happy to include a discussion on the different definitions of having randomness in a defense.\n\n> The idea of subspace grid sweep lacks novelty.\n\nCould you point us to work that shows ours lacks novelty?\n\n> 1.Why do the results in Figure 2 and Figure 7(a) show differences?\n\nFigure 2 does show that deterministic smoothing is less robust than randomized smoothing when they use lower numbers of corruptions, but converges to the same robustness as the number of corruptions they use increase (implying randomness is not required for empirical robustness for randomized smoothing as discussed in Section 4.1, and it is more important instead to use a sufficient number of corruptions). \n\nFigure 7a is part of Section D in the appendix, which, separately, describes a new threat model where the attacker repeatedly tries the same input to defeat a defense. Figure 7a shows that a random defense will become less robust than a deterministic defense given this threat model (hence the gap)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700101578725,
                "cdate": 1700101578725,
                "tmdate": 1700101578725,
                "mdate": 1700101578725,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1G3V084YOA",
                "forum": "8S7eGD15b6",
                "replyto": "r3Ccxg559F",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7742/Reviewer_2Hxt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7742/Reviewer_2Hxt"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the rebuttal. I will keep my rating."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700655350204,
                "cdate": 1700655350204,
                "tmdate": 1700655350204,
                "mdate": 1700655350204,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OTQW6ULji8",
            "forum": "8S7eGD15b6",
            "replyto": "8S7eGD15b6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7742/Reviewer_LfCf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7742/Reviewer_LfCf"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes the Subspace Grid-sweep method. It argues that using a grid for evaluating robustness can provide additional perspective on existing robustness evaluation methods. Also the authors argue that randomness introduced in many popular defences could be redundant and fixed random seed could be used instead."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Originality. Using structured search for robustness evaluation was used before, in particular, for adversarial patches. See e. g. Derandomized Smoothing [1]. However, using it for Lp bounded perturbations seems to be novel. A look at making randomized attacks deterministic is also interesting. \n\n2. Quality. The paper proposed a simple and interesting idea. The authors discuss the limitations of their method. \n\n3. Clarity. The main idea is easy to grasp.\n\n4. Significance. Evaluating model robustness is important for safety-critical applications. Providing additional perspective on robusness evaluation methods makes them more interpretable. \n\n[1] Alexander Levine and Soheil Feizi. (De)Randomized Smoothing for Certifiable Defense against Patch Attacks. In NeurIPS 2020."
                },
                "weaknesses": {
                    "value": "1. Clarity/Quality. In the main paper and Appendix I haven\u2019t found the explanation on how exactly Auto-Attack was restricted to the continious K-dimensional space? Given that K is extremely small (1-6), this could drastically affect the overall Auto-Attack performance. Without understanding that it is hard to interpret the results e. g. in Table 1.\n\n2. Quality. In Section 4.1 the Authors evaluate Randomized Smoothing with standard PGD. Although they claim to use up to 10 000 steps, it still doesn\u2019t seem like a suitable evaluation method. A-PGD or AutoAttack would be better.\n\n3. Quality. If I understood correctly, all the evaluations were performed on small-resolution datasets (CIFAR-10, SVHN, MNIST). However, Randomized Smoothing is able to provide provable defence e. g. for models on ImageNet. Given the exponential nature of the method, it is hard to understand whether it would scale to datasets of higher resolution.  \n\n4. In the limitations the authors admit that fixing random seed in e. g. Randomized smoothing makes it vulnerable for an attacker that knows it. I assume that with the knowledge of Subspace Grid-sweep\u2019s grid  a model could be trained that seems robust when evaluated in the predefined grid points but contains adversarial regions in-between. That could result in backdoor-attacks on seemingly robust models. Thus the method can provide false sense of security."
                },
                "questions": {
                    "value": "1. What is the role of parameter \u03c3 in GridSweepPerturbationsL2(K, B, M, \u03c3)? (Section 3.1)\n\n2. How exactly was the orthonormal basis chosen (Section 3.1)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7742/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7742/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7742/Reviewer_LfCf"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7742/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698764291765,
            "cdate": 1698764291765,
            "tmdate": 1699636945178,
            "mdate": 1699636945178,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "olgIsQ6DRQ",
                "forum": "8S7eGD15b6",
                "replyto": "OTQW6ULji8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7742/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7742/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response from Authors 1/2"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for considering our work as interesting, easy to understand, and important. Also, we thank the reviewer for listing out their questions and concerns. Below, we respond to each of them providing clarification and paper edit proposals. We hope this alleviates the noted concerns and confusion and allows the strengths and usefulness of our work to be clearer. \n\n> Clarity/Quality. In the main paper and Appendix I haven\u2019t found the explanation on how exactly Auto-Attack was restricted to the continious K-dimensional space? Given that K is extremely small (1-6), this could drastically affect the overall Auto-Attack performance. Without understanding that it is hard to interpret the results e. g. in Table 1.\n\nRunning AutoAttack in this K-dimensional basis was accomplished by simply adding the K x D matrix (discussed in Sec 3.1) that defines the subspace to the forward function right before the input (where there are K dimensions in the subspace and D is the shape of the original input). This is essentially the same as adding an encoder to the input where AutoAttack modifies the basis weights (and receives backpropagated loss for the basis weights) for the K orthonormal basis vectors that define the subspace instead of the individual pixels of the input. In addition to this straightforward implementation, we know AutoAttack works fine in the subspaces by looking at the results in Tables 1 and 2 that show AutoAttack\u2019s A-PGD finding nearly every vulnerable datapoint (99.8%) that Subspace Grid-sweep found on the undefended (base) classifiers.\n\nThe code implementing this forward function for AutoAttack is method `forward_with_basis_vectors(...)` in lines 464-482 in `code/core.py` in the supplementary material.\n\n> Quality. In Section 4.1 the Authors evaluate Randomized Smoothing with standard PGD. Although they claim to use up to 10 000 steps, it still doesn\u2019t seem like a suitable evaluation method. A-PGD or AutoAttack would be better.\n\nWe believed this evaluation seems the most appropriate as it is the same attack used (and recommended) by the original authors of Randomized Smoothing (Cohen et al., 2019), and so we wanted to show that Deterministic Smoothing would have been evaluated to have the same robustness as Randomized Smoothing in the original paper. Using a different attack to compare the robustness would be beside the point of the comparison.\n\nHowever, we actually did use AutoAttack\u2019s A-PGD on Deterministic Smoothing as reported in Section 5 to compare with Subspace Grid-sweep\u2019s ability to find vulnerable datapoints in lower-dimensional subspaces and found no indication that Deterministic Smoothing is especially more vulnerable to A-PGD than PGD. Therefore, as we suspect Randomized Smoothing would also not be especially more vulnerable to A-PGD than PGD, our findings do not change if we consider A-PGD instead of PGD in Section 4.1. These results can be highlighted if the reviewer prefers.   \n\n\n>Quality. If I understood correctly, all the evaluations were performed on small-resolution datasets (CIFAR-10, SVHN, MNIST). However, Randomized Smoothing is able to provide provable defence e. g. for models on ImageNet. Given the exponential nature of the method, it is hard to understand whether it would scale to datasets of higher resolution.\n\nAs the purpose of Subspace Grid-sweep is to be comparison point to standard evaluation attacks (to see if they are finding vulnerable points that Subspace Grid-sweep finds), then our reported experiments showing issues with defenses using smaller-resolution datasets are still valid, as a defense that is obscuring gradients in a lower-resolution dataset is sufficient cause for concern. \n\nWhile we have no reason to believe using a larger dataset would not work (as we would still only search a subspace of less than 10 dimensions in a higher-resolution input space), we do not understand any benefit of doing so when we show that using lower-resolution (and much easier to experiment with) datasets is sufficient to show the issue in modern defenses."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700101225657,
                "cdate": 1700101225657,
                "tmdate": 1700101225657,
                "mdate": 1700101225657,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kDxPIuR4GZ",
                "forum": "8S7eGD15b6",
                "replyto": "OTQW6ULji8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7742/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7742/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response from Authors 2/2"
                    },
                    "comment": {
                        "value": "> In the limitations the authors admit that fixing random seed in e. g. Randomized smoothing makes it vulnerable for an attacker that knows it. I assume that with the knowledge of Subspace Grid-sweep\u2019s grid a model could be trained that seems robust when evaluated in the predefined grid points but contains adversarial regions in-between. That could result in backdoor-attacks on seemingly robust models. Thus the method can provide false sense of security.\n\nTo clarify, we posed as future work to explicitly explore if the attacker somehow has an edge by knowing the random seed randomized smoothing is fixed to. However, our experimental results in Sections 4 and 5 and Figure 2 instead found empirical evidence that the attacker does not have an edge, at least with current attacks used for evaluation. \n\nAlso, as later shown in Section 5 using Subspace Grid-sweep in Table 2 where (Smoothing) Defended has a higher Robustness against Grid-sweep than (Smoothing) Base, we show that the attacker may have a hard time using the knowledge of the random seed because there are simply smaller adversarial regions (i.e., many fewer adversarial examples) that can be found when using randomized smoothing, regardless of whether it is random or deterministic.\n\nIn response to the posed threat model on if knowledge of Subspace Grid-sweep\u2019s subspaces and grid sizes could allow an adversary to modify their defense to make Subspace Grid-sweep incorrectly deem a defense robust, we point out the following characteristics of Subspace Grid-sweep that should alleviate concerns.\n* Subspace Grid-sweep is not designed to be a standalone *attack*. The purpose of Subspace Grid-sweep is to check whether benchmark attacks (such as AutoAttack\u2019s A-PGD) are finding vulnerable datapoints that should be found, so AutoAttack\u2019s A-PGD is also executed against the defense being evaluated. In the case of an adversary specifically evading Subspace Grid-sweep, they would also need to ensure that AutoAttack\u2019s A-PGD also cannot find these vulnerable regions placed in-between grid points.\n* Additionally, the subspaces for Subspace Grid-sweep are chosen at random (via randomly sampled orthonormal basis vectors), and many subspaces and grids are sampled for an evaluation (shown in Appendix A), so the attacker would need to know, and figure out how to avoid, all the different subspaces and grids of all future evaluators that may want to test the defense.\n* Finally, we also do a random search in the subspaces (results in Appendix A), that would not adhere to the known grid points, and this could also find any hidden adversarial regions places in-between grid points\n\n> What is the role of parameter \u03c3 in GridSweepPerturbationsL2(K, B, M, \u03c3)? (Section 3.1)\n\nWe apologize for the confusion. $\\sigma$ represents the $L_2$ constraint distance that the generated perturbations must adhere to. We are happy to change this to $\\epsilon$ to match how we refer to the constraint in the text.\n\n> How exactly was the orthonormal basis chosen (Section 3.1)?\n\nThe orthonormal basis is chosen randomly by using a seed defined by the dimensionality of the subspace (K) and the number of bins (B) requested. This is to ensure that the same subspace is used to compare Defended and Undefended (base) classifiers in the several different subspaces we evaluate with (enumerated in Appendix A).\n\nFirst, we randomly sample a $K \\times D $ matrix (after setting the seed), transpose it, use `scipy.linalg.orth` to create orthogonal columns, transpose it again, divide each row by its $L_2$ norm, then exhaustively verify all K rows are orthonormal.\n\nFor more detail, the code calculating the orthonormal basis vectors that define the subspaces we search in is in lines 134-146 in `code/basis_experiments.py` in the supplementary material."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700101248245,
                "cdate": 1700101248245,
                "tmdate": 1700101248245,
                "mdate": 1700101248245,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eXzqUdrBDY",
                "forum": "8S7eGD15b6",
                "replyto": "OTQW6ULji8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7742/Reviewer_LfCf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7742/Reviewer_LfCf"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer's response"
                    },
                    "comment": {
                        "value": "Thanks for your response.\n\nDo I understand correctly that the one of the points of your paper and, in particular, of the Table 1 in Section 3.2 is to show that existing attacks (e. g. A-PGD in Table 1) may provide false sense of security by not finding all vulnerable datapoints? In that case I am not sure that you are really showing that since you are significantly restricting A-PGD by appending that KxD matrix to it and, essentially, obtain another artificial attack to which you compare in your low-dimensional space. Could you clarify how your results show the flaws of original A-PGD in the original input space?\n\nConcerning the high-dimensional space, I agree that your approach with up to 10 dimensions can still be applied to it. The question is whether your results will look the same, because by downsampling a 224x224 image to a 10-dimensional space you lose much more information than when you downsample a 32x32 image to a 10 dimensional space. You state that your approach was not meant as a standalone attack but I still wonder whether it maintains it\u2019s useful properties when applied in combination with attacks on datasets that are closer to the real-world problems than e. g. CIFAR10."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700595004431,
                "cdate": 1700595004431,
                "tmdate": 1700674846747,
                "mdate": 1700674846747,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "f7MvKfduMm",
                "forum": "8S7eGD15b6",
                "replyto": "OTQW6ULji8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7742/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7742/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your detailed questions! We hope our responses below help clarify our approach.\n\n> Do I understand correctly that the one of the points of your paper and, in particular, of the Table 1 in Section 3.2 is to show that existing attacks (e. g. A-PGD in Table 1) may provide false sense of security by not finding all vulnerable datapoints? In that case I am not sure that you are really showing that since you are significantly restricting A-PGD by appending that KxD matrix to it and, essentially, obtain another artificial attack to which you compare in your low-dimensional space. Could you clarify how your results show the flaws of original A-PGD in the original input space?\n\nThis understanding is mostly correct, except that the flaw we find is in the defense being evaluated, not A-PGD. We would extend your first sentence to say that Table 1 shows that AutoAttack\u2019s A-PGD is providing a false sense of security **when used to attack k-Winners-Take-All (k-WTA)**. This symptom of A-PGD failing to find vulnerable points is the result of the k-WTA defense obfuscating gradients (as found in other work (Tram\u00e8r et al., 2020)), and not from a flaw in A-PGD. \n\nWe can see A-PGD works fine in the lower-dimension subspace when k-WTA is not applied (the first rows of Table 1a and 1b) and can find all vulnerable datapoints. In fact, in all cases where a defense was not used (in Tables 1 and 2), A-PGD successfully found 99.8% of all vulnerable datapoints, indicating that its performance was not hindered at all by searching in a smaller subspace. Intuitively, this makes sense since we are actually giving A-PGD an easier job (only needing to search a 1-6 dimensional space for an adversarial perturbation instead of a 1000+ dimensional space).\n\nIn short, the only difference between the first and second rows of Table 1a and 1b is that in row 1, the kWTA defense is not being used, and A-PGD works fine and can find all vulnerable datapoints that grid-sweep can. In contrast, in row 2, where the only difference is that kWTA is being applied, A-PGD is not able to find vulnerable datapoints that grid-sweep can find. The finding of these results is that kWTA is hindering A-PGD from finding existing vulnerable datapoints as the only difference between these two rows is the absence or presence of k-WTA.\n\n\n> Concerning the high-dimensional space, I agree that your approach with up to 10 dimensions can still be applied to it. The question is whether your results will look the same, because by downsampling a 224x224 image to a 10-dimensional space your lose much more information than when you downsample a 32x32 image to a 10 dimensional space. You state that your approach was not meant as a standalone attack but I still wonder whether it maintains it\u2019s useful properties when applied in combination with attacks on datasets that are closer to the real-world problems than e. g. CIFAR10.\n\nTo clarify, we are not downsampling the input image to a lower dimension, we are simply restricting the dimensionality of the space of perturbations allowed to be applied to the image by both grid-sweep and A-PGD. The classifier and defense still gets the normal input image, but with a constrained space of adversarial perturbations added to it.\n\nOur answer to the previous question also provides some answer to your concern about us not using ImageNet. To figure out whether a defense is hindering A-PGD, we simply need to compare A-PGD\u2019s performance to Grid-sweep when the defense is absent to when the defense is present. The dataset used only matters in that it does not change between comparisons. We used CIFAR10 since it was used by the original defenses in their evaluation and experiments could be completed much faster (since each model inference is faster) than if we used Imagenet. Switching this comparison from using CIFAR10 to Imagenet would essentially just make the experiments more compute intensive, but could give us no more of a signal on whether a defense is obfuscating gradients than what we have already learned on CIFAR10. That being said, if the reviewer still feels an ImageNet experiment is necessary, we will include one on Randomized Smoothing in Section 5 in our revision. Unfortunately, these experiments will take a couple of weeks to run, so we cannot include them immediately."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700710717506,
                "cdate": 1700710717506,
                "tmdate": 1700710858958,
                "mdate": 1700710858958,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dmRireYKtn",
            "forum": "8S7eGD15b6",
            "replyto": "8S7eGD15b6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7742/Reviewer_EJoi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7742/Reviewer_EJoi"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a method to evaluate the effectiveness of defenses in enhancing an image classifier's robustness to adversarial attacks. At the core of many defenses is inherent randomness, which causes difficulty in evaluating whether the defense is truly effective or is merely obfuscating gradients. The authors study whether randomized defenses retain their robustness if they are made deterministic and find that deterministic defenses can be just as robust against white-box attackers as randomized defenses."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The paper studies an interesting problem: How do we correctly evaluate defenses against adversarial attacks?\n\n* The paper has nice Figures and is well-written \n* The paper features a detailed Appendix with more information"
                },
                "weaknesses": {
                    "value": "**Implication.** On a high level, I do not understand what the authors advocate regarding using randomness in evaluating a defense. While they show that deterministic defenses are, at best, as good as their randomized counterparts, I do not understand in what sense the randomness limits the evaluation of a defense. In other words, if an attack succeeds against a deterministic variant of a defense, what implications does that have on the robustness of the randomized version?\n\n**Subspace Projection.** What is the advantage of using a grid in a subspace over simply randomly sampling noisy perturbations? Also, function $\\texttt{GridSweepPerturbationsL2}$ is unclear to me. What is $\\sigma$ (I assume this should be $\\epsilon$)? For g=0 and B=6, the new g assignment is -1, but the comment is misleading because it says the points are normalized between [0,1]. \n\n**Randomized Smoothing.** The insights gained from Section 4.1 is not conclusive to me. The authors state the following. \n\n> This indicates that the empirical robustness of Randomized Smoothing does not come from\nthe randomness of the noise, but from the self-ensembling effect of aggregating multiple inferences\nwithin proximity of the original point.\n\nFigure 2 shows that a white-box attacker achieves the same success rate regardless of whether the noised perturbations are being resampled during the optimization procedure. However, the inability of an attacker to find an adversarial example does not imply that the randomness does not help. If the attacker knows the corruptions used by the defender, more successful attacks are conceivable. \n\n**Minor.**\n\n* Table 1 is confusing to me. Why is the robustness higher for the undefended model? What is the difference between the first column and the second and third?"
                },
                "questions": {
                    "value": "* Why is it meaningful to study the robustness of deterministic defenses? \n\n* What is the advantage of iterating over a grid in a subspace rather than over randomly sampled noisy perturbations?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7742/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698797521889,
            "cdate": 1698797521889,
            "tmdate": 1699636945053,
            "mdate": 1699636945053,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5U4tHw9qZC",
                "forum": "8S7eGD15b6",
                "replyto": "dmRireYKtn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7742/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7742/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response part 1/2"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for considering our work interesting, well-written, and detailed, which we are happy to see is a common note in the reviews. Below, we provide responses that we hope alleviate the reviewer\u2019s concerns. Please let us know if there are any other questions we can answer.\n\n> I do not understand in what sense the randomness limits the evaluation of a defense\n\nA more detailed discussion of the problems with randomness in evaluation is in Section 6. A brief summary of these issues is that including randomness in a defense prevents an evaluator from using several useful tools (e.g., black-box attacks, checking whether the underlying gradient is smooth, Subspace Grid-sweep). This extra difficulty in evaluating some random defenses, as shown in Section 4, may not be necessary for the defense to operate and can obscure otherwise easier to find issues (as discussed in Section 5). A recent example of this is concurrent work that had to create a specialized attack to show that DiffPure is masking/obscuring gradients, and is not as robust as originally reported last year [A]. However, our analysis in Section 5 shows this finding in a simpler way with the observation that Subspace Grid-sweep is able to find adversarial examples that AutoAttack\u2019s A-PGD cannot.\n\n> In other words, if an attack succeeds against a deterministic variant of a defense, what implications does that have on the robustness of the randomized version?\n> Why is it meaningful to study the robustness of deterministic defenses?\n\n\nThis is a good question. Whether a deterministic variant failing indicates a flaw in the original random variant likely depends on the specific defense being evaluated. If we find that the deterministic variant is showing obfuscated gradients, then adding randomness likely does not solve the obfuscated gradients. However, if a deterministic defense is much less robust to APGD than the random version, then this could mean different things based on what the details of the defense are, but it at least means that the unpredictability of the randomness to the attacker is important for this defense to work, as with only that component removed the defense is degraded. Overall, it gives more information to the evaluator.\n\n> What is the advantage of using a grid in a subspace over simply randomly sampling noisy perturbations?\n> What is the advantage of iterating over a grid in a subspace rather than over randomly sampled noisy perturbations?\n\nAs we understand it, there are two parts to this question: (1) Why use a grid search instead of random sampling? And (2) Why search in a lower-dimensional subspace? We\u2019ll give our reasoning on the first question and then the second.\n\nThe advantage of using a grid is that it allows us to get a sort of *ground-truth* for what adversarial regions exist that should be able to be found by gradient-guided attacks usually used for evaluation (such as AutoAttack\u2019s A-PGD). As a grid search checks every point at evenly distanced intervals within a space, a grid search ensures that no regions larger than the interval between grid points will be missed. This is in contrast to random sampling that, from bad luck, could miss a large adversarial region in a space by not choosing a point there. However, exhaustively doing a grid-search in a large dimensional space is prohibitive.\n\nHence, the answer to question 2. The reason we do grid-search in a lower-dimensional subspace is to make it feasible. By also constraining gradient-guided attacks to the same subspace, we have a sort of ground-truth (for regions of size above the interval between grid points) for what datapoints an attack should find an adversarial example for.\n\nOn top of this, we also do random search in our lower-dimensional subspace as an extra measure to find adversarial regions that could be hiding between grid points (results in Appendix A).\n\n\n> Also, function GridSweepPerturbationsL2 is unclear to me. What is $\\sigma$ (I assume this should be epsilon)? For g=0 and B=6, the new g assignment is -1, but the comment is misleading because it says the points are normalized between [0,1].\n\nThank you for catching this typo! You are correct, the comment should say \u201cscale grid to [-1,1] and origin center\u201d. Also, you are correct that $\\sigma$ represents the $L_2$ constraint distance. We are happy to change this to $\\epsilon$ to match how we refer to the constraint in the text."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700100229459,
                "cdate": 1700100229459,
                "tmdate": 1700100229459,
                "mdate": 1700100229459,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CRdN0WwjNV",
                "forum": "8S7eGD15b6",
                "replyto": "dmRireYKtn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7742/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7742/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response part 2/2"
                    },
                    "comment": {
                        "value": "> However, the inability of an attacker to find an adversarial example does not imply that the randomness does not help. If the attacker knows the corruptions used by the defender, more successful attacks are conceivable.\n\nAs discussed in Section 6, we posed as future work to explicitly explore if the attacker somehow has an edge.\n\nHowever, for the purposes of this paper in showing whether currently used evaluation attacks empirically evaluate robustness well, this edge does not show up given benchmark attacks. Some explanation of why this edge does not show is discussed in Section 5 using Subspace Grid-sweep in Table 2 where (Smoothing) Defended has a higher Robustness against Grid-sweep than (Smoothing) Base, which implies there are simply smaller adversarial regions (i.e., many fewer adversarial examples) that can be found when using randomized smoothing, regardless of whether it is random or deterministic.\n\n> Table 1 is confusing to me. Why is the robustness higher for the undefended model? What is the difference between the first column and the second and third?\n\nYou have read this table correctly, and this shows the main result from Section 3, in that kWTA-defended models are actually less robust than the original undefended versions. Our Subspace Grid-sweep shows this issue without catering an attack specifically to kWTA as previous works had to do (Tram\u00e8r et al., 2020).\n\nThe first column shows robustness against Grid-sweep, as defined in algorithm 2 (1 - (number of vulnerable datapoints / number of datapoints)). The second\nand third columns show the number of vulnerable datapoints Grid-sweep found and how many of those AutotAttack\u2019s A-PGD found. The idea behind these columns is to make it easily visible when AutoAttack\u2019s A-PGD is not finding many of the vulnerable datapoints that Grid-sweep is finding, which would indicate the defense is hindering AutoAttack\u2019s A-PGD from finding existing adversarial regions/examples.\n\n[A] Zhao, Zhengyu, et al. \"Towards Good Practices in Evaluating Transfer Adversarial Attacks.\" arXiv preprint arXiv:2211.09565 (2022)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700100249710,
                "cdate": 1700100249710,
                "tmdate": 1700100249710,
                "mdate": 1700100249710,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KSheVlEXGI",
                "forum": "8S7eGD15b6",
                "replyto": "CRdN0WwjNV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7742/Reviewer_EJoi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7742/Reviewer_EJoi"
                ],
                "content": {
                    "title": {
                        "value": "Revised paper"
                    },
                    "comment": {
                        "value": "Thank you for your responses. They make sense to me! Could you please provide a revised paper with all the promised changes (ideally highlighted in some color)?"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700329898301,
                "cdate": 1700329898301,
                "tmdate": 1700329898301,
                "mdate": 1700329898301,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2VEyq8dbYS",
                "forum": "8S7eGD15b6",
                "replyto": "UciD2ijjpA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7742/Reviewer_EJoi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7742/Reviewer_EJoi"
                ],
                "content": {
                    "comment": {
                        "value": "Yes, I meant all revisions. I would like to see everything that changes in the paper. Thank you"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657726417,
                "cdate": 1700657726417,
                "tmdate": 1700657726417,
                "mdate": 1700657726417,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BaXp06lfUG",
            "forum": "8S7eGD15b6",
            "replyto": "8S7eGD15b6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7742/Reviewer_YZ44"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7742/Reviewer_YZ44"
            ],
            "content": {
                "summary": {
                    "value": "Recent studies have demonstrated that many defenses, which appear to defend leading-edge attacks at first, may eventually fail against an adaptive attacks. Defenses can seem effective when adversarial examples aren't discovered due to issues like gradient masking, limited computing resources, or poor initial conditions. In this paper, the authors propose a step forward in improving defense assessment by developing a new tool called Subspace Grid-sweep. This tool uses deterministic inference to simplify the process of assessing adversarial robustness. By applying Subspace Grid-sweep, the authors illustrate that a defense method previously thought to be effective\u2014and later proven ineffective\u2014could have been identified as vulnerable without conducting an exhaustive adaptive attack. Furthermore, to extend the Subspace Grid-sweep\u2019s utility to defenses that incorporate randomness, the authors demonstrate methods for creating deterministic versions of these random defenses that maintain comparable empirical performance. Consequently, their findings suggest that randomness might not be essential for the robustness of such defenses."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper unveiled an interesting finding that some randomness-based defenses do not rely on their randomness for the improved robustness.\n2. The authors leveraged some interesting methods to transform the randomness-based defenses to deterministic ones to show their robustness.\n3. The paper is well-organized and clearly presented."
                },
                "weaknesses": {
                    "value": "1. The grid-sweep-based attacks seem to be very straightforward, and I think if the hidden space is large, the search space for this attack could be still huge.\n2. The experiments of diffPure are not clear to me. I believe that diffPure still needs some randomness to be successful otherwise it would be no different from a plain denoiser network. Why is the robust accuracy still as high as around 65% in Figure 4?"
                },
                "questions": {
                    "value": "Please refer to the weakness section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7742/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698886126613,
            "cdate": 1698886126613,
            "tmdate": 1699636944946,
            "mdate": 1699636944946,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "e8O3MXY8f5",
                "forum": "8S7eGD15b6",
                "replyto": "BaXp06lfUG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7742/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7742/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks so much for your review! We are happy to hear you found our work interesting, well-organized, and easy to understand. We respond to the weaknesses you point out below, and hope this allows the paper strengths and usefulness to be increasingly apparent.\n\n> The grid-sweep-based attacks seem to be very straightforward, and I think if the hidden space is large, the search space for this attack could be still huge.\n\nWe agree that if the subspace dimensionality is too large (i.e., more than 20), then it would stop making sense to exhaustively search it with subspace grid-sweep. However, the underlying point of the evaluation is not to prove that a defense is robust, but to see if the current attacks being used to evaluate a defense are good enough. For that, we found that using much smaller subspaces (1-6 dimensions) did pretty well, as discussed in Sections 3 and 5, with more detail given in the Appendix. For this reason, we do not think the search space needs to be large to check whether gradient-guided attacks are being hindered by a defense.\n\n> The experiments of diffPure are not clear to me. I believe that diffPure still needs some randomness to be successful otherwise it would be no different from a plain denoiser network. Why is the robust accuracy still as high as around 65% in Figure 4?\n\nThis is a good question! Firstly, we are pretty certain that this is the correct result, since we use the exact same code as the original DiffPure paper, except that we fix the random seed to be the same before each inference. We suspect that there is some gradient masking occurring that does not allow AutoAttack to find adversarial examples, which is also supported by our Subspace Grid-sweep results with DiffSmall in Section 5. Fortunately, some recent concurrent work came to the same conclusion by spending a lot more effort specifically examining DiffPure, and found compelling evidence that DiffPure has obfuscated gradients that prevent AutoAttack from finding valid adversarial examples [A]. Our results using Subspace Grid-sweep also discover these issues, and they do so in a more straightforward and general way.\n\n[A] Zhao, Zhengyu, et al. \"Towards Good Practices in Evaluating Transfer Adversarial Attacks.\" arXiv preprint arXiv:2211.09565 (2022)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700099611588,
                "cdate": 1700099611588,
                "tmdate": 1700099611588,
                "mdate": 1700099611588,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZBgpPVUuhO",
                "forum": "8S7eGD15b6",
                "replyto": "e8O3MXY8f5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7742/Reviewer_YZ44"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7742/Reviewer_YZ44"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "Thanks for the rebuttal. I will keep my positive rating at 6."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7742/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700500141312,
                "cdate": 1700500141312,
                "tmdate": 1700500141312,
                "mdate": 1700500141312,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]