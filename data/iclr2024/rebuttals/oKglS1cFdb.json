[
    {
        "title": "Feature Accompaniment: Is It Feasible to Learn Out-of-Distribution Generalizable Representations with In-Distribution Data?"
    },
    {
        "review": {
            "id": "UHx0sCzp6g",
            "forum": "oKglS1cFdb",
            "replyto": "oKglS1cFdb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1827/Reviewer_jEjy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1827/Reviewer_jEjy"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers the question of whether it is feasible to learn good representations for OOD generalization with only ID data, without considering inductive biases of the architecture and learning algorithm. First, the paper looks at an experiment where models are trained to learn the features of pretrained models that exhibit good OOD performance. It is found that these distilled models have OOD performance better than standard models only trained on the ImageNet training set, but not as good as the original pretrained models, suggesting that it is not possible to learn good OOD representations from ID data, even given access to \u201coracle\u201d representations known to perform well OOD. Via theoretical analysis of 2-layer ReLU networks, the paper then unveils a novel failure mode of OOD generalization called feature accompaniment. This failure mode is shown theoretically to stem from inductive biases of nonlinear networks, and is absent in deep linear models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The paper identifies a novel and intuitive failure mode of out-of-distribution generalization, distinct from the prevailing attention on spurious correlations\n* The paper provides principled theoretical foundations that prove the existence of this failure mode for 2-layer\n* The takeaways of the study are applicable to future theoretical study of OOD generalization. In particular, the paper attempts to make the highly of-interest case that existing theoretical models of OOD generalization may not cover why OOD generalization failure happens in practice."
                },
                "weaknesses": {
                    "value": "* The claim in Section 2 of the existence of an OOD generalization failure mode beyond the reach of generalization theory, and related to feature learning may not be fully justified by the empirical results in the section. Please see Question 1 below.\n* The study does not suggest how one might make the findings actionable in an empirical setting to improve or predict OOD generalization ability. It is thus unclear how significant the results or the identified failure mode are.\n* Relatedly, the paper does not make a case for how much OOD generalization failure is attributable to this failure mode in empirical settings, if any at all."
                },
                "questions": {
                    "value": "Question 1:\nI am unconvinced that the empirical results of section 2 imply the existence of a failure mode related to the feature learning process. The empirical result may not necessarily be due to nonlinear feature-learning dynamics in this experiment, but rather just that pre-trained CLIP models contain features covering a much larger data distribution than is captured by models distilled on ImageNet. In particular, if you distill a model from CLIP on the ImageNet training set, are some CLIP features that are not represented in ImageNet not likely to be left out? These features could still be helpful in OOD classification. For example, the OOD bird and car could have core features that look different from the core features seen in-distribution. Pretrained models may contain these features, while distilled models may not have learned them if they do not appear in the training set.\n\n\nQuestion 2:\nCould there be discussion on how feature accompaniment relates to the previous studies on simplicity bias and gradient starvation [1,2], which find that networks rely on simple features and ignore more complex features? In particular, work on Gradient Starvation [2] suggests that an increase in strength of a simpler feature inhibits the learning of other more complex features. Are these results contradictory to those suggested by feature accompaniment?\n\n\n\n[1] The Pitfalls of Simplicity Bias in Neural Networks, https://arxiv.org/abs/2006.07710\n\n[2] Gradient Starvation: A Learning Proclivity in Neural Networks, https://arxiv.org/abs/2011.09468"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1827/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1827/Reviewer_jEjy",
                        "ICLR.cc/2024/Conference/Submission1827/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1827/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817249758,
            "cdate": 1698817249758,
            "tmdate": 1699787104572,
            "mdate": 1699787104572,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Nymgeu0iuD",
                "forum": "oKglS1cFdb",
                "replyto": "UHx0sCzp6g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1827/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1827/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jEjy"
                    },
                    "comment": {
                        "value": "Thank you for your detailed and insightful review! Your feedback has led to some fruitful discussions among the authors and is helpful for us to improve our paper. In general, there are three major concerns raised in your review:\n\n- Connections between feature accompaniment and the empirically observed OOD generalization failure and the significance of our results.\n\n- Can the empirical observations in Sec. 2 be explained by the fact that CLIP may have learned some OOD core features that are absent in the ID data?\n\n- Discussion on how feature accompaniment relates to the simplicity bias of neural networks.\n\nSince the first concern is common among all reviewers, we provide a general response to it in our \"General response to all reviewers\" panel, along with a discussion on the significance of our contributions. Please see our general response for more details.\n\nFor the second concern, we believe that **our experimental results cannot be solely explained by CLIP having the ability to extract OOD core features due to large-scale pre-training**. This is due to our linear probe evaluation protocol: first train the linear probe on top of given (CLIP/distilled) representations _**on the training set (ID data)**_, then test its accuracy on both ID and OOD test sets. In other words, _this linear probe still needs to solve an OOD generalization task_---although with inputs as CLIP/distilled representations instead of raw images.\n\n- Having this in mind, now assume that the distribution shift robustness of CLIP is attributed to it having the ability to extract OOD core features from OOD images. If this is indeed the case, then the linear probe based on CLIP representations should **not** exhibit very good OOD generalization performance. This is because those OOD features are **absent** in the ID images (and hence also absent in the CLIP representations of ID images as well) and thus the linear probe trained using only ID data is **not** able to learn to use those OOD features to achieve OOD generalization.\n\n- As a piece of empirical evidence that supports the above explanation, we have visualized the CLIP representations for both ID and OOD images (see Fig. 9 in Sec. G for details). The visualization shows that ID and OOD representations exhibit the geometry where the distribution shift on the features is roughly in a direction **orthogonal** to the classification boundary, so it is not likely that the CLIP's advantage in robustness is due to learning \"more\" OOD features that provide additional information for prediction.\n\nFor the third concern, we believe that feature accompaniment and simplicity bias are two **complementary** feature learning biases of neural networks that may manifest in different data distributions: both works [1] and [2] consider a data model involving multiple features that are **equally predictive** for the task; the network may then only learn some of them due to the simplicity bias. By contrast, the background features in our data model are assumed to be **not predictive at all** for the task, yet we theoretically show that the network may still learn them due to feature accompaniment---note that _this does not contradict the simplicity bias_ since the resulting solution can still be \"simple\", e.g., learning sparse core/background features. Instead, **we believe that feature accompaniment as a novel inductive bias of neural networks helps extend our general understanding of the feature learning biases of neural networks**. Technically, we would also like to note that the theory in [1] only considers a specific dataset (LSN dataset with one linear coordinate and one 3-slab coordinate) while we consider a more general data model, and the theory in [2] is under the neural tangent kernel regime, in which the network is essentially linearized and the features are **not** learned (they stay at random initialization).\n\n**We hope that you can consider raising the score if our response has addressed your concerns and we are willing to engage in further discussion if any of our explanations are still unclear.** We are looking forward to your reply.\n\n---\n\n[1] The Pitfalls of Simplicity Bias in Neural Networks.\n\n\n[2] Gradient Starvation: A Learning Proclivity in Neural Networks."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1827/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700071270064,
                "cdate": 1700071270064,
                "tmdate": 1700142197567,
                "mdate": 1700142197567,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TBhqAeKcfJ",
            "forum": "oKglS1cFdb",
            "replyto": "oKglS1cFdb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1827/Reviewer_9qmH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1827/Reviewer_9qmH"
            ],
            "content": {
                "summary": {
                    "value": "This paper tries to study whether it is possible to learn OOD-generalizable representations with only in-distribution data. The authors discover a new failure model that they refer to as feature accompaniment, which is caused by the inductive biases of training process of nonlinear neural networks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "* This paper studies OOD from the perspective of inductive bias, which has rarely considered in existing literature. They consider the training process of neural network, which is more practical than directly considering the global minimum.\n* From their theoretical analyses, they find an interesting failure mode ''feature accompaniment''. In my understanding, this means that due to the asymmetry of activation, each neuron tends to correlate more with one class than another. Then, this can further make the projection of gradients onto background features non-zero, which makes the final model also use background features to classify. I think this ''feature accompaniment'' may be a very fundamental phenomenon caused by the asymmetry of activation, which can also be used to understand other properties of neural network. I think my own research can draw some inspiration from it.\n\nI hope to obtain more insights from upcoming discussions with the authors and I'm happy to further raise my score."
                },
                "weaknesses": {
                    "value": "* I think the experiment part in Section 2 is a bit disconnected from the theoretical part in Section 4. They consider different settings and different learning objective. I don't think theory in Section 4 can explain the experimental results in Section 2. I know that Section 2 is probably just a starting point of studying whether OOD-generalizable representations are learnable, so this's okay. But I think it could be better to connect them more in the writing. \n* The training process the authors mainly studied is based on ERM with regularization. Since there is only one training domain, this is okay. But I want to know if there are several domains whose background distributions are different, will the training process of Equation (1) (instead of ERM) still cause \"feature accompaniment\"? Are there any cases even if we have multiple domains we can still not learn a OOD-generalizable neural network?"
                },
                "questions": {
                    "value": "Please see Cons."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1827/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698901745815,
            "cdate": 1698901745815,
            "tmdate": 1699636112030,
            "mdate": 1699636112030,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ejs5KEdSSF",
                "forum": "oKglS1cFdb",
                "replyto": "TBhqAeKcfJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1827/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1827/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9qmH"
                    },
                    "comment": {
                        "value": "Thank you for your positive review and your approval of our work! We are very glad to know that you found our theoretical results novel, foundamental and may benefit understanding the learning process of neural networks in other scenarios. In general, your review has raised two major concerns:\n\n- The experiment part and the theoretical part seem a bit disconnected.\n\n- What will happen if we have **multiple** training domains (with different background feature distributions) instead of only one training domain?\n\nSince the first concern is common among all reviewers, we provide a general response to it in the \"General response to all reviewers\" panel, along with a discussion on the significance of our contributions. Please see our general response for more details.\n\nFor the second concern, we further break it down into two questions as mentioned in your review:\n\n 1. Are there any cases where we cannot achieve OOD generalization even with multiple domains?\n\n 2. If we do have multiple domains and run some more advanced algorithms rather than ERM, will feature accompaniment still happen?\n\nFor Q1, our answer is yes: note that \"OOD generalization with multiple training domains\" is typically studied in the context of _domain generalization_ (see [1,2] for some examples). Indeed, prior studies in this area [1,2] have pointed out that large OOD generalization gaps remain even with multiple training domains and more advanced algorithms than ERM---in fact, they show that under a fair evaluation protocol, even ERM is often comparable to many of the recently published OOD generalization algorithms! Also, our experiments in the DomainNet dataset (results are in the last panel of Fig. 1) are based on this domain generalization setup where we have three training domains and one test domain, yet the distilled model still underperforms OOD.\n\nFor Q2, we believe the answer is also yes for the following reasons:\n\n- As we mentioned above, current algorithmic improvement over ERM is still limited---we believe this fact indicates that other objectives may also suffer from similar generalization difficulty as ERM.\n\n- Our empirical evidence indicates that feature accompaniment is a general phenomenon that also happens beyond the binary classification setting used by our theory (see our general response for more details). We thus believe that feature accompaniment can also happen for many objectives other than ERM.\n\n- For the particular objective mentioned in your review, i.e., directly minimizing Eq. (1) (note that in practice we need to substitute $\\mathbb{D}$ with $\\mathbb{D}_\\mathrm{train}$ since we only have access to the training domains), our analysis can directly apply: Proposition 2 in [3] proves that minimizing this objective is equivalent to minimizing a weighted mixture of the risk in each training domain, which is then equivalent to performing ERM on the corresponding weighted data distribution. Our results then trivially hold if this weighted training distribution satisfies Def. 1. Empirically, our experiments in the DomainNet dataset also consider this objective as one of the standard models (referred to as \"GroupDRO\"; please see Sec. E.4 for more details), which does not yield good OOD generalization performance.\n\n**We hope that you can consider raising the score if our response has addressed your concerns and we are willing to engage in further discussion if any of our explanations are still unclear.** We are looking forward to your reply.\n\n---\n\n[1] I. Gulrajani and D. Lopez-Paz. In search of lost domain generalization. In ICLR, 2021.\n\n\n[2] P. W. Koh et al. Wilds: A benchmark of in-the-wild distribution shifts. In ICML, 2021.\n\n\n[3] M. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1827/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700071358287,
                "cdate": 1700071358287,
                "tmdate": 1700071358287,
                "mdate": 1700071358287,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "bssFR96CEv",
            "forum": "oKglS1cFdb",
            "replyto": "oKglS1cFdb",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1827/Reviewer_Lh7e"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1827/Reviewer_Lh7e"
            ],
            "content": {
                "summary": {
                    "value": "This work tries to answer \"Can we learn OOD generalizable representations from in-distribution data?\" empirically and theoretically. \n\nIn the empirical part, the term **OOD generalizable representation\" mainly indicates a representation that contains rich features. The author investigates the OOD linear probing performance of three kinds of pretrained models: 1) a CLIP pretrained model on super large&diverse dataset; 2) a supervised pretrained model on Imagenet dataset; 3) a supervised pretrained model on Imagenet dataset with more objective information (i.e. prediction the representation of a CLIP model). \n\nThe author treats the 3rd model as the oracle objective function --- *\"representation learning objective itself cannot be further improved in general\"*. Hence concludes that *\"OOD generalizable representations may not be learnable using only ID data without explicitly taking into account the inductive biases of the model or the task.\"*\n\nIn the theoretical part, however, the term **OOD generalizable representation** changes to indicate \"a representation that doesn't contain spurious signals (or background feature signals) and only contains invariant signals (or core feature signals). The author uses a 2-layers Relu network to show that --- a non-convex network (especially with asymmetric activations) could \"learn and store\" some background feature signals in the representation even though these background features have no correlation with the target label."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- It is interesting to investigate out-of-distribution generalization problem through the rich-representation (a representation contains a rich set of features that could be redundant in-distribution but crucial out-of-distributation) point of view.  \n\n- It is also interesting to show that a non-convex network could \"learn and store\" some irrelative signals (per-example level spurious features) in the representation even though these signals are not (or weakly) correlated with the target label in the whole-dataset level."
                },
                "weaknesses": {
                    "value": "- As I commented in the **Summary**, the empirical part and theoretical part use different principles. So that they can not support each other. Please check **Summary** for details. In my opinion, that is the biggest weakness.\n\n- In the empirical part, this work treats \"good OOD linear probing performance\" as \"good generalization representation\" (Figure 1). The principle here is \"rich-representation\"[1][2]. I suggest the author clarify the principle. \n\n- The author treats -- a supervised pretrained model on Imagenet dataset with more objective information (i.e. prediction the representation of a CLIP model) -- as the **oracle** objective function. By comparing this model (pretrained on Imagenet) with CLIP (pretrained on a large dataset), the author concludes that *\"OOD generalizable representations may not be learnable using only ID data without explicitly taking into account the inductive biases of the model or the task.\"* \n\nOn one hand, this comparison doesn't support the conclusion. From the rich-representation's principle (which is used in the linear probing experiment), OOD linear probing benefits from a representation that contains diverse and simple features. Indeed, CLIP (pretrained on a large dataset) contains rich features. But please remember that CLIP uses more data. It is possible that the model  above (pretrained on Imagenet) is already the best (by say \"best\", I mean a model that achieves the best OOD linear probing performance) imagenet pretraining model. In short, CLIP model (pretrained on a large dataset) should not be assumed as an achievable upper bound of other Imagenet pretrained model. \n\nOn the other hand, this Imagenet pretrained model is not **oracle** in terms of rich-representation. Compared with Imagenet's 1k target categories, indeed this object contains more supervision information (with the help of CLIP and CLIP's pretraining dataset). But how about 22k target categories, for example? \n\n- The theoretical section didn't discuss the relationship between works about SGD and features, e.g. [3][4][5].\n\n[1] Zhang, J., Lopez-Paz, D., & Bottou, L. (2022, June). Rich feature construction for the optimization-generalization dilemma. In International Conference on Machine Learning (pp. 26397-26411). PMLR.\n[2] Zhang, J., & Bottou, L. (2023, July). Learning useful representations for shifting tasks and distributions. In International Conference on \n[3]Andriushchenko, M., Varre, A. V., Pillaud-Vivien, L., & Flammarion, N. (2023, July). Sgd with large step sizes learns sparse features. In International Conference on Machine Learning (pp. 903-925). PMLR.\n[4]Blanc, G., Gupta, N., Valiant, G., & Valiant, P. (2020, July). Implicit regularization for deep neural networks driven by an ornstein-uhlenbeck like process. In Conference on learning theory (pp. 483-513). PMLR.\n[5]Pezeshki, M., Kaba, O., Bengio, Y., Courville, A. C., Precup, D., & Lajoie, G. (2021). Gradient starvation: A learning proclivity in neural networks. Advances in Neural Information Processing Systems, 34, 1256-1272."
                },
                "questions": {
                    "value": "- please see weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1827/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1827/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1827/Reviewer_Lh7e"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1827/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699651416290,
            "cdate": 1699651416290,
            "tmdate": 1699651416290,
            "mdate": 1699651416290,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "emfdnT7L9p",
                "forum": "oKglS1cFdb",
                "replyto": "bssFR96CEv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1827/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1827/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Lh7e"
                    },
                    "comment": {
                        "value": "Thank you for your detailed and constructive review! We have found your feedback very helpful in improving the quality of our work. However, _**we believe that there is a key misunderstanding of our experimental protocol in your review, which may have led to some misinterpretations of our work**_. We apologize if it was our current writing that caused the misunderstanding and will clarify our experimental protocol according to your feedback to avoid potential misunderstanding from future readers.\n\n- Your review mentioned that _\"In the empirical part, this work treats 'good **OOD linear probing** performance' as 'good generalization representation'. The principle here is 'rich-representation' [1,2].\"_ We are aware of the insightful works by Zhang et al. [1,2] and would like to clarify that **our evaluation protocol is _not OOD linear probing_** (i.e., training and testing the linear probe on both OOD data, similar to the linear probing protocol in [2]), **but _OOD testing of the linear probe_** (i.e., training the linear probe on **ID** data and testing it on OOD data). Note that this is a **fundamental distinction**: in OOD linear probing, the linear probe essentially solves an **ID generalization** task since its training and testing are on the **same** distribution---in this case, the richness of the representation indeed plays a key role (under the theoretical framework of [2]). However, in our experiments, the linear probe still needs to solve an **OOD generalization** task, albeit with inputs as some pre-trained representations rather than raw images. In this setting, _better linear probing performance does not necessarily mean a 'richer' representation_ (see below for more discussion).\n\n- Given the above clarification, \"an OOD-generalizable representation (oracle representation)\" in our experiments should be viewed as \"a representation based on which an ID-trained (linear) classifier can generalize OOD\". In this regard, **the CLIP representation is indeed a proper approximation of the \"oracle representation\"** since it has been known for exhibiting strong robustness to distribution shifts [3]. Our experiments thus **do support our conclusion** that \"OOD-generalizable representations may not be learnable using only ID data\".\n\nWe then address your other concerns regarding (1) the \"rich-representation\" interpretation of our empirical results and (2) comparison with other theoretical work on neural network features.\n\n- **On the rich-representation interpretation:** as mentioned in your review, a potential explanation of the generalizability of the CLIP representation is its richness (in the sense that it _\"contains a rich set of features that could be redundant in-distribution but crucial out-of-distribution\"_, taken from your review). We **do agree** that this representation richness contributes to generalizability and hypothesize that this may be one of the reasons for the distilled model outperforming standard models. However, _if the generalizability of CLIP can be solely attributed to its richness, then the distilled model _should also achieve good OOD generalization_ since it **directly learns**_ this rich representation during training (instead of learning the representation by a _proxy_ classification task, which may fail to capture _all_ useful features, as shown by [1,2]). However, our experiments suggest that this is **not** the case. Instead, we believe that **feature accompaniment can explain the remaining large gap between distilled models and CLIP**; please see our general response for more details.\n\n\n- **Comparison with work on neural network features:** To our knowledge, our discovery of feature accompaniment is novel, and **no** existing theoretical study on SGD-trained neural networks has explored this feature learning bias. As shown by the cited works in your review and **Reviewer jEjy**, one of the major focuses of existing theory on the inductive biases of neural networks is the **simplicity bias**, i.e., the bias of finding solutions that are \"simple\" or sparse. Feature accompaniment **complements existing observations** by showing that neural networks also have the bias of learning \"_irrelevant_ features\" in certain tasks (note that this does not contradict with sparsity).\n\nAgain, we are very thankful for your feedback and will add the discussion on the rich-representation to the revised paper.\n**We hope that you can consider raising the score if our response has addressed your main concerns and we are willing to engage in further discussion if any of our explanations are still unclear.** We are looking forward to your reply.\n\n---\n\n[1] Zhang, J., Lopez-Paz, D., & Bottou, L. Rich feature construction for the optimization-generalization dilemma. In ICML, 2022.\n\n\n[2] Zhang, J., & Bottou, L. Learning useful representations for shifting tasks and distributions. In ICML, 2023.\n\n\n[3] A. Radford et al. Learning transferable visual models from natural language supervision. In ICML, 2021."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1827/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700071499265,
                "cdate": 1700071499265,
                "tmdate": 1700071499265,
                "mdate": 1700071499265,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0yoQsuTejg",
                "forum": "oKglS1cFdb",
                "replyto": "emfdnT7L9p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1827/Reviewer_Lh7e"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1827/Reviewer_Lh7e"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your answer!\n\nThe linear probing here is trained on ID and evaluated on both ID and OOD. I got you. Thank you. \n\nLet's list the experiments in this paper (before the theory part):\n1) CLIP representation, trained on large & diverse data, achieves the best OOD performance.\n2) Supervised representation, trained on ID data (imagenet) and imagenet 1k labels, achieves the worst OOD performance. \n3) Supervised representation, trained on ID data (imagenet) and CLIP generated continuous targets (so called representation), locates in between. \n4) During the comparisons above, ID performance is aligned. \n \nI agree with your comment **\"the CLIP representation is indeed a proper approximation of the \"oracle representation\"\"**. We can treat CLIP representation in 1) as a good oracle representation. That is totally fine. \n\nMy question in my previous review is about **\"oracle objective function\"**. The paper treats 3) as an oracle objective function. (e.g.  - *\"representation learning objective itself cannot be further improved in general\"*.) The paper further concludes the main point *\"OOD generalizable representations may not be learnable using only ID data without explicitly taking into account the inductive biases of the model or the task.\"* based on this \"oracle objective function\" assumption. Is it an oracle objective function? What if I use both imagenet 1k labels and this CLIP generated continuous targets? \n\nOn the other hand, the ID (in-distribution) concept should include both inputs (e.g. images) and targets (e.g. labels). In method 3), the target is changed, so it is not *ID data* any more..."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1827/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711433612,
                "cdate": 1700711433612,
                "tmdate": 1700711433612,
                "mdate": 1700711433612,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Je71WC4nmi",
                "forum": "oKglS1cFdb",
                "replyto": "bssFR96CEv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1827/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1827/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Continual response to Reviewer Lh7e"
                    },
                    "comment": {
                        "value": "**Thank you for your continual engagement! We are glad that we have solved your concerns regarding our linear probing protocol and the comparison between our protocol and the rich-representation principle after our clarification.** We address your remaining concerns in the following:\n\n- **On the \"oracle\" objective:** Thank you for your clarification on your comment. Yes, we believe that our representation distillation objective **is** \"oracle\" in terms of representation learning (if we do not have additional prior knowledge of the model/the task and thus treat the model as a black box). We can actually argue this mathematically: consider each model $h$ to be composed of an encoder $\\Phi$ and a (linear) classifier $f$. Given each input $x$ (e.g., an image), we treat $\\Phi(x)$ as **the representation of $x$** and $f$ as the classifier on top of the representations extracted by $\\Phi$. Given a training set $(X,Y) = {(x_i,y_i)}_{i=1}^N$, **the aim of an ideal representation learning process is that the learned representation $\\Phi(x_i)$ recovers some oracle representation** $\\Phi^*({x_i})$ **for every** $x_i \\in X$, where we approximate $\\Phi^*$ using CLIP. In this regard, our representation distillation objective (i.e., $\\mathrm{minimize}\\  \\lVert \\sum_i \\Phi(x_i) - \\Phi^*(x_i) \\rVert_2^2$ ) is indeed consistent with the above idealization since its minimizer does uniquely recovers $\\Phi^*(x)$ for every $x$---we thus refer to it as the **oracle objective** in the above sense. **By contrast**, existing objectives perform representation learning using proxy tasks such as a classification objective defined over the final output of $h(x)$, or other auxiliary objectives based on $h(x)$ and/or $\\Phi(x)$. However, minimizing such objectives does **not** guarantee the unique recovery of $\\Phi(x) = \\Phi^*(x)$ as guaranteed by our oracle objective. Of course, even by minimizing the oracle objective, we cannot guarantee $\\Phi = \\Phi^*$ since we only have $\\Phi(x) = \\Phi^*(x)$ for a finite training set, which leads to the remaining gap between the distilled model and CLIP---our theoretical results suggest that feature accompaniment plays an important role in this gap when $\\Phi$ is instantiated as a non-linear neural network trained by SGD.\n\n   **Empirically**, in our preliminary experiments, we have also tested using both the original labels and the CLIP representations in representation learning, as the ablation mentioned in your review. Our results suggested that this variant indeed does **not** outperform only using CLIP representations. Due to time limitations, we will add those results to the camera-ready version of the paper if it is accepted.\n\n- **On the definition of ID and OOD:** This is a good point! Yes, strictly speaking, the notion of \"distribution\" in both \"ID\" and \"OOD\" is the _joint distribution_ over $\\mathcal{X}\\times\\mathcal{Y}$. However, in a practical OOD generalization regime (and also in existing benchmarks), \"ID\" and \"OOD\" often essentially stem from the distribution difference over the **input space** $\\mathcal{X}$ since the ground-truth labeling function (i.e., $p(y|x)$) is often assumed to be consistent across different distributions. Therefore, we consider the notions \"ID data\" and \"OOD data\" mainly as \"ID inputs\" and \"OOD inputs\" but not their (conditional) label distributions. Note that this setting is natural and consistent with existing algorithmic studies since many existing OOD generalization algorithms also construct auxiliary objectives (e.g., by using domain labels [1, 2, 3, 4]) other than the original labels.\n\nWe hope that the above clarification can solve your concerns. Also, **we would like to note that the theoretical part constitutes a more significant part of the overall contributions of our work beyond the experiments** (please see our general response for a discussion on the significance of our contributions).\n\n---\n\n[1] Y. Ganin et al. Domain-adversarial training of neural networks. Journal of Machine Learning Research, vol. 17, no. 59, pp. 1\u201335, 2016.\n\n[2] D. Krueger et al. Out-of-distribution generalization via risk extrapolation (rex). In ICML, 2021.\n\n[3] M. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-Paz. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019.\n\n[4] A. Rame, C. Dancette, and M. Cord. Fishr: Invariant gradient variances for out-of-distribution generalization. In ICML, 2022."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1827/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725876673,
                "cdate": 1700725876673,
                "tmdate": 1700726140587,
                "mdate": 1700726140587,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]