[
    {
        "title": "Structured Packing in LLM Training Improves Long Context Utilization"
    },
    {
        "review": {
            "id": "rzyoADUAUn",
            "forum": "PN0SuVRMxa",
            "replyto": "PN0SuVRMxa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5879/Reviewer_ixkc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5879/Reviewer_ixkc"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a novel method called Structured Packing for Long Context (SPLICE) aimed at improving the context utilization in long-context Large Language Models (LCLMs). The authors identify that the lack of long-range semantic dependencies in typical training data hinders the effective utilization of context in LCLMs. To address this, they propose incorporating related documents more frequently into training inputs. By using BM25 to collate the most mutually relevant documents into a single training context, the authors demonstrate that SPLICE can enhance model performance across various tasks and can be used to train large models to better utilize long contexts. The method was validated by training a large 3B model and showed improvements in perplexity and better long-context performance on a benchmark key-value retrieval task."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper introduces an innovative method to improve the context utilization of LCLMs. The SPLICE approach, which involves structuring training data using BM25, is interesting and it can be applied to any textual data, making it more generally applicable.\nThe paper demonstrates that the application of SPLICE results in improvements in perplexity across various tasks."
                },
                "weaknesses": {
                    "value": "1.  Using Lexical matching methods to concatenate the documents into a longer one is a very engineering technique and it is a straightforward solution to construct longer samples. \n\n2. The experimental results are almost based on PPL, lacking experiments on real-world tasks. More experiments on benchmarks such as zeroScrolls[1] or L-Eval[2] to validate their models are needed. More extensive testing across a broader range of tasks and datasets would provide a more comprehensive evaluation of the method.\n\n3. Presently, the prevalent strategies for training long context models involve the use of extensive conversations and literary works. A comparative analysis of SPLICE with these existing methodologies is thus a necessary step.\n\n[1] ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding, 2023\n[2] L-Eval: Instituting Standardized Evaluation for Long Context Language Models, 2023"
                },
                "questions": {
                    "value": "1. How does the choice of the BM25 method for document retrieval affect the model's performance? Would other document retrieval methods yield similar results?\n\n2. How can we SPLICE  on very large pertaining corpus which usually has more than 400B tokens?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5879/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5879/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5879/Reviewer_ixkc"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5879/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698753204892,
            "cdate": 1698753204892,
            "tmdate": 1699636622952,
            "mdate": 1699636622952,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5byYoYU39B",
                "forum": "PN0SuVRMxa",
                "replyto": "rzyoADUAUn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5879/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5879/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the Reviewer for a thoughtful review.\n\nWe are not sure what point 1 of weaknesses means. We gently ask the reviewer for clarification. \n\nWe agree that perplexity is not enough to fully evaluate the method. We now provide positive results for two datasets: TREC [1, 2] and Qasper [3, 4]. See details in the general answer and the revised pdf. In the general answer, we also provide new experiments with neural retrieval, which allow for efficient scaling (see the table below).\n\nWe consider the techniques relying on extensive conversations (e.g. LongChat [5]) as complementary to our work. We note that SPLiCe can utilize data that already exists in vast quantities and can be easily applied to different types of text (like code, Wikipedia articles, stack exchange questions and answers, etc.). It is interesting to evaluate how both would interact, which we now include in the future work section on page 9.\n\n**Answers to the questions:**\n\n**Q1:** We now test our method using Contriever-MSMARCO [6]  instead of BM25 and observe that the results are very close. We attach the details in the general response and in the revised version of the paper (see Table 1 on page 5).\n\n**Q2:** SPLiCe using the Contriever-MSMARCO retrieval is scalable to large datasets. Namely, the dataset can be effectively queried using an approximate index (Faiss [7] in our case) which is a common technique, used in e.g. kNN-LM [8], RETRO [9]. \nWe have also performed an experiment to check whether SPLiCe will bring improvements when applied to a random subset of a larger set. For this, we have extracted 2GB from English Wikipedia and augmented it using SPLiCe. The results are presented in Table 3 on page 6. The outcome indicates that SPLiCe does not need to retrieve from the whole dataset to provide performance improvements. For example, one could split the training dataset into smaller parts and run SPLiCe independently for each of them reducing the complexity even further.\n\n[1] Li et. al. Learning Question Classifiers  \n[2] Hovy et. al. Toward Semantics-Based Answer Pinpointing  \n[3] Dasigi et. al. A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers  \n[4] Shaham et. al. SCROLLS: Standardized CompaRison Over Long Language Sequences  \n[5] Li et. al. How Long Can Open-Source LLMs Truly Promise on Context Length?  \n[6] Izacard et.al. Unsupervised Dense Information Retrieval with Contrastive Learning  \n[7] Johnson et. al. Billion-scale similarity search with GPUs  \n[8] Khandelwal et. al. Generalization through Memorization: Nearest Neighbor Language Models  \n[9]  Borgeaud et. al. Improving language models by retrieving from trillions of tokens"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5879/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700075088516,
                "cdate": 1700075088516,
                "tmdate": 1700075088516,
                "mdate": 1700075088516,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TFIaOrvsKS",
                "forum": "PN0SuVRMxa",
                "replyto": "rzyoADUAUn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5879/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5879/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We hope that our explanations and additional results show the potential of SPLiCe. In particular, our evaluation on additional downstream tasks along with results regarding Contriever-MSMARCO and Faiss replacing BM25.\n\nConsidering the approaching rebuttal deadline, we ask the Reviewer for comments and a score increase in case our findings meet the requirements."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5879/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502946657,
                "cdate": 1700502946657,
                "tmdate": 1700502946657,
                "mdate": 1700502946657,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "r2nwC2olxZ",
            "forum": "PN0SuVRMxa",
            "replyto": "PN0SuVRMxa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5879/Reviewer_wDkv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5879/Reviewer_wDkv"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes structured packing for long context (SPLiCE) that constructs long context training examples by retrieving relevant documents using BM25. After experiments on a small language model with different datasets and configurations, SPLiCE is applied to large-scale language models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The main idea of constructing better training examples makes sense. SPLiCE is not too complicated and does not require expensive overhead or external models by relying on BM25."
                },
                "weaknesses": {
                    "value": "Considering the additionally introduced complexity (though the SPLiCE algorithm is simple), the performance improvement looks very marginal, especially for large-scale models. \nOnly the part of the training is replaced with SPLiCE from the random baseline. That might be one of the reasons for marginal improvement, but it also implies that SpLiCE is not a standalone solution that can completely replace the existing training algorithm. \n\nLanguage modeling perplexity is the main evaluation metric. Comparing performance on other NLP downstream tasks that require long context modeling might be better to evaluate the effectiveness of the proposed method."
                },
                "questions": {
                    "value": "I raised several concerns about why SPLiCE is not sufficient (or at least not fully validated) as it is. Could you address them?\n\nI guess the number of neighbors for each document is skewed, meaning that there exists hub documents. In that case, although a root document is randomly sampled from the document corpus, the retrieved documents are not uniformly distributed in terms of their likelihood and order. Couldn't this be a problem that may result in an imbalance in training? \n\nPacked documents are unnatural and different from contiguous documents. Is there any way to alleviate this issue?\n\nAs expected, using related documents in a long context is better than the random baseline. However, any design choices (top-k, order, or even REPO vs. SPLiCE) give clear differences.\nIn particular, top-1 is the best, and in that case, BFS is the same as DFS. \n\nWhy is Table 1 required? Table 2 fully covers Table 1.\n\nThe structure of the paper can be improved. For example, it is awkward that Section 4 also includes experiments while the title of Section 3 is experiments. Also, multiple Figures and Tables can be merged to spare some space for more extensive experiments or discussion."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5879/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698901806798,
            "cdate": 1698901806798,
            "tmdate": 1699636622861,
            "mdate": 1699636622861,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GqtnRAYA4P",
                "forum": "PN0SuVRMxa",
                "replyto": "r2nwC2olxZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5879/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5879/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the Reviewer for their insightful feedback.\n\n\nWe note that SPLiCe is a stand-alone solution. Namely, it could be used for the whole training. We use it in fine-tuning due to computational constraints. We also note that it is fairly standard to train on a short context for the majority of training steps and extend it in the final phase, see e.g. CodeLlama [1]. We believe that the performance gains validate the usage of the algorithm, taking into account that it is simple and its computational overhead is negligible compared to the training costs. \n\nWe totally agree with the request for NLP evaluation. We now provide results for TREC [2, 3] and Qasper [4, 5] datasets, which confirm the usefulness of the SPLiCe training. Please refer to the results in the general answer.\n\nIn what follows, we address the Reviewer\u2019s questions:\n* Regarding the imbalance. We use each document only once. A document that is used in training is removed (see the last lines of Alg 1). This design choice ensures that for one epoch the whole dataset is seen. What is more, we mix the SPLiCe prepared data with data prepared in a standard way, which should prevent the model from overfitting to the structure of SPLiCe constructed examples. In our setup, we have not noted training instabilities.  We agree that there is some subtlety here and open questions, which we now note in the further research section (see page 9).\n* We agree that there is some risk in using \u2018unnatural\u2019 packed documents. To alleviate this we train on a mixture of data prepared in the standard way and SPLiCe (which is now explained in Section 3.1). Based on [1, 6] we think that using data mixture is enough to alleviate false biases.\n* SPLiCe is a general framework for constructing long context data using short documents. For example, for k=1 and the retriever returning elements from the closest subdirectory in the repository we get the Repo method [7]. With larger k and reversed order we get the data that resembles inputs to retrieval augmented models (e.g.  REALM [8]). We believe that leaving the possibility for other choices paves the way for future research and is of use to the community. \n* Thank you for pointing out several exposition improvement areas. We have made multiple changes towards this goal (e.g. the mentioned split of the results in Table 1 and Table 2 and the titles) and we will gladly accept any further suggestions.\n\n[1] Rozi\u00e8re et. al. Code Llama: Open Foundation Models for Code  \n[2] Li et. al. Learning Question Classifiers  \n[3] Hovy et. al. Toward Semantics-Based Answer Pinpointing  \n[4] Dasigi et. al. A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers  \n[5] Shaham et. al. SCROLLS: Standardized CompaRison Over Long Language Sequences  \n[6] Ouyang et. al. \u201dTraining language models to follow instructions with human feedback  \n[7] Wu et. al. Memorizing Transformers  \n[8] Guu et. al.  REALM: Retrieval-Augmented Language Model Pre-Training"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5879/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700074916041,
                "cdate": 1700074916041,
                "tmdate": 1700130902691,
                "mdate": 1700130902691,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ahdhPnvvLQ",
                "forum": "PN0SuVRMxa",
                "replyto": "r2nwC2olxZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5879/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5879/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Using the additional time, we managed to evaluate our method using larger 7B parameter models, longer context and more downstream tasks (the details are present in general response). We hope that our additional results and explanations showcase the potential of SPLiCe.\n\nConsidering the approaching rebuttal deadline, we ask the Reviewer for comments and a score increase in case our findings meet the requirements."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5879/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502907034,
                "cdate": 1700502907034,
                "tmdate": 1700502907034,
                "mdate": 1700502907034,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FeVHkpST3N",
            "forum": "PN0SuVRMxa",
            "replyto": "PN0SuVRMxa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5879/Reviewer_21Pg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5879/Reviewer_21Pg"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes SPLICE, a similarity-based approach of grouping documents into pretraining examples to training better long context language models. For each example, the method starts with a single document and uses a BM25 retriever to include more relevant documents in the example in a BFS fashion. \n\nWhen applied to training a 270M model, the method outperforms the random baseline on both text and code perplexity. The model is also on par with the REPO method which relies on knowledge of the corpus structure. When used to train a 3B model, the method also outperforms the baseline on both perplexity and the key-value retrieval task. Ablation studies are included to analyze the impact of hyperparameters."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The method is simple yet effective and can be easily applied to different scenarios.\n- Reproducibility: The authors attach the source code, which is great. Please also release the code if the paper is accepted.\n- Clarity: the paper is well written and easy to understand.\n- Significance: the significance of the paper is okay."
                },
                "weaknesses": {
                    "value": "- The effectiveness of the method is only validated on language modeling and the key-value retrieval task. This does not guarantee the resultant model is stronger on realistic use cases. To test the usefulness of SPLICE, I would highly recommend comparing the models on more challenging and realistic long-context downstream tasks such as Quality and Squality.\n- It would be great if the method is tested on more settings: Use a neural retriever in addition to BM25, go beyond 3B and 32K, etc.\n- Novelty: The main idea is quite similar to many existing methods like the ones discussed in the paper (e.g. retro). However, I don't think the paper should be rejected only because of this."
                },
                "questions": {
                    "value": "- 3.4 \u201cOn the code evaluations, the improvements are small.\u201d - Why say so? The average improvement on code datasets is 0.0625 and the improvement on arxiv is 0.07. The improvements seem to be similar.\n- 4.2: \u201cThe perplexity difference is larger for tokens further in the document\u201d - I might misunderstand something, but it seems the improvement is also large at the start? (Figure 3)\n- Typo: 3.2 \u201cMoreover, even thorough this method uses only the code data\u201d\n- In the abstract \u201cOur results indicate that SPLICE enhances model performance across various task\u201d - The context of this sentence is the 270M model. It is in fact only tested on one task: language modeling (though there are different datasets). You might want to rephrase to reduce confusion."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5879/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5879/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5879/Reviewer_21Pg"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5879/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699093257321,
            "cdate": 1699093257321,
            "tmdate": 1699636622767,
            "mdate": 1699636622767,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3y7zPl2jAp",
                "forum": "PN0SuVRMxa",
                "replyto": "FeVHkpST3N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5879/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5879/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the Reviewer for an encouraging review. \n\nWe now provide results showing that the SPLiCe training is useful for downstream NLP tasks: TREC [1, 2] and Qasper [3, 4]. Likewise, we are happy to report results for SPLiCe using Contriever-MSMARCO [5]. Please see the details in the general answer and the revised pdf and let us know if this is satisfactory. \n\nDue to computational constraints, we have not been able yet to go beyond the current size/context length but we hope to provide results for the 7B/32K model and 270M/64K model before the rebuttal deadline. \n\n\n\n**Regarding questions:**\n\n**Q1:** We acknowledge that this statement is confusing. We have rephrased it as follows: \"Despite the non-code nature of StackExchange we still note perplexity improvements on code. Unsurprisingly, the perplexity on code is not as good as when training directly on code data.\u201d.\n\n**Q2:** We apologize, but a mistake has slipped through, and we have unintentionally provided results of the 3B parameter model that was trained with a context length smaller than 32K. We corrected that in the revised version, providing the corresponding perplexity improvement results that now match the description. We also re-evaluated the model on the kv retrieval task and added the comment about improved performance. See the revised pdf, page 8 and 20.\n\n**Q3:** We thank you for spotting the typo. We have fixed it and merged sections 3.2 and 3.3 into one.\n\n**Q4:** Thank you for the suggestion. We have rephrased this part as follows: \u201cOur results indicate that SPLiCe enhances model performance and can be used to train large models to utilize long contexts better.\u201d\n\n\n\n[1] Li et. al. Learning Question Classifiers  \n[2] Hovy et. al. Toward Semantics-Based Answer Pinpointing  \n[3] Dasigi et. al. A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers  \n[4] Shaham et. al. SCROLLS: Standardized CompaRison Over Long Language Sequences  \n[5] Izacard et.al. Unsupervised Dense Information Retrieval with Contrastive Learning"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5879/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700074738089,
                "cdate": 1700074738089,
                "tmdate": 1700074738089,
                "mdate": 1700074738089,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WmjfSSoZhw",
                "forum": "PN0SuVRMxa",
                "replyto": "FeVHkpST3N",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5879/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5879/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Using additional time, we extended our research to 7B parameter models with 32K context and 270M parameter models with 64K context length. We add additional results to the general response. We hope that those findings along with the ones presented in our previous responses showcase the potential of SPLiCe. In particular our previous results have tested the method on two additional downstream tasks.\n\nConsidering the approaching rebuttal deadline, we ask the Reviewer for comment and a score increase in case our findings meet the requirements."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5879/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700502831123,
                "cdate": 1700502831123,
                "tmdate": 1700502966538,
                "mdate": 1700502966538,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]