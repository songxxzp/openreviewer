[
    {
        "title": "A Data Perspective on Enhanced Identity Preservation for Diffusion Personalization"
    },
    {
        "review": {
            "id": "RpeyTLa037",
            "forum": "jYsowwcXV1",
            "replyto": "jYsowwcXV1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6466/Reviewer_AXFR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6466/Reviewer_AXFR"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new method to inject new visual concepts into the generation, using few images. The authors propose a novel regularization dataset generation strategy on both the text and image level. The formulated dataset can help to prevent losing text coherence and prompt better identity preservation. The results are established on benchmarks, demonstrating the effects of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed data approach is effective on the chosen benchmarks."
                },
                "weaknesses": {
                    "value": "1.\tThe formatted prompt generation is limited to several categories. For example, according to the supp., for live objects, the prompts are all obtained via the subject of animal. How about the prompts for human? I do not think this prompt generation strategy is general enough.\n\n2.\tMoreover, I think the prompts should be generated according to different input images, employing multi-modality models.\n\n3.\tI think the new objects to be inserted into the generation in this paper\u2019s experiments are few. More cases are needed to analyze the effects of the proposed method.\n\n4.\tIn Fig.4, why the performance of w/o format will lead to worse results as the increase of iteration number? Even without the use of formatted prompts, the generation results should be more fitted with the target object along with the training."
                },
                "questions": {
                    "value": "1.\tI wonder the performance of the proposed method if there are fewer input examples, like 1-3 examples.\n\n2.\tCan the prompts be generated online with the training? It will save a lot of time.\n\n3.\tFew examples can not reflect the true quality of the generation, is there any subjective evaluation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The generated images may be harmful to the protection of copyright."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6466/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698502079194,
            "cdate": 1698502079194,
            "tmdate": 1699636723339,
            "mdate": 1699636723339,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rHbWioHTrM",
                "forum": "jYsowwcXV1",
                "replyto": "RpeyTLa037",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6466/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6466/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for the helpful comments. We address each question below.\n\n## W1: Generalization to other categories?\nWe use object/animal as categories, which covers most kinds of living and non-living objects. For a particular application domain, one could always generate more specific prompts with minor modifications. It would only take a few seconds by using our LLM strategy. We add an example on humans and show the results in Figure 15 (updated manuscript) along with the comparison with other methods. It demonstrates that the same prompt generation works for humans and that our approach preserves better identity than other methods while still aligning with the text prompts.\n\n## W2: Generating prompts from images instead of generating images from prompts?\nWe agree that it is an interesting alternative direction, but it is dissimilar from our goals. One of our goals is to exploit the broad distributions over words and their relation learned by LLMs to make the image generation more diverse.\n\n## W2 & Q2: Can the prompts be generated online with the training, from the training images, using multi-modality models?\nThis is a promising direction that however bears its own challenges. We did an initial experiment on the \u201c\u200b\u200btortoise plushie\u201d in Figure 14 (updated manuscript). We use the multi-modal model BLIP to generate the captions and then use ChatGPT to reformulate the sentence. Subsequently, we use an LLM to convert these into the correct format with the special <new> token inserted. We show the details in Appendix E (updated manuscript). This alleviates writing the prompt examples manually, i.e., it replaces the manual steps in Section 3. As shown in Figure 14, the results are as good as the ones with the manually created prompts and outperform other methods.\n\n\n## W3: Need more qualitative results?\nWe already have a range of qualitative results in Figure 9-12 (numbers referring to the updated manuscript). We added some more results for comparison in Figure 13-15. In addition, we evaluated on the DreamBench testsuite. We added this comparison to the supplementary materials. Due to the space limit (<100M), we only sample each prompt once for each subject, resulting in 750 comparison images. \n\n## W4: Why does the experiment of \u201cw/o format\u201d lead to worse results as the increase of iteration number?\nIt is a consistent finding as we also observed this before and analyzed in Sec 4.2 first paragraph \u201cHow important is the formatted prompt? \u2026\u201d. We additionally noticed that in the first 2,000 iterations, the simplistic regularization set with prompt \u201ca [subject]\u201d effectively prevents overfitting. However, it fails to do so with prolonged training and thus cannot preserve the fine details of the subjects. We believe it is due to the deviation between the simplistic prompt \u201ca [subject]\u201d and the complexity of the images. Such a simplistic prompt is not detailed enough to explain the image. Therefore, with longer training, the model degenerates. Interestingly, even without the regularization set, if the prompts of the training example are accurate instead of simplistic, the model overfits in terms of limited diversity but without degeneration, as shown in Figure 4 (w/o reg dataset).\n\n## Q1: Performance of the proposed method with fewer input examples?\nWe added the ablation tests, see common question #1.\n\n## Q2: \nSee W2 above.\n\n## Q3: Missing human evaluation?\nWe added one, see common question #3."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6466/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722981208,
                "cdate": 1700722981208,
                "tmdate": 1700723076173,
                "mdate": 1700723076173,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xI4zpyH1Sx",
            "forum": "jYsowwcXV1",
            "replyto": "jYsowwcXV1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6466/Reviewer_rHiC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6466/Reviewer_rHiC"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a data-driven approach to improve personalized generation. The paper first discovers that previous class regularization is ineffective in alleviating overfitting due to a lack of diversity. The paper proposes to first enhance the training prompts associated with the concept images by including more specific class names and background descriptions. Based on the training prompts, the regularization prompts are further enhanced by introducing structural components including shape, color, and texture, which are then amplified with more diverse backgrounds and styles."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) The paper attempts to improve personalized T2I generation from a data-centric perspective, which focuses on automatically generating a rich and informative regularization dataset. Despite there exist a few methods that improve T2I generation via better prompting [1, 2] (note that [1] was submitted to arXiv on 25 Oct 2023, hence not in the scope of this work), this paper is the first work to improve personalized generation by diversifying the regularization data. \n\n(2) This paper provides insights into the importance of the quality of the regularization dataset in order to prevent overfitting, and is complementary to existing works that attempt to improve architectures and training schemes. This may inspire future research on further improving diffusion personalization.\n\n(3) The proposed method demonstrates a notable improvement in generating personalized images with higher fidelity and is capable of preventing overfitting especially when facilitating larger training iterations.\n\n\n[1] Segalis, Eyal, et al. \"A Picture is Worth a Thousand Words: Principled Recaptioning Improves Image Generation.\" arXiv preprint arXiv:2310.16656 (2023).\n[2] Wang, Yunlong, Shuyuan Shen, and Brian Y. Lim. \"RePrompt: Automatic Prompt Editing to Refine AI-Generative Art Towards Precise Expressions.\" Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. 2023."
                },
                "weaknesses": {
                    "value": "(1) The baselines that are compared in this paper are textual inversion and DreamBooth, which are both pioneering works in diffusion personalization. However, there exist many more improved personalization methods, e.g. Custom Diffusion [3], that are also widely used. Experimenting based on more methods will further emphasize the generalizability and complementarity of the proposed method.\n\n(2) On top of Custom Diffusion [3], it would be also interesting to see whether the data-driven approach can benefit multi-concept learning.\n\n(3) The method requires generating a relatively large regularization dataset (containing ~2000 images), which inevitably leads to much longer training time.\n\nSmall TYPO:\n1. TYPO in section 5, the first bullet in the first paragraph should be \u201c1)\u201d instead of \u201c2)\u201d\n\n[3] Kumari, Nupur, et al. \"Multi-concept customization of text-to-image diffusion.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023."
                },
                "questions": {
                    "value": "Please see the weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6466/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698769418681,
            "cdate": 1698769418681,
            "tmdate": 1699636723172,
            "mdate": 1699636723172,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3s6wKNPedF",
                "forum": "jYsowwcXV1",
                "replyto": "xI4zpyH1Sx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6466/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6466/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for the helpful comments and appreciation of our method. We address each question below.\n\n## W1: New baseline CustomDiffusion?\nWe added the qualitative comparison in Figure 13-15 (updated manuscript). Our generated images have higher subject alignment than theirs while having better or equal text alignment. According to our user study in comparison to DreamBooth, our method is preferred in subject alignment by 64.9%. An indirect comparison is possible to CustomDiffusion which, in an equivalent user study, scored only 56.62% compared to DeamBooth.  Due to the time limit, we will add direct comparison with CustomDiffusion in the camera-ready version.\n\n## W2: Data-driven approach for multi-concept learning?\nIt is indeed an interesting topic we would like to explore in the future. For this paper, we focus on the detailed subject alignment, i.e., identity preservation. We added this to our conclusion.\n\n## W3: Too large regularization set?\nWe kindly refer you to the question #1 in the common response. A smaller regularization set (<500) also works.\n\n## Typo:\nFixed, thx.\n\n## Related Work\nAdded and discussed in Section 2. Thx."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6466/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722847373,
                "cdate": 1700722847373,
                "tmdate": 1700723202537,
                "mdate": 1700723202537,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zWb2GdoY6t",
            "forum": "jYsowwcXV1",
            "replyto": "jYsowwcXV1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6466/Reviewer_hHDA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6466/Reviewer_hHDA"
            ],
            "content": {
                "summary": {
                    "value": "It seems (please refer to Weaknesses for reasons why I use the word \"seem\") that the authors introduced an extension of DreamBooth, which can better preserves the details of the objects of interest. The authors proposed to achieve this goal by generating a larger set of regularization images. Specifically, they seem to be generated using prompts that are (1) generated by a large language model (LLM) or (2) generated following specific templates. Experiments are conducted on DreamBench and metrics show that the proposed method generates higher quality images than existing methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. Extensive visual comparison with existing methods are presented in the paper. It is evident that, for the examples provided, the proposed method better preserves the details of the objects of interest\n\n2. The use of English is satisfactory. \n\n3. Ablations are conducted to help readers understand several design choices made by the authors."
                },
                "weaknesses": {
                    "value": "1. Poor presentation: I find this paper hard to follow. Several important aspects of the proposed method remain a mystery, e.g., how the 2000 \"regularization images\" are created, why the 2000 images are called regularization images (I am not able to see how they can regularize the model from \"Each training batch contains one example from training set and one example from regularization set.\"). It seems to me that this paper tries to extend DreamBooth [Ruiz, 2023a] and the 2000 regularization images may be generated using Stable Diffusion and the 2000 samples may be used to compute the class-specific prior preservation loss to regularize the model. However, a reader needs to be very familar with DreamBooth in order to make these guesses and they are just guesses.\n\n2. To me, this is a trival extension of DreamBooth. The authors proposed to generate more \"regularization images\" using prompts that are (1) generated by a large language model (LLM) or (2) generated following specific templates. It is hard for me to agree that this paper meets the bar for an ICLR paper.\n\n3. Lack of human evaluation."
                },
                "questions": {
                    "value": "1. How are the 2000 regularization images created? Are they generated by a pre-trained diffusion model? If so, why do you use the word \"created\"?\n\n2. Why the 2000 images are called \"regularization images\"? How can they help regularize the model? If you follow DreamBooth [Ruiz, 2023a], please specifically mention this.\n\n3. Would be great to see a comparison with DreamBooth + LoRA and Textual Inversion + LoRA.\n\n4. How does the performance of the proposed method change when more number of samples are available, e.g., 20 samples? How does the performance of the purposed method compare with other methods when more number of samples are available?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6466/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6466/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6466/Reviewer_hHDA"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6466/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698820277031,
            "cdate": 1698820277031,
            "tmdate": 1699636723011,
            "mdate": 1699636723011,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xxqiqI0GBa",
                "forum": "jYsowwcXV1",
                "replyto": "zWb2GdoY6t",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6466/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6466/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for the helpful comments. We address each question below.\n\n## W1: Hard to follow without knowing DreamBooth?\nWe sincerely thank you for pointing out the points where writing can be improved. To make it easier for readers, we created a separate subsection at the beginning of our method section (Section 3), to introduce DreamBooth in the updated manuscript.\n\n## W2: Trivial extension to DreamBooth? \nDreamBooth also generates a dataset, which is called a prior set. Their purpose is to preserve the ability to generate similar subjects so that the model does not overfit. While they can train 2000 iterations without overfitting, the model still overfits and gets degenerated with longer training, as shown in Figure 4. Our insight is that their simplistic prompt \u201ca [subject]\u201d and its generated images are not powerful enough to prevent overfitting. Furthermore, the simplistic prompt \u201ca [subject]\u201d may not be enough to explain the diverse generated images. The deviation between the prompts and the images may result in model degeneration with prolonged training, as shown in Figure 4 (w/o format). Interestingly, even without the regularization set, if the prompts of the training example are accurate instead of being simplistic, the model only overfits without breaking down entirely, as shown in Figure 4 (w/o reg dataset). Our paper focuses on the importance of accurate prompts and their similar prompts while DreamBooth focuses on the importance of images and their similar images. Our prompts prevent the model from degeneration. They work as hard examples, forcing the model to relate the details with the special token. Instead of being a trivial extension to DreamBooth, our approach provides a new perspective to the problem, pushing the direction taken by DreamBoth further and deviating from the current trend to hand-tune model architectures. With the significant improvement we show, we expect a range of future work to follow in this underexplored direction.\n\n## W3: Missing human evaluation?\nWe added one, see common question #3.\n\n## Q1 & Q2: Missing description of the regularization set generation process?\nWhile our main approach is stated in Section 3, the details of how to generate the prompts and the corresponding images are in supplementary A. To make it easier to find, we now concatenated it with the original paper. As stated in Section 4, the 2000 regularization images are generated from Stable Diffusion XL with structured but diverse prompts. We use the word \u201ccreate\u201d because the regularization set contains not only the images but also the created prompts.\n\n## Q3: Comparison with DreamBooth + LoRA and Textual Inversion + LoRA?\nInvestigating different model architectures and fine-tuning strategies is orthogonal to our goal of improving the data. To show that our method outperforms also the more recently developed models, we compare our method with the recent CustomDiffusion model. The results are illustrated in Figure 13-15 (updated manuscript). Our generated images have higher subject alignment than theirs while having better or equal text alignment. According to our user study in comparison to DreamBooth, our method is preferred in subject alignment by 64.9%. An indirect human comparison is possible to CustomDiffusion which, in an equivalent user study, scored only 56.62% compared to DeamBooth. Due to the time limit, we will add direct comparison with CustomDiffusion in the camera-ready version.\n\n## Q4: Performance when more examples are available?\nWe show in Figure 14-15, examples using theCustomConcept101(https://github.com/adobe-research/custom-diffusion/tree/main/customconcept101) dataset. With more than 10 examples, our method still outperforms other methods, especially in the subject alignment."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6466/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722758931,
                "cdate": 1700722758931,
                "tmdate": 1700722758931,
                "mdate": 1700722758931,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "R24MOfGENv",
            "forum": "jYsowwcXV1",
            "replyto": "jYsowwcXV1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6466/Reviewer_vEAP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6466/Reviewer_vEAP"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose to perform prior preservation in personalized text-to-image generation with a regularization set. The authors construct this set by using ancestral sampling with formatted prompts. They tested their newly proposed regularization set on Stable Diffusion based models and achieved improvement compared to baseline."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The authors show qualitative and quantitative improvements compared to baselines in their experiments. In their qualitative examples, we can also observe that the fine-grained details of the objects are preserved better than the baselines."
                },
                "weaknesses": {
                    "value": "1. It is very difficult to convince myself that the novelty presented in this paper is significant enough to warrant an acceptance. The main contribution of this paper is to construct a regularization set using a predefined and handcrafted format for prompting and ChatGPT for picking the phrases to fit the format, and the details of the format are not very well justified.\n\n2. Continuing from Weakness 1, it is unclear to me how the authors choose the format described in Section 3 \u201cGenerating Against Training Prompts\u201d and \u201cAmplifying Diversity with Structured Prompts\u201d since there is no related literature or ablation study to justify the effectiveness of each component in the format.\n\n3. The additional time required for generating the regularization set is more substantial (2000 images for this setting v.s. < 1000 images for the original DreamBooth setting).\n\n4. There is no evaluation on the fidelity (e.g. FID score) of the generated image, and fidelity score is a standard metric for this task."
                },
                "questions": {
                    "value": "Does the size of the regularization set affect the performance? (e.g. will a smaller regularization set also work? Can the authors provide more ablation studies on this?)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6466/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6466/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6466/Reviewer_vEAP"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6466/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699214933121,
            "cdate": 1699214933121,
            "tmdate": 1699636722892,
            "mdate": 1699636722892,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JbBgeMJyh4",
                "forum": "jYsowwcXV1",
                "replyto": "R24MOfGENv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6466/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6466/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for the helpful comments. We address each question below.\n\n## W1: Trivial contribution?\nWe kindly refer you to the common question #2.\n\n## W2: Justification of the proposed prompt format? \nIn Section 4.2, we already ablate the importance of the prompt format. Without our format, the model tends to overfit to the training examples. Our format \u201ca [adjective] [subject] [in some background]\u201d is a simple and straight extension of the commonly used prompt \u201ca [subject]\u201d. We believe this simple yet effective method could inspire future researchers to advance this direction further.\n\n## W3: Requires too many images for regularization?\nWe kindly refer you to the common question #1 in the common response. According to our ablation tests, it only needs < 500 images in practice for simple cases, such as dogs and backpacks.\n\n## W4: No evaluation on FID, which is standard for this task?\nWe would like to kindly point out that FID is not a standard metric for the task of diffusion model personalization. We checked the 8 most related methods  [1,2,3,4,5,6,7,8], including the two pioneers, TextInversion [1] and DreamBooth [2]. None of them uses FID, except CustomDiffusion [3], which uses FID only to check whether the model can still generate other subjects, instead of quantifying the fidelity. In fact, in this task, researchers usually split the fidelity scores into subject fidelity and prompt fidelity: quantified with (DINO score and CLIP-I score) and CLIP-T score, respectively; which are already included in Table 1. FID is not directly applicable since it computes similarity of distributions in feature space, typically requiring thousands of reference images. However, here only a handful of example images of the target image are available. With such a low number, the FID score would be unreliable.\n\n## Q: Does the size of the regularization set affect the performance?\nWe kindly refer you to the question #1 in the common response. A smaller regularization set (<500) also works.\n\n\n[1] An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion\n\n[2] DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation\n\n[3] Multi-Concept Customization of Text-to-Image Diffusion\n\n[4] Cones: Concept Neurons in Diffusion Models for Customized Generation\n\n[5] SVDiff: Compact Parameter Space for Diffusion Fine-Tuning\n\n[6] StyleDrop: Text-to-Image Generation in Any Style\n\n[7] Key-Locked Rank One Editing for Text-to-Image Personalization\n\n[8] Subject-driven Text-to-Image Generation via Apprenticeship Learning"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6466/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700722595405,
                "cdate": 1700722595405,
                "tmdate": 1700722647193,
                "mdate": 1700722647193,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]