[
    {
        "title": "DiffCPS: Diffusion Model based Constrained Policy Search for Offline Reinforcement Learning"
    },
    {
        "review": {
            "id": "0RCgOz2UEc",
            "forum": "0aEUd9UtiA",
            "replyto": "0aEUd9UtiA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1330/Reviewer_4x5N"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1330/Reviewer_4x5N"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a new algorithm called DiffCPS for solving offline RL problems. They claim that DiffCPS can learn a diffusion-based policy avoiding the difficult density calculation brought by traditional AWR framework. They present theoretical justification for their approach and perform an empirical study to validate their proposed algorithms."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The numerical experiments are well designed, with a detailed section of ablation study. Also, the toy example is intuitive and interesting."
                },
                "weaknesses": {
                    "value": "* Corollary 3.1.1 and Theorem 3.2 are flawed. In the proof, the authors claim $J(\\mu)$ is an affine function of $\\mu$, which is **false**. In fact, $J(\\mu)$ is **not** convex w.r.t. $\\mu$ and the duality results can not hold. Consequently, the theoretical analysis presented in this paper is compromised, significantly undermining its contributions.\n\n* The paper lacks clarity. Multiple mathematical objects are introduced without clear definitions. For example, $Q_\\phi(s,a)$, $\\epsilon_\\theta(x_i,i)$, $f_\\phi(y\\mid x_i)$, $H(\\cdot,\\cdot)$, etc. . If the reader is not familiar with the literature on RL and diffusion models, she/he will definitely get confused by the undefined notations. Also, I think the authors should provide more explanations about the exact methods used to update $\\mu$ and $\\lambda$. It would be appreciated if the authors could give a complete and precise description of DiffCPSS. (If there are problems with page limits the authors can put it in the appendix.)\n\n* The empirical performance of the proposed algorithm exhibits only marginal improvement when compared to the baselines."
                },
                "questions": {
                    "value": "* The authors deploy diffusion-based policies in place of the traditional Gaussian policies to handle the multi-modal problem. I wonder whether there are choices other than diffusion models and naive Gaussians. For example, flow models can express a rich distribution class and have explicit densities. Is it possible to fit the flow models into the framework of AWR? \n\n* In the toy example, the authors claim that \"SfBC incorrectly models the dataset as a circle instead of a noisy circle\". I checked the image and did not find such a difference. Can the authors give an explanation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1330/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697549814468,
            "cdate": 1697549814468,
            "tmdate": 1699636060289,
            "mdate": 1699636060289,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GqQWWOCJHj",
                "forum": "0aEUd9UtiA",
                "replyto": "0RCgOz2UEc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1330/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1330/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to 4X5N"
                    },
                    "comment": {
                        "value": "We thank reviewer 4X5N for pointing out the drawbacks of our paper. Below, we address the concerns raised in your review point by point. Please let us know if you have any further concerns or whether this adequately addresses all the issues that you raised with the paper.\n>   **Q1:** Corollary 3.1.1 and Theorem 3.2 are flawed. \n\n  **A1:** Thanks for pointing this out.  We have rigorously re-proven the strong duality of Equation 12 in Appendix A.2. New proof relies on a well-known result from perturbation theory connecting strong duality to the convexity of the perturbation function. Here, we formally state the lemma used in the proof,  which is referred to [1] Cor. 30.2.2 and [2] Ch 3. \n>If (i) $r$ is bounded; (ii) Slater's condition holds for the diffusion-based constrained policy search and (iii) its perturbation function~$P(\\xi)$ is concave, then strong duality holds for equation 12.\n\nThe proof consists of two main steps (See details in Appendix A.2. in our revision): (i) demonstrating that the perturbation problem of the simplified diffusion-based constrained search problem, as per Theorem 3.1, is a concave function, and (ii) proving the Slater's condition holds for the diffusion-based constrained search problem. We have also rephrased the main theorems of the paper to ensure a more coherent logical flow.\n\n>   **Q2:** The paper lacks clarity. Multiple mathematical objects are introduced without clear definitions. If the reader is not familiar with the literature on RL and diffusion models, she/he will definitely get confused by the undefined notations. Also, I think the authors should provide more explanations about the exact methods used to update $\\mu$ and $\\lambda$ . It would be appreciated if the authors could give a complete and precise description of DiffCPSS. (If there are problems with page limits the authors can put it in the Appendix.)\n\n  **A2:** Thanks for suggesting this. We have added the Notation table in Appendix B and rephrased our DiffCPS algorithm in Chapter 3. Here, we reiterate the logic of our algorithm. We first establish the strong duality for Equation 12, allowing us to use Equation 14 to find the optimal policy. The approach to solving Equation 14 follows SAC [3], using alternating optimization to optimize the policy and Lagrange multipliers separately. Equation 19 and Equation 20 represent the loss functions for the Lagrange multipliers and the policy, respectively.\n\n>  **Q3:**  The empirical performance of the proposed algorithm exhibits only marginal improvement when compared to the baselines.\n\n\n\n**A3:** \n| D4RL Tasks              | DiffCPS (Ours)  | DiffusionQL [6] (ICLR 2023) | SfBC [4] (ICLR 2023)   |IQL [5] (ICLR 2022)|\n|-------------------------|---------------|-------------------|-------------|-----------|\n| Locomotion Average      | 92.26         | 87.9            | 75.6       |76.9|\n| AntMaze Average       | 81.69         | 69.8             | 74.2      |63.0|\n\n**Table 1:** D4RL performance of different offline RL methods.\nThe table above illustrates the performance of recent ICLR offline RL algorithms. SfBC (ICLR 2023) shows a similar performance to IQL (ICLR 2022), with an average decrease of 1.7% in locomotion tasks and a 17.8% improvement in antmaze tasks. The improvements of our DiffCPS compared to these algorithms are shown in the table below.\n\n| D4RL Tasks              | DiffusionQL (ICLR 2023) | SfBC (ICLR 2023)   |IQL (ICLR 2022)|\n|-------------------------|-------------------|-------------|-----------|\n| Locomotion Average       | 5%           | 22%       |20%|\n| AntMaze Average          | 17%            | 10%      |30%|\n\n**Table 2:** The improvements of our DiffCPS compared to recent ICLR offline RL algorithms.\n\nCompared with other offline reinforcement learning algorithms, it's evident that our algorithm exhibits a significant improvement over the baseline.\n\n>   **Q4:** In the toy example, the authors claim that \"SfBC incorrectly models the dataset as a circle instead of a noisy circle\". I checked the image and did not find such a difference. Can the authors give an explanation?\n  \n**A4:** Thanks for pointing this out. We've realized that our statement might lead to misunderstandings. We've revised the statement in our revision. What we intended to convey is that due to the architecture of SfBC introducing unnecessary sampling errors at T=15, it generates points that differ significantly from the dataset."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545049246,
                "cdate": 1700545049246,
                "tmdate": 1700545049246,
                "mdate": 1700545049246,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mHJNSPSX8V",
                "forum": "0aEUd9UtiA",
                "replyto": "0RCgOz2UEc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1330/Reviewer_4x5N"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1330/Reviewer_4x5N"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the authors' response. I find the revised version more readable. The revised proof appears to be sound. Nevertheless, I still have several questions after checking the proof in detail.\n\n1, **About equation (34).** In equation (34), the authors use the relationship $\\rho^{\\pi_b}(s)\\approx\\rho^{\\mu_t}(s)\\approx\\rho^{\\mu_1}(s)\\approx\\rho^{\\mu_2}(s)$. What does the $\\approx$ mean here? Why does such a relationship hold?\n\n2, **About the Slater's condition.** When verifying the Slater's condition, the authors only verify that for a specific $\\kappa_0$ the Slater's condition holds. My interpretation is that this means the Slater's condition might only be satisfied for certain choices of $\\epsilon$. Am I correct? However, it seems the authors conclude that the Slater's condition holds for all $\\epsilon$.\n\nIf the authors could address my concerns I would consider raising my score.\n\n(*Some random thoughts*: I understand this paper may not be a theory-oriented work. And I agree that intuitively it is totally fine to apply the dual methods even if the strong duality is only \"approximately true\". But I think if the authors choose to state their intuitions in the form of mathematical theorems, then they should ensure full technical rigor. )"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633684762,
                "cdate": 1700633684762,
                "tmdate": 1700633710956,
                "mdate": 1700633710956,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xdiBNtB5EM",
            "forum": "0aEUd9UtiA",
            "replyto": "0aEUd9UtiA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1330/Reviewer_hHF8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1330/Reviewer_hHF8"
            ],
            "content": {
                "summary": {
                    "value": "The authors tackle a KL-constrained offline RL problem, where a RL policy is trained but additional constraints are put on the distribution shift between the original policy and the trained policy. The authors point out the weakness in using a unimodal Gaussian policy and propose to use a diffusion process as a policy parametrization. The authors show some desirable properties of using diffusion as a policy parametrization, and propose a primal-dual iteration algorithm to solve KL-constrained offline RL. The authors compare against baselines in D4RL and present ablation studies of the algorithm."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Overall, this is a good paper with strong theoretical and empirical results. \n2. The visualization for the problem of having unimodal policies is very intuitive, and motivates a richer class of policies that can model more multimodal distributions in offline RL. \n3. The ablation studies of the algorithm is extensive and the authors have done good research into best practices in training diffusion models."
                },
                "weaknesses": {
                    "value": "1. The proposed baselines are entirely numerical and quantitative; while convincing, it would have been nice to see some qualitative behavior of DiffCPS compared to other baselines as well in order to strengthen the authors' claims. For instance, trajectory stitching is a popular example in offline RL where unimodal policies might possibly fail if done naively. In simple offline RL tasks such as the X data collection task in Diffuser [1], does DiffCPS succeed?\n\n2. One interpretation of the authors' work is that when we use a generative model of the training data from the behavior policy, this has the effect of automatically constraining the distribution shift between the learned policy and the behavior policy. The authors are missing some relevant work in this direction in the context of model-based offline RL. For instance, [2] motivates a very similar objective with DiffCPS, where the cross-entropy term is penalized rather than constrained (with the difference that the distribution shift is on the state-action occupation measures rather than the action distribution in the model-based setting).\n\n3. On a related note, offline RL also has model-free and model-based approaches, and the author's approach is model-free. The title might give the impression that this method is model-based, though I am aware that the authors' intention was to say it's based on a diffusion generative model. Maybe Diffusion based might be a better title?\n\n[1] Janner et al., \"Planning with Diffusion for Flexible Behavior Synthesis\", ICML 2022\n\n[2] Suh et al., \"Fighting Uncertainty with Gradients: Offline Reinforcement Learning with Diffusion Score Matching\", CoRL 2023"
                },
                "questions": {
                    "value": "1. Have the authors considered using Augmented Lagrangian?\n2. In Theorem 3.1, what is $d_{\\pi_b(s)}$? Should this be the occupation measure of the behavior policy, which the authors previously denote using $\\rho$? or the initial distribution of the behavior policy?\n3. How specific are the authors' claims to diffusion models? For instance, if we had trained a denoising autoencoder model (assuming they can be trained well), would the theorems still hold?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1330/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1330/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1330/Reviewer_hHF8"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1330/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698638100011,
            "cdate": 1698638100011,
            "tmdate": 1699636060207,
            "mdate": 1699636060207,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IpRLpRyV8u",
                "forum": "0aEUd9UtiA",
                "replyto": "xdiBNtB5EM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1330/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1330/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to hHF8"
                    },
                    "comment": {
                        "value": "We thank reviewer hHF8 for providing positive feedback and helpful suggestions. Below please find a response.\n\n>  **Q1:** Qualitative behavior of DiffCPS compared to other baselines\n\n**A1:**\nThanks for suggesting this. We have added an experiment to show the behavior of DiffCPS in Appendix F.1\n\n>  **Q2:** missing some relevant work\n\n  **A2:** Thanks for pointing that out. We have supplemented information about the related work on model-based offline reinforcement learning in Appendix E. And we will emphasize here the distinctions between our approach and [1]. [1] introduces a method that utilizes the score function of the Diffusion Model to approximate the gradient of the proposed uncertainty estimation. Our method, DiffCPS, diverges from this approach in several key aspects:\n(i) DiffCPS tackles the constrained policy search problem in offline RL through Lagrange dual and recursive optimization, while the method in the paper employs first-order optimizers directly.\n(ii) DiffCPS is a model-free method, in contrast to the model-based nature of the method in [1].\n(iii) The objective function in DiffCPS differs as it incorporates the loss from DDPM instead of score matching, and our coefficient is auto-adaptive.\n(iv) Our experimental results in D4RL demonstrate the superior performance of DiffCPS over the method in the paper, attributed to the design of our algorithm.\n\n>   **Q3:** Maybe Diffusion based might be a better title?\n\n  **A3:** Thanks for suggesting this. We notice that \"diffusion model-based\" could be ambiguous to some extent. Following your advice, we have changed the title and algorithm names to Diffusion-based constrained policy search to eliminate ambiguity.\n\n>   **Q4:** Have the authors considered using the Augmented Lagrangian Method (ALM)?\n\n  **A4:** Initially, for simplicity, we directly applied the alternating optimization (Dual Ascent) of SAC [2] to solve the dual problem. Theoretically, using ALM could offer better convergence performance. Our experiments on HalfCheetah demonstrate that using ALM  increases training stability. However, the final score is slightly lower than the Dual Ascent method. This could be attributed to (i) ALM introducing penalty terms, making the optimal target KL divergence for Dual Ascent not necessarily optimal for ALM, and (ii) the penalty function making the policy too conservative, thereby preventing the adoption of high-reward actions. We plan to further explore the use of ALM to optimize our problem in future research. You can find our discussion on the application of ALM in our algorithm in Appendix G.\n\n>  **Q5:**  question about $d_{\\pi_b(s)}$\n\n  **A5:** Thanks for pointing this out. $d_{\\pi_b(s)}$ denotes the unnormalized discounted state visitation frequencies of the behavior policy. We acknowledge the issues with the representation of some notations in the paper. In the revised version, we use $\\rho^{\\pi_b}(s)$ to represent the unnormalized discounted state visitation frequencies of the behavior policy and provide a notation table in Appendix B.\n\n>   **Q6:** How specific are the authors' claims to diffusion models? For instance, if we had trained a denoising autoencoder model (assuming they can be trained well), would the theorems still hold?\n\n  **A6:** In fact, when using a denoising autoencoder instead of the diffusion model, the theoretical properties of DiffCPS are no longer guaranteed. This is because (i) Constraint $\\int_a \\mu(a\\vert s)da =1$  is challenging to establish for other models; it holds for the diffusion model because $\\mu(a\\vert s) = \\int \\mu(a^{0:T}\\vert s)da^{1:T}$ holds. (ii) Denoising autoencoders struggle to accurately approximate the constraint on KL divergence.\n\n\n[1]  Suh et al., \"Fighting Uncertainty with Gradients: Offline Reinforcement Learning with Diffusion Score Matching\", CoRL 2023\n\n[2] Haarnoja, Tuomas, Aurick Zhou, Pieter Abbeel, and Sergey Levine. \u201cSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\u201d arXiv, August 8, 2018. https://doi.org/10.48550/arXiv.1801.01290."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545017564,
                "cdate": 1700545017564,
                "tmdate": 1700545017564,
                "mdate": 1700545017564,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1jb9o3Whg6",
                "forum": "0aEUd9UtiA",
                "replyto": "xdiBNtB5EM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1330/Reviewer_hHF8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1330/Reviewer_hHF8"
                ],
                "content": {
                    "title": {
                        "value": "comment"
                    },
                    "comment": {
                        "value": "I would like to thank the authors for the detailed response, I mostly agree with the points and will keep the score as is."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700701593681,
                "cdate": 1700701593681,
                "tmdate": 1700701593681,
                "mdate": 1700701593681,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CrIclP2Knk",
            "forum": "0aEUd9UtiA",
            "replyto": "0aEUd9UtiA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1330/Reviewer_UbgT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1330/Reviewer_UbgT"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies a constrained policy search in offline reinforcement learning. To increase the expressivity of Gaussian-based policies, the authors propose to use diffusion model to represent policy. The authors formulate a diffusion model based constrained policy optimization problem, and propose a constrained policy search algorithm. The authors also provide experiments to show the performance of this method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well organized, and the key idea is delivered. \n\n- The authors provide an example to show the expressivity limitation in standard advantage regression methods, which justifies the necessity of introducing diffusion models. \n\n- The authors use a popular diffusion model: DDPM to represent policy, and present a new constrained policy search method, which is intuitively simple and easy to implement. \n\n- Experimental results demonstrate comprable performance compared with state-of-the-art methods."
                },
                "weaknesses": {
                    "value": "- The importance or motivation of propositions, theorems, and corollaries is not well explained. Most results can be more directly obtained using simple calculations that are known in diffusion model. \n\n- The use of diffusion model as policy in constrained policy search has been studied in offline RL. Due to the similarity, it is important to distinguish them in an explicit way. \n\n- The authors claim strong duality for Equation (12) according to the duality in the convex optimization. However, the constrained policy search is a non-convex problem. It is not justified if the strong duality still holds.  \n\n- The main result is empirical. It is useful if the authors could provide performance analyses, which can strengthen the method with solid theoretical guarantees."
                },
                "questions": {
                    "value": "- A large paragraph of this paper introduces known results. Can the authors highlight more new developments compared with existing methods?\n\n- It is not clear to me the strong duality of Equation (12). Can the authors justify it? \n\n- How does the problem (18)-(19) can solve the original problem (12)? \n\n- Are there other diffusion models useful for constrained policy search? It is useful if the authors could discuss the generalization of this approach. \n\n- Training diffusion model can be inefficient. What are computational times for the methods in experiments?\n\n- The examples in experiments are created in simulated environments. Any realistic offline dataset you can use to show performance of your algorithm?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1330/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1330/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1330/Reviewer_UbgT"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1330/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698990785326,
            "cdate": 1698990785326,
            "tmdate": 1699636060138,
            "mdate": 1699636060138,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WsK1EfCe4k",
                "forum": "0aEUd9UtiA",
                "replyto": "CrIclP2Knk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1330/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1330/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to UbgT"
                    },
                    "comment": {
                        "value": "We thank reviewer UbgT for the comments and suggestions. Below, we address the concerns raised in your review point by point. Please let us know if you have any further concerns or whether this adequately addresses all the issues that you raised with the paper.\n>   **Q1:** The importance or motivation of propositions, theorems, and corollaries is not well explained. Most results can be more directly obtained using simple calculations that are known in the diffusion model.\n\n  **A1:** In fact, the logic of our theorems, proposition, and corollary is as follows: Theorem 3.1 and Corollary 3.1.1 establish conditions for Theorem 3.2 to hold. Theorem 3.2 proves the strong duality for the constrained policy search problem with policies based on the diffusion model. This allows us to use the dual problem to find the optimal solution (optimal policy) for the original problem (Theorem 3.2 in the revised paper). Proposition 3.1 is introduced for solving the dual problem, stating that the loss of DDPM can serve as an approximation to $H(\\pi_b,\\mu)$. In the revised version, we have reorganized the theorems, proposition, and collary for a clearer presentation.\n\n>  **Q2:** A large paragraph of this paper introduces known results. Can the authors highlight more new developments compared with existing methods?\n\n  **A2:** Compared to other methods using the diffusion model, we directly employ diffusion-based policy to solve constrained policy search. We demonstrate the previously unexplored strong duality for the constrained policy search problem with policies based on the diffusion model. Our research indicates that the favorable properties of the diffusion model can be leveraged for constrained policy search in offline RL, addressing the limited expressivity of Gaussian-based models.\n\n>   **Q3:** It is not clear to me the strong duality of Equation (12). Can the authors justify it?\n\n  **A3:** We have rigorously re-proven the strong duality of Equation 12 in Appendix A.2. New proof relies on a well-known result from perturbation theory connecting strong duality to the convexity of the perturbation function. Here, we formally state the lemma used in the proof, which is referred to [1] Cor. 30.2.2 and [2] Ch 3. \n\n>If (i) $r$ is bounded; (ii) Slater's condition holds for the diffusion-based constrained policy search and (iii) its perturbation function~$P(\\xi)$ is concave, then strong duality holds for equation 12.\n\nThe proof consists of two main steps (See details in Appendix A.2. in our revision): (i) demonstrating that the perturbation problem of the simplified diffusion-based constrained search problem, as per Theorem 3.1, is a concave function, and (ii) proving the Slater's condition holds for the diffusion-based constrained search problem. And we have also rephrased the main theorems of the paper to ensure a more coherent logical flow.\n\n>   **Q4:** How does the problem (18)-(19) can solve the original problem (12)? \n\n  **A4:** The dual function for Problem 12 is given by Equation 14. Following SAC[3], we can use alternating optimization to solve the dual problem. Theorem 3.2 also proves that the policy obtained through the dual problem is optimal for the original problem. In fact, Equation 18 and Equation 19 represent the objective functions for the policy and Lagrange multipliers. By applying stochastic gradient descent to Equation 18 and Equation 19, we can approximately solve the dual problem Equation 14, and consequently address Problem 12. The updated paper includes a clearer pseudocode (Ch3) for DiffCPS, highlighting this process.\n\n>   **Q5:** Are there other diffusion models useful for constrained policy search? It is useful if the authors could discuss the generalization of this approach.\n\n  **A5:** Other methods based on the diffusion model can also leverage our framework. This is because other diffusion model-based methods, such as Score-Based Generative Models (SGMs) and Stochastic Differential Equations (Score SDEs), define a common distribution for all actions. Thus, $\\mu(a\\vert s) = \\int \\mu(a^{0:T}\\vert s)da^{1:T}$ holds for other diffusion model-based methods, allowing the elimination of constraints $\\int_a \\mu(a\\vert s)da =1$  and the application of our theoretical framework."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700544958059,
                "cdate": 1700544958059,
                "tmdate": 1700544958059,
                "mdate": 1700544958059,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Mm2KLdnslB",
                "forum": "0aEUd9UtiA",
                "replyto": "WsK1EfCe4k",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1330/Reviewer_UbgT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1330/Reviewer_UbgT"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. I have a few further questions.\n\nFor Q3, does there exist a weaker condition for strong duality than Slater condition? It looks that the constraint function in Problem 12 is strictly convex. Does this help getting tighter duality?\n\nFor Q4, it is not very clear to me how the stochastic gradient updates for Equation 18 and Equation 19 can solve Problem 12. It is known that it is not true even for a linear program. Aslo, Algorithm 1 has missed the constraint sets."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1330/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690784423,
                "cdate": 1700690784423,
                "tmdate": 1700690784423,
                "mdate": 1700690784423,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]