[
    {
        "title": "Change Point Detection via Variational Time-Varying Hidden Markov Model"
    },
    {
        "review": {
            "id": "TecXWaRjv1",
            "forum": "I5MquO1g7R",
            "replyto": "I5MquO1g7R",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3599/Reviewer_bXnv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3599/Reviewer_bXnv"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles the problem of change point detection in the offline setting. While a large span of previous methods rely on Hidden Markov Models, the authors introduce TV-HMM, a variant of the Hidden Markov Model incorporating the time-varying location transition matrix. An EM-based algorithm is proposed for inference with theoretical guarantees. The TV-HMM model is shown to lead to more robust results compared to standard HMMs and is better suited when the number of change points is not known.\n\nAn extension of the TV-HMM model to a semi-parametric setting is proposed, getting rid of the usual restrictive distribution assumptions."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The authors propose a different viewpoint on the change point detection problem. As far as I know, their approach is new and allows to obtain more robust results compared to standard methods when the number of change points is not known.\n\n- The authors compare their approach with other benchmark methods and show the good performance and the robustness of their method.\n\n- The authors propose an interesting extension of their model to bypass the restrictive parametric assumption on the distribution of the observations."
                },
                "weaknesses": {
                    "value": "- The current version contains a few typographical errors and some notational issues, which make it somewhat challenging to read.\n\n- It seems that the results and details of the simulations corresponding to Section 4 (the semi-paramteric model) are not given in the paper. (In particular, I would have been curious to know how the author select the mapping $\\phi$ in their simulations.)\n\n- A more detailed comparison with other methods (particularly in terms of computer complexity) would have been useful."
                },
                "questions": {
                    "value": "I thank the authors for this nice submission. Some of my questions are already listed in the previous sections. My others questions/comments (including some typos) are presented below. \n\n- I appreciate the discussion on computational complexity at the end of Section 2.2. Nevertheless, I would have been interested to see in the paper a more detailed discussion about the computational complexity of the algorithm (in terms of time and space) compared to others approaches.\n\n- A closing parenthesis is missing at page 4 in the E-step equation. Same in the first line of Eq.(2). \n\n- In assumption A1, I think in the Gaussian distribution $S_k$ should be $\\Lambda_k$ (to be consistent with the notations used in Eq.(1)).\n\n- There is a typo in the use of the notation $M_{K+1}$. For example, in the first bullet point of Assumption A3, it should be $t_i\\in \\{1,\\dots, M_{k+1}-1\\}$. (Let me point out that $M_k$ for $k\\neq K+1$ have not been defined and I think that they should not be used anyway). The same holds for the second bullet point of assumption A3, Theorem 1, Figure 1 and Section C of the Appendix.\n\n- In Section 4, the use of the notation $\\theta_k$ is confusing. Indeed, before section 4, we work in the full parametric case and $\\theta_k=(u_k,\\Lambda_k)$ (the parameter of the Gaussian distribution from which points are independently sampled from after the $k$-th change point). However, $\\theta_k$ in Section 4 needs to be a vector in $\\mathbb R^D$ (i.e. in the same space as the observations). Therefore, it might be good to use another notation or to stress at the beginning of Section 4 the properties of $\\theta_k$.\n\n- In Eq.(4), the parameters $\\pi_{k,m,n}$ are still considered but the update from line 8 (Algo 1) is not presented anymore in Algo 2. I would be grateful if the authors could clarify this point.\n\n- Below Algorithm 2, there should not be a final point: \"Unlike Equation 2 in the parametric model, where $Q(\\theta)$ must be derived using variational inference. Here, $Q(\\theta)$ can be generally modeled using non-parametric density estimation...\". Furthermore, using \"must\" suggests that no alternative can be considered. However, I would say that standard MCMC techniques could be used instead of variational inference. If the authors agree, I would suggest to reformulate the sentence. In the second sentence, it is also mention that $Q$ can be modeled using non parametric techniques, but the following sentence (and the presented algorithm) only deal with a parametric distribution (with parameter $\\Phi$). It might be good to mention how the algorithm is changed when a non-parametric density estimation method is used. \n\n- To be consistent with the notations introduced in sections 2 and 3, should $K$ in Algorithm 2 be replaced by $\\tilde K$ (since $K$ is the true and unknown number of change points) ?\n\n- In Section 4, \"from MMD-based message passing of Equaton 4...\" : it should be \"Equation\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3599/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3599/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3599/Reviewer_bXnv"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3599/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698685662300,
            "cdate": 1698685662300,
            "tmdate": 1699636315472,
            "mdate": 1699636315472,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UbhbrRi26k",
                "forum": "I5MquO1g7R",
                "replyto": "TecXWaRjv1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3599/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3599/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you very much for your comments. \\\n\\\nQ1: \u201cThere is a typo in the use of the notation M_K+1. For example, in the first bullet point of Assumption A3, it should be $t_i \\in1 ,\\ldots, M_{k+1} -1$. (Let me point out that \n for $k \\neq K+1$ have not been defined and I think that they should not be used anyway). The same holds for the second bullet point of assumption A3, Theorem 1, Figure 1 and Section C of the Appendix. \u201d\n\nThank you very much for raising this question. In the initialization, $M_{K+1}$ is the number of regimes in the sequence, so there are $M_{K+1}-1$ initialized change points. Suppose there are $K$ true change points, $t_{M_k}$ for $k \\neq K+1$ is defined as the initialized point closest to $T_k$, where $t_{M_k}$ is specifically located on the right side of $T_k$. As stated in A3, the interval $[t_{M_k -1}, t_{ M_k }]$ covers the change point $T_k$, and points in this interval are not identically distributed. \\\nNotation: $T_k$ is the $k$-th true change point, $k \\in 1,\\ldots,K$\\\n$t_{M_k}$ is the right closest initialized point of $T_k$, $k \\in 1,\\ldots,K$\\\n$t_{M_{k-1}}$ is the right closest initialized point of $T_{k-1}$\\\n$t_{M_k-1}$ is the adjacent initialized point of $t_{M_k}$ on the left.\\\n\\\nQ2: \u201cIn Eq.(4), the parameters \\pi are still considered but the update from line 8 (Algo 1) is not presented anymore in Algo 2. I would be grateful if the authors could clarify this point. \u201d\\\n\\\nIndeed, the parameters $\\pi$ still need to be considered in Algorithm 2, and we have added one more step for $\\pi$ in the Algorithm.\\\n\\\nQ3: \u201cFurthermore, using \"must\" suggests that no alternative can be considered. However, I would say that standard MCMC techniques could be used instead of variational inference. If the authors agree, I would suggest to reformulate the sentence. In the second sentence, it is also mention that \\Q can be modelled using non parametric techniques, but the following sentence (and the presented algorithm) only deal with a parametric distribution (with parameter ). It might be good to mention how the algorithm is changed when a non-parametric density estimation method is used.\u201d\\\n\\\nThank you very much for your suggestion. Here we use \"must\" because $Q(\\theta)$ with explicit form in Algorithm 1 is developed under the framework of Variational Inference. Although we can use MCMC to perform inference on parametric TV-HMM, the computational cost is higher. The main difference between the non-parametric and parametric TV-HMM is that we substitute the parametric likelihood measurement with the Maximum Mean Discrepancy(MMD). We first sample $\\tilde K+1$  sequences from non-parametrically estimated $Q(\\zeta)$. Each $Q(\\zeta_k)$ represents a distinguished regime distribution of the sequence. Calculating the MMD between the sampled sequences and the original sequence, we can find the best-matching segment to determine the change point location. Although the parametric model assumption can help derive analytic solutions and enhance the convergence rate, the model may become compromised if the assumption is violated. e.g., the DPHMM method is not well performed on a sequence with piecewise linear segments.\\\n\\\nQ4: \u201cThe current version contains a few typographical errors and some notational issues, which make it somewhat challenging to read.\u201d\\\n\\\nThank you very much for the correction. We\u2019re sorry about the typographical errors and the notational issues in the paper and have already corrected them. We have changed all $S_k$ into $\\Lambda_k$ and replaced $K$ with $ \\tilde K$ in Algorithm 2, for consistent usage of symbols. The $\\theta_k$ in Section 4 is substituted with $\\zeta_k$ to avoid confusion."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3599/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669035267,
                "cdate": 1700669035267,
                "tmdate": 1700718489307,
                "mdate": 1700718489307,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1seYRhzPck",
            "forum": "I5MquO1g7R",
            "replyto": "I5MquO1g7R",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3599/Reviewer_98Hf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3599/Reviewer_98Hf"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a change point detection approach based on a time-varying Hidden Markov Model. The paper introduces a Hidden Markov Model with a time-varying location transition matrix and a corresponding inference method for this model based on variational expectation maximization (EM). The aim of the proposed method is the ability to deal with an undefined number of change points and, therefore, robustness against a mis specified number of change points. Furthermore, stochastic approximation allows to ease computation burden. Finally, the proposed approach operates within the common piece-wise i.i.d. setting and does not necessarily assume Gaussian likelihood."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The benefits of the approach are in its flexibility, in particular, the ability to learn the number of change points and flexibility in terms of likelihood specification."
                },
                "weaknesses": {
                    "value": "Overall, the paper still needs improvement in clarity and more convincing experimental setup, which clearly shows in which situations the proposed approach would be preferred over existing methods and how it compares to other approaches in terms of computational speed."
                },
                "questions": {
                    "value": "Detailed comments:\n-\tAt times, writing is not always clear. For example, it is unclear what is the role of section 4 in the paper as there is no experimental evaluation of this extension. There are quite a few typos. \n\n-\tWith the current details, I would struggle to implement the method and reproduce the results of the paper. I would suggest writing an additional section with implementation details, which ensures reproducibility. \n\n-\tThe experimental setup is limited. In particular, the simulation study includes equally spaced change points, which is quite simplistic. \n\n-\tThere is no computational comparison between the methods, and therefore, it is unclear how much one has to sacrifice in terms of speed to gain a little extra performance. \n\n-\tThere are somewhat marginal differences in the performance between the proposed approach and other methods (both when the proposed approach underperforms and overperforms). From the current evaluation, it is unclear in what scenarios it would be beneficial to use the proposed method. \n\n-\tTable 1 lacks standard deviations over the 100 runs. \n\n-\tSome references do not include journal information or arxiv identifier. \n\n- Figures are sometimes missing labels and captions of the figures/tables are not self-contained."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3599/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698688995372,
            "cdate": 1698688995372,
            "tmdate": 1699636315384,
            "mdate": 1699636315384,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pjHt5lNMH3",
                "forum": "I5MquO1g7R",
                "replyto": "1seYRhzPck",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3599/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3599/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you very much for your comments.\\\n\\\nQ1: \u201cWith the current details, I would struggle to implement the method and reproduce the results of the paper. I would suggest writing an additional section with implementation details, which ensures reproducibility.\u201d\\\n\\\nThank you very much for your advice. We have summarized all the experiment settings in Appendix D. The input parameters such as the initialized number of change points $\\tilde K$, the sample sequence $Y$, and how it is generated are all included. We also add an additional experiment about the piecewise linear sequence. We are delighted to release the code after entering the final revision. Thank you again for your understanding and support.\\\n\\\nQ2: \u201cThere is no computational comparison between the methods, and therefore, it is unclear how much one has to sacrifice in terms of speed to gain a little extra performance.\u201d\\\n\\\nThe novelty is about extending the Hidden Markov Model. Since our proposed method is based on Variational Inference, the computational speed is indeed faster than previous DPHMM using MCMC sampling. The computational complexity for our proposed method is $\\mathcal{O}(KN^2)$. If equipped with stochastic approximation, the complexity can be reduced to $\\mathcal{O}(KS^2)$, and $S$ is the sub-sample size. The computational speed we can achieve for each step during iteration is approximately 1.73 seconds. \\\n\\\nQ3: \u201cTable 1 lacks standard deviations over the 100 runs. Some references do not include journal information or arxiv identifier. Figures are sometimes missing labels and captions of the figures/tables are not self-contained.\u201d\\\n\\\nThank you very much for pointing out these issues. We have added the standard deviations over 100 runs and included the required information. We also check the reference section to ensure each reference includes journal information or arxiv identifier. Missing labels in the figures are fixed and captions are adjusted.\n|            |     Model 1     |     Model 2     |      Model 3     |\n|:----------:|:---------------:|:---------------:|:----------------:|\n|   WBSLSW   | 0.9068(0.08744) | 0.3596(0.22447) |  0.3849(0.23038) |\n|    ECP30   | 0.9156(0.04346) | 0.9580(0.05754) |  0.9737(0.01215) |\n| DPHMM      | 0.9637(0.02257) | 0.8727(0.03066) | 0.8869(0.02651)  |\n| KCP        | 0.9501(0.05622) | 0.8436(0.01631) | 0.8836(0.04664)  |\n| $D_m$-BOCD | 0.8123(0.11902)  | 0.8411(0.01894) | 0.8413(0.02208) |\n| TV-HMM     | 0.9523(0.05174) | 0.9756(0.01339) | 0.9615(0.02004)  |\n\n\\\n\\\nExperiment:\\\n\\\n\\\nThank you very much for your suggestion. We also test the semi-parametric TV-HMM on a sequence with piecewise linear segments to show the improvement. In this experiment, the data are generated from the model $\\beta_0+\\beta_1*t+\\epsilon$, $\\epsilon$ follows a normal distribution with mean 0 and variance 1. The length of the sequence is 600 with 4 inequal distanced change points. The change point locations are $\\tau=[ 80 , 230, 330, 480]$ indicating lengths of each segments are $[ 80,150,100,150,120]$. $\\tilde{K}$ is 10. The $\\beta_0$ and $\\beta_1$ are shown in the following table. \n\nModel: $y(t)=\\beta_0+\\beta_1*t+\\epsilon$,  $\\epsilon \\sim N(0,1)$\\\nTotal sample size: $N=600$\\\nChange point location: $\\tau=[ 80 , 230, 330, 480]$\\\nInitialized number of change point: $\\tilde{K}=10$\n\n|           | Segment 1 | Segment 2 |  Segment 3 |  Segment 4 | Segment 5 |\n|----------:|:---------:|:---------:|:----------:|:----------:|:----------:|\n| $\\beta_0$ |     0     | $seg_1(80)$ |$ seg_2(150)$ | $seg_3(100)$ | $seg_4(150)$ |\n| $\\beta_1$ |    0.1    |    -0.3   |     0.3    |    -0.2    |    -0.2    |\n\n $seg_i(j)$ stands for the $j$-th element in $i$-th segment.\\\n\\\nThe summarized average Rand Index and its standard deviation are shown in the following table. It can be observed that Semi-parametric TV-HMM outperforms the other compared methods, indicating that our proposed method can handle the change point detection problem in sequences with linear trends. Moreover, the standard deviation of the Randx Index measurement is 0.02512, demonstrating the stability of our method.\n|            |   semi-TVHMM   |      DPHMM     |     ECP3O    |      WBSLSW      |       KCP      |   $D_m$-BOCD   |\n|-----------:|:--------------:|:--------------:|----------------|:--------------:|:--------------:|:--------------:|\n| Rand Index |      0.9533    |      0.7882    |      0.8580    |      0.2209    |      0.8199    |      0.7637    |\n|     SD     |     0.02512    |     0.06822    |     0.00422    |     0.05569    |     0.00197    |     0.09845    |"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3599/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664878447,
                "cdate": 1700664878447,
                "tmdate": 1700664878447,
                "mdate": 1700664878447,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4P5eZ8ZV99",
            "forum": "I5MquO1g7R",
            "replyto": "I5MquO1g7R",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3599/Reviewer_MqQg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3599/Reviewer_MqQg"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the change point detection problem with a time-varying hidden Markov model. The authors develop a variational inference algorithm for parameter estimation as well as a semi-parametric extension using the Maximum Mean Discrepancy (MMD)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The model formulation is natural for the change point detection problem.\n2. The authors generalize the log-likelihood based estimation with MMD to achieve improved robustness against model misspecification and outliers.\n3. Theoretical guarantees are provided for the consistency of parameter estimation."
                },
                "weaknesses": {
                    "value": "1. Placing the ARD prior on the elements of the transition matrix \\Pi does not seem to be correct. How does this guarantee that the elements are nonnegative and add up to one? Typically, a Dirichlet prior is used which also induces sparsity. How does the proposed method perform compared to a Dirichlet prior?\n2. Similarly the number of change points is determined by examining the posterior of the transition matrix. This also suffers from the issue above.\n3. Is the left-to-right Markov chain assumption necessary? There could be identical regimes and the HMM can switch to a previously observed regime. \n4. The authors adopted a SGD-type update for the posterior, e.g., line 8 of Algorithm 1. The update does not respect the sum to one constraint for the transition matrix."
                },
                "questions": {
                    "value": "- How is the transition matrix posterior updated with the ARD prior? Specifically, why line 8. of Algorithm 1 ensures that the updated estimates are in a simplex?\n- How does the proposed method perform compared to using Dirichlet priors on \\Pi?\nMinor:\n\\tau is used to denote change points (Sec 2) and also the step size in Algorithm 1."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3599/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698780664077,
            "cdate": 1698780664077,
            "tmdate": 1699636315308,
            "mdate": 1699636315308,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "q0Zcava9lF",
                "forum": "I5MquO1g7R",
                "replyto": "4P5eZ8ZV99",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3599/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3599/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you very much for your comments.\\\n\\\nQ1: \u201cHow is the transition matrix posterior updated with the ARD prior? Specifically, why line 8. of Algorithm 1 ensures that the updated estimates are in a simplex?\u201d \\\n\\\nThe prior is initialized using an upper triangular matrix. In each row, entries are assigned with equal probability summing up to 1. Based on the concept of ARD prior, we investigate that the optimal $\\Pi$ after differentiating the ELBO with respect to $\\Pi$ is  the transition probability of the previous step $Q^S(t_k(n)=1|t_{k-1}(m)=1)$, which ensures the update estimates are in a simplex\\\n\\\nQ2: \u201cHow does the proposed method perform compared to using Dirichlet priors on \\Pi? Minor: \\tau is used to denote change points (Sec 2) and also the step size in Algorithm 1.\u201d\\\n\\\nThank you very much for pointing out the symbol issue. We have changed the symbol of the step size of $\\tau$ into $\\eta$. As mentioned before, each row of the initialized prior is uniformly distributed, with equal probabilities assigned to each element within the row. Our proposed algorithm can update the $\\Pi_k$ iteratively based on the data. The specific entry $\\pi_{k,m,n}$ of the ARD prior $\\Pi_k$ measures the importance of the $k$-th change point transitioning from position $m$ to position $n$, which can be inferred from the data. The central concept of ARD prior is that the hyperparameters (in our cases, $\\Pi_k$) are typically optimized by maximizing the marginal likelihood of the data.  Applying Dirichlet priors to $\\Pi_k$ can yield similar results to iteratively updating $\\Pi_k$. However, this approach might diminish the interpretability of  $\\Pi_k$ , as $\\pi_{k,m,n}$ becomes a random variable and can no longer be interpreted as a measure of importance or relevance. Moreover, when $\\pi_{k,m,n}$ is treated as a random variable, it becomes less straightforward to visualize the sparsity of $\\Pi_k$ directly in the plot."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3599/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700664562019,
                "cdate": 1700664562019,
                "tmdate": 1700722591693,
                "mdate": 1700722591693,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jlptdXQEKo",
            "forum": "I5MquO1g7R",
            "replyto": "I5MquO1g7R",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3599/Reviewer_hP3N"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3599/Reviewer_hP3N"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses modeling time series data with sudden regime shifts, noting the limitations of the widely-used Hidden Markov Model (HMM). Introducing the TV-HMM, a variation with a time-varying location transition matrix, the authors offer a novel variational EM algorithm that pinpoints change point locations and quantities. This method remains robust against misidentification of change point numbers and has optimized computational efficiency. Statistical consistency under the Gaussian likelihood is assured, and a semi-parametric TV-HMM, free from distribution constraints, is also proposed."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed variational EM algorithm is designed to be resilient against misidentification of change point numbers, and through the integration of stochastic approximation techniques, the paper addresses the computational intensity traditionally associated with HMMs.\n\n2. The paper not only ensures the statistical consistency of change point location estimation under the Gaussian likelihood but also broadens its application by introducing a semi-parametric TV-HMM, which operates without stringent distribution assumptions, enhancing its adaptability to diverse data sets."
                },
                "weaknesses": {
                    "value": "1. While the paper does propose a semi-parametric model free from stringent distribution assumptions, a significant portion of the study, including the assurance of statistical consistency, is still based on the Gaussian likelihood assumption, which may not always be applicable in real-world scenarios.\n\n2. No improvement with respect to competitors."
                },
                "questions": {
                    "value": "1. A clear discussion about the assumptions would be helpful. \n\n2. Is assumption 2 strong with respect to literature? \n\n3. The simulation study does not show an improvement. I think it would be helpful to see a more substantial improvement."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3599/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3599/Reviewer_hP3N",
                        "ICLR.cc/2024/Conference/Submission3599/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3599/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698806997883,
            "cdate": 1698806997883,
            "tmdate": 1700707279798,
            "mdate": 1700707279798,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6PHPz3Fvp7",
                "forum": "I5MquO1g7R",
                "replyto": "jlptdXQEKo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3599/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3599/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer,\n\nThank you very much for your comments. \\\n\\\nQ1&2: \"A clear discussion about the assumptions would be helpful. Is assumption 2 strong with respect to literature? \"\\\n\\\nThank you very much for your advice. The assumption A1 describes the order of each change point, and the last change point is fixed at $T$ where the first change point $T_1$ is after 0. We also assume the data $y$ in different segments are drawn from a normal distribution with different parameters depending on the regimes. For assumption A2, we assume the number of observations within a specific interval is of polynomial order in the total number of samples, N. Lastly, assumption A3 is about the characteristics of initialized change points. Equally distanced initial change points can be separated into junction and non-junction points. The junction point $t_i$ means there is a true change point within the interval $[t_{i-1},t_i]$. Junction and non-junction points are also demonstrated in Figure 1. \n\nWhile the number of observations within an interval is assumed to be polynomially related to the total sample N, this assumption is considered acceptable for developing the algorithms. Let $\\alpha = (n-m)/T$, clearly $\\alpha$ is less than or equal to 1, which means the rate is slower than N as N approaches infinity. For example, Chakar et al. (2017) prove their change point consistency as N approaches infinity. Thus, our assumption A2 is mild with respect to literature.\n\n\nReference\n\nS. Chakar. E. Lebarbier. C. L\u00e9vy-Leduc. S. Robin. \"A robust approach for estimating change-points in the mean of an AR(1) process.\" Bernoulli 23 (2) 1408 - 1447, May 2017\\\n\\\n\\\n\\\nExperiment:\\\n\\\n\\\nThank you very much for your suggestion. We also test the semi-parametric TV-HMM on a sequence with piecewise linear segments to show the improvement. In this experiment, the data are generated from the model $\\beta_0+\\beta_1*t+\\epsilon$, $\\epsilon$ follows a normal distribution with mean 0 and variance 1. The length of the sequence is 600 with 4 inequal distanced change points. The change point locations are $\\tau=[ 80 , 230, 330, 480]$ indicating lengths of each segments are $[ 80,150,100,150,120]$. $\\tilde{K}$ is 10. The $\\beta_0$ and $\\beta_1$ are shown in the following table. \n\nModel: $y(t)=\\beta_0+\\beta_1*t+\\epsilon$,  $\\epsilon \\sim N(0,1)$\\\nTotal sample size: $N=600$\\\nChange point location: $\\tau=[ 80 , 230, 330, 480]$\\\nInitialized number of change point: $\\tilde{K}=10$\n\n|           | Segment 1 | Segment 2 |  Segment 3 |  Segment 4 | Segment 5 |\n|----------:|:---------:|:---------:|:----------:|:----------:|:----------:|\n| $\\beta_0$ |     0     | $seg_1(80)$ |$ seg_2(150)$ | $seg_3(100)$ | $seg_4(150)$ |\n| $\\beta_1$ |    0.1    |    -0.3   |     0.3    |    -0.2    |    -0.2    |\n\n $seg_i(j)$ stands for the $j$-th element in $i$-th segment.\\\n\\\nThe summarized average Rand Index and its standard deviation are shown in the following table. It can be observed that Semi-parametric TV-HMM outperforms the other compared methods, indicating that our proposed method can handle the change point detection problem in sequences with linear trends. Moreover, the standard deviation of the Randx Index measurement is 0.02512, demonstrating the stability of our method.\n|            |   semi-TVHMM   |      DPHMM     |     ECP3O    |      WBSLSW      |       KCP      |   $D_m$-BOCD   |\n|-----------:|:--------------:|:--------------:|----------------|:--------------:|:--------------:|:--------------:|\n| Rand Index |      0.9533    |      0.7882    |      0.8580    |      0.2209    |      0.8199    |      0.7637    |\n|     SD     |     0.02512    |     0.06822    |     0.00422    |     0.05569    |     0.00197    |     0.09845    |"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3599/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663334651,
                "cdate": 1700663334651,
                "tmdate": 1700663334651,
                "mdate": 1700663334651,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Zg9uqlB2EM",
                "forum": "I5MquO1g7R",
                "replyto": "6PHPz3Fvp7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3599/Reviewer_hP3N"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3599/Reviewer_hP3N"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the authors' detailed response and their efforts to address the points raised in my initial review. Consequently, I have decided to change my rating accordingly to \"marginally below the acceptance threshold\"."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3599/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692414540,
                "cdate": 1700692414540,
                "tmdate": 1700692414540,
                "mdate": 1700692414540,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]