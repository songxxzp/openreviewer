[
    {
        "title": "Diffusion with Synthetic Features: Feature Imputation for Graphs with Partially Observed Features"
    },
    {
        "review": {
            "id": "HAKo8xqbve",
            "forum": "M11LONBkx1",
            "replyto": "M11LONBkx1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9301/Reviewer_8JVT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9301/Reviewer_8JVT"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the task of learning on graphs with missing features, specifically focusing on improving the application of graph neural networks (GNNs) to real-world graph-structured data. The paper introduces a novel diffusion-based imputation scheme called Feature Imputation with Synthetic Features (FISF).\n\n(1) It generates synthetic features via pre-diffusion for randomly chosen nodes in these channels.\n\n(2) The diffusion process spreads these synthetic features while also considering observed features simultaneously.\n\n(3) The proposed scheme has been empirically tested, showing promising results, especially in scenarios with a high rate of missing \nfeatures. It shows robust performance in both semi-supervised node classification and link prediction tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper addresses a significant and practical problem in the domain of graph learning.\n\n2. The proposed FISF method is novel, focusing on low-variance channels that were overlooked by previous approaches.\n\n3. The paper seems to provide extensive experiments on graphs with varying rates of missing features, demonstrating the robustness of the proposed method."
                },
                "weaknesses": {
                    "value": "1. From the provided content, it's unclear how the proposed method scales with large real-world graphs or its computational efficiency.\n\n2. The abstract and introduction don't mention how generalizable the method is across diverse types of graph-structured data or different"
                },
                "questions": {
                    "value": "See weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9301/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698515713447,
            "cdate": 1698515713447,
            "tmdate": 1699637171011,
            "mdate": 1699637171011,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qgD2AVVkNu",
                "forum": "M11LONBkx1",
                "replyto": "HAKo8xqbve",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9301/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 8JVT"
                    },
                    "comment": {
                        "value": "We appreciate your encouragement and hope that the following answers can make you more confident.\n\n> $\\textbf{Q1.}$ From the provided content, it's unclear how the proposed method scales with large real-world graphs or its computational efficiency.\n\nFISF operates in structural-missing settings with a time complexity of $O(|\\mathcal{E}|+(1+\\gamma F)N^2)$ and in uniform-missing settings with a complexity of $O(|\\mathcal{E}|+(1+\\gamma) F N^2)$. During the rebuttal period, to address the increasing time complexity in uniform-missing settings, we have sought a way to replace channel-wise inter-node diffusion with FP in pre-diffusion as a light version of FISF, called FastFISF. Consequently, the time complexity of FastFISF decreases to $O(|\\mathcal{E}|+\\gamma FN^2)$ regardless of the missing way.\n\nThe table below displays the training time of methods. FP exhibits the shortest training time among the methods. However, FISF notably enhances performance in downstream tasks compared to FP. For instance, in structural-missing setups with $r_m=0.995$, FISF achieves significant gains in node classification accuracy over FP, showing improvements of $7.43$% and $4.94$% on Cora and PubMed, respectively. Additionally, FastFISF demonstrates substantial reductions in training time under uniform-missing settings. A detailed explanation of FastFISF including performance in downstream tasks is in Appendix C.5.\n\nOGBN-Arxiv with ~0.2M nodes is the largest real-world graph dataset in this paper. On OGBN-Arxiv, we show the effectiveness of FISF on semi-supervised node classification tasks and FISF requires only 16.1 seconds for imputation. We agree with the reviewer that the computational efficiency of FISF was unclear. Hence, we added complexity analysis in Appendix C.6.\n\n<Table A: Training time of methods. OOM denotes an out-of-memory error.>\n\n| Missing way | Structural | missing   | Uniform | missing |\n|-------------|:----------:|:---------:|:-------:|:-------:|\n| Method      |    Cora    |   PubMed  |   Cora  |  PubMed |\n| GCNMF       |   $10.3$s  |  $19.4$s  | $9.87$s | $28.3$s |\n| GRAFENNE    |   $47.9$s  |  $74.7$s  | $51.1$s | $74.0$s |\n| MEGAE       |   $1753$s  |    OOM    | $1801$s |   OOM   |\n| FP          |   $2.36$s  |  $3.12$s  | $2.25$s | $2.90$s |\n| PCFI        |   $2.45$s  |  $3.23$s  | $11.1$s | $34.1$s |\n| FastFISF    |   $13.4$s  |  $34.6$s  | $11.8$s | $42.5$s |\n| FISF        |   $13.4$s  |  $34.8$s  | $17.6$s | $78.2$s |\n\n\n> $\\textbf{Q2.}$ The abstract and introduction don't mention how generalizable the method is across diverse types of graph-structured data or different.\n\nFISF has good generalizability across diverse types of graph-structured data, extending its applicability to both  hypergraphs and heterogeneous graphs. In case of hypergraphs, a hypergraph can be transformed into a homogeneous graph via clique expansion. Therefore, missing features of nodes in a hypergraph can be also imputed by FISF. In case of heterogeneous graphs, our FISF can be applied to heterogeneous graphs throughout  meta-paths. However, since this application is not straightforward, we mention these extensions as future work in Conclusion."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700595596627,
                "cdate": 1700595596627,
                "tmdate": 1700595596627,
                "mdate": 1700595596627,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pTYCKKsChn",
            "forum": "M11LONBkx1",
            "replyto": "M11LONBkx1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9301/Reviewer_VoZJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9301/Reviewer_VoZJ"
            ],
            "content": {
                "summary": {
                    "value": "This paper claims that existing methods output low variance channels within imputed features and then presents a method called FISF, which performs diffusion with randomly injected synthetic features on low variance channels discovered from pre-diffusion process."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper empirically shows that existing diffusion-based methods causes many low variance channels within imputed features, while proposed method FISF can solve those problem (Figure 1).\n- The paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "-\tThis paper does not clearly justify why alleviating the low variance channel problem of the existing diffusion methods is significant for graph learning tasks, even though it is a key motivation of this paper. Without additional evidences, it is hard to agree with this paper\u2019s argument that low variance channels contribute very little to performance.\n-\tThis paper lacks justification of using na\u00efve random synthetic features. Authors claim that more distinctive representations are crucial for classification tasks, but I have concern that randomly injected synthetic features can lead to lower intra-class node representation similarity thus can be harmful on downstream tasks. In my opinion, discussion on aforementioned concern is required.\n-\tIt is required to include the time complexity of presented method. Since V_{k}^{(a)} differs by channel \u2018a\u2019 as synthetic feature-injected node differs by channels, diffusion process should be done by channels. Moreover, as presented method further requires pre-diffusion process, presented FISF seems to have much higher time complexity compared to existing methods, especially FP.\n-\tAs presented in the Table 5 in the Appendix, hyperparameters have been tuned with respect to each feature missing rate r_m. Regarding complexity of setting optimal hyperparameters, there is some concern about practical usage of FISF. \n-\tLastly, considering the similarity of Label Propagation (LP) and FP, and the argument about low variance channels (in respect of very high missing feature rates), I think this paper shares similar motivation with Poisson Learning [1], which mitigates very low label rate problem in LP. Poisson Learning points out that LP outputs almost constant pseudo-labels for most of unlabeled samples, while this paper points out existing diffusion-based methods including FP causes \u201clow variance channels\u201d, i.e. almost constant features (or an exact constant when there is only one observed feature value, as presented in this paper). Regarding aforementioned similarity, further discussion with Poisson Learning might be beneficial."
                },
                "questions": {
                    "value": "-\tCan you provide the feature variance distribution (Figure 1) in more realistic missing feature settings (I.e. r_m <= 0.9)? In my opinion, authors have to show that existing diffusion-based methods still suffer from the problem of making low variance channels in various missing feature settings, especially in more realistic scenarios.\n-\tCan you provide further theoretical or empirical evidences that can explain why random synthetic feature works well? (to resolve concern in W2)\n-\tWhat happens if pre-diffusion method is replaced to FP from PCFI? (Why this paper have chosen PCFI as a pre-diffusion method?)\n\n[1] Jeff Calder, Brendan Cook, Matthew Thorpe and Dejan Slepcev. Poisson Learning: Graph Based Semi-Supervised Learning At Very Low Label Rates. In International Conference on Machine Learning, pp. 1306-1316. PMLR, 2020."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9301/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9301/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9301/Reviewer_VoZJ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9301/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698665101437,
            "cdate": 1698665101437,
            "tmdate": 1700662232231,
            "mdate": 1700662232231,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iT3RCBcb4f",
                "forum": "M11LONBkx1",
                "replyto": "pTYCKKsChn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9301/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VoZJ [Part 1/5]"
                    },
                    "comment": {
                        "value": "Thank you for the time and valuable feedback. We provide the answers below.\n\n> $\\textbf{Q1.}$ This paper does not clearly justify why alleviating the low variance channel problem of the existing diffusion methods is significant for graph learning tasks, even though it is a key motivation of this paper. Without additional evidences, it is hard to agree with this paper\u2019s argument that low variance channels contribute very little to performance.\n\n$\\textbf{A1.}$ Since low-variance channels do not have any distinctive features, they contribute very little to performance in downstream tasks. To justify addressing the low-variance channel problem, we carry out extra experiments that provide evidence that low-variance channels have little contribution to downstream tasks. We compare performance by removing partial channels from an initial feature matrix using two distinct ways. One method involves excluding channels in descending order of variance, beginning with the highest and based on a fixed percentage. The second way is excluding channels in ascending order of variance. As demonstrated in Figure 5 and Figure 6 in Appendix C.3, performance remains relatively steady even as we remove an increasing proportion of low-variance channels. However, removing channels starting from the highest variance results in notable performance drops, even with a small proportion of channel removal.\n\n> $\\textbf{Q2.}$ This paper lacks justification of using na\u00efve random synthetic features. Authors claim that more distinctive representations are crucial for classification tasks, but I have concern that randomly injected synthetic features can lead to lower intra-class node representation similarity thus can be harmful on downstream tasks. In my opinion, discussion on the aforementioned concern is required. Can you provide further theoretical or empirical evidences that can explain why random synthetic feature works well?\n\n$\\textbf{A2.}$ To provide evidence regarding smoothness and intra-class node feature similarity, we conduct additional experiments. First, we measure Dirichlet energy of imputed features to compare graph-level smoothness. The table below shows that FISF gives the highest Dirichlet energy (distinctiveness) among the imputation methods.\n\n<Table A: log($E_D$) of imputed features under a structural-missing setting with $r_m=0.995$, where $E_D$ is the Dirichlet energy. Original denotes original given features.>\n\n|     Missing way     |        | Structural |         |        |  Uniform |         |\n|:-------------------:|:------:|:----------:|:-------:|:------:|:--------:|:-------:|\n| Method $\\downarrow$ |  Cora  |  CiteSeer  |  PubMed |  Cora  | CiteSeer |  PubMed |\n| Original            | $4.36$ |   $4.49$   |  $3.11$ | $4.36$ |  $4.49$  |  $3.11$ |\n| FP                  | $1.90$ |   $1.94$   | $0.798$ | $1.89$ |  $1.91$  | $0.805$ |\n| PCFI                | $3.14$ |   $2.59$   |  $1.49$ | $2.52$ |  $2.64$  |  $1.43$ |\n| FISF (Ours)         | $3.25$ |   $2.92$   |  $4.15$ | $2.69$ |  $2.70$  |  $4.34$ |\n\nIn addition, we conduct further experiments to investigate smoothness within classes. The table below demonstrates the intra-class cosine similarity calculated from imputed features by FISF. Ratio denotes average intra-class similarity/inter-class similarity. If Ratio is greater than 1, inter-class similarity becomes less than the average intra-class similarity, which means the feature is distinctive enough for classification of node features.  We include t-SNE plots that visualize imputed features and deep features in Appendix C.4. We verify that FISF provides clearer cluster structures for both imputed features and deep features than the other imputation methods. Since the imputed features still give distinctiveness and improve the classification performance, we can say that the randomly injected synthetic features are not  harmful on downstream tasks. \n\n<Table B: Average cosine similarity of imputed features by FISF, under a structural-missing setting with $r_m=0.995$.>\n\n| Dataset  | Inter-class | class 1 | class 2 | class 3 | class 4 | class 5 | class 6 | class 7 | Average intra-class |  Ratio |\n|----------|:-----------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------------------:|:------:|\n| Cora     |   $0.760$   | $0.858$ | $0.902$ | $0.902$ | $0.844$ | $0.691$ | $0.826$ | $0.870$ |       $0.842$       | $1.11$ |\n| CiteSeer |   $0.279$   | $0.267$ | $0.341$ | $0.636$ | $0.282$ | $0.513$ | $0.380$ |    -    |       $0.403$       | $1.45$ |\n| PubMed   |   $0.871$   | $0.893$ | $0.936$ | $0.880$ |    -    |    -    |    -    |    -    |       $0.903$       | $1.04$ |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593674271,
                "cdate": 1700593674271,
                "tmdate": 1700593713163,
                "mdate": 1700593713163,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FmN07sQ1B2",
                "forum": "M11LONBkx1",
                "replyto": "pTYCKKsChn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9301/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VoZJ [Part 2/5]"
                    },
                    "comment": {
                        "value": "> $\\textbf{Q3.}$ It is required to include the time complexity of presented method. Since V_{k}^{(a)} differs by channel \u2018a\u2019 as synthetic feature-injected node differs by channels, diffusion process should be done by channels. Moreover, as presented method further requires pre-diffusion process, presented FISF seems to have much higher time complexity compared to existing methods, especially FP.\n\n$\\textbf{A3.}$ FISF operates in structural-missing settings with a time complexity of $O(|\\mathcal{E}|+(1+\\gamma F)N^2)$ and in uniform-missing settings with a complexity of $O(|\\mathcal{E}|+(1+\\gamma) F N^2)$. During the rebuttal period, to address the increasing time complexity in uniform-missing settings, we have sought a way to replace channel-wise inter-node diffusion with FP in pre-diffusion as a light version of FISF, called FastFISF. Consequently, the time complexity of FastFISF decreases to $O(|\\mathcal{E}|+\\gamma FN^2)$ regardless of the missing way.\n\nThe table below displays the training time of methods. FP exhibits the shortest training time among the methods. However, FISF notably enhances performance in downstream tasks compared to FP. For instance, in structural-missing setups with $r_m=0.995$, FISF achieves significant gains in node classification accuracy over FP, showing improvements of $7.43$% and $4.94$% on Cora and PubMed, respectively. Additionally, FastFISF demonstrates substantial reductions in training time under uniform-missing settings. A detailed explanation of FastFISF including performance in downstream tasks is in Appendix C.5.\n\n<Table C: Training time of methods. OOM denotes an out-of-memory error.>\n\n| Missing way | Structural | missing   | Uniform | missing |\n|-------------|:----------:|:---------:|:-------:|:-------:|\n| Method      |    Cora    |   PubMed  |   Cora  |  PubMed |\n| GCNMF       |   $10.3$s  |  $19.4$s  | $9.87$s | $28.3$s |\n| GRAFENNE    |   $47.9$s  |  $74.7$s  | $51.1$s | $74.0$s |\n| MEGAE       |   $1753$s  |    OOM    | $1801$s |   OOM   |\n| FP          |   $2.36$s  |  $3.12$s  | $2.25$s | $2.90$s |\n| PCFI        |   $2.45$s  |  $3.23$s  | $11.1$s | $34.1$s |\n| FastFISF    |   $13.4$s  |  $34.6$s  | $11.8$s | $42.5$s |\n| FISF        |   $13.4$s  |  $34.8$s  | $17.6$s | $78.2$s |"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700593879948,
                "cdate": 1700593879948,
                "tmdate": 1700594270796,
                "mdate": 1700594270796,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "01YFqEBeUH",
                "forum": "M11LONBkx1",
                "replyto": "pTYCKKsChn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9301/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VoZJ [Part 3/5]"
                    },
                    "comment": {
                        "value": "> $\\textbf{Q4.}$ As presented in the Table 5 in the Appendix, hyperparameters have been tuned with respect to each feature missing rate r_m. Regarding complexity of setting optimal hyperparameters, there is some concern about practical usage of FISF.\n\n$\\textbf{A4.}$ Despite outperforming performance of FISF, conducting a hyperparameter search for FISF with three hyperparameters ($\\alpha$, $\\beta$, and $\\gamma$) can be burdensome in certain situations. However, both $\\alpha$ and $\\beta$ ($0<\\alpha,\\beta<1$) play a similar role in a base of distance during calculating PC (\\textit{i.e.} $\\xi^*_{i,b}=\\alpha^{\\mathbf{S}^*_{i,b}}$ and $\\xi^s_{i,a}=\\beta^{\\mathbf{S}^s_{i,a}}$). Thus we can combine them into one, i.e., $\\alpha = \\beta$. By doing this, the search complexity can be reduced from $5^3$ to $5^2$ without the performance degradation by setting five search points for each hyperparameter. The tables below show that the FISF* with the light search does not degrade performance on semi-supervised node classification and link prediction. The version with the light search requires from 20 minutes to 10 hours depending on the datasets, therefore this burden is manageable for practical usage of FISF.\n\n<Table D: Performance on semi-supervised node classification tasks under structural-missing settings with $r_m=0.995$, measured in accuracy (%)>\n\n| Method |      Cora      | CiteSeer | PubMed | Photo | Computers | OGBN-Arxiv | Average |\n|--------|:--------------:|:--------:|:------:|:-----:|:---------:|:----------:|:-------:|\n| FISF   | $79.29\\pm1.72$ |     $69.68\\pm2.47$     |    $76.90\\pm1.50$    |    $88.22\\pm0.79$   |      $79.40\\pm1.11$     |       $69.92\\pm0.17$     |    $77.24$     |\n| FISF*  |         $78.68\\pm1.72$        |     $69.68\\pm2.47$     | $76.74\\pm1.84$       |   $88.22\\pm0.79$   |      $79.40\\pm1.11$     |    $69.92\\pm0.17$        |    $77.11$     |\n\n<Table E: Performance on semi-supervised node classification tasks under uniform-missing settings with $r_m=0.995$, measured in accuracy (%)>\n\n| Method |      Cora      | CiteSeer | PubMed | Photo | Computers | OGBN-Arxiv | Average |\n|--------|:--------------:|:--------:|:------:|:-----:|:---------:|:----------:|:-------:|\n| FISF   | $79.09\\pm1.73$ |     $69.52\\pm1.81$      |     $77.53\\pm1.28$   |    $88.32\\pm1.37$    |     $82.12\\pm0.51$                  |     $69.81\\pm0.16$    | $77.73$  |\n| FISF*  |       $79.09\\pm1.73$         |      $69.52\\pm1.81$    |            $76.89\\pm2.01$   |      $88.32\\pm1.37$     |     $81.56\\pm0.47$       |      $69.81\\pm0.16$   | $77.53$ | \n\n<Table F: Performance on link prediction tasks under structural-missing settings with $r_m=0.995$, measured in ROC AUC score (%)>\n\n| Method |      Cora      | CiteSeer | PubMed | Photo | Computers| Average |\n|--------|:--------------:|:--------:|:------:|:-----:|:---------:|:----------:|\n| FISF   | $87.26\\pm1.44$   | $84.12\\pm1.17$       | $83.19\\pm0.78$     | $95.86\\pm0.21$    | $94.70\\pm0.30$  | $89.03$      |\n| FISF*  |         $86.80\\pm1.27$   | $84.12\\pm1.17$       | $82.46\\pm0.94$     | $95.76\\pm0.33$    | $94.39\\pm0.82$ | $88.70$     |\n\n<Table G: Performance on link prediction tasks under uniform-missing settings with $r_m=0.995$, measured in ROC AUC score (%)>\n\n| Method |      Cora      | CiteSeer | PubMed | Photo | Computers| Average |\n|--------|:--------------:|:--------:|:------:|:-----:|:---------:|:----------:|\n| FISF   | $87.44\\pm0.80$   | $83.45\\pm2.53$       | $85.33\\pm0.47$     | $96.64\\pm0.18$    | $95.13\\pm0.35$ | $89.60$      |\n| FISF*  |         $87.56\\pm1.29$   | $81.15\\pm1.17$       | $82.46\\pm0.69$     | $95.68\\pm0.42$    | $94.94\\pm0.27$ | $88.36$     |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594256738,
                "cdate": 1700594256738,
                "tmdate": 1700726797714,
                "mdate": 1700726797714,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Kgi9VRqUZu",
                "forum": "M11LONBkx1",
                "replyto": "pTYCKKsChn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9301/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VoZJ [Part 4/5]"
                    },
                    "comment": {
                        "value": "> $\\textbf{Q5.}$ Lastly, considering the similarity of Label Propagation (LP) and FP, and the argument about low variance channels (in respect of very high missing feature rates), I think this paper shares similar motivation with Poisson Learning [1], which mitigates very low label rate problem in LP. Poisson Learning points out that LP outputs almost constant pseudo-labels for most of unlabeled samples, while this paper points out existing diffusion-based methods including FP causes \u201clow variance channels\u201d, i.e. almost constant features (or an exact constant when there is only one observed feature value, as presented in this paper). Regarding aforementioned similarity, further discussion with Poisson Learning might be beneficial.\n\n$\\textbf{A5.}$ We thank the reviewer for pointing out a relevant reference [1]. As the reviewer mentioned, our paper and [1] commonly address problems in Laplacian learning adopted by both LP and FP. As you recommended, we added further discussion with Poisson learning [1] to the related work in the revised manuscript as follows. ``The problems being addressed are different, and the causes of each problem are completely different. In [1], proposed Poisson learning is a feature-agnostic method that only propagates given labels like LP, tackling semi-supervised classification. Furthermore, the problem addressed in [1] arises from the very narrow area of a localized spike, generated by the propagation of a given label. The problem assumes having a wide variety of labels evenly distributed despite very low label rates. However, we discover and address the problem of low-variance channels caused by nearly identical observed values with a feature channel. We provide theoretical proof about the cause of the problem of low-variance channels.\u201d \n\n[1] Calder, Jeff, et al. \"Poisson learning: Graph based semi-supervised learning at very low label rates.\" International Conference on Machine Learning. PMLR, 2020.\n\n> $\\textbf{Q6.}$ Can you provide the feature variance distribution (Figure 1) in more realistic missing feature settings (I.e. r_m <= 0.9)? In my opinion, authors have to show that existing diffusion-based methods still suffer from the problem of making low variance channels in various missing feature settings, especially in more realistic scenarios.\n\n$\\textbf{A6.}$ In the revised version, we provided distributions of feature variances for each channel on CiteSeer containing 90\\% missing features in Figure 1. We also added distributions of feature variances on Cora in Appendix E.2. We can observe that the problem of low-variance channels is not limited to high $r_m$ and also occurs in more realistic missing feature settings. We further demonstrate variance distributions of original features without any missing. We confirm that the original features only contain a few low-variance channels like output features from FISF."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700594753906,
                "cdate": 1700594753906,
                "tmdate": 1700594753906,
                "mdate": 1700594753906,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zNJGNpji2x",
                "forum": "M11LONBkx1",
                "replyto": "pTYCKKsChn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9301/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VoZJ [Part 5/5]"
                    },
                    "comment": {
                        "value": "> $\\textbf{Q7.}$ What happens if pre-diffusion method is replaced to FP from PCFI? (Why this paper have chosen PCFI as a pre-diffusion method?)\n\n$\\textbf{A7.}$ We select channel-wise inter-node diffusion in PCFI as pre-diffusion to maximize performance in downstream tasks. For not low-variance channels, features obtained via pre-diffusion are preserved until diffusion with synthetic features ends. Therefore, since PCFI outperforms FP in terms of performance in downstream tasks, FISF shows slightly better performance than FastFISF in most cases. We define FISF using FP for pre-diffusion as FastFiSF and the table below shows the comparison results in terms of semi-supervised node classification accuracy. While the original FISF outperforms FastFISF, the performance of FastFISF is comparable to FISF.\n\n<Table H: Performance on semi-supervised node classification tasks under structural-missing settings with $r_m=0.995$, measured in accuracy (%)>\n\n| Method |      Cora      | CiteSeer | PubMed | Photo | Computers | OGBN-Arxiv | Average |\n|--------|:--------------:|:--------:|:------:|:-----:|:---------:|:----------:|:-------:|\n| FISF   | $79.29\\pm1.72$ |     $69.68\\pm2.47$     |    $76.90\\pm1.50$    |    $88.22\\pm0.79$   |      $79.40\\pm1.11$     |       $69.92\\pm0.17$     |    $77.24$     |\n| FastFISF  |         $78.94\\pm1.92$   | $69.42\\pm1.44$       | $77.14\\pm0.94$     | $88.10\\pm1.38$    | $79.09\\pm1.42$        | $69.53\\pm0.21$ | $77.04$     |\n\n<Table I: Performance on semi-supervised node classification tasks under uniform-missing settings with $r_m=0.995$, measured in accuracy (%)>\n\n| Method |      Cora      | CiteSeer | PubMed | Photo | Computers | OGBN-Arxiv | Average |\n|--------|:--------------:|:--------:|:------:|:-----:|:---------:|:----------:|:-------:|\n| FISF   | $79.09\\pm1.73$ |     $69.52\\pm1.81$      |     $77.53\\pm1.28$   |    $88.32\\pm1.37$    |     $82.12\\pm0.51$                  |     $69.81\\pm0.16$    | $77.73$  |\n| FastFISF  |       $79.29\\pm1.84$   | $69.39\\pm1.57$       | $77.41\\pm1.77$     | $88.03\\pm1.46$    | $81.70\\pm0.54$        | $69.45\\pm0.18$  | $77.55$ | \n\n\nThe advantage of using FastFISF is a reduction in time complexity. Since channel-wise inter-node diffusion requires more time compared to FP in uniform-missing settings, FastFISF decreases the training time in uniform-missing settings. Table C in $\\textbf{A3}$ shows the training time of methods.  While FP has the lowest training time among the methods, FISF brings great performance improvement compared to FP. For instance, on Cora and PubMed, FISF demonstrates improvements in node classification accuracy by $7.43$\\%p and $4.94$\\%p respectively, compared to FP."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700595103418,
                "cdate": 1700595103418,
                "tmdate": 1700625678610,
                "mdate": 1700625678610,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "c7wC0WzKVK",
                "forum": "M11LONBkx1",
                "replyto": "pTYCKKsChn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9301/Reviewer_VoZJ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9301/Reviewer_VoZJ"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the authors' earnest response. Overall, most of the doubtful points were addressed. Thus, I have raised my ratings.  \nThe following are the remaining minor issues in your responses.\n- A2: Can you provide the average (inter/intra-class) cosine similarity of original features like the results presented in Table B (Table 7 in the manuscript) for imputed features? In my opinion, comparing the results shown in Table B with those results might be beneficial and help understand the significance of the results of Table B.\n- A5. I appreciate the authors' responses in general. However, the discussion with Poisson learning added to the manuscript seems insufficient because it is abbreviated compared to the authors' responses. Therefore, we request that the authors' responses be reflected in the manuscript as they are."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700662151076,
                "cdate": 1700662151076,
                "tmdate": 1700662174089,
                "mdate": 1700662174089,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sMuQz9cNcs",
                "forum": "M11LONBkx1",
                "replyto": "pTYCKKsChn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9301/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Second Response to Reviewer VoZJ"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer's decision. Your insightful feedback has further improved our paper.\n\n> $\\textbf{Q8.}$ Can you provide the average (inter/intra-class) cosine similarity of original features like the results presented in Table B (Table 7 in the manuscript) for imputed features? In my opinion, comparing the results shown in Table B with those results might be beneficial and help understand the significance of the results of Table B.\n\n$\\textbf{A8.}$ Table J shows the intra-class cosine similarity calculated from original features. The results indicate that original features also have values of Ratio greater than 1 across the datasets. This means that the datasets also originally have higher intra-class feature similarity compared to inter-class feature similarity. Despite the introduction of synthetic features during diffusion, as shown in Table B, we can observe that imputed features by our scheme consistently maintains higher intra-class feature similarity than inter-class feature similarity. We included this discussion and Table J in Appendix C.4 of the revised manuscript.\n\n<Table J: Average cosine similarity of original features.>\n\n| Dataset  | Inter-class | class 1 | class 2 | class 3 | class 4 | class 5 | class 6 | class 7 | Intra-class Average |  Ratio |\n|----------|:-----------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|:-------------------:|:------:|\n| Cora     |   $0.0578$           | $0.841$       | $0.113$       | $0.0896$       | $0.683$       | $0.0690$       | $0.0853$       | $0.109$       | $0.0883$           | $1.53$ |\n| CiteSeer |   $0.0470$          | $0.655$      | $0.0601$      | $0.0617$      | $0.0650$      | $0.762$  | $0.0581$  | -   | $0.0644$ | $1.37$ |\n| PubMed   |   $0.0719$         | $0.112$    | $0.937$     | $0.0779$    | -    | -    | -    | -    | $0.0946$         | $1.32$ |\n\n> $\\textbf{Q9.}$  The discussion with Poisson learning added to the manuscript seems insufficient because it is abbreviated compared to the authors' responses. Therefore, we request that the authors' responses be reflected in the manuscript as they are.\n\n$\\textbf{A9.}$ We thank you for the suggestion. We added our full discussion with Poisson learning to the related work in the revised manuscript.\n\nPlease let us know if the remaining issues are addressed. If you have any further concerns, we would like to have an opportunity to address them."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700698936698,
                "cdate": 1700698936698,
                "tmdate": 1700726523775,
                "mdate": 1700726523775,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1r3HyIKu9R",
            "forum": "M11LONBkx1",
            "replyto": "M11LONBkx1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9301/Reviewer_VMHb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9301/Reviewer_VMHb"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method for missing value imputation on top of pseudo-confidence-based feature imputation (PCFI) by manipulating the features with low variance, whose were left behind by PCFI. The idea is to insert another random feature for each low variance."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "I think the original idea is fair that the paper try to amplify low variance features."
                },
                "weaknesses": {
                    "value": "The direction of amplifying low variance features can be a good idea and if it is done right, there might be an optimal point that maximize performance on top of its baseline. \nHowever, the paper shows the algorithm, how to manipulate the synthetically generated random features without a clue why it has to be done that way, for what purpose. Overall, the paper really proposes a \"new method\" without actually showing what is the problem it is solving and how the method help achieving the goal."
                },
                "questions": {
                    "value": "What is the idea of a synthetic random feature and how it help improving performance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9301/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698722404427,
            "cdate": 1698722404427,
            "tmdate": 1699637170760,
            "mdate": 1699637170760,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5ejvsSgQET",
                "forum": "M11LONBkx1",
                "replyto": "1r3HyIKu9R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9301/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer VMHb"
                    },
                    "comment": {
                        "value": "Thank you for the time and valuable feedback. We provide the answer below.\n\n> $\\textbf{Q1.}$ What is the idea of a synthetic random feature and how it help improving performance? A clue why it has to be done that way, for what purpose.\n\n$\\textbf{A1.}$ The synthetic random feature is used as a virtual observed feature instead of a randomly chosen  missing feature. This synthetic feature is introduced to change a low variance channel into a high variance channel to increase distinctiveness. The distinctiveness leads to high performance of the downstream tasks.\n\nIn Figure 1 in the revised manuscript, we demonstrate that existing diffusion-based imputation methods generate low-variance channels that contribute very little to distinguishing nodes. A low-variance channel occurs when observed features within the channel have nearly identical values. Furthermore, to provide a clue on how synthetic random features improve performance, we conduct additional experiments on real-world datasets (Appendix C.3). We empirically verify that low-variance channels (not distinctive channels) contribute very little to performance in downstream tasks. To make features in low-variance channels help GNNs perform downstream tasks well, we inject synthetic features with randomly sampled values. When a synthetic feature is provided to a low-variance channel, there exists a feature value different from the observed features within the channel. Hence, treating the synthetic feature as a virtual observed feature can make the channel deviate from low variance. We observe that only a few low-variance channels remain in imputed features by our scheme. Across various missing settings, our scheme achieves state-of-the-art performance in both semi-supervised node classification and link prediction."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590675732,
                "cdate": 1700590675732,
                "tmdate": 1700590675732,
                "mdate": 1700590675732,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vuvrNmX09u",
                "forum": "M11LONBkx1",
                "replyto": "5ejvsSgQET",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9301/Reviewer_VMHb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9301/Reviewer_VMHb"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your clarification. \n\nI think I am clear in the fact that the paper add random features to low variance channel that help to distinguish nodes, which is a way of adding more variance to low variance channels. What I still find missing is the justification of how more variance in any channel help to have better performance. One can add any amount of variance to any channel by random features, which increases distinctiveness, there is no clues why it should help any downstream task. \nI does not see this is a valid argument to explain performances. I keep my evaluation."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700628871973,
                "cdate": 1700628871973,
                "tmdate": 1700628871973,
                "mdate": 1700628871973,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "keSC8RTUWt",
                "forum": "M11LONBkx1",
                "replyto": "1r3HyIKu9R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9301/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Second Response to Reviewer VMHb [Part 1/2]"
                    },
                    "comment": {
                        "value": "We agree with the reviewer\u2019s opinion that injecting many synthetic features into a low-variance channel disrupts the influence of the observed features due to the generation of many artificial spikes. As shown in Table 3 of our ablation study (Appendix C.1), injecting one synthetic feature per channel yields better performance in downstream tasks compared to injecting two synthetic features. We included this discussion on the reason for improving performance in downstream tasks in Appendix C.7. To explain the reason succinctly, we injected one synthetic feature into each low variance channel, but placed it at a different location for each channel. Then the diffused node feature vector containing every low-variance channel feature after diffusion becomes distinctive from others by reflecting the graph structure. We will illustrate a visualization on the distinctiveness of the diffused feature vector by our scheme in Appendix C.7."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700654273286,
                "cdate": 1700654273286,
                "tmdate": 1700668338394,
                "mdate": 1700668338394,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Tq1Zn2r2jU",
                "forum": "M11LONBkx1",
                "replyto": "1r3HyIKu9R",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9301/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Second Response to Reviewer VMHb [Part 2/2]"
                    },
                    "comment": {
                        "value": "We have added Figure 9 (in Appendix C.7), which visualizes the distinctiveness of a node feature vector obtained by our method, to the revised manuscript. \n\nWe appreciate your constructive feedback. Please let us know if our responses address your concerns."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668311122,
                "cdate": 1700668311122,
                "tmdate": 1700668350744,
                "mdate": 1700668350744,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wZWVLfjC1A",
            "forum": "M11LONBkx1",
            "replyto": "M11LONBkx1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9301/Reviewer_mYHT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9301/Reviewer_mYHT"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles the missing feature problem in graphs through the lens of low-variance channels. To address this, FISF first pre-diffuses the known features to unknown features and generates a synthetic feature on a specific low-variance channel. Finally, it diffuses the synthetic feature widely, treating it as a known feature throughout the graph. Performance across various missing rates demonstrates the efficacy of FISF."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The problem of the low-variance channel is interesting and provides a new perspective on the missing feature issue in the graph community.\n  \n2. The use of generating synthetic features seems to be a straightforward remedy for the low-variance channel.\n  \n3. The paper is well-written and easy to follow."
                },
                "weaknesses": {
                    "value": "1. Although the authors demonstrated the existence of a low-variance channel after current diffusion-based methods, FP and PCFI, the link explaining how these low-variance channels act as a bottleneck for overall performance is not comprehensively provided. For example, the performance of node classification could be provided after excluding some portions of low-variance channels. Additionally, I am curious whether the original variance of the dataset, without any missing features, shows low variance as depicted in Figure 1. In this context, the authors should explain why a low-variance channel is especially burdensome in scenarios with missing features.\n  \n2. I wonder if the low-variance problem is due to zero-initialization. In cases of severe missing data and zero-initialization is equipped, the majority of the feature matrix would consist of zeros, so the output matrix would naturally contain many zeros (i.e., biases, if adding biases is enabled), especially considering that most graph datasets use one-hot encoding via bag-of-words for feature matrices. If the initialization for the missing feature were from random sampling, such as a uniform or normal distribution, the low variance problem might be easily addressed.\n  \n3. Although diffusion via synthetic features can enhance the distinctiveness across features, it might undermine the GNN's key inductive bias, which is the smoothness across connected nodes. A more in-depth discussion of the trade-off between feature distinctiveness and the smoothness of connected features should be provided.\n  \n4. While the authors proposed the use of synthetic features, excluding this module, the pre-diffusion and diffusion with synthetic features are exactly aligned with the existing work, PCFI. This raises concerns about the overall novelty of this paper.\n  \n5. The complexity of FISF compared to existing works is not comprehensively addressed. Given that the adjacency matrix is created for each feature dimension, the complexity would be exceedingly high, potentially limiting the practical application of FISF.\n  \n6. The proposed missing rates of 0.995 and 0.999 seem unrealistic. Furthermore, edge information can also be missing in real-world scenarios, a factor that should be considered."
                },
                "questions": {
                    "value": "See the Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9301/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9301/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9301/Reviewer_mYHT"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9301/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698821336968,
            "cdate": 1698821336968,
            "tmdate": 1700741016715,
            "mdate": 1700741016715,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8XQsc4b4MD",
                "forum": "M11LONBkx1",
                "replyto": "wZWVLfjC1A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9301/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mYHT [Part 1/3]"
                    },
                    "comment": {
                        "value": "Thank you for the time and valuable feedback. We provide the answers below.\n\n> $\\textbf{Q1.}$ The link explaining how these low-variance channels act as a bottleneck for overall performance is not comprehensively provided. Additionally, I am curious whether the original variance of the dataset, without any missing features, shows low variance as depicted in Figure 1.\n\n$\\textbf{A1.}$ We conduct additional experiments to demonstrate little contribution of low-variance channels in downstream tasks. We compare performance by excluding partial channels from an original feature matrix using two different ways. The first way is excluding channels in descending order of variance, starting from the highest, based on a fixed proportion. The second way is excluding channels in ascending order of variance. As shown in Figure 5 and Figure 6 in Appendix C.3, the performance persists despite an increasing removal proportion of low-variance channels. However, cases of channel removal from the highest variance suffer significant performance degradation even with a low proportion of channel removal.\n\nWe added the original variance of the datasets before missing, to Figure 1 in the revised manuscript and Figure 9 in Appendix E. Through the original variance of the dataset, we can confirm that the original dataset also contains a few low-variance channels like output features from FISF. \n\n> $\\textbf{Q2.}$ I wonder if the low-variance problem is due to zero-initialization. In cases of severe missing data and zero-initialization is equipped, the majority of the feature matrix would consist of zeros, so the output matrix would naturally contain many zeros (i.e., biases, if adding biases is enabled), especially considering that most graph datasets use one-hot encoding via bag-of-words for feature matrices.  If the initialization for the missing feature were from random sampling, such as a uniform or normal distribution, the low variance problem might be easily addressed.\n\n$\\textbf{A2.}$ The low-variance channels in an output matrix are not caused by zero initialization for unknown features. According to the proof in Appendix A, initial values for unknown features do not affect the steady state of diffusion. During the diffusion, observed features are preserved by resetting to their original values at every iteration while missing features are updated by aggregating features from neighboring nodes. Eventually, the missing features converge to a steady state depending on only the observed features regardless of the initial values of missing features. That is, the influence of the initial values of the missing features converge to zero.\n\nIn the case of Cora, CiteSeer, and PubMed, which are sparse datasets containing many zeros in their original feature matrices, many low (almost zero)-variance channels can occur due to nearly identical observed values within a channel, resulting in a low-variance channel. This phenomenon due to sparsity is entirely different from the zero initialization of missing features. In the case of Photo, Computers, and OGBN-Arxiv, the ratio of zeros within the feature matrix is not high. For example, in OGBN-Arxiv, only 0.0002% of features are zeros since features are obtained by 128-dimensional deep embeddings of words. For all the aforementioned datasets, FISF improves the performance in downstream tasks by addressing low-variance channels regardless of their frequent values."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700589964019,
                "cdate": 1700589964019,
                "tmdate": 1700590170554,
                "mdate": 1700590170554,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HLTQppm7Gh",
                "forum": "M11LONBkx1",
                "replyto": "wZWVLfjC1A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9301/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mYHT [Part 2/3]"
                    },
                    "comment": {
                        "value": "> $\\textbf{Q3.}$ A more in-depth discussion of the trade-off between feature distinctiveness and the smoothness of connected features should be provided.\n\n$\\textbf{A3.}$ As the reviewer mentioned, there exists a trade-off between feature distinctiveness and smoothness on a graph. We discussed feature distinctiveness and smoothness in depth through various experiments. Initially, to explore the graph-level smoothness, we measure Dirichlet energy on imputed features and original features. As shown in the table below, FP designed to minimize Dirichlet energy via diffusion demonstrates the lowest Dirichlet energy. In contrast, FISF makes Dirichlet energy of the imputed features similar to that of the original features without considering the trade-off. Note that our FISF shows the highest Dirichlet energy (distinctiveness) among the methods.\n\n<Table A: log($E_D$) of imputed features under a structural-missing setting with $r_m=0.995$, where $E_D$ is the Dirichlet energy. Original denotes original given features.>\n\n|     Missing way     |        | Structural |         |        |  Uniform |         |\n|:-------------------:|:------:|:----------:|:-------:|:------:|:--------:|:-------:|\n| Method $\\downarrow$ |  Cora  |  CiteSeer  |  PubMed |  Cora  | CiteSeer |  PubMed |\n| Original            | $4.36$ |   $4.49$   |  $3.11$ | $4.36$ |  $4.49$  |  $3.11$ |\n| FP                  | $1.90$ |   $1.94$   | $0.798$ | $1.89$ |  $1.91$  | $0.805$ |\n| PCFI                | $3.14$ |   $2.59$   |  $1.49$ | $2.52$ |  $2.64$  |  $1.43$ |\n| FISF (Ours)         | $3.25$ |   $2.92$   |  $4.15$ | $2.69$ |  $2.70$  |  $4.34$ |\n\nWe conduct further qualitative analysis on imputed and deep features to examine feature distinctiveness. Figures 7 and 8 in Appendix C.4 depict t-SNE plots visualizing imputed and deep features. These plots indicate that FISF provides clearer cluster structures for both imputed and deep features compared to other imputation methods. While smoothness is an important inductive bias for GNNs, our experimental results confirm that features displaying excessive smoothness lead to poor performance in downstream tasks.\n\n> $\\textbf{Q4.}$  The pre-diffusion and diffusion with synthetic features are exactly aligned with the existing work, PCFI.\n\n$\\textbf{A4.}$ As the pre-diffusion stage in FISF, we adopt channel-wise inter-node diffusion in PCFI. However, the second diffusion stage involving synthetic features is a new scheme, distinct from PCFI.  Beyond addressing the problem of low-variance channels by generating synthetic features, we introduce a new diffusion stage that employs two distinct types of distance encoding. By combining these two distances, synthetic features exert a stronger influence than observed features during the diffusion process. As a result, FISF outperforms PCFI by a large margin at high rates of missing features.\n\n> $\\textbf{Q5.}$  The complexity of FISF compared to existing works.\n\n$\\textbf{A5.}$ FISF operates in structural-missing settings with a time complexity of $O(|\\mathcal{E}|+(1+\\gamma F)N^2)$ and in uniform-missing settings with a complexity of $O(|\\mathcal{E}|+(1+\\gamma) F N^2)$. During the rebuttal period, to address the increasing time complexity in uniform-missing settings, we have sought a way to replace channel-wise inter-node diffusion with FP in pre-diffusion as a light version of FISF, called FastFISF. Consequently, the time complexity of FastFISF decreases to $O(|\\mathcal{E}|+\\gamma FN^2)$ regardless of the missing way.\n\nThe table below displays the training time of methods. FP exhibits the shortest training time among the methods. However, FISF notably enhances performance in downstream tasks compared to FP. For instance, in structural-missing setups with $r_m=0.995$, FISF achieves significant gains in node classification accuracy over FP, showing improvements of $7.43$% and $4.94$% on Cora and PubMed, respectively. Additionally, FastFISF demonstrates substantial reductions in training time under uniform-missing settings. A detailed explanation of FastFISF including performance in downstream tasks is in Appendix C.5.\n\n<Table B: Training time of methods. OOM denotes an out-of-memory error.>\n\n| Missing way | Structural | missing | Uniform | missing |\n|-------------|:----------:|:---------:|:-------:|:-------:|\n| Method      |    Cora    |   PubMed  |   Cora  |  PubMed |\n| GCNMF       |   $10.3$s  |  $19.4$s  | $9.87$s | $28.3$s |\n| GRAFENNE    |   $47.9$s  |  $74.7$s  | $51.1$s | $74.0$s |\n| MEGAE       |   $1753$s  |    OOM    | $1801$s |   OOM   |\n| FP          |   $2.36$s  |  $3.12$s  | $2.25$s | $2.90$s |\n| PCFI        |   $2.45$s  |  $3.23$s  | $11.1$s | $34.1$s |\n| FastFISF    |   $13.4$s  |  $34.6$s  | $11.8$s | $42.5$s |\n| FISF        |   $13.4$s  |  $34.8$s  | $17.6$s | $78.2$s |"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590086196,
                "cdate": 1700590086196,
                "tmdate": 1700726723071,
                "mdate": 1700726723071,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "A65hp85od6",
                "forum": "M11LONBkx1",
                "replyto": "wZWVLfjC1A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9301/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mYHT [Part 3/3]"
                    },
                    "comment": {
                        "value": "> $\\textbf{Q6.}$ The proposed missing rates of 0.995 and 0.999 seem unrealistic. Furthermore, edge information can also be missing in real-world scenarios, a factor that should be considered.\n\n$\\textbf{A6.}$ Research aimed at handling extremely high rates of missing data is actively progressing across various fields (e.g., 99.99% missing in electronic health records [1], 97.5% missing in semiconductor manufacturing data, and 95% missing in an image [3]). In line with this trend, diffusion-based imputation methods have been addressing challenging missing scenarios with a missing rate of 99%/99.5% [4, 5]. To demonstrate that our scheme endures even in more extreme missing scenarios, we include experimental results under 99.9% missing settings.\t\t \t \t \t\t\n\nAs an extreme scenario, during the early stages when few people (e.g., 0.1%) purchase a new product, most people (e.g., 99.9%) linked in a social network, having similar purchase tendencies, do not have any features related to this product. In this case, from product-specific features of the very few early adaptors, the proposed diffusion scheme can impute the features. Then, the fully filled matrix can be used as an input by GNNs for a learning task of product recommendation. \n\nWe concur with the reviewer\u2019s point that connectivity between entities is not provided in many real-world scenarios. To use graph-based methods in such scenarios, several efforts have been made to form graphs by using the k-NN algorithm [6, 7]. \n\n[1] Kim, Yeo-Jin, and Min Chi. \"Temporal Belief Memory: Imputing Missing Data during RNN Training.\" In Proceedings of the 27th International Joint Conference on Artificial Intelligence (IJCAI-2018). 2018\\\n[2] Park, Sewon, et al. \"Bayesian nonparametric classification for incomplete data with a high missing rate: an application to semiconductor manufacturing data.\" IEEE Transactions on Semiconductor Manufacturing (2023).\\\n[3] Yoon, Seongwook, and Sanghoon Sull. \"GAMIN: Generative adversarial multiple imputation network for highly missing data.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020.\\\n[4] Rossi, Emanuele, et al. \"On the unreasonable effectiveness of feature propagation in learning on graphs with missing node features.\" Learning on Graphs Conference. PMLR, 2022.\\\n[5] Um, Daeho, et al. \"Confidence-Based Feature Imputation for Graphs with Partially Known Features.\" The Eleventh International Conference on Learning Representations. 2022.\\\n[6] Telyatnikov, Lev, and Simone Scardapane. \"EGG-GAE: scalable graph neural networks for tabular data imputation.\" International Conference on Artificial Intelligence and Statistics. PMLR, 2023.\\\n[7] Yun, Sukwon, Junseok Lee, and Chanyoung Park. \"Single-cell RNA-seq data imputation using Feature Propagation.\" ICML workshop. 2023."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590221319,
                "cdate": 1700590221319,
                "tmdate": 1700722188807,
                "mdate": 1700722188807,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yDio1baOJR",
                "forum": "M11LONBkx1",
                "replyto": "wZWVLfjC1A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9301/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "Dear Reviewer mYHT,\n\nWe're grateful for your feedback on our work. As the discussion period nears its end, we would like to confirm if our responses have sufficiently clarified and addressed your concerns. We are happy to provide any additional clarification and discussion.\n\nThank you."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700699527798,
                "cdate": 1700699527798,
                "tmdate": 1700699555516,
                "mdate": 1700699555516,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xjUjkjZ0fC",
                "forum": "M11LONBkx1",
                "replyto": "yDio1baOJR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9301/Reviewer_mYHT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9301/Reviewer_mYHT"
                ],
                "content": {
                    "title": {
                        "value": "Response from Reviewer mYHT"
                    },
                    "comment": {
                        "value": "I appreciate the author's detailed rebuttal. The majority of my concerns have been addressed, and I am particularly pleased with the addition of Figure C.3 and the updates made to Figure 1. However, I still have a concern regarding the impact of high missing rates on your model. Specifically, in cases like 99% and 99.9% missing, some columns might consist entirely of zeros, and thus the propagated values would remain zeros.\n\nIn this regard, could you provide results similar to those in Figure 1 but with random initialization? This would help address my concern. If my concern is satisfactorily resolved, I will directly raise my current score. However, considering the limited remaining time for the rebuttal process, I am prepared to raise my score before the deadline."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700733340630,
                "cdate": 1700733340630,
                "tmdate": 1700733340630,
                "mdate": 1700733340630,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SEJQYtZPBx",
                "forum": "M11LONBkx1",
                "replyto": "wZWVLfjC1A",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9301/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9301/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Second Response to Reviewer mYHT"
                    },
                    "comment": {
                        "value": "Thank you for the positive comments and constructive advice. \n\nTo address your remaing concern, we have included Figure 11 in Appendix E.3 of the revised manuscript. Figure 11 compares variance distributions when zero initialization and random initialization are used. Figure 11 shows that many low-varince channels persist despite random initialization, but there is a slight difference between the distributions despite using the same setting. This is because all the diffusion-based methods approximate the steady state with a sufficiently large hyperparameter $K$, indicating the number of diffusion iteration (e.g. $K=40$ is used in FP and $K=100$ is used in PCFI and FISF). However, we have further confirmed that variance distributions becomes identical with very large $K$ values (e.g., $K=1000$) regardless of initialization. Although the final approximated results are not affected by initialization for missing features with a large $K$, careful consideration is needed when determining $K$, depending on the initialization."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740438233,
                "cdate": 1700740438233,
                "tmdate": 1700740995906,
                "mdate": 1700740995906,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wBzXrMV1id",
                "forum": "M11LONBkx1",
                "replyto": "34gQR74uJ1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9301/Reviewer_mYHT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9301/Reviewer_mYHT"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the last-minute discussion and all the authors' efforts.\n\nI have raised my score to 6."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9301/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700741123459,
                "cdate": 1700741123459,
                "tmdate": 1700741123459,
                "mdate": 1700741123459,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]