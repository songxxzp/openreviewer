[
    {
        "title": "Advancing the Lower Bounds: an Accelerated, Stochastic, Second-order Method with Optimal Adaptation to Inexactness"
    },
    {
        "review": {
            "id": "Tpqs0wK2Kd",
            "forum": "otU31x3fus",
            "replyto": "otU31x3fus",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5556/Reviewer_xECU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5556/Reviewer_xECU"
            ],
            "content": {
                "summary": {
                    "value": "This work studies the unconstrained minimization of convex and strongly convex functions with continuously differentiable gradients through second-order methods with access to inexact gradients and Hessians. Further with the assumption of unbiased gradients they show a faster convergence rate of their proposed algorithm than the state of the art. Also, they show the worst-case complexity lower bound for their problem. Then, the proposed method is extended to tensor analysis and a restart scheme is proposed for strongly convex functions. Experimental results confirm the superiority of the proposed method to Extra-Newton and SGD methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1- Good flow in introduction and problem setup.\n\n2- With additional assumption on the unbiasedness of the gradient, they show the lower bound on convergence rate of inexact Hessian & gradient plus proposing a method which achieves the lower bound exept for the last term.\n\n3- They did a relatively thorough analysis by extending their analysis to tensor methods and special cases like strongly convex cases."
                },
                "weaknesses": {
                    "value": "1- Though the text has a good flow in terms of the context, there is still room for improvements: e.g. \n\n(a) In page 2 there are two loose sentences in the middle of the page. They can be integrated with the previous paragraph or just in a new independent paragraph. \n\n(b) Below assumption 1.2: $E[g(x,\\xi)]$ or $E[F(x,\\xi)]$?\n\n(c) **\\citet** was used wrongly in many places. Please consider replacing the wrong ones with **\\citep**.\n\n(d) function $\\psi_t(x)$ above algorithm 1 is not defined (I think it is in the algorithm so you should refer to that or simply introduce it)\n\n(e) The definition of $f$ in section 7 page 8 is an abuse of notation: $f(x)=E[f(x,\\xi)]$\n\n(f) I suggest larger font size on Figures 1,2\n\n(g) Large gap in the appendix after (26)\n\n2- I think contributions 3 & 1 should be integrated.\n\n3- Table 1 might lead to misunderstanding. Your result is based on the asssumption of unbiased gradients. The rate by Agafonov does not have this assumption. Thus, it seems like an unfair comparison.\n\n4- The title of the section \u201cLower Bound\u201d is very generic and also the description is vague. For example, the first sentence of this section is vague."
                },
                "questions": {
                    "value": "1- Is the $\\boldsymbol \\Phi_x$ defined at the beginning of Section 2 used? Same question holds for $\\boldsymbol\\Phi_x$ defined under Assumption 5.1.\n\n2- Lemma 2.1 seems like a special case of a similar lemma in Agafonov et al 2020. Is there a reason you did not cite their work when you present the Lemma? Same question holds for Lemma 5.2. \n\n3- How did you find the dynamic strategy for the precision level $(\\tau_t=c/(t^{3/2}))$\n\n4- According to your investigation and results, does it make sense to analyze inexactness in Hessian (or higher order information of the objective function) when gradients are inexact? This question mainly concerns the $O(1/\\sqrt{T})$ convergence rate related to the inexactness of the gradients which dominates the convergence rate as $T\\rightarrow \\infty$."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5556/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5556/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5556/Reviewer_xECU"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5556/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698501401528,
            "cdate": 1698501401528,
            "tmdate": 1699636571438,
            "mdate": 1699636571438,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gN1HADAHeR",
                "forum": "otU31x3fus",
                "replyto": "Tpqs0wK2Kd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5556/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5556/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your valuable input and considerate questions!\n\n>**Weakness 1 :** *\u201c Though the text has a good flow in terms of the context, there is still room for improvements: e.g. \n(a) In page 2 there are two loose sentences in the middle of the page. They can be integrated with the previous paragraph or just in a new independent paragraph;\n(b) Below assumption 1.2: E[g(x,\u03be)] or E[F(x,\u03be)];\n(c) \\citet was used wrongly in many places. Please consider replacing the wrong ones with \\citep;\n(d) function \u03c8t(x) above algorithm 1 is not defined (I think it is in the algorithm so you should refer to that or simply introduce it);\n(e) The definition of f  in section 7 page 8 is an abuse of notation: f(x)=E[f(x,\u03be)];\n(f) I suggest larger font size on Figures 1,2;\n(g) Large gap in the appendix after (26);\n\u201d*\n\n**Response:** Thank you for suggestions and pointing out several inaccuracies. We agree with comments (a, c, d, e, f , g) and will change it in the revised version. Regarding comment (b), we maintain the expression \"E[g(x,\u03be)]\" exactly as it is below Assumption 1.2\n\n>**Weakness 2:** *\"I think contributions 3 & 1 should be integrated.\"*\n\n**Response:**  Contribution 1 signifies an enhanced convergence rate, whereas Contribution 3 underscores the successful integration of an inexact subproblem solution\u2014a novel aspect not previously addressed in inexact second-order and high-order convex methods. We appreciate your thoughtful input and will consider how best to integrate these contributions for an improved presentation of our research.\n\n>**Weakness 3:** *\"Table 1 might lead to misunderstanding. Your result is based on the asssumption of unbiased gradients. The rate by Agafonov does not have this assumption. Thus, it seems like an unfair comparison.\"*\n\n**Response:** Certainly, we acknowledge that a direct comparison of the convergence of these methods is not feasible, as elucidated in the paragraph following Assumption 2.3. To provide a comprehensive overview of the current landscape in the realm of inexact/stochastic second-order methods, we present Table 1. This table includes a column outlining assumptions related to inexactness, explicitly noting that the unbiasedness of the gradient was not assumed in the work [1]. We believe that Table 1's clear articulation of assumptions minimizes any potential misunderstandings regarding the comparative context.\n\n>**Weakness 4:** *\"The title of the section \u201cLower Bound\u201d is very generic and also the description is vague. For example, the first sentence of this section is vague.\"*\n\n**Response:** Could you kindly offer additional insights on why the title and the first sentence might be considered vague? We value your perspective and would appreciate any suggestions you may have for improvement. While the current wording seems acceptable to us, we are open to refining it based on your feedback. Thank you.\n\n>**Question 1:** *\" Is the $\\Phi_x$ defined at the beginning of Section 2 used? Same question holds for $\\Phi_x$ defined under Assumption 5.1.\"*\n\n**Response:** Formally, its utilization is confined to the Appendix. Nevertheless, we believe it is crucial to introduce the exact Taylor polynomial $\\Phi_x(y)$, as it provides valuable insights into the construction of high-order methods.\n\n>**Question 2:** *\"Lemma 2.1 seems like a special case of a similar lemma in Agafonov et al 2020. Is there a reason you did not cite their work when you present the Lemma? Same question holds for Lemma 5.2.\"*\n\n**Response:**\nThank you, we agree with your comment, and we will add a direct citation near Lemmas 2.1 and 5.1 in revised version.\n\n>**Question 3:** *\"How did you find the dynamic strategy for the precision level $\\tau_t = c / t^{3/2}$?\"*\n\n**Response:** \nThank you for this question. We have a typo here, it should be $\\tau = \\frac{c}{t^{5/2}}$, we will fill it in the revised version.\n\nFrom the convergence, we have for any $t \\geq 1$\n\n$f(x_{t}) - f(x^*) \\leq O(\\frac{\\sigma_1 R}{\\sqrt{t}} + \\frac{\\sigma_2 R^2}{t^2} + \\frac{L_2 R^3}{t^3} + \\frac{\\sigma_1 R}{\\sqrt{t}})$\n\nBy choosing $\\tau = \\frac{c}{t^{5/2}}$, we get for any $t \\geq 1$\n\n$f(x_{t}) - f(x^*) \\leq O(\\frac{\\sigma_1 R}{\\sqrt{t}} + \\frac{\\sigma_2 R^2}{t^2} + \\frac{L_2 R^3}{t^3} + \\frac{c R}{t^3}) $\n\n*References*\n\n[1] Agafonov, Artem, et al. \"Inexact tensor methods and their application to stochastic convex optimization.\" arXiv preprint arXiv:2012.15636 (2020).\"*"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5556/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700004454843,
                "cdate": 1700004454843,
                "tmdate": 1700044280205,
                "mdate": 1700044280205,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bpoL0QrVaF",
                "forum": "otU31x3fus",
                "replyto": "gN1HADAHeR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5556/Reviewer_xECU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5556/Reviewer_xECU"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. Regarding weakness 4 some suggestions might be \"worst-case analysis\" or \"Theoretical Complexity Lower Bounds\".\n\nThe term lower bound is used throughout the paper. Maybe just mention at the beginning that for brevity you abbreviate theoretical complexity lower bound with simply lower bound. Other points are clarified, but they were minor issues. Thus, I keep my score unchanged. Thanks!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5556/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700408623895,
                "cdate": 1700408623895,
                "tmdate": 1700408623895,
                "mdate": 1700408623895,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "XqVEWIEUj6",
            "forum": "otU31x3fus",
            "replyto": "otU31x3fus",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5556/Reviewer_2pJU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5556/Reviewer_2pJU"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an accelerated stochastic second-order algorithm using inexact gradients and Hessians, demonstrating nearly optimal convergence rates."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper proposes an accelerated stochastic second-order algorithm using inexact gradients and Hessians, demonstrating nearly optimal convergence rates."
                },
                "weaknesses": {
                    "value": "see questions."
                },
                "questions": {
                    "value": "1) The paper lacks an intuitive explanation of the proposed algorithm. It is suggested to explain the motivation and impact of parameters such as $\\alpha_t$, the choice of the model $\\omega_x^{M,\\bar{\\delta}}$, and the technique of estimating sequence on the algorithm's performance and convergence. \n2) When comparing computational time, is the proposed algorithm better than existing methods?\n3) The figures in the paper are too small, making it difficult for readers to interpret the results."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5556/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5556/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5556/Reviewer_2pJU"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5556/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698603551522,
            "cdate": 1698603551522,
            "tmdate": 1699636571336,
            "mdate": 1699636571336,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qDhMsqW0fL",
                "forum": "otU31x3fus",
                "replyto": "XqVEWIEUj6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5556/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5556/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your helpful feedback and thoughtful inquiries!\n\n>**Question 1:** *\u201cThe paper lacks an intuitive explanation of the proposed algorithm. It is suggested to explain the motivation and impact of parameters such as \u03b1t, the choice of the model $\\omega_{x, M}^{\\bar \\delta}$ and the technique of estimating sequence on the algorithm's performance and convergence.\u201d*\n\n**Response:** Kindly examine the common commentary titled \"On intuition behind the algorithm.\" \n\n>**Question 2:** *\u201cWhen comparing computational time, is the proposed algorithm better than existing methods?\u201d*\n\n**Response:** In computational time comparisons, the efficiency is often contingent on the implementation rather than solely on the algorithms themselves. The relative speed of the proposed algorithm compared to existing methods can vary depending on the specific implementation and the characteristics of the minimization problem. To ensure a fair evaluation, we have presented our results in terms of iteration complexity. \n\nWhen comparing our implementations of SGD, ExtraNewton (EN), and Accelerated Stochastic Cubic Newton (ASCN), all implemented using PyTorch 2.0, in terms of computation time, we observe a slight advantage of ASCN over EN. Notably, both ASCN and EN outperform SGD in scenarios with small Hessian batch sizes ranging from 150 to 1000. We are open to incorporating these additional experiments into the revised version of the paper.\n\n>**Question 3:** *\u201cThe figures in the paper are too small, making it difficult for readers to interpret the results.\u201d*\n\n**Response:** Thank you for your feedback regarding the figures in our paper. We acknowledge your concern about their size impacting the ease of interpretation for readers. In the revised version, we will try to address this by resizing the figures for better visibility and comprehension. However, it's essential to note that we are constrained to 9 pages, and any adjustments to figure sizes may necessitate the removal of other elements from the work to maintain brevity. Alternatively, we are open to adding figures from Section 7 to the Appendix, presenting them with increased size as shown in Figures 3 and 4. Your further suggestions on this ma tter are appreciated."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5556/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700003791009,
                "cdate": 1700003791009,
                "tmdate": 1700003791009,
                "mdate": 1700003791009,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "mbXMgfWWEp",
                "forum": "otU31x3fus",
                "replyto": "XqVEWIEUj6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5556/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5556/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 2pJU,\n\nWe have submitted a revised version of our work. In this iteration, we have made several enhancements to address your feedback.\n\nFirstly, we have included running time experiments in Appendix C (Figures 7 and 8). Our findings indicate that. in a setup with a gradient batch size of 10,000, and Hessian batch sizes of 150 and 450, Accelerated Stochastic Cubic Newton outperforms both Extra-Newton and SGD.\n\nAdditionally, we have increased the font size for Figures 1 and 2. To provide further clarity and accessibility, we have added a larger version of these graphics to Appendix C (Figures 5 and 6).\n\nFurthermore, we have included a discussion on the intuition behind our method to Appendix B.\n\nWe appreciate your valuable insights. Thank you for your time and consideration."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5556/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681797038,
                "cdate": 1700681797038,
                "tmdate": 1700681907562,
                "mdate": 1700681907562,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yze1PETABz",
            "forum": "otU31x3fus",
            "replyto": "otU31x3fus",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5556/Reviewer_S1GY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5556/Reviewer_S1GY"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a new accelerated stochastic second-order method that is robust to both gradient and Hessian inexactness, typical in machine learning. It looks to achieve the lower bounds."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "None"
                },
                "weaknesses": {
                    "value": "The algorithm of step 4 also includes another optimization problem."
                },
                "questions": {
                    "value": "How does the algorithm work in step 4 of the proposed algorithm?  Does it work in practice?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5556/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5556/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5556/Reviewer_S1GY"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5556/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698746178781,
            "cdate": 1698746178781,
            "tmdate": 1699636571232,
            "mdate": 1699636571232,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qb08CiE364",
                "forum": "otU31x3fus",
                "replyto": "yze1PETABz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5556/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5556/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">**Weaknesses :** *\"The algorithm of step 4 also includes another optimization problem.\"*\n\n>**Questions:** *\"How does the algorithm work in step 4 of the proposed algorithm? Does it work in practice?\"*\n\n\n**The subproblem in line 4 of Algorithm 1 admits a closed form solution**, \nso it cannot be considered as a separate optimization problem. We apologize for not presenting it explicitly in the paper. Allow us to rectify that here.\n\nFirst, note that the problem in line 4 is convex. Let us write optimality condition\n$$0 = \\nabla \\psi_t(y_t) = (\\lambda_{t}  + \\kappa_2^{t} + \\kappa_3^{t} ||y_{t} - x_0||)(y_{t} - x_0) + \\sum \\limits_{j=0}^{t-1} \\frac{\\alpha_j}{A_j} g(x_{j+1}).$$\nThus,\n$$(\\lambda_{t}  + \\kappa_2^{t} + \\kappa_3^{t} ||y_{t} - x_0||)||y_{t} - x_0||  =  || \\sum \\limits_{j=0}^{t-1}\\frac{\\alpha_j}{A_j} g(x_{j+1})||.$$\nLet us denote $r_{t} = ||y_{t} - x_0||$ and $S_t = || \\sum \\limits_{j=0}^{t-1}\\frac{\\alpha_j}{A_j} g(x_{j+1})||$. Then, we get\n$$\\kappa_3^{t} r_{t}^2 + (\\lambda_{t}  + \\kappa_2^{t})r_{t} - S_t = 0.$$\nNext, we get the solution of quadratic equation\n$$r_{t} = \\frac{\\sqrt{(\\lambda_{t}  + \\kappa_2^{t})^2 + 4 \\kappa_3^{t}S_t } - (\\lambda_{t} + \\kappa_2^{t} )}{2\\kappa_3^{t}}.$$\nFinally, we get explicit solution\n\n$$y_{t} = x_0 - \\frac{\\sum \\limits_{j=0}^{t-1}\\frac{\\alpha_j}{A_j} g(x_{j+1}) }{\\lambda_t + \\bar{\\kappa}_2^t + \\bar{\\kappa}_3^t r_t}.$$\n\nWe will add this derivation to the revised version of the paper.\n\n\nFurthermore, we would like to point out that *solving minimization subproblems are in the heart of  algorithm design*. For example, even the iconic gradient descent scheme stems directly from minimizing quadratically regularized 1st order Taylor approximation sub-problem:\n$$x_{k+1} = \\arg \\min_x  (f(x_k) + \\langle \\nabla f(x_k), x- x_k\\rangle + \\frac{1}{2\\gamma}||x - x_k||^2). $$\n Gradient descent allows for closed form solution as a subproblem in line 4 in Algorithm 1 . Subproblems like line 4 in Algorithm 1 naturally arise from acceleration techniques. See line 1 of (4.8) from [1], lines 6-7 of Algorithm 1 from [2].  Furthermore, the absence of a closed-form solution for a subproblem should not be viewed as a deterrent when considering a specific optimization method, such as Mirror-Prox [3], Frank-Wolfe methods [4, 5], or Trust-Region methods [5].\n\n**Practical performance**\n\n\nAs we have established, the practical performance of subproblem 4 is not an issue. We use the explicit solution in our implementation of the algorithm. We provide numerical evidence for the overall algorithm in section 7. The proposed method works well in practice, one can see the behavior on Figures 1, 2.\n\n\n*Having outlined the specifics of the subproblem in line 4 of Algorithm 1 along with its explicit solution, we wonder whether this might be the reason for the pronounced strong rejection.*\n\n[1] Nesterov, Yu. \"Accelerating the cubic regularization of Newton\u2019s method on convex problems.\" Mathematical Programming 112.1 (2008): 159-181.\n\n[2] Allen-Zhu, Zeyuan, and Lorenzo Orecchia. \"Linear coupling: An ultimate unification of gradient and mirror descent.\" arXiv preprint arXiv:1407.1537 (2014).\n\n[3] Nemirovskij, Arkadij Semenovi\u010d, and David Borisovich Yudin. \"Problem complexity and method efficiency in optimization.\" (1983).\n\n[4] Jaggi, Martin. \"Revisiting Frank-Wolfe: Projection-free sparse convex optimization.\" International conference on machine learning. PMLR, 2013.\n\n[5] Nocedal, Jorge, and Stephen J. Wright, eds. Numerical optimization. New York, NY: Springer New York, 1999."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5556/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699867004329,
                "cdate": 1699867004329,
                "tmdate": 1699867004329,
                "mdate": 1699867004329,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LQz2F23yUJ",
            "forum": "otU31x3fus",
            "replyto": "otU31x3fus",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5556/Reviewer_KkLj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5556/Reviewer_KkLj"
            ],
            "content": {
                "summary": {
                    "value": "This paper present a stochastic second-order method  based on nesterov acceleration and cubic newton, which is proven to be robust to gradient and Hessian inexactness. The faster convergence rate of this algorithm is established for stochastic Hessian and inexact Hessian compared with previous work. The lower bound for the stochastic/inexact second-order methods for convex function with smooth gradient and Hessian is also established, verifying the tightness of their convergence upper bound. The inprecision produced by solving the cubic subproblem is also taken account of in the analysis. The method is also extended to higher-order minimization with stochastic/inexactness, and a restrated accelerated stochastic tensor method is also proposed for strongly-convex function."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The author of this article considers several interesting questions that naturally arise for the inexactness in practice, and are therefore valuable, especially the proposed algorithm and its tight convergence rate, and the lower bounds for inexact second-order methods. \n2. The article follows a natural logic in exploring the questions,  progressing in a layered manner.\n3. The proof techniques the authors used seem solid and sound."
                },
                "weaknesses": {
                    "value": "1. While I grasp the overarching concept of the algorithm and the principal steps in the proof, the exposition doesn't offer much in the way of intuitive understanding. Could the authors elucidate the rationale behind the algorithm's design and the parameter choices? Enhancing the exposition with additional intuitive insights into the algorithm would be highly beneficial.\n2. The assumptions (2.2 and 2.3) of stochacity and inexactness differ but seem highly related, as they lead to two quite similar convergence rates and proof for your algorithm. In this paper the way of discussing the stochasticity and inexactness settings seems a bit nesting.  Maybe it could be better if their highlevel relations  are set forth in a proper manner. \n3. Clarification: O(1/T^2) ->\\Omega(1/T^2) in P2 line 2; bounded variance stochastic Hessian ->stochastic Hessian with bounded variance; formatting issues such as sequence numbers in the algorithm list."
                },
                "questions": {
                    "value": "1. In section 3, the subproblem you defined is $\\omega_{x}^{M,\\bar{\\delta}}=f\\left(x\\right)+\\left\\langle g\\left(x\\right),y-x\\right\\rangle +\\frac{1}{2}\\left\\langle y-x,H\\left(x\\right)\\left(y-x\\right)\\right\\rangle +\\frac{\\bar{\\delta}}{2}\\left\\Vert x-y\\right\\Vert ^{2}+\\frac{M}{6}\\left\\Vert x-y\\right\\Vert ^{3}$. Compared to the original cubic regularized subproblem, in addition to the modification of the inexactness and stochasticity, your formulation has an additional quadratic term $\\frac{\\bar{\\delta}}{2}\\left\\Vert x-y\\right\\Vert ^{2}$. Are there any reason or intuition for this term?\n\n2. Could you bring more insights for the aggregation of stochastic linear models above algorithm 1?\n\n3. The gloabal convergent (accelerated, cubic newton type) second-order methods, while they have accelerated global convergence, they usually can be proven to have superlinear local convergence rate. Is the parallel local characteristic worth to be mentioned and investigated in your stochastic/inexact setting?\n\n4. The open question mentioned, \"what's the optimal trade-off between inexactness in gradients and the Hessian?\", where in the article is intuitively investigated? \n\n5. Regarding Algorithm 3, you mentioned the 'Restarted Accelerated Stochastic Tensor Method'. Could you further elaborate on the specific mechanism and necessity of this 'restarting'? Under what circumstances should a restart be performed?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5556/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5556/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5556/Reviewer_KkLj"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5556/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698930072702,
            "cdate": 1698930072702,
            "tmdate": 1700747019934,
            "mdate": 1700747019934,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3YuYxwBqpO",
                "forum": "otU31x3fus",
                "replyto": "LQz2F23yUJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5556/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5556/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your insightful feedback and valuable questions!\n\n>**Weakness 1:** *\u201cWhile I grasp the overarching concept of the algorithm and the principal steps in the proof, the exposition doesn't offer much in the way of intuitive understanding. Could the authors elucidate the rationale behind the algorithm's design and the parameter choices? Enhancing the exposition with additional intuitive insights into the algorithm would be highly beneficial.\u201d*\n\n**Response:**  Please review the common commentary \u201cOn the intuition behind the algorithm\u201d. \n\n>**Weakness 2:** *\u201cThe assumptions (2.2 and 2.3) of stochacity and inexactness differ but seem highly related, as they lead to two quite similar convergence rates and proof for your algorithm. In this paper the way of discussing the stochasticity and inexactness settings seems a bit nesting. Maybe it could be better if their highlevel relations are set forth in a proper manner.\u201d*\n\n**Response:** \nWe appreciate your observation regarding the nesting of Assumptions (2.2 and 2.3) and their apparent similarities in leading to analogous convergence rates and proof for our algorithms. While we recognize their close relationship, it's crucial to emphasize a fundamental distinction between the two.  Assumption 2.2 imposes a bound on the expected norm of the difference between the Hessian and the stochastic Hessian, whereas Assumption 2.3 pertains to the difference along the direction $y - x$. Consequently, 2.3 is necessary only for iterations of the Algorithm, and the value of $\\delta_2^{x, y}$ can be significantly smaller than the norm of the difference between the Hessian and its approximation. This delineation underscores why we present both assumptions and provide separate theoretical results for each. If you believe that presenting both assumptions is unnecessary, we welcome further discussion on this matter.\n\n>**Weakness 3:** *\u201cClarification: O(1/T^2) ->\\Omega(1/T^2) in P2 line 2; bounded variance stochastic Hessian ->stochastic Hessian with bounded variance; formatting issues such as sequence numbers in the algorithm list.\u201d*\n\n**Response:**  Thank you for bringing that to our attention; we will make the necessary changes to the paper in the revised version.\n\n>**Question 1:** *\u201cIn section 3, the subproblem you defined is $\u03c9_{x, M}^{\\bar \u03b4}(y)=f(x)+\u27e8g(x),y\u2212x\u27e9+\\frac{1}{2}\u27e8y\u2212x,H(x)(y\u2212x)\u27e9+\\frac{\\bar \u03b4}{2}\u2016x\u2212y\u2016^2+\\frac{M}{6}\u2016x\u2212y\u2016^3$. Compared to the original cubic regularized subproblem, in addition to the modification of the inexactness and stochasticity, your formulation has an additional quadratic term $\\frac{\\bar \\delta}{2}\u2016x\u2212y\u2016^2$. Are there any reason or intuition for this term?.\u201d*\n\n**Response:** Please have a look at the part A of  common commentary \u201cOn the intuition behind the algorithm\u201d. \n\n>**Question 2:** *\u201cCould you bring more insights for the aggregation of stochastic linear models above algorithm 1?\u201d*\n\n**Response:** Please have a look at the part B of  common commentary \u201cOn the intuition behind the algorithm\u201d. \n\n>**Question 3:** *\u201cThe gloabal convergent (accelerated, cubic newton type) second-order methods, while they have accelerated global convergence, they usually can be proven to have superlinear local convergence rate. Is the parallel local characteristic worth to be mentioned and investigated in your stochastic/inexact setting?\u201d*\n\n**Response:** Thank you for your insightful commentary. \n\nWhile local convergence seems to be an interesting theoretical direction, it lies beyond the scope of this paper, which primarily concentrates on global convergence without assumptions on initial point. Local convergence normally requires some very restrictive assumptions on starting point and batch size (gradient accuracy) which are hard to satisfy in practice. \n\nIllustratively, consider the local superlinear convergence from [1], which requires $f(x_0) - f(x^*) \\leq \\frac{\\mu^3}{L_2^2}$. In most practical scenarios desired accuracy $\\varepsilon \\lesssim  \\frac{\\mu^3}{L_2^2}$. \n\nIn summary, while the local convergence of stochastic/inexact accelerated second-order/high-order methods remains an open question, we believe that exploring this aspect may not make the proposed scheme much more practical. Consequently, we maintain our focus on global performance within the defined scope of this work.\n\n>**Question 4:** *\u201cThe open question mentioned, \"what's the optimal trade-off between inexactness in gradients and the Hessian?\", where in the article is intuitively investigated? \u201d*\n\n**Response:** We establish the lower bound $\\Omega(\\frac{\\sigma_1 R}{\\sqrt{T}} + \\frac{\\sigma_2 R^2}{T^2} + \\frac{L_2 R^{3}}{T^{7/2}})$, which shows that the best possible tradeoff between inexactness is $\\Omega(\\frac{\\sigma_1 R}{\\sqrt{T}} + \\frac{\\sigma_2 R^2}{T^2})$, which is achieved by proposed algorithm. \n\n[1] Kovalev, Dmitry, Konstantin Mishchenko, and Peter Richt\u00e1rik. \"Stochastic Newton and cubic Newton methods with simple local linear-quadratic rates.\" arXiv preprint arXiv:1912.01597 (2019)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5556/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700003282627,
                "cdate": 1700003282627,
                "tmdate": 1700044553510,
                "mdate": 1700044553510,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]