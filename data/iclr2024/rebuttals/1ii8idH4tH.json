[
    {
        "title": "Simple Minimax Optimal Byzantine Robust Algorithm for Nonconvex Objectives with Uniform Gradient Heterogeneity"
    },
    {
        "review": {
            "id": "vUR6ZHDfKM",
            "forum": "1ii8idH4tH",
            "replyto": "1ii8idH4tH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2284/Reviewer_uTUe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2284/Reviewer_uTUe"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new Byzantine robust algorithm called Momentum Screening (MS) for nonconvex federated learning. MS uses a simple screening test to detect and remove potentially malicious gradients from Byzantine workers. The remaining gradients are aggregated using standard momentum SGD. The algorithm is adaptive to the Byzantine fraction $\u03b4$.\n\nThis paper gives theoretical analysis on the proposed algorithm, showing that MS achieves an optimization error of O($\u03b4^2 \u03b6^2_{max}$) for the class $C_{UH}(\\zeta_{max})$. A matching minimax lower bound is also provided for $C_{UH}(\\zeta_{max})$. This rate differs from the rate $O(\\delta \\zeta_{mean}^2)$ for the class $C_{MH}(\\zeta_{mean})$.\n\nExperiments on MNIST and CIFAR10 with neural nets demonstrate MS outperforms existing methods like Centered Clipping, especially for small $\u03b4$. This aligns with the better dependence on $\u03b4$."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed algorithm (MS) is simple to implement yet performs well. It also does not need to know the Byzantine fraction $\\delta$ in advance, which is practical.\n1. The rate of convergence of MS is better than the previous best known rate of $O(\u03b4\u03b6^2_{mean})$ under the (corrected) condition when $\\delta \\leq (\\zeta_{mean}/\\zeta_{max}/)^2$, so the method is preferred if Byzantine workers are very few.\n1. The author also provide a computationally efficient algorithm for their proposed MS method, whose performance is only worse than the original one by a constant factor.\n1. Some experiments on MNIST and CIFAR10 with neural nets shows that MS outperforms other method."
                },
                "weaknesses": {
                    "value": "1. In the literature the rate $O(\u03b4\u03b6^2_{mean})$ is derived from $C_{MH}(\\zeta_{mean})$, while this paper gives the rate O($\u03b4^2 \u03b6^2_{max}$) for $C_{UH}(\\zeta_{max})$. Now, to give a better rate, one needs\n$$\u03b4^2 \u03b6^2_{max} \\leq \u03b4\u03b6^2_{mean}  \\Leftrightarrow \\delta \\leq (\u03b6_{mean} / \u03b6_{max})^2, $$\nwhere the RHS is $\\leq 1$ since $ \u03b6_{max} \\geq \u03b6_{mean}$. \nTherefore, **the requirement of $\\delta$ is wrong throughout the paper** (the authors give $\\delta \\leq ( \u03b6_{max}/ \u03b6_{mean})^2$).\nThe authors even did not notice this mistake when they write $\\delta = \\Omega(1)$ (in Section 7) but in fact Byzantine fraction  $\\delta < 0.5$.\nSuch mistake makes me doubt the correctness of the proof in this paper, but I do not have enough time to check the whole proof.\n\n1. As argued in this paper, $ \u03b6_{max} \\gg \u03b6_{mean} $, meaning that the method is only favourable when $\\delta$ is very small, which seems to be not practical in the Byzantine workers setting. Moreover, since $C_{UH}(\\zeta_{max})$ and $C_{MH}(\\zeta_{mean})$ are different hypothesis classes, directly comparing rates seems to be improper. An analysis of MS in $C_{MH}(\\zeta_{mean})$ is also needed.\n\n1. Although the hyperparameter $\\tau_t$ is adaptive to the Byzantine fraction $\\delta$, it has to be be chosen according to $\\zeta_{max}$, which is unknown in priori, so an inproper choice of $\\tau$ could harm the performance of the algorithm. \nIt would be favourable to provide an empirical way to choose $\\tau_t$.\n\n1. For the presentation of the paper, it would be clearer if the author provides a sketch of the proof rather than presenting directly some propositions."
                },
                "questions": {
                    "value": "1. Could the authors comment more on the relation between $\\zeta_{max}$ and $\\zeta_{mean}$, particularly with some real datasets?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2284/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2284/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2284/Reviewer_uTUe"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2284/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698304484337,
            "cdate": 1698304484337,
            "tmdate": 1700656049848,
            "mdate": 1700656049848,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ViBDujBV3C",
                "forum": "1ii8idH4tH",
                "replyto": "vUR6ZHDfKM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2284/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2284/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer uTUe"
                    },
                    "comment": {
                        "value": "Thank you for your important comments. \n\n**About Weakness 1**.\n\nAs you pointed out, the expression $\\delta \\leq (\\zeta_\\mathrm{max}/\\zeta_\\mathrm{mean})^2$ should be fixed as $\\delta \\leq (\\zeta_\\mathrm{mean}/\\zeta_\\mathrm{max})^2$ (please check the revision paper). We thank you for your pointing out these typos and apologize for them, but, they are simple systematic typos and do not affect the proofs given in Sections A and B.  \n\n**About Weakness 2 and Question 1**\n\nWe believe that $\\delta \\leq (\\zeta_\\mathrm{mean}/\\zeta_\\mathrm{max})^2$ is reasonable because ***$\\zeta_\\mathrm{mean}/\\zeta_\\mathrm{max}$ is not too small in practice***. In fact, after reading your review, we conducted additional experiments to address the concern that $\\zeta_\\mathrm{mean}/\\zeta_\\mathrm{max}$ is very small in practice. Specifically, we examined the empirical values of $\\zeta_\\mathrm{mean}/\\zeta_\\mathrm{max}$ on the settings of Section 6 and additionally on Fed-EMNIST, which is a real FL dataset. The experimental results can be found in Section E of the revised paper (p.36). From Figure 14 in Section E, we can see that ***$\\zeta_\\mathrm{mean}/\\zeta_\\mathrm{max}$ was in $0.5 \\sim 0.93$ on MNIST and CIFAR10, and was in $0.34 \\sim 0.70$ on Fed-EMNIST***. From these observations, we conclude that the condition ***$\\delta \\leq (\\zeta_\\mathrm{mean}/\\zeta_\\mathrm{max})^2$ is practical enough and the benefits of MS are not so limited***.  \n\n> Moreover, since $\\mathcal C_\\mathrm{UH}(\\zeta_\\mathrm{max})$\n and $\\mathcal C_\\mathrm{MH}(\\zeta_\\mathrm{mean})$\n are different hypothesis classes, directly comparing rates seems to be improper. An analysis of MS in $\\mathcal C_\\mathrm{MH}(\\zeta_\\mathrm{mean})$\n is also needed.\n\nWe disagree with the first point. In general, it is very common in the machine learning and optimization literature to show that an algorithm achieves a better rate (generalization error, optimization error, etc.) for a smaller hypothesis class than the rate of another algorithm targeting a larger hypothesis class. For example, it is well known that a typical theory of LASSO shows that LASSO achieves a better generalization error than OLS when the true parameter is sparse, where LASSO assumes a smaller hypothesis class than OLS does. \n\nRegarding the second point, we agree that an analysis of MS in $\\mathcal C_\\mathrm{MH}(\\zeta_\\mathrm{mean})$ may be necessary to judge which algorithm is better in $\\mathcal C_\\mathrm{MH}(\\zeta_\\mathrm{mean})$. However, our main focus is to construct a minimax optimal algorithm for the hypothesis class $\\mathcal C_\\mathrm{UH}(\\zeta_\\mathrm{max})$, and as shown in Figure 14 in Section E of the revised paper, it is empirically justified to assume that the local objectives are in $\\mathcal C_\\mathrm{UH}(\\zeta_\\mathrm{max})$ for a reasonable $\\zeta_\\mathrm{max}$ because $\\zeta_\\mathrm{max} \\not \\gg \\zeta_\\mathrm{mean}$ empirically. Thus, this point does not detract from the importance of this study. \n\n**About Weakness 3**.\n\nIn our experiments, as described in Section D.2, we used a heuristic strategy to stabilize the performance that $\\tau_t$ was increased by $1.5$ times from the original $\\tau_t$ as long as $|\\hat {\\mathcal G}| < n/2$, although  $\\tau_\\infty$, which is the limit of $\\tau_t$ ($t \\to \\infty$) , had to be roughly tuned. As you said, it is practically desirable that the algorithm be adaptive not only to Byzantine fraction $\\delta$ but also to heterogeneity $\\zeta_\\mathrm{max}$ (or $\\zeta_\\mathrm{mean}$). This direction is definitely an important future work. \n\n**About Weakness 4**.\n\nThanks for your suggestion. Actually, we give a brief overview of the analysis in the first part of Section 4 to clarify our proof strategy. If that is not enough, we would like to add a more detailed sketch of the proof.\n\n\nWe would be very happy if your concerns were addressed and the score would be raised."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700063019554,
                "cdate": 1700063019554,
                "tmdate": 1700098354348,
                "mdate": 1700098354348,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Qchs7wemQz",
                "forum": "1ii8idH4tH",
                "replyto": "ViBDujBV3C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2284/Reviewer_uTUe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2284/Reviewer_uTUe"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. \nIt clarifies my particular concern on the quantity $\\zeta_{mean} / \\zeta_{max}$.\nI would like to raise my score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700655986320,
                "cdate": 1700655986320,
                "tmdate": 1700655986320,
                "mdate": 1700655986320,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dtunMNYo8y",
            "forum": "1ii8idH4tH",
            "replyto": "1ii8idH4tH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2284/Reviewer_BeSt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2284/Reviewer_BeSt"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the problem of federated learning with Byzantine workers who can send arbitrary responses to the central server. In the non-IID case where the local distributions of non-Byzantine workers are heterogeneous, the standard aggregations will fail empirically, as shown in previous works. In this paper, the authors developed a new, simple byzantine robust algorithm that have better minimax optimal optimization error compared to the best previous algorithm when the maximum gradient heterogeneity is not much larger than the average gradient heterogeneity, whose optimality in this parameter regime is demonstrated by establishing a lower bound result. Moreover, the authors conducted numerical experiments to support their theoretical analysis."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The algorithm is novel and simple, makes it relatively easy to be implemented in practice. Moreover, the improvement in the minimax optimal optimization error is significant in the parameter regime where the maximum gradient heterogeneity is around the same order as the average gradient heterogeneity, which seems like a common assumption in various practical situations. The performance of the algorithm is also well demonstrated in the various numerical experiments."
                },
                "weaknesses": {
                    "value": "The convergence rate in terms of the number of steps $T$ might not be optimal. In particular, the algorithm is a momentum-based method, however, the convergence rate exhibits the form of a non-momentum based method, and it is unclear to me why the momentum is needed here."
                },
                "questions": {
                    "value": "Will the convergence rate of the algorithm remain unchanged if the momentum is removed? Or, is there a better momentum-based algorithm that has better convergence rate?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2284/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2284/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2284/Reviewer_BeSt"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2284/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698817816592,
            "cdate": 1698817816592,
            "tmdate": 1699636161131,
            "mdate": 1699636161131,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "53cSK6T7C3",
                "forum": "1ii8idH4tH",
                "replyto": "dtunMNYo8y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2284/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2284/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer BeSt"
                    },
                    "comment": {
                        "value": "Thank you for your insightful comments and questions. \n\n> The convergence rate in terms of the number of steps \n might not be optimal. In particular, the algorithm is a momentum-based method, however, the convergence rate exhibits the form of a non-momentum based method, and it is unclear to me why the momentum is needed here.\n\n> Will the convergence rate of the algorithm remain unchanged if the momentum is removed? Or, is there a better momentum-based algorithm that has better convergence rate?\n\nOur algorithm relies on a heavy-ball method rather than the famous Nesterov's acceleration method. Thus, it is natural that the convergence rate matches that of non-momentum based methods when $\\delta \\to 0$, since a heavy-ball method does not improve the convergence rate of non-momentum methods at least in the standard nonconvex optimization theory (see, for example,  Mai and Johansson, 2020 [1]). Although the main focus of this paper is on the asymptotic optimization error, obtaining the optimal convergence rate in terms of the number of steps $T$ based on Nesterov's acceleration is an interesting future direction. \n\nThe reason why we introduce the momentum in our algorithm is that ***the momentum mitigates the effect of the stochastic noise*** by canceling out the noise thanks to the accumulation of the previous stochastic gradients. This is very important when $\\delta > 0$ because the screening algorithm judges the workers to be non-Byzantine or Byzantine based on comparing the distance between the workers' outputs. If the momentum is removed, the screening algorithm must aggregate the stochastic gradients only at the current iteration, which will degrade the performance of the distance-based detection of the Byzantine workers due to the large  stochastic noise. As a result, ***the convergence rate will be degraded when the momentum is removed***.  \n\n[1] Mai and Johansson, 2020:  Convergence of a Stochastic Gradient Method with Momentum\nfor Non-Smooth Non-Convex Optimization."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700063015686,
                "cdate": 1700063015686,
                "tmdate": 1700063015686,
                "mdate": 1700063015686,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nyBGkmPWuY",
                "forum": "1ii8idH4tH",
                "replyto": "53cSK6T7C3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2284/Reviewer_BeSt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2284/Reviewer_BeSt"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for answering my questions, and I remain my rating."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700697270170,
                "cdate": 1700697270170,
                "tmdate": 1700697270170,
                "mdate": 1700697270170,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "AaPWvT7b7P",
            "forum": "1ii8idH4tH",
            "replyto": "1ii8idH4tH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2284/Reviewer_p51w"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2284/Reviewer_p51w"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies nonconvex federated learning (FL) in the presence of byzantine workers with a fraction of  $\\delta$ out of the workers. Then the authors proposed the Momentum Screening (MS) algorithm for such setting, achieving $O(\\delta^2 \\zeta^2_{max})$ error rate for $\\zeta_{max}$-uniform gradient heterogeneity, and showed the minimax optimality of the proposed method in such setting. Experimental results are then given to validate the MS algorithm."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The algorithmic structure of the MS algorithm is simple and can adapt to the Byzantine fractions $\\delta$, all of which can be practically attractive. Furthermore, the minimax optimality results seem like the first of its kind for such setting of $\\zeta_{max}$-uniform gradient heterogeneity."
                },
                "weaknesses": {
                    "value": "1. The consideration of algorithmic design for uniform gradient heterogeneity as in this paper has been done in the literature. In fact, the rate achieved here seems to be the same as the CCLIP method (Karimireddy et al. (2022)) (ref [1] as below for convenience). Yet, such literature was not well discussed enough in the paper. \n2. Following up the above point, many results in the paper are quite the same as those in CCLIP without improvement, and the analysis is quite natural and motivated from previous work. The true technical novelty of the paper, besides the MS method with simplicity, is perhaps the fact that they proved lower bound in the minimax sense for uniform gradient heterogeneity. However, this is quite a natural extension from the first work that proved such results for the case of mean gradient heterogeneity.\n3. Systematic typo throughout the paper: note that yours is better than CCLIP when $\\delta \\leq ( \\zeta_{mean}/ \\zeta_{max})^2$. Can you give a sense of what $\\zeta_{mean}/ \\zeta_{max}$ can be in real datasets, especially those considered in your experiments? Because I think such fraction can be very small in practice, which is also acknowledged in your Section 2.1. So the regime in which MS provides benefits is in fact quite limited. \n\n\n\n\n\n[1] https://arxiv.org/pdf/2006.09365.pdf"
                },
                "questions": {
                    "value": "Please see weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2284/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699232822799,
            "cdate": 1699232822799,
            "tmdate": 1699636161068,
            "mdate": 1699636161068,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "66PbVHMCWL",
                "forum": "1ii8idH4tH",
                "replyto": "AaPWvT7b7P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2284/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2284/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer p51w"
                    },
                    "comment": {
                        "value": "Thank you for your helpful feedback. \n \n**About Weaknesses 1 and 2**.\n\nFirst of all, we want to emphasize that the most critical difference of our study from CCLIP paper [1] is the improvement of the optimization error; ***our obtained error is $\\delta$ times smaller than that of CCLIP in the best case*** (i.e., $\\zeta_\\mathrm{max} \\approx \\zeta_\\mathrm{mean}$). Since $\\delta$ is often a small value in distributed learning systems, this improvement is quite important from both a theoretical and a practical point of view. The condition $\\delta \\leq \\zeta_\\mathrm{mean} / \\zeta_\\mathrm{max}$ holds empirically in our experiments (please see the comments to ``About Weakness 3'').  \n\nBelow, we summarize the main differences between our Momentum Screening (MS) and CCLIP [1] that are mentioned in our paper.\n+ Algorithmically, as shown in Section 3, our algorithm relies on the screening technique rather than clipping, which is critical for our theoretical analysis.    \n+ Theoretically, as described in Section 1 (Main contribution and Related work) and Section 4 (Remark 2), the optimization error can be $\\delta$ times better than that of CCLIP. \n+ Empirically, as provided in Section 7, we demonstrated the consistent superiority of our method over CCLIP in various numerical experiments.\n\nAlso, the derivation and the results of the aggregation error bound (Proposition 2) and the momentum diameter bound (Proposition 3) under the uniform gradient heterogeneity condition is the key technical part of our analysis, and is clearly different from the analysis of CCLIP, where only the mean gradient heterogeneity condition is assumed and the aggregation error bound is $\\delta$ times worse than ours due to the clipping bias. \n\n**About Weakness 3**.\n\nFirst, the expression $\\delta \\leq (\\zeta_\\mathrm{max}/\\zeta_\\mathrm{mean})^2$ was fixed in the revised paper as $\\delta \\leq (\\zeta_\\mathrm{mean}/\\zeta_\\mathrm{max})^2$. The typos do not affect the proofs given in Sections A and B.  We thank you for pointing this out and apologize for these typos. \n\nSecond, regarding your concern, we recognize that empirical validation of the condition $\\delta \\leq (\\zeta_\\mathrm{mean}/\\zeta_\\mathrm{max})^2$ is critical in our study. After reading your review, we examined the empirical values of $\\zeta_\\mathrm{mean}/\\zeta_\\mathrm{max}$ on the settings of Section 6 and additionally on Fed-EMNIST, which is known as a FL dataset for a realistic situation. From Figure 14 in Section E of the revision paper (p.36), we can see that ***$\\zeta_\\mathrm{mean}/\\zeta_\\mathrm{max}$ was in $0.5 \\sim 0.93$ on MNIST and CIFAR10, and was in $0.34 \\sim 0.70$ on Fed-EMNIST***. Thus, we conclude that the condition ***$\\delta \\leq (\\zeta_\\mathrm{mean}/\\zeta_\\mathrm{max})^2$ is practical enough and the benefits of MS are not so limited***.  \n\nWe would be very happy if your concerns were addressed and the score would be raised."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2284/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700063009825,
                "cdate": 1700063009825,
                "tmdate": 1700063009825,
                "mdate": 1700063009825,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]