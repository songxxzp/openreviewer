[
    {
        "title": "Sudden Drops in the Loss: Syntax Acquisition, Phase Transitions, and Simplicity Bias in MLMs"
    },
    {
        "review": {
            "id": "h4GN7UKjBj",
            "forum": "MO5PiKHELW",
            "replyto": "MO5PiKHELW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3867/Reviewer_uEHm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3867/Reviewer_uEHm"
            ],
            "content": {
                "summary": {
                    "value": "This paper monitors masked language models\u2019 development of syntactic attention structure (SAS) in the attention pattern, grammar capability (measured by the BLiMP dataset) and the GLUE score. They find that\n\n- The model\u2019s grammar capability spikes right after the spike of the model\u2019s SAS score.\n- They measure the complexity of the model throughout the pretraining process, and claim that the trend is aligned with the Information Bottleneck Theorem.\n\nThey also use a \u201cregularization loss\u201d to interfere with the acquisition of SAS. They find that enhancing/suppressing SAS improves/harms the grammar capabilities of the model. \n\nThough the model is able to develop an \u201calternative strategy\u201d for acquiring the grammar capabilities when SAS is suppressed, they find that lifting the suppression of SAS during the \u201cphase transition\u201d leads to worse grammar capabilities, while lifting the suppression before the \u201cphase transition\u201d can result in better grammar capabilities. They discuss this phenomenon as related to the simplicity bias of models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. They empirically show the close relationship between the spike of SAS score, grammar capability, and GLUE score. This is interesting because it may validate the role of understanding syntactic structure for downstream tasks.\n2. This work provides many experimental results, which could be useful for better understanding of the model pretraining dynamics."
                },
                "weaknesses": {
                    "value": "# Main concerns:\n## 1. The causal relationship between SAS and BLiMP/GLUE scores\n\nI think even though the spike of the BLiMP/GLUE score follows closely after the spike of the SAS score, it is not well substantiated to say that SAS is necessary for the capabilities required for BLiMP and GLUE. The intervention experiment in Sec 4.2, 4.3 can not support the causal relationship between them either. If there is a latent factor X that causes the better SAS, BLiMP, GLUE scores, adding the regularization term may suppress that latent factor X in addition to the SAS score. In this case, suppressing SAS also leads to worse BLiMP and GLUE scores. So that SAS is necessary for BLiMP and GLUE is not the only explanation for the observation in Sec 4.3, 4.3.\n\n\n## 2. The arguments about the simplicity bias is not clear (to me)\n\nIt seems that this paper suggests that SAS indicates that the model suffers from some simplicity bias issue, which is counterintuitive to me. In general I think people use simplicity bias to explain some robustness issues because some spurious (unreliable/non-causal) features are simpler to learn than causal features, or say the model is doing some shortcut learning. However, it is hard to imagine that the syntactic structure is something the model shouldn\u2019t rely on to solve any NLP problem.\n\nI think to talk about simplicity bias, the author should be more clear about the definition of \u201csimplicity\u201d and provide more evidence that the model\u2019s prediction is really \u201cbiased\u201d by that specific \u201csimple\u201d feature.\n\n\n## 3. The motivation of the study\n\nIt\u2019s unclear to me why we should look at the development of these \u201ccapabilities\u201d. I would like to know how the findings in this paper can potentially direct future research directions?\n\nIn general I feel that it\u2019s cool that this paper uses some fancy techniques to show many findings and defines some interesting terminologies. However, it\u2019s unclear to me what the high-level message of this paper is. Unable to capture the coherent theme of this paper, I found it difficult to put all the information in this paper together.\n\n# Minor issues:\n\n1. This paper should be more specific about the definition of \u201ccapabilities\u201d."
                },
                "questions": {
                    "value": "## Q1: About section 4.1.1\n\nThe authors discuss their findings along with the information bottleneck (IB) theory. \n\n1. It\u2019s unclear to me how the findings agree with what part of the IB theory.\n2. It\u2019s also unclear to me how this is related to the findings or the arguments in this paper.\n\n## Q2:  The specific meaning of phase transition in Sec 4.2\n\nIn Sec 4.2, the term \u201cphase transition\u201d. Could you clarify what it refers to? Does it refer to the period between the structure onset and the capabilities onset.\n\n\n## Q3:  The importance of understanding phase transition in general\n\nI understand that *phase transition* is a *hot topic* for some model interpretability community. However, in this work, could you provide more context in which studying *phase transition* is important?\n\n## Suggestions\n\nI understand that every paper needs a reasonable scope to work on and I don\u2019t expect that one single paper explains everything. However, I would suggest that the authors scope this paper more explicitly. \u201cEmergence\u201d, \u201cphase transition\u201d and \u201ccapability\u201d, for example, I think are some very general terms, and this paper focuses only on some specific aspects of them. Scoping more clearly and explicitly in the introduction section will help readers (at least me) understand this paper more easily, especially when this paper is discussing MLM models while these terms are usually co-occur with autoregressive language models."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3867/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3867/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3867/Reviewer_uEHm"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3867/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697928575767,
            "cdate": 1697928575767,
            "tmdate": 1700626862770,
            "mdate": 1700626862770,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "L47GVzC1iE",
                "forum": "MO5PiKHELW",
                "replyto": "h4GN7UKjBj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3867/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3867/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your feedback! Revisions are in red and responses are below"
                    },
                    "comment": {
                        "value": "Thank you for volunteering the time and energy to review our paper \u2013 we appreciate your detailed feedback on places where our paper could be made more clear. Below are our responses to your concerns and questions.\n- \"it is not well substantiated to say that SAS is necessary for the capabilities required for BLiMP and GLUE. The intervention experiment in Sec 4.2, 4.3 can not support the causal relationship between them either. If there is a latent factor X that causes the better SAS, BLiMP, GLUE scores, adding the regularization term may suppress that latent factor X in addition to the SAS score. In this case, suppressing SAS also leads to worse BLiMP and GLUE scores. So that SAS is necessary for BLiMP and GLUE is not the only explanation for the observation in Sec 4.3, 4.3.\"\n    - Thank you for highlighting these subtleties of causal reasoning. Our regularization explicitly suppresses only the maximum attention weights associated with syntactic structure \u2013 if our suppression somehow also suppresses some other latent factor X, then X is likely associated with an internal representation of syntax. Furthermore, our manipulations of SAS consistently impact the timing of both the structure onset and capabilities onset (Figure 5; Figure 18, Appendix N) \u2013 *i.e.*, the BLiMP breakthrough always occurs shortly after the UAS spike (if the UAS spike occurs), regardless of whether we accelerate or delay the UAS spike using our SAS regularization. Taken together, our many experiments provide overwhelming evidence that SAS is causally related with the development of complex linguistic capabilities.\n    - It is difficult to completely eliminate the possibility of a latent cause, but our insights about the relationship between the timing of phase transitions and the interactions between different learning strategies hold regardless of whether the underlying cause is exactly SAS or another internal structure that is strongly associated with syntax.\n- \"In general I think people use simplicity bias to explain some robustness issues because some spurious (unreliable/non-causal) features are simpler to learn than causal features, or say the model is doing some shortcut learning. However, it is hard to imagine that the syntactic structure is something the model shouldn\u2019t rely on to solve any NLP problem. I think to talk about simplicity bias, the author should be more clear about the definition of \u201csimplicity\u201d and provide more evidence that the model\u2019s prediction is really \u201cbiased\u201d by that specific \u201csimple\u201d feature.\"\n    - Thank you for mentioning this point \u2013 we see how it may be unclear. It is true that many studies of simplicity bias employ experiments with heuristics that are not just simplistic, but also spurious. However, generally the definitions of simplicity bias characterize only the complexity of the learner and not its use of spurious features ([Dingle et al.](https://www.nature.com/articles/s41467-018-03101-6), [P\u00e9rez et al.](https://arxiv.org/abs/1805.08522), [Nakkiran et al.](https://arxiv.org/abs/1905.11604), [Shah et al.](https://proceedings.neurips.cc/paper/2020/hash/6cfe0e6127fa25df2a0ef2ae1067d915-Abstract.html)). It may so happen that these simplistic heuristics are \"shortcuts\" and therefore non-causal features, but this is not a requirement for the definition of simplicity bias. Instead, we are merely referring to the observation that *too much* focus on a simplistic strategy (even an effective, non-spurious one) may distract from a more nuanced and complex learning strategy. In some cases, this more complex strategy may simply add more features on top of the simplistic strategy, rather than replacing it.\n    - We have added a brief explanation at the end of Section 4.1 (in red) that also clarifies the definition of simplicity bias \u2013 thank you for helping us improve our paper. Appendix C.1 also expands upon the relevant prior work.\n   - As for evidence that the model's prediction is really biased, our aim is only to show that the model's learning is biased towards learning human interpretable artifacts such as SAS, not that the predictions are biased by SAS. We show that BERT-Base naturally learns SAS both without intervention (Fig. 1) and even after limited amounts of suppression (Section 4.3.1)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3867/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700604323073,
                "cdate": 1700604323073,
                "tmdate": 1700666900354,
                "mdate": 1700666900354,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WzBiMsskM7",
                "forum": "MO5PiKHELW",
                "replyto": "h4GN7UKjBj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3867/Reviewer_uEHm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3867/Reviewer_uEHm"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply. \n\nFor the first point, I totally agree your response. I would suggest you make your argument in this way in the paper. Though you claim that it's subtle, **apparently the argument here is different from the main argument in the paper**.\n\nFor the second point, after reading your elaboration you added at the end of Section 4.1, I think the argument is much clearer now.\n\nFinally, I think my questions were answered. I appreciate your response."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3867/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700626507779,
                "cdate": 1700626507779,
                "tmdate": 1700626842910,
                "mdate": 1700626842910,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "27MPVnFF79",
            "forum": "MO5PiKHELW",
            "replyto": "MO5PiKHELW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3867/Reviewer_4MRy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3867/Reviewer_4MRy"
            ],
            "content": {
                "summary": {
                    "value": "The research highlights that understanding model behavior requires observing the training process trajectory, not just analyzing a fully trained model. This study looks into syntax acquisition in masked language models, focusing on the Syntactic Attention Structure (SAS). It shows that SAS emerges suddenly during a specific pretraining phase, marked by a significant loss reduction. Further experiments manipulating SAS confirm its essential role in developing linguistic capabilities, whereas the experiments also find that briefly suppressing SAS improves model quality. The authors explain that SAS competes with other effective traits."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well-written and easy to follow. \n- The research idea and findings (including the appendices) are both intriguing and worthy of being shared with the community.\n- The experiments were conducted and executed effectively."
                },
                "weaknesses": {
                    "value": "- I didn't find any major weaknesses, just a few minor questions (detailed below).\n- Some individuals might express concerns that the experimental setup is somewhat minimal and may suggest the inclusion of additional elements, such as utilizing RoBERTa or evaluating the model on other, possibly more recent, benchmarks."
                },
                "questions": {
                    "value": "- I would like to know what kind of method/technique is used for encoding positional information. Is it the same as the original positional encoding in Vaswani et al. 2017 (https://arxiv.org/abs/1706.03762) or something more recent variants such as Rotary Positional Encoding (https://arxiv.org/abs/2104.09864v4)? I'm asking this because the syntactic dependency relates to positional information in the sentence. I wonder how much it affects (or does not affect) the experimental setup in this paper.\n\n- This is a more open-ended question but I wonder whether we can observe similar breakthrough (steep drop in loss) in auto-regressive (causal / decoder-only) LMs."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3867/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698768733279,
            "cdate": 1698768733279,
            "tmdate": 1699636345027,
            "mdate": 1699636345027,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2iUpuo3pv4",
                "forum": "MO5PiKHELW",
                "replyto": "27MPVnFF79",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3867/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3867/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review! Responses are below"
                    },
                    "comment": {
                        "value": "Thank you for your careful review \u2013 we appreciate that you also found these ideas interesting and worth sharing with the community. Below are our answers to your questions:\n\n- \"I would like to know what kind of method/technique is used for encoding positional information. Is it the same as the original positional encoding in Vaswani et al. 2017 (https://arxiv.org/abs/1706.03762) or something more recent variants such as Rotary Positional Encoding (https://arxiv.org/abs/2104.09864v4)? I'm asking this because the syntactic dependency relates to positional information in the sentence. I wonder how much it affects (or does not affect) the experimental setup in this paper.\"\n    - We use the same positional embeddings implementation as in the original BERT paper (Devlin et al., https://arxiv.org/abs/1810.04805) \u2013 i.e., fully trained absolute sinusoidal positional embeddings. It would definitely be interesting to explore how various positional embeddings affect the emergence of SAS and the subsequent capabilities onset. Although we were unable to re-run our entire suite of experiments with other positional encodings, there does exist limited evidence that language models learn internal syntax representations even without positional embeddings (such as in the ELMo model, in [Hewitt and Manning](https://aclanthology.org/N19-1419/)) and in decoder-only models with causal attention (such as in the GPT-2 model, in [Vig and Belinkov](https://arxiv.org/abs/1906.04284)). These syntax probes are not exactly the same as the one we use, but are likely related (especially the attention-based probe in [Vig and Belinkov](https://arxiv.org/abs/1906.04284)).\n- \"This is a more open-ended question but I wonder whether we can observe similar breakthrough (steep drop in loss) in auto-regressive (causal / decoder-only) LMs.\"\n    - This is an interesting question \u2013 there seems to be some empirical evidence that similar breakthroughs exist in auto-regressive LMs. For example, Fig. 2 in [Kaplan et al.](https://arxiv.org/pdf/2001.08361.pdf) shows initial steep loss drops in large decoder-only Transformers, and [Olsson et al.](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html#phase-change-more-closely) show that the steep loss drop in shallow attention-only models is associated with a break in in-context learning abilities and induction head formation."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3867/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700604493527,
                "cdate": 1700604493527,
                "tmdate": 1700604493527,
                "mdate": 1700604493527,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "y5d8QaevNy",
                "forum": "MO5PiKHELW",
                "replyto": "2iUpuo3pv4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3867/Reviewer_4MRy"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3867/Reviewer_4MRy"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response and references. They will be helpful for other readers, too."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3867/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700704140367,
                "cdate": 1700704140367,
                "tmdate": 1700704140367,
                "mdate": 1700704140367,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QEvAsnvxBM",
            "forum": "MO5PiKHELW",
            "replyto": "MO5PiKHELW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3867/Reviewer_cVcx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3867/Reviewer_cVcx"
            ],
            "content": {
                "summary": {
                    "value": "This paper analyzes sudden transitions in the training loss of BERT models, identifying two components to this drop: the development of attention patterns correlated with syntax (SAS), and the subsequent emergence of the ability to make grammaticality judgements. The paper then manipulates SAS via a additional term in the loss and analyzes the effect of these manipulations on the second component. The findings are twofold: (i) acquisition of SAS is a pre-requisite for the grammatical capabilities and (ii) briefly supressing SAS leads to a subsequent increase in grammatical capabilities."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This is one of relatively few papers which analyzes learning dynamics in BERT and the transitions identified are intriguing. The paper has the potential of stimulating more work in the same vein, applied to models more advanced than BERT."
                },
                "weaknesses": {
                    "value": "I did not find any major weaknesses in the paper. There is a lot going on, but this is understandable given the novelty of the approach. \n\nThat said, some of the framing and terminology could be explicated a bit more carefully. I found the issue of simplicity and simplicity bias especially muddled (see questions below)."
                },
                "questions": {
                    "value": "Do I understood correctly you equate the syntax-like attention patterns with simplicity bias, and at some point call them \"simple heuristics\" (section 5.1)?\nThis is a bit confusing as in the NLP literature terminology like \"simple heuristics\" refers to undesirable reliance on surface lexical patterns (like bigrams), and reliance on syntax is considered the opposite of a simple heuristic. It would be good to make sure your unusual framing is not a cause of confusion to the readers.\n\nMinor doubt: since you use WSJ data for testing, why use silver Stanford parser dependencies instead of gold, converted from the manually created trees?\n\nDo you have any inkling of what your mystery alternative strategy may involve?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3867/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698830577398,
            "cdate": 1698830577398,
            "tmdate": 1699636344959,
            "mdate": 1699636344959,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "648Jqx9Lww",
                "forum": "MO5PiKHELW",
                "replyto": "QEvAsnvxBM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3867/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3867/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review! Our revisions are in red and our responses are below"
                    },
                    "comment": {
                        "value": "Thank you for your feedback \u2013 we appreciate that you highlighted the novelty of the approach and the potential for stimulating more such work.\n\n- \"Do I understood correctly you equate the syntax-like attention patterns with simplicity bias, and at some point call them \"simple heuristics\" (section 5.1)? This is a bit confusing as in the NLP literature terminology like \"simple heuristics\" refers to undesirable reliance on surface lexical patterns (like bigrams), and reliance on syntax is considered the opposite of a simple heuristic. It would be good to make sure your unusual framing is not a cause of confusion to the readers.\"\n    - Thank you for mentioning this potential point of confusion! We have added a brief clarification at the end of Section 4.1 (in red) \u2013 it is true that the definition of \"simple\" is relative, and one might refer to bigram patterns as simple vs. syntax as complex. For our purposes, we refer to any human interpretable artifact as simple, since it must be simple enough to be understood. In addition, syntax patterns can still be considered linguistically simple because it considers only the surface form of language, and not the semantics.\n    - When we refer to \"simple heuristics,\" we are referring only to the relative functional complexity of the strategy, as compared to the complexity of all possible functions that a neural network could encode. This excludes whether the heuristic is a desirable strategy or not. Although many studies of simplicity bias do employ experiments with heuristics that are both simplistic and spurious (and therefore undesirable), generally the definitions of simplicity bias characterize only the complexity of the learner and not its use of spurious features ([Dingle et al.](https://www.nature.com/articles/s41467-018-03101-6), [P\u00e9rez et al.](https://arxiv.org/abs/1805.08522), [Nakkiran et al.](https://arxiv.org/abs/1905.11604), [Shah et al.](https://proceedings.neurips.cc/paper/2020/hash/6cfe0e6127fa25df2a0ef2ae1067d915-Abstract.html)). We have also referenced other work on simplicity biases in Appendix C.1.\n - \"Minor doubt: since you use WSJ data for testing, why use silver Stanford parser dependencies instead of gold, converted from the manually created trees?\"\n    - Thank you for pointing this out \u2013 this is a minor miswording on our part that we have corrected in our paper (in red). For UAS evaluation, we indeed used the manual syntax annotations on WSJ from [PTB-3](https://catalog.ldc.upenn.edu/LDC99T42), but we converted these annotations into Stanford Dependencies format using the [Stanford Dependencies converter](https://nlp.stanford.edu/software/stanford-dependencies.shtml). This was done in order to align with the procedure in [Clark et al.](https://arxiv.org/abs/1906.04341) and the associated [codebase](https://github.com/clarkkev/attention-analysis).\n- \"Do you have any inkling of what your mystery alternative strategy may involve?\"\n    - We explored a couple hypotheses about the alternative strategy in Appendix M. Unfortunately the evidence did not appear strong enough to include these results in the main text, but we found limited evidence to support the hypotheses that $BERT_\\text{SAS-}$ acquires unstructured n-gram statistics sooner than $BERT_\\text{Base}$ does (Appendix M, Fig. 15). We also found limited evidence that while $BERT_\\text{Base}$ appears to rely heavily on position information early on in training, $BERT_\\text{SAS-}$ may rely more on factors other than position (such as semantic information) (Appendix M, Fig. 16)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3867/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700605141648,
                "cdate": 1700605141648,
                "tmdate": 1700664599779,
                "mdate": 1700664599779,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8qQzDbu6uB",
                "forum": "MO5PiKHELW",
                "replyto": "lHywQTSLHE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3867/Reviewer_cVcx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3867/Reviewer_cVcx"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "After reading the other reviews and the authors' response my view of the paper is largely unchanged. I am keeping my recommendation."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3867/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740447964,
                "cdate": 1700740447964,
                "tmdate": 1700740447964,
                "mdate": 1700740447964,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sWqYlQ1CP1",
            "forum": "MO5PiKHELW",
            "replyto": "MO5PiKHELW",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3867/Reviewer_qPGn"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3867/Reviewer_qPGn"
            ],
            "content": {
                "summary": {
                    "value": "This paper provides a detailed _developmental_ account of (masked) language models' acquisition of grammatical abilities: the authors measure the the time at which (i) attention heads which pay attention to syntactic structure (dependencies, following prior work showing they emerge) and (ii) grammatical abilities (performance on BLiMP, a linguistic diagnostic set) emerge during training.  In particular, they find that (i) occurs reliably just prior to (ii), though both are abrupt, and that (i) occurs with a _sudden drop in the MLM loss_.  This suggests that the model reliably acquires a certain bit of important latent knowledge (of dependency structure) before behavioral evidence of a skill that uses that knowledge (e.g. grammaticality judgments).  The paper also explores regularization to promote or demote (i), with many, many interesting results about when the sudden drop in loss occurs and how it connects to downstream performance.  This kind of causal intervention shows that MLMs _do_ in fact use syntactic attention heads both when doing masked language modeling and grammaticality judgments, teaching us much more about these phenomena than existing probing methods based on static model artifacts."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- Very detailed analysis of the emergence of certain knowledge and skills _across training time_ in a language model.\n- Demonstrates that drops in loss correspond to acquisition of syntactic knowledge, which then translates to grammatical performance.\n- Methodologically and technically innovative (e.g. regularization as a causal intervention) in a way that moves the state of the probing field forward.\n- Extremely wide range of experiments, helping isolate exactly which features of training and measurement matter for this phenomena.  Crucially, they show that these emergence phenomena are not measurement artifacts, since they persist when a discrete scale (training time) is replaced with several continuous ones."
                },
                "weaknesses": {
                    "value": "- All of the results are on a single model architecture (BERT base).  On the one hand, this makes sense, since an extremely wide range of experiments are carried out.  On the other hand, we don't know whether the connection between sudden drops in the loss and syntactic knowledge would apply at larger scales, with causal language modeling, etc.\n- There are so many experiments and interesting observations that the main paper makes very frequent reference to a plethora of appendices for more detail.  This makes it a bit hard in places to figure out _exactly what_ is being reported and what it all means.  (E.g. the discussion of the Information Bottleneck was fairly hard to follow, even to someone who knows a bit about that literature.)"
                },
                "questions": {
                    "value": "- Fig 1b: why do you think there's so much more variance in the BLiMP results than in the loss curves and UAS scores?\n\n- I'm curious about whether it matters that silver dependencies were used in regularization.  Did you try any other \"data-free\" regularizers to see if they impact SAS similarly?  E.g. since each token has one head, a regularizer that promotes sparsity of attention should implicitly promote SAS as well and vice versa.\n\n- Missing references: (i) Liu et al 2021, \"Probing Across Time\": https://aclanthology.org/2021.findings-emnlp.71/ .  (ii) p 4: \"Causal methods...\" I have an idea of what works the authors have in mind, but think they should be explicitly cited here.\n\n- Will code and data be made publicly available?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "10: strong accept, should be highlighted at the conference"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3867/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698965616444,
            "cdate": 1698965616444,
            "tmdate": 1699636344871,
            "mdate": 1699636344871,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qog0lS9dIr",
                "forum": "MO5PiKHELW",
                "replyto": "sWqYlQ1CP1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3867/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3867/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review! Revisions are in red and responses are below"
                    },
                    "comment": {
                        "value": "Thank you for your review, we really appreciate that you also found the techniques innovative and the results interesting! Your feedback has also helped us clarify the writing in our paper.\n- \"All of the results are on a single model architecture (BERT base). On the one hand, this makes sense, since an extremely wide range of experiments are carried out. On the other hand, we don't know whether the connection between sudden drops in the loss and syntactic knowledge would apply at larger scales, with causal language modeling, etc.\"\n    - We agree it would be valuable follow-up work to study whether these connections continue to be true with both auto-regressive LMs and larger scales. Although we were unable to re-run our entire suite of experiments using larger or more varied types of language models (due to both time and compute constraints), there does exist limited evidence that causal language models also demonstrate syntactic attention patterns ([Vig and Belinkov](https://arxiv.org/abs/1906.04284)) and that syntax learning improves with scale ([P\u00e9rez-Mayos](https://arxiv.org/abs/2109.03160)). Based on these findings, it seems reasonable to speculate that syntactic attention patterns may exhibit a similarly close relationship to the initial loss drop as in BERT-Base. However, confirming this and carefully analyzing the dynamics of this relationship is work we leave for future research.\n- \"There are so many experiments and interesting observations that the main paper makes very frequent reference to a plethora of appendices for more detail. This makes it a bit hard in places to figure out exactly what is being reported and what it all means. (E.g. the discussion of the Information Bottleneck was fairly hard to follow, even to someone who knows a bit about that literature.)\"\n    - Thank you for this feedback, we have revised the Information Bottleneck section (Section 4.1) to try to simplify and clarify its connection to the rest of the paper. It's true that there are a lot of results in this paper, and we appreciate your feedback on how to clarify it all.\n- \"Fig 1b: why do you think there's so much more variance in the BLiMP results than in the loss curves and UAS scores?\"\n    - The confidence interval we computed for the BLiMP results in Fig. 1(b) is based on variance across different BLiMP challenges (mentioned in the caption). In general, it is common for models to vary in accuracy across the different challenges ([Warstadt et al.](https://arxiv.org/abs/1912.00582)), since each tests a very specific linguistic capability. The loss curve, on the other hand, has a confidence interval computed from the variance across different examples, which we expect to be lower since the model is solving the same task for each example (masked language modeling). The UAS scores do not have a confidence interval in this particular figure (but in Fig. 3 we include confidence intervals across the three model seeds) since the implementation from [Clark et al.](https://arxiv.org/abs/1906.04341) did not admit straightforward calculations of variance. Since each example may have different syntax relations, it would be difficult to interpret the variance of UAS across different examples.\n- \"I'm curious about whether it matters that silver dependencies were used in regularization. Did you try any other \"data-free\" regularizers to see if they impact SAS similarly? E.g. since each token has one head, a regularizer that promotes sparsity of attention should implicitly promote SAS as well and vice versa.\"\n    - We did at one point try a *uniform* regularizer that would uniformly regularize any attention weight corresponding to a pair of syntactically related tokens. Although this still required dependency relation labels (and is therefore not \"data-free\" as you asked about), it would still promote sparsity of attention for $\\lambda>0$. However, we found that this uniform regularizer did not suppress SAS as effectively as our proposed regularizer. This is likely because if we try to suppress sparsity with the uniform regularizer, the model can still achieve a low loss with a mostly uniform distribution that spikes only on a single attention weight in a specific head and layer between syntactically related pairs of tokens.\n- \"Missing references: (i) Liu et al 2021, \"Probing Across Time\": https://aclanthology.org/2021.findings-emnlp.71/ . (ii) p 4: \"Causal methods...\" I have an idea of what works the authors have in mind, but think they should be explicitly cited here.\"\n    - Thank you, we have added Liu et al. to our related work section in Appendix C.2. We have already cited multiple causal methods in App. C.2.1 but have also added these citations to the first paragraph of Section 4."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3867/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700605366644,
                "cdate": 1700605366644,
                "tmdate": 1700605366644,
                "mdate": 1700605366644,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]