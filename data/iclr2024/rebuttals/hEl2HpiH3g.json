[
    {
        "title": "FedJETs: Efficient Just-In-Time Personalization with Federated Mixture of Experts"
    },
    {
        "review": {
            "id": "6guvzdfYzk",
            "forum": "hEl2HpiH3g",
            "replyto": "hEl2HpiH3g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4174/Reviewer_Na6T"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4174/Reviewer_Na6T"
            ],
            "content": {
                "summary": {
                    "value": "The paper uses Mixture of Experts architecture with a gating function to select \"the most relevant\" experts for each client data \"just-in-time\" for federated learning. They also take advantage of a pretrained model as \"common expert\". The authors aim i) global generalization ii) enhance global model via personalized models ii) solve \"cold-start\" problem."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- It is good to see that the authors have used MoE for federated learning, differently from the FedMix paper.\n- Reducing communication costs is critical and very good.\n- Using anchor users seems useful."
                },
                "weaknesses": {
                    "value": "- The contribution lacks novelty, as using a gating function and common expert is not new. Even there are multi-gate mixture of experts architectures in the literature [1].\n- The architecture is similar to STAR model in paper [2] without anchor users.\n- There are couple of places mention that our approach lower communication costs but there is no experimental results that show that how much improvement is there as in FedMix paper.\n- There are no experiments for the cold-start problem as claimed in the paper ( this is not same as unseen new users for testing. Testing, of course, should be unseen). \n\n[1] Modeling Task Relationships in Multi-task Learning with\nMulti-gate Mixture-of-Experts (https://dl.acm.org/doi/pdf/10.1145/3219819.3220007)\n[2] One Model to Serve All: Star Topology Adaptive Recommender\nfor Multi-Domain CTR Prediction (https://dl.acm.org/doi/pdf/10.1145/3459637.3481941?casa_token=X928_yKMvcsAAAAA:WKNfD3i-ELk5CTxjIqs8t6MxMN0LSmwwhIvbEY7lvKaoqp8BC0zQdUOuZHXQKUkMUH1poak8ZFxZ)"
                },
                "questions": {
                    "value": "- The claim \"we partition the data samples by classes to turn full datasets into non-i.i.d. subsets\", how do you make sure that samples with different class labels with same data is non - i.i.d ? \n- This work also very similar to multi-task learning, one of the main problem is conflicting gradients. Since you claim the data is non i.i.d. have you ever encountered this problem as in these papers [3] [4]\n\n[3] MAMDR: A Model Agnostic Learning Framework\nfor Multi-Domain Recommendation (https://dl.acm.org/doi/pdf/10.1145/3459637.3481941?casa_token=X928_yKMvcsAAAAA:WKNfD3i-ELk5CTxjIqs8t6MxMN0LSmwwhIvbEY7lvKaoqp8BC0zQdUOuZHXQKUkMUH1poak8ZFxZ)\n[4] Gradient Surgery for Multi-Task Learning (https://proceedings.neurips.cc/paper/2020/file/3fe78a8acf5fda99de95303940a2420c-Paper.pdf)\n[5] Conflict-Averse Gradient Descent\nfor Multi-task Learning (https://proceedings.neurips.cc/paper_files/paper/2021/file/9d27fdf2477ffbff837d73ef7ae23db9-Paper.pdf)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4174/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4174/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4174/Reviewer_Na6T"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4174/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698347044708,
            "cdate": 1698347044708,
            "tmdate": 1699636383306,
            "mdate": 1699636383306,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WcREl80UWk",
                "forum": "hEl2HpiH3g",
                "replyto": "6guvzdfYzk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4174/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4174/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer Na6T"
                    },
                    "comment": {
                        "value": "We want to thank the reviewer for the detailed comments. Below, we respond to the points raised one by one. We hope our responses resolve further concerns and are available for other questions.\n \n-**Regarding lack of novelty:** We understand your point of view; however, we believe our paper has a different focus than the references pointed out in the following ways:\n\n  -*MMoe* [1]: [1] proposes to learn task relationships by sharing ALL expert models across ALL tasks while using different gating functions that learn task similarities across all the experts. FedJET's main advantage is that it dynamically selects the most relevant experts (according to the client distribution) and only sends these models across the network, avoiding the communication costs of sending all the experts as FedMix. On the other hand, FedJETs uses a single gating function to learn the differences across different domains. At the same time, MMoe utilizes multiple gates (one per task) that learn how to model the different relationships across the experts and weigh them so they can be used differently.\n\n  -*Star Topology* [2]: [2] proposes a method that constantly updates two models during training: a) Global model, which shares parameters across all personalized models, and b) In-domain personalized models (1 per domain). In FedJETs, the \"pretrained model\" remains frozen at all times, it is never updated during training or testing, and its only purpose is to serve as a feature extractor and feed its embeddings to the gating function so that it can rank the expert models. During inference, [2] combines these weighted networks and unifies them into a new one via an element-wise multiplication process. FedJETs ranks the subset of matching experts per sample level, capitalizing on a single expert more relevant to the sample distribution. Lastly, in [2], the domain ID needs to be explicitly fed into the network to facilitate the model learning; FedJETs dynamically identify the relevant experts by training the gating function.\n\n-**Regarding lower communication cost**:\nIn our experiments, we demonstrate how our method is more communication efficient than FedMix by showing how our approach can achieve significantly better final performance using the same communication budget shown in Tables 1 and 5 (in the appendix). In Table 5, if we compare FedJETs using two experts with FedMix using five experts, we can see FedJETs achieve better final performance with a smaller communication budget. \n\n-**Regarding cold-start problem**:\nOur paper does not use the term \"cold-start\" for our targeted scenario. Our method will adapt to new clients with unseen local dataset distributions and without labels during testing by dynamically selecting experts. We are not using any label from the test client, so we define this as zero-shot penalization.\n\n-**Regarding data non-iidness**: \nWe reproduce two main partitioning strategies proposed in [19] to simulate the non-iidness in our clients. The first partitioning is quantity-based label imbalance, and the second uses a more standard approach, distribution-based, which simulates the Dirichlet function across the total number of clients. All the details about the clients' partition can be found in Appendix A, and a more detailed evaluation of FedJETs under these distributions is included in Appendix B.\n\n-**Regarding conflicting gradients**: \nDuring experiments, we discovered that anchor clients act as regularizers and help maintain consistency in the experts' updates when the gating function makes the wrong selection. The pretrained model - permanently frozen - serves as a feature extractor; these embeddings feed the gating function, ensuring that only the relevant experts are sent to each client and the expert updates are directed towards the same objectives, thus avoiding model drift. Additionally, there is an initial degree of randomness in the gating function. During the first couple of iterations, the gating function sends random top $K$ experts to each client while the experts learn to specialize in the different regions of the label space. We found a way to keep consistency during these initial rounds: through the anchor clients. By introducing at least 30\\% anchor clients during each training round, we can ensure a balance between the wrong selection of the gating function by allowing them to act as regularizers in light of conflicting updates on the experts. Appendix C provides a detailed explanation of this phenomenon encountered during training."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4174/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700580359019,
                "cdate": 1700580359019,
                "tmdate": 1700679971673,
                "mdate": 1700679971673,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kSnw3exQI1",
                "forum": "hEl2HpiH3g",
                "replyto": "6guvzdfYzk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4174/Reviewer_Na6T"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4174/Reviewer_Na6T"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. I will keep my score."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4174/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732470483,
                "cdate": 1700732470483,
                "tmdate": 1700732470483,
                "mdate": 1700732470483,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Rb9o7sOC4J",
            "forum": "hEl2HpiH3g",
            "replyto": "hEl2HpiH3g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4174/Reviewer_k6Nb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4174/Reviewer_k6Nb"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies a federated learning setting where the goal is to fine-tune the models. The main framework FedJETs is given a pretrained model and contains multiple ``expert'' models and a gating function. When new client data comes in, the gating function utilizes the representation from the pre-trained model to decide which K experts to update. Then, using the client's data, FedJETs obtain updates for the gating function as well as the K experts and send them back to the server. The server aggregates and updates the new weights."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper presented the main idea as well as the FEDJETs algorithm in a clear and intuitive manner. The idea of having individual expert models and a gating function to select experts is intuitive and reasonable. The authors also discussed the technical difficulties coming with this design. The experimental results suggest the efficacy of the proposed method."
                },
                "weaknesses": {
                    "value": "My biggest concern is the novelty of the proposed method. The general framework of having individualized models and selecting a subset of experts for performing ensemble learning is a traditional topic. The specific setting of having a pre-trained model along with a gating function to select a subset of experts to update is new. I am not entirely familiar with the current federated learning literature, so I will leave other reviewers to decide on the novelty of the paper to the federated learning community. \nIn addition to the concern about the novelty of the work, another concern I have is the applicability of the method when expert models need to be very large. It seems to be inefficient to use the common expert (a large pre-trained model) to just perform expert selection. Would it be more reasonable, computation-wise at least, to not have individual expert models but different expert heads so that the pretrained common expert can be used to extract a common representation to pass into different experts?"
                },
                "questions": {
                    "value": "- How should the number of experts scale with the number of clients? How should one choose the ``K'' hyperparameter?\n- Could the authors comment on the computation and memory costs of having individual experts? How big should the expert model be compared to the pretrained common expert model? \n- Other than using the pre-trained model for obtaining representation for the gating function, is it used in some other ways, e.g., is there a way to combine its output with the expert model?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4174/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698806440783,
            "cdate": 1698806440783,
            "tmdate": 1699636383223,
            "mdate": 1699636383223,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IirjYFa7VD",
                "forum": "hEl2HpiH3g",
                "replyto": "Rb9o7sOC4J",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4174/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4174/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer k6Nb"
                    },
                    "comment": {
                        "value": "We want to thank the reviewer for the detailed comments. We address each point raised and hope our responses will alleviate further concerns. We remain available to answer any additional questions you may have.\n \n-**Regarding differences with ensemble learning**: There is a critical difference between ensemble learning and FedJETs: while the former combines multiple models to create a more accurate prediction during testing, FedJETS uses a weighted average per sample on all the selected $K$ experts, and utilizes the highest expert ranking score to fully capitalize on the specialization of a single expert at the sample level. Algorithm 1 (Testing) shows this is not a trivial adaptation of ensembles. Also, Appendix D provides further details on the gating function behavior during inference; if required, we can move some of this material into the main text to make ideas emerge more naturally. To the best of our knowledge, though, we are unaware of any works that learn mechanisms to select on-the-fly subsets of experts to be combined during inference; if there is such literature we are missing, we are eager to know.\n\n-**Regarding large-scale models, efficiency, and pretrained models**: First, we note that one of the main advantages of our method is that the ``pretrained common expert'' is considered a black box. This means that the architecture of the pretrained model is not strongly coupled with that on the expert's side. This allows us to leverage the knowledge of any pretrained model with capabilities over the different domains on the dataset. Further, this allows us to not rely on the model's full capacity; therefore, we assume this model is not extremely large, and neither has to be fully trained. Yet, it is noteworthy that during the early stages of our design, we experimented with the case where information (e.g., the whole model or part of the model, or even embeddings) from the pretrained model was available to be used along with the experts selected. Yet, we noticed that this often led to performance degradation, and the experts required further training to achieve some specialization.\n\n-**Regarding expert/clients ratio**: The experts in FedJETs are agnostic to the total number of clients. Experiments presented in Table 1 demonstrate the behavior of FedJETs for various $M$ values (total number of experts) with a fixed number of clients $K$. Each expert should have one \"anchor\" client that matches its specialization range to ensure that the gating function learns to assign clients to the right experts in the early stages of training. Appendix A, Figure 6, shows an example of how we randomly assigned specialization ranges to the experts. We hope this information clarifies the reviewer's concern.\n\n-**Regarding computational/memory costs**: \nThe architecture of the experts and that of the pretrained model are not necessarily coupled; instead, the pretrained model operates as a black box that decides which experts should be selected and activated. Further, the pretrained model does not require full training, as it is used as a feature extractor. Conversely, since the experts specialize in specific labels of the input space, the model size can be considered \"small\" compared to a monolithic model that covers all labels.\n\n-**Regarding combining the expert model with that of pretrained model**:\nWhile we do not exclude the possibility that the pretrained model can be somehow combined with expert models, as we mentioned above, we have observed a performance degradation in our initial attempts. This is an exciting research direction that we will be closely looking into in the near future."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4174/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700579785777,
                "cdate": 1700579785777,
                "tmdate": 1700679964825,
                "mdate": 1700679964825,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SUCxsOeio3",
            "forum": "hEl2HpiH3g",
            "replyto": "hEl2HpiH3g",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4174/Reviewer_pX49"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4174/Reviewer_pX49"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes FedJETs, a distributed system that connects and extends Mixture of Experts in FL setting. The system features multiple independent models as experts, in contrast to common MoE settings where different parts of a model is considered as experts. The authors introduce a pretrained common expert and a novel gating functionality to guide the specialization of experts during training. The authors claim that the combined system can exploit the characteristics of each client\u2019s dataset and adaptively select experts suitable during training. FedJETs also claims to be able to dynamically select experts and adjust to unseen clients on-site."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper proposes FedJETs, a distributed system that connects and extends Mixture of Experts in FL setting. The system features multiple independent models as experts, in contrast to common MoE settings where different parts of a model is considered as experts. The authors introduce a pretrained common expert and a novel gating functionality to guide the specialization of experts during training. The authors claim that the combined system can exploit the characteristics of each client\u2019s dataset and adaptively select experts suitable during training. FedJETs also claims to be able to dynamically select experts and adjust to unseen clients on-site."
                },
                "weaknesses": {
                    "value": "The paper proposes FedJETs, a distributed system that connects and extends Mixture of Experts in FL setting. The system features multiple independent models as experts, in contrast to common MoE settings where different parts of a model is considered as experts. The authors introduce a pretrained common expert and a novel gating functionality to guide the specialization of experts during training. The authors claim that the combined system can exploit the characteristics of each client\u2019s dataset and adaptively select experts suitable during training. FedJETs also claims to be able to dynamically select experts and adjust to unseen clients on-site."
                },
                "questions": {
                    "value": "It would be appreciated, considering the nature of this paper, if more results regarding Non-IID datasets other than the CIFAR data suite could be demonstrated. For FL scenarios, there are plenty of available datasets beyond the CIFAR data suite with more obvious levels of Non-IID features (e.g., the LEAF benchmark datasets).\n\nBesides, the ablation study regarding the anchor client ratio might not be sufficient as to determine the claimed \u201coptimal\u201d ratio. It served the proposal to address the significance of anchor clients, but there seems to be more to explore regarding such a key component of the entire method. Is it possible for a higher anchor-normal client ratio to achieve faster convergence or even a better overall performance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4174/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4174/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4174/Reviewer_pX49"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4174/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698847960957,
            "cdate": 1698847960957,
            "tmdate": 1699636383135,
            "mdate": 1699636383135,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Qnznh9kFoQ",
                "forum": "hEl2HpiH3g",
                "replyto": "SUCxsOeio3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4174/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4174/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer pX49"
                    },
                    "comment": {
                        "value": "We want to thank the reviewer for the detailed comments. Below, we respond to the points raised by the reviewer, one by one. We hope our responses will resolve any further concerns, and we are available for any other questions.\n \n-**Regarding more datasets**: \nFirst, we agree with the reviewer that including more datasets can help to illustrate the differences among different domains better. For this reason, we provide new results on the EMNIST dataset (table below) using the same setup as Table 1 in the paper, which further demonstrates the advantage of our method; note that completing the existing experiments in the paper took a significant amount of GPU time, given the excessive ablation study results we present. Moreover, we point out that the CIFAR data suite is still highly relevant for current benchmarks on Federated Learning. Table 1 shows a noticeable difference in behavior between the two datasets; CIFAR10 has more difficulty reaching the desired performance due to its fewer classes, exacerbating the model drift problem in all baselines. FedJETs is the only method able to surpass the initial accuracy of the Common expert, whereas the rest only degraded during training. \n| FedJETs | FedMix |\n|---------|--------|\n0.9639 | 0.9259\n\n-**Regarding anchor clients ratio**: \nThe ablation study examines the importance of the ``anchor clients\" during sampling, i.e., the clients that share the same expert for the target task. It shows that we need at least 30\\% of anchor clients to guarantee that our method can surpass the initial accuracy of the pretrained common expert. Still, Figure 5 also explores two other scenarios: $i)$ How does our approach perform if we randomly sample clients across the training iterations without any control over the expert distribution? And $ii)$ How does our method benefit from increasing the proportion of anchor clients to 50-50? While the latter ensures faster convergence of our method, it is not very realistic, as enforcing higher ratios of anchor clients in federated learning may affect the efficiency/deployment costs and privacy. Appendix B (Figure 9) shows the detailed performance of our method under $i$ and $ii)$ setups for both datasets."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4174/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700581191534,
                "cdate": 1700581191534,
                "tmdate": 1700679953596,
                "mdate": 1700679953596,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]