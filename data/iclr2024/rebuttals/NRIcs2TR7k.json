[
    {
        "title": "Extending Multi-modal Contrastive Representations"
    },
    {
        "review": {
            "id": "2ozIf6Sf0a",
            "forum": "NRIcs2TR7k",
            "replyto": "NRIcs2TR7k",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3649/Reviewer_GFCr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3649/Reviewer_GFCr"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a training-efficient and paired-data-free method to flexibly learn unified contrastive representation space for more than three modalities by integrating the knowledge of existing MCR spaces."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(1) The paper formulation is good and clear.  \n\n(2) The proposed method can align multiple existing MCRs into the same based MCR, which can effectively preserve the original semantic \nalignment of the based MCR."
                },
                "weaknesses": {
                    "value": "(1) The reviewer finds the presentation of figures confusing. For instance, in Table 1, superior results are subtly highlighted in grey to minimize attention, while the results of the proposed model are emphasized in bold to draw more focus.\n\n(2) In Table 3, the value 11.19 is highlighted in bold instead of 11.30.\n\n(3) Considering the issues raised earlier, the reviewer remains uncertain about the authenticity and reproducibility of the results.\n\n(4) Apart from presenting numerical data, what are the additional findings and conclusions drawn from the ablation studies?"
                },
                "questions": {
                    "value": "Please see the comments above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3649/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698442828343,
            "cdate": 1698442828343,
            "tmdate": 1699636321234,
            "mdate": 1699636321234,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3kLy4a7J4V",
                "forum": "NRIcs2TR7k",
                "replyto": "2ozIf6Sf0a",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3649/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3649/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer GFCr"
                    },
                    "comment": {
                        "value": "### **W1: grey mark in Table 1 and Table 2:**\n\nA1: CLIP and CLAP are used as base-MCR and leaf-MCR, and they only focus on contrastive representation between two modalities (i.e., image-text and audio-text). Our experiments mainly focus on comparing the unified representations for three or more modalities over various cross-modal downstream tasks rather than the performance on specific image-text or audio-text tasks. The results of CLIP and CLAP are listed to compare how well C-MCR and Ex-MCR inherit the semantic alignment of original MCR spaces. Moreover, in the responses to reviewer 6GyA\u2019s W1 & Q1 & Q3, we discuss in detail why Ex-MCR\u2019s performance on audio-text or 3D-image lags behind that of the leaf-MCR is acceptable and some future directions to further improve performance.\n\nIn Table 1, the baseline WAV2CLIP is trained on VGG-Sound, and the evaluation dataset VGGSS is a subset of the VGG-Sound test set. This means the evaluation of WAV2CLIP on VGGSS is supervised. However, other methods do not use any VGG-sound data for training, so other results on VGGSS are evaluated in a zero-shot manner. Therefore, we grayed out the results of WAV2CLIP on VGGSS.\n\nSorry for the misunderstanding caused by not clearly explaining the gray markings, and we have added these explanations in the newly submitted version. \n\n### **W2: 11.30 and 11.19 in Table 3**\n\nA2: I suppose you are probably referring to 11.30 and 11.19 in Table 4. Sorry for the misunderstanding, this is a typo, 11.30 should be bolded, not 11.19. We are not intentionally bolding it incorrectly. In the similar case in Table 6, the marks are correct.\n\nMoreover, in Table 11, we display more ablation results about alignment objectives. Using all alignment objectives shows obvious advantages over most tasks and metrics. This incorrect bold mark would not affect the conclusion that the dense alignment objective is effective.\n\nThank you for your kind reminder. We have modified this typo in the newly submitted paper.\n\n### **W3 Considering the issues raised earlier, the reviewer remains uncertain about the authenticity and reproducibility of the results.**\n\nA3: The gray marks in Tables 1 and 2 are explained in the response to W1. For W2, it is a typo and it has been modified. The detailed ablation experimental results are also shown in the appendix, further supporting our conclusions.\n\nTo completely eliminate concerns about the authenticity and reproducibility of the results, we provide Ex-MCR pre-trained weights, well-packaged code, download links for the pre-trained models used, and guidelines for using our Ex-MCR in the new submission's supplementary materials for reproduction. These pre-trained weights and code will be open source.\n\n### **W4 Additional findings and conclusions drawn from the ablation studies?**\n\nFor ablation results on \u201cVarious modality-centric data\u201d, we further find that pseudo-pairs from audios are critical to the performance of audio-text retrieval, demonstrating the importance of various modality-centric data, and proving that previous single modality-centric data really can not fully reflect the audio representation space.\n\nFor ablation results on \u201cDense alignment objective\u201d, we find that directly aligning the pseudo audio-image or audio-text embedding pairs leads to sub-optimal audio alignment, whereas aligning spaces by overlapping text modality brings better alignment than learning alignment directly from pseudo pairs. This observation further suggests that overlapping modalities play a key pivotal role in aligning different spaces.\n\nFor ablation results on \u201cStructure of $f_m(\\cdot)$\u201d, we summarize that good results are achieved no matter how many layers of MLP, which demonstrates the robustness of our method. According to more detailed experiments, empirically, MLP with 2 or 3 layers achieves a good balance between expressivity and learning difficulty."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3649/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496691713,
                "cdate": 1700496691713,
                "tmdate": 1700496691713,
                "mdate": 1700496691713,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3zKKKXwY2p",
                "forum": "NRIcs2TR7k",
                "replyto": "2ozIf6Sf0a",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3649/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3649/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Deadline coming. Looking forward to your feedback."
                    },
                    "comment": {
                        "value": "Dear Reviewer GFCr,\n\nThanks again for your comments. We would like to kindly remind you that we tried our best to address the concerns you raised.\n\nWe kindly request your feedback as the rebuttal deadline is approaching in less than 1 day. We would be happy to discuss in detail if you have additional comments about our paper.\n\nBest regards, Authors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3649/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663733053,
                "cdate": 1700663733053,
                "tmdate": 1700663733053,
                "mdate": 1700663733053,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3lkz4DtgdO",
            "forum": "NRIcs2TR7k",
            "replyto": "NRIcs2TR7k",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3649/Reviewer_CZb4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3649/Reviewer_CZb4"
            ],
            "content": {
                "summary": {
                    "value": "this paper introduces Ex-MCR, a method for learning unified contrastive representations across more than three modalities, focusing on being training-efficient and not requiring paired data"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "the idea of extending multi-modal contrastive representation (Ex-MCR) to incorporate more than three modalities without relying on paired data is an interesting and important topic given there are appearing more multimodal foundation models, but each of them only covers a fraction of the modalities.\nThe proposed approach is technically sound."
                },
                "weaknesses": {
                    "value": "one of the concerns here is the baselines and the results, even the methods like imagebind use paired data, the reviewer still thinks that it's worthwhile to study what will be the performance differences, as it could indicate the limitation/boundary of this kind of paired-data-free methods."
                },
                "questions": {
                    "value": "minor issues:\ntypos in the first summarized contribution in the introduction.\nfigure 1 seems to lack some clarity, and the caption is not very helpful for understanding this figure, consider improving this part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3649/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822351665,
            "cdate": 1698822351665,
            "tmdate": 1699636321119,
            "mdate": 1699636321119,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2xoxY8Mj2L",
                "forum": "NRIcs2TR7k",
                "replyto": "3lkz4DtgdO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3649/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3649/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer CZb4"
                    },
                    "comment": {
                        "value": "### **W1: the imagebind baselines & the limitation/boundary of this kind of paired-data-free methods**\n\n**Comparison with imagebind:**\n\nThe current compared baselines: AudioCLIP, WAV2CLIP and ULIP, ULIP v2. These methods are all trained on paired audio-image-text or 3D-image-text data, and their model sizes are equivalent to Ex-MCR. Thus, the main difference between Ex-MCR and these baselines is the training data and training strategy. Comparison with these baselines better demonstrates the strengths of our \u201cextending\u201d learning scheme.\n\nHowever, imagebind not only uses paired data for training, but also uses a much stronger pre-trained model, (Imagebind uses OpenCLIP ViT-Huge trained on LAION-2B while Ex-MCR uses CLIP ViT-Base trained on WIT-400M). Considering the audio-image-text representation, the parameters number of imagebind are 630M (image encoder), 303M (text encoder), and 86M (audio encoder), while the parameters number of our Ex-MCR is 88M (image encoder), 63M (text encoder) and 32M (audio encoder).\n\nAlthough imagebind uses both data pairs and stronger pre-trained models, we still provide a performance comparison on audio-image-text as:\n\n|                             | | FlickrNet | AVE  | VGGSS | Audiocaps | COCO |\n| -------------------------- |--| --------- | ---- | ----- | --------- | ---- |\n|                            | Total params | R@5       | R@5  | R@5   | R@5       | R@5  |\n| ImageBind (OpenCLIP ViT-H) | 1,019M | **20.78** | **40.11** | **35.67** | 8.29 | **74.29** |\n| Ex-MCR (CLIP ViT-B)        | 183M | 5.95 | 4.93 | 8.12 | 16.65 | 57.62 |\n| Ex-MCR (OpenCLIP ViT-H)    | 965M | 6.35 | 6.90 | 11.59 | **25.00** | **74.29** |\n\nOn the image-audio task, imagebind achieves better results since it uses large-scale paired data and stronger visual pre-training. On the audio-text task, our method is far better than imagebind. The performance difference in image-text tasks is due to using the different CLIP models. \n\n**The potential of this kind of paired-data-free methods**\n\nIn the responses to Reviewer 6GyA, we fine-tune the projector with paired data which brings performance improvements. Besides, we further discuss the potential of Ex-MCR in scalability and low computing resource situations.\n\nRegarding the boundaries and potential of absolute performance, we try to replace the CLIP ViT-Base with the larger OpenCLIP ViT-Huge. The results are also reported in the above table. Simply using a larger version of CLIP, the performances on audio-image, audio-text, and image-text tasks are significantly improved. \n\nIn addition, there are many future directions to further improve the absolute performance of Ex-MCR, such as using more unimodal data, integrating paired data into the training process, employing stronger leaf-MCR pre-training, and designing more sophisticated space alignment architecture and learning objectives. Our future work will be dedicated to further exploring the potential of such paired-data-free methods.\n\n### **Q1: Typos. Unclear Figure 1 and its caption.**\n\nThanks for pointing out the typos and providing suggestions about figures, we have fixed them in the newly submitted paper.\n\nAdditionally, in our response to reviewer pnPG, we analyzed most of the modifications that were made to improve the clarity and readability of the presentation."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3649/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496599400,
                "cdate": 1700496599400,
                "tmdate": 1700496599400,
                "mdate": 1700496599400,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SYArbIpYls",
                "forum": "NRIcs2TR7k",
                "replyto": "3lkz4DtgdO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3649/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3649/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Deadline coming. Looking forward to your feedback."
                    },
                    "comment": {
                        "value": "Dear Reviewer CZb4,\n\nThanks again for your comments. We would like to kindly remind you that we tried our best to address the concerns you raised.\n\nWe kindly request your feedback as the rebuttal deadline is approaching in less than 1 day. We would be happy to discuss in detail if you have additional comments about our paper.\n\nBest regards, Authors"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3649/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663693502,
                "cdate": 1700663693502,
                "tmdate": 1700663693502,
                "mdate": 1700663693502,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kGR6Pc1hvW",
                "forum": "NRIcs2TR7k",
                "replyto": "SYArbIpYls",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3649/Reviewer_CZb4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3649/Reviewer_CZb4"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response. I've read other reviews and rebuttals."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3649/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709079080,
                "cdate": 1700709079080,
                "tmdate": 1700709079080,
                "mdate": 1700709079080,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mokZa5V3I1",
            "forum": "NRIcs2TR7k",
            "replyto": "NRIcs2TR7k",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3649/Reviewer_6GyA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3649/Reviewer_6GyA"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel method, Extending Multimodal Contrastive Representation (Ex-MCR), to address challenges in multi-modal learning for more than three modalities. Traditional methods are constrained by the need for large-scale, high-quality paired data and high training costs. Ex-MCR offers a training-efficient solution that doesn't rely on paired data, by aligning multiple existing MCRs into a base MCR, preserving their original semantic alignment. The method enhances the alignment process through various techniques, including modality-centric pseudo data pairs and a decoupled projector. Experiments demonstrate its state-of-the-art performance on various tasks, showcasing its potential in representation learning and modality extensibility."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper introduces Ex-MCR, which extends one MCR space (leaf-MCR) into another fixed MCR space (base-MCR). This approach optimizes the preservation of modality alignment within the base MCR, showcasing its potential for integrating multiple MCRs.\n2. This method operates without the need for paired data, demonstrating its potential for scalability.\n3. This method employs a dense contrastive loss on pseudo-pairs between all possible modalities, enhancing the learned alignment's stability.\n4. Ex-MCR achieves competitive performance across various zero-shot tasks, even better than strong baselines trained by paired data."
                },
                "weaknesses": {
                    "value": "1. While Ex-MCR outperforms baseline methods, its performance lags behind that of the leaf-MCR.\n2. The authors highlight the scalability of Ex-MCR; however, it's contingent upon the new MCR having a modality already present in the base MCR. Otherwise, biases or errors may be exacerbated."
                },
                "questions": {
                    "value": "1. How do you explain the performance gap between Ex-MCR and the original leaf-MCR? Are there specific challenges or limitations inherent to Ex-MCR that contribute to this disparity?\n2. Given the premise that a new MCR should have a modality already present in the base MCR for effective expansion, how does Ex-MCR handle scenarios where entirely new modalities (or only modalities in leaf-MCR) need to be integrated?\n3. Have you experimented with incorporating paired data into the training process for Ex-MCR? If so, how did it impact the results?\n4. If the configuration were altered such that CLAP served as the base-MCR and both CLIP and ULIP were treated as leaf-MCRs, would the performance outcomes remain favorable?\n5. How many resources do you use for the training?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3649/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839436152,
            "cdate": 1698839436152,
            "tmdate": 1699636321049,
            "mdate": 1699636321049,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lCHyFvq2a3",
                "forum": "NRIcs2TR7k",
                "replyto": "mokZa5V3I1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3649/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3649/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6GyA (1/2)"
                    },
                    "comment": {
                        "value": "### **W1 & Q1 & Q3: why Ex-MCR\u2019s performance lags behind that of the leaf-MCR and possible solutions.**\n\n**1) Taking extending CLAP to CLIP as an example, if you mean why the projected CLAP's audio-text representations lag behind the original CLAP audio-text representations:** \n\nIn order to avoid misunderstanding, we further clarify that the final unified Ex-MCR representations are composed of CLIP's image, text representations, projected CLAP's audio representations, and projected ULIP's 3D representations (i.e., the $\\mathbf{t}^I_i$, $\\mathbf{v}^I_i$, $f_m^A(f_l^A(\\mathbf{a}^A_i))$, $f_m^U(f_l^U(\\mathbf{p}^U_i))$ are the final audio-text-image-3D unified representation). All our experimental results are based on these four unified aligned representations. Therefore, the audio-text results are evaluated by the projected CLAP's audio and CLIP text representations (i.e., the $\\mathbf{t}^I_i$, $f_m^A(f_l^A(\\mathbf{a}^A_i))$ ), rather than the projected CLAP's audio-text representations (i.e.,  the $f_m^A(\\mathbf{t}^A_i)$, $f_m^A(f_l^A(\\mathbf{a}^A_i))$ ).\n\n**2) Taking extending CLAP to CLIP as an example, if you mean why the newly aligned audio-text representation in Ex-MCR's unified space lags behind the CLAP audio-text representations:** \n\nThe following are reasons and possible solutions for performance lags behind leaf-MCR.\n\n**Reason for performance lags behind leaf-MCR:**\n\nWe think that the performance of Ex-MCR lags behind that of leaf-MCRs for three reasons:\n\n 1\u3001Unchanged features extraction capability. The final Ex-MCR's audio representation is only a transformation of the CLAP's audio representation, and the ability to extract meaningful features from the original data is unchanged.\n\n2\u3001Non-optimal target space. The representation distribution in fixed base-MCR may not fully distinguish the modalities of leaf-MCRs. For example, \u201cpeople are talking on the beach\u201d and \u201cpeople are talking in the living room\u201d are different in CLIP\u2019s text representation, but are similar in the audio-text domain.\n\n3\u3001Limited training data. The pseudo data we use to extend leaf-MCR to base-MCR are mainly derived from the inherent alignment of leaf-MCR and base-MCR. Therefore, this extending process can be regarded as distilling leaf-MCR\u2019s alignment knowledge to construct a new alignment for base-MCR.\n\nMoreover, Ex-MCR is designed to learn unified representations for more than three modalities rather than for only two modalities. And current experiments in the paper aim to demonstrate the potential of Ex-MCR in the extreme situation of no data pairs can be used. Considering the above facts, we think it is reasonable and acceptable that the current Ex-MCR lags behind leaf-MCR in performance on downstream tasks in leaf-MCR\u2019s specific modality.\n\n**Possible solutions for performance lags behind leaf-MCR:**\n\nCombining the above analyses of limitations, there are many possible solutions to further improve the performance of Ex-MCR. Such as using more unimodal data, integrating paired data used by leaf-MCR into the training process, and employing stronger leaf-MCR and base-MCR pre-trainings.\n\nAs you suggested in Q3, we try to integrate the paired data used by leaf-MCR. With limited rebuttal time, we simply use part of CLAP\u2019s training audio-text data (audiocaps training set, and 200k pairs of laion630k) to fine-tune the projector. The results are as follows:\n\n|                    | FlickrNet | AVE      | VGGSS    | Audiocaps |\n| ------------------ | --------- | -------- | -------- | --------- |\n|                    | R@5       | R@5      | R@5      | R@5       |\n| CLAP               | -         | -        | -        | 35.23     |\n| C-MCR              | 5.97      | 4.91     | 7.69     | 13.62     |\n| Ex-MCR             | 5.95      | 4.93     | **8.12** | 16.65     |\n| Ex-MCR (fine-tune) | **6.09**  | **5.03** | 8.00     | **21.11** |\n\nBy tuning the learned projector with audio-text data pairs, the performance of audio-text is greatly improved without sacrificing the audio-image performance. This demonstrates the compatibility of our structure with paired data, and paired data can further boost the final performance.\n\nWe also evaluate using a larger base-MCR pre-training, which also brings performance improvement. Please refer to the response to Reviewer CZb4's W1."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3649/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496401320,
                "cdate": 1700496401320,
                "tmdate": 1700496416870,
                "mdate": 1700496416870,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FqpviX58aK",
                "forum": "NRIcs2TR7k",
                "replyto": "mokZa5V3I1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3649/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3649/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6GyA (2/2)"
                    },
                    "comment": {
                        "value": "### **W2 & Q2 & Q4: The new MCR should have a modality already present in the base MCR for effective expansion. And the results of extending new MCR via leaf-MCR.**\n\nAs discussed in recent work imagebind [1] and languagebind [2], most existing modalities can be bound to either images or language, and most existing MCRs contain either images or language. From a practical perspective, choosing the powerful CLIP as base-MCR can cover almost all scenarios.\n\n*[1] ImageBind: One Embedding Space To Bind Them All. Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, Ishan Misra. CVPR2023*\n\n*[2] LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment. Bin Zhu, Bin Lin, Munan Ning, Yang Yan, JiaXi Cui, Hongfa Wang, et al. Arxiv2023*\n\nHowever, to further explore the potential of Ex-MCR, we still try to expand a leaf-MCR through the overlapping modality of another leaf-MCR, **like a chain structure**. As you mentioned in Q4, we regard CLAP as base-MCR and CLIP as leaf-MCR, the audio-image-text results are\uff1a\n\n|                                  | FlickrNet | AVE      | VGGSS    | Audiocaps | COCO      |\n| -------------------------------- | --------- | -------- | -------- | --------- | --------- |\n|                                  | R@5       | R@5      | R@5      | R@5       | R@5       |\n| Ex-MCR (Original Tree Structure) | **5.59**  | 4.93     | **8.12** | 16.65     | **57.62** |\n| Ex-MCR (Chain Structure)         | 5.15      | **5.08** | 7.18     | **35.23** | 16.30     |\n\nWhen using CLAP as base-MCR, the audio-text retrieval accuracy on AudioCaps is much higher than using CLIP as base-MCR. Correspondingly, its image-text retrieval accuracy on COCO is lower than using CLIP as base-MCR. For audio image retrieval, the performance of both variants is comparable.\n\nThe most important experiment: use ULIP as leaf-MCR of leaf-MCR, which means indirectly aligning the 3D representation of ULIP to the text representation of CLAP through CLIP. The final 3D-image-text results are:\n\n|                          | ModelNet40 (3D-Text) |           |           | Objaverse-LVIS (3D-Image) |          |           |\n| ------------------------ | -------------------- | --------- | --------- | ------------------------- | -------- | --------- |\n|                          | Acc@1                | Acc@3     | Acc@5     | mAP                       | R@1      | R@5       |\n| ULIP                     | 60.40                | 79.00     | 64.40     | 3.54                      | 1.45     | 4.51      |\n| ULIP v2                  | **73.06**            | 86.39     | 91.50     | **11.41**                 | **6.00** | **15.64** |\n| C-MCR                    | 64.90                | 87.00     | 92.80     | 3.84                      | 1.36     | 4.80      |\n| Ex-MCR (Tree Structure)  | 66.53                | **87.88** | **93.60** | 6.23                      | 2.54     | 8.25      |\n| Ex-MCR (Chain Structure) | 48.82                | 64.75     | 73.99     | 2.26                      | 0.77     | 3.09      |\n\n The final 3D-text-image performance is indeed lower than directly using ULIP as a leaf-MCR and using CLIP as base-MCR. But it still achieved performance comparable to most baselines, which shows the error accumulation of chain structure is within the acceptable range. \n\n### **Q5: Training resources:**\n\nIn our implementation, extending CLAP into CLIP only requires 4G GPU memory. When training on an A100, it takes 6 hours to converge. Due to its extremely low GPU memory requirements, users can train Ex-MCR on almost any device. We also try to reduce the batch size, and when only 2GB GPU memory is used (of course requiring a longer training time), there is almost no performance degradation.\n\nOur training is extremely resource-efficient. Previous contrastive representation learning methods need to set a larger batch size for training stability, which results in very high GPU memory usage. However, in our method, all pre-trained encoders are frozen, so all features can be pre-extracted and saved offline, and only a linear and MLP layer-based projector is learnable, which greatly reduces the GPU memory cost when increasing the batch size."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3649/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496520564,
                "cdate": 1700496520564,
                "tmdate": 1700534290330,
                "mdate": 1700534290330,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KifjmXBeCj",
                "forum": "NRIcs2TR7k",
                "replyto": "mokZa5V3I1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3649/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3649/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Deadline coming. Looking forward to your feedback."
                    },
                    "comment": {
                        "value": "Dear Reviewer 6GyA,\n\nThanks again for your comments. We would like to kindly remind you that we tried our best to address the concerns you raised.\n\nWe kindly request your feedback as the rebuttal deadline is approaching in less than 1 day. We would be happy to discuss in detail if you have additional comments about our paper.\n\nBest regards, Authors"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3649/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663650827,
                "cdate": 1700663650827,
                "tmdate": 1700663650827,
                "mdate": 1700663650827,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "R23zRVzNdH",
            "forum": "NRIcs2TR7k",
            "replyto": "NRIcs2TR7k",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3649/Reviewer_pnPG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3649/Reviewer_pnPG"
            ],
            "content": {
                "summary": {
                    "value": "The work proposes an effective and efficient recipe to integrate multiple Multi-modal Contrastive Representation (MCR) spaces into one, making it possible to have a 3D-Text-Vision-Audio model without requiring any additional paired data. This is achieved by leveraging the modality that is shared across the multiple spaces, learning to project each source space (called leaf-MCR) to a chosen target space (base-MCR) by using a InfoNCE loss over pseudo-pairs containing the different modality combinations. To further improve the alignment, the intra-MCR modality gap is closed with an additional regularizing term. The experiments show improved results when considering the union of more than 3 modalities (3D, Text, Vision, Audio), evaluating the performance on cross-modal retrieval tasks over all the possible combinations."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I find the paper to offer a significant contribution in multi-modal learning, proposing an efficient way to achieve a multi-modal model on more than 3 modalities without the need for a multi-modal paired dataset annotated for all the considered modalities. As a possible solution to overcome the need for large-scale multi-modal paired datasets, the approach is advisable and useful for the community. In the current AI environment, it is good to see that there is still a strive to push for more accessible approaches that don\u2019t require extensive resources. \n\n### Novelty\n\n- The work goes to build upon C-MCR, inheriting its benefits and effectively overcoming its limitations, in practice extending its applicability to multiple modalities altogether.\n\n### Quality\n\n- The methods employed are simple yet effective, and the overall framework is rather lightweight, making it directly usable by any practitioner desiring cheap multi-modality;\n- The claims are backed by proper experimental evidence.\n\n### Significance\n\n- The experiments over all the combinations of cross-modal tasks involving two of the fourmodalities considered are promising, and it\u2019s impressive that these are obtained without the need for an expensive paired dataset covering all the involved modalities;\n- Source code is provided for reproduction purposes."
                },
                "weaknesses": {
                    "value": "### \n\n- ULIP is already trained to be aligned with CLIP in the original paper. This work instead states it as a contribution; \n    - while it may be the case that the framework improved this alignment, I find it should be stated clearly that this was indeed the case\n- Clarity could be greatly improved, the amount and repetition of acronyms makes the overall paper hard to read and hardly a pleasing experience;\n- After severeal reads, I still found it hard to decipher the \u201cmodality-centric consistency\u201d part.\n    - Being a core component of the framework, I find that the paper really needs to state it more clearly for the reader to understand;\n- Even key messages in the introduction are somewhat vague and not immediately clear; e.g.\n    - in \u201c*from  the  training  data  perspective,  we  extract  various  modality-centric pseudo  data  pairs,  aiming  to  alleviate  the  semantic  bias  of  pseudo  pairs  in  Wang  et  al.  (2023b) and reflect MCR space more comprehensively.*\u201d what does it mean for a data pair to be modality-centric? what semantic bias is the work referring to? what does it mean to reflect a MCR space comprehensively?\n    - the whole period \u201c*C-MCR employs data from overlapping modalities to aggregate semantic consistent embedding of non-overlapping modalities, thereby creating pseudo-pairs*\u201d is not clear and should be rephrased for clarity.\n    - These are only two examples, but this kind of ambiguous phrasing is very frequent in the text, unfortunately hindering my understanding of the contributions;\n- The notation is not always clear, and sometimes the details are missing to fully comprehend the equations, such as the normalization factor of the softmax and the tilde symbols in (1) and following equations .\n- Table 3, 4, 5 and 6 don\u2019t report what metric is actually being shown.\n- I can\u2019t decipher what\u2019s going on in Figure 1, especially in the left part. Overall the figure seems quite crowded, it probably would help splitting it in two figures. The caption doesn\u2019t seem to help.\n\nOverall, clarity is the principal reason I am inclined to reject as it prevents me from fully understanding the proposed method and contributions, therefore hindering my capability to assess its merits and drawbacks. Moreover, I find the work to tackle an interesting and impactful topic, and I would like it to be accessible to a broad audience; I find the current writing to pose a significant hurdle in this regard, and therefore hope it can be improved. I am more than willing to increase my score if this is properly addressed."
                },
                "questions": {
                    "value": "### Questions\n- What\u2019s the denominator in the softmax computation in (1)? is it over all the samples in the batch?\n- I don\u2019t understand what the tilde means in Eq (1) and the following ones, how does $\\tilde{t}$ differ from $t$?\n- in 3.2.1., it mentions three kinds of semantically consistent embeddings (audio-centric, text-centric, image-centric) (here the e.g. should actually be a i.e.) but then four objects are given, $\\{\\tilde{a}_i^A, \\tilde{t}_i^A, \\tilde{t}_i^I, \\tilde{v}_i^I\\}$. What\u2019s the explanation here?\n### Typos\n- 3.2.1. header: centirc \u2014> centric\n- The text sometimes refers to base MCR as based MCR"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3649/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3649/Reviewer_pnPG",
                        "ICLR.cc/2024/Conference/Submission3649/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3649/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698841155106,
            "cdate": 1698841155106,
            "tmdate": 1700724225810,
            "mdate": 1700724225810,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "MvkwzZZh7p",
                "forum": "NRIcs2TR7k",
                "replyto": "R23zRVzNdH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3649/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3649/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pnPG (1/2)"
                    },
                    "comment": {
                        "value": "Thank you very much for your appreciation of our work and your valuable suggestions on the presentation of our manuscript. In our newly submitted PDF, we have carefully revised the presentation of most paragraphs to improve its clarity and readability. We hope the new submission can solve your concerns about writing. \n\n### **W1: statements about ULIP**\n\nULIP aligns a 3D encoder to an image-text model called SLIP [1] (not CLIP) through 3D-image-text data. Ex-MCR only uses the aligned 3D-image representation of ULIP and extends it to a different image-text model (i.e., CLIP) via the paired-data-free way. So we are not reproducing or refining the 3D-text alignment of ULIP, but building a new alignment between the 3D representation and new image-text pre-training.\n\n[1] Slip: Self-supervision meets language image pre-training. Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie. ECCV 2022\n\n### **W2: repetition of acronyms**\n\nThanks for your suggestions, in the newly submitted paper we carefully refine the usage of acronyms to improve readability. The modifications of acronyms can be summarized as:\n\n1. Avoiding confusion due to many similar acronyms. Replace \u201cMCR\u201d with \"space\", \"intra-MCR alignment\" with \"intra-space alignment\", and \"inter-MCR alignment\" with\"inter-space alignment\". Remove unnecessary acronyms, like \u201cMLP\u201d, \u201cInfoNCE Loss\u201d and \u201cL2 Loss\u201d \n2. Use \"audio-text space\" \"image-text space\" and \"image-3D space\" in the introduction instead of using \"CLAP\", \"CLIP\", and \"ULIP\" to directly explain the aligned modalities in each space.\n\n### **W3 & W4: Statements in the \u201cmodality-centric consistency\u201d part**\n\nWe reorganize and rewrite the entire \"modality-centric consistency\" part in the new submission.\n\nIn the introduction, we avoid directly giving many self-defined terms without explicit explanation, such as: \u201cmodality-centric\u201d, \u201csemantic bias\u201d and \u201creflect an MCR space comprehensively\u201d. The so-called modality $\\mathcal{A}$-centric pseudo data means using only data of modality $\\mathcal{A}$ to retrieve semantically similar data in other modalities, while \"semantic bias\" means that one modality cannot be fully represented by another modality (giving examples of audio and images to support), and \"various modality-centric data\" denotes combining the pseudo pairs retrieved by different modalities.\n\nIn the first and second paragraphs of the Section 3.2.1, we define the \"single modality-centric data\" and \"various modality-centric data\", and provide examples of image of \u201cmushroom\u201d and audio of \u201cwind noise\u201d to illustrate that single modality-centric data cannot comprehensively reflect representations of different modalities in different MCR spaces.\n\n### **W5 & Q1 & Q2: unclear notation**\n\nThe $softmax(\\cdot)$ in Equation 1 and Equation 2 is over all the samples in used datasets.\n\nThe InfoNCE function in Equation 6 is calculated on all the samples in a training batch.\n\nThe tilde symbols mean the features are processed to be semantically consistent.\n\n### **W6: The metrics in Table 3,4,5,6**\n\nAt the caption of Table 3 and the beginning of Section 4.5, we add descriptions about the metrics used in Tables 3, 4, 5, and 6. In ablation experiments, we report the mAP metrics on audio-image retrieval (AVE) and audio-text retrieval (AudioCaps). In the Appendix, we provide detailed ablation experiment results on all the datasets and metrics.\n\n### **W7: Too crowded Figure 1**\n\nWe redesign Figure 1 and rewrite its caption. Figure 1 is divided into two sub-figures (a) (b). \n\n**To improve the simplicity of the figure, we made the following changes to Figure 1 (a):**\n\n1). Remove unnecessary text such as \"pull close\" and \"pull close & push away\" and replace them with a gray bold dash line.\n\n2). Replace text such as \"audio features\" \"CLAP Text features\" with corresponding vector symbols. On the one hand, keeping there are only vector symbols, modules, and lines in the pipeline figure, can display the calculation process more clearly. On the other hand, it echoes the notation definitions in Section 3.2 and helps the reader to quickly understand the meaning of the defined notations.\n\n3). Frame the base-MCR and two leaf-MCRs into three boxes, which not only improves the aesthetics and simplicity, but also emphasizes that the three pre-trained spaces are separate.\n\n4). Add explanations about some symbols at the bottom to help understand the illustrations.\n\n**For Figure 1 (b), the subfigure about various modality-centric data**\n\nwe cancel the repeated \"text-centric consistency\", \"audio-centric consistency\", and \"image-centric consistency\" which are difficult to understand. More emphasis is placed on combining pseudo-data pairs from different sources to form a data pool.\n\nFor captions, we do not broadly explain the entire picture, but provide relevant explanations for each sub-picture. And use more space to explain the more complex Figure 1 (b)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3649/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496177707,
                "cdate": 1700496177707,
                "tmdate": 1700497259341,
                "mdate": 1700497259341,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u4ZyYj7bHM",
                "forum": "NRIcs2TR7k",
                "replyto": "R23zRVzNdH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3649/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3649/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pnPG (2/2)"
                    },
                    "comment": {
                        "value": "### **Q3: The explanation of $\\{\\tilde{\\mathbf{a}}_i^A, \\tilde{\\mathbf{t}}_i^A, \\tilde{\\mathbf{t}}_i^I, \\tilde{\\mathbf{v} }_i^I\\}$**\n\nSince the pseudo data pairs retrieved using different modalities are shuffled to form the final data pool, we do not distinguish symbolically between data pairs from different sources. The $\\{ \\tilde{\\mathbf{a}}_i^A, \\tilde{\\mathbf{t}}_i^A, \\tilde{\\mathbf{t}}_i^I, \\tilde{\\mathbf{v} }_i^I \\}$ represents a pseudo-pair sample in the final data pool, and this sample can be generated from any source."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3649/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700496298380,
                "cdate": 1700496298380,
                "tmdate": 1700497271273,
                "mdate": 1700497271273,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jsk3c4ss2W",
                "forum": "NRIcs2TR7k",
                "replyto": "R23zRVzNdH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3649/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3649/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Deadline coming. Looking forward to your feedback."
                    },
                    "comment": {
                        "value": "Dear Reviewer pnPG,\n\nThanks again for your comments. We would like to kindly remind you that we tried our best to address the concerns you raised.\n\nWe kindly request your feedback as the rebuttal deadline is approaching in less than 1 day. We would be happy to discuss in detail if you have additional comments about our paper.\n\nBest regards, Authors"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3649/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700663539997,
                "cdate": 1700663539997,
                "tmdate": 1700663539997,
                "mdate": 1700663539997,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nQkOPeLkX8",
                "forum": "NRIcs2TR7k",
                "replyto": "MvkwzZZh7p",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3649/Reviewer_pnPG"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3649/Reviewer_pnPG"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their rebuttal. While I find the overall clarity and writing of the paper to still be improvable, I find the modifications to already point in the right direction and be enough for the reader to understand the method. I am confident the paper will be further improved for the camera-ready version. I am therefore raising my score to 6."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3649/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700724193293,
                "cdate": 1700724193293,
                "tmdate": 1700724193293,
                "mdate": 1700724193293,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]