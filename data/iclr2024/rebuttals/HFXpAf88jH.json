[
    {
        "title": "Beyond Implicit Bias: The Insignificance of SGD Noise in Online Learning"
    },
    {
        "review": {
            "id": "UcxVkXZ3dH",
            "forum": "HFXpAf88jH",
            "replyto": "HFXpAf88jH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5885/Reviewer_7b7M"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5885/Reviewer_7b7M"
            ],
            "content": {
                "summary": {
                    "value": "This paper investigates the influence of SGD noises, specifically batch size and learning rate, on implicit bias within an online learning context. Through comprehensive experiments, the authors demonstrate that unlike in offline settings, SGD noise does not confer any additional advantages in online learning.\n\nFurthermore, the authors introduce and explore the \"golden path hypothesis\" in relation to online learning. Empirical analysis suggests that for real-world data utilizing deep neural networks, a \"noiseless\" or \"golden\" path trajectory may be present, implying that SGD could potentially mimic the trajectory of gradient flow algorithms."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The problem studied in this paper is important as the LLMs might adopt the examined online method to update their parameters. This paper performs extensive experiments to support their emperical findings."
                },
                "weaknesses": {
                    "value": "The online learning setting investigated lacks a rigorous and detailed formulation.  See more details in Questions."
                },
                "questions": {
                    "value": "1. The online learning protocol discussed in this paper is not entirely clear to me. Could the authors provide with a more detailed formulation of the online learning procedure using SGD? In the online learning contexts I'm familiar with, such as Prediction with Experts' Advice, regret is typically employed as a performance measure. Could the authors clarify how the algorithm's loss is assessed in the online learning setting under consideration?\n\n2. I'm curious about the relationship between the convergence rate and the choice of adaptive learning rate. Is the observed behavior consistent when using optimizers like Adam?\n\n3. How does this research account for or negate the effects of the neural network's architecture?\n\n4. I'm interested in understanding the design of the experiments. Given that in the real-world online learning setting, achieving comparable performance can be more challenging without full access to the dataset, yet it offers efficiency advantages. Were there particular measures or modifications incorporated to guarantee an fair comparison with the offline setting?\n\nWhile empirical studies involving SGD algorithms fall outside my primary domain of expertise, I am open to further discussions on the topic."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "1: You are unable to assess this paper and have alerted the ACs to seek an opinion from different reviewers."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5885/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5885/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5885/Reviewer_7b7M"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5885/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698669853362,
            "cdate": 1698669853362,
            "tmdate": 1699636623854,
            "mdate": 1699636623854,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XJcAoNvLGd",
                "forum": "HFXpAf88jH",
                "replyto": "UcxVkXZ3dH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5885/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5885/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 7b7M"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their comments. We address specific points below:\n\n>The online learning protocol discussed in this paper is not entirely clear to me. Could the authors provide with a more detailed formulation of the online learning procedure using SGD? In the online learning contexts I'm familiar with, such as Prediction with Experts' Advice, regret is typically employed as a performance measure. Could the authors clarify how the algorithm's loss is assessed in the online learning setting under consideration?\n\nThe definition of online learning is provided on the first line of Page 2 as single epoch training. In particular, we are considering the *stochastic online learning* setting, where at each instance the loss function is drawn from an underlying distribution, and the regret is measured according to the loss exhibited by the current algorithm and an underlying truth (which is assumed to be providing the correct labels at each step). \n\n> I'm curious about the relationship between the convergence rate and the choice of adaptive learning rate. Is the observed behavior consistent when using optimizers like Adam?\n\nYes, the experiments in section 2 on Imagenet and C4 dataset are with Adam. The CIFAR-5m experiments use SGD.\n\n>How does this research account for or negate the effects of the neural network's architecture?\n\nThis paper shows that the result holds on architectures including CNNs and Transformers, thus showcasing the agnostic nature of the results with respect to architecture.\n\n>I'm interested in understanding the design of the experiments. Given that in the real-world online learning setting, achieving comparable performance can be more challenging without full access to the dataset, yet it offers efficiency advantages. Were there particular measures or modifications incorporated to guarantee an fair comparison with the offline setting?\n\nNote that in this work, we are considering the *stochastic online setting*, and thus the samples are indeed drawn from an underlying distribution even for the online setup, which does not change with time. In this sense, this is not the adversarial online setting. \nMoreover, the only difference between online and offline setting in this work is whether we run training for multiple epochs or not.\n\nPlease consider raising your score if you feel that your concerns were answered, and if not we are happy to continue discussing your comments."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5885/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259655936,
                "cdate": 1700259655936,
                "tmdate": 1700259784310,
                "mdate": 1700259784310,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rg7HS9AoJJ",
            "forum": "HFXpAf88jH",
            "replyto": "HFXpAf88jH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5885/Reviewer_6xnX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5885/Reviewer_6xnX"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the potential implicit bias effect of SGD noise in online learning. The authors observe from experiments that in online learning settings. SGD noise does not bring any implicit bias and it is \"just noise\".  Next, based on experiments, the authors also proposed the \"golden path\" hypothesis, which states that SGD with different noise levels follows the same trajectory (which they call \"golden path\") in function space in online learning setting. The authors also perform experiments to support their hypothesis."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The main result in this paper that SGD follows the same path in online learning settings is an interesting finding in my opinion.\n\n2. The experiments support the main claims well, and the claims made by the paper are stated clearly in general."
                },
                "weaknesses": {
                    "value": "1. I would like to understand more about the scope of the main results:\n\n- The experiments are performed on Resnet18, ConvNext-T, and GPT-2 small, which are relatively large models. I'm wondering if the main hypothesis of this paper also holds for smaller models, or if this phenomenon might be due to the overparameterization of the models?\n\n- The study of this paper focuses on SGD noise, i.e. the noise comes from not using full-batch. I'm wondering if the main hypothesis also holds for manually added noise (e.g. noisy gradient descent like Langevin dynamics) ?\n\n- A minor point: In the paper, your main findings and hypothesis are made for SGD, while in your experiments, the optimizers used are SGD with momentum (for ResNet18), AdamW (for ConvNext-T), and \"default optimizer in Mosiac ML\" . So the main hypothesis is not only for SGD but also for difference optimizers?\n\n- As you mentioned in the discussion at the bottom of page 2, the \"golden path\" is the noiseless gradient flow. I'm wondering if you could compare the trajectory of SGD to the actual gradient flow (i.e. GD with full batch, and very small step size) ?\n\n2. The experiments on reducing the step size are not very clear to me, since the step size also affects the sharpness of the solution SGD can find (as you the \"edge of stability\" phenomenon you mentioned in Appendix D). So it seems to me that the decrease of loss after decreasing the step size may be due to the fact that the dynamic is around a local minimum of certain sharpness, and a smaller step size allows it to go into this local minimum, rather than a better approximation of the \"golden path\" due to smaller SGD noise. Similar arguments could also made for the experiments on increasing step size."
                },
                "questions": {
                    "value": "Please refer to the strengths and weaknesses part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5885/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5885/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5885/Reviewer_6xnX"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5885/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698685918112,
            "cdate": 1698685918112,
            "tmdate": 1699636623752,
            "mdate": 1699636623752,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "y5RZMobG2K",
                "forum": "HFXpAf88jH",
                "replyto": "rg7HS9AoJJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5885/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5885/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 6xnX"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their insightful comments. We appreciate the reviewer observing that our paper is interesting and their general curiosity towards the work. We address the comments raised by the reviewer below. \n\n>The experiments are performed on Resnet18, ConvNext-T, and GPT-2 small, which are relatively large models. I'm wondering if the main hypothesis of this paper also holds for smaller models, or if this phenomenon might be due to the overparameterization of the models?\n\nWe start by noting that for Resnet18 and GPT-2 small the number of parameters in the model is smaller than the dataset. Further we did experiments on narrower variants of Resnet18 (8 channels instead of 64) as well as small CNNs and found no differences in our results. We would be happy to add these to the final version. We would also be happy to share these in the rebuttal period if the reviewer thinks this is critical to their opinion of the paper.\n\n\n> A minor point: In the paper, your main findings and hypothesis are made for SGD, while in your experiments, the optimizers used are SGD with momentum (for ResNet18), AdamW (for ConvNext-T), and \"default optimizer in Mosiac ML\" . So the main hypothesis is not only for SGD but also for difference optimizers?\n\n\n\nIn general, by SGD noise, we refer to the noise induced due to mini-batch sampling and finite learning rates. Our general claim is about trajectory with and without SGD noise. We would clarify this in the paper that our results hold for other optimizers as well. We note that the MosaicML optimizer is also AdamW.\n\n>The experiments on reducing the step size are not very clear to me, since the step size also affects the sharpness of the solution SGD can find (as you the \"edge of stability\" phenomenon you mentioned in Appendix D). So it seems to me that the decrease of loss after decreasing the step size may be due to the fact that the dynamic is around a local minimum of certain sharpness, and a smaller step size allows it to go into this local minimum, rather than a better approximation of the \"golden path\" due to smaller SGD noise. Similar arguments could also made for the experiments on increasing step size.\n\nYes, we indeed agree that the drop in loss happens both due to Edge of Stability-like phenomena and due to reduction in noise. The main claim is that SGD noise doesn\u2019t affect the overall dynamics in the sense that you can recover the golden path by reducing the noise during training. We are not claiming that the SGD path itself is a noisy version of the gradient flow/golden path.\n\n>The study of this paper focuses on SGD noise, i.e. the noise comes from not using full-batch. I'm wondering if the main hypothesis also holds for manually added noise (e.g. noisy gradient descent like Langevin dynamics) ?\n\n\n\n\nThis is an interesting question and our current intuition would be that it would not help in online learning tasks. But we do not have any experiments to this end since our work was focused on SGD noise. \n\n\n\n>As you mentioned in the discussion at the bottom of page 2, the \"golden path\" is the noiseless gradient flow. I'm wondering if you could compare the trajectory of SGD to the actual gradient flow (i.e. GD with full batch, and very small step size) ?\n\nWe do this in:\n1. Loss trajectory: Figure 10 where we show that with smaller learning rates and larger batch sizes the loss trajectory converges to gradient flow and the performance improves with smaller learning rates and larger batch sizes.\n2. Function space trajectory: In Figure 14 we decrease the learning rate by more than a factor of 100 and increase the batch size by a factor of 16 and find that after we decrease the SGD noise the TV distance curves match.\n\nWe note that the main claim is that SGD noise doesn\u2019t affect the overall dynamics in the sense that you can recover the golden path by reducing the noise during training. We are not claiming that the SGD path itself is a noisy version of the gradient flow/golden path.\n\nPlease consider raising your score if you feel that your concerns were answered, and if not we are happy to continue discussing your comments."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5885/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700260034576,
                "cdate": 1700260034576,
                "tmdate": 1700260034576,
                "mdate": 1700260034576,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "98m9Ek8yHD",
            "forum": "HFXpAf88jH",
            "replyto": "HFXpAf88jH",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5885/Reviewer_RXA2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5885/Reviewer_RXA2"
            ],
            "content": {
                "summary": {
                    "value": "This paper carries out a series of experiments to compare offline learning (multiple-epoch training) with online (single epoch) learning. The experiments are based on commonly used image and language data and the focus of the experiments is the role played by \"SGD noise\". High SGD noise refers to high learning rate or small batch size. Unlike the offline learning, the benefits of SGD noise are not observed in the experiments for online learning. It is conjectured that SGD in the online learning case can be interpreted as noisy learning along the \"golden path\" of the noiseless gradient flow algorithm."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. It is very interesting that SGD noise plays a different role between single and multiple epoch regimes.\n\n2. Figures are well-presented and convey succinct summary of experimental results.\n\n3. The expressions \"Fork in the Road\" and \"Golden Path\" are eye-catching terms that create instant curiosity."
                },
                "weaknesses": {
                    "value": "1. The paper is mostly well written; however, the details behind the experimental results are somewhat sparse, including the appendix. Some further clarifications would strengthen the paper substantially. For example, on page 5, it is stated that \"To imitate the online regime with ImageNet, we only train for 10 epochs with data augmentation.\" In the abstract, online learning refers to the single epoch regime but on page 5, it seems that this is not the case. Furthermore, Appendix A contains very short explanations for each of experiments. It is hard to understand exactly what was done in the experiments given the sparse information provided in the paper.\n\n2. All the claims in the paper are entirely driven by the experiments; there are no theoretical results. It would be more prudent if the author(s) could provide the limitation of the current paper on page 9."
                },
                "questions": {
                    "value": "1. It is unclear how many epochs are considered in multiple-epoch training across different experiments. For example, in Figure 1, the top and bottom rows, respectively, show the results from offline learning (multiple-epoch training) and those from online learning (single-epoch training). The training steps are on the same scale between the top and and bottom rows. In the case of top rows, there is no indication of how many epochs are considered. It would be helpful to provide further details. \n\n2. Related to the previous point, is it OK to interpret the X axis the same way between the top and bottom figures in Figure 1? For example,  would it be possible that the patterns observed in offline learning can appear if the number of training steps in online learning is much larger, say, 10 or 100 time larger than 4000? The early paths observed in offline learning are quite similar to those observed in online learning. \n\n3. In addition, what are details of multiple-epoch training? Is multiple-epoch training conducted via random shuffling of the datapoints after each epoch or a simple random sampling of data points with replacement at each step (or something different)? Again it would be helpful to understand the exact nature of multiple-epoch training.\n\n4. The provided supplementary material does not include replication files. Given that the current paper is experimental, it would be useful if all replication files are provided on public domain."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5885/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5885/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5885/Reviewer_RXA2"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5885/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698880711083,
            "cdate": 1698880711083,
            "tmdate": 1699636623659,
            "mdate": 1699636623659,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "lZ2vO324bP",
                "forum": "HFXpAf88jH",
                "replyto": "98m9Ek8yHD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5885/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5885/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer RXA2"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for their time reviewing our work. We appreciate the reviewer observing that our paper is interesting, well presented and their general curiosity towards the work. We address the comments raised by the reviewer below. \n\n**Details about the experimental results:** We provided these in Appendix A across all three datasets, including details about all pertinent hyperparameters. Please let us know if there is specific information absent in this section; we are happy to discuss them and provide any other details.\n\n\n**Multiple epochs for Imagenet:** Since the Imagenet dataset has fewer samples than the other two datasets we use (CIFAR-5m and C4) to achieve reasonable accuracy we had to train for more than one epoch. To that end, we used data augmentation as a way to remain in the \u201conline regime\u201d. We verified that we were in the online regime by checking that train and test loss/error were the same. We agree this is a valid limitation and will further discuss it in the final version. \n\n\n**Theoretical results:** Yes, we do agree that we don\u2019t have any theoretical results as of now. However, for convex setting, we can show that our arguments hold for specific cases, and we provide an example below. For non-convex settings, it is not difficult to come up with a counter-example in which small learning rate trajectories get stuck in local minima and hence  perform worse. However, as our results hold in practical settings, we believe it would be interesting to come up with the correct assumptions in non-convex settings for which our results hold. Below we describe a simple convex setting where our results hold:\n\nCertainly, here is the text with LaTeX formatting for the mathematical expressions, where each expression is enclosed in $ signs to render as math when compiled with LaTeX:\n\nConsider a 1D convex optimization problem where $ x \\sim N(0, 1) $ and $ y^* = 0 $. The square loss is given by $ 0.5w^2x^2 $. For SGD in online learning, with batch size 1 and learning rate $ \\eta $, the iterate at time $ t $ is given by $ w_t = \\prod_{i=1}^{t} (1 - \\eta x_i^2)w_0 $. Then, the expected loss at time $ t $ is given by $ 0.5w_0^2(1 - 2\\eta + 3\\eta^2)^t $.\n\nConsider another SGD trajectory, with learning rate $ \\eta/2 $; its loss at time $ 2t $ is given by $ 0.5w_0^2(1 - \\eta + 0.75\\eta^2)^{2t} $. Now, if at time $ t $, we switch the first trajectory from $ \\eta $ to $ \\eta/2 $, then the expected loss at time $ t' > t $ is given by $ 0.5w_0^2(1 - 2\\eta + 3\\eta^2)^t(1 - \\eta + 0.75\\eta^2)^{t'-t} $. Thus, in this convex setting, the shape of the expected loss curve will be the same for the two curves, but only after an appropriate time shift to allow for the losses to match.\n\n\n**Unclear how many epochs are in each plot:**  We will add the number of epochs in the caption. For now we give the number of epochs in Fig 3. 3a (top): 15, 60, 240 epochs for batch size 32, 128 and 512 batch size respectively. 3a (bottom): 240, 60, 30 for LRs .025, .1 and .2 respectively. 3b (top): 56, 112, 224 for batch sizes 2048, 4096 and 8192 respectively. 3b (bottom): 168, 84, 56 epochs for LRs .0003, .0006 and .001 respectively.\n\n**Offline curves track online curves:** The reason that offline curves track the online ones is not a coincidence. Rather, every offline training run starts with new data never seen before. Then, as data is repeated more and more, the train and test performance curves diverge and the difference between the regimes becomes noticeable. See Nakkiran et al. 2020 (https://arxiv.org/abs/2010.08127) for a detailed discussion of this phenomenon. \n\n**Effect of longer training in online regime:** In all of our experiments, to the best of our budget, we never saw that the lower noise (with the right movement axis) was performing better than the high noise ones. This includes runs of larger scale than Figure 1.\n\n**Details of multiple-epoch training:** The multi epoch training was done with full shuffle before each epoch. \n\n**Code:** The codebase used for the C4 experiments is LLM foundry: https://github.com/mosaicml/llm-foundry/tree/main. We will add the codebases for CIFAR-5m and Imagenet in a few days.\n\n\nPlease consider raising your score if you feel that your concerns were answered, and if not we are happy to continue discussing your comments."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5885/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700260419017,
                "cdate": 1700260419017,
                "tmdate": 1700260676980,
                "mdate": 1700260676980,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]