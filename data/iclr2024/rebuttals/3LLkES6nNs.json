[
    {
        "title": "Infinitely Deep Residual Networks: Unveiling Wide Neural ODEs as Gaussian Processes"
    },
    {
        "review": {
            "id": "Nvb9FQIgN2",
            "forum": "3LLkES6nNs",
            "replyto": "3LLkES6nNs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1999/Reviewer_TYVT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1999/Reviewer_TYVT"
            ],
            "content": {
                "summary": {
                    "value": "The authors develop a connection between Gaussian process and autonomous and nonautonomous neural ODE based on random matrix theory. They also show that the kernel of the Gaussian process is strictly positive definite if the input is restricted to the unit sphere. Furthermore, they provide an algorithm for computing th covariance matrix."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors analyze neural ODE from the perspective of Gaussian process. They provide the covariace matrix of the Gaussian process. The analysis is benefitial for better understandings of Neural ODE."
                },
                "weaknesses": {
                    "value": "The paper is sometimes hard to follow. Some assumptions are not clearly stated, for example,\n- For Eq. (4), do we need some assumptions regarding $A$? I don't think any square matrix satisfies Eq. (4). \n- After Propostion 3.1., the authors say \"our ResNet $f_{\\theta}^L$ approximates the Neural ODE $f_{\\theta}$ effectively in the limit as L approaches infinity\". In my understanding, that happens if $T$ is fixed. In that case, as the step size goes to 0, $L$ goes to infinity. If that is correct, please clarify that.\n- In my understanding, to prove Theorem 3.2, we need Lemmas C.1 and C.2, which involves the controllability of the function. The authors say \"The convergence is achived under the assumption of a controllable activation function.\" before Definition 3.1. However, in Theorem 3.2, they do not mention the controllability. Should we assume the activation functions are controllable throughout the paper? In that case, please clarify that."
                },
                "questions": {
                    "value": "Minor comments:\n- Equations should be referred with parenthesis (e.g. equation (1)). Use \\eqref instead of \\ref.\n- Some references should also be cited with parenthesis. Use \\citep if you need the parenthesis.\n- In Lemma C.2, there is an unknown label for an equation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1999/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1999/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1999/Reviewer_TYVT"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1999/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698441199231,
            "cdate": 1698441199231,
            "tmdate": 1699636131367,
            "mdate": 1699636131367,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "HqxDYLgh1X",
                "forum": "3LLkES6nNs",
                "replyto": "Nvb9FQIgN2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1999/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1999/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for taking the time to review our paper and for providing constructive feedback. We have addressed the raised concerns by providing detailed responses as follows. Additionally, we have restructured certain sections, including the introduction and preliminary parts, to enhance the paper's comprehensibility and readability.\n\n**Response to Weaknesses 1**: Yes, you're correct. Equation (4) holds if each $A_{ij}$ follows i.i.d. standard Gaussian, and this assumption can be relaxed up to independent sub-Gaussian random variables [1]. We have corrected it in our revised version.\n\n**Response to Weakness 2**: Yes, you are correct. We assume the time $t$ has a fixed range $[0,T]$ with $T$ being fixed constant. By establishing time step $\\beta=T/L$ through the Euler's method,  we can consider $f_{\\theta}^{L}$ as a discrete approximation to the Neural ODE $f_{\\theta}$. As time step $\\beta$ approaches to $0$ or the depth $L$ tends to infinity, the discrete approximation $f_{\\theta}^{L}$ converges to $f_{\\theta}$.\n\n**Response to Weakness 3**: Yes, you are correct. Indeed, a controllable activation function is a requirement for the proof of Theorem 3.2. As stated in Theorem~3.2, a Lipschitz continuous activation function can be easily shown to be controllable, a fact that has been addressed in [2] and proven in [3]. We've enhanced this explanation in the revised version to clarify this crucial aspect.\n\n**Response to Question 1**: Thank you for highlighting this. We've updated the equation references using \"eqref\" instead of \"ref\".\n\n**Response to Question 2**: Thank you for pointing that out. We've updated the reference citation style to use \"citep\" instead of \"cite\".\n\n**Response to Question 3**: The label for the equation in Lemma C.2 has been updated.\n\n[1] Roman Vershynin. High-dimensional probability: An introduction with applications in data science, volume 47. Cambridge University Press, 2018.\n\n[2] Yang, Greg. \"Wide feedforward or recurrent neural networks of any architecture are Gaussian processes.\" Advances in Neural Information Processing Systems 32 (2019).\n\n[3] Gao, Tianxiang, et al. \"Wide Neural Networks as Gaussian Processes: Lessons from Deep Equilibrium Models.\" arXiv preprint arXiv:2310.10767 (2023)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1999/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700210282374,
                "cdate": 1700210282374,
                "tmdate": 1700210282374,
                "mdate": 1700210282374,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CAxEHypNHL",
                "forum": "3LLkES6nNs",
                "replyto": "HqxDYLgh1X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1999/Reviewer_TYVT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1999/Reviewer_TYVT"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. I will keep my score."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1999/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674274370,
                "cdate": 1700674274370,
                "tmdate": 1700674274370,
                "mdate": 1700674274370,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Pb61maj9Z1",
            "forum": "3LLkES6nNs",
            "replyto": "3LLkES6nNs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1999/Reviewer_ECfA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1999/Reviewer_ECfA"
            ],
            "content": {
                "summary": {
                    "value": "This paper uses infinite ResNet to analyze the convergence and prediction performance of wide neural ODEs, where it thinks the Neural ODEs can be regarded as an infinite ResNet."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper regards the NODE as the infinite deep ResNet, which is interesting.\n2. The motivation is meaningful to understand the neural network with the help of the relationship of the ODE and the NN."
                },
                "weaknesses": {
                    "value": "1. Poor format and logic. e.g.\"width is infinity..\"; in Fig.1, \"Distribution of one output neuron: Neural ODE and ResNet w/wo shared weights\" does not correspond to its legend \"Neural ODE, Shared ResNet, Indep. ResNet\".\n2. Contribution is poor and is not well supported."
                },
                "questions": {
                    "value": "1. As I know, not every layer of the standard ResNet has the same number of parameters/channels, so how do we achieve the shared parameter? If you are using a simplified version of ResNet, it should be clarified. \n2. In the first sentence of the second paragraph of the introduction, missing some references to support your presentation. Can you give me some references\uff1f\n3.\"we are faced with infinite-depth residual neural networks with shared weights...\" Although intuitively, I think this may be right, there still is not enough evidence to support it."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1999/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698630693593,
            "cdate": 1698630693593,
            "tmdate": 1699636131300,
            "mdate": 1699636131300,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pQgNPr0WgM",
                "forum": "3LLkES6nNs",
                "replyto": "Pb61maj9Z1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1999/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1999/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your feedback and have carefully addressed the concerns raised. Here are our responses to each point:\n\n**Response to Weakness 1**:\n* The phrase \"width is infinity\" has been revised to \"as the width goes towards infinity.\"\n* The caption of Figure 1 has been corrected to match the legend, replacing \"ResNet without shared weights\" with \"Indp. ResNet.\"\n\n**Response to Weakness 2**:\nWe would like to emphasize the significant **contributions** of our paper:\n* We propose a new framework for studying Neural ODEs by considering them as infinite-depth neural networks with shared weights. Using Euler's method, we introduce a ResNet with shared weights as a discrete version of Neural ODEs, ensuring the approximation's validity as the depth approaches infinity (Proposition 3.1).\n* By considering Neural ODEs as infinite-depth neural networks with shared weights, we establish the Neural Network Gaussian Process (NNGP) correspondence for Neural ODEs. Our results (Theorem 3.2 and Theorem 3.6) reveal distinct convergence Gaussian processes with different covariance functions, dependent on whether shared weights are applied.\n* Previous studies [1-4] have demonstrated the importance of a strictly positive definite NNGP kernel for global convergence and good generalization. Here, we prove that the NNGP kernel for Neural ODEs is strictly positive definite when the activation is non-polynomial (Theorem 3.4 and Theorem 3.9).\n* Due to skip connections and shared weights, we provide a new dynamic programming in Algorithm 1 to approximately compute the NNGP kernel's explicit form efficiently, a critical contribution not introduced in previous work.\n\nMoreover, our theoretical findings are **supported** by numerical experiments:\n* Gaussian behaviors of wide Neural ODEs are observed in Figure 1.\n* Distinct Gaussian behaviors of ResNets are depicted in Figure 1 and Figure 2, supporting our theoretical distinctions. Notably, Neural ODE and ResNet with shared weights exhibit similar Gaussian behaviors, whereas ResNet with independent weights demonstrates a smaller variance.\n* The strictly positive definiteness of the NNGP kernel for Neural ODEs is validated in Figure 3.\n* To complement our theoretical findings, we conducted numerical experiments on real datasets to assess the performance of Gaussian Processes (GP) with NNGP kernel and finite-width Neural ODEs in terms of test accuracy. As depicted in the last plot of Figure 3, we observed that NNGP generally outperforms finite-width Neural ODEs. Moreover, the performance of finite-width Neural ODEs converges to that of NNGP with the growth of width.\n\n**Response to Question 1**: In our study of Neural ODEs that commonly employ shared parameters in practice, we utilize Euler's method to approximate them via a ResNet with scaling factor $\\beta=T/L$ on the residual branch. This approximation is detailed in Equations (1) and (3) in the introduction and preliminary section. Additionally, shared weights are a common setup, particularly in the analysis of sequential data, as seen in recurrent neural networks.\n\n**Response to Question 2**:\nWe have added additional references ([1-4]) to further support our observations based on previous works.\n\n**Response to Question 3**: In the principle of Euler's method, we theoretically prove in proposition 3.7 that Neural ODEs can be considered as infinite-depth neural networks with shared weights. We also conduct numerical experiments to support these results. Due to space limitations, we defer the numerical results to Figure 4 in Appendix J. We invite the reviewer to take a look.\n\n\n[1] Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. ICML 2019. \n\n[2] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. ICML 2019. \n\n[3] Quynh Nguyen and Marco Mondelli. Global convergence of deep networks with one wide layer followed by pyramidal topology. ICLR 2020. \n\n[4] Arora, Sanjeev, et al. \"Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks.\"\u00a0ICML 2019."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1999/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700210987915,
                "cdate": 1700210987915,
                "tmdate": 1700328777842,
                "mdate": 1700328777842,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Qvvh7nYX48",
                "forum": "3LLkES6nNs",
                "replyto": "pQgNPr0WgM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1999/Reviewer_ECfA"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1999/Reviewer_ECfA"
                ],
                "content": {
                    "comment": {
                        "value": "Considering the points addressed in the rebuttal, I still find the contribution's significance to be limited, and the adherence to the required writing format remains a concern. Therefore, I am maintaining my previous assessment and score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1999/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700558315618,
                "cdate": 1700558315618,
                "tmdate": 1700558315618,
                "mdate": 1700558315618,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YtwM4bxLR9",
            "forum": "3LLkES6nNs",
            "replyto": "3LLkES6nNs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1999/Reviewer_CRfF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1999/Reviewer_CRfF"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies signal propagation in neural ODEs and their discretizations at initialization. The authors consider both temporally constant weights and temporally independent weights and derive convergence rates for the preactivations for the case where weights are temporally constant. They also derive recursions for the limiting covariance kernels for the two cases of weight tying. These recursions can be solved with a dynamic programming method by simply storing results for previously computed kernels for earlier layers and looking them up to compute later layer correlations. The authors show that temporally shared weights give rise to different kernels than temporally independent weights."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper studies an interesting topic of infinite depth residual networks, which has recently begun to receive more attention from theorists. The results appear correct and sensible, especially to someone who is familiar with NNGP/NTK type results for non-residual architectures. The proof techniques appear valid, though I am not an expert in this area of random matrix theory. Further, the authors provide some numerical simulations, demonstrating Gaussianity of preactivations and convergence of covariance kernels to their limit."
                },
                "weaknesses": {
                    "value": "The primary weakness of this work is that (as far as I can tell) the main result is not as novel as claimed. The ODE model with temporally constant weights focused on in this paper is not a new architecture but is really just a randomly connected recurrent RNN, which has been analyzed by physicists and theoretical neuroscientists for decades. In particular, this is exactly the Cristanti and Sompolinsky model of a RNN (https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.61.259, https://journals.aps.org/pre/abstract/10.1103/PhysRevE.98.062120) without a leak term. Techniques like dynamical mean field theory (DMFT) can be used to calculate the large width limit of these dynamics and would recover identical dynamics for the covariances as the authors provide. Some other relevant papers are Helias & Dahmen book (https://arxiv.org/abs/1901.10416).  There are also prior works which solve the equations without non-stationary assumptions (https://proceedings.neurips.cc/paper_files/paper/2022/file/e6f29fb27bb400f89f5584c175005679-Paper-Conference.pdf).\n\nThe authors should make contact with this literature and discuss a comparison with their derived limit and the limit for DMFT for random RNNs."
                },
                "questions": {
                    "value": "1. In equation 3, is it clear that beta is always O(1/L) ? I would expect it needs to scale as O(1/sqrt{L}) if the weights are independent across times. If so, this should be stated clearly somewhere. I think this is important because the scaling of beta with T should determine whether one gets the Log-Gaussian result of Li et al (beta ~ O(1) and L ~ N) or the SDE type limit Hayou and Yang beta ~ 1/sqrt{L}. Further the convergence result for preactivations should only hold in the shared weights case with U fixed and for a fixed realization of the weights W.   \n2. What is going on in eq 4? Not clear at all how this relates or what assumptions on A make this equation hold.\n3. Figure 2: why are the shared weight networks converge to their limit, but the non-shared networks do not converge to their limit? Is there any analysis of finite size error that would predict this? \n4. Figure 3: It is unclear to me if the smallest eigenvalue is the proper metric. Numerical stability of algorithms usually depends on some kind of normalized metric of smallest eigenvalue like condition number which compares largest to smallest eigenvalue. I am wondering if the shared weights has better condition number rather than just larger minimum eigenvalues. \n5. Why do the authors refer to this model as a Neural ODE rather than a RNN? My impression was that weight sharing across layers is what distinguished RNNs from standard feedforward networks. \n\n\nIf the authors could answer these questions and address the discuss the connection to random RNN models, I would consider raising my score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1999/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698671421910,
            "cdate": 1698671421910,
            "tmdate": 1699636131216,
            "mdate": 1699636131216,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "H00o4bQsqm",
                "forum": "3LLkES6nNs",
                "replyto": "YtwM4bxLR9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1999/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1999/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We greatly appreciate the time and effort you dedicated to reviewing our paper and providing constructive comments. In response to your concerns, we've clarified and updated our manuscript. We hope that these revisions address your points and potentially contribute to an improved score.\n\n**Response to Weaknesses**: We sincerely appreciate your thoughtful review and the valuable references you provided. Addressing your concerns, we'd like to clarify the uniqueness and relevance of our work within the domain of Neural ODEs in comparison to the references cited.\n\nOur study primarily focuses on investigating the trainability and learnability of Neural ODEs. This distinction is essential as the referenced works primarily explore the stability of the dynamic systems or the solution of these systems themselves. This fundamental difference in purpose and motivation significantly shapes the trajectory of our theoretical investigations, setting our study apart from the referenced literature.\n\nPrimarily, while our paper delves into the NNGP correspondence of Neural ODEs, **another significant contribution**, as highlighted in Theorem 3.9, is the **positive definiteness of NNGP kernel $\\Sigma^{*}$** or the covariance function. This result, unfortunately overlooked in the review's summary section, is crucial. Prior studies on finite-depth neural networks [4-7] have shown that a strictly positive definite NNGP kernel serves as a pivotal condition for establishing global convergence of the training process on given data and for estimating generalization performance on unseen data. Hence, Theorem 3.9 serves as a fundamental metric for future investigations into the trainability and generalization estimations for Neural ODEs.\n\nApart from our primary focus, several other differences distinguish our study from the referenced works. For instance, we confine Neural ODEs to a fixed time range $[0, T]$, where $T$ remains a constant, while [1-3] explore the system's dynamics in a long-time setting as $T\\rightarrow\\infty$. Additionally, unlike the referenced works, we don't assume the weight matrix $W_{ij}$ (or synaptic matrix $J_{ij}$ in referenced works) to have zero diagonals. Notably, the assumption of zero diagonals could be significant within the context of DMFT as it simplifies the analysis by removing self-connection within the neural networks. On the other hand, with $W_{ii}=0$, our results encompass and could potentially derive their findings using different mathematical tools like random matrix theory. Moreover, our study maintains a relatively broader scope by assuming only Lipschitz continuity for the activation function $\\phi$, thereby encompassing choices like tanh and ReLU utilized in the cited works.\n\nSince the primary focus of the references is on long-time dynamics of the dynamical systems, the **covariance matrix computation** is not their concern. In our work, we provide an efficient dynamic programming algorithm to compute the covariance matrix of the Gaussian process, which can be used for practical purposes like Bayesian inference on a given dataset (as shown by our numerical example) or for theoretical purposes in further study on the neural tangent kernel and training of neural ODEs, which is not provided nor necessary in previous studies as most prior studies focus on finite-depth neural networks with independent weights.\n\nIn summary, while certain intermediate results might resemble those in the references, our study's distinct focus, assumptions, and broader implications set it apart in terms of understanding the trainability and generalization of Neural ODEs.\n\n\n**Response to Question One**: As we study Neural ODEs using ResNets, the scaling of $\\beta$ aligns with the time step, a fundamental aspect derived from Euler's method. Hence, this scaling typically is $\\beta = T/L$ and thus is of order $\\mathcal{O}(1/L)$. We've demonstrated this relationship in Proposition 3.1 and Figure 4 in Appendix J, showcasing how our approximation of $f_{\\theta}^{L}$ can efficiently approximate $f_{\\theta}$ as $\\beta$ tends to zero or as $L$ approaches to infinity. We agree with the reviewer that scaling the residual branch is critical. Different combinations of scaling and independence in weights lead to varied outcomes, such as diverse stochastic differential equations or distinct distributions of the solution. These aspects will be considered as our future works.\n\nFurthermore, the results from random matrix theory [8] and tensor program [9] allow us to achieve convergence of preactivation without the need for a fixed $U$. These results guarantee convergence even for independence weights. However, as emphasized in in Theorem~3.2 and Proposition 3.7, the resulting Gaussian processes vary based on whether the weights are shared or not, impacting the covariance function of NNGP kernel differently."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1999/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700211646258,
                "cdate": 1700211646258,
                "tmdate": 1700211739062,
                "mdate": 1700211739062,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Q5hXSK8c9v",
                "forum": "3LLkES6nNs",
                "replyto": "YtwM4bxLR9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1999/Reviewer_CRfF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1999/Reviewer_CRfF"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for their in-detail response to my questions. The authors emphasize that one of their key contributions is establishing the positive definiteness of the NNGP kernel, which I acknowledge as a novel contribution. I disagree that pre-existing approaches/studies on RNNs could not handle or compute the kernels either (a) finite time intervals (rather than long time behavior) or (b) random diagonal weight entries (see for instance https://arxiv.org/abs/1809.06042). \n\nI am still unsure of whether this contribution is sufficiently important to justify acceptance so I am currently keeping my score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1999/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700320376161,
                "cdate": 1700320376161,
                "tmdate": 1700320376161,
                "mdate": 1700320376161,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Q7RwIA5Sdm",
                "forum": "3LLkES6nNs",
                "replyto": "BisrjHhQ3r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1999/Reviewer_CRfF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1999/Reviewer_CRfF"
                ],
                "content": {
                    "title": {
                        "value": "Some additional follow ups"
                    },
                    "comment": {
                        "value": "A few more follow up discussion points\n\n1. The second paragraph is referring to a symmetric case for $J$. Please check equation $4$ for the asymmetric model.  Further, in the asymmetric RNN calculation, I think the inclusion or exclusion of diagonals $J_{ii}$ does not change the result as it only leads to a $\\mathcal{O}(1/N)$ correction to the correlation functions. Introducing symmetry or correlation between off-diagonal elements would change the result as it introduces response functions. \n2. While the $t_0 \\to -\\infty$ limit is often taken *after* the two-time DMFT is derived, this is not strictly necessary. A non-stationary version of the theory can be solved for time x time matrices."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1999/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665611004,
                "cdate": 1700665611004,
                "tmdate": 1700665611004,
                "mdate": 1700665611004,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BcnFZUKblM",
            "forum": "3LLkES6nNs",
            "replyto": "3LLkES6nNs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1999/Reviewer_igTL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1999/Reviewer_igTL"
            ],
            "content": {
                "summary": {
                    "value": "The paper analyzes infinite-depth residual models (NeuralODEs) and shows that as width converges to infinity, they converge to a Gaussian Process."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The topic of understanding and analyzing infinite-depth and infinite-width models is interesting and timely. The authors analyze NeuralODE, an infinite-depth-limit of a ResNet, and show its relationship, in the infinite-width limit, to a Gaussian Process, with a  difference in the resulting process depending on whether weight matrices are shared across layers or not."
                },
                "weaknesses": {
                    "value": "The manuscript is an incremental addition to a recent analysis (Gao et al., NeurIPS\u201923, arxiv:2310.10767) of a similar infinite-depth model DEQ (Deep Equilibrium Model), following similar investigation outline with differences due to the presence of residual connections (e.g., $\\sum_{i=1}^l z^i$ for NeuralODE in Thm. 3.2 eq (8) vs $z^l$ for DEQ in ) and the possibility of differing weights. The conclusions of the investigation do not provide sufficiently new insights about infinite-depth models."
                },
                "questions": {
                    "value": "See Weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1999/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698786056832,
            "cdate": 1698786056832,
            "tmdate": 1699636131134,
            "mdate": 1699636131134,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fCaTCK7ZFd",
                "forum": "3LLkES6nNs",
                "replyto": "BcnFZUKblM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1999/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1999/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate your evaluation of our paper and would like to address your concerns, particularly regarding the clarification of differences and the contribution of our work. We want to emphasize that our work is based on a completely different setting, and previous techniques cannot be directly applied due to the continuous-time nature of neural ODE. We believe characterizing our contribution as incremental solely due to the use of some prior techniques is unfair. As researchers, we are all standing on the shoulders of giants. In the following, we provide a detailed comparison with the previous work [1].\n\n* The Neural ODE and DEQs are fundamentally different. \nNeural ODE, designed as a continuous model for dynamic system learning, diverges notably from DEQs, which leverage equilibrium equations to generate later feature vectors. Importantly, DEQs require tuning the covariance hyperparameter to ensure a fixed point's existence, a need that Neural ODE doesn't share.\n\n* Neural ODE lacks a fixed point and doesn't permit tuning covariance hyperparameters for convergence and convergence rate. Instead, it relies on Euler's method, fixing the scaling to $T/L$. This becomes a noteworthy constraint when considering the two limits\u2014depth and width. In general, the ratio of convergence rates between width and depth impacts the behavior of large-depth neural networks. For instance, research [2-4] reveals that fully connected and ResNet models converge to a log-Gaussian or heavy-tail distribution instead of a Gaussian distribution when the depth's convergence speed dominates the width.\n\n* The methods in the work [1] are based on the fact that the fixed point provides a uniform bound on the Gaussian kernel matrix. However, for neural ODEs, the Gaussian kernel is a function of time, and no such bounds can be obtained. This brings additional difficulty to the theory of neural ODEs.\n\n* In Neural ODEs, the interaction between different times makes the corresponding Gaussian process quite different for the weight-shared and weight-unshared cases. While for DEQs, the Gaussian covariance matrices are the same for both cases. This also poses additional difficulty. Thus, we have to provide a fine-grained analysis of the dynamics of the NNGP kernel $\\Sigma^{\\ell}$ and derive their explicit form in Proposition 4.6, enabling us to analyze the strict positive definiteness of the limiting NNGP kernel $\\Sigma^{*}$. \n\nWe believe that our work provides new **insights** into infinite-depth neural networks, specifically in the context of skip connections with ResNet-type architectures and neural ODEs. \n\n* Unlike DEQs, which lack skip connections, our investigation, stated in Theorem 3.2 and Proposition 3.7, reveals that infinitely deep ResNets converge to a distinctive Gaussian process with a unique NNGP kernel, depending on whether weights are shared. This phenomenon, arising from skip connections, distinguishes ResNets or Neural ODEs from DEQs. Furthermore, our results, outlined in Proposition 3.7, demonstrate that with scaling $\\mathcal{O}(1/L)$, infinite-depth ResNets with time-varying or independent weights converge towards a shallow two-layer neural network as the width approaches infinity. This insight contributes to the active research area in scaling law and normalization, suggesting researchers to further explore and refine scaling and normalization strategies.\n\n* Our method is closely related to a more recent work [2], which studies the limiting behavior of infinite-depth ResNet with time-varying or independent weights. In their work, they employ results from stochastic differential equations (SDE) to demonstrate that the two limits, depth and width, can commute. In contrast, our methodology adopts a distinct proof strategy, utilizing insights from random matrix theory. This approach not only verifies the commutability of the two limits but also establishes the NNGP correspondence for Neural ODEs, i.e., infinite-depth ResNets with shared weights.\n\n* Moreover, we establish the strict positive definiteness of the NNGP kernel for Neural ODE. This result, not provided by [2], holds significance, as previous studies [5-8] have shown that a strictly positive definite NNGP kernel is crucial for ensuring global convergence and good generalization performance in gradient-based methods.\n\nGiven the continuous nature of neural ODEs, we find it imperative to provide the explicit form of the NNGP kernel and its positive definiteness\u2014a step not addressed in [1]. The explicit form of the NNGP kernel is provided in Eq. (17). Considering the influence of skip connections, we introduce a dynamic programming algorithm for computing the corresponding NNGP kernel. Notably, this algorithm, which sets our contribution apart, was neither introduced nor considered necessary in [1], further enhancing our understanding of infinite-depth neural networks."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1999/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700212605124,
                "cdate": 1700212605124,
                "tmdate": 1700212605124,
                "mdate": 1700212605124,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]