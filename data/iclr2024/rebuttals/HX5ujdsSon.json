[
    {
        "title": "In-Context Learning through the Bayesian Prism"
    },
    {
        "review": {
            "id": "4KssggeauI",
            "forum": "HX5ujdsSon",
            "replyto": "HX5ujdsSon",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8985/Reviewer_Rt1y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8985/Reviewer_Rt1y"
            ],
            "content": {
                "summary": {
                    "value": "This paper empirically showed the relationship between trained transformers and Bayesian optimal estimators. They showed that in Gaussian Mixture Models(GMM) with equal rates and two components, the prediction of trained transformers follows the PME over the two components, which is kind of different to the PME over prior distribution being the distribution of either. They also showed that the prediction follows the PME of a strong baseline (when the PME is intractible) for linear regression problems like dense linear regression, sparse linear regression, signed vector linear regression or low-rank linear regression. \n\nThen they investigated the simplicity bias of ICL of transformers. They showed with a single task distribution, the trained TF (transformers) do not give any bias on the frequency of training data, but with multiple sources of tasks  distribution, they will tend to give more weight to the low-frequency --- which is natural, since the low-frequency data appears in more tasks than high-frequency ones. They then show the in-distribution and OOD generalization capacity of IC-trained TFs on linear regression on some polynomial features, and showed that they mimics the OLS on the subset of features that appears in the test prompts. \n\nThey also showed some interesting phenomenon which are novel and  worth investigating in the future. For example, they showed the deviation from the IC-trained TFs and the Bayesian optimal estimators, which well supplements the experiments of Garg;s paper and Akyureck's paper who both showed that TFs mimic the PME. Additionally, they showed a very interesting 'forgetting' phenomenon.\n\n I will vote for acceptance for this paper. Based on the authors' reply, I raised my score to eight."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. They have sufficient experiments and the results show some very interesting phenomenon. In some linear and non-linear tasks, the TFs are competitive with the PME or some strong baselines, which showed that TFs are very efficient to learn some algorithms on different linear/non-linear regression tasks. \n\n2. I totally agree with the implication the authors proposed in page 5 that TFs do not need to first recognize the underlying tasks and then solve it  in order to approximate the Bayesian optimal solutions. The authors frankly show that in some cases, the prediction of TFs mimics the PME while in some case the prediction can deviate from that.\n\n3. For  the simplicity bias and the generalization parts, the authors gave many baseline for comparasion and showed that TFs could potentially do something very non-trivial. The experimental evidence is sufficient to convince me that the generalization capacity for OOD tasks increases when the task diversity improves."
                },
                "weaknesses": {
                    "value": "See 'Questions'."
                },
                "questions": {
                    "value": "Here, I have some questions, and as well as some personal suggestions. I will appreciate it if you can address my confusion.\n\n1. You have proposed a setting called HMICL, which you claim is different from the MICL setting proposed in [1]. I am wondering what is the difference between two settings (more formally, from the definitions in [2])? In HMICL, the task is sampled from a hierarchical distribution, which can still be viewed as 'a specific distribution', right? So to my understanding, you did not propose a 'new setting' but instead, you are considering the exact MICL setting where the task distribution is hierarchical, right? I am also wondering why you think this hierarchical structure of task distributions matters in reality? Is there any evidence showing that the linguistic data or some other real-world data follows this hierarchical distribution?\n\n2. In PME paragraph in page 3, you claim that 'the predictions of the model can be computed using the posterior mean estimator (PME) from Bayesian statistics.' I think this is not that rigorous, since this happens only when the function class you are considering (here, it's the TF class) are expressive enough so that the PME is included in this function class, right? (Namely, this only happens in the realizable case). When the model class is not large enough, the TFs can not express the PME and hence, can only 'approximate' it.\n\n3. In GMM experiments, it's good to see that TFs follows the PME over the prior of two mixed components. Two questions I have: (1): In the figure, you only show k <= 10, which is a under-determined case for a linear model, since the dimension you use is d=10. In the over-determined case, what is the PME (is it OLS) and how does the TFs behave?  (2). Can you also plot the curve for OLS for all context length? I am wondering what the difference between a TFs and OLS here. \n\n4. For the experiments on multiple linear models (DR, SR, SVR, Skew-DR,etc), even though the PME is not analytically tractible, it is still possible to numerically compute the PME. Have you tried numerical computation for these PME and compare the TF to them? I think this will be a strong evidence to say whether the TFs are really mimicking the Bayesian optimal estimators. Although this could be hard in high-dimension case (I am not an expert in Bayesian but I guess so), it should be doable in a d=5 or d=10 case?\n\nAnother question is, how do you determine the regularization coefficient of LASSO? Does this require more data (if so, that mean you are actually using a longer context to do LASSO than to do ICL using TFs.)\n\n5. For the experiments on Frourier series and second-degree polynomials, an important question is that, how did you input the data? Did you simply encode original x_i s into token matrices, or you use \\phi(x_i) (where \\phi is a Frourier basis or a second-degree polynomial) to serve as the input? \n\n6. I kind of feel that there are more recent works about the relationship between GD, ICL, OLS and distribution shift [2,3,4]. I am wondering whether their results can somehow explain some of your experiments or to some extent are related to your results?\n\n[1]. What Can Transformers Learn In-Context? A Case Study of Simple Function Classes?\n[2]. Trained Transformers Learn Linear Models In-Context.\n[3]. A Closer Look at In-Context Learning under Distribution Shifts\n[4]. Transformers learn to implement preconditioned gradient descent for in-context learning"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8985/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8985/Reviewer_Rt1y",
                        "ICLR.cc/2024/Conference/Submission8985/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8985/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697601871091,
            "cdate": 1697601871091,
            "tmdate": 1700245772522,
            "mdate": 1700245772522,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "mSlWhLkD2F",
            "forum": "HX5ujdsSon",
            "replyto": "HX5ujdsSon",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8985/Reviewer_C7Ly"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8985/Reviewer_C7Ly"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on the phenomenon of in-context learning (ICL) in large language models (LLMs) and aims to understand the underlying inductive biases that enable successful generalization to new functions. The authors propose a hierarchical meta-ICL setup that encompasses multiple task families and investigate the empirical performance of LLMs in this expanded setting. They find evidence that high-capacity transformers mimic the behavior of Bayesian predictors in cases where Bayesian inference is tractable. This Bayesian perspective sheds light on the inductive biases of ICL and how transformers perform tasks when trained on multiple tasks. Additionally, the paper explores situations where transformers deviate from the Bayesian predictor, leading to new insights and hypotheses."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper addresses a significant problem by conducting extensive experiments that aim to provide an explanation for the in-context learning capabilities of large language models (LLMs) from a Bayesian inference perspective. This research direction holds promise for advancing our understanding of LLMs.\n2. The paper is well-organized and effectively communicates its ideas. The authors present a natural and logically motivated experimental setting, which not only facilitates comprehension but also encourages further exploration and experimentation in the field."
                },
                "weaknesses": {
                    "value": "1. The settings examined in this study predominantly rely on synthetic data, which creates a notable disparity between the experiments and real-world data. This limits the generalizability and applicability of the findings to realistic scenarios.\n2. Although the experiments are comprehensive and thought-provoking, there are certain definitions that would benefit from further clarification. Specifically, many explanations in the current version are contingent upon the capacity constraints of transformers. To enhance the illustration of the impact of transformer capacity, it is suggested that the authors include plots depicting the size of transformers in relation to the observed deviation phenomenon. This would provide a clearer understanding of the relationship between capacity and performance.\n3. As an empirical investigation paper, I would suggest that the authors consider providing the code to facilitate further investigation and replication of their findings."
                },
                "questions": {
                    "value": "It is well-known that the order of demonstrations has a significant impact on the final results of in-context learning. However, it appears that the current experimental settings do not account for this influential factor. Please correct me if I have misunderstood any aspects. Moreover, I am curious if increasing the model size of transformers can alleviate the forgetting phenomenon."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8985/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8985/Reviewer_C7Ly",
                        "ICLR.cc/2024/Conference/Submission8985/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8985/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698637622025,
            "cdate": 1698637622025,
            "tmdate": 1700295158309,
            "mdate": 1700295158309,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "dVrD6eJKl3",
            "forum": "HX5ujdsSon",
            "replyto": "HX5ujdsSon",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8985/Reviewer_P1i1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8985/Reviewer_P1i1"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers in-context learning for large language models. This paper extends the previous work \"What can transformers learn in-context? A case study of simple function classes.\" by considering multiple function families, out-of-distribution detection, and Bayesian prediction tasks. This paper conducts extensive experiments and large-scale analysis in the experiment section."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The author conducts extensive experiments on prompting engineering for large language models over multiple tasks."
                },
                "weaknesses": {
                    "value": "- The paper is not well organized which makes it very hard to read through. \n    - The Figures are very hard to understand, making it challenging to summarize the conclusion from the figures.\n    - It's not a good idea to keep stating \"refer to Appendix\" for the main methodology part of the paper. \n    - It is also not a good idea to keep stating that some steps in the proposed method are similar to a specific paper and cite that paper. At least, the authors need to give a clear reason for doing so.\n- The paper mentioned a lot of technique terms without giving any rational and proper definitions. For example, \n    - the \"Bayesian predictor\" is not properly explained in the context of \"in-context learning\". \n    - I'm not familiar with The \"deviations from Bayesian prediction\" . The author should explain that clearly.\n    - The \"Simplicity bias\" should be better replaced with \"Occam\u2019s razor\" or \"no free lunch theorem\". Because the suggested terms are used very often in the literature. \n    - If you mean \"generalization is out of distribution (OOD) detection\". Then just using OOD detection would reduce a lot of confusion.\n- The correctness of Equation 2 is questionable. What is the exact definition of \"$df$\", the differentiation with respect to a function $f$?\n- As the main component of this work, the paper lacks the reason for extending ICL to hierarchical ICL. Why do you want to sample from a mixture of the function family?"
                },
                "questions": {
                    "value": "- Considering the paper's main topic is about prompting engineering and large language models, the author might consider trying for a compatible conference in NLP."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8985/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8985/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8985/Reviewer_P1i1"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8985/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698734788123,
            "cdate": 1698734788123,
            "tmdate": 1700908802706,
            "mdate": 1700908802706,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]