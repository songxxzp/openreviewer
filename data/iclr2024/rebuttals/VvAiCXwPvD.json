[
    {
        "title": "Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations"
    },
    {
        "review": {
            "id": "iK4tPqcZXs",
            "forum": "VvAiCXwPvD",
            "replyto": "VvAiCXwPvD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2760/Reviewer_Sh5n"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2760/Reviewer_Sh5n"
            ],
            "content": {
                "summary": {
                    "value": "The authors investigate the properties of explanations provided by LLMs. Specifically, they investigate a) Whether explanations are `precise`, i.e. can simulate how the model would behave under counterfactual questions b) are `general`, whether they help with the simulatability of queries that are different than the original query. The authors provide a methodology to evaluate explanations for these two criteria and evaluate two LLMs (GPT-3, 4) in two tasks (StrategyQA, SHP)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "I find that the paper has an interesting object to evaluate (explanations provided by LLMs) and a well-constructed methodology to do so. It appears to be a good addition to the methodologies being developed to evaluate LLM explanations, such as ones provided in the paper (Turpin et al., Creswell et al.). \n\nMore specifically,\n\n1. The idea that the quality of an expectation could be measured by the simulatability of counterfactuals is a useful one. The ability of explanations to help humans construct mental models is one that is discussed in the existing literature \u2013 however, instantiation in the context of language models appears to be an interesting idea.\n\n2. Formalizing simulatability using logical entailment is again a useful idea. I\u2019m not sure if this exists in the literature \u2013 but could be a good way to automate explanation evaluations in general.\n\n3.  I appreciate the sanity checks in Section 5.1 to justify several design choices made by the authors, before moving on to more complex experiments. This clarifies several questions in the mind of a potential reader."
                },
                "weaknesses": {
                    "value": "1. There is a human study in the paper that does not mention whether the study has an IRB approval. Citing ICLR Code of Ethics `Where human subjects are involved in the research process (e.g., in direct experiments, or as annotators), the need for ethical approvals from an appropriate ethical review board should be assessed and reported. ` **How to address: I recommend the authors to apply for an IRB approval from their respective institution to resolve this concern. Ideally this should have done prior to data collection, but I am leaving this to the judgment of Ethics Reviewers / ACs.**\n\n2. One of the conceptual contributions authors make is the proposed criteria to evaluate explanations. However, I\u2019m a bit concerned with both desiderata (generality and precision) provided by the authors:\n- 2.1 - The terminological choice of precision looks very much like faithfulness (See Q1), which created significant confusion for me. The definition of precision in Section 3.2 is referred to as faithfulness in many other contexts (as in the cited related works). Unless I\u2019m missing something here, I\u2019m not sure why one needs to invent a new name for it.\n- 2.2 - I find the claim `an ideal explanation .. should be generalizable, \u2026 it should also reveal how well the model reasons on unseen outputs` unjustified. Why do we really want generalizable claims? Why is it a non-ideal thing to only explain the answer to the question one is responding to? As long as the explanation is faithful, I cannot say being more general or more specific is preferable in this context. **How to address: I\u2019d be curious to hear authors\u2019 justifications for a) Why Precision is not simply Faithfulness? b) How do we claim that we want generality in these explanations?**\n\n3. I am also unclear about what conclusions we can reliably draw from the experiments, or how these may benefit the users/consumers of the explanations. \n- One general conclusion appears to be around a more capable model (GPT-4) providing `better`(under the criteria in the paper) explanations than a less capable model (GPT-3). \n- However, beyond the relative inferences, I cannot understand whether these numbers are good or not in an absolute sense, thus whether explanations are good or not (Q3). In general, a lack of comparison to interpretable numbers makes the results hard to interpret.  **How to address: One way would be to provide baseline explanations. For instance, one can ask the same set of questions to a set of human users, let the human users provide explanations, and compare those explanations with the ones provided by the models.**\n\nMinor: \n\n1. Imho, the title is not precise. \u201cDo models explain themselves\u201d is pretty generic with the use of the word \u201cmodel\u201d, but really the authors are focusing on a specific model class, which is autoregressive language models. I\u2019d personally err on the side of precision."
                },
                "questions": {
                    "value": "1. I\u2019m slightly confused about the proposed terminology here. The authors argue that precision is that `they should lead to mental models that are consistent with the model\u2019s behavior.`; this definition sounds quite a bit like faithfulness, in my opinion (also in the related works authors refer to faithfulness as `It is different from faithfulness, which measures whether an explanation is consistent with the model\u2019s own decision process`). \n\nIn that, this argues explanation should be faithful to the model\u2019s behavior. Personally, I find precision to have a stronger connotation around explaining what could be explained, and nothing beyond it. I\u2019d love to hear the authors\u2019 opinions about why this is not faithfulness and is precision. \n\n2. The authors empirically argue that generality and precision do not seem correlated -- however I do not necessarily agree with this, and I\u2019m unclear about whether an explanation has to be general as long as it is precise. To be more concrete, one could make an extremely local explanation (i.e. hard to find BLT because markets do not sell BLT) that is extremely precise. For instance, using the formalization in the paper, we could have $|C|$ large but $|C^*|$ small, potentially $0$ or $1$, e.g. if the model uses the answer to explain the answer. Are these two desiderata independent? If they are not, I believe the paper would also benefit from a discussion around this, and also further discussion in Section 5.2.2 where the authors suggest independence. \n\n3. I\u2019m not sure how one can claim that 80% is not precise enough (i.e. the claim `explanations have low precision (80% for binary classification)` in the intro); the number seems nontrivial. Why do we expect a larger number? Do we have statistics about how precise human explanations are? Can this number be even larger than for humans\u2019? \n\n4. What is being reported in Table 5? Is it the correlation coefficient? Is it Spearman or Pearson? What are the p-values? Are the numbers statistically significant? \n\nMinor\n1. The inline citations are confusing to me, perhaps it would clarify to replace \\cite calls with \\citep calls to clarify reading. \n2. Page 1, \u201cMuslin countries\u201d -> \u201cMuslim countries\u201d\n3. Figure 2 says \u201crobot\u2019s answer\u201d; I\u2019m assuming this is the LLMs\u2019 answer? Otherwise, what\u2019s the robot?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The authors perform human simulations via Mechanical Turk, but I do not see whether the authors have IRB approvals or exemptions for the human subject experiment.\n\n**Rebuttal:** Answered by the authors, they suggest they do have an IRB."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2760/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2760/Reviewer_Sh5n",
                        "ICLR.cc/2024/Conference/Submission2760/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2760/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697928841832,
            "cdate": 1697928841832,
            "tmdate": 1700428220958,
            "mdate": 1700428220958,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kOzHIq2RYU",
                "forum": "VvAiCXwPvD",
                "replyto": "iK4tPqcZXs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2760/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2760/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to your review (part 1/2)"
                    },
                    "comment": {
                        "value": "Thanks so much for your questions!\n\n&nbsp;\n\nQ: Did you apply for IRB?\n\nA: Yes we did! Our IRB was approved on 05/09/2023 prior to data collection. We did not submit the certificate due to anonymity constraints, but will include it if the paper is accepted.\n\n&nbsp;\n\nQ: How to understand the precision numbers \u201cin an absolute sense\u201d? Are human-written explanations more precise?\n\nA: Following your advice, we __ran a new evaluation__ to measure the precision of human-written explanations. Just like how we scored LLM explanations, we asked a human annotator to write explanations for the same set of questions, used GPT-4 to generate counterfactual questions, asked the human annotator to answer the counterfactual questions, and scored how often the human annotator\u2019s answer to the counterfactuals is consistent with the simulator\u2019s answer. __Human-written explanations achieved a simulation precision of 95.0, much higher than the precision of LLM-generated explanations (80.6) with p-value < 0.001__. We\u2019ve added this result to the pdf (Section 5.2.1 last paragraph).\n\n&nbsp;\n\nQ: Why is 80% precision \u201cnot precise enough\u201d?\n\nA: Explanations are especially useful in high-stakes domains, where 80% precision is insufficient. Plus, human-written explanations have a precision of 95.0, indicating there could be a large room for improvement for LLM generations.\n\n&nbsp;\n\nQ: Why do we want generalizable explanations? Why is an explanation \u201cnon-ideal\u201d if it \u201conly explains the answer to the question one is responding to\u201d?\n\nA: When humans form mental models of how a model behaves, generalizability is important so that they can correctly infer the model\u2019s output on examples where they do not see the model\u2019s explanation. For example, a general explanation \u201cbirds can fly\u201d enables humans to infer the model\u2019s output on all questions asking whether each kind of bird can fly (e.g., eagles, sparrows, etc.). That way humans do not have to read the model\u2019s explanation for each individual bird one by one, which is tedious and inefficient.\n\n&nbsp;\n\nQ: Intuitively \u201cone could make an extremely local explanation that is extremely precise\u201d. What is reported in Table 5? Is it Spearman or Pearson? Are the numbers statistically significant?\n\nA: We agree with your intuition that an extremely precise explanation can lead to low generality, which motivates our experiments to measure the correlation between generality and precision to see if there is any trade-off (Section 5.2.2 Paragraph 3). Table 5 shows near-zero Spearman correlations between precision and generality, and we observe similar near-zero correlations for Pearson (StrategyQA - BLEU: 0.023, Cosine: 0.003, Jaccard: -0.001; SHP - BLEU: 0.048, Cosine: 0.008, Jaccard: 0.013). We observe statistically significant correlations (p-value < 0.05) only on <3% of the examples where we calculated the correlations. Given these numbers, we conclude that there is no significant trade-off between precision and generality."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700152398917,
                "cdate": 1700152398917,
                "tmdate": 1700152398917,
                "mdate": 1700152398917,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1xsCIqj9Uv",
                "forum": "VvAiCXwPvD",
                "replyto": "r5wdme9nHg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2760/Reviewer_Sh5n"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2760/Reviewer_Sh5n"
                ],
                "content": {
                    "title": {
                        "value": "Response to the rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your rebuttal! \n\n- **IRB**\n\nThanks for the clarification! Do you also have the IRB for the latest human evaluation (one where you ask humans to generate explanations to compare precision)? What is the data collection procedure? \n\n- **Human-written explanations**\n\nThis will be good to have! I have a couple more questions to understand and interpret the new experiment better:\n\nCould you provide more details about how you recruited the human annotator? Did you recruit through MTurk, if yes why only 1 annotator? Could you also provide a bit more quantitative details about the experiment? Specifically: What is the number of explanations/questions you asked from the human annotator to write?\n\nPersonally, I do not agree with the statement *Explanations are especially useful in high-stakes domains, where 80% precision is insufficient*. In particular, i) yes it seems to have a face value, but ii) in this case neither the authors are evaluating high-stakes settings, nor 80% is clearly good enough or not for such settings. However, I do agree with a statement that would compare against the human baseline and claim there is a gap to close, given that we agree the number 95% is computed reasonably. I will revise my thoughts around this once I have more details about the experiment.\n\n- **Table 5, Precision and Generality**\n\nI appreciate the clarification. It would be good to indicate in the caption of the table what the numbers are. If I'm not missing it, in the paper you do not mention what kind of correlation coefficients you are computing (although you did explain it to me in your rebuttal text). \n\nOne point I'm still not convinced about is the authors' argument that \"there is no significant tradeoff\". e.g. in the specific example regime I gave where we have $|C|$ large but $|C^*|$ small, there is clearly a tradeoff (or isn't there?). While a marginal analysis between the two properties may not reveal a relation, there may be a correlation in certain regimes if we look at conditional distributions, which disappear when averaged out.\n\n- **Faithfulness vs Simulation**\n\nThank you for the clarification here, this was my own personal confusion about the existing terminology.\n\n**Overall**, thank you again for your clarifications. I will revisit my score once the remaining points are clarified."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700261632216,
                "cdate": 1700261632216,
                "tmdate": 1700261632216,
                "mdate": 1700261632216,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ITLFe2LM7i",
                "forum": "VvAiCXwPvD",
                "replyto": "OCTmJDAw6H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2760/Reviewer_Sh5n"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2760/Reviewer_Sh5n"
                ],
                "content": {
                    "title": {
                        "value": "Thank you"
                    },
                    "comment": {
                        "value": "Thank you for your clarifications, I appreciate the effort you put into your rebuttal, the new experiments, and presentation improvements. I will be adjusting my score accordingly."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700428183192,
                "cdate": 1700428183192,
                "tmdate": 1700428183192,
                "mdate": 1700428183192,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8dHYQQukdg",
            "forum": "VvAiCXwPvD",
            "replyto": "VvAiCXwPvD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2760/Reviewer_wXPv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2760/Reviewer_wXPv"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the concept of counterfactual simulatability to evaluate natural language explanations generated by large language models (LLMs). The authors propose two metrics, simulation generality and simulation precision, to measure the ability of an explanation to enable humans to infer the model's outputs on diverse counterfactuals. They evaluate state-of-the-art LLMs on multi-hop factual reasoning and reward modeling tasks and find that the explanations have low precision and do not correlate with plausibility. The authors suggest that naively optimizing for human approval may not be sufficient to improve the quality of explanations. The paper emphasizes the importance of building explanations that help humans build accurate mental models of model behavior."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) This work introduces novel metrics to evaluate the quality of generated explanations by LLMs, with a focus on whether those explanations help humans build mental models of those LLMs.\n\n2) The paper is well written, with clear reasoning and explanations. The methodology and results are clear and easy to follow. \n\n3) The paper addresses an important issue on explainable generations. The proposed evaluation framework provides a valuable tool for assessing the quality of explanations. \n\n4) The experiments are conducted on two different tasks, multi-hop factual reasoning and reward modeling, and the results demonstrate the limitations of existing explanation methods and the need for improvement.\n\n5) The paper's findings and insights have the potential to shape future research and development in the field."
                },
                "weaknesses": {
                    "value": "1) The paper does not compare the proposed metrics with other existing evaluation metrics for explanations. Without such comparisons, it is challenging to determine how the proposed metrics perform in relation to other approaches.\n\n2) The evaluation of counterfactual simulatability is limited to classification tasks, and there is a need to extend it to more complex generation tasks.\n\n3) The human simulation task is complex and subjective, leading to only moderate agreement among human annotators, raising concerns about the reliability of human evaluation. That's extended to LLMs as well. \n\n4) The study assumes that language models have some form of \"knowledge\", which is not actually the case. Language models don't \"know\" or \"understand\" information in the way humans do - they generate responses based on patterns they've learned from training data.\n\n5) Some citations do not follow the formatting instructions (When the authors or the publication are included in the sentence, the citation should not be in parenthesis using \\citet{}... Otherwise, the citation should be in parenthesis using \\citep{})"
                },
                "questions": {
                    "value": "1) The discussion of LLM simulators as proxies for human simulators is interesting, but it would be beneficial to provide more insights into the limitations of using LLMs in this role. What are the potential shortcomings or biases that LLM simulators may introduce? How well do they capture the full range of human reasoning and decision-making?\n\n2) Could you elaborate more on the \"Forced\" strategy you used for the sanity checks? \n\n3) Could the authors elaborate on how the counterfactual questions were generated? Did they follow specific patterns or were they entirely randomly created?\n\n4) Your evaluation method assumes that the knowledge contained in the explanation is the only information used by the model to make its decisions. How would your method account for scenarios where the model uses additional information not contained in the explanation to make its decision?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2760/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2760/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2760/Reviewer_wXPv"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2760/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698681311924,
            "cdate": 1698681311924,
            "tmdate": 1699636218694,
            "mdate": 1699636218694,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "raTzlu9tBT",
                "forum": "VvAiCXwPvD",
                "replyto": "8dHYQQukdg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2760/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2760/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to your review (part 1/2)"
                    },
                    "comment": {
                        "value": "Thanks so much for your questions! \n\n&nbsp;\n\nQ: Did you \u201ccompare the proposed metrics with existing evaluation metrics for explanations\u201d?\n\nA: Yes, we did! We compared our metrics with plausibility, which is the most widely used metric for explanations [1,2,3] (Section 5.2.2 Paragraph 2). We tested the correlation between our simulation precision metric and the plausibility metric to study if our simulation precision metric is already covered by plausibility. We only observed a very weak correlation of +0.012 (Pearson) and +0.021 (Spearman) between simulation precision and plausibility, and thus concluded that the new counterfactual precision metric we proposed is orthogonal to plausibility.\n\n&nbsp;\n\nQ: The human simulation task is complex and subjective, \u201cleading to only moderate agreement among human annotators\u201d. Is the human evaluation still \u201creliable\u201d? Could you elaborate more on the \"Forced\" strategy you used for the sanity checks?\n\nA: To address the moderate agreement rate, we ran a sanity check experiment in Section 5.1 to check whether our evaluation procedure can distinguish between explanation systems of different quality under the simulation noise. We constructed a baseline system \u201cForced\u201d where we forced the model to generate a Post-Hoc explanation conditioned on the answer it assigns a lower score to, and compared its precision score to the \u201cNormal\u201d system where the model generates a Post-Hoc explanation conditioned on the answer it assigns a higher score to. \u201cNormal\u201d outperforms \u201cForced\u201d significantly by +45.2 precision points (38.2 vs. 83.4; p-value < 1e-16), which verifies that our evaluation procedure can discriminate between explanation systems of different quality despite the simulation noise.\n\nRegarding the fact that the IAA is only moderate: prior work also observed that __simulation is known to be inherently subjective__, and claimed that \u201cvariance in explanation ratings is quite high, relative to their scale\u201d when they evaluated simulatability with humans [4]. However, despite the noisy simulation, our work and [4] are still able to __draw reliable conclusions that are statistically significant__ even from noisy human evaluation.\n\n&nbsp;\n\nQ: How would your metric account for scenarios where the model uses additional information not contained in the explanation to make its decision?\n\nA: It depends on what information of the decision process is missing in the explanation. Explanations more specific than the model\u2019s decision process (e.g., the model explains \u201canimals with wings can fly\u201d but uses the decision process \u201call animals can fly\u201d) are penalized with a lower generality score. Conversely, explanations more generic than the decision process (e.g., the model uses the decision process \u201canimals with wings can fly\u201d but the explanation misses the modifier \u201cwith wings\u201d) are penalized with a lower precision score.\n\n&nbsp;\n\nQ: Could the authors elaborate on how the counterfactual questions were generated?\n\nA: We use LLMs to generate counterfactuals (Figure 2). To generate counterfactuals for each explanation from the QA model, we show the generator LLM the explanation and prompt it to generate ten follow-up questions for which it can confidently guess the QA model\u2019s answer based on the shown explanation. We show the prompts we use in Appendix B. We show that our LLM-based generator outperforms the PolyJuice baseline on diversity (Section 5.1 Paragraph 4).\n\n&nbsp;\n\n[1] Herman, Bernease. \"The promise and peril of human evaluation for model interpretability.\" arXiv preprint arXiv:1711.07414 (2017).\n\n[2] Lage, Isaac, et al. \"An evaluation of the human-interpretability of explanation.\" arXiv preprint arXiv:1902.00006 (2019).\n\n[3] Jacovi, Alon and Yoav Goldberg. \u201cTowards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness?\u201d Annual Meeting of the Association for Computational Linguistics (2020).\n\n[4] Hase, Peter, and Mohit Bansal. \"Evaluating explainable AI: Which algorithmic explanations help users predict model behavior?.\" arXiv preprint arXiv:2005.01831 (2020)."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700152139855,
                "cdate": 1700152139855,
                "tmdate": 1700152139855,
                "mdate": 1700152139855,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DLfPIIXjG0",
                "forum": "VvAiCXwPvD",
                "replyto": "8dHYQQukdg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2760/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2760/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to your review (part 2/2)"
                    },
                    "comment": {
                        "value": "Q: How can we evaluate counterfactual simulatability on \u201cmore complex generation tasks\u201d?\n\nA: That\u2019s an important but also challenging research question,  our paper discussed possible solutions in Section 6 Paragraph 1 as future work. The key difference between classification tasks and generation tasks is that multiple answers can be correct in generation tasks, so it is harder to define what it means for a human to \u201ccorrectly\u201d simulate the model\u2019s output. One possible solution we proposed in the paper is contrastive simulation [5,6,7], where a human simulator is shown the model\u2019s output mixed with fake outputs (distractors) and selects which output is from the model based on the model\u2019s explanation.\n\n&nbsp;\n\nQ: The discussion of LLM simulators as proxies for human simulators is interesting, but it would be beneficial to provide more insights into the limitations of using LLMs in this role.\n\nA: Inspired by [8], one possible future direction is to study if GPT-4 can simulate each annotator even better when given a few annotations from that annotator to learn annotator-specific pattern/bias. Using one LLM simulator for each individual and aggregating the outputs from different simulators may further improve IAA.\n\n&nbsp;\n\nQ: The study assumes that language models have some form of \"knowledge\", which is not actually the case.\n\nA: We do not assume that LLMs have any form of \u201cknowledge\u201d. Our evaluation procedure is behavioral, which only relies on examining the input-output behavior of the model.\n\n&nbsp;\n\nQ: Some citations do not follow the formatting instructions.\n\nA: Thanks for pointing this out! We have corrected the citation format in the pdf.\n\n&nbsp;\n\n[5] Jacovi, Alon et al. \u201cContrastive Explanations for Model Interpretability.\u201d EMNLP 2021.\n\n[6] Miller, Tim. \u201cContrastive Explanation: a Structural-Model Approach.\u201d The Knowledge Engineering Review, vol. 36, 2021, p. e14., doi:10.1017/S0269888921000102.\n\n[7] Kayo Yin and Graham Neubig. 2022. Interpreting Language Models with Contrastive Explanations. EMNLP 2022.\n\n[8] Aher, Gati, Rosa I. Arriaga, and Adam Tauman Kalai. \u201cUsing large language models to simulate multiple humans.\u201d arXiv preprint arXiv:2208.10264 (2022)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700152218990,
                "cdate": 1700152218990,
                "tmdate": 1700152218990,
                "mdate": 1700152218990,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LxMVFjxQbE",
                "forum": "VvAiCXwPvD",
                "replyto": "8dHYQQukdg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2760/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2760/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Questions or comments for our rebuttal/paper?"
                    },
                    "comment": {
                        "value": "As we are reaching the end of the rebuttal period, please let us know if you have any further questions or comments about our paper/rebuttal. Thanks again for reviewing our paper!"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688572366,
                "cdate": 1700688572366,
                "tmdate": 1700688572366,
                "mdate": 1700688572366,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QRrAtLQxjO",
                "forum": "VvAiCXwPvD",
                "replyto": "LxMVFjxQbE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2760/Reviewer_wXPv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2760/Reviewer_wXPv"
                ],
                "content": {
                    "title": {
                        "value": "Thanks."
                    },
                    "comment": {
                        "value": "Thank you for your thorough and insightful clarifications in response to the review. I appreciate the significant effort you've invested in addressing the concerns raised and enhancing the presentation of your findings."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700697191695,
                "cdate": 1700697191695,
                "tmdate": 1700697191695,
                "mdate": 1700697191695,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qEaN6t0suO",
            "forum": "VvAiCXwPvD",
            "replyto": "VvAiCXwPvD",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2760/Reviewer_yudN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2760/Reviewer_yudN"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes counterfactual simulatability, which pertains to whether a human can predict a model's response to several questions when given its previous answer and explanation of a similar question. Due to the disadvantages of human evaluation, the authors leverage GPT-4 and GPT-3 for counterfactual generation and GPT-4 for imitating a human simulator. By leveraging this pipeline, they claim that they discover the defects of LLMs on simulatiprecision and the weak correlation of simulation precision and plausibility, which drive them to draw the conclusion that RLHF may not improve counterfactual simulatability."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The topic of the paper, which can be potentially applied to solving the hallucination of LLM, is interesting and important to the current NLP field.\n2. The analysis of the experiments is abundant.\n3. The paper is clearly written and easy to follow."
                },
                "weaknesses": {
                    "value": "1. Whether GPT-4 can approximate human simulators remains a doubt. As demonstrated in Table 2, the authors state that the numbers reported under the columns of  H-GPT3 and H-GPT4 are calculated by the exact IAA with humans devided by the average IAA between humans. Therefore, the number is a percentage and the true number is much lower, which means that GPT-4 has similar agreement with humans as humans do with each other is not convincing.\n2. Within the same context, if GPT-4 exhibits a level of agreement with humans akin to that among humans themselves, it does not inherently establish its competence as an accurate representation of human cognition. An effective approximation should inherently exhibit a strong alignment with human behavioral patterns. In simpler terms, the findings merely attest to GPT-4's ability to replicate the same range of diversity observed among humans in their interactions with one another.\n3. The paper's underlying motivation is not explicitly articulated. The rationale for assessing such a metric remains obscure, as it exhibits characteristics reminiscent of the phenomenon of \"hallucination.\" Consequently, the apparent issue appears to revolve around the occurrence of hallucinatory responses to similar questions, which fails to underscore its novelty or distinctive contribution to the field.\n4. The conclusion of Table 3 is not correct. As shown in the table, GPT-mix demonstrates a higher similarity score compared with other models, therefore the conclusion should be GPT-mix generates more relevant counterfactuals but not more diverse ones.\n5. Typos: The 2nd paragraph of Section 3.1: \"Expectation\"  doesn't match with the formula listed below."
                },
                "questions": {
                    "value": "Please refer to weaknesses section for details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2760/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698833573614,
            "cdate": 1698833573614,
            "tmdate": 1699636218625,
            "mdate": 1699636218625,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XUpZ3bdQeO",
                "forum": "VvAiCXwPvD",
                "replyto": "qEaN6t0suO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2760/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2760/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to your review"
                    },
                    "comment": {
                        "value": "Thanks so much for your questions!\n\n&nbsp;\n\nQ: How is your simulation precision metric related to the phenomenon of \u201challucination\u201d?\n\nA: In the literature, \u201challucination\u201d is commonly defined as \u201cgeneration of wrong facts\u201d [1,2,3,4]. Our simulation precision metric does not measure hallucination -- it measures if the explanation __reflects the true decision process of the model__. Below is an example where the explanation is not a hallucination but not precise either. If a model answers \u201cyes\u201d to the question \u201care chickens warm-blooded?\u201d and explains that \u201call birds are warm-blooded\u201d, but answers \u201cno\u201d to the question \u201care sparrows warm-blooded?\u201d, then 1) this explanation is not a hallucination, since it is factually correct that \u201call birds are warm-blooded, but 2) this explanation is not precise, in that it does not reflect the true decision process of the model on counterfactuals.\n\n&nbsp;\n\nQ: Re: GPT-4 as a proxy of human simulators, even though \u201cGPT-4 exhibits a level of agreement with humans akin to that among humans themselves\u201d, did you check if GPT-4 \u201cexhibit[s] a strong alignment with human behavioral patterns\u201d?\n\nA: Following your advice, we conducted __an additional evaluation__ to test if GPT-4 has similar behavioral patterns as human simulators. Specifically, we studied if GPT-4 has higher agreement with humans on counterfactuals where human-human agreement is high. We measured the correlation between human-GPT-4 agreement and human-human agreement across 1532 counterfactual questions, and observed a strong correlation of Pearson coefficient +0.398 (p-value = 1e-58). This result shows that the GPT-4 simulator has some similar behavioral patterns as human simulators. We\u2019ve added this result to the pdf (Section 5.1 Paragraph 3).\n\nTo strike a balance between scientific rigor and economic feasibility, our work __followed the practice from the prior literature__ to use LLMs for automatic evaluation [5,6,7], supported by the fact that LLMs do exhibit similar behavioral patterns as humans in many situations [8,9,10]; in fact, __we consider it necessary to use automatic evaluation to maintain rigor as well__, since using GPT-4 as the simulator for part of the experiments is necessary in order to be able to do all the robustness checks (e.g., making sure that our metric is sensible and can distinguish between good and poor explanation systems in Section 5.1 Paragraph 2) given a fixed budget. \n\nFinally, our paper also __used real actual human subjects__ for all experiments on StrategyQA (Section 5.2) and showed that all conclusions are equally valid.\n\n&nbsp;\n\nQ: Table 3 shows that GPT-mix \u201cdemonstrates a higher similarity score compared with other models\u201d, so the conclusion should be that GPT-mix does not generate \u201cmore diverse ones\u201d.\n\nA: We\u2019d like to clarify some potential misunderstandings. The numbers in Table 3 are diversity scores, not similarity scores. We define diversity in Section 3.1 as one minus the expected similarity between any two simulatable counterfactuals, so Table 3 shows that GPT-mix generates more diverse ones.\n\n&nbsp;\n\nQ: Typo: The \u201cexpectation\u201d in 2nd paragraph of Section 3.1 doesn\u2019t match with the formula listed below.\n\nA: Good catch! The expectation formula should state that x\u2019 \\neq x\u2019\u2019. We have updated this in the pdf.\n\n&nbsp;\n\n[1] Dhuliawala, Shehzaad, et al. \u201cChain-of-verification reduces hallucination in large language models.\u201d arXiv preprint arXiv:2309.11495 (2023).\n\n[2] Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval Augmentation Reduces Hallucination in Conversation. In Findings of the Association for Computational Linguistics: EMNLP 2021. ACL.\n\n[3] Zhang, Muru, et al. \u201cHow language model hallucinations can snowball.\u201d arXiv preprint arXiv:2305.13534 (2023).\n\n[4] Manakul, Potsawee, Adian Liusie, and Mark JF Gales. \u201cSelfcheckgpt: Zero-resource black-box hallucination detection for generative large language models.\u201d arXiv preprint arXiv:2303.08896 (2023).\n\n[5] Liu, Yixin, et al. \u201cOn Learning to Summarize with Large Language Models as References.\u201d arXiv preprint arXiv:2305.14239 (2023).\n\n[6] Fu, Jinlan, et al. \u201cGptscore: Evaluate as you desire.\u201d arXiv preprint arXiv:2302.04166 (2023).\n\n[7] Liu, Yang, et al. \u201cGpteval: Nlg evaluation using gpt-4 with better human alignment.\u201d arXiv preprint arXiv:2303.16634 (2023).\n\n[8] Huijzer, Rik, and Yannick Hill. \u201cLarge Language Models Show Human Behavior.\u201d PsyArXiv, 31 Jan. 2023. Web.\n\n[9] Binz, Marcel, and Eric Schulz. \u201cTurning large language models into cognitive models.\u201d arXiv preprint arXiv:2306.03917 (2023).\n\n[10] Aher, Gati, Rosa I. Arriaga, and Adam Tauman Kalai. \u201cUsing large language models to simulate multiple humans.\u201d arXiv preprint arXiv:2208.10264 (2022)."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700151142118,
                "cdate": 1700151142118,
                "tmdate": 1700151867591,
                "mdate": 1700151867591,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hxSnPWPp20",
                "forum": "VvAiCXwPvD",
                "replyto": "qEaN6t0suO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2760/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2760/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Questions or comments for our rebuttal/paper?"
                    },
                    "comment": {
                        "value": "As we are reaching the end of the rebuttal period, please let us know if you have any further questions or comments about our paper/rebuttal. Thanks again for reviewing our paper!"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688534275,
                "cdate": 1700688534275,
                "tmdate": 1700688534275,
                "mdate": 1700688534275,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6UQ5sIh3Ua",
                "forum": "VvAiCXwPvD",
                "replyto": "hxSnPWPp20",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2760/Reviewer_yudN"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2760/Reviewer_yudN"
                ],
                "content": {
                    "title": {
                        "value": "Not all concerns are well addressed"
                    },
                    "comment": {
                        "value": "Dear Authors,\n\nThank you for the effort in providing a rebuttal. However, some of my proposed concerns are not even touched, e.g., **Weakness 3**. Moreover, the Q1 may not be interpreted correctly. The concern lies in the appropriateness of computing the simulation appropriateness metric. There is also no indication in the revision that the detailed answer to Q4 will be well-reported in the final version. Overall, I will keep my original score and tend to reject this work."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2760/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707306152,
                "cdate": 1700707306152,
                "tmdate": 1700707306152,
                "mdate": 1700707306152,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]