[
    {
        "title": "Outliers Memorized Last: Trends in Memorization of Diffusion Models Based on Training Distribution and Epoch"
    },
    {
        "review": {
            "id": "gpiLMZmhne",
            "forum": "6ZuDeSHzjj",
            "replyto": "6ZuDeSHzjj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8934/Reviewer_uuB7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8934/Reviewer_uuB7"
            ],
            "content": {
                "summary": {
                    "value": "Engaging in  discussion on memorization within generative models, but set entirely against a\n synthetic toy data backdrop. The absence of explicit definitions for terms like inliers, outliers, and memorization stands out. It is a preliminary study and calls for diving deeper into the empirical analysis."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The topic of memorization in diffusion models is interesting and requires in depth study."
                },
                "weaknesses": {
                    "value": "- The claims and definitions are unclear.\n- Experiments are carried out only on an arbitrary synthetic dataset"
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8934/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8934/Reviewer_uuB7",
                        "ICLR.cc/2024/Conference/Submission8934/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8934/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698443189331,
            "cdate": 1698443189331,
            "tmdate": 1700600062234,
            "mdate": 1700600062234,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "qGvCneh1GG",
            "forum": "6ZuDeSHzjj",
            "replyto": "6ZuDeSHzjj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8934/Reviewer_c1Ba"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8934/Reviewer_c1Ba"
            ],
            "content": {
                "summary": {
                    "value": "Diffusion models have attracted significant attention due to their ability to generate high-quality synthetic images. However, recent research discovers that diffusion models are likely to memorize training images, i.e., produce identical training images during the inference phase. In this paper, the authors aim to undermine the relation between memorization and train distribution. They observe that memorization of \u2019outliers\u2019 is less likely early in the training process until eventually matching with the rest of the dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The discussed topic -- memorization of diffusion models is important and interesting."
                },
                "weaknesses": {
                    "value": "- The content of the paper is not substantial. The current version has only four pages.\n- Lack of experiments on real data. The experiments consider only synthetic dataset, which makes the observation less convincing.\n- SOTA text-to-image diffusion models, e.g., stable diffusion, are not discussed in this paper."
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8934/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698721465227,
            "cdate": 1698721465227,
            "tmdate": 1699637125098,
            "mdate": 1699637125098,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "X9MmWlCuA5",
            "forum": "6ZuDeSHzjj",
            "replyto": "6ZuDeSHzjj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8934/Reviewer_FcWa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8934/Reviewer_FcWa"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the impact of the position of a data point within a training distribution on memorization rates across epochs in diffusion models. A simplistic 2-feature toy dataset is used to assess the memorization of both injected outliers and inliers, along with near duplicates, throughout the training process.\n\nA key discovery is the slower rate of outlier memorization during the early training stages, which, with time, aligns with the memorization rate of other data points. I think this is an interesting finding which supports a similar result in LLM literature by Tirumala et. al. https://arxiv.org/abs/2205.10770\n\nHowever, this is only a preliminary study, calling for additional empirical analysis to validate these findings and delve into the underlying causes in practical scenarios. The purported applications also warrant further validation to ensure their efficacy."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The exploration of the impact of a data point's position on its memorization is an intellectually interesting question, and also has practical implications centered around privacy.\n2. The finding that there is slower rate of outlier memorization during the early training stages is interesting."
                },
                "weaknesses": {
                    "value": "1. This paper does not compare with related work. \n2. The draft is a very preliminary compilation of results on a toy setup and its quality is below that of a conference paper.\n3. Discussion with https://arxiv.org/abs/2205.10770 might be helpful.\n\nI encourage the authors to continue working on the same and present it as a solid and generalizable result across different input complexities."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8934/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698938333423,
            "cdate": 1698938333423,
            "tmdate": 1699637124977,
            "mdate": 1699637124977,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "4LqPwC456x",
            "forum": "6ZuDeSHzjj",
            "replyto": "6ZuDeSHzjj",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8934/Reviewer_ehcz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8934/Reviewer_ehcz"
            ],
            "content": {
                "summary": {
                    "value": "The paper investigates memorization in diffusion models, and in particular how in-distribution, out of distribution/outlier and duplicate points are memorized. To do so, the authors train a toy model on a toy dataset with different numbers of outliers and duplicates. They find that in-distribution and duplicate points are memorized earlier, and outliers later during training."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Memorization in generative models is a highly relevant topic. The paper approaches an interesting problem in this space, by investigating how the memorization dynamics during training depend on factors such as likelihood under the training distribution and frequency/duplication inside the training dataset, in the context of diffusion models."
                },
                "weaknesses": {
                    "value": "- The results and the methodology in the paper are very preliminary. All experiments in the paper use a single toy diffusion model, as well as a single toy synthetic dataset. While such an approach can be valuable to form initial hypothesis about how models might memorize in more realistic settings, they need to be followed up with an extensive validation on more realistic datasets and realistic and diverse model architectures (or a thorough theoretical analysis). Otherwise it is not clear whether the findings from the toy setting will translate to practical scenarios.\n- The paper uses a questionable notion of memorization. A datapoint is considered memorized if it is closer to a training, rather than a test example. This definition of memorization has several issues:\n    - It does not consider model behavior or internals in any way. It will make the same memorization judgements for any models, even if they behave very differently. This is not realistic, as memorization should somehow depend on the model.\n    - It strongly depends on the sampling procedure of the training and test sets. In particular, whether a point is deemed memorized is a function of how many and how densely test points are sampled, while keeping the training set fixed.\n    - There is no formal definition of memorization, and it's not clear what distance metric is used and how the 2:1 ratio in the definition is applied.\n- Many details of the experimental setup are not specified, making it difficult to interpret the results:\n    - The dataset used in the experiments is not properly described. The text mentions two features as well as two moons with some noise, but does not provide a description or visualizations of samples from the dataset beyond that. Is the dataset a 2D image dataset or does it consist of just two numbers per data point? What sampling parameters are used? How are outlier points generated? Is the dataset based on prior work or was it created by the authors?\n    - What hyperparameters are used for the model architecture and training: how many layers, what hidden size, how many parameters in total, how many diffusion timesteps, what are the optimization parameters, etc.?\n    - What are the differences between the training runs in Figure 1 - 4? Only the number of inliers, outliers, training epoch and the dataset size, or are other parameters varied as well? Why were those combinations of hyperparameters investigated, i.e. why are they of particular interest?\n- Some minor points:\n    - The citation format is a bit strange and does not seem to correspond to the style guide.\n    - The paper is written in the \"I\" form. While this is a matter of taste, it is rather unusual."
                },
                "questions": {
                    "value": "- What does it mean to have e.g. 100 inliers in Figures 3 and 4, but 10000 dataset points? According to the definition in 3.2 inliers are the majority of the training points, so it's not clear what the difference is.\n- In Section 5 you say that the observed memorization behavior is a property of the training process and not of the model. How can you know that when you only investigate one type of model?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8934/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8934/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8934/Reviewer_ehcz"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8934/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698938652036,
            "cdate": 1698938652036,
            "tmdate": 1699637124872,
            "mdate": 1699637124872,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]