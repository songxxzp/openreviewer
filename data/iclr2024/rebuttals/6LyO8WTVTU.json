[
    {
        "title": "A Teacher-Guided Framework for Graph Representation Learning"
    },
    {
        "review": {
            "id": "Ax3DJz8oeR",
            "forum": "6LyO8WTVTU",
            "replyto": "6LyO8WTVTU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1714/Reviewer_RhAx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1714/Reviewer_RhAx"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a contrastive learning method for graph representation learning, where a self-supervised pre-trained teacher model is used to guide the training of a student model to obtain more generalized representations. Extensive experimentation demonstrates the effectiveness of the proposed method in improving the performance of graph classification and link prediction tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea of using teacher-student architecture in graph representation learning is interesting;\n\n2. The authors designed several loss functions to make use of the soft labels learned from the teacher model;\n\n3. The authors did comprehensive experiments on link prediction and classification tasks;\n\n4. This paper is well written and easy to follow."
                },
                "weaknesses": {
                    "value": "1. The major concern is the technical soundness of the teacher-student architecture. Intuitively, if you have a perfect teacher model, you can use it directly to calculate graph embeddings. Is it necessary to design such complicated contrastive learning losses to distill from the teacher model? The teacher-student model is originally designed to distill knowledge from large models and inject into small models, however, in this paper, the purpose of teacher-student is not to reduce the model size.\n\n2. The performance of the student model is theoretically bounded by the teacher model. I'm curious why the propose model TGCL-DSLA can perform better than the teacher DSLA and GraphLoG?\n\n3. For graph classification task, the datasets are all on molecule properties, which is quite limited. It is better to add more types of graph classification tasks to see how the proposed model performs on various type of graphs.\n\n4. From Table 1, most experimental results do not show significant improvement over baselines.\n\n4. According to Figure 4, why the proposed TGCL-DSLA learns the best graph embedding? It is hard to see the difference between (a) and (c)."
                },
                "questions": {
                    "value": "See weaknesses above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1714/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698711645925,
            "cdate": 1698711645925,
            "tmdate": 1699636100268,
            "mdate": 1699636100268,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1FRR5UF7ZG",
                "forum": "6LyO8WTVTU",
                "replyto": "Ax3DJz8oeR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1714/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1714/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We thank the reviewer for providing their valuable feedback for our paper.\n\n**Weakness 1: The major concern is the technical soundness of the teacher-student architecture. Intuitively, if you have a perfect teacher model, you can use it directly to calculate graph embeddings**\n\nYes. A perfect teacher model can produce perfect embeddings. However, as mentioned in our introduction (and in Figure 1), the existing techniques do not produce such a perfect model.\n\nWhile the teacher-student model is originally designed to distill knowledge from large models and inject into small models, several recent advancements are proposed and shown that the applications of KD technique is not only limited to reducing the size of the models\n\n**Weakness 2: The performance of the student model is theoretically bounded by the teacher model. I'm curious why the proposed model TGCL-DSLA can perform better than the teacher DSLA and GraphLoG?**\n\nPlease note that *\u201cperformance of the student model is theoretically bounded by the teacher model\u201d* is **incorrect**. We would like to request the reviewer to provide a reference for this claim so that we can make further comments.\n\nAlso, we would like to point out that Menon et al. [1] theoretically proved that \u2013 *\u201cBayes teacher\u201d providing the true class probabilities can lower the variance of the student objective, and thus improve performance.\u201d* [1]. (also refer to our Appendix B for detailed discussions on this.)\n\nSeveral studies also empirically verified that. For example, quoting Furlanello et al. [2] \u2013 *\u201cWe study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modelling tasks.\u201d* \n\nThe argument \u201cperformance (or generalization capacity) of the student model is usually upper bounded by the teacher model\u201d is only empirically valid when the capacity of the student model is lower than the teacher (see our experiments in Section 4.3). \nKey differences: We highlighted the key differences between our paper and the existing works in Section 2.2\n\n[1] A Statistical Perspective on Distillation (ICML 2021)\n\n[2] Born-Again Neural Networks (ICML 2018)\n\n**Weakness 3: For graph classification task, the datasets are all on molecule properties, which is quite limited. It is better to add more types of graph classification tasks to see how the proposed model performs on various type of graphs.**\n\nWe followed the previous existing papers to set up our experiments [1,2,3]. \nWe have shown results on general/social network graphs such as COLLAB, IMDB-Binary, and IMDB-Multi (see Table 2). We have achieved up to $\\approx$ 5% improvements using our proposed method.\n\n[1] Graph contrastive learning automated. (ICML 2021)\n\n[2] Self-supervised graph-level representation learning with local and global structure (ICML 2021)\n\n[3]  Graph self-supervised learning with accurate discrepancy learning. (NeurIPS, 2022)\n\n**Weakness 4: From Table 1, most experimental results do not show significant improvement over baselines.**\n\nPlease note that even a 1% improvement on molecular property prediction datasets is significant. For example, DSLA (accepted at NeurIPS\u201922) outperformed GraphLoG (ICML\u201921) by <1%. In comparison, we improved the performance by $\\approx$ 1.6%.\nFurther, in Table 2, we outperformed the existing models by up to $\\approx$ 5% on general, social network graphs.\n\n**Weakness 5: According to Figure 4, why the proposed TGCL-DSLA learns the best graph embedding? It is hard to see the difference between (a) and (c).**\n\nYes. Visually Figure (a) and (c) produces similar t-SNE plots and it is not easy to determine the best graph embeddings. We have updated our paper accordingly."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1714/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699900410798,
                "cdate": 1699900410798,
                "tmdate": 1699900410798,
                "mdate": 1699900410798,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QCIuxGEJAx",
            "forum": "6LyO8WTVTU",
            "replyto": "6LyO8WTVTU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1714/Reviewer_xgHF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1714/Reviewer_xgHF"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a contrastive learning (CL) based knowledge distillation (KD) framework for Graph Neural Networks (GNNs).  Particularly, the authors incorporated `soft labels` to facilitate a more regularized discrimination. In the teacher-student KD framework, the student network learns the representation by distilling the representations produced by the teacher network trained using unlabelled graphs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The overall written is clear and easy to follow.\n2. The experiments show that the distilled results perform better than the original teachers."
                },
                "weaknesses": {
                    "value": "I'm not the expert in contrastive learning. But as far as I know, the soft labeled CL is not novel. For example, \"Soft-Labeled Contrastive Pre-training for Function-level Code Representation\" in EMNLP 2022 has already proposed to leverage soft labels to mitigate similar issues of hard labels, .e.g, semantic problem, false negative case. I'm not sure whether other related works also applied the similar soft labeled CL framework."
                },
                "questions": {
                    "value": "In the experiments, the student model can consistently outperform the teacher model via the distillation. What if you use the current student model as the next round teacher model, can your performance continue to increase? or how many rounds KD will make the improvement negligible?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1714/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698784887699,
            "cdate": 1698784887699,
            "tmdate": 1699636100192,
            "mdate": 1699636100192,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OJFIVLTa2f",
                "forum": "6LyO8WTVTU",
                "replyto": "QCIuxGEJAx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1714/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1714/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We thank the reviewer for providing their valuable feedback for our paper.\n\n**Weaknesses:** \n*I'm not the expert in contrastive learning. But as far as I know, the soft labeled CL is not novel. For example, \"Soft-Labeled Contrastive Pre-training for Function-level Code Representation\" in EMNLP 2022 has already proposed to leverage soft labels to mitigate similar issues of hard labels, .e.g, semantic problem, false negative case. I'm not sure whether other related works also applied the similar soft labeled CL framework.*\n\n\n**Ans.** \nPlease note that soft-labelled CL (or the idea of incorporating KD with CL) is not new, and several papers are proposed and several advancements are made in the subsequent works. For example, **SimCLR-V2** [1] was among the first papers to incorporate KD with CL. However, several follow-up works were proposed and advanced the literature, and well accepted in different Tier-1 conferences.\n\nIn Section 2.2, we have systematically reviewed the existing works and provided a detailed discussion identifying the key differences between our proposed method compared to the existing works.\n\n[1] Big self-supervised models are strong semi-supervised learners (NeurIPS 2020)\n\n**Questions:**\n*In the experiments, the student model can consistently outperform the teacher model via the distillation. What if you use the current student model as the next round teacher model, can your performance continue to increase? or how many rounds KD will make the improvement negligible?*\n\n\nAn iterative evolutionary paradigm does not help the model performance. The reasons are as follows:\n\n- A better teacher model does not necessarily produce a better student model. [1,2,3]\n- A teacher model already provides an estimation of the Bayes class probability distribution over the labels. \n\nSection 4.1 of [1] theoretically explained the Bias-Variance Bound trade-off for Distillation from an imperfect teacher. The Bias-Variance Bound trade-off explains why our KD-based TGCL framework does not need a perfect model. Therefore, a student model typically achieves the best performance after training using the first-level teacher model. The performance saturates in their subsequent iterations.\n\n[1] A Statistical Perspective on Distillation (ICML 2021)\n\n[2] Knowledge distillation: Bad models can be good role models. (NeurIPS 2022)\n\n[3]  Better teacher better student: Dynamic prior knowledge for knowledge distillation (ICLR 2023)"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1714/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699899780988,
                "cdate": 1699899780988,
                "tmdate": 1699899780988,
                "mdate": 1699899780988,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WPcRDcGtsh",
            "forum": "6LyO8WTVTU",
            "replyto": "6LyO8WTVTU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1714/Reviewer_2WDw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1714/Reviewer_2WDw"
            ],
            "content": {
                "summary": {
                    "value": "**Summary**\n\nThe paper proposed an distillation-based unsupervised representation learning method. Given a teacher model pretrained on an unlabel dataset, a student model is pretrained on the same pretrained dataset with the TGCL objective, and then further finetuned on the downstream datasets. \n\n**Contributions**\n 1. The authors modified existing contrastive learning objectives (e.g. NTXent, D-SLA) using the distilled perceptual distance. \n - In NTXent, they reweight the cosine similarity with the proposed distilled perceptual distance. \n - In D-SLA, they replace the graph edit distance with the proposed distilled perceptual distance\n\n 2. This approach is implemented and evaluated on various node and graph classification datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper is well-organized and well-written. The motivation and methodology is clearly stated. \nAddressing my concerns would be greatly appreciated, and it could lead to an increase in my rating."
                },
                "weaknesses": {
                    "value": "**Concern 1. Why can the student model trained by TGCL outperform the teacher model on the downstream tasks?**\n\nTGCL primarily relies on the semantic similarity (soft label) provided by the teacher model in a knowledge distillation manner. In general distillation settings (e.g. model compression via distillation), the performance (or generalization capacity) of the student model is usually upper bounded by the teacher model. \n\nThus, I wonder why TGCL-student outperforms the teacher model? Which part of the algorithm contributes to the performance / information gain over the teacher model? \n\n\n\n**Concern 2. Is TGCL sensitive to the pretraining method of the teacher model?**\n\nAs shown in Table 1, TGCL-analogues do not consistently reach the state-of-the-art (SOTA) performance. Does TGCL exclusively apply to teacher models trained using specific pretraining methods, such as GraphLog and D-SLA?\n\nIf TGCL can enhance teacher models trained by any algorithm, could it serve as an iterative evolutionary paradigm? For instance, a teacher model undergoes k epochs of pretraining, followed by k epochs of TGCL updates, and this process repeats until convergence.\n\n\n\n**Concern 3: Does TGCL's performance depend on the quality of the teacher model?**\n\nAs TGCL heavily relies on the semantic similarity of teacher embeddings, I wonder to what extent the performance of TGCL relies on the representation quality of the teacher model\n\nIntuitively, obtaining a proficient teacher model is as challenging as achieving high-quality unsupervised representations. If TGCL only works with a well-trained teacher models, it may not address unsupervised representation learning directly. Instead,  it serves as an incremental improvement to existing effective unsupervised pretraining methods.\n\n\n\n**Concern 4: Does TGCL work when the teacher model overfits to the pretraining domain?**\n\nIf the teacher model fits the pretraining dataset well (or, \u2018overfits the pretraining distribution\u2019), it may encode domain-specific biases that hinder out-of-domain generalization. In this scenario, does it harm the downstream performance of the student model? What aspect of TGCL can help mitigate pretraining bias and enhance downstream generalization?"
                },
                "questions": {
                    "value": "My questions are stated in the 'weakness' section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1714/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1714/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1714/Reviewer_2WDw"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1714/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699202472956,
            "cdate": 1699202472956,
            "tmdate": 1699636100105,
            "mdate": 1699636100105,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0IWD5RwhY6",
                "forum": "6LyO8WTVTU",
                "replyto": "WPcRDcGtsh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1714/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1714/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "We thank the reviewer for providing their valuable feedback for our paper. \n\n**Concern 1. Why can the student model trained by TGCL outperform the teacher model on the downstream tasks?**\n\n**Ans.** \nPlease note that the \u201cperformance (or generalization capacity) of the student model is usually upper bounded by the teacher model\u201d is *incorrect*. In fact, it has been theoretically proven that \u2013 *\u201cBayes teacher\u201d providing the true class probabilities can lower the variance of the student objective, and thus improve performance.\u201d* [1]. (also refer to our Appendix B for detailed discussions on this.)\n\nSeveral studies also empirically verified that. For example, quoting Furlanello et al. [2] \u2013 *\u201cWe study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks.\u201d* \nThe argument \u201cperformance (or generalization capacity) of the student model is usually upper bound by the teacher model\u201d is only empirically valid when the capacity of the student model is lower than the teacher (see our experiments in Section 4.3). \n\n\n**Key differences:** We highlighted the key differences between our paper and the existing works in Section 2.2\n\n[1] A Statistical Perspective on Distillation (ICML 2021)\n\n[2] Born-Again Neural Networks (ICML 2018)\n\n**Concern 2. Is TGCL sensitive to the pretraining method of the teacher model?**\n\n**Concern 3: Does TGCL's performance depend on the quality of the teacher model?**\n\n**Ans.**\nYes. Similar to the existing KD methods, TGCL is also sensitive to the pretraining method of the teacher model. However, it is not exclusively applied to teacher models trained using specific pretraining methods. Also, an iterative evolutionary paradigm does not help the model performance. The reasons are as follows:\n\n- A better teacher model does not necessarily produce a better student model.\n- A teacher model already provides an estimation of the Bayes class probability distribution over the labels. \n\nSection 4.1 of [1] theoretically explained the Bias-Variance Bound trade-off for Distillation from an imperfect teacher. \n\n**Answer to Concern 2:** In Table 1, our TGCL-DSLA consistently outperforms vanilla DSLA (teacher). Similarly, TGCL-GraphLoG consistently outperforms vanilla Graph-LoG (teacher).\n\n**Answer to Concern 2 and 3:** The Bias-Variance Bound trade-off explains why our KD-based TGCL framework does not need a perfect model. Therefore, a student model typically achieves the best performance after training using the first-level teacher model. Their performance saturates in their subsequent iterations.\n\n[1] A Statistical Perspective on Distillation (ICML 2021)\n\n**Concern 4: Does TGCL work when the teacher model overfits to the pretraining domain?**\n\n- Since we are dealing with unsupervised training for the teacher, in general, it should not overfit to a specific domain.\n- Even if the model gets overfitted to a specific domain, our TGCL framework still performs well as a KD-based student model does not require a perfect teacher."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1714/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699899513540,
                "cdate": 1699899513540,
                "tmdate": 1699899513540,
                "mdate": 1699899513540,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "r5MBo2L9xQ",
                "forum": "6LyO8WTVTU",
                "replyto": "0IWD5RwhY6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1714/Reviewer_2WDw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1714/Reviewer_2WDw"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors' Answer to Concerns 1, 2 and 3"
                    },
                    "comment": {
                        "value": "**Ans 1 to Concern 1.** \n\nPlease note that the \u201cperformance (or generalization capacity) of the student model is usually upper bounded by the teacher model\u201d is *incorrect*. ......\n\n**Response 1-1. Response to Ans 1 to Concern 1.**  \n\n**Theoretical analysis would be helpful.**\n\nThanks for the clarification. To my understanding, using the distilled perceptual distance can reduce the Bayes-distilled risk, which in this paper corresponds to the data-noise binary classification error in contrastive learning. However, there is still a gap between achieving an improved contrastive loss and ahieving an improved classification loss in the downstream task, i.e. the final graph / node classification tasks. \n\nIt would be helpful if the authors can add some theoretical analysis that explains how to utilize the propositions in Appendix B to establish a generalization bound control for the final classification task using the TCGL method. This approach could be more illustrative than merely listing previous theoretical results without mathematical derivation, as it would clarify the necessary conditions for achieving improved generalization bound with the KD-based contrastive learning method.\n\n**Novelty of TCGL method on graph.**\n\nI appreciate that the authors have highlighted the key differences between this paper and previous works in Section 2.2. To my understanding, since the KD + CL paradigm is not new, the main contribution of this paper should lies on how is the proposed method exclusively tailored to graph data learning. While in the Limitations & Challenges section 2.2, the authors claim that TGCL can detect semantic changes between graphs with minor perturbations when compared to previous KD + CL methods, I believe this capability is primarily achieved through the use of the D-SLA framework (Section 3.3.2) and the D-SLA teacher, in which the graph edit distance plays an essential role. \n\nAlthough TGCL-DSLA and TGCL-GraphLoG is better than the vanilla teacher DSLA, and GraphLoG, the authors also show that TGCL-NTXent with DSLA teacher (e.g. the plain distilled perceptual loss) fails to outperform DSLA on two datasets. Does this means the DSLA framework plays a more decisive role in capturing graph features, than the proposed distilled perceptual loss?\n\nIt would be helpful if the authors could provide more in-depth discussions on why the proposed distilled perceptual loss is specifically tailored to graph data learning. This additional explanation would help ensure the novelty of TCGL are not underestimated.\n\n\n\n**Ans to Concern 2 and Concern 3**\n\nYes. Similar to the existing KD method ...\n\n**Response 2-2,3. Response to Ans to Concern 2 and Concern 3.**  \n\nAccording to [1], a closer alignment of the imperfect teacher with the Bayes probability results in reduced variance, including the generalization bound, for the distilled student. Since the Bayes dsitill risk in this paper is the contrastive loss, rather than the downstream classification loss, the teacher model (e.g. DSLA) only provides an estimation of the Bayes probability over the data-loss binary classification probability, rather than the Bayes probability over the labels. \n\nAs I have mentioned in the Response 1-1 above, a concrete theoretical analysis on how TCGL ahieves improved downstream performance would be helpful.\n\nAs shown in the Table 2 of [2], a sequentially distilled model is able to outperform a one-shot distilled student. According to [1], this can be achieved when each time the distilled student (which is also the next teacher) is trained to be closed to the Bayes teacher. Given the results in [1] and [2], I can hardly draw the conclusion \u2018iterative evolutionary paradigm does not help the model performance\u2019 without empirical experiments. Instead, I am curious on\n\n1. Why the current self-supervised teacher is able to provide Bayes probability over labels? There are no likelihood calibration on the training of self-supervised teacher.\n2. At each iteration, can we penalize the student and the teacher towards the Bayes teacher, to pursue a potential improvement in the successive distillation processes.\n\n\n\n[1] *Aditya K Menon, Ankit Singh Rawat, Sashank Reddi, Seungyeon Kim, and Sanjiv Kumar. A statistical perspective on distillation. In International Conference on Machine Learning, pp. 7632\u20137642. PMLR, 2021.*\n\n[2] *Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar. Born again neural networks. In International Conference on Machine Learning, pp. 1607\u20131616. PMLR, 2018.*"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1714/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700279565650,
                "cdate": 1700279565650,
                "tmdate": 1700279565650,
                "mdate": 1700279565650,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "m13RYqqYAI",
                "forum": "6LyO8WTVTU",
                "replyto": "0IWD5RwhY6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1714/Reviewer_2WDw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1714/Reviewer_2WDw"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors' Answer to Concern 4"
                    },
                    "comment": {
                        "value": "**Ans to Concern 4**\n\nSince we are dealing with unsupervised training for the teacher ...\n\n**Response 3-4. Response to Ans to Concern 4.**  \n\nThanks for the clarification. I am convinced that the unsupervised teacher will not overfit to the specific domain. According to [1], the generalization bound improvement requires the teacher to be closed to a Bayes probability estimator. But I doubt on whether this requirement can be achieved by a unsupervised teacher or overfitted teacher. \n\nWithout adding explicit probability calibration, how can the DSLA teacher provides a Bayes probability estimation on labels? According to Section 4.2, \u2018Why can more accurate teachers distill worse\u2019 in [1],  overfitted teacher is far worse than imperfect, as it provides accurate yet over-confident and poorly calibrated predictions. It would be highly appreciated if the authors can provide some theoretical or empirical analysis on this seeming contradiction."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1714/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700279604368,
                "cdate": 1700279604368,
                "tmdate": 1700279604368,
                "mdate": 1700279604368,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "prmR3pmLn9",
                "forum": "6LyO8WTVTU",
                "replyto": "icfSXzaQqB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1714/Reviewer_2WDw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1714/Reviewer_2WDw"
                ],
                "content": {
                    "title": {
                        "value": "Response to Ans to Response 2-2,3"
                    },
                    "comment": {
                        "value": "I appreciate the experiments, but I'm still unclear about arguments (a) and (b). The claim in phase (a), \"a better teacher model does not necessarily produce a better student model,\" needs more clarity. It's crucial to define what **\"better\"** means here. Does it refer to improved classification accuracy or closeness to a Bayes teacher?\n\nFrom my understanding, I am convinced that an accurate teacher model doesn't always yield a better student, as accuracy can lead to overconfident, distorted probabilities. In contrast, a model resembling a Bayes teacher could reduce student model variance and enhance performance.\n\nIn argument (b), the authors suggest the TGCL-student receives sufficient calibration from the DSLA teacher model. If TGCL is close to a Bayes teacher, the theory in [1] suggests that it should ensure strong distillation performance. However, this contradicts the empirical results of TGCL$^2$-DSLA, where it underperforms compared to TGCL.\n\nIn summary, I can hardly raise my score, since (a) more theoretical analysis (explicit conclusion instead of quoting separated existing works) are needed to illustrate 'when a good student can be distilled from a Bayes-like teacher' (b) how to quantitatively evaluate the distance between the Bayes teacher and the true teacher. (c) How is this method tailored to the graph domain? Since it seems like a standard KD + CL framework that has been witnessed in NLP / CV domains, the paper should emphasize more on what technique is used on the graph domain. (d) the logical derivation in this paper can be further strengthen."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1714/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736824296,
                "cdate": 1700736824296,
                "tmdate": 1700736824296,
                "mdate": 1700736824296,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fVSnykFJN6",
                "forum": "6LyO8WTVTU",
                "replyto": "WPcRDcGtsh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1714/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1714/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authros' response"
                    },
                    "comment": {
                        "value": "We appreciate the Reviewer\u2019s response to our rebuttal. We further want to clarify the doubts in the following:\n\n- **It's crucial to define what \"better\" means here.**\n\n**Ans.** For self-supervised representation learning models, we can define the \u201cbetter\u201d representation learning model based on their performance in the downstream tasks. However, using such a better representation learning model as a teacher may not lead to a better student model (as suggested in the previous distillation literature).\n\n- **If TGCL is close to a Bayes teacher, the theory in [1] suggests that it should ensure strong distillation performance.**\n\n**Ans.** Please note that TGCL produces better performance. However, it does not necessarily mean that it can be treated as closer to the Bayes teacher.\n\nFurther, even when we keep improving the teacher model (i.e., moving closer to the Bayes teacher), we shall reach a saturation point from which we cannot expect any further improvement. Our experimental results suggest that we reach that saturation point after just 1-level of teacher. Otherwise, by iteratively using teacher models, one would be able to achieve 100\\% accuracy for any dataset!\n\n-  **more theoretical analysis**\n\n**Ans.** All the required theoretical analyses questioned by the reviewers are already well-known in the literature, making our paper theoretically well-supported. Please refer to our previous responses and the theories cited in our paper. \n\n- **how to quantitatively evaluate the distance between the Bayes teacher and the true teacher.**\n\n**Ans.** Please refer to  A Statistical Perspective on Distillation (ICML 2021) which theoretically analyzed the above-mentioned concern.\n\n- **How is this method tailored to the graph domain?**\n\n**Ans.** \nPlease refer to the Introduction (2nd paragraph) and Figure 1 which clearly describe the importance of our proposed method for graph-specific datasets. We briefly summarize these points as follows:\n\n(1) We introduce distilled perceptual distance for graphs where minor changes may lead to major semantical differences. Such a notion of distance is not present in the graph literature.\n\n(2) While existing methods (e.g., DSLA) used the Edit distance, they cannot capture the semantic difference between two graphs with minor changes due to their discrete nature.\n\n(3) Finally, to the best of our knowledge, such distilled perceptual distance is not required for CV or NLP datasets as we can easily apply minor perturbations without changing the semantics drastically."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1714/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740655980,
                "cdate": 1700740655980,
                "tmdate": 1700742647127,
                "mdate": 1700742647127,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]