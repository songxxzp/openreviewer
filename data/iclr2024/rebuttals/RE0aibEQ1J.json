[
    {
        "title": "IG-Net: Image-Goal Network for Offline Visual Navigation on A Large-Scale Game Map"
    },
    {
        "review": {
            "id": "lK7vqOjOBI",
            "forum": "RE0aibEQ1J",
            "replyto": "RE0aibEQ1J",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7803/Reviewer_UHPS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7803/Reviewer_UHPS"
            ],
            "content": {
                "summary": {
                    "value": "This papers presents an agent design named IG-NET for image-goal navigation on large-scale environments. \n\nThis agent is trained on an offline learning fashion and relies only on images to navigate. \n\nThe model architecture proposed is an image to goal model, trained end-to-end with offline data from experts, with a representation enhancing mechanisms that incorporates position and navigation information prediction.\n\nExperiments show IG-Net robust navigation, being the model that achieves the best performance in all the different difficulty settings tested in the ShooterGame environment."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- The paper presents a somehow novel problem: offline image-goal navigation in large-scale game maps. \n\n- Section 5.4 (Case Study) is broad and it illustrates and provides with further evidence to support why IG-Net performs better than the other algorithms evaluated."
                },
                "weaknesses": {
                    "value": "* Section 2 does not provide a clear discussion of why the proposed model (IG-Net) is different from previously published models. It simply mentions that 'IG-Net is meticulously designed to navigate significantly larger maps,' but that is not sufficient.\n\n* Habitat Matterport 3D Semantics Dataset (see missing reference [MR1]) has not been included in the comparison of table 1. This is probably the largest dataset to date, at it should be listed.\n\n* Novelty problems:\n  * This is not the first model to propose to solve the navigation problem with offline data. First and foremost, the manuscript overlooks conducting a review of the literature related to imitation learning (behavior cloning) and even recent offline RL, where comparisons with many previous models that have addressed the problem of navigation in virtual environments under the same constraint\u2014using only offline data\u2014could be made. \n  * In Section 4.1, no significant differences with respect to previous works are detailed. Previous models used only images as inputs as well [MR2]. There are also works that use positional information to enhance model learning (PIRLNav model does it when using the compass and GPS information as iputs), or even auxiliary tasks (see [MR3]).\n  * More works should be included in a detailed discussion: e.g. (Shah et al., 2021) (Kahn et al., 2021) (Kim et al., 2022).\n  \n  \n* Clarity needs to be improved:\n  * Sections 4.2 - 4.4 need improvement in terms of clarity. Details are missing on how the training of the entire model is structured, and especially, details of the proposed architecture.\n  * One cannot use the same symbol for both the model weights and the angles.\n  * If the algorithm is trained in an offline manner, which was the offline algorithm used? I could not find any reference about any algorithm.\n\n* Weaknesses of the experimental setup:\n  * It is an offline model that just need a small dataset of 200 trajectories. That is not too much. I wonder how this impacts the performance of the model.\n  * The way the model is tested does not follow the standard in embodied AI literature. SR is measured when the agent arrives to the target AND samples an stop action, which means that (somehow) it knows that it has arrived. How does the proposed model know that it has arrived to the target? This is not considered in this paper, and the evaluation metrics used do not help to evaluate this fact.\n  * More baselines are needed. The selection of VGM is adequate but other approaches have to be considered. Offline-RL models should be considered as well (see [MR4]). \n  * How does the proposed approach perform in standard embodied AI navigation benchmarks? Habitat datasets, for example. There is offline data that can be used (see PIRLNav paper dataset of 77k navigation trajectories) or the authors can maybe generate new data (it seems that with just 200 trajectories the IG-Net model has enough information to be trained).\n  \n  \n  \n\nMissing references:\n===================\n[MR1]   Habitat-Matterport 3D Semantics Dataset.Yadav, Karmesh and Ramrakhya, Ram and Ramakrishnan, Santhosh Kumar and Gervet, Theo and Turner, John and Gokaslan, Aaron and Maestre, Noah and Chang, Angel Xuan and Batra, Dhruv and Savva, Manolis, arXiv preprint , https://arxiv.org/abs/2210.05633, 2022. https://aihabitat.org/datasets/hm3d-semantics/\n\n[MR2] Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J. Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi. Target-driven visual navigation in indoor scenes using deep reinforcement learning. In ICRA, 2017.\n\n[MR3] Joel Ye and Dhruv Batra and Abhishek Das and Erik Wijmans. Auxiliary Tasks and Exploration Enable ObjectNav, ICCV, 2022.\n\n[MR4] Offline Reinforcement Learning for Visual Navigation, Dhruv Shah and Arjun Bhorkar and Hrishit Leen and Ilya Kostrikov and Nicholas Rhinehart and Sergey Levine,6th Annual Conference on Robot Learning, 2022."
                },
                "questions": {
                    "value": "I've tried to detail most of the limitations and weaknesses of the proposed model in previous section, with some points that would need to be addressed in a rebuttal.\n\nOverall, I see here a manuscript with incremental contributions. From a theoretical perspective, the proposed model follows a short of imitation learning paradigm with some interesting loss functions that have been specifically designed for the problem of interest. Technically, the proposed architecture is a Masked Auto-Encoder for encoding the images, followed with some embedding concatenations plus positional and action decoding. I can hardly see here a considerable technical contribution either. Overall, the paper describes some ideas that are not adequate, in my humble opinion, for an ICLR conference."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Privacy, security and safety",
                        "Yes, Potentially harmful insights, methodologies and applications"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "This work describes a model for creating autonomous agents capable of various tasks: guiding costumers,  building autonomous robots, etc.\n\nThe introduced approach  allow to improve the navigation capabilities of autonomous agents. Of course, like most scientific work in the STEM sector, this might lead to some negative societal impacts, which our\nwork shares with most robotics applications: military robots (used in wrong hands), surveillance etc. The work described in the manuscript was carried out in simulation and as such is unlikely to have produced unethical results, except the impact of large-scale training on CO2 output."
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7803/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698681358023,
            "cdate": 1698681358023,
            "tmdate": 1699636954588,
            "mdate": 1699636954588,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Gnh0IoBjUY",
                "forum": "RE0aibEQ1J",
                "replyto": "lK7vqOjOBI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7803/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7803/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "To Reviewer UHPS: Thanks for your valuable feedback! We provide the response to each of your questions as follows. \n\n***Q1: Related work section does not provide a clear discussion of why the proposed model (IG-Net) is different from previously published models***\n\nA1: IG-Net stands out from previous models because it builds an implicit map comprehension through training the model with the several proposed auxiliary tasks: relative and absolute position prediction, distance prediction and navigation path prediction. Specifically, we show the implicit map understanding is especially helpful in large game environments like ShooterGame as compared to VGM, ViNT, and other baselines.\n\n---\n\n***Q2: This is not the first model to propose to solve the navigation problem with offline data; Missing related work for offline RL***\n\nA2: We thank your comment for mentioning those related works which we don\u2019t fully cover, and we will add these works in the discussions of our related work part. In the meantime, our method mainly focuses on designing auxiliary tasks for implicit map understanding that benefits the performance for behavior cloning methods, which is different from the main contributions of offline RL algorithms. Also, our method does not leverage reward information, which is in a different setting compared with offline RL methods.\n\n---\n\n***Q3: Missing entire model architecture in Section 4.2 - 4.4***\n\nA3: We add the network architecture in Section 4.3 in our revision. Briefly, IG-Net uses MAE-pretrained ViT to separately encode the observation image and goal image, and feed the embedding into four additional transformer layers, and concat the final joint embedding to feed into four different encoders for corresponding tasks. \n\n---\n\n***Q4: How the number of trajectories impacts the performance of the model***\n\nA4: We agree that it is important to test the scaling-up ability of our dataset and algorithm, which will be an important part of study in our future work. We also believe that 200 trajectories are sufficient to validate the effectiveness of our proposed approach as our model can successfully reduce all losses on both training and validation sets. \n\n---\n\n***Q5: More baselines are needed***\n\nA5: We have added 4 baselines to the experiments including ViNT, GNM, NoMaD, and NoMaD-EMA, shown in Table 2. These baselines all use RGB observations and can be trained without RL. Results show that IG-Net outperforms all the baselines in ShooterGame. We believe the added baselines suffer significant performance loss due to lack of a z-axis, lack of long-horizon training, and lack of a low-level navigator.\n\n---\n\n***Q6: How does the proposed approach perform in standard embodied AI navigation benchmarks?***\n\nA6: We have added Gibson experiments where IG-Net is evaluated on different seen and unseen floorplans. Since Gibson environments are much smaller (Table 1), IG-Net doesn\u2019t have an advantage compared to previous approaches. This experiment validates IG-Net has generalization ability to novel environments and the implicit training is especially important in larger game experiments.\n\n---\n\n***Q7: No stop action, and performance is not aligned with embodied AI literatures***\n\nA7: In the above experiments on Gibson, we have added the Stop action for both IG-Net and baseline method, and measure the success of navigation only when the agent calls the stop action within the distance threshold of the goal position. We didn\u2019t add Stop action for ShooterGame because this may add an additional factor to the evaluation (the strategy for calling stop action, etc.). However, we will add the Stop action in the future experiments to align with existing literature.\n\n---\n\n| Setting               | Setting      | Total Trials | SR    | SPL   | DDR   |\n|-----------------------|--------------|--------------|-------|-------|-------|\n| IG-Net                | Gibson-Train | 7200         | **0.539** | **0.471** | **0.447** |\n| IG-Net                | Gibson-Eval  | 1400         | **0.584** | **0.505** | **0.417** |\n| IG-Net (no auxiliary) | Gibson-Train | 7200         | 0     | 0     | 0.323 |\n| IG-Net (no auxiliary) | Gibson-Eval  | 1400         | 0     | 0     | 0.326 |\n\n**Table 1: Generalization capability of IG-Net on Gibson environment on 14 unseen environments.**"
                    },
                    "title": {
                        "value": "Response to Reviewer UHPS"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7803/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731625871,
                "cdate": 1700731625871,
                "tmdate": 1700732382161,
                "mdate": 1700732382161,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6FigJ9mqqT",
                "forum": "RE0aibEQ1J",
                "replyto": "lK7vqOjOBI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7803/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7803/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer UHPS (Part 2)"
                    },
                    "comment": {
                        "value": "| Difficulty       |          | Easy     |          |          | Medium   |          |          | Hard     |          |\n|------------------|----------|----------|----------|----------|----------|----------|----------|----------|----------|\n| Metric           | SR       | SPL      | DDR      | SR       | SPL      | DDR      | SR       | SPL      | DDR      |\n| VGM              | 0.24     | 0.09     | 0        | 0        | 0        | 0.3      | 0        | 0        | 0.27     |\n| VGM-Tuned        | 0.2      | 0.09     | 0.39     | 0.08     | 0.05     | 0.39     | 0        | 0        | 0.32     |\n| VGM-Loaded       | 0.32     | 0.14     | 0.49     | 0.04     | 0.02     | 0.26     | 0        | 0        | 0.27     |\n| VGM-Tuned-Loaded | 0.2      | 0.08     | 0.32     | 0.04     | 0.02     | 0.36     | 0.08     | 0.05     | 0.42     |\n| NoMaD            | 0.1      | 0.06     | 0.31     | 0.02     | 0.01     | 0.2      | 0        | 0        | 0.16     |\n| NoMaD-EMA        | 0.08     | 0.05     | 0.27     | 0.06     | 0.04     | 0.26     | 0        | 0        | 0.23     |\n| ViNT             | 0.08     | 0.07     | 0.17     | 0.02     | 0.02     | 0.16     | 0        | 0        | 0.15     |\n| GNM              | 0.04     | 0.04     | 0.13     | 0.04     | 0.03     | 0.15     | 0.02     | 0.02     | 0.14     |\n| IG-Net           | **0.54** | **0.24** | **0.75** | **0.26** | **0.17** | **0.65** | **0.24** | **0.15** | **0.75** |\n| IG-Net noaux     | 0.18     | 0.09     | 0.36     | 0.14     | 0.08     | 0.42     | 0        | 0        | 0.44     |\n\n**Table 2: Navigation Performance of IG-Net compared with NoMaD, ViNT, and GNM.**"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7803/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731745997,
                "cdate": 1700731745997,
                "tmdate": 1700732392681,
                "mdate": 1700732392681,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yoVv4MC1zS",
            "forum": "RE0aibEQ1J",
            "replyto": "RE0aibEQ1J",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7803/Reviewer_rCRb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7803/Reviewer_rCRb"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents the Image-Goal Network (IG-Net), a novel approach to address the challenges of navigating large-scale gaming environments with limited offline data. The model is trained end-to-end, incorporating position, path, and distance prediction for implicit spatial mapping. Experiments in the ShooterGame environment validate IG-Net's efficacy, and the paper also contrasts it with existing methods in visual navigation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The authors present an interesting and amazing work in the paper and raise the meaningful challenge of navigation in large-scale environments to the community. \n3. The authors introduce an expansive environment ShooterGame environment. The ShooterGame environment is nearly 20 times larger than the previous environment on one map. \n3. IG-Net utilizes a variety of auxiliary tasks, including local and global path planning and navigation distance prediction, to strengthen its visual navigation capabilities. These auxiliary tasks enrich the agent's representation and improve its navigation performance.\n2. In comparison to other methodologies like VGM, IG-Net outperforms in terms of success rate, success weighted by path length, and navigation efficiency. It achieves higher success rates and navigation efficiency even under more challenging settings."
                },
                "weaknesses": {
                    "value": "1. The authors perform the testing on the seen environment during training. I believe that demonstrating the performance in unseen environments will consolidate the work. \n2. The authors do not provide the details for the architecture of the IG-Net. I am a little bit confused about the input/output of each network component. \n3. The ablation study in Table 4 shows an experiment \"IG-Net (no position).\" I am curious that the experiment demonstrates the performance of IG-Net without both absolute/relative position prediction or either of them. A similar question about the ablation experiment \"IG-Net (no path and dist)\", why did the authors remove two auxiliary tasks at the same time in the ablation study?"
                },
                "questions": {
                    "value": "1. Although the coverage of the ShooterGame environment is much larger than other environments, it also seems to be more empty than other environments. Could the authors discuss the difficulty differences in navigation among these environments except the coverage area? Meanwhile, I am curious about the number of different states in the ShooterGame environment.\n1. How do the authors concatenate the encoding embeddings from the first step into a unified representation? Is that a FIFO stack or an MLP/pooling layer serving as a projector to ensure the same size of concatenated features during different lengths of episodes?\n2. In the third part of the architecture design, the authors mentioned \"Utilizing position, path, distance, and action decoders. \" I am curious whether the historical information, apart from visual embedding, is included in the action prediction. If the authors do not use historical action/state representations, how does the agent maintain the search efficiency instead of being stuck in a small area?\n4. Suppose we put the trained agent in an unseen environment, how will the agent perform? In the unseen environment, will the agent still be able to perform these auxiliary tasks? How does the agent perform on these auxiliary tasks, e.g., absolute/relative position prediction?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7803/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7803/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7803/Reviewer_rCRb"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7803/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698765379428,
            "cdate": 1698765379428,
            "tmdate": 1699636954283,
            "mdate": 1699636954283,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iaJX6qaUb3",
                "forum": "RE0aibEQ1J",
                "replyto": "yoVv4MC1zS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7803/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7803/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer rCRb"
                    },
                    "comment": {
                        "value": "To Reviewer rCRb: Thanks for your valuable feedback! We provide the response to each of your questions as follows. \n\n***Q1: Although being larger, ShooterGame is more empty. Discuss other metrics for navigation difficulty.***\n\nA1: Other works use metrics including specific surface area and navigation complexity to show the complexity of the environment (see Table 1 in the paper of Gibson dataset https://arxiv.org/pdf/1808.10654.pdf). We believe that these two metrics overlook the scale of the environment which greatly affects the navigation difficulty in the environment. We think that the minimum angle of turns alongside the navigation episode is a great metric. We will add more metrics for comparing navigation complexity in our future work. \n\n---\n\n***Q2: The paper doesn\u2019t provide network architecture for IG-Net, and input/output for the network***\n\nA2: We add the network architecture in Section 4.3 in our revision. Briefly, IG-Net uses MAE-pretrained ViT to separately encode the observation image and goal image, and feed the embedding into four additional transformer layers, and concat the final joint embedding to feed into four different encoders for corresponding tasks. \n\n---\n\n***Q3: IG-Net trains and tests in the same environment. Should demonstrate results in novel environments***\n\nA3: Please refer to our new experiments on Gibson, where the agent is evaluated on 14 unseen environments for short horizon navigation.  Results show that IG-Net can navigate well in unseen environments and outperforms no auxiliary task baselines, showing the generalizability of IG-Net.\n\n---\n\n***Q4: The definition of \u201cNo position\u201d, and why removing two auxiliary tasks at the same time***\n\nA4: \u201cNo position\u201d ablation study simultaneously removes both relative position prediction and absolute position prediction. We remove two auxiliary tasks at the same time to verify the joint effect of position loss as a whole. We have realized that removing each single auxiliary task for ablation study is better and will perform the experiment in our future work. \n\n---\n\n***Q5: Whether historical states serve as inputs for IG-Net. Historical states are crucial to searching behaviors***\n\nA5: We really appreciate the comment. Currently, IG-Net only uses one current image and one goal image as inputs, which we also observe failure searching behavior around the corner of the map and hope to try adding historical states in the inputs. In the future work, we will carefully design experiments to verify the design for historical states."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7803/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731537389,
                "cdate": 1700731537389,
                "tmdate": 1700732151530,
                "mdate": 1700732151530,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "mLBWeGQUbc",
            "forum": "RE0aibEQ1J",
            "replyto": "RE0aibEQ1J",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7803/Reviewer_Sxb4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7803/Reviewer_Sxb4"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes IG-Net, a end-to-end method designed for visual navigation in large scale environments like the shooter game environment from unreal engine. Instead of building an explicit map IG-Net leverages visual and implicit pose information to learn better representations for navigation. To do this IG-Net uses a combination of relative pose prediction, absolute pose prediction, navigation distance and navigation path prediction as auxiliary tasks on offline trajectories and randomly sampled pose and visual data. The paper also shows IG-Net outperforms a Visual-Graph-Memory baseline which uses topological graph representation in the ShooterGame environment."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper proposes a method to test Image-Goal Navigation in large-scale game environments which require long-horizon navigation skills\n2. IG-Net outperforms VGM baseline by a significant margin on all 3 evaluation splits (easy, medium, hard)\n3. Ablation in table 2 nicely show the importance of auxiliary objectives in boosting evaluation performance for IG-Net\n4. Experiments in section 5.4.2. show that IG-Net is also robust to noise to some extent and doesn\u2019t lead to significant drop in performance on the hard split. But for some other splits we do see a large drop in performance.\n5. Paper is easy to follow"
                },
                "weaknesses": {
                    "value": "1. The experiments section doesn\u2019t show comparison with state-of-the-art methods on ImageNav. There has been some recent works in ImageNav which use end-to-end RL training using pretrained representations (visual and pose) [1, 2, 3, 4] which achieve significant performance on the ImageNav benchmark. The current state-of-the-art method [1] acheives ~92% SR using simple concatenation of goal and observation image being fed to a single ResNet model to generate a observation embedding. OVRL [3] and OVRL-v2 [2] have been using pretrained visual representations using data from omnidata which leads to improved perfomance. It is essential that the authors compare IG-Net to the state-of-the-art baselines. I\u2019d suggest adding comparison to FGPrompt-EF baseline from [1] and OVRL and OVRL-v2 pretrained baselines from [2, 3]. [4] seems to be a concurrent work with no code released so no need to add comparison with it.\n2. To the best of my understanding, the experimental setup is not testing generalization to new maps in the ShooterGame environment. Can authors please show results of generalization to different maps in the environment? If the performance is being tested in the same scene then both methods should be able to achieve significantly higher success. Can authors please share more details about the VGM baseline\u2019s training and evaluation setup? In addition, it would be good if the authors can add more details about the ShooterGame dataset they used for training and testing.\n3. Based on description in section 4.3 it is unclear what the policy architecture of IG-Net looks like. Can authors please share a policy architecture figure?\n4. Ablations in section 5.5 need to ablate each component of auxiliary task separately. In the current results it looks like row 2 only uses relative and absolute pose prediction and row 3 doesn\u2019t use navigation path prediction and distance aux tasks. In each ablation authors should sequentially remove every single loss from list of auxiliary losses to clearly demonstrate effectiveness of each components.\n\n[1] X. Sun, P. Chen, J. Fan, T. Li,\u2026 FGPrompt: Fine-grained Goal Prompting for Image-goal Navigation. In NeurIPS, 2023\n[2] K. Yadav, A. Majumdar, R. Ramrakhya, N. Yokoyama, A. Baevski, Z. Kira, O. Maksymets, and D. Batra. Ovrl-v2: A simple state-of-art baseline for imagenav and objectnav. arXiv preprint arXiv:2303.07798, 2023.\n[3]  K. Yadav, R. Ramrakhya, A. Majumdar, V.-P. Berges, S. Kuhar, D. Batra, A. Baevski, and O. Maksymets. Offline visual representation learning for embodied navigation. In International Conference on Learning Representations (ICLR), 2022.\n[4] G. Bono, L, Antsfeld, B.  Chidlovskii, P. Weinzaepfel, C. Wolf End-to-End, (Instance)-Image Goal Navigation Through Correspondence as an Emergent Phenomenon, arxiv preprint arxiv:2309.16634"
                },
                "questions": {
                    "value": "1. The best methods [1,2,3,4] on ImageNav task use RL for training. It would be good if authors can add comparison with a RL trained baseline using FGPrompt-EF. Can authors provide more details into why it is not possible to get online data? Is it due to slow simulator speed?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7803/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7803/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7803/Reviewer_Sxb4"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7803/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699247134407,
            "cdate": 1699247134407,
            "tmdate": 1699636954123,
            "mdate": 1699636954123,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tDLBzPnCkV",
                "forum": "RE0aibEQ1J",
                "replyto": "mLBWeGQUbc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7803/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7803/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Sxb4"
                    },
                    "comment": {
                        "value": "To Reviewer Sxb4: Thanks for your valuable feedback! We provide the response to each of your questions as follows. \n\n***Q1: Compare with state-of-the-art methods on ImageNav***\n\nA1: We have included comparison between ViNT, GNM, and NoMaD baselines, shown in Table 2. We thank the reviewers for suggesting SOTA methods such as FGPrompt and OVRL. However, to keep the comparison fair and due to the restrictiveness of our environment in simulation speed and perception domains, we have to focus on non-RL algorithms that use RGB observations only.\n\n---\n\n***Q2: The experimental setup is not testing generalization to new maps in the ShooterGame environment***\n\nA2: We perform additional experiments of IG-Net on new maps in Gibson experiments, where the agent is trained on 396 environments and evaluated on 14 novel ones, shown in Table 1. Results show that IG-Net can navigate well in the easy setting and outperforms no auxiliary task baselines, showing the generalizability of IG-Net.\n\n---\n\n***Q3: Unclear architecture for policy network***\n\nA3: We add the network architecture in Section 4.3 in our revision. Briefly, IG-Net uses MAE-pretrained ViT to separately encode the observation image and goal image, and feed the embedding into four additional transformer layers, and concat the final joint embedding to feed into four different encoders for corresponding tasks. \n\n---\n\n***Q4: Ablate each auxiliary task separately***\n\nA4: We really appreciate the comment, and will perform the experiment for ablating each auxiliary task in our future revision of this work. \n\n---\n\n***Q5: Why it is not possible to get online data for RL approach***\n\nA5: We thank the reviews for suggesting the RL approach. Currently, our simulator runs at a maximum of 5 FPS to wait until an action is executed through keyboard simulation. This is too slow for RL training compared to 62.5 FPS in MuJoCo [1] and 8000-10000 FPS in Habitat-Sim [3][4].\n\n---\n\n[1] MuJoCo Official Documentation: https://mujoco.readthedocs.io/en/stable/programming/samples.html?highlight=fps#sabasic\n\n[2] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026\u20135033, 2012. doi: 10.1109/IROS.2012.6386109.\n\n[3] Habitat-Sim Official Documentation: https://aihabitat.org/#:~:text=Habitat%2DSim%20simulates%20a%20Fetch,body%20dynamics%20for%201%2F30sec.\n\n[4] Manolis Savva*, Abhishek Kadian*, Oleksandr Maksymets*, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra. Habi-tat: A Platform for Embodied AI Research. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019.\n\n| Setting               | Setting      | Total Trials | SR    | SPL   | DDR   |\n|-----------------------|--------------|--------------|-------|-------|-------|\n| IG-Net                | Gibson-Train | 7200         | **0.539** | **0.471** | **0.447** |\n| IG-Net                | Gibson-Eval  | 1400         | **0.584** | **0.505** | **0.417** |\n| IG-Net (no auxiliary) | Gibson-Train | 7200         | 0     | 0     | 0.323 |\n| IG-Net (no auxiliary) | Gibson-Eval  | 1400         | 0     | 0     | 0.326 |\n\nTable 1: Generalization capability of IG-Net on Gibson environment on 14 unseen environments. \n\n| Difficulty       |          | Easy     |          |          | Medium   |          |          | Hard     |          |\n|------------------|----------|----------|----------|----------|----------|----------|----------|----------|----------|\n| Metric           | SR       | SPL      | DDR      | SR       | SPL      | DDR      | SR       | SPL      | DDR      |\n| VGM              | 0.24     | 0.09     | 0        | 0        | 0        | 0.3      | 0        | 0        | 0.27     |\n| VGM-Tuned        | 0.2      | 0.09     | 0.39     | 0.08     | 0.05     | 0.39     | 0        | 0        | 0.32     |\n| VGM-Loaded       | 0.32     | 0.14     | 0.49     | 0.04     | 0.02     | 0.26     | 0        | 0        | 0.27     |\n| VGM-Tuned-Loaded | 0.2      | 0.08     | 0.32     | 0.04     | 0.02     | 0.36     | 0.08     | 0.05     | 0.42     |\n| NoMaD            | 0.1      | 0.06     | 0.31     | 0.02     | 0.01     | 0.2      | 0        | 0        | 0.16     |\n| NoMaD-EMA        | 0.08     | 0.05     | 0.27     | 0.06     | 0.04     | 0.26     | 0        | 0        | 0.23     |\n| ViNT             | 0.08     | 0.07     | 0.17     | 0.02     | 0.02     | 0.16     | 0        | 0        | 0.15     |\n| GNM              | 0.04     | 0.04     | 0.13     | 0.04     | 0.03     | 0.15     | 0.02     | 0.02     | 0.14     |\n| IG-Net           | **0.54** | **0.24** | **0.75** | **0.26** | **0.17** | **0.65** | **0.24** | **0.15** | **0.75** |\n| IG-Net noaux     | 0.18     | 0.09     | 0.36     | 0.14     | 0.08     | 0.42     | 0        | 0        | 0.44     |\n\nTable 2: Navigation Performance of IG-Net compared with NoMaD, ViNT, and GNM."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7803/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731391695,
                "cdate": 1700731391695,
                "tmdate": 1700732062831,
                "mdate": 1700732062831,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iCfDo2ePAd",
            "forum": "RE0aibEQ1J",
            "replyto": "RE0aibEQ1J",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7803/Reviewer_JFQf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7803/Reviewer_JFQf"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method of training navigation agents which can navigate large spaces using ego-centric RGB observations without assuming access to accurate position information. The task is to navigate to an image-goal in a large simulated environment. The method utilizes several auxiliary losses to learn the spatial map and how to navigate it implicitly. The authors demonstrate their method on several trajectories of varying difficulty in a single environment."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper studies navigating large-spaces using RGB egocentric observations without accurate localisation which is an interesting problem that hasn\u2019t been extensively studied before.\n- The environment studied in the setup is dynamic which makes the task even more challenging (with varying lighting conditions, moving objects and other distractors)\n- The authors train an end-to-end approach for navigating in large spaces which hasn\u2019t been shown to work before.  They compare their method against meaningful baselines (baselines that build an explicit representation of the environment as graphs) and show superior results"
                },
                "weaknesses": {
                    "value": "- The authors test their method on a single environment which is a fairly restrictive experimental setup. We don\u2019t know if the model is capable of handling / memorising multiple environments. I would have liked to see experiments in which a single model can be learned for multiple environments.\n- The experiments also assume perfect actuation \u2014 left/right and forward will always lead to the same amount of actuation. I wonder how does the agent behave when the actuation during test-time is noisy / different from those used during training. Such a situation is fairly common in the real world and it would be nice to show experiments for the more general setting.\n- More importantly, the paper doesn\u2019t have any analysis and discussion about the train-time vs test-time distribution. How different are the trajectories used at train-time compared to test-time trajectories. This analysis is important to understand the model\u2019s generalisation capability and to actually test if it has build an efficient implicit representation of the map or not.\n- I would have liked to see some more experiments to probe this implicit understanding of the map. For instance, if the agent has only been trained on a complex trajectory, can it learn to output the shortest path between two pair of observations?\n- Similar to the last point, I would have also liked to see the effect of distractors in the observation on task performance. For instance, does the performance decrease when we add more distractors in the observation at test time? Since one of the novelties of this task setup is dynamic observations, it feels that this kind of analysis is important for a more thorough evaluation."
                },
                "questions": {
                    "value": "- Not a question, but a comment. The paper overloads the meaning of \\theta in equation (2) and (3) which made parsing those equations slightly confusing. I would also encourage the authors to run a more extensive grammar check to improve the writing of the paper. \n\n- According to my understanding, none of the losses in Section 4.2 actually involves predicting actions. How does the method learn to predict actions?\n\n- Related to weaknesses:\n    - it\u2019d be great to study the effect of agent\u2019s observation field of view and  different action spaces on task performance.\n    - it\u2019d be interesting to study the robustness of the method when action spaces change during test time\n    - can the authors comment on how different test time trajectories are to the ones seen during training? For example, for each location of the agent for a trajectory at test time, what is the nearest location observed during training?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7803/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699316446359,
            "cdate": 1699316446359,
            "tmdate": 1699636953937,
            "mdate": 1699636953937,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cSRoEzIpne",
                "forum": "RE0aibEQ1J",
                "replyto": "iCfDo2ePAd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7803/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7803/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JFQf"
                    },
                    "comment": {
                        "value": "To Reviewer JFQf: Thanks for your valuable feedback! We provide the response to each of your questions as follows. \n\n***Q1: Experiments with a single model learned for multiple environments***\n\nA1: We perform additional experiments of IG-Net on new maps in Gibson experiments, where the agent is trained on 396 environments and evaluated on 14 novel ones, shown in Table 1. Results show that IG-Net can navigate well in the easy setting and outperforms no auxiliary task baselines, showing the generalizability of IG-Net.\n\n---\n\n***Q2: Performance when observation and actuation noise exists, which is common in the real world***\n\nA2: Actuation noise is a very important problem for real world applications, and it can be simulated in game engines by adding action noise. Exploring how IG-Net can handle actuation noise is a good research direction and we will explore it in future work. Observation noise requires specific design in the camera, and we will test the performance in the future work.\n\n---\n\n***Q3: Show train-time and test-time state-goal distribution***\n\nA3: We verify the difference of test-time state-goal pair and training state-goal pair by defining a metric of minimum state-goal distance. For each evaluation state-goal pair $(s,g)$, the minimum state-goal distance is defined to be $\\min_{i} \\|s_i - s\\|_2 + \\|g_i - g\\|_2$, where $(s_1, g_1), \u2026, (s_N, g_N)$ are $N=200$ state-goal pairs for navigation training data. Minimum state-goal distance describes the lowest difference in the training dataset of an evaluation state-goal pair. The mean value of minimum state-goal distance on the whole evaluation data is $2130$ on $D<4000$ difficulty, $2260$ on the $4000<D<8000$ difficulty, and $2375$ on the $D>8000$ difficulty, which is much larger than the distance threshold for success judgment ($800$). This demonstrates that the evaluation state-goal pairs have a small overlap with the training dataset. \n\n---\n\n***Q4: Experiment for probing the implicit understanding of the map, e.g. predicting shortest path***\n\nA4: This is a great direction for testing the internal scene understanding in the IG-Net. We will perform detailed analysis on designing probing tasks for pretrained IG-Net to verify this in the future work. \n\n---\n\n***Q5: How does IG-Net perform action predictions***\n\nA5: Actions are categorical {Forward, Left, Right} and are learned through Cross-Entropy loss between expert training data. We have added the description for action prediction in Section 4.2 in our revisions. \n\n---\n\n| Setting               | Setting      | Total Trials | SR    | SPL   | DDR   |\n|-----------------------|--------------|--------------|-------|-------|-------|\n| IG-Net                | Gibson-Train | 7200         | **0.539** | **0.471** | **0.447** |\n| IG-Net                | Gibson-Eval  | 1400         | **0.584** | **0.505** | **0.417** |\n| IG-Net (no auxiliary) | Gibson-Train | 7200         | 0     | 0     | 0.323 |\n| IG-Net (no auxiliary) | Gibson-Eval  | 1400         | 0     | 0     | 0.326 |\n\n**Table 1: Generalization capability of IG-Net on Gibson environment on 14 unseen environments.**"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7803/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731008186,
                "cdate": 1700731008186,
                "tmdate": 1700732287022,
                "mdate": 1700732287022,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ESZUN5xpR3",
            "forum": "RE0aibEQ1J",
            "replyto": "RE0aibEQ1J",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7803/Reviewer_m9pY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7803/Reviewer_m9pY"
            ],
            "content": {
                "summary": {
                    "value": "This paper deals with the problem of Image Goal tasks when agent pose in environment is unknown, and shows how training with limited navigation data (but good amount of positional data) in large scale game space like areas -  is still possible to yield good results. The authors have introduced a Network archiecture to represent spatial map information implicitly based on prediction of position, distance and path. Although the application setup is novel, but the paper needs an overhaul to bring in the novel aspects and add technical challenges that were overcome. The ablation studies and experiments do support the writings."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The usage of distance decrement rate (DDR) is logical in this paper wrt task, however a scaling may help in evaluations.\nThe code is available and executed as mentioned.\nThe scope of application is good and will open up research communities to tackle the problem with game to real transfer later."
                },
                "weaknesses": {
                    "value": "In Fig. 1, the separating boundary walls are quite distant due to the open space, making exploration easy compared to close-looped indoor environment with full or partial occlusions. \nThe SPL compared to success rate in Table 2 section 1, is not that great - any logic for that? Although a gap in the metrics will help the future researchers to improve the SOTA.\nThe architecture of the work should be clearly highlighted and mentioned.\nThe video is not clear in terms of what is being planned to be achieved - this needs serious improvement - like how the scene is processed and the action outputs wrt image goal is coming.\nMore SOTA study and real world transfer is expected."
                },
                "questions": {
                    "value": "Not sure how much the trained policy is suitable for FPS games compared to real world robotic deployments in the wild in human co-occupied spaces.\nThe action space should be clearly mentioned for the testbed. Also how dependent on camera intrinsics.\nIncorporation of Auxiliary Tasks - how is it done - need finer details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 6,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7803/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699341171044,
            "cdate": 1699341171044,
            "tmdate": 1699636953721,
            "mdate": 1699636953721,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Lqi8gXJgka",
                "forum": "RE0aibEQ1J",
                "replyto": "ESZUN5xpR3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7803/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7803/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer m9pY"
                    },
                    "comment": {
                        "value": "To Reviewer m9pY: Thanks for your valuable feedback! We provide the response to each of your questions as follows.\n\n***Q1: Boundary walls are distant so exploration is easy compared to indoor environments***\n\nA1: Although the boundary walls are relatively distant, the small width of the corridors compared to the large game map still poses great challenges for exploration. \n\n---\n\n***Q2: SPL is not good compared to SR***\n\nA2: We mention the reason for this in Section 5.2. We use Euclidean distance between state and goal position since geodesic distance is not available in ShooterGame, which makes SPL smaller compared to normal cases. We should more clearly show this fact in the table. We may also use expert path length as comparison for SPL calculation to alleviate the problem. \n\n---\n\n***Q3: Network architecture should be more clear***\n\nA3: We add the network architecture in Section 4.3 in our revision. Briefly, IG-Net uses MAE-pretrained ViT to separately encode the observation image and goal image, and feed the embedding into four additional transformer layers, and concat the final joint embedding to feed into four different encoders for corresponding tasks. \n\n---\n\n***Q4: How much is the policy suitable for FPS games compared to real world robots***\n\nA4: Our challenge setting mimics the real-world human player perception in games. By not requiring a panoramic camera, depth camera, or global pose, we showcase the robustness of our navigation algorithm. We do agree that there is a domain gap between game simulators and real-world when humans and other agents are involved, which is a different type of setting than this paper. \n\n---\n\n***Q5: Clearly mention the action space***\n\nA5: Our action is discrete and similar to how humans play games using a keyboard. Specifically, the actions are {Forward, Turn Left, Turn Right} with key-pressing time being {0.4, 0.2, 0.2} seconds, respectively. We use UnrealCV as the APIs for executing the actions in the unreal engine simulator."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7803/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730788391,
                "cdate": 1700730788391,
                "tmdate": 1700731953442,
                "mdate": 1700731953442,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]