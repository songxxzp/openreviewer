[
    {
        "title": "Doubly Robust Structure Identification from Temporal Data"
    },
    {
        "review": {
            "id": "6dYtuF2XzG",
            "forum": "xbUlKe1iE8",
            "replyto": "xbUlKe1iE8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission231/Reviewer_biK3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission231/Reviewer_biK3"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an approach for learning time varying causal features of a target variable using Granger causality and doubly robust methods. The approach can also be used for full causal discovery and does not require the faithfulness or causal sufficiency assumptions.\n\nA theorem is given that under the assumptions made, Granger causality is equivalent to true causation. The approach then proceeds by choosing a target causal feature, fitting parameters and testing significance of the parameters.\n\nThe approach is empirically evaluated on a semisynthetic dataset (Dream3) and is one of the top performing methods in 3/5 of the experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) The approach is very novel to the best of my knowledge\n\n2) If the theorem that under the given setting, Granger causality is equivalent to true causation is correct (I am unable to check the proof of this theorem in the appendix), then the approach is sound\n\n3) Background and related work are extensively reviewed"
                },
                "weaknesses": {
                    "value": "1) Generally speaking, the paper is hard to follow and the goals of the proposed method are unclear given the entire paper. The approach is motivated as to be for (time-varying) causal feature selection for a target variable. However, it is claimed in the paper that it can also be used for full causal discovery, but it's not clear the evaluation is for either causal feature selection or full causal discovery.\n\n2) The task performed in the evaluation section is not described at all. Presumably the task should be causal feature selection, but all the reader is told is the metric used for evaluation is AUC, which doesn't sound like we're evaluating the correct set of causal features. Furthermore, the methods used in the evaluation section do not appear to be causal feature selection methods and are different from the related methods mentioned earlier in the paper. \n\n3) Aside from the above confusion about the evaluation section, the empirical work is minimal in general and standard errors are not included."
                },
                "questions": {
                    "value": "1) Can the authors explain the evaluation task? Is it causal feature selection? What is the actual target that AUC is reported for?\n\n2) How is the time-varying nature accounted for in the evaluation?\n\n3) Why are the baselines in the evaluation section different from the methods mentioned in the introduction for the problem the method is proposed for? Has the approach been compared to the other causal feature selection methods mentioned earlier in the paper?\n\n4) Why are standard errors missing form the baselines? Is the improvement significant?\n\n5) Is there a limitation when extending the approach to the full causal discovery setting?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission231/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698687971778,
            "cdate": 1698687971778,
            "tmdate": 1699635948880,
            "mdate": 1699635948880,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eTEeHsj7Dy",
                "forum": "xbUlKe1iE8",
                "replyto": "6dYtuF2XzG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission231/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission231/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarification"
                    },
                    "comment": {
                        "value": "We appreciate the reviewer for evaluating our paper and acknowledging its novelty. We address each of your concerns in the following and look forward to further discussion if you have additional feedback. We are committed to making the necessary revisions to enhance the clarity, accessibility, and overall coherence of our writing. \n\n\n> Generally speaking, the paper is hard to follow and the goals of the proposed method are unclear given the entire paper. The approach is motivated as to be for (time-varying) causal feature selection for a target variable. However, it is claimed in the paper that it can also be used for full causal discovery, but it's not clear the evaluation is for either causal feature selection or full causal discovery.\n\nThank you for your comment! If you have specific suggestions on how to improve the write-up, we will gladly incorporate your suggestions in our manuscript. \n\nTo answer your question, in the experiments, we consider both causal feature selection and causal discovery. The reason why we consider both scenarios is to ensure a fair comparison with competitors. Note that causal feature selection for timeseries data is relatively underexplored. Hence, we compared our method with causal structure learning algorithms to showcase performance of our method with stronger evidence. In Section 6.1 we focus on causal discovery, whereas in Section 6.2 / Appendix J.1 we focus on causal feature selection. We are happy to make this point clearer in our submission.\n\n> The task performed in the evaluation section is not described at all. Presumably the task should be causal feature selection, but all the reader is told is the metric used for evaluation is AUC, which doesn't sound like we're evaluating the correct set of causal features. Furthermore, the methods used in the evaluation section do not appear to be causal feature selection methods and are different from the related methods mentioned earlier in the paper.\n\n\n\nYes, thank you for pointing it out! Our experimental framework essentially replicates previous related work. In our experimental section, we report on all state-of-the-art benchmarks for the DREAM3 datasets. Furthermore, the use of the AUC is consistent with previous related work [1] (state-of-the-art). There are several reasons for the usage of AUC other than other metrics like accuracy, F1 score, recall, etc, when comparing the performance of statistical algorithms that involve statistical tests. All of those metrics need a p-value thresholding which may lead to the following potential problems in the comparisons: vulnerability to class imbalance, task-specific threshold choices, model sensitivity to false positives and negatives, sensitivity to class probabilities, etc.\n\n\n\n> Aside from the above confusion about the evaluation section, the empirical work is minimal in general and standard errors are not included.\n\nYes, thank you for pointing it out. Our experimental framework considers a variety of baselines, and it is consistent with previous related work [1,2,3,4]. Throughout our experimental investigaion, we did not find that standard errors play a significant role, as it was also observed in related work [1]. Please inform us of other experiments/datasets/plots that you would think that our manuscript can benefit from, so we can incorporate your comments in our experiment section.\n\n> Can the authors explain the evaluation task? Is it causal feature selection? What is the actual target that AUC is reported for?\n\nIn the semi-synthetic experiment involving the Dream3 dataset (section 6.1, Appendix J.2), our objective is full causal discovery. Each of the five tasks within Dream3 involves observing the evolution of a system comprising 100 genes over time, subjected to 46 different perturbations. The ground truth causal graph delineating the relationships among these genes is provided. Out of the $100*99$ possible directed edges among the genes, some are indeed present, indicating direct causal influence, while others are not. This forms the actual target for AUROC calculations.\n\nIn the additional synthetic experiments (Appendix J.1), the focus shifts to causal feature selection. Here, the ground truth used in the AUROC metric is represented by the $\\Delta \\times m$ binary adjacency matrix $\\Sigma^{\\mathbf{Y}}$, where $\\Delta$ signifies the lag, and $m$ represents the number of potential causes.\""
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission231/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259001167,
                "cdate": 1700259001167,
                "tmdate": 1700259001167,
                "mdate": 1700259001167,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "osWSTAHiGc",
            "forum": "xbUlKe1iE8",
            "replyto": "xbUlKe1iE8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission231/Reviewer_fyvt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission231/Reviewer_fyvt"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes an algorithm to discover causal relationship using time series data. It is based on the double/debiased machine learning (DML) framework that has been popular in the recent literature. There are two main theoretical results: (I) Theorem 4.1 shows that under a set of axioms (A to C, in particular), true causality is equivalent to Granger causality, and (ii) Theorem 4.2 claims that under axioms A to D, Granger causality is equivalent to checking whether two expectations are the same or not. The algorithm called Structure Identification from Temporal Data (SITD) is given on page 7 and its numerical performance is illustrated using the Dream3 dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The research question addressed in the paper is of very high importance. As mentioned in the first paragraph on page 1, there are numerous scientific fields where causality questions need be addressed with time series data."
                },
                "weaknesses": {
                    "value": "1. The paper focuses on time series data but there is no statistical analysis focusing on time series data. For example, on page 7, it is stated that \"Under mild conditions on the convergence of $g_j^0$, $g_j^i$ and $\\alpha_j^0$, $\\alpha_j^i$, the quantity $\\theta^0 \u2212 \\theta^i$ has $\\sqrt{n}$-consistency\" and that \"We refer the reader to Chernozhukov et al. (2022; 2018) for a proof of the $\\sqrt{n}$-consistency for estimates as $\\theta^0$ and $\\theta^i$.\"  I do not think the cited references deal with time series data directly. It is disappointing that the paper does not provide any extensive treatment of time series analysis. \n\n2. Lemma A.1 claims that conditional mean independence in part 1 is equivalent to the conditional dependence in part 2. This seems mainly driven by Axiom (A) where the error $\\varepsilon$ is exogenous independent noise. I feel that this is a rather restricted setting. For example, suppose that Y is the time series of financial returns (e.g., S&P 500) and X is the causal factor that does not affect the conditional mean of returns but does affect the conditional variance of returns (typically called volatility in finance). It seems that the framework in the current paper excludes this kind of scenario. It is unclear to me what sense Axiom (A) is necessary; related to this point, Appendix A.3 is difficult to understand (see question 1 below)."
                },
                "questions": {
                    "value": "1. Appendix A.3 is difficult to understand. What are roles of $W_t$ and $Z_t$? $\\Sigma$ is not a positive definite matrix here and seems too irregular. Some further comments would be useful.\n\n2. The derivation on page 18 after \"We now prove the claim\" is difficult to follow. It seems to me that it is already assumed that $E[Y_ T | X_t^i = x, I_T^{\\backslash i} = i] = E[Y_T | X_t^i =x', I_T^{\\backslash i} = i]$ for any $x$ and $x'$ in the derivation; but I am not sure why. Does the current proof imply the if and only if result for equation (3)? Some clarifications would be helpful. \n\n3. I cannot follow why equation (4) is a good property. This indicates that the bias multiplied by $\\sqrt{n}$ goes to zero. It might be better to show that the root mean squared error multiplied by $\\sqrt{n}$ goes to zero as $n \\rightarrow \\infty$. Some explanations would be helpful.\n\n4. In the experiments on page 8, the area under the ROC curve (AUROC) is used as the performance metric. It would be beneficial why this metric is related to causality concerns."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission231/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission231/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission231/Reviewer_fyvt"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission231/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698790982246,
            "cdate": 1698790982246,
            "tmdate": 1699635948787,
            "mdate": 1699635948787,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "skpsbDZk4s",
                "forum": "xbUlKe1iE8",
                "replyto": "osWSTAHiGc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission231/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission231/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarification"
                    },
                    "comment": {
                        "value": "We express our gratitude for reviewer's interest and constructive feedback regarding the paper\u2019s clarity. We address your concerns in the following. We look forward to your further feedback to make sure the contributions of the paper are clarified clearly.\n\n> The paper focuses on time series data but there is no statistical analysis focusing on time series data. For example, on page 7, it is stated that \"Under mild conditions on the convergence of $g^0_j$, $g^i_j$, and $\\alpha_j^0, \\alpha_j^i$, the quantity $\\theta^0 - \\theta^i$ has $\\sqrt{{n}}$-consistency\" and that \"We refer the reader to Chernozhukov et al. (2022; 2018) for a proof of the \n$\\sqrt{{n}}$-consistency for estimates as $\\theta^0$ and $\\theta^i$.\" I do not think the cited references deal with time series data directly. It is disappointing that the paper does not provide any extensive treatment of time series analysis.\n\nPlease note that, even though the data has time series structure, but i.i.d. trajectories of the data are provided and each trajectory can be treated as a single draw from an i.i.d. distribution. Hence, the strong consistency of our method follows directly from Chernozhukov et al. (2022; 2018). In our paper, we only provide an informal discussion on this point, to avoid a complicated notation that would have made the paper difficult to read. However, we are happy to provide a formal statment and an in-depth discussion of this point, if required. It is important to note that double ML can be extended to handle data of stochastic nature (Markovian non-i.i.d.) by applying it iteratively in a backward order, starting from the end of the process and moving towards the beginning. At each time step, following the doubly robust estimation, residuals of causal effects are computed to update the auxiliary random variables from the preceding time step. It can be demonstrated that the resulting scores derived from these auxiliary random variables satisfy the Neyman Orthogonality condition. This way the Markovian structure of the stochastic process at each step is cancelled out. The algorithm then proceeds to the previous time step. For further details, please refer to [1]. A similar approach can be employed here with appropriate adjustments, but delving into those modifications is beyond the scope of the main message conveyed in this manuscript.\n\n> Lemma A.1 claims that conditional mean independence in part 1 is equivalent to the conditional dependence in part 2. This seems mainly driven by Axiom (A) where the error \n is exogenous independent noise. I feel that this is a rather restricted setting. For example, suppose that Y is the time series of financial returns (e.g., S&P 500) and X is the causal factor that does not affect the conditional mean of returns but does affect the conditional variance of returns (typically called volatility in finance). It seems that the framework in the current paper excludes this kind of scenario. It is unclear to me what sense Axiom (A) is necessary; related to this point, Appendix A.3 is difficult to understand (see question 1 below).\n \nAxiom (A) essentially requires that no confounding effect exists between the target variable Y and the causal parents. This assuption is essential for identifiability. As you correctly observe, this setting is restrictive. However, without any assumption we simply cannot get identifiability. Conditional mean independence is a necessary assumption in this setting. In other words, if this assumption fails, as in the example that you provide, than no method will provably be able to recover the true underlying causal structure due to identifiability issues. Please refer to appendix A.3 for an example of this identifiability problem without Axiom (A).\n\n>Appendix A.3 is difficult to understand. What are roles of $W_t$ and $Z_t$? $\\Sigma$ is not a positive definite matrix here and seems too irregular. Some further comments would be useful.\n\n$W_t$ and $Z_t$ are simply two time series, much like $X_t$ and $Y_t$. We use different notation, to highlight that the time series are generated differently. The matrix $\\Sigma$ is a 2x2 positive semi-definite matrix, with eigenvalues lambda=2 and lambda=0. We hope this clarifies the point?"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission231/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259455156,
                "cdate": 1700259455156,
                "tmdate": 1700259455156,
                "mdate": 1700259455156,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RPU4Aqc0kZ",
                "forum": "xbUlKe1iE8",
                "replyto": "Lne9KH8drG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission231/Reviewer_fyvt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission231/Reviewer_fyvt"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the author(s) for providing detailed responses. However, my concerns still remain at large. First, it would be very demanding to restrict the scope to i.i.d. data in the context of time series/temporal data. Adding Markov or other types of time series assumptions would require a substantial additional analysis. Second, to my reading, equation (4) is a good property only in the sense that the __expected__ error (that is, the __mean__ of the error) converges to zero; however, its variance may not necessarily go to zero."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission231/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670838053,
                "cdate": 1700670838053,
                "tmdate": 1700670838053,
                "mdate": 1700670838053,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hEibsefIEZ",
                "forum": "xbUlKe1iE8",
                "replyto": "JkwwJIVH9g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission231/Reviewer_fyvt"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission231/Reviewer_fyvt"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your quick responses. For the first response, perhaps I might be mistaken regarding the sampling structure. More importantly, my original comment simply points out that the current paper does not have fully established the desired statistical results; it is unclear to me whether they can be obtained easily from Chernozhukov et al. (2022; 2018). For the second response, I agree that if you have asymptotic normality as you described, you will have $\\sqrt{n}$-consistency. As a side remark, strong consistency typically refers to the situation where a sequence of estimators converges almost surely to the true value of the parameter. I find that your use of strong consistency seems different from this convention. In short, I can see that there are many positive parts in the paper but it seems that the paper may need further careful revision."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission231/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678694883,
                "cdate": 1700678694883,
                "tmdate": 1700678694883,
                "mdate": 1700678694883,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gExjy8cpko",
            "forum": "xbUlKe1iE8",
            "replyto": "xbUlKe1iE8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission231/Reviewer_95xj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission231/Reviewer_95xj"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a doubly robust structure identification method for temporal data that can identify the direct causes of the target variable assuming additive noises, even in the presence of cyclic structures and in the absence of faithfulness or causal sufficiency."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors offer a discussion connecting Granger's causality with Pearl's framework, which is thought-provoking.\n2. The authors propose an algorithm based on a parameter estimation framework, namely DoubleML, to detect the causes of the target variable. \n3. The literature review is comprehensive.\n4. The authors conduct extensive experiments on semi-synthetic and synthetic datasets, compared with several baselines."
                },
                "weaknesses": {
                    "value": "1. In the contribution, the authors claim that the proposed algorithm can be used for full causal discovery under some assumptions. The related discussion in section 5.2 is limited without details.\n2. In principle, the approach adheres to steps (1) through (4) in section 4.2, yet the practical algorithm has been adjusted to account for the time-consuming nature of \"large instances.\" While the approach outline aligns with the proven theorem, a gap exists between the outlined approach and the modified algorithm. Is it feasible to implement the approach strictly in smaller instances, adhering to steps (1) through (4)? Furthermore, what does \"large instances\" imply in this context?\n3. There is no real-world application provided in the paper.\n4. Regarding the baselines, from my understanding, some of them are designed for full causal discovery, encompassing the detection of causes for target variables and beyond. In contrast, the proposed algorithm primarily focuses on feature detection. In the experiment section, are there any specific modifications necessary to ensure a fair comparison?"
                },
                "questions": {
                    "value": "1. Can you please provide a brief explanation of the role played by the causal graph in the proposed algorithm? Personally, I am under the impression that the causal graph is unrelated to the proposed method, rendering the faithfulness assumption, cyclic structure, and causal sufficiency irrelevant to the algorithm. Thus, I do not consider the relaxation of this assumption as an advantage of the method, as it falls outside the scope of the algorithm. Please correct me if I missed the point.\n2. In the first equation on page 4, what is $N$?\n3. In equation 3, what is $n$? Should it be $k$?\n4. I felt lost that in equation 3, $g^0_0$ and $g^i_0$ equal to the same conditional expectation as $\\alpha^0_0$ and $\\alpha_0^i$ in the second point in section 5.1. Are they the same things?\n5. As the appendix states, $k$ ranges from 3 to 7. What is the value of $k$ used in each experiment? Is the algorithm output sensitive to the value of $k$?\n6. The term \"trajectories\" means the time series, correct? In Fig.2, what does $N_feat$ represent? Is $N_feat$ indicative of the number of trajectories? Additionally, in Fig.3, all the algorithms exhibit improved performance with an increase in the number of trajectories. Could you provide a brief explanation for this trend? Moreover, why does Fig.3 depict the performance in low-sample regimes, and how are \"low-sample regimes\" reflected in the Fig.3?\n7. Is there a specific reason for using only one baseline algorithm in the experiments presented in the appendix?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission231/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission231/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission231/Reviewer_95xj"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission231/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698803895720,
            "cdate": 1698803895720,
            "tmdate": 1699635948714,
            "mdate": 1699635948714,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "uI0A6HHr9E",
                "forum": "xbUlKe1iE8",
                "replyto": "gExjy8cpko",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission231/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission231/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarification"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the reviewer\u2019s recognition regarding the significance of the contributions made by this research in the challenging domain of causal structure learning for time-series under highly general assumptions.\n\n> In the contribution, the authors claim that the proposed algorithm can be used for full causal discovery under some assumptions. The related discussion in section 5.2 is limited without details.\n\nThanks a lot for point this out! This simple reduction was not included in the interest of space and it goes as the following:\n\nIf the underlying causal structure doesn't have hidden confounders (noted as \"fully-observed\" in the manuscript) and doesn't contain any cycles (noted as \"acyclic\" in the manuscript), then we can iteratively select each node of the causal graph as our target variable and apply our causal feature selction algorithm to find its direct causes. These two conditions are necessary for the Axiom (A) to be true when any node of the causal graph could be selected as the target variable. In the end, the whole causal structure will be discovered. \n\nWe will make sure to clarify further in the final version of the manuscript.\n\n> In principle, the approach adheres to steps (1) through (4) in section 4.2, yet the practical algorithm has been adjusted to account for the time-consuming nature of \"large instances.\" While the approach outline aligns with the proven theorem, a gap exists between the outlined approach and the modified algorithm. Is it feasible to implement the approach strictly in smaller instances, adhering to steps (1) through (4)? Furthermore, what does \"large instances\" imply in this context? \n\nThanks a lot for your precise and detailed question! When we mention \"large instances\", we are specifically referring to cases where the number of nodes, denoted as $m$, is substantial.  The principled approach necessitates $\\mathcal{O}(m)$ regressions, which is linear in the number of nodes. However, by incorporating the zero-masking heuristic, we can optimize the process, reducing the required regressions to just $\\mathcal{O}(1)$. This proves particularly advantageous in situations where computational resources are limited, but the causal graph is huge. It's essential to highlight that, as demonstrated in Appendix A.7, there is no discernible difference between the principled approach and the one utilizing the heuristic when the underlying estimator is linear regression.\n\n> There is no real-world application provided in the paper.\n\nIndeed, thank you for highlighting this consideration. In previous related works [2,3,4,5], Dream3 was employed as a gold-standard benchmark which is of systems' biology nature. However, we acknowledge that over-tuning causal discovery algorithms exclusively for a single dataset could present potential issues. As you aptly observe, other datasets might be pertinent in this context. We are open to the inclusion of any additional datasets you may suggest.\n\n> Regarding the baselines, from my understanding, some of them are designed for full causal discovery, encompassing the detection of causes for target variables and beyond. In contrast, the proposed algorithm primarily focuses on feature detection. In the experiment section, are there any specific modifications necessary to ensure a fair comparison?\n\nThere are no modifications necessary to ensure fair comparison. The reason is that our algorithm naturally extends to full causal discovery, without any modifications, by simply treating each variable as a target. The only subtle issue that could arise is the following: since a two-sample Student\u2019s t-test is used to determine whether a particular covariate is a causal parent of the target, then a multiple hypothesis testing issue could arise, because we apply this procedure for every pair of variables. However, we do not face such a problem here because, according to the standard practice, the AUROC metric is used for evaluation in the Dream 3 benchmark. For this metric, we simply sort the predictions about existence/absence of an edge; that is, only the relative order matters."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission231/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700259698850,
                "cdate": 1700259698850,
                "tmdate": 1700259698850,
                "mdate": 1700259698850,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IfRXtEDg2m",
                "forum": "xbUlKe1iE8",
                "replyto": "gExjy8cpko",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission231/Reviewer_95xj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission231/Reviewer_95xj"
                ],
                "content": {
                    "comment": {
                        "value": "\"In equation 3, what is n? Should it be k?\" Sorry that I made a mistake, and it should be equation 4.\n\nThank you for the detailed clarification; my questions have been well answered."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission231/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677036008,
                "cdate": 1700677036008,
                "tmdate": 1700677122411,
                "mdate": 1700677122411,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AMruTR7Uwk",
                "forum": "xbUlKe1iE8",
                "replyto": "YVuLMSt10z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission231/Reviewer_95xj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission231/Reviewer_95xj"
                ],
                "content": {
                    "comment": {
                        "value": "Q3 can be described in a more precise manner as follows. $\\theta^0-\\theta^i$ does not incorporate $n$. How might one obtain the distribution of $\\theta^0-\\theta^i$ concerning convergence as $n$ approaches infinity, considering the absence of $n$ within the expression?"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission231/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700679190872,
                "cdate": 1700679190872,
                "tmdate": 1700679190872,
                "mdate": 1700679190872,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ko9TPFiy61",
                "forum": "xbUlKe1iE8",
                "replyto": "gExjy8cpko",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission231/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission231/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to reviewer"
                    },
                    "comment": {
                        "value": "Thank you for pointing this out. Indeed, $\\theta^0$ and $\\theta^i$ are sample estimates of the true parameters $\\theta^0_0 = \\mathbb{E}[Y_T \\cdot g^0_0 ]$ and $\\theta^i_0 = \\mathbb{E}[Y_T \\cdot g^i_0 ]$ respectively and  of course their distribution depends on $n$. The only reason we did not make this dependence explicit in the notation was simplicity, in order not to overload variable symbols with too many subscripts and superscripts."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission231/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737168891,
                "cdate": 1700737168891,
                "tmdate": 1700741265996,
                "mdate": 1700741265996,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "E7wSb690yz",
            "forum": "xbUlKe1iE8",
            "replyto": "xbUlKe1iE8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission231/Reviewer_uCS8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission231/Reviewer_uCS8"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an algorithm for doubly robust structure identification for Granger causality. It also provides asymptotical guarantees that the proposed method can discover the direct causes even when there are cycles or hidden confounding and that the algorithm has $\\sqrt(n)$-consistency."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed doubly robust structure identification for Granger causality is novel, as far as I know. The paper also provides identifiability guarantees in the presence of cycles or hidden confoundings."
                },
                "weaknesses": {
                    "value": "The paper did not analyze or give an intuition why the proposed method allows the existence of cycles or hidden confoundings."
                },
                "questions": {
                    "value": "Why does the proposed method allow the existence of cycles or hidden confounding?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission231/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698816745468,
            "cdate": 1698816745468,
            "tmdate": 1699635948638,
            "mdate": 1699635948638,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vYBNgHlUMY",
                "forum": "xbUlKe1iE8",
                "replyto": "E7wSb690yz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission231/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission231/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarification"
                    },
                    "comment": {
                        "value": "Thank you very much for taking the time to review our work and recognize its merits!\n\n> The paper did not analyze or give an intuition why the proposed method allows the existence of cycles or hidden confoundings. Why does the proposed method allow the existence of cycles or hidden confounding?\n\nTo give an intuition of why we can handle hidden confounding and cycles, denote with $Y$ the target time series and with $\\\\{ X^1, \\dots, X^n \\\\}$ the observed time series that are potential causes for $Y$. Using Axioms $A$-$C$, we prove that in this case, performing an intervention on all the variables  $\\\\{ X^1, \\dots, X^n \\\\}$ is equivalent to conditioning (this fact is specific to our causal model and it is not true in general). This type of conditioning \"cancels out\" the effect of cycles or hidden confounding on $\\\\{ X^1, \\dots, X^n \\\\}$."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission231/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700260312035,
                "cdate": 1700260312035,
                "tmdate": 1700260312035,
                "mdate": 1700260312035,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uos0Fy14gW",
            "forum": "xbUlKe1iE8",
            "replyto": "xbUlKe1iE8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission231/Reviewer_WUkY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission231/Reviewer_WUkY"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a novel and efficient Doubly Robust Structure Identification from Temporal Data (SITD) algorithm, offering theoretical guarantees including $\\sqrt{n}$-consistency. It establishes a technical connection between Granger causality and Pearl's time series framework, outlining the conditions under which the approach is suitable for feature selection and full causal discovery. The paper's theoretical insights highlight the algorithm's ability to handle non-linear cyclic structures and hidden confounders, even without relying on faithfulness or causal sufficiency. In extensive experiments, the approach demonstrates remarkable robustness, speed, and performance compared to state-of-the-art methods, making it a valuable contribution to causal discovery in various applications."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- They've introduced a doubly robust structure identification method for analyzing temporal data. It doesn't rely on strict faithfulness and causal sufficiency assumptions, making it versatile enough to handle general non-linear cyclic structures and hidden confounders.\n\n- The innovative application of the double machine learning framework to Granger causality is a significant contribution.\n\n- The paper is well-structured, maintaining a coherent and easily-followed flow from beginning to end.\n\n- The paper extensively references related work, offering a comprehensive overview of prior research that not only provides valuable context for the study but also underscores the authors' profound understanding of the field."
                },
                "weaknesses": {
                    "value": "- Regarding the \"stationary causal relation\" assumption, you mentioned that the results could potentially apply to models that do not meet this axiom. Have you formally demonstrated this claim in any specific section, or are you implying that the proof of Theorem 4.1 does not rely on this assumption?"
                },
                "questions": {
                    "value": "- How do you identify cyclic structures? Does Algorithm 1 have the capability to detect cyclic structures, and does this imply the presence of confounders?\n\n- In your method, is the time lag $k$ fixed, or does it remain stationary but vary among different variables?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission231/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission231/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission231/Reviewer_WUkY"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission231/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698838335959,
            "cdate": 1698838335959,
            "tmdate": 1699635948567,
            "mdate": 1699635948567,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "m311eAOE66",
                "forum": "xbUlKe1iE8",
                "replyto": "uos0Fy14gW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission231/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission231/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Clarification"
                    },
                    "comment": {
                        "value": "Thank you for enumerating the strengths of our work. We address each of your concerns in the following and look forward to further discussion if you have additional feedback.\n\n> Regarding the \"stationary causal relation\" assumption, you mentioned that the results could potentially apply to models that do not meet this axiom. Have you formally demonstrated this claim in any specific section, or are you implying that the proof of Theorem 4.1 does not rely on this assumption?\n\nUnder the stationarity assumption, we can test direct causal effects, by focusing on a target $Y_T$ for a fixed time T. If stationarity does not hold, then we will have to perform the test on $Y_t$, for all time steps t. This fact follows directly from our proof.\n\n> How do you identify cyclic structures? Does Algorithm 1 have the capability to detect cyclic structures, and does this imply the presence of confounders?\n\nOur algoirthm only identifies direct causal effects on a target variable of interest. It does not identify cycilc structure. However, as detailed in Axioms (A)-(B), our algorithm works even under confounding or cycles among the covariates. In other words, even if hidden confounders and cycles are present, we are still able to uncover the direct causes.\n\n> In your method, is the time lag $k$ fixed, or does it remain stationary but vary among different variables?\n\nIn general, we only assume stationarity, and $k$ may vary among different variables. However, in our experimental framework we set $k$ to always be the same for simplicity. Specifically, in all our experiments, time lag is equal to 2. This is quite a standard value accross the literature [1,2,3,4].\n\n\n[1] Gong, Wenbo, Joel Jennings, Cheng Zhang, and Nick Pawlowski. \u201cRhino: Deep Causal Temporal Relationship Learning with History-dependent Noise.\u201d In The Eleventh International Conference on Learning Representations. 2022.\n\n[2] Saurabh Khanna and Vincent YF Tan. Economy statistical recurrent units for inferring nonlinear granger causality. arXiv preprint arXiv:1911.09879, 2019.\n\n[3] Roxana Pamfil, Nisara Sriwattanaworachai, Shaan Desai, Philip Pilgerstorfer, Konstantinos Geor- gatzis, Paul Beaumont, and Bryon Aragam. Dynotears: Structure learning from time-series data. In International Conference on Artificial Intelligence and Statistics, pp. 1595\u20131605. PMLR, 2020.\n\n[4] Alex Tank, Ian Covert, Nicholas Foti, Ali Shojaie, and Emily Fox. Neural granger causality for nonlinear time series. stat, 1050:16, 2018."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission231/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700260379055,
                "cdate": 1700260379055,
                "tmdate": 1700260379055,
                "mdate": 1700260379055,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]