[
    {
        "title": "PF-LRM: Pose-Free Large Reconstruction Model for Joint Pose and Shape Prediction"
    },
    {
        "review": {
            "id": "mhgl0z4oWL",
            "forum": "noe76eRcPC",
            "replyto": "noe76eRcPC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission750/Reviewer_XwFE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission750/Reviewer_XwFE"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel method for lifting a few unposed images directly into a 3D representation as well as estimating the corresponding camera poses. The key contributions are a novel object-centric pose estimation pipeline based on the Perspective-n-Point (PnP) algorithm, scaling up training to achieve strong cross-dataset generalization, and instant estimation of the underlying NeRF (as opposed to training offline via gradient descent). The authors demonstrate impressive results on object-centric scenes across datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-The motivation for the method is clear - reconstructing object-centric 3D scenes without knowledge of the camera poses.\n-The method is simple (which is a good thing) and straightforwardly scalable.\n-The approach for camera pose estimation is neat, containing an attractive 3D inductive bias and mostly outperforms common object-centric pose estimation methods.\n-Generated results are strong and aesthetically pleasing, with significant cross-dataset generalization demonstrated.\n-Ablations are informative."
                },
                "weaknesses": {
                    "value": "-I have a suspicion that the baselines - RelPose++ and Forge - are not trained on the same datasets, but that the authors are rather using the pre-trained methods and just evaluate them on these datasets. Could the authors clarify?\n- One baseline is very clearly missing - the Scene Representation Transformer, Sajjadi et al, which can similarly perform novel view synthesis from unposed images at test time while requiring camera poses at training time. \n- The prior point is even more critical since the authors don't always outperform RelPose++ on Co3D. If RelPose++ wasn't trained at a similar scale, this is concerning.\n- In the evaluation section, the authors state that they evaluate PSNR against the input images. That is questionable - they should evaluate the PSNR against held-out test views?\n- The authors should clarify - already in the introduction and the abstract - that their method still requires ground-truth poses at training time. I.e., it is only pose-free at test time!\n- Why do the authors choose to report results on Co3D in the appendix as opposed to the main paper? I don't see a principled difference here. Just as the other results, this result should be reported in the main paper. The fact that their method requires training on an additional real-world dataset is important and relevant and should not be hidden in the appendix.\n- The authors should demonstrate their method on some real-world captures instead on only synthetic examples.\n\nRelevant references missed:\n- Triplanes for neural fields were first proposed in \"Convolutional Occupancy Networks\", Peng et al. and should be discussed.\n- The paper \"FlowCam\" also incorporates 3D shape information during the camera pose prediction process and should be discussed.\n\nMinor concerns:\n- In all the tables, the best-performing method should be bolded.\n- \"output-performing\" --> outperforming (last paragraph before related work)\n- I would recommend to avoid phrases like \u201cpretty well\u201d - sounds a bit casual."
                },
                "questions": {
                    "value": "See weaknesses - in summary, I think this paper could be strong, but if the authors want to claim that their method outperforms prior pose-estimation methods, then they have to provide a fair benchmark and train these other methods on the same datasets and with a comparable level of compute as well, or at least provide a clear ablation study where they uncover that their model scales better with data than prior methods.\n\nIf the authors address this issue, I'm happy to raise my score significantly."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission750/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission750/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission750/Reviewer_XwFE"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission750/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698617823066,
            "cdate": 1698617823066,
            "tmdate": 1700665721245,
            "mdate": 1700665721245,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9jEHGYCXUZ",
                "forum": "noe76eRcPC",
                "replyto": "mhgl0z4oWL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission750/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission750/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewers for the valuable feedback. We will include the missing references and fix typos in a revision. We address your major concerns below.\n\n**Evaluating RelPose++ and Forge**: we used the pretrained checkpoints from RelPose++ and Forge to evaluate their cross-dataset generalization. We would like to emphasize that we focus on cross-dataset generalization in this work as it is vital for practical applications; our model was not trained on any of the evaluation datasets either. Moreover, we would like to highlight that we also reconstruct NeRF while RelPose++ doesn\u2019t. This joint prediction of 3D and pose is very helpful for boosting pose prediction accuracy in our experiments, as shown in Tab. 4. \n\n**Missing Scene representation transformer (SRT) baseline**:  In our humble opinion, we can not consider SRT as a fair baseline to be compared, since there\u2019s a clear distinction in objectives and scopes between our work and SRT. PF-LRM targets 3D reconstruction and pose estimation, while SRT only focuses on novel view synthesis. It means that SRT does not estimate poses for input images which is one of the main goals of this paper and densely measured. Poses are important for practical applications like robotic navigation, etc. Also, geometry can be naturally extracted from our reconstructed NeRF, while SRT is a geometry-free method that does not directly predict geometry. However, we thank the reviewer for bringing this work to our attention, we will cite SRT and discuss it in a revision. \n\n**Input-view PSNR**: We will include novel-view psnr in a revised draft before the rebuttal deadline.  The current input-view psnr is to show that our predictions (both NeRF and poses) are well-aligned with the input images, as we render our predicted NeRF using our predicted poses (in Fig. 3); we would like to highlight that we do well in this important objective of matching re-renderings with inputs for joint 3D and pose prediction, while baselines struggle with this goal.  We also show extracted geometry in Fig. 3, 4 and interactive NeRF viewers on our project page to demonstrate that we did not cheat by predicting degenerate solutions. In Fig. 4, we show re-rendering of our reconstructed NeRF using ground-truth pose, to demonstrate that removing pose prediction from our model leads to lower reconstruction quality. This is to highlight the synergy between NeRF reconstruction and pose estimation, which was not shown in prior work like SRT. \n\n**Comparison with RelPose++ on Co3D**: as shown in Tab. 8 of the appendix, we outperform RelPose++ on the majority of the Co3D categories, even though our model is not trained on Co3D while RelPose++ was trained on it. As to why we are worse on certain categories, we attribute this to the noisy in-accurate masks provided by Co3D, which we used to remove background before running our method. A clear performance drop can also be seen in Tab. 8 when we test RelPose++ on such background-free images. We think handling background in our framework is an interesting future direction. \n\n**Move Co3D results to the main paper**: We think it\u2019s a good idea to move Co3D results to the main paper. By the time of submission, we did not have time and resources to train our model to full convergence on a mix of Objavese and MvImgNet data; hence we put the non-convergent model\u2019s results in the appendix. We will include the results of our convergent model in the main paper in a revision.\n\n**Missing real-data results in the paper**: We respectfully disagree with this claim. We have shown our model\u2019s results on the real DTU dataset (Fig 5 and Tab 9 in appendix), OmniObject3D dataset (Tab 1), Co3D dataset (Tab 8 in appendix). Please refer to our main paper and appendix for details."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699846443863,
                "cdate": 1699846443863,
                "tmdate": 1699846443863,
                "mdate": 1699846443863,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HH9Glybycm",
                "forum": "noe76eRcPC",
                "replyto": "mhgl0z4oWL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission750/Reviewer_XwFE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission750/Reviewer_XwFE"
                ],
                "content": {
                    "title": {
                        "value": "Reply"
                    },
                    "comment": {
                        "value": "Dear Authors,\n\nthank you for the kind and detailed reply.\n\n## Comparison to RelPose\n\nI do not think that your argument for evaluating with the pre-trained checkpoints for RelPose is valid. If I understand correctly, you are basically saying \"we are evaluating cross-dataset generalization, therefore, it is reasonable to evaluate the pre-trained checkpoint for RelPose.\". I don't think that is a valid argument at all. You are *claiming* that your *method* outperforms RelPose on pose prediction, as you literally say in your abstract: \n\n\"When trained on a huge amount of multiview data, PF-LRM shows strong cross-dataset generalization ability, and outperforms baseline methods by a large margin in terms of pose prediction accuracy and 3D reconstruction quality on various evaluation datasets\". \n\nIf you want to claim that you outperform either of these methods on pose estimation accuracy, then you have two options: 1) you train your method on the *same* dataset as RelPose and then show that you are doing better or 2) you are training RelPose on the same dataset that you used for training and then show that you do better on the corresponding test set.\n\nOtherwise, you have not supported your claim. In other words, you would be putting out a paper that might lead many scientists down the wrong path: You are suggesting that anyone who is doing pose estimation ought to use your method, when in practice, it might well be that RelPose would in fact outperform your method when trained on the same dataset. \n\nIMO, it is standard procedure in our field to train the baseline on the same dataset as the proposed method to allow for a fair comparison. \n\n## Other points\nThank you for clarifying the other points, they clarified my other questions!"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700443950538,
                "cdate": 1700443950538,
                "tmdate": 1700512285164,
                "mdate": 1700512285164,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wpg436BkW6",
                "forum": "noe76eRcPC",
                "replyto": "HH9Glybycm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission750/Reviewer_XwFE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission750/Reviewer_XwFE"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up"
                    },
                    "comment": {
                        "value": "I have conferred with the other reviewers and got their opinion on this topic, and they similarly think that the comparison, as done, does not support the claims of the paper.\n\nHowever, I also think that this should not be in the way of this paper, which I think all the reviewers agree is good & novel!\n\nHowever, I would ask the authors to add several disclaimers to their paper about the RelPose comparison. Specifically, I would ask the authors to (1) add a \"Limitations\" section and (2) add a sentence to the experiments section and (3) add a sentence to the introduction section where they respectively point out that RelPose++ or Forge might perform better if trained on a larger-scale dataset, that this has not been attempted in this paper, and that hence, it is not clear whether the performance stems from a methods improvement or from the large-scale training, and that that is left to future work.\n\nOther than that, I have increased my score to \"5\", since I believe this paper is good and warrants publication. I will increase my score to \"7\" if the authors are willing to address this shortcoming, and will happily argue for acceptance."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700623928907,
                "cdate": 1700623928907,
                "tmdate": 1700623928907,
                "mdate": 1700623928907,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hMwClt2i9C",
                "forum": "noe76eRcPC",
                "replyto": "mhgl0z4oWL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission750/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission750/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "We sincerely understand your concerns of fair comparison with baselines. We spent the rebuttal period scaling up the training of the baseline RelPose++ on the Objaverse dataset, and report the performance of the re-trained baseline in the following table. We can see that our method (trained on exactly the same data) still outperforms the retrained baseline by a wide margin, demonstrating the superiority of our proposed method. We also attempted to scale up the training of baseline FORGE, but its complex multi-stage training (6 stages) seems non-trivial to scale up in our trials. We leave it to future work to investigate the scalability issue of FORGE.\n\n---\n\nOmniObject3D\n\n| Method                                \t| R. error | Acc.@15   | Acc.@30   | T. error  |\n|-------------------------------------------|----------|-----------|-----------|-----------|\n| RelPose++ (w/o bg, pretrained)        \t| 69.22\t| 0.070 \t| 0.273 \t| 0.712 \t|\n| RelPose++ (w/o bg, trained on Objaverse)  | 58.67\t| 0.304 \t| 0.482 \t| 0.556 \t|\n| Ours (S)                              \t| 15.06\t| 0.695 \t| 0.910 \t| 0.162 \t|\n| Ours (L)                              \t| **7.25** | **0.958** | **0.976** | **0.075** |\n\n---\n\nGSO\n\n| Method                                \t| R. error | Acc.@15  | Acc.@30  | T. error  |\n|-------------------------------------------|----------|----------|----------|-----------|\n| RelPose++ (w/o bg, pretrained)        \t| 107.49   | 0.037\t| 0.098\t| 1.143 \t|\n| RelPose++ (w/o bg, trained on Objaverse)  | 45.58\t| 0.600\t| 0.686\t| 0.407 \t|\n| Ours (S)                              \t| 13.08\t| 0.848\t| 0.916\t| 0.135 \t|\n| Ours (L)                              \t| **2.46** | **0.976**| **0.985**| **0.026** |\n\n---\n\nABO\n\n| Method                                \t| R. error  | Acc.@15   | Acc.@30   | T. error  |\n|-------------------------------------------|-----------|-----------|-----------|-----------|\n| RelPose++ (w/o bg, pretrained)        \t| 102.30\t| 0.060 \t| 0.144 \t| 1.103 \t|\n| RelPose++ (w/o bg, trained on Objaverse)  | 45.39 \t| 0.693 \t| 0.708 \t| 0.395 \t|\n| Ours (S)                              \t| 26.31 \t| 0.785 \t| 0.822 \t| 0.249 \t|\n| Ours (L)                              \t| **13.99** | **0.883** | **0.892** | **0.131** |\n\n---\n\nUpdated draft: we have updated (in-place) the abstract, introduction, related work, conclusion sections. We\u2019ve mentioned in the updated abstract that we used multi-view posed data for training. We\u2019ve cited SRT paper and discussed the scope difference between our work and SRT. We also added citations to the Convolutional Occupancy Networks paper and FlowCam paper, as the reviewer suggested. For the experiment section, we did not modify it in-place because our modifications will change the figure/table numbers, hence increasing the reading difficulty for the reviewers. Instead, we put the updated experiment section in Appendix B. We plan to integrate it upon acceptance. We are also happy to integrate it now, if the reviewers don\u2019t think this increases reading burden. You can find our novel-view PSNR in Tab. 14 and our novel-view renderings (along with ground-truth) in Fig. 9. We also include a detailed limitation section in Appendix C."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641641252,
                "cdate": 1700641641252,
                "tmdate": 1700642788957,
                "mdate": 1700642788957,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hxlzqg9zyV",
                "forum": "noe76eRcPC",
                "replyto": "hMwClt2i9C",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission750/Reviewer_XwFE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission750/Reviewer_XwFE"
                ],
                "content": {
                    "title": {
                        "value": "Reply"
                    },
                    "comment": {
                        "value": "This is great - thank you very much for being receptive to this feedback, I think that these experiments serve as a great reference and really support your claim!\n\nI increased my score to 8!"
                    }
                },
                "number": 30,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665709488,
                "cdate": 1700665709488,
                "tmdate": 1700665709488,
                "mdate": 1700665709488,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ogPyPJsh0D",
            "forum": "noe76eRcPC",
            "replyto": "noe76eRcPC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission750/Reviewer_T3us"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission750/Reviewer_T3us"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a 3D reconstruction method that focuses on shape and pose predictions given a sparse set of un-posed input images. The proposed method consists of two major components: 1) a transformer that aggregates DINO-based image tokens from multi-view images and 2) a DSAC-fashion pose estimator that predicts 3D points coordinates from aggregated features followed by a differentiable PnP refinement. While aggregating multi-view features, the transformer also enables shape reconstruction by predicting NeRF parameters in terms of Triplane features."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The paper is generally well written and easy to follow.\n2. The method looks convincing and the paper is addressing a critical challenge in this field.\n3. The anonymous website provides extra demos."
                },
                "weaknesses": {
                    "value": "1. In Figure 3 and Figure 4, I am confused about why the results are comparing input images with their predictions? Shouldn\u2019t it be comparing GT novel view images with rendered novel views, as in Figure 5? Given input images, it\u2019s not surprising that the reconstruction of input images should be almost perfect right?\n2. The first issue leads to further confusion when reading Figure 4. I am not sure I understand correctly, but it seems like pose prediction makes limited difference.\n3. Missing related works. Section 3.3 is about pose predictions and PnP. This approach is closely related to the line of work in *scene coordinate regression* [a][b], which is not mentioned at all in the related work section.\n\n**Reference**\n\n[a] Shotton, Jamie, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, and Andrew Fitzgibbon. \"Scene coordinate regression forests for camera relocalization in RGB-D images.\" In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2930-2937. 2013.\n\n[b] Brachmann, Eric, Alexander Krull, Sebastian Nowozin, Jamie Shotton, Frank Michel, Stefan Gumhold, and Carsten Rother. \"Dsac-differentiable ransac for camera localization.\" In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6684-6692. 2017."
                },
                "questions": {
                    "value": "Refer to points 1 and 2 in the \"Weaknesses\" section.\n\nWhile the paper presents a novel idea and convincing quantitative results, these two points create confusions and prevent me from making a definitive evaluation. Once these issues are resolved, I am willing to increase my evaluation to accept."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission750/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission750/Reviewer_T3us",
                        "ICLR.cc/2024/Conference/Submission750/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission750/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698707548668,
            "cdate": 1698707548668,
            "tmdate": 1700644592432,
            "mdate": 1700644592432,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "iAnrd5sASX",
                "forum": "noe76eRcPC",
                "replyto": "ogPyPJsh0D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission750/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission750/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the valuable feedback. We will add the suggested references ([a],[b]) in a revised draft soon before the rebuttal deadline. Please find our answers to your questions below. \n\n**Input-view PSNR**: We will include novel-view psnr in a revised draft before the rebuttal deadline.  The current input-view PSNR is to show that our predictions (both NeRF and poses) are well-aligned with the input images, as we render our predicted NeRF using our predicted poses; we would like to highlight that we do well in this important objective of matching re-renderings with inputs for joint 3D and pose prediction, while baselines struggle with this goal. We also show extracted geometry in Fig. 3, 4 and interactive NeRF viewers on our project page to demonstrate that we did not cheat by predicting degenerate solutions."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699846313288,
                "cdate": 1699846313288,
                "tmdate": 1699846313288,
                "mdate": 1699846313288,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "FicwxacUyh",
                "forum": "noe76eRcPC",
                "replyto": "iAnrd5sASX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission750/Reviewer_T3us"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission750/Reviewer_T3us"
                ],
                "content": {
                    "title": {
                        "value": "No further questions"
                    },
                    "comment": {
                        "value": "Dear authors, \n\nThanks for the detailed response. I don't have further questions about the content, but it seems like there is no revised draft updated at this moment?\n\nBest,\nT3us"
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700524888723,
                "cdate": 1700524888723,
                "tmdate": 1700524888723,
                "mdate": 1700524888723,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "85heNSjFSG",
                "forum": "noe76eRcPC",
                "replyto": "sGJafOvH7G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission750/Reviewer_T3us"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission750/Reviewer_T3us"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for updating the draft with the new tables and figures. These results have largely addressed my concerns. I have updated my rating to accept."
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644612944,
                "cdate": 1700644612944,
                "tmdate": 1700644612944,
                "mdate": 1700644612944,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SvU7Rxy9t9",
            "forum": "noe76eRcPC",
            "replyto": "noe76eRcPC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission750/Reviewer_Pk6q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission750/Reviewer_Pk6q"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method to reconstruct 3d models from unposed sparse-view images. To enable this goal, the authors use a single-stream transformer to simultaneously process image patch tokens and nerf tokens, resulting in a simultaneously estimation of triplane-nerf and camera poses. The estimation of camera poses utilizes predicted per-view coarse geometry and differentiable pnp. This method shows strong cross-dataset generalization ability and outperforms baselines on various datasets."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. The authors conducted rich experiments to prove the effectiveness of their method. \n2. Reconstructing 3d models from sparse-view images is an important topic in the AIGC era. \n3. Employing differentiable pnp into nerf reconstruction is a good idea to handle camera pose uncentainty."
                },
                "weaknesses": {
                    "value": "The authors mainly handled the cases of 2-4 views (e.g. Figure 1). Is there any criteria for the authors to select image views? It seems that the tested camera views have suitable (or informative) relative angles (about 30 degrees to about 120 degrees). What would happen if extreme/degenerated views are provided (e.g. the front and back views that are parallel, or views with small angles, or views with translational displacement only)?   I think explaing this would make the study stronger."
                },
                "questions": {
                    "value": "No further questions. I think the paper provides enough  details."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission750/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission750/Reviewer_Pk6q",
                        "ICLR.cc/2024/Conference/Submission750/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission750/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698742299085,
            "cdate": 1698742299085,
            "tmdate": 1700651403476,
            "mdate": 1700651403476,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "L8Ov5LCZtF",
                "forum": "noe76eRcPC",
                "replyto": "SvU7Rxy9t9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission750/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission750/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the positive feedback about our empirical evaluation, technical soundness, and potential impact on the 3D generative AI. We address your questions below. \n\n**View selection**: For the evaluation on ABO and GSO dataset, we randomly select 4 sparse views such that viewing angles between any two views are at least 45 degrees to ensure sparsity. This is to test our method\u2019s test-time performance on extreme inputs. But our method can work on different viewpoint settings, as during training, we did not enforce any restriction in selecting input viewpoints. The bunny and frog in Fig. 1 shows that our method can work well even when the two input views have little overlap. Our model can also handle the easier case where viewpoint changes are small."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699846151102,
                "cdate": 1699846151102,
                "tmdate": 1699846151102,
                "mdate": 1699846151102,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WyZrMSu3mS",
                "forum": "noe76eRcPC",
                "replyto": "L8Ov5LCZtF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission750/Reviewer_Pk6q"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission750/Reviewer_Pk6q"
                ],
                "content": {
                    "title": {
                        "value": "Comments for the feedback"
                    },
                    "comment": {
                        "value": "Thanks for the feedback! It solve my concerns. However, after reading all the comments of other reviewers, I notice that I missed a truth that the proposed method \"still requires ground-truth poses at training time\" (as reviewer XwFE  pointed out). This fact lowers my rating of this paper. But I still think the application of differentiable pnp solver in this task is interesting."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700403802710,
                "cdate": 1700403802710,
                "tmdate": 1700403802710,
                "mdate": 1700403802710,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "67tYB7aF03",
            "forum": "noe76eRcPC",
            "replyto": "noe76eRcPC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission750/Reviewer_5PYC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission750/Reviewer_5PYC"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents an amortized inference model for a fast NeRF reconstruction of 3D objects from a few source views. These sources are either obtained from a view-aware diffusion model such as zero123 or give a ground truth. The work trains a large transformer model i.e. 0.59B parameter model for fast (1.3s) inference on A100GPUs. The transformer model outputs coarse pointclouds as well as triplanes for differentiable PNP based pose estimation and NeRF reconstruction respectively. The model is trained on Objaverse dataset and shows generalization capability on other datasets such as ABO, GSO, Omni Objects3D etc."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper proposes a technically sound amortized inference approach for 3D object reconstruction in a fast manner from a few unposed images. The strnegths of this paper are as follows:\n\n1. Clearly better quantitative results against relevant pose predictor baselines\n2. Good ablations showing how various factors affect the model performance, especially mask noise\n3. A good downstream application of the method showing text-to-3D results which is an important application for this work"
                },
                "weaknesses": {
                    "value": "Although the paper is nicely written and the experiments are good, I have the following questions:\n\n1. How much of the improvement comes from the data? The pipeline is not that novel since all of which the authors have presented have been shown before, even the differentiable PnP solver comes from prior work. I would have liked to see scaling laws of the performance on the increased number of data points.\n\n2. No comparison with sparse NeRF methods[1, 2, 3] is shown. Is this because they require accurate camera poses? I would have liked to see some comparison or discussion with these methods, some of them already show sparse novel view synthesis using triplane formulations[1]\n\n3. Relating to point 1, how much does the network learn training data distribution? Is it possible that due to the massive scale of the objaverse dataset, some of the generalizable evaluation, might be in questions, since we don't know if the network has seen similar data before. Can the authors comment on that with some analysis/discussion?\n\n[1] Irshad et al, NeO 360: Neural Fields for Sparse View Synthesis of Outdoor Scenes\n[2] Niemeyer et al, RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs\n[3] Truong et al, SPARF: Neural Radiance Fields from Sparse and Noisy Poses"
                },
                "questions": {
                    "value": "Please see my questions in the weakness section above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission750/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission750/Reviewer_5PYC",
                        "ICLR.cc/2024/Conference/Submission750/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission750/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699207337242,
            "cdate": 1699207337242,
            "tmdate": 1700656374045,
            "mdate": 1700656374045,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aDRgVDQx5i",
                "forum": "noe76eRcPC",
                "replyto": "67tYB7aF03",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission750/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission750/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the valuable feedback. Please see our answers to your questions below. \n\n**Limited pipeline novelty**: We respectfully disagree with this point. We would like to reiterate the contributions of our work: first, we propose a novel single-stream transformer architecture for joint NeRF and pose prediction, which is different from the encoder-decoder one used in the LRM, Instant3D work; the single-stream architecture not only allows the NeRF tokens to be contextualized by image tokens for reconstruction purpose, but  also allows the image tokens to be contextualized by NeRF tokens for predicting per-view coarse geometry used for solving poses. Second,  as far as we know, we are the first to apply the differentiable PnP solver to the task of jointly predicting 3D and pose from unposed sparse views, though the solver itself is inspired by the EPro-PNP work. This is in stark contrast to most prior work, e.g., RelPose++, Forge, that uses direct pose regression from images. Meanwhile, unlike the 6DoF pose estimation and 3D object detection tasks shown in EPro-PNP, we don\u2019t have access to ground-truth 3D points/models during training (we only use multi-view posed images as training data); hence we propose the novel solution of supervising the sparse point predictions by distilling the predicted NeRF geometry in an online manner.\n\n**Overfitting concern**:  As shown by Fig. 1, we have tested our method on the generated multi-views from Zero1-to-3, MVDream, SyncDreamer etc. These images are unlikely to be in the Objaverse dataset, as they are generated by 2D diffusion models, but our method still works well on them, showcasing our model\u2019s strong cross-dataset generalizability. Moreover, we\u2019ve also demonstrated our method\u2019s generalization to the real scanned datasets OmniObjects3D, as shown in Fig. 3 and Tab. 1. This dataset was published later than Objaverse, making it unlikely to be covered by Objaverse. \n\n**Scaling laws**: we have shown scaling laws for model sizes in Table 1,2,3 and 4, where bigger models clearly outperform smaller models. Due to limited compute resources, we do not show scaling laws for data sizes using our largest model (as the scaling law of data is not insightful under smaller model and with inadequate compute [1]); but we\u2019re happy to include one in the final draft.     [1] Scaling Laws for Neural Language Models\n\n**Comparison with sparse NeRF methods**:  In this work, we focus on the challenging case of sparse unposed images as inputs, for the practical concern of estimating poses from sparse unposed images is a very challenging task itself. Hence, most sparse NeRF methods (e.g., [1,2,3] as the reviewer pointed out) requiring poses cannot work on such unposed inputs, especially when overlapping between input views is minimized (e.g., the frog and bunny in our Fig. 1)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699846106611,
                "cdate": 1699846106611,
                "tmdate": 1699846697112,
                "mdate": 1699846697112,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sG91LJrZos",
                "forum": "noe76eRcPC",
                "replyto": "ICREv8qygk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission750/Reviewer_5PYC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission750/Reviewer_5PYC"
                ],
                "content": {
                    "title": {
                        "value": "Response to author's rebutal"
                    },
                    "comment": {
                        "value": "Thank you for answering my questions. I still think the authors need to mention a discussion to sparse-view view synthesis literature i.e. [1,2,3] in their work, especially the ones that utilize triplanes for example [1] and others to highlight how they are lacking and the author's approach is superior. \n\nI genuinely think the work is a nice contribution, however after reading other reviewer's responses and discussion, i believe the work is missing one key comparison which is comparison to relpose++ and forge on the same dataset the authors trained on. This is to ensure an apples-to-apples comparison. It looks like high-quality datasets are really the key and if the authors are claiming their approach is superior, they should not be handicapping other approaches with limited datasets and expect that they would generalize better than them. I am decreasing my score one level due to these missing comparison and my comments above."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700503261757,
                "cdate": 1700503261757,
                "tmdate": 1700503261757,
                "mdate": 1700503261757,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "h1wOfhqvOI",
                "forum": "noe76eRcPC",
                "replyto": "QUY1CjQRV4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission750/Reviewer_5PYC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission750/Reviewer_5PYC"
                ],
                "content": {
                    "title": {
                        "value": "response resolved my concerns, I am willing to increase my score"
                    },
                    "comment": {
                        "value": "Thanks to the authors for including additional comparisons and discussion to related sparse view NeRF methods. It resolved all my concerns and I increased my score to accept."
                    }
                },
                "number": 29,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission750/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700656453848,
                "cdate": 1700656453848,
                "tmdate": 1700656453848,
                "mdate": 1700656453848,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]