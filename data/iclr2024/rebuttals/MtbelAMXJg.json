[
    {
        "title": "Learning Invariances via Neural Network Pruning"
    },
    {
        "review": {
            "id": "06pPRsD4M9",
            "forum": "MtbelAMXJg",
            "replyto": "MtbelAMXJg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8987/Reviewer_iziS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8987/Reviewer_iziS"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a pruning framework called Invariance Unveiling Neural Networks (IUNET) that automatically discovers invariance-preserving subnetworks from deep and dense supernetworks. The framework combines pruning with a novel invariance learning objective to improve the network's inductive bias. In particular, the training objective minimizes the distance between representations of inputs under invariant perturbations for the supernetwork such that the pruned subnetwork architecture captures the invariance. The authors demonstrate that their approach outperforms existing methods in terms of efficiency and effectiveness on both vision and tabular datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The strengths of the paper lie in its comprehensive exploration of invariance in neural network architectures, its demonstration of improved performance on various datasets, and its proposal of a training objective that considers both maximum likelihood and desired invariance properties. The paper also addresses the challenges in designing invariant neural networks for tabular data, which is a valuable contribution to the field."
                },
                "weaknesses": {
                    "value": "- As far as I understand, how connectivity models achieve invariance / equivariance does not only rely on the architecture but also on certain constraints on weights (especially weight sharing), for example, the translation invariance / equivariance with convolution. This work only emphasizes on the architecture, but there are no discussions on weights. I am doubtful of the claim (hypothesis) that the subnetwork after retraining \"preserves desired invariances and hence improves the inductive bias\" and I wonder if this is empirically the case. \n\n- There lacks a justification or analysis on how pruning of architecture is able to achieve for certain transformation groups. The mechanisms of transformations on the values of input tensors (e.g. colour) should apparently be different from the transformations on the domain (e.g. translation and rotation).\n\n- Rotation, as one of the most explored transformation in the literature of invariant/equivariant deep learning, is however not included in the study of this work.\n\n- A couple of details in the experimental setup are not clear enough, and those will highly affect understanding the effectiveness of the proposed method. See questions."
                },
                "questions": {
                    "value": "-  After pruning, is ILO or data augmentation used in training the subnetwork?\n\n- In table 3, are the weights of pruned IUNets also re-initialised or re-trained before evaluating the consistency? And if re-trained is ILO or augmentation used in training? As I mentioned, the inductive bias of invariance / equivariance does not only exist in the architecture but also in the shared weights. So if the IUNet weights are re-initialised then the results are too good to be true, otherwise an unfair comparison.\n\n- Also in table 3 and 4, what transformations are used?\n\n- In table 4, what if data augmentation is applied to all models, no matter with and without ILO? I wonder how ILO can improve over simple augmentation (especially with a pruned model trained without ILO)\n\n- Why in fig2(e) ILO is better than IUNet?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8987/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698823778114,
            "cdate": 1698823778114,
            "tmdate": 1699637131361,
            "mdate": 1699637131361,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9O3NpQW8j9",
                "forum": "MtbelAMXJg",
                "replyto": "06pPRsD4M9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8987/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8987/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Response Part 1"
                    },
                    "comment": {
                        "value": "Thank you for your insightful comments! We provided additional experiments and clarifications to address your concerns. Please let us know if you have any remaining doubts!\n\n**[On Architectural Emphasis]**\n\nOur goal is to improve the **network architecture\u2019s inductive bias**.\n\nWe agree weights play an important role in preserving invariance; however, the **network architecture\u2019s inductive bias** is **independent of its weights** and refers to the inductive bias captured by **the structure of the network**. Improving the network architecture\u2019s structure will improve its prior and optimization trajectory[1] **leading to better trained models**. In fact convolutional nets can be used as feature extractors **even under random initialization**[1,2,3]. Learning invariance by solely updating the weights will not achieve a better architectural inductive bias.\n\nWe empirically verified IUNet\u2019s pruned network has **a better structure and inductive bias in our ablation studies, Table 4.** Our results show the pruned subnetwork achieves **substantially better** downstream accuracy than both the supernetwork trained normally and the supernetwork trained with our novelties, PIs and ILO.\n\nIt is *\u201cempirically the case\u201d IUNet\u2019s captures better such invariances. As detailed in the [On Consistency Experiment] rebuttal section, network pruning is *\u201cnecessary for obtaining an invariance-preserving network\u201d* under maximum likelihood training.\n\nWe will update our main text to emphasize our motivation for improving the **network architecture\u2019s inductive bias** rather than just the weights.\n\n**[On Consistency Experiment 1/2]**\n\nThe weights are re-initialized to right after the first epoch fairly for all models in Table 3. \n\nWe greatly appreciate your suggestion to deeply re-analyze these results. We found an edge case that the consistency metric did not properly capture. Although IUNet\u2019s consistency metric is >98%, **the model predicts the same class for nearly all inputs**! This is because IUNet has majority small weights and not been fully trained, so the encoder consistently predicts a near-zero vector representation, regardless of input image.\n\n**To correct for this edge case, we rerun Table 3 and additional consistency experiments on CIFAR10, CIFAR100, and SVHN.** Because IUNet lacks weight sharing, we expect randomly-initialized CNN to outperform both it and MLPs. With maximum likelihood training, we expect IUNet to effectively bridge the invariance consistency gap between MLPs and CNNs, by learning CNN-like weights from its improved inductive bias. Overall, we expect IUNet to outperform MLPs in accuracy by learning a network structure with better inductive bias."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8987/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700592624462,
                "cdate": 1700592624462,
                "tmdate": 1700592624462,
                "mdate": 1700592624462,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6H8jgTGqG4",
                "forum": "MtbelAMXJg",
                "replyto": "6SPtfr3PLm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8987/Reviewer_iziS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8987/Reviewer_iziS"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the reviewers for very detailed response. A number of my concerns are well addressed but not all of them. \n\nSpecifically, I hold the opinion that for a variety of equivariance, inductive priors can be built on the architecture for two aspects: connectivity and weight sharing (e.g. Cohen and Welling. Group Equivariant Convolutional Networks), while pruning can only address the former, but not the latter. As an example, it is not enough to achieve ConvNet structure from MLPs merely from pruning the connections to be local, because the weight sharing patterns of the sliding window filter also plays an important role. These two aspects together make convnets a good inductive prior for images.  \n\nAnother related concern is that the addressed types of transforms g() are too simple to stress test the ability and limitation of applying pruning methods to learning invariance. More challenging transforms should be involved, e.g. rotation, which is the most explored one in the invariant/equivariant learning literature. The authors did not include such an experiment because in the SVHN dataset there will be ambiguities with \"6\" and \"9\", but this is very easy to overcome by just removing one class from the dataset."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8987/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700692048257,
                "cdate": 1700692048257,
                "tmdate": 1700692048257,
                "mdate": 1700692048257,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "RQWleHvBo5",
            "forum": "MtbelAMXJg",
            "replyto": "MtbelAMXJg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8987/Reviewer_LdpC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8987/Reviewer_LdpC"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to address the issue of pruning neural networks (MLP) which affects the accuracy of the new model, it mainly addresses the issue of invariance during pruning (meaning the weights which might have learned the invariance during the training are dropped) which affects the model accuracy after typical pruning on transformed or original data. It also uses techniques of Initialization of weights and carefully dropping the weights during pruning. They combine Contrastive loss and Maximum likelihood loss to finetune the subnetworks which eventually outperform the original model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper has a really good notion of using existing work to initialize the weights, dropping the weights and even reinitializing the weights after pruning.\n\n- An automated procedure the construct a completely new sub-network.\n\n- They discover invariance-preserving sub-networks that outperform the invariance-agnostic supernetworks, removing inductive bias in Super net is addressed.\n\n- Paper claims really good improvement on MLP after Pruning when the transformation is performed which fills the gap between CNN and MLP performance."
                },
                "weaknesses": {
                    "value": "- Most of the ideas are derived from different papers and combined together.\n\n- Some errors in annotations, missing graph line for different values of K, repetition of the words and techniques throughout the paper.\n\n- The annexure table 7 for tabular data is a bit unclear as it highlights only a few good instances, while the other pruning methods still work better than the proposed method. The reason is not mentioned. While the idea is to capture and preserve the invariance during the pruning, this means somewhere something is still missing in the proposed method, it solves the problem in some of the datasets but not all.\n\n- The paper mentions the encoder-decoder architecture of the proposed method, but throughout the paper, the convention is not followed properly.\n\n- The paper aims to learn an architecture which creates an encoder which represents the same encoding values for all types of transformation."
                },
                "questions": {
                    "value": "- The authors mention that their method improves compression performance on existing models, how about a completely new model? Does it still work the same? Or do the pre-trained models need to be trained extensively?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8987/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8987/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8987/Reviewer_LdpC"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8987/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698933985191,
            "cdate": 1698933985191,
            "tmdate": 1699637131234,
            "mdate": 1699637131234,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "C5NcD7cwko",
                "forum": "MtbelAMXJg",
                "replyto": "RQWleHvBo5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8987/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8987/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Response Part 1"
                    },
                    "comment": {
                        "value": "Thank you for your response! We provided additional experiments and clarifications to address your concerns. Please let us know if you have any remaining doubts!\n\n**[On Compressing Pretrained vs Randomly Initialized Networks]**\n\nOur experiments are run on **randomly initialized networks** (i.e. *\u201ccompletely new models\u201d*) using **\u201cexisting\u201d architectures** (CNNs and MLPs). IUNet (1) **randomly initializes** the supernetwork with weights $\\theta_{M^\\dagger}^{(0)}$, (2) scales the random weights with PIs, $\\theta_M^{(0)} = \\kappa \\theta_{M^\\dagger}^{(0)}$, (3) trains the supernetwork **from scratch** using ILO to get $\\theta_M^{(T)}$, (4) prunes the trained supernetwork into a subnetwork, (5) **randomly re-initializes** the pruned subnetwork as $\\theta_{P}^{(0)}$ with Lottery Ticket Reinitialization (i.e. resetting weights to that of $\\theta_{M^\\dagger}^{(0)}$), and (6) finetunes the **randomly re-initialized** subnetwork **from scratch** using maximum likelihood to get $\\theta_P^{(T)}$. \n\nYes, **IUNet works on pretrained networks with minimal adaptation!** We provide **new experiments** on **pretrained transformers**, following the SMC-Benchmark[1], which prunes the pretrained RoBERTa[2] on the Common Sense QA dataset[3]. Using the official Github repo, *https://github.com/VITA-Group/SMC-Bench/*, we reran SMC\u2019s provided pruning algorithms OMP (after), OMP (before), and SNIP[4]. Additionally, we implemented and ran IUNet and GRaSP[5] on this benchmark. We report the accuracy under different sparsities.\n\nFor a fair comparison, we leave out ILO from IUNet and adopt the same finetuning protocol and hyperparameters as SMC. We tried 2 different settings for kappa: [0.125, 0.0625]. Because SMC uses pretrained models: (1) IUNet scales the pretrained weights rather than random initialization by kappa, (2) IUNet reinitializes the pruned models weights to the fine-tuned model initialized from pretrained weights, not random initializations.\n\nNote, IUNet is a **post-tuning pruning algorithm** (similar to OMP (after) in the benchmark paper), where the model is first pretrained, then fine-tuned (either with maximum likelihood or ILO), then pruned, then sparsely fine-tuned to convergence. OMP(before), SNIP, and GRaSP are **pruning at initialization (PaI)**[1, 6] algorithms, where the model first pretrained, then pruned (iterating through the train set once) then sparsely fine-tuned to convergence.\n\n```\nMethod\\Sparsity   | 0.2   | 0.36  | 0.488 | 0.590 | 0.672\n\u2014-----------------+-------+-------+-------+-------+-------\nOMP (Before)      | 26.54 | 58.89 | 25.88 | 19.57 | 19.57\nSNIP              | 18.67 | 20.48 | 18.84 | 19.00 | 19.16\nGRaSP             | 21.79 | 19.25 | 18.43 | 17.77 | 20.63\n\u2014-----------------+-------+-------+-------+-------+-------\nOMP (After)       | 76.09 | 73.87 | 30.14 | 19.57 | 19.57\n\u2014-----------------+-------+-------+-------+-------+-------\nIUNet* (k=0.125)  | 75.10 | 74.53 | 34.89 | 19.57 | 19.57\nIUNet* (k=0.0625) | 75.02 | 74.37 | 53.80 | 19.57 | 19.57\n*no-ILO\n```\n\nThe densely fine-tuned model reaches an accuracy of 75.76%.\n\nAt low sparsities (sparsity= [0.2, 0.36]), **all post-tuning pruning algorithms (OMP(After) and IUNet) outperform PaI algorithms (OMP(Before), SNIP, GRaSP)**. At these sparsities, *OMP(After) and IUNet are within experimental uncertainty of each other*. This agrees with prior work[1, 6] which finds PaI is less effective than post-tuning algorithms.\n\nAt middle sparsities (sparsity=0.488), **IUNet outperforms all baselines (OMP (after), OMP (before), SNIP, GRaSP),** because PIs alleviates the lazy learning phenomenon which is a key bottleneck in existing pruning algorithms[4]. This is a notable finding, showing **IUNet scales to larger sparsities than baselines.**\n\nAt high sparsities (sparsity= [0.5, 0.672]), **all methods fail**, which agrees with SMC[4], suggesting at least 50% of weights are crucial to the transformer models on these tasks.\n\n**Hence, as shown in results above, IUNet generalizes to pre-trained models and outperforms strong baselines.**"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8987/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700599288165,
                "cdate": 1700599288165,
                "tmdate": 1700627602811,
                "mdate": 1700627602811,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YSu2YPLz9z",
                "forum": "MtbelAMXJg",
                "replyto": "RQWleHvBo5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8987/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8987/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Response Part 2"
                    },
                    "comment": {
                        "value": "**[On Novelty and Existing Work]**\n\nWe highlight our contributions over the existing works and will update our writing to make this more clear.\n\nOn task novelty:\n\n**IUNet is the first general-purpose invariance learning via pruning approach**. Our approach tackles vision, text, and tabular data across MLPs, CNNs, and transformers. Previous work[8] only explored computer vision where the authors state they hope to *\u201csee if [network pruning for invariance learning] can be used in other contexts\u201d*.\n\n\nOn method novelty:\n\n1) ILO: **We are the first to combine maximum likelihood (cross entropy) with contrastive learning (NCE) for network pruning. This combination improves network pruning, which contradicts previous work[7] that found contrastive learning hurts network pruning.** The authors of [7] claim they hope their negative results *\u201cspark interest in developing more representation-friendly pruning methods for supervised contrastive learning\u201d.*  We successfully demonstrate previous efforts failed because they overfit the contrastive objective[14]. Combining both maximum likelihood and contrastive learning is necessary to boost pruning performance.\n\n2) PIs: **We are the first to propose small weight initialization for network pruning.** Small initial weights force the neural network to only grow weights that are truly important. Otherwise, unimportant weights will stay large, causing important weights to be pruned (i.e. lazy training phenomenon[1, 15]). This issue occurs more on deeper networks[15]. Hence, unlike earlier invariance learning for pruning work[8], **IUNet improves the scalability of said approach to deep networks**.\n\n**[On Tabular Dataset Results]**\nThe competing baseline \u201cMLP+C\u201d performs time-consuming (few days for all 40 datasets) hyperparameter optimization over regularization techniques, whereas our approach does not (few hours for all 40 datasets). Thus, there is a trade-off between MLP+C's more time-consuming good results compared to IUNet\u2019s more economical, yet still quite good results.\n\nIt is well known that tabular data has tricky results as the dataset quality can drastically vary: from sparse to dense inputs and from few to many samples[9, 10, 11, 12]. Some datasets are naturally biased towards more sample efficient algorithms like XGBoost[10, 13]. Other datasets may be bottlenecked by regularization rather than architectural design[9, 11, 12]. Despite this, **IUNet boosts MLP performance on tabular data in aggregate and achieves the best performance without expensive hyperparameter tuning.** \n\n**[Clarifying Figure 2]**\n\nThank you for pointing out this errata! We will updated the paper with the correct x- and y-labels. Results with kappa=0.0625 are too poor (below the visible y-axis). We will update our figures to clarify this as well.\n\n**[Clarifying Encoder and Decoder Blocks]**\n\nThank you for pointing out this detail! We will update Section 3.2 to be consistent. In terms of empirical results, as mentioned in Appendix E.4, we follow the encoder-decoder architecture, where only the encoder is pruned and the decoder (final linear layer following conventions) remains dense.\n\n**[Learning different Pruning Masks for different transformation types]**\n\nWe thank the reviewer for this excellent suggestion, but leave this as future work."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8987/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700599878678,
                "cdate": 1700599878678,
                "tmdate": 1700600686086,
                "mdate": 1700600686086,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eUlT7JIeH2",
            "forum": "MtbelAMXJg",
            "replyto": "MtbelAMXJg",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8987/Reviewer_paCL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8987/Reviewer_paCL"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to prune the original dense network (supernetwork) to find an invariance-preserving subnetwork, called IUNet. \nToward this, the authors introduce the Invariance Learning Objective (ILO) that helps to learn representation invariant to \nseveral data transformations and Proactive Initialization (PI).\nThe authors show that minimizing constrastive learning objective is equivalent to minimizing the distance between\nrepresentations of inputs under a set of invariant perturbations, thereby devising ILO.\nThrough several experiments on vision and tabular tasks, this paper validates the effectiveness of proposed IUNet and\nconducted several empirical analysis."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "It is novel to me that this paper proposes to obtain invariance-preserving network \"via network pruning\".\nIn reality, the experimental results on various tasks show the superior consistency measure, which is much higher than the baselines.\nThe theoretical insight between the invariant perturbations and the contrastive learning objective is also impressive."
                },
                "weaknesses": {
                    "value": "Here are my main concerns.\n\n1. The authors try to find the invariance-preserving network \"via network pruning\", but the proposed ILO could also be applied to just desne networks without network pruning. In this sense, there seems to be a lack of motivation as to why network pruning is necessary for obtaining an invariance-preserving network. Additionally, with magnitude-based one-shot pruning, a method simply based on parameter magnitude will be very sensitive to initialization. To alleviate this, more robust methods for initialization, such as SNIP and GraSP, have been proposed, but in this paper, there is no discussion or experiment on how sensitive to initialization.\n\n2. It seems that many presentations of the paper should be revised. For example, in Section 3.2, the explanation for relations of parameters of supernetwork and subnetwork are quite confusing. Since $\\theta_P^{(0)}$ and $\\theta_M^{(T)}$ are parameter \"vectors\", hence the set inclusion operation is not suitable. Also, the expression $|\\theta_P^{(0)}| < |\\theta_M^{(T)}|$ does not imply that $\\theta_P^{(0)}$ is more sparse than $\\theta_M^{(T)}$, which does not give any information about the network capacity. Rather, the authors should express the sparsity or network capacity via $\\ell_0$-norm (number of non-zeros, nnz). In the same sense, in Section 3.2.1, it seems difficult to say that the higher weight value of criterion (3) is a criterion of network capacity. Even if the parameter value is small, if it is non-zero, it will eventually be counted in the FLOPs calculation, so (for example) the number of non-zeros should be used rather than weight magnitude.\n\n3. In the case of proactive initialization (PI), it is as if only the parameter size was simply adjusted, but looking at the results in Figure 3, the parameters were initialized small with $\\kappa = 0.125$. It is natural that the result of $\\kappa = 0.125$ has many weights with smaller values. There needs to be some mathematical evidence or experimental results for proactive initialization that can further demonstrate motivation and reasoning about it.\n\n4. It is true that IUNet shows good performance in various experimental results, but it is difficult to interpret Figure 4 that the weights have any pattern. In this sense, I recommend that the authors consider a slightly larger size network parameter or describe more details about which parts are different between the first row and the second row in Figure 4."
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8987/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699416565568,
            "cdate": 1699416565568,
            "tmdate": 1699637131123,
            "mdate": 1699637131123,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5L2ETjnQD0",
                "forum": "MtbelAMXJg",
                "replyto": "eUlT7JIeH2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8987/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8987/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Response Part 1"
                    },
                    "comment": {
                        "value": "Thank you for your review! We provided additional experiments and clarifications to address your concerns. Please let us know if you have any remaining doubts!\n\n**[Motivation for Pruning]**\n\nOur goal is to improve the **network architecture\u2019s inductive bias**.\n\nWe agree weights play an important role in preserving invariance; however, the **network architecture\u2019s inductive bias** is **independent of its weights** and refers to the inductive bias captured by **the structure of the network**. Improving the network architecture\u2019s structure will improve its prior and optimization trajectory[1] **leading to better trained models**. In fact convolutional nets can be used as feature extractors **even under random initialization**[1,2,3]. Learning invariance by solely updating the weights (i.e. applying ILO *\u201cwithout network pruning\u201d*) will not achieve such properties, hence pruning is necessary.\n\nWe empirically verified IUNet\u2019s pruned network has **a better structure and inductive bias in our ablation studies, Table 4.** Our results show the pruned subnetwork achieves **substantially better** downstream accuracy than both the supernetwork trained normally and the supernetwork trained with our novelties, PIs and ILO. Hence, **IUNet indeed finds a pruned subnetwork with better inductive bias (or prior) than that of the supernetwork**.\n\nWe will update our main text to emphasize the necessity of pruning clearer.\n \n**[On \u201cSNIP\u201d and \u201cGRaSP\u201d]**\n\nWe agree that comparing IUNet with SNIP and GraSP can strengthen our claim that IUNet improves general pruning performance.\n\nTo this end, **we compare IUNet against SNIP and GRaSP** on the SMC-Benchmark[4], which prunes the pretrained RoBERTa[5] on the Common Sense QA dataset[6]. Using the official Github repo, *https://github.com/VITA-Group/SMC-Bench/*, we reran SMC\u2019s provided pruning algorithms OMP (after), OMP (before), and SNIP[7]. Additionally, we implemented and ran IUNet and GRaSP[8] on this benchmark. We report the accuracy under different sparsities.\n\nFor a fair comparison, we leave out ILO from IUNet and adopt the same finetuning protocol and hyperparameters as SMC. We tried 2 different settings for kappa: [0.125, 0.0625]. Because SMC uses pretrained models: (1) IUNet scales the pretrained weights rather than random initialization by kappa, (2) IUNet reinitializes the pruned models weights to the fine-tuned model initialized from pretrained weights, not random initializations.\n\nNote, IUNet is a **post-tuning pruning algorithm** (similar to OMP (after) in the benchmark paper), where the model is first pretrained, then fine-tuned (either with maximum likelihood or ILO), then pruned, then sparsely fine-tuned to convergence. OMP(before), SNIP, and GRaSP are **pruning at initialization (PaI)**[4, 9] algorithms, where the model first pretrained, then pruned (iterating through the train set once) then sparsely fine-tuned to convergence.\n\n ```\nMethod\\Sparsity   | 0.2   | 0.36  | 0.488 | 0.590 | 0.672\n\u2014-----------------+-------+-------+-------+-------+-------\nOMP (Before)      | 26.54 | 58.89 | 25.88 | 19.57 | 19.57\nSNIP              | 18.67 | 20.48 | 18.84 | 19.00 | 19.16\nGRaSP             | 21.79 | 19.25 | 18.43 | 17.77 | 20.63\n\u2014-----------------+-------+-------+-------+-------+-------\nOMP (After)       | 76.09 | 73.87 | 30.14 | 19.57 | 19.57\n\u2014-----------------+-------+-------+-------+-------+-------\nIUNet* (k=0.125)  | 75.10 | 74.53 | 34.89 | 19.57 | 19.57\nIUNet* (k=0.0625) | 75.02 | 74.37 | 53.80 | 19.57 | 19.57\n*no-ILO\n```\n\nThe densely fine-tuned model reaches an accuracy of 75.76%.\n\nAt low sparsities (sparsity= [0.2, 0.36]), **all post-tuning pruning algorithms (OMP(After) and IUNet) outperform PaI algorithms (OMP(Before), SNIP, GRaSP)**. At these sparsities, *OMP(After) and IUNet are within experimental uncertainty of each other*. This agrees with prior work[4, 9] which finds PaI is less effective than post-tuning algorithms.\n\nAt middle sparsities (sparsity=0.488), **IUNet outperforms all baselines (OMP (after), OMP (before), SNIP, GRaSP),** because PIs alleviates the lazy learning phenomenon which is a key bottleneck in existing pruning algorithms[4]. This is a notable finding, showing **IUNet scales to larger sparsities than baselines.**\n\nAt high sparsities (sparsity= [0.5, 0.672]), **all methods fail**, which agrees with SMC[4], suggesting at least 50% of weights are crucial to the transformer models on these tasks.\n\n**Hence, as shown in results above, IUNet generalizes to pre-trained models and outperforms both SNIP and GRaSP.**"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8987/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700589458655,
                "cdate": 1700589458655,
                "tmdate": 1700599307017,
                "mdate": 1700599307017,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nkLxpPaeOJ",
                "forum": "MtbelAMXJg",
                "replyto": "eUlT7JIeH2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8987/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8987/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**[On Sensitivity to Initialization]**\n\nWe ran all experiments 3 times from scratch under different random seeds for all methods. Hence, the trends we report between IUNet and OMP already consider OMP\u2019s sensitivity to initialization.\n\n**[On Paper Presentation]**\n\nThank you for pointing out this confusing notation. In Section 3.2, we will update our notation from $\\theta$ to $\\omega$, where $\\omega$ denotes the set of parameters in each network: $|\\omega_P|<<|\\omega_M|$ and $\\omega_P \\subset \\omega_M$.\n\nYour understanding of the l0-norm is correct, because the l0-norm of parameters where pruned parameters are zero is equivalent to the size of the set of remaining parameters. More formally:\n\n$$\nnnz(\\theta_M) = \\sum_{\\theta_{Mi} \\in \\theta_{M}} 1[\\theta_{Mi} \\ne 0] = \\sum_{\\omega_{Pi} \\in \\omega_P} 1 = |\\omega_P|\n$$\n\n**[On PIs]**\n\nPIs will scale the original weights by multiplying the Kaiming-initialized weights by a kappa scalar as stated in Section 3.2.2:\n\n$\\theta_M^{(0)} = \\kappa \\theta_{M \\dagger}^{(0)} $\n\n**We agree it is natural thus for the learned weights to also have smaller magnitudes!** This accomplishes our goal of filtering out unimportant weights, because most weights will have small magnitudes, while only important weights will have large magnitudes. In Figure 3, **we empirically verify** that the magnitude of important weights are the same (large) with and without PIs, yet the magnitude of most weights are small **only with PIs.**\n\n**Encouraging most weights to be small matters**, because the change in output logits is proportional to the magnitude of pruned weights. This relation between the magnitude of pruned weights and output logits can be seen in a simple 2 layer neural network with k-Lipschitz activation:\n\nLet the 2-layer supernetwork be: $f(x) = \\sigma(\\sigma(xW_{1})W_2), x\\in R^d, W_1 \\in R^{d\\times d}, W_2 \\in R^{d\\times 1}$. \n\nLet the pruned subnetwork be: $g(x) = \\sigma(\\sum_{i\\in P_2'}\\sigma(\\sum_{j,i\\in P_1'}xW_{1,j,i})W_{2,i}))$ and pruned weight indices be $P_2 = argtopk(-|W_2|), P_1 = argtopk(-|W_1|)$ and unpruned weight indices be $P_2' = ${$ 1,...,d $}$ \\setminus P_2, P_1' = ${$ 1,...,d $}$ ^2 \\setminus P_1$. \n\nThen the difference in the pruned network\u2019s and original network\u2019s output logits is:\n\n$|f(x) - g(x)| \\le k|\\sigma(xW_{1})W_2 - \\sum_{i\\in P_2'}\\sigma(\\sum_{j,i\\in P_1'}xW_{1,j,i})W_{2,i}|$\n\n$$|\nf(x) - g(x)| \\le k C_1 + k^2 C_2\n$$\n\nwhere $C_1 = |\\sum_{i \\in P_2} \\sigma(xW_{1})_i|$\n\nand $C_2 = \\sum_{i\\in P_2'} (W_{2,i}\\sum_{j,i\\in P_1}|xW_{1,j,i}|)$\n\n$|f(x) - g(x)| = \\mathcal{O}(\\sum_{i\\in P_2}|W_{2,i}|+\\sum_{j,k\\in P_1}|W_{1,j,k}|)$\n\n**Figure 2abc presents empirical evidence of PIs efficacy**. As we decrease kappa, there is a **tradeoff** between performance of the unpruned network and performance improvement in the pruned network. Interestingly, **kappa has a sweet spot, where the pruned network\u2019s performance grows higher than that of the Kaiming-initialized (kappa=1.0) supernetwork\u2019s!** We highlight this is **a substantial finding among the network pruning literature**[10] and supports our hypothesis that through pruning, the subnetwork can improve the inductive bias of the supernetwork. \n\n**Table 4, ablation studies, presents empirical evidence of PIs efficacy**. If we train a model without PIs, performance is worse on computer vision datasets, as the pruning stage cannot differentiate important from unimportant weights. Tabular datasets show less consistent results since they are much more varied in size and feature type [14, 15, 16, 17]. In some tabular datasets, regularization becomes the bottleneck rather than model architecture [15, 16, 17]. Nonetheless, our ablation studies demonstrate **PIs will improve the downstream accuracy of IUNet**.\n\n**\u201cOur [On \u201cSNIP\u201d and \u201cGRaSP\u201d] rebuttal section also presents empirical evidence of PIs efficacy.** By simply adjusting the initialization, **PIs scales transformers to larger sparsities than that of baselines**.\n\nWe will update section 5.4 to make this empirical evidence more salient."
                    },
                    "title": {
                        "value": "Rebuttal Response Part 2"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8987/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590776242,
                "cdate": 1700590776242,
                "tmdate": 1700590793164,
                "mdate": 1700590793164,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wiJg5AOLRI",
                "forum": "MtbelAMXJg",
                "replyto": "eUlT7JIeH2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8987/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8987/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Response Part 3"
                    },
                    "comment": {
                        "value": "**[On Figure 4]**\n\nWe provide further discussion on the visualizations below and will update our text accordingly.\n\nIn Figure 4, we visualize randomly-chosen filters from the first layer of each network. We randomly chose the filters for fairness, and picked the first layer because it is the easiest to interpret. The main conclusion from Figure 4 is: **IUNet bridges the gap between MLPs and CNNs and decision trees architecturally**. We make the following specific observations:\n\n**1) Locally Connected Regions:** For Figures 4efg, IUNet finds locally connected regions with high RGB values. Local connections help preserve translation and scale invariance as shown in Table 3 and verified by previous works[1, 2, 3, 11]. The presence of locally-connected regions visually confirms that IUNet can rediscover CNN architectures from MLPs.\n\n**2) Color Invariance:** In Figures 4eg (CIFAR10/SVHN), IUNet assigns high values to locally connected regions uniformly across all RGB channels, which improves color invariance. In Figure 4f (CIFAR100), this is not as consistent due to a more challenging learning objective, though we find it still improves color invariance over the MLP supernetwork.\n\n**3) Discovering Substructures:** Nearly all IUNet output neurons favor a clear pattern of input neurons (the exception being the top left filter of Figure 4f), whereas nearly no OMP output neurons exhibit any pattern. This suggests IUNet does a good job recovering invariance preserving substructures across the supernetwork\u2019s first layer.\n\n**4) Axis-Aligned Tree Structures:** For Figure 4h, IUNet discovers subnetworks that focus on primarily 1 input feature for each output neuron. This behavior mirrors tree-based models where neurons are activated where single features reach some threshold [12, 13, 14]. Such network architecture follows existing work which found the axis-aligned inductive bias in tree-based models is critical to XGBoost\u2019s performance over standard MLP setups [14].\n\n**[References]**\n\n[1] Ulyanov, Dmitry, Andrea Vedaldi, and Victor Lempitsky. \"Deep image prior.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.\n\n[2] Cao, Yun-Hao, and Jianxin Wu. \"A random cnn sees objects: One inductive bias of cnn and its applications.\" Proceedings Of The AAAI Conference On Artificial Intelligence. Vol. 36. No. 1. 2022.\n\n[3] Rosenfeld, Amir, and John K. Tsotsos. \"Intriguing properties of randomly weighted networks: Generalizing while learning next to nothing.\" 2019 16th conference on computer and robot vision (CRV). IEEE, 2019.\n\n[4] Liu, Shiwei, et al. \"Sparsity May Cry: Let Us Fail (Current) Sparse Neural Networks Together!.\" arXiv preprint arXiv:2303.02141 (2023).\n\n[5] Liu, Yinhan, et al. \"Roberta: A robustly optimized bert pretraining approach.\" arXiv preprint arXiv:1907.11692 (2019).\n\n[6] Talmor, Alon, et al. \"Commonsenseqa: A question answering challenge targeting commonsense knowledge.\" arXiv preprint arXiv:1811.00937 (2018).\n\n[7] Lee, Namhoon, Thalaiyasingam Ajanthan, and Philip HS Torr. \"Snip: Single-shot network pruning based on connection sensitivity.\" arXiv preprint arXiv:1810.02340 (2018).\n\n[8] Wang, Chaoqi, Guodong Zhang, and Roger Grosse. \"Picking winning tickets before training by preserving gradient flow.\" arXiv preprint arXiv:2002.07376 (2020).\n\n[9] Frankle, Jonathan, et al. \"Pruning neural networks at initialization: Why are we missing the mark?.\" arXiv preprint arXiv:2009.08576 (2020).\n\n[10] Ma, Xiaolong, et al. \"Sanity checks for lottery tickets: Does your winning ticket really win the jackpot?.\" Advances in Neural Information Processing Systems 34 (2021): 12749-12760.\n\n[11] Neyshabur, Behnam. \"Towards learning convolutions from scratch.\" Advances in Neural Information Processing Systems 33 (2020): 8078-8088.\n\n[12] Marton, Sascha, et al. \"GRANDE: Gradient-Based Decision Tree Ensembles.\" arXiv preprint arXiv:2309.17130 (2023).\n\n[13] Marton, Sascha, et al. \"GradTree: Learning Axis-Aligned Decision Trees with Gradient Descent.\" NeurIPS 2023 Second Table Representation Learning Workshop. 2023.\n\n[14] Grinsztajn, L\u00e9o, Edouard Oyallon, and Ga\u00ebl Varoquaux. \"Why do tree-based models still outperform deep learning on typical tabular data?.\" Advances in Neural Information Processing Systems 35 (2022): 507-520.\n\n[15] Gorishniy, Yury, et al. \"Revisiting deep learning models for tabular data.\" Advances in Neural Information Processing Systems 34 (2021): 18932-18943.\n\n[16] Kadra, Arlind, et al. \"Well-tuned simple nets excel on tabular datasets.\" Advances in neural information processing systems 34 (2021): 23928-23941.\n\n[17] McElfresh, Duncan, et al. \"When Do Neural Nets Outperform Boosted Trees on Tabular Data?.\" arXiv preprint arXiv:2305.02997 (2023)."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8987/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590880548,
                "cdate": 1700590880548,
                "tmdate": 1700591007759,
                "mdate": 1700591007759,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "V20ZJpdONz",
                "forum": "MtbelAMXJg",
                "replyto": "eUlT7JIeH2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8987/Reviewer_paCL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8987/Reviewer_paCL"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "I appreciate the authors for their kind rebuttals, but they only partially address my concerns.\n\nI understand why Proactive initialization helps the network pruning in theory (though the authors simply justify only for 2-layer networks) and practice. Also, the descriptions on Figure 4 helps me understand the points than before reading the rebuttals.\n\nHowever, I have still concern about \"how and why pruning is required for learning invariance\" (I think it is the most important part since it is the motivation of this paper). The authors say that inductive bias heavily depends on the architecture itself rather than on the model parameter, but this paper fully utilizes the pretrained models, in which one trains model at \"good initialization\".\n\nAccording to author's claim, I think that this paper should conduct the experiments without pretrained models, i.e., training from scratch with Invariance Learning Objective (ILO). Furthermore, in my view, ILO still seems far away from network pruning, and applying ILO to dense models (without pruning) is expected to better performance than pruned networks.\n\nConsidering these points, I will maintain my score in the current shape."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8987/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700612051699,
                "cdate": 1700612051699,
                "tmdate": 1700618231250,
                "mdate": 1700618231250,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NTFURt7Brh",
                "forum": "MtbelAMXJg",
                "replyto": "eUlT7JIeH2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8987/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8987/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Important Clarification"
                    },
                    "comment": {
                        "value": "Thank you for your timely response! We want to make an important clarification.\n\nAs stated in Section 3.2, our paper experiments **are** run on **randomly initialized networks**, not pretrained models. \n\nSpecifically, in the paper, IUNet (1) **randomly initializes** the supernetwork with weights $\\theta_{M^\\dagger}^{(0)}$, (2) scales the random weights with PIs, $\\theta_M^{(0)} = \\kappa \\theta_{M^\\dagger}^{(0)}$, (3) trains the supernetwork **from scratch** using ILO to get $\\theta_M^{(T)}$, (4) prunes the trained supernetwork into a subnetwork, (5) **randomly re-initializes** the pruned subnetwork as $\\theta_{P}^{(0)}$ with Lottery Ticket Reinitialization (i.e. resetting weights to that of $\\theta_{M^\\dagger}^{(0)}$), and (6) finetunes the **randomly re-initialized** subnetwork **from scratch** using maximum likelihood to get $\\theta_P^{(T)}$. \n\nTables 1 and 2 compare the **randomly initialized** supernetwork and the **randomly initialized** pruned network (IUNet) both trained under the **same** maximum likelihood objective, where we find *\"the inductive bias heavily depends on the architecture itself rather than on the model parameters\"*. Table 4 shows the pruned networks (IUNet) outperforms applying ILO to dense models without pruning (IUNet (No-Prune)).\n\nWe will update our notation and wording in Section 3.2 to make this clearer. Please let us know if you have any remaining questions. Thanks again!"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8987/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700619596864,
                "cdate": 1700619596864,
                "tmdate": 1700690741617,
                "mdate": 1700690741617,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hXDYvsYkf5",
                "forum": "MtbelAMXJg",
                "replyto": "eUlT7JIeH2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8987/Reviewer_paCL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8987/Reviewer_paCL"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "That's my confusion, thanks for the correction to the authors. Also, I appreciate the authors for their response."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8987/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700621514299,
                "cdate": 1700621514299,
                "tmdate": 1700630493789,
                "mdate": 1700630493789,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]