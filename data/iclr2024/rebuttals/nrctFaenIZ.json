[
    {
        "title": "GradSkip: Communication-Accelerated Local Gradient Methods with Better Computational Complexity"
    },
    {
        "review": {
            "id": "uSLhDuA2W1",
            "forum": "nrctFaenIZ",
            "replyto": "nrctFaenIZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5973/Reviewer_su5x"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5973/Reviewer_su5x"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses an intriguing issue in the domain of distributed optimization algorithms. Conventionally, in these algorithms, clients need to have periodic communication and each client performs an equal number of local training steps per communication round. The authors question this norm, pointing out that some clients might face more complex data or difficult problems, potentially necessitating more local training. \n\nThe paper introduces a novel algorithm, GradSkip, which realizes this intuition. The authors also provides a clear mathematical analysis and proof. The paper demonstrates that the number of local gradient steps can be reduced relative to the local condition number without undermining the communication complexity. Furthermore, the paper extends its discussion to include other scenarios like variance reduction and gradient compression, leading to the development of GradSkip+."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper uncovers a notable conclusion that clients with simpler data or problems might require fewer local training steps, a concept not widely addressed in current literature.\n\n2. The authors support their findings with stringent and well-articulated mathematical proofs, enhancing the credibility and academic rigor of their work.\n\n3. The analysis provided is detailed and easy to follow, making the complex concepts accessible to readers.\n\n4. Introduction of unbiased compression operators is a significant technical innovation. This concept broadens the scope for a range of new algorithms, marking a substantial contribution to the field.\n\n5. The paper succeeds in providing a comprehensive framework that not only encompasses many known algorithms (ProxGD, ProxSkip, RandProx-FB) but also suggests the potential for several unknown algorithms through its unbiased compression operator."
                },
                "weaknesses": {
                    "value": "One minor critique is that the paper's theoretical bounds are not tight in constant terms."
                },
                "questions": {
                    "value": "Even though I acknowledges the theoretical contributions of this work, I have a question regarding its practical relevance. Specifically, how severe the issue of statistical heterogeneity is in machine learning? How large is the divergence of curvatures among clients? This question is related to the significance of GradSkip algorithm (and potentially any following works) in real-world scenarios."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5973/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698617667159,
            "cdate": 1698617667159,
            "tmdate": 1699636638294,
            "mdate": 1699636638294,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ElP0DDLgov",
                "forum": "nrctFaenIZ",
                "replyto": "uSLhDuA2W1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5973/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5973/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Gratitude and Clarification"
                    },
                    "comment": {
                        "value": "Thank you for your positive review of our paper. We greatly appreciate the time and effort you dedicated to providing a thorough evaluation of our work. Your recognition of the strengths of our work is immensely encouraging.\n\nResponse to the question:\n\n>Even though I acknowledges the theoretical contributions of this work, I have a question regarding its practical relevance. Specifically, how severe the issue of statistical heterogeneity is in machine learning? How large is the divergence of curvatures among clients? This question is related to the significance of GradSkip algorithm (and potentially any following works) in real-world scenarios. \n\nIn Figure 3 in the Appendix (last page), we conducted a similar experiment to that in the main part of the paper, using the ``Australian'' dataset from the LibSVM library. In this experiment with 20 devices, we identified 8 ill-conditioned devices, demonstrating that such highly heterogeneous datasets do exist in real-world scenarios. GradSkip showed almost twice the efficiency compared to ProxSkip in terms of total computational complexity. In the fourth column of the results, it is observed that, on average, 9 out of 20 devices perform 5 times less work than the device with the $L_{\\max}$-smoothness constant. Although our experiments on real datasets were limited, we believe that in practical situations where local datasets are highly heterogeneous, significant differences in local smoothness constants $L_i$ are likely. Consider, for instance, a scenario in federated image classification where users from diverse groups contribute their images for model training."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5973/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700400573637,
                "cdate": 1700400573637,
                "tmdate": 1700400573637,
                "mdate": 1700400573637,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "262kuP0tZe",
                "forum": "nrctFaenIZ",
                "replyto": "ElP0DDLgov",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5973/Reviewer_su5x"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5973/Reviewer_su5x"
                ],
                "content": {
                    "title": {
                        "value": "Reply to author's rebuttal"
                    },
                    "comment": {
                        "value": "Indeed, LibSVM is quite limited. I am curious about the potential of GradSkip's core idea, that some clients have simpler tasks and thus can train less, in more complex scenarios. For instance, in federated learning for ResNet image classification, how significant would the variation in optimization difficulty be across different clients?\n\nI agree with other reviewers that this work is an extension of ProxSkip. However, I do not see this as a drawback. I think this paper makes unique contributions beyond what ProxSkip offers, advancing the understanding of client-specific training needs in distributed optimization.\n\nOverall, I vote \"accept\" due to the novel idea of varying training intensity across different clients based on the varying task difficulty they encounter."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5973/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700711900791,
                "cdate": 1700711900791,
                "tmdate": 1700711900791,
                "mdate": 1700711900791,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "f8vHya43Aj",
            "forum": "nrctFaenIZ",
            "replyto": "nrctFaenIZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5973/Reviewer_NZhx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5973/Reviewer_NZhx"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes a new local gradient-type method for distributed optimization with communication and computation constraints. The proposed method inherits the same accelerated communication complexity from ProxSkip while further improving computational complexity. And two variants of the proposed method, i.e., GradSkip+ and VR-GradSkip+ are proposed."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. A new local gradient-type method for distributed optimization with communication and computation constraints is proposed in this work, which is the extension of the ProxSkip method. The proposed method inherits the same accelerated communication complexity from ProxSkip while further improving computational complexity.\n\n2. And two variants of the proposed method, i.e., GradSkip+ and VR-GradSkip+ are proposed."
                },
                "weaknesses": {
                    "value": "1. The assumption that functions $f_i(x)$ are strongly convex is too strong since many functions will not satisfy this assumption when utilizing neural networks.\n\n2. Lack of theoretical analysis of the communication complexity of the proposed method. In distributed optimization, communication complexity is crucial for minimizing inter-node communication to enhance system efficiency and reduce communication costs.\n\n3. The experimental results are limited, the authors should conduct more experiments to verify the performance of the proposed method.\n\n4. The writing of this work is poor. I can't find the Conclusion section. And the summary of contributions is excessively lengthy.\n\n5. There are lots of mistakes in this work, for example, \n\n``Appendix ??'',\n\n ``see Algorithm ?? in the Appendix'', \n\n`` (see Appendix)''"
                },
                "questions": {
                    "value": "Please see the weakness above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5973/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5973/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5973/Reviewer_NZhx"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5973/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698742719924,
            "cdate": 1698742719924,
            "tmdate": 1700658859634,
            "mdate": 1700658859634,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kOGyNsE21w",
                "forum": "nrctFaenIZ",
                "replyto": "f8vHya43Aj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5973/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5973/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors"
                    },
                    "comment": {
                        "value": "Thank you for your review. We below address the weaknesses you mentioned.\n\nWeaknesses:\n\n>1. The assumption that functions $f_i(x)$ are strongly convex is too strong since many functions will not satisfy this assumption when utilizing neural networks.\n\nAll theory relies on some assumptions: there is no free lunch. Please note that these assumptions are standard in the field. Countless published works use precisely these assumptions. To the best of our knowledge, the phenomena we study exist under these assumptions, and it is not known whether they hold under more relaxed settings. While this is indeed a limitation, it is important to recognize that every theoretical paper has some limitations. The critical factor is whether the limitations are reasonable given the subject being studied. In our work, this is clearly the case. We use the same setup as the ProxSkip work but are able to develop a more refined method with better theoretical properties.\n\nTheory papers can be judged on at least two axes: depth, which refers to the sharpness of the results in a limited setting, and breadth, which involves extensions to other settings. In our work, we primarily focus on depth rather than breadth. We believe that depth is significantly more important - both we and others can work on extensions and generalizations later. However, for such work to be meaningful, it needs to be built on strong or deep foundations.\n\n>2. Lack of theoretical analysis of the communication complexity of the proposed method. In distributed optimization, communication complexity is crucial...\n\nWhat do you mean by the lack of theoretical analysis of the communication complexity? In Theorem 3.6, we demonstrate the communication complexity of our GradSkip method, which matches that of the state-of-the-art ProxSkip method. The complete proof of this theorem, along with all other statements we make, is included in the appendix.\n\n>3. The experimental results are limited, the authors should conduct more experiments to verify the performance of the proposed method.\n\nFirstly, we'd like to understand your perspective more clearly: what specific types of experiments or settings did you have in mind that would further verify the performance of our proposed method?\n\nThe goal of our experiments is to illustrate our theory. In our view, strong theoretical research does not necessarily require extensive experimental validation, just as robust practical work may not always need a theoretical basis to be publishable. Our experiments are carefully designed to clearly illustrate our theoretical claims. We adhere to the principle of Occam's Razor, advocating for simplicity in experimentation when it suffices to validate the theory. This approach, we believe, provides clearer insights than conducting complex or excessive experiments.\n\nOur experiments focus on comparing our algorithm, GradSkip, with ProxSkip \u2013 the only known local training algorithm with accelerated communication complexity available at the time of our research. We chose not to compare with algorithms that lack accelerated communication complexity since our primary goal was to enhance computational complexity within the realm of local training algorithms with accelerated communication. The ProxSkip research already demonstrates that previous generations of local training methods fall short in terms of communication complexity. Thus, our comparison with the state-of-the-art (SOTA), ProxSkip, is both relevant and sufficient for the scope of our study.\n\nIn light of this, do you agree that our experiments sufficiently and effectively corroborate our theoretical claims?\n\n>4. The writing of this work is poor. I can't find the Conclusion section. And the summary of contributions is excessively lengthy.\n\nWe understand your concerns regarding the Conclusion section and the summary of contributions.\n\nDue to space constraints, we have integrated the key points typically found in a Conclusion section within other parts of the paper. We believe this approach still effectively communicates the core findings and implications of our research. However, for the camera-ready version of the paper, we will review it again to ensure that these key points are clearly highlighted and easily identifiable to the reader.\n\nRegarding the summary of contributions, we have strived to be as concise as possible while ensuring that all critical aspects of our work are adequately covered. We will take another look at this section to see if any further streamlining is possible without omitting essential details.\n\n>5. There are lots of mistakes in this work...\n\nWe apologize for the LaTeX errors in our initial submission. These were formatting issues, and we assure you they have been corrected. In the full version of the paper in the supplementary materials, these issues do not exist. Furthermore, we have recently uploaded a revised version of the main part of the paper, where all these issues have been addressed and fixed."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5973/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700400892790,
                "cdate": 1700400892790,
                "tmdate": 1700400892790,
                "mdate": 1700400892790,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kiPfuzUwoz",
                "forum": "nrctFaenIZ",
                "replyto": "f8vHya43Aj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5973/Reviewer_NZhx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5973/Reviewer_NZhx"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your responses. My concerns have not been resolved. It's quite common for theoretical proofs to include certain assumptions, but these assumptions can be categorized as either strong or mild. In your case, the assumption that f needs to simultaneously satisfy both L-smooth and u-strongly convex is not a very mild assumption in the field of Federated Learning [1, 2, 3, 4], especially when you're using neural networks. \n\n[1] Local SGD with Periodic Averaging: Tighter Analysis and Adaptive Synchronization\n\n[2] On the Convergence of Communication-Efficient Local SGD for Federated Learning\n\n[3] Faster\u00a0non-convex federated learning\u00a0via global and local momentum\n\n[4] Asynchronous Parallel Stochastic Gradient for Nonconvex Optimization\n\nSo, it's only then that I would consider the scenarios where the proposed work can be applied are limited, and that's why I requested you to supplement more experiments to demonstrate the generality of the proposed work. However, I haven't seen a satisfactory response."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5973/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700658719441,
                "cdate": 1700658719441,
                "tmdate": 1700659918140,
                "mdate": 1700659918140,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zz7FOsUgbd",
            "forum": "nrctFaenIZ",
            "replyto": "nrctFaenIZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5973/Reviewer_8o1X"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5973/Reviewer_8o1X"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes Gradskip for solving federated optimization problems with smooth strongly convex objective. Gradskip improves local gradient computation complexity and achieves the optimal communication complexity. The paper further extends the idea of Gradskip to propose Gradskip+ and VR-Gradskip+, which covers a wider range of application."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed Gradskip method and its extensions modify Scaffnew by allowing skipping local gradient computation and improve the local gradient computation complexity to $O(\\min(\\sqrt{\\kappa_{\\max}},\\kappa_i)\\log(1/\\epsilon))$ from $O(\\sqrt{\\kappa_{\\max}}\\log(1/\\epsilon))$, while still achieving the optimal communication complexity $\\sqrt{\\kappa}\\log(1/\\epsilon)$. I suggest the authors summarize their results and existing work in table.\n2. Allowing skipping gradient computation is helpful to address system heterogeneity as slow clients can compute less in a communication round."
                },
                "weaknesses": {
                    "value": "1. The novelty of this paper looks somewhat limited. The novelty and main contribution is that Gradskip doesn't always compute local gradient and thus requires $O(\\min(\\sqrt{\\kappa_{\\max}},\\kappa_i)\\log(1/\\epsilon))$ proposes Gradskip, instead of $O(\\sqrt{\\kappa_{\\max}}\\log(1/\\epsilon))$. However, the framework and analysis of proposed Gradskip is similar to Scaffnew.\n2. The improvement on computational cost heavily depends on the values of $q_i$, which rely on $\\kappa_i$. However, Remark 3.3 says GradSkip addresses heterogeneity by assigning $q_i$ to clients in accordance with their local computational resources. It is unclear how to connect $\\kappa_i$ to the local computational resources.\n3. Can Gradskip also make improvement on computation time over Scaffnew? What is the time cost for computing gradient in each iteration?"
                },
                "questions": {
                    "value": "see the section of weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5973/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699010987047,
            "cdate": 1699010987047,
            "tmdate": 1699636638043,
            "mdate": 1699636638043,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LcLT8qPtKa",
                "forum": "nrctFaenIZ",
                "replyto": "zz7FOsUgbd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5973/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5973/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors 1"
                    },
                    "comment": {
                        "value": "Thank you for reviewing our work and taking the time to look at our submission. We're glad to have the chance to respond to your comments and questions. Here are our answers to the points you brought up. If you have any more questions or concerns, please let us know.\n\nAddressing Weaknesses:\n\n>1. The novelty of this paper looks somewhat limited. The novelty and main contribution is that Gradskip doesn't always compute local gradient and thus requires $O(\\min(\\sqrt{\\kappa_{max}}, \\kappa_i) \\log(1/\\epsilon))$. However, the framework and analysis of proposed Gradskip is similar to Scaffnew.\n\nThe novelty in GradSkip is an extension of the original ProxSkip method in various directions. We provide the entire Section 2 to summarize our contributions.\n\nWe manage to modify ProxSkip in such a way that we preserve its accelerated communication complexity while provably improving its computation complexity. That is, our local training method, GradSkip, is still fast from a communication round viewpoint but \"bothers\" each client only as much as necessary, depending on the quality/importance of the local data stored by each client. This is the first time a result of this type appears in the local training / federated learning literature. In all prior works, there is no theoretical result that would allow the clients to take a different number of local steps and, at the same time, benefit from this. This is the main and key contribution of our work.\n\nStarting from the second paragraph of Section 2.1, we discuss the key technical innovations behind GradSkip, primarily the construction of auxiliary shifts. We believe that the design and provision of rigorous convergence guarantees for auxiliary shifts, particularly those tailored to address specific constraints, is a complex and nuanced task requiring thorough consideration. Additionally, we believe that a small change leading to a significant impact is often more surprising and valuable than a larger change achieving the same outcome.\n\n>2. The improvement on computational cost heavily depends on the values of $q_i$, which rely on $\\kappa_i$. However, Remark 3.3 says GradSkip addresses heterogeneity by assigning $q_i$ to clients in accordance with their local computational resources. It is unclear how to connect $\\kappa_i$ to the local computational resources.\n\nOur selection of optimal values for $q_i$, which depend on the condition numbers $\\kappa_i$, leads to a reduction in the total number of gradient computations while keeping the communication complexity unchanged. However, it's crucial in federated learning to minimize unnecessary client involvement, i.e., to use their devices for local training as infrequently as possible. This consideration is somewhat independent of time complexity. Clearly, if there are two training methods that take the same amount of time, but one requires less client engagement, then this approach is preferable, all other factors being equal.\n\nAn alternative strategy involves considering the computational power of the devices when determining the $q_i$ values. This approach can significantly reduce training time, as we discussed in the 'System Heterogeneity' subsection of the Introduction. Essentially, slower clients are assigned lower $q_i$ values to lessen their local workload, whereas faster clients receive higher $q_i$ values. This method aims to minimize delays caused by slower clients during model synchronization. For a clearer understanding of this setup and the choice of $q_i$ values, let us provide a more detailed explanation.\n\nLet $T_i$ denote the time required for one local step on client $i$, where $i=1,2,\\ldots,n$. We assume that these $T_i$ values can be measured on each device $i$. Then, the average time required to perform local training before communication on client $i$ is given by: \n$$\n\\frac{T_i}{1-q_i(1-p)}, \n$$ \nsince on average device $i$ does $\\frac{1}{1-q_i(1-p)}$ local steps (see Lemma 3.2). We can minimize the waiting time by choosing appropriate values of $q_i$ for each device $i$. We start by setting $q_i=1$ for the fastest clients, i.e., for clients $i\\in\\arg\\min_i(T_i)$. For the remaining clients, we choose $q_i$ such that the average time taken for local training before communication is equal to the average time taken by the fastest device. Mathematically, this can be expressed as \n$$ \n\\frac{T_i}{1-q_i(1-p)} = \\frac{T_{min}}{p}, \n$$ \nyielding the value of $q_i$ as\n$$ \nq_i = \\max \\left \\\\{ \\frac{1-p\\frac{T_i}{T_{min}}}{1-p}, 0 \\right \\\\}. \n$$\n\nUsing these values of $q_i$, we can minimize the time delay during communication. It is worth noting that in practice, we can update the values of $q_i$ during training if the estimates of $T_i$ change. \n\nWe have not elaborated on this in great detail in the main part of the paper, but we are ready to include a more comprehensive explanation in the appendix of the camera-ready version."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5973/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700401285200,
                "cdate": 1700401285200,
                "tmdate": 1700401285200,
                "mdate": 1700401285200,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "z6UCFh96Kx",
            "forum": "nrctFaenIZ",
            "replyto": "nrctFaenIZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5973/Reviewer_ENdC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5973/Reviewer_ENdC"
            ],
            "content": {
                "summary": {
                    "value": "Built upon ProxSkip, authors proposed GradSkip (and variants GradSkip+) algorithms by incorporating new randomness with each client. The proposed algorithm attains better computation complexity compared to existing works."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The key novelty lies in the newly introduced client-wise randomness, which induces fake local steps and less local steps (Lemma 3.1 and 3.2), the idea is elegant.\n2. Better computation complexity."
                },
                "weaknesses": {
                    "value": "1. Compared to ProxSkip (Mishchenko et al. (2022)), the algorithm here requires finer structure information from the devices, i.e., individualized function smoothness parameters, while ProxSkip only requires a global smoothness parameter. And all clients are required to coordinate in advance to know the global information $\\kappa_{\\max}$, which may be a bit unrealistic.\n2. According to Theorem 3.6, the client gradient query number is improved from $\\sqrt{\\kappa_{\\max}}$ to $\\min(\\kappa_i, \\sqrt{\\kappa_{\\max}})$, while the iteration and communication complexity does not change, the claimed $O(n)$ superiority only appears in scenarios where the devices are very unbalanced (most of them have small $\\kappa$, while few of them attain very large $\\kappa$. As mentioned in your experiments, only one ill-conditioned device), I may view such scenarios to be relatively rare in real world (or it is better if authors can rationalize it). If so the derived improvement seems to be a little bit weak.\n3. As far as I understand, the proof heavily relies on the proof of ProxSkip, which restricts the significance of the contribution a bit.\n\nTo summarize, I think the algorithm is an interesting extension of ProxSkip with an elegant modification, while I concern that the improvement may be a bit marginal to cross the bar. Please definitely indicate if I misunderstood any points. Thank you very much for your efforts."
                },
                "questions": {
                    "value": "1. In Assumption 3.4, why not extend each $f_i$ to attain a personalized strong convexity parameter $\\mu_i$? I think it should be expected.\n2. As a separate question, compared to communication complexity, whether improving individual computation complexity is an important question to the FL community, I expect that such improvement should be attractive to marginalized devices."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5973/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699408828322,
            "cdate": 1699408828322,
            "tmdate": 1699636637932,
            "mdate": 1699636637932,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zF9IpEd4YK",
                "forum": "nrctFaenIZ",
                "replyto": "z6UCFh96Kx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5973/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5973/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal by Authors 1: Addressing Weaknesses"
                    },
                    "comment": {
                        "value": "Thank you for your review and the time you invested in evaluating our submission. We appreciate the opportunity to address the points you raised. Here are our responses to the weaknesses and questions you highlighted. Please reply if you have any remaining questions or concerns.\n\nAddressing Weaknesses:\n\n> 1. Compared to ProxSkip (Mishchenko et al. (2022)), the algorithm here requires finer structure information from the devices, i.e., individualized function smoothness parameters, while ProxSkip only requires a global smoothness parameter. And all clients are required to coordinate in advance to know the global information $\\kappa_{max}$, which may be a bit unrealistic.\n\nWhen ProxSkip is applied to Federated Learning, it reduces to Scaffnew. In this case, it requires that each $f_i$ be $L$-smooth and $\\mu$-strongly convex (see **Assumption 4.1** in [1]). Therefore, in a heterogeneous setting where each $f_i$ is $L_i$-smooth, this assumption is satisfied with $L = L_{\\max}$. In the case of ProxSkip, finding $L = L_{\\max}$ involves determining the $L_i$-smoothness constants for each device and then taking their maximum. GradSkip also requires finding the $L_i$ constants, but it benefits from the fact that each $f_i$ has a different $L_i$, leading to less work compared to ProxSkip. Thus, the task of determining $L_i$-smoothness constants remains relevant for ProxSkip as well. Although we did not specifically focus on this aspect in our work, it is worth noting that $L_i$-smoothness can be practically estimated using backtracking line-search strategies (see [2], [3], [4]). Our primary aim was to show that computational complexity can be significantly reduced by exploiting the heterogeneity of devices.\n\n> 2. According to Theorem 3.6, the client gradient query number is improved from $\\sqrt{\\kappa_{max}}$ to $min(\\kappa_i, \\sqrt{\\kappa_{max}})$, while the iteration and communication complexity does not change, the claimed $O(n)$ superiority only appears in scenarios where the devices are very unbalanced (most of them have small $\\kappa$, while few of them attain very large $\\kappa$). As mentioned in your experiments, only one ill-conditioned device), I may view such scenarios to be relatively rare in real world (or it is better if authors can rationalize it). If so the derived improvement seems to be a little bit weak.\n\nIn Figure 3 in the Appendix (last page), we conducted a similar experiment to that in the main part of the paper, using the ``Australian'' dataset from the LibSVM library. In this experiment with 20 devices, we identified 8 ill-conditioned devices, demonstrating that such highly heterogeneous datasets do exist in real-world scenarios. GradSkip showed almost twice the efficiency compared to ProxSkip in terms of total computational complexity. In the fourth column of the results, it is observed that, on average, 9 out of 20 devices perform 5 times less work than the device with the $L_{\\max}$-smoothness constant. Although our experiments on real datasets were limited, we believe that in practical situations where local datasets are highly heterogeneous, significant differences in local smoothness constants $L_i$ are likely. Consider, for instance, a scenario in federated image classification where users from diverse groups contribute their images for model training.\n\n>3. As far as I understand, the proof heavily relies on the proof of ProxSkip, which restricts the significance of the contribution a bit.\n\nSince GradSkip is a generalization of ProxSkip (Scaffnew concretely) by adding an additional randomness, it does rely on the proof of the ProxSkip. But we think that it does not restrict the significance of our contribution. With such a small trick, we can get such a huge advantage over ProxSkip. We think that a small change leading to a large effect is much more surprising and valuable than a large change achieving the same.\n\n\n[1] Mishchenko, K., Malinovsky, G., Stich, S., Richtarik, P.. (2022). ProxSkip: Yes! Local Gradient Steps Provably Lead to Communication Acceleration! Finally!. Proceedings of the 39th International Conference on Machine Learning, in Proceedings of Machine Learning Research 162:15750-15769 Available from https://proceedings.mlr.press/v162/mishchenko22b.html.\n\n[2] Goldstein, A. A. (1962). Cauchy's method of minimization. Numerische Mathematik 4 (1): 146-150.\n\n[3] Armijo, L. (1966). Minimization of functions having Lipschitz continuous first partial derivatives. Pacific Journal of mathematics, 16 (1), 1-3.\n\n[4] Y. Nesterov. (1983). A method of solving a convex programming problem with convergence rate O(1/k2 ). Soviet Mathematics Doklady, 27(2):372\u2013376."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5973/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700401656654,
                "cdate": 1700401656654,
                "tmdate": 1700401656654,
                "mdate": 1700401656654,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GvF7amdANM",
                "forum": "nrctFaenIZ",
                "replyto": "PGDMSjDr9r",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5973/Reviewer_ENdC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5973/Reviewer_ENdC"
                ],
                "content": {
                    "title": {
                        "value": "Thank you"
                    },
                    "comment": {
                        "value": "Thanks authors for the detailed response.\n\nI agree that the resemblance with ProxSkip in terms of technical novelty should not be a main reason for rejection. But I still concern that the main claimed superiority of the proposed algorithm only stands out at a (potentially) relative corner case. So I keep my score here as to reflect my evaluation in terms of the significance. To further enhance the significance, maybe authors can further extend the study into the strictly convex case, or turn to show the optimality of the algorithm (lower bound) with the finer local Lipschitz constant structure. Thank you for the efforts."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5973/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700724890377,
                "cdate": 1700724890377,
                "tmdate": 1700724890377,
                "mdate": 1700724890377,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]