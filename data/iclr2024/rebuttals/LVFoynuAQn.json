[
    {
        "title": "A universal metric of dataset similarity for multi-source learning"
    },
    {
        "review": {
            "id": "GDBr4WdUU1",
            "forum": "LVFoynuAQn",
            "replyto": "LVFoynuAQn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6108/Reviewer_U4Pq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6108/Reviewer_U4Pq"
            ],
            "content": {
                "summary": {
                    "value": "This paper developed a metric to quantify the similarity between two datasets. To obtain the metric, the first step is computing cosine similarity between pairs of data samples, added by Hellinger distance between the same pair of labels. Then, we obtain the distance between the two datasets by applying optimal transport over the cosine and Hellinger distances."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper studies a foundational problem in machine learning and may have a broad impact on many areas."
                },
                "weaknesses": {
                    "value": "1. The paper first constructs a metric between a pair of data samples using cosine similarity and Hellinger distance. However, the Hellinger distance is not well motivated. What's the key advantage of the Hellinger distance over the Wasserstein distance?\n2. The theoretical insights (Section 6) need improvements. The authors consider a scenario where \"any pair of vectors drawn from two random and independent datasets\". Such a scenario may not be representative enough because two different datasets may have overlaps. Also, the Hellinger distance is not discussed in the theoretical analysis.\n3. I do not believe \" estimating gradients similarity without model training\" is a feature of the proposed approach. Deriving a bound of gradient similarity using data similarity is straightforward for a Lipschitz function. In this sense, any metric quantifying data similarity can estimate gradient similarity."
                },
                "questions": {
                    "value": "N/A."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6108/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698793523738,
            "cdate": 1698793523738,
            "tmdate": 1699636659859,
            "mdate": 1699636659859,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "e9IDLi7qDw",
                "forum": "LVFoynuAQn",
                "replyto": "GDBr4WdUU1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6108/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6108/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "***the Hellinger distance is not well motivated. What's the key advantage of the Hellinger distance over the Wasserstein distance?***...***Also, the Hellinger distance is not discussed in the theoretical analysis***\n\nWe thank the reviewer for this observation. We recognize the need for a clearer motivation behind our choice of the Hellinger distance. Our primary reason for using the Hellinger distance is to leverage a bounded cost function that is admissible with the optimal transport framework. Specifically, the Wasserstein distance is a special case of the (f, \u0393)-divergence family where the \u0393 cost function is bounded 1-Lipschitz functions (e.g., L1 or L2 norm) (*Birrell et al., JMLR, 2022*). However, the (f, \u0393)-divergence family framework inherently allows for flexibility in choosing alternate  \u0393 functions provided it possesses certain properties. Given we use the Hellinger Distance to generate the cost map, and the Hellinger Distance uses a bounded, continuous function, it can be substituted. The advantage of this modification is that it shifts interpretation from assessing distribution differences in Euclidean geometry to comparing probability densities within the manifold. This is because the Hellinger distance can be interpreted as representing the shortest geodesic distance in the statistical manifold. This approach provides a meaningful way to compare label distributions, preserving the optimal transport interpretation, while ensuring boundedness and relevance in probability space. We now include this more detailed rationale for Hellinger distance in *Section 6.2*.\n\n***The authors consider a scenario where \"any pair of vectors drawn from two random and independent datasets\". Such a scenario may not be representative enough because two different datasets may have overlaps.***\n\nWe thank the reviewer for this feedback. Our discussion focuses on scenarios involving pairs of vectors drawn from two random and independent datasets as this assumption allows us to produce our orthogonality bound. However, we state that this bound of orthogonality does not hold when datasets have overlap because we can no longer assume independence of the sample means and covariances (*Section S.3.1.4*). This bound and when it is violated motivates our primary reason for using the cosine similarity, i.e., that as datasets become increasingly less similar, more vectors are found to be orthogonal. Our empirical findings align with this argument. We have updated *Section 6.1* to make this assumption and when it is violated clearer.\n\n***I do not believe \" estimating gradients similarity without model training\" is a feature of the proposed approach. Deriving a bound of gradient similarity using data similarity is straightforward for a Lipschitz function. In this sense, any metric quantifying data similarity can estimate gradient similarity.***\n\nWe thank the reviewer for this comment. We would like to clarify that the assertion is not that our metric estimates gradient similarity without model training. Rather, our claim centers on the correlation between our metric and gradient similarity, and how this relationship can explain our metric's ability to estimate model performance without the need for explicit model training. While any Lipschitz continuous function can bound gradient similarity, the effectiveness of our metric lies in its stronger correlation. We attribute this to our specific choice of distances (cosine similarity + Hellinger distance) as we believe they are more closely aligned with the dynamics of neural network training. To test this empirically, we compared the correlation between gradient diversity and two metrics: our proposed metric and the standard Wasserstein distance. Our findings reveal that our metric consistently exhibits a higher correlation with gradient similarity compared to the Wasserstein distance across all datasets (with a statistically significant paired t-test, p=0.028, see *Table S.9*). This empirical evidence suggests that the modifications we have introduced to the conventional Wasserstein distance enhance its ability to estimate model performance. Note, in *Section S5.4.1* we already demonstrate how our metric aligns more closely with model performance compared to the Wasserstein distance."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6108/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700175737538,
                "cdate": 1700175737538,
                "tmdate": 1700175737538,
                "mdate": 1700175737538,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qGL9ARirv3",
                "forum": "LVFoynuAQn",
                "replyto": "GDBr4WdUU1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6108/Reviewer_U4Pq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6108/Reviewer_U4Pq"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Authors"
                    },
                    "comment": {
                        "value": "Thanks for replying. There are some interesting ideas in the comments, such as the specific choice of distances (cosine similarity + Hellinger distance) may be more closely aligned with the dynamics of neural network training. However, they are not very well justified in such a limited space.\n\nTherefore, I hope the authors could carefully revise this work and provide solid technical support to the claims."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6108/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700517796126,
                "cdate": 1700517796126,
                "tmdate": 1700517796126,
                "mdate": 1700517796126,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "u0tJCgCH98",
            "forum": "LVFoynuAQn",
            "replyto": "LVFoynuAQn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6108/Reviewer_8TNG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6108/Reviewer_8TNG"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the multi-source learning setting where different datasets may be non-identically-distributed. This paper improves on prior work by proposing a metric that satisfies many useful properties simultaneously: being bounded, being applicable to supervised learning (via accounting for the label distribution) and not requiring model training. The metric is based on optimal transport, cosine similarity, and hellinger distance. Experimental evaluation focuses on showing that this metric correlates with model performance as data is made more non-iid and with gradient diversity. This work also shows how to compute metrics inn SMPC setting."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This work has several strengths.\n\nFirst, this metric is the first to achieve many desired quantities simultaneously. Though this work does so by using techniques already studied in machine learning (e.g., cosine distance, hellinger distance), it does combine these in a new way that leads to this benefit.\n\nSecond, this work is mostly clear and well written. For example, algorithm 1 clearly shows the cost metric and how it is computed in various settings. The notation is clear and easy to follow. There is also sufficient description of how to interpret the metric (e.g., how values of > 0.5 induce negative learning).\n\nThird, there is sufficient related works and background. This makes it easy to understand the key contributions and placement in the literature, as well as interpret/understand the results.\n\nFourth, there are many experiments, including synthetic and real datasets covering many different cases (regression, multi-class image classification, etc.). The full details are also included enabling reproducibility. The results show a correlation between the metric and the desired quantity being measured: learning performance under varying degrees of non-iid datasets."
                },
                "weaknesses": {
                    "value": "The first weakness is that this work claims privacy-preserving computation as a main contribution. However, this contribution is not clear, lacks any significant treatment of the techniques used in the main-text, and, is also missing important analysis. On clearness, this work claims to enable \"privacy-preserving\" computation many times early in the paper (abstract, fourth main contribution, to name two). This is vague. Does this mean DP, SMPC, or something else? This only becomes clear on the fourth page of the paper when the work first mentions SMPC. Importantly, though, this SMPC contribution is rather limited. On treatment and analysis, it does not include computation analysis, a security proof, and, appears to only use SMPC for the features (and not labels) as observed in Algorithm 1. This conflates the true security guarantees with what is provided as the work claims to provide a \"privacy-preserving method for the metric\". Further, this work also claims to provide a method that introduces no error, but the linked proof is actually a background (supplement S.1).\n\nThe second weaknesses is the empirical performance of the metric. Though there is certainly a correlation, this correlation appears to not be too strong in that it only well separates settings of IID ( where the metric is around 0.1 or lower) to those of heavily non-iid (where the metric is 0.3 or higher). In between this, there is large variation where the metric does not well correlate and has high variance in results across datasets. That being said, this result may be useful in itself, and so, this weakness is not major.\n\nThird, there is lacking exploration with respect to non-iid learning approaches. The result that these approaches can impact utility on iid settings is interesting, however, this work also seems to show that these non-iid approaches often perform worse even in noniid settings. This is counter to their design and requires more exploration. Is there a reason that this is occurring? \n\nNITs:\n\nSupplemental broken citation at top of page 15."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6108/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699248188966,
            "cdate": 1699248188966,
            "tmdate": 1699636659757,
            "mdate": 1699636659757,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "D5RAM7fCni",
                "forum": "LVFoynuAQn",
                "replyto": "u0tJCgCH98",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6108/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6108/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Please note: A revised manuscript has been submitted which we reference in our response**\n\n***claims privacy-preserving computation as a main contribution...***\n\nThank you for your valuable feedback. We realize that our initial presentation of the privacy-preserving methods was not as clear as it could have been. Our primary contribution lies in developing a cost metric that can be calculated in a privacy-preserving way with little computational overhead (see *Section S.5.4.3*). This is desirable for many practitioners in distributed settings where privacy constraints limit multi-source learning. We feel this is an important contribution as, to our knowledge, other dataset similarity measures do not support privacy-preserving calculation. In particular, methods that use model training are not feasible using cryptographic techniques due to the performance overhead. In addition,  differential privacy can lead to considerable performance degradation when applied to complex models (*Bagdasaryan et al., NeurIPS, 2019*). In response to your feedback, we have revised the manuscript to more accurately reflect the scope of our contribution in this area. We also provide clearer guidance on where to find a comprehensive discussion of the methods.\n\n***this SMPC contribution is rather limited.***\n\nWe recognize that the treatment of SMPC in our manuscript does not include a security proof. Since our approach leverages existing methods, we omitted full handling of security guarantees as these have been shown elsewhere. We now make it clearer in the manuscript where such security guarantees can be found. \n\nRegarding computation analysis, we have provided this in *Section S.4.4.3* where we compare the computational cost for the dot product and comparison against the computational cost of Euclidean distance. This analysis demonstrates the feasibility and efficiency of our approach in practical scenarios. We have revised the manuscript accordingly.\n\n***only use SMPC for the features (and not labels)***\n\nWe thank the reviewer for this comment. We initially provided a method for the features as this required sharing of sample-level information whereas label costs are calculated on summary statistics. However, we acknowledge that in the low sample size setting, summary statistics can also leak information. In response to your feedback, we have revised our approach to ensure that both features and labels are treated with appropriate privacy-preserving methods. For labels, we use a zero-Concentrated Differential Privacy (\u03c1-zCDP, an intermediate method between pure and approximate DP) method that enables release of the summary statistics (*Biswas et al., 2020, NeurIPS*) . This method was chosen as it is computationally light and has sample complexity similar to the non-privacy-preserving methods. We show that under a strong privacy guarantee, \u03c1=0.1 or \u03b5-\u03b4= 0.99,0.01, (*Near and Abuah, Programming Differential Privacy, 2021*) we  retain high accuracy in the label cost calculation as we are able to provide the true sample measures a priori (see *Figure S.5.2*). We have amended *Algorithm 1* and added *Section S.2.3* describing how we apply this method in more detail.\n\n***provide a method that introduces no error***\n\nWe thank the reviewer for catching this typo. The manuscript should point to *Section S.2.2* for a theoretical proof and *S.5.1* for the empirical comparison of the plaintext and ciphertext values showing perfect agreement. \n\n***there is lacking exploration with respect to non-iid learning approaches.***\n\nWe thank the reviewer for this insightful comment. Initially, we observed that pFedMe and Ditto performed well in non-IID settings (cost > 0.3) in the majority of datasets (7/7 and 5/7, respectively). However, your observation prompted a more thorough examination of these algorithms, particularly regarding optimization. In our initial analysis we used a default regularization parameter which controls the strength of personalization vs. aggregation (lower regularization means greater personalization). We have now completed a grid search over the regularization parameter and learning rate and found that non-IID model performance can be improved. While this adjustment does not fully bridge the performance gap in all cases, it reduces it. This led to an interesting discovery: there's a clear correlation between regularization strength and our metric. In IID settings, more regularization (favoring aggregation) performs better while in non-IID settings less regularization (favoring personalization) performs better. While this result seems intuitive, it further highlights the practical utility of our metric as complimentary to these non-IID algorithms. We believe our metric can support regularization parameter tuning for practitioners without incurring the high computational cost of a grid search. We have updated the *Section 7.3* and added the regularization parameter results to the tables in *Section S4.1.2*, accordingly"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6108/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700175569658,
                "cdate": 1700175569658,
                "tmdate": 1700521379362,
                "mdate": 1700521379362,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sN0QnNxPWk",
            "forum": "LVFoynuAQn",
            "replyto": "LVFoynuAQn",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6108/Reviewer_qVBz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6108/Reviewer_qVBz"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method to calculate dataset similarity which is model agnostic and does not requires any model training. The similarity score can help guide model training with multiple data sources. The paper provide theoretical intuition on the method and presents empirical evidence showing the correlation between the score and utility of model that is trained on multiple datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The similarity metric seems to be easy to compute and can be helpful for practitioners who want to train models with multiple data sources."
                },
                "weaknesses": {
                    "value": "1. I'm a bit confused about the relation between data similarity and model utility. Intuitively I think the model utility should be improved the most when we add a dataset that is either too similar (e.g. apparently adding the same dataset would not help at all) or too different (e.g. if we invert all labels, the model might become garbage). But the paper seems to suggest that datasets should be as similar as possible, e.g. in the empirical evaluation Fig 1, and theoretical insights (\"When the cosine similarity between x1 and x2 is close to zero, it implies a negligible change in w, leading to minimal improvement in loss\"). This is a bit counter-intuitive to me.\n\n2. I think some important concepts and settings need to be explained in more details (which might help resolve my confusions in (1) as well).\na). What is \u03bc and \u03a3 in the algorithm? I guess they are some Gaussian parameters but I don't understand what distribution we're talking about here. (And I presume the \"u\" in (5) is meant to be \"\u03bc\"?) In general the intuition of the algorithm is not quite clear to me. I think it might be helpful if the authors can demonstrate a few similarity values for some simple cases, e.g. when D1 and D2 are the same, when one is a subset of the other, or when they're fresh samples drawn from the same distribution etc.\nb). In the experiments, how is the data partitioned to form datasets with different level of similarities? I think it's important to examine whether these artificially created datasets reflect the real life scenario where slightly different datasets might be owned by different institutions who want to jointly train model."
                },
                "questions": {
                    "value": "(Those mentioned in the previous question.)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6108/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699512472774,
            "cdate": 1699512472774,
            "tmdate": 1699636659644,
            "mdate": 1699636659644,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pkEteA3F9t",
                "forum": "LVFoynuAQn",
                "replyto": "sN0QnNxPWk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6108/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6108/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "**Please note: A revised manuscript has been submitted which we reference in our response**\n\n***the relation between data similarity and model utility....***\n\nWe thank the reviewer for their question. When we refer to datasets as 'similar,' we imply that they have similar distribution.  In our results we observe that more similar datasets improve performance and dissimilar datasets worsen performance. This finding aligns with the widely accepted notion that, under the scenario that test-sets are drawn from the same distributions, combining IID datasets improves model performance, but adding non-IID datasets may worsen performance (*Kairouz et al.,arXiv, 2019,  Lin et al., ICML, 2021*). \n\nWe also explored scenarios involving datasets with inverted labels or with deliberately contrasting feature distributions per your point. We found that the cost is higher than what we would expect from two random datasets i.e.,> 0.5 and that model performance is no better than chance. We believe this is in keeping with your intuition *\u2018if we invert all labels, the model might become garbage\u2019*. We have added this to the manuscript *Figure S.5*) \n\n***and theoretical insights When the cosine similarity between x1 and x2 is close to zero...***\n\nIn the theoretical section we are referring to the cosine of the angle between the vectors which ranges from [-1,1], *i.e.,* a score of 0 implies orthogonality. We then use (1-cosine of the angle) in our similarity calculation to ensure the cost is non-negative. We have amended the text to make this clearer.\n\n***think some important concepts and settings need to be explained in more details...***\n\nWe thank the reviewer for this observation. In the algorithm, \u03bc and \u03a3 represent the label mean and covariance matrix. As stated in Section 5. Proposed framework, we model labels as distributions over their features. We have amended the algorithm to clarify what \u03bc and \u03a3 represent. We have also amended u to \u03bc in *eq.5*.\n\n***demonstrate a few similarity values for some simple cases...***\n\nWe thank the reviewer for this question. We use the synthetic dataset to demonstrate various scenarios. As detailed in *Section S.4.1.1* and *Table S.1*, for the synthetic dataset, our approach involves drawing samples from two distinct distributions, varying how much of the two distributions the datasets share. This setup allows us to systematically explore and quantify the similarity under different conditions:\n\n1. **Datasets drawn from the same distribution:** Both datasets are drawn from the same distribution. We obtain a score of 0.03 (i.e,. highly similar datasets) and find a large improvement in model performance from multi-source learning (MSL).\n\n2. **Datasets drawn from different distributions**: Each dataset is  drawn from a distinct distribution. We obtain a score 0.5 (i.e., datasets are random to each other) and find that  performance is worse in MSL.\n\n3. **Identical Datasets**: The datasets are totally identical. We obtain a score of 0  i.e., datasets can be mapped at no cost).\n\n4. **Dataset is a subset of another**: We subsample (50%) of one dataset to produce the other dataset. We obtain a similarity score of 0.01 i.e., highly similar datasets.\n\nNote points 3 and 4 are new analysis based on your suggestion and have been added to the manuscript in S4.1.1 and Table S.1.\n\n***how is the data partitioned to form datasets***\n\nTo create datasets with varying levels of similarity, we employed different strategies depending on the dataset. *Section S4.1.1* provides a more detailed overview of the approach. As a summary:\n\n1. **Datasets Without Natural Partition** (Credit, EMNIST, CIFAR-100): For datasets that do not have an inherent or natural partitioning, we implemented label and feature skew as described by *Hsieh et al., ICML, 2019*. These are established techniques for generating non-IID datasets in the federated learning literature.\n2. **Datasets With Natural Partition (Weather, ISIC, IXITiny)**: \n\n     2a. Weather: We partitioned the data based on climate as described by *Malinin, et al., NeurIPS, 2021*.\n\n     2b. ISIC and IXITiny: For these medical imaging datasets, we partition based on the site and equipment used for data collection as described by du Terrail, Ayed et al., NeurIPS, 2022 \n\n***examine whether these artificially created datasets reflect the real life scenario***\n\nWe address this using the ISIC and IXITiny medical imaging datasets, which originate from different institutions and utilize different imaging machines. In *Section 7.2*, we provide a discussion for how our metric captures real-world differences in these datasets and matches intuition on which datasets are likely to be similar. Notably, our analysis with the ISIC dataset reveals an intriguing insight: the type of imaging machine plays a more critical role in dataset similarity than the geographical location of the sites. This finding underscores the practical relevance and applicability of our metric in real-world scenarios."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6108/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700175230725,
                "cdate": 1700175230725,
                "tmdate": 1700521311832,
                "mdate": 1700521311832,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]