[
    {
        "title": "LeBD: A Run-time Defense Against Backdoor Attack in YOLO"
    },
    {
        "review": {
            "id": "XsoxBOjVw8",
            "forum": "7vKWg2Vdrs",
            "replyto": "7vKWg2Vdrs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1696/Reviewer_JMJB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1696/Reviewer_JMJB"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a real-time backdoor attack detection system for Deep Neural Networks, specifically on the YOLOv5 object detector. Utilizing LayerCAM and counterfactual attribution, the proposed detectors, LeBD and CA-LeBD, aim to locate and mitigate backdoor triggers efficiently. Experiments in both digital and physical settings show that the methods work for patch-based triggers."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper tries to solve the backdoor attacks on object detection tasks. To the best of my knowledge, few papers focus on this important open problem. \n\n2. The work improves upon the NEO algorithm by enhancing efficiency and relaxing blocker size constraints.\n\n3. Incorporating counterfactual attribution to enhance LayerCAM is a novel and intriguing approach."
                },
                "weaknesses": {
                    "value": "1. The proposed techniques are specialized for defending against patch-based attacks (BadNets-like patterns). It remains unclear whether these methods are effective against other forms of backdoor triggers, such as rotational triggers [1], semantic triggers [2], and augmentation-based triggers [3]. Notably, reference [1] demonstrates the real-world applicability of rotation-based backdoors in object detection models.\n\n2. The experimental evaluation is limited to the YOLO-v5 architecture for object detection. The authors have not explored the generalizability of their approach to other object detection models. Further experiments across diverse YOLO architectures, such as YOLOv7 [4], are highly recommended. \n\nTypo: LayrCAM at page 5 \n\n[1] Wu et al. Just rotate it: Deploying backdoor attacks via rotation transformation. AISec 2022\n\n[2] Bagdasaryan et al. How To Backdoor Federated Learning. AISTATS, 2020.\n\n[3] Rance et al. Augmentation Backdoors. ArXiv 2022. \n\n[4] Wang et al. YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors, CVPR 2023."
                },
                "questions": {
                    "value": "Can the proposed method extend to vision transformer architectures?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1696/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698083940061,
            "cdate": 1698083940061,
            "tmdate": 1699636098194,
            "mdate": 1699636098194,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "eLSShwMfzc",
                "forum": "7vKWg2Vdrs",
                "replyto": "XsoxBOjVw8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1696/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1696/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer JMJB"
                    },
                    "comment": {
                        "value": "We thank the reviewer for valuable time reviewing our work and for many very insightful suggestions on this work. We respond to the questions as below.\n\n> W1\uff1aThe proposed techniques are specialized for defending against patch-based attacks (BadNets-like patterns). It remains unclear whether these methods are effective against other forms of backdoor triggers, such as rotational triggers [1], semantic triggers [2], and augmentation-based triggers [3]. Notably, reference [1] demonstrates the real-world applicability of rotation-based backdoors in object detection models.\n\nFirstly, we want to emphasize that this article focuses on backdoor defenses in the physical world. In this setting, the attack must also be feasible in the physical world, which means the backdoor trigger must be physically deployable. This is also why we choose BadNets-like patterns as the trigger. Secondly, in a broad sense, the rotational triggers, semantic triggers, and augmentation-based triggers you mentioned also fall into BadNets-like triggers. Because they all satisfy the formula $\\hat x = \\left( {1 - m} \\right) \\odot x + m \\odot \\Delta $ as described in Section 3.1 mathematically. To be specific, rotational triggers and semantic triggers take objects that exist in the physical world as triggers. But what kind of object or pattern to choose as a trigger makes no difference to our defenses. As long as LayerCAM locates the trigger, we occludes it and achieve the backdoor detection purpose. Augmentation-based triggers can\u2019t bypass our defenses too, because it studies the backdoor injection process. It utilizes GAN-based or AugMix method to make the augmented image approximate the poisoned image in pixel values or gradients during the network training period, which increase the stealth of backdoor injection. During the testing period, a real and visible trigger is required to activate the backdoor. Therefore, it is also a BadNets-like attack to the defenders and our methods can work on it.\n\n> W2: The experimental evaluation is limited to the YOLO-v5 architecture for object detection. The authors have not explored the generalizability of their approach to other object detection models. Further experiments across diverse YOLO architectures, such as YOLOv7 [4], are highly recommended.\n\nYOLOv5 is currently widely used, so we focus on performing backdoor defense on it. The failure of classical GradCAM in locating trigger drives us to propose an effective strategy for yolov5 backdoor defense. Through abundant experiments and analysis, we figure out the reason of GradCAM failure, and put forward a low-latency backdoor defense algorithm based on LayerCAM, which can be applied in a real time object detection scene in the physical world. According to our analysis, two-stage object detection networks, especially YOLO series networks have similar problems as YOLOv5 in locating trigger by GradCAM. Our research provides a good reference for backdoor defense of such networks. We will also supplement experiments on such network structures in the subsequent revised version.\n\n> Typo: LayrCAM at page 5.\n\nWe have correct the typo in the revised version.\n\n> Q: Can the proposed method extend to vision transformer architectures?\n\nThe original intention of our proposed algorithms is to solve the problem that GradCAM cannot locate the trigger in the backdoored YOLOv5 network. This problem is mainly attributed to the fact that yolov5 is an anchor-based object detection system as described in Section 3.2. We can't say for sure that the vision transformer architectures will have similar issues without digging into it. If GradCAM can locate the trigger in the vision transformer architectures, we still recommend using it rather than LayerCAM. Because LayerCAM has better quality on saliency maps in shallow layers of the network, whereas GradCAM performs better in deep layers. Therefore, GradCAM requires less computation than LayerCAM. We will still verify the validity of our approach on vision transformer architectures in our follow-up work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1696/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700215870055,
                "cdate": 1700215870055,
                "tmdate": 1700215870055,
                "mdate": 1700215870055,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GKWBsh3hRc",
                "forum": "7vKWg2Vdrs",
                "replyto": "eLSShwMfzc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1696/Reviewer_JMJB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1696/Reviewer_JMJB"
                ],
                "content": {
                    "comment": {
                        "value": "Thank the authors for the detailed reply. I have some further questions. \n\n>As long as LayerCAM locates the trigger, we occlude it and achieve the backdoor detection purpose.\n\nIn the literature, like rotation triggers [1], the attacker uses \"rotation angle\" as the trigger. That is if the stop sign is rotated to, e.g., 30 degrees, then the object detector will not detect the stop sign. Here, if the LayerCAM still locates and occludes the stop sign, I don't think LayerCAM can detect the sign. That means if the trigger is an actual object and should be detected correctly, LayerCAM cannot work. \n\n> Experiments for further exploration of different architectures. \n\nLooking forward to seeing those results. I think they can significantly strengthen the paper."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1696/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700501723643,
                "cdate": 1700501723643,
                "tmdate": 1700501723643,
                "mdate": 1700501723643,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ELNLgiX5a4",
            "forum": "7vKWg2Vdrs",
            "replyto": "7vKWg2Vdrs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1696/Reviewer_4B6q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1696/Reviewer_4B6q"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores how to defend against backdoor attacks for YOLOv5 in real-world scenarios. Specifically, the authors first argue that the only capable solution is saliency-map-based methods due to efficiency requirements. After that, they reveal three failure modes of directly using GradCAM for YOLOv5, based on which the author proposes to exploit LayerCAM to replace GradCAM. The authors evaluate their method on the COCO dataset and real-world settings with three baseline defenses."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The idea is easy to follow.\n2. The topic is of sufficient significance.\n3. The authors took into account real scenarios, which should be encouraged."
                },
                "weaknesses": {
                    "value": "1. The scope is limited. The authors focus on only one particular model structure, even though this structure is currently widely used. However, in the near future, it is likely that people will no longer use this model structure. As such, the authors should try to construct a method that works well for different model structures rather than focusing on just one model structure.\n2. The technical contributions are limited. Technically, this work is a simple extension to the GradCAM-based one by replacing GradCAM with LayerCAM, which is proposed in the previous work. More importantly, why this method is used instead of another CAM method seems to need to be analyzed.\n3. Missing important experiments. Firstly, the author should evaluate the proposed method under different trigger patterns, especially those scattered ones and those not located in the center of the bounding box.\n4. There is no discussion about the resistance to potential adaptive attacks. What if the attackers know this defense? Can they design an adaptive method to bypass this defense easily?"
                },
                "questions": {
                    "value": "Please refer to the 'Weaknesses' part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1696/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1696/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1696/Reviewer_4B6q"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1696/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698545998734,
            "cdate": 1698545998734,
            "tmdate": 1699636098110,
            "mdate": 1699636098110,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "id0ghJA92P",
                "forum": "7vKWg2Vdrs",
                "replyto": "ELNLgiX5a4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1696/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1696/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 4B6q"
                    },
                    "comment": {
                        "value": "We thank the reviewer for valuable time reviewing our work and for many very insightful suggestions on this work. We respond to the questions as below.\n\n> W1: The scope is limited. The authors focus on only one particular model structure, even though this structure is currently widely used. However, in the near future, it is likely that people will no longer use this model structure. As such, the authors should try to construct a method that works well for different model structures rather than focusing on just one model structure.\n\nAs you say, YOLOv5 is currently widely used, so we first perform backdoor defense on it. Our previous experiments on YOLOv5 demonstrate that classical GradCAM does not work on locating trigger. This prompts us to dig into the causes of this phenomenon and propose an effective strategy for yolov5 backdoor defense. We find that the failure of GradCAM is due to the anchor-based network structure as described in Section 3.2. According to our analysis, two-stage object detection networks, especially YOLO series networks will have similar problems. Our research provides a good reference for backdoor defense of such networks. We will also verify the validity of our approach in such networks in our follow-up work.\n\n> W2: The technical contributions are limited. Technically, this work is a simple extension to the GradCAM-based one by replacing GradCAM with LayerCAM, which is proposed in the previous work. More importantly, why this method is used instead of another CAM method seems to need to be analyzed.\n\nAs mentioned above, we find GradCAM doesn\u2019t work in YOLOv5, so we look for other ways to locate the backdoor trigger. Certainly, we also tried a number of other CAM methods and interpretability methods along the way. To be specific, GradCAM, GradCAM++ and Guided GradCAM fail to locate the trigger like GradCAM. ScoreCAM, smooth ScoreCAM and Ablation CAM are tome consuming. Considering the requirement of high backdoor detection rate and low latency, LayerCAM is the best solution we can find at present.\n\n> W3: Missing important experiments. Firstly, the author should evaluate the proposed method under different trigger patterns, especially those scattered ones and those not located in the center of the bounding box.\n\nFor the completeness of the experiment, it is necessary to evaluate different trigger patterns. But from a principle point of view, different trigger patterns have few effect on the performance of our methods, as long as the backdoor attack is BadNets-like. We will supplement relevant experiments in the appendix of the later revised version.\n\n>W4: There is no discussion about the resistance to potential adaptive attacks. What if the attackers know this defense? Can they design an adaptive method to bypass this defense easily?\n\nAdaptive attacks do not bypass CAM-based backdoor detection methods, which has been verified in [1]. We will still supplement relevant experiments in the appendix of the later revised version for further verification.\n\n[1] B. G. Doan, E. Abbasnejad, and D. C. Ranasinghe. Februus: Input Purification Defense against Trojan Attacks on Deep Neural Network Systems."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1696/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700215868211,
                "cdate": 1700215868211,
                "tmdate": 1700215868211,
                "mdate": 1700215868211,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XUUZbZYVTT",
                "forum": "7vKWg2Vdrs",
                "replyto": "id0ghJA92P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1696/Reviewer_4B6q"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1696/Reviewer_4B6q"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your responses. However, since you failed to address my concerns. I keep my score unchanged. Specifically,\n\n1. Please experiment on at least the non-YOLO-based approach.\n2. It is not a technical contribution. Besides, you failed to provide in-depth analyses about how to find a suitable CAM method.\n3. Please show me the results.\n4. There are still many other potential backdoor defenses (other than Februus). For example, the adversaries can design a strong adaptive attack by including your method as a regularization."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1696/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700559751160,
                "cdate": 1700559751160,
                "tmdate": 1700559751160,
                "mdate": 1700559751160,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kKEpPpt14x",
            "forum": "7vKWg2Vdrs",
            "replyto": "7vKWg2Vdrs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1696/Reviewer_dW4R"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1696/Reviewer_dW4R"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed an input-level backdoor detection, specifically aiming at the object detection task. The main idea is that 1) exploiting the counterfactual attribution (CA) LayerCAM to locate the crucial region, which leads to the final prediction output, 2) occluding the chosen region of the original image; 3) putting original image and the occluded image into the object detection model and comparing their outputs. If the two outputs are different, the crucial region is considered as the trigger."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The main contribution is that the authors reimplement this old trick in the new object detection domain."
                },
                "weaknesses": {
                    "value": "The main idea has been exploited by Februus (Doan et al., 2020) and also following unmentioned reference.. Considering this, I don\u2019t think there is enough novelty to publish it on ICLR.\n[1] Chou, Edward, Florian Tramer, and Giancarlo Pellegrino. \"Sentinet: Detecting localized universal attacks against deep learning systems.\" 2020 IEEE Security and Privacy Workshops (SPW). IEEE, 2020."
                },
                "questions": {
                    "value": "What the proposed method will do when the chosen region is the ground-truth feature? For instance, assume there is a \u2018face\u2019 object in the object detection task. Given a benign image with a human face, the CA layerCAM locates the ground-truth facial area as the most important area of the \u2018face\u2019 object and then occludes this area. I can expect that there exists a label flipping in this case. I doubt whether the proposed work may have a high false positive ratio for trigger detection or not."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1696/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698696769353,
            "cdate": 1698696769353,
            "tmdate": 1699636098023,
            "mdate": 1699636098023,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yoCMosr5hZ",
                "forum": "7vKWg2Vdrs",
                "replyto": "kKEpPpt14x",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1696/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1696/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dW4R"
                    },
                    "comment": {
                        "value": "We thank the reviewer for valuable time reviewing our work and for many very insightful suggestions on this work. We respond to the questions as below.\n\nWe must declare that the contribution of this paper is not a simple reimplementation of \u201cold tricks in the new object detection domain\u201d. We acknowledge that Februus was an inspiration to our work. But we found that GradCAM in Februus couldn't locate the trigger in YOLOv5 as shown in Figure 2,7 and Table 6. We analyze and explain this phenomenon in section 3.2, and propose practical defense schemes to meet the real-time requirements in the physical world. \n\n> What the proposed method will do when the chosen region is the ground-truth feature? For instance, assume there is a \u2018face\u2019 object in the object detection task. Given a benign image with a human face, the CA layerCAM locates the ground-truth facial area as the most important area of the \u2018face\u2019 object and then occludes this area. I can expect that there exists a label flipping in this case. I doubt whether the proposed work may have a high false positive ratio for trigger detection or not.\n\nFor your concern about false positive ratio, we evaluate it in the experiment (see FP). According to the experimental results, our algorithms will not raise high false alarm. Given a benign image, CA layerCAM scores the importance of each pixel. The intuitive understanding is that the \u201cface\u201d area will have a higher score than the background. But CA-LeBD does not occlude the entire \u201cface\u201d because we set the hyper-parameters to constraint the max and min ratio of occluded region to bounding box. After partial occlusion of \u201cface\u201d, the remaining \u201cface\u201d still contributes to accurate object detection and classification. \n\nBy the way, the saliency map calculated based on gradients does not quantificationally represent the extent to which different regions of the image contribute to the classification (see Figure 2 in [1]). As a result, for a benign image, occlusion of the region with highest score of CA LayerCAM does not necessarily flip the label. While for a poisoned image, the occlusion destroys the integrity of the trigger. When enough of the trigger is occluded, the remaining trigger cannot facilitate the attack, thus our defense succeeds.\n\n[1] H. Wang et al. Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1696/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700215865345,
                "cdate": 1700215865345,
                "tmdate": 1700215865345,
                "mdate": 1700215865345,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dkmcVteJqA",
            "forum": "7vKWg2Vdrs",
            "replyto": "7vKWg2Vdrs",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1696/Reviewer_9mm5"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1696/Reviewer_9mm5"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed an approach of defensing backdoor attack in YOLO at run-time. Specifically, the proposed LayerCAM-enabled backdoor detecotr (LeBD) utilized LayerCAM to locate the backdoor trigger, aiming to addressing the real-time requirement of application scenes in the physical world."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The study focuses on an interesting and important topic, the run-time defense against backdoor attacks in object detection network.\n+ The paper is well-written and easy to follow.\n+ The idea of using LayerCAM to locate the trigger is inspiring."
                },
                "weaknesses": {
                    "value": "- The digital world and physical world\n\nIf my understanding is correct, one of the key motivations is that the existing defense focuses more on backdoor attacks in the \"digital world\" rather than attacks in the \"physical world.\" However, I would suggest a more detailed and explicit definition of the digital world and the physical world. It would be better and necessary to provide a more in-depth description and explanation of why this assumption is sound. For example, you could discuss the main constraints that limit the application of backdoor attacks in the physical world. I found this assumption somehow confusing, as it suggests that a backdoored sample with a pixel-level trigger can still be printed and placed in the physical world.\n\n- The performance of LeBD in the digital world\n\nIn Table 1, although the discussion explains that \"In the physical world, affected by the shooting angle, light, and so on\", photographed triggers are more vulnerable to defenses, the performance gaps of the proposed LeBD and CA-LeBD between the digital world and the physical world are still much larger than those observed in benchmarks. It appears that the performance of the proposed approach is highly influenced by the strength of backdoor attacks, with weak triggers leading to more significant performance improvements. Please provide more discussion on this point. Another concern is, the experiments in digital world scenario only involves the same backdoor attack in the physical world scenario, however, there are more attacks can be applied in the digital world, as described in previous sections. \n\n- The application scenario of CA-LeBD\n\nIn Section 5.1, it has been claimed that \"although LeBD and CA-LeBD are inferior to NEO, they are much faster than the latter\". However, according to the experimental results in Section 5.4, the runtime overhead of CA-LeBD could be several times higher than NEO (if applied to all 80 classes). Please provide more discussion on how to apply CA-LeBD in practice. For instance, how to determine the appropriate number of classes when using CA-LeBD and how it might influence the defense performance."
                },
                "questions": {
                    "value": "1. Please define the digital and physical worlds with a more detailed definition and explain why other backdoor attacks are hard to be applied in the physical world.\n2. Why the performance gap of proposed approach is much higher than benchmarks?\n3. How to determine the appropriate number of classes protected in defense when using CA-LeBD and how it might influence the defense performance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1696/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1696/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1696/Reviewer_9mm5"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1696/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698834465972,
            "cdate": 1698834465972,
            "tmdate": 1699636097942,
            "mdate": 1699636097942,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YBBHePkTji",
                "forum": "7vKWg2Vdrs",
                "replyto": "dkmcVteJqA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1696/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1696/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9mm5"
                    },
                    "comment": {
                        "value": "We thank the reviewer for valuable time reviewing our work and for many very insightful suggestions on this work. We respond to the questions as below.\n\n* Q1: The digital world and physical world\n\nThe physical world and digital world can be distinguished based on when the trigger is attached to the sample. In the digital world, an adversary first obtains a benign image and then makes some modifications to it to create a poisoned sample. In the physical world, the adversary needs to ensure that the image shot by the camera is itself the desired poisoned sample, which means the backdoor trigger needs to exist in the real world.\n\nAs you say, most defenses focus on backdoor attacks in the digital world. However, most backdoor attacks in the digital world can hardly be deployed in the physical world, especially in the real-time object detection scene, because triggers of these attacks (e.g. blended[1], Poison frogs[2], PoisonInk[3], WaNet[4]) need to make pixel-by-pixel modifications to benign images. These triggers cannot be printed in advance. In contrast, using a Badnets-like pattern as the trigger is a feasible scheme in the physical world. On the one hand, the trigger can be pre-printed. On the other hand, pattern-like triggers ensure the attack successful rate because the adversary can make various transforms to the trigger during the backdoor injection phase to increase the robustness. We will add detailed explanations in the introduction of the revised version.\n\n* Q2: The performance of LeBD in the digital world\n\nThere are three factors that cause the performance gap of proposed approaches much higher than benchmark. First, we set very good prior hyper-parameters for NEO as described in Appendix C. For example, the trigger blocker is larger than the trigger and the step size ensures at least 1/4 of the trigger is blocked. However, it is worth noting that these priors are not available in the actual scene. Besides, although such parameter settings have a good backdoor detection rate (TP), they also bring a high false alarm (FP). Second, although LayerCAM can roughly figure out where the trigger is, the location isn't precision all the time. The hot region in the saliency map sometimes doesn't perfectly align the trigger. LeBD utilizes the connect graph to find suspicious regions, which also increases the risk of misalignment between the occluded region and the trigger. By contrast, CA-LayerCAM locates more accurately and CA-LeBD achieves better performance. Third, the backdoor attack is robust and succeeds even if part of the trigger is occluded in the digital world, which has been extensively verified in the existing work. However, in the physical world, affected by lighting, shooting angle and photo pixel quality, the attack is naturally weakened and its robustness to trigger occlusion is reduced. So the performance in the physical world is higher than that in the digital world. It should be emphasized that the focus of this article is the backdoor defense in the physical world. At this point, our algorithms have comparable backdoor detection rates with the baseline while consume fewer time. We will add more explanation on this question in Section 5 of the revised version.\n\nIn addition, you mentioned \u201cthe experiments in digital world scenario only involves the same backdoor attack in the physical world scenario\u201d. We have to emphasize again that this article focuses on backdoor defenses in the physical world. As a result, we just consider backdoor attacks that can be deployed in the physical world. As explained in Q1, BadNets-like attacks are the most threatening.\n\n* Q3: The application scenario of CA-LeBD\n\nThe first thing we need to be clear about is that as long as the source class of the backdoor attack is contained in the classes to perform CA-LayerCAM, the backdoor detection rate won't decrease. And the running time increases almost linearly with the number of classes, which is apparent from line 4-19 in Algorithm 2. To this end, we have conducted additional experiments and find that CA-LeBD is still slightly faster than NEO even in the case of 20 classes. \n\nThe total classes of YOLOv5 is 80 because the network is trained on the COCO dataset. But we might just care about a few classes among them, and some classes will even never appear in a real usage scene. For example, in road monitoring, we only focus on vehicles and pedestrians rather than elephants, books and so on. Therefore, we only need to perform CA-LayerCAM on these classes. We think 5 classes are sufficient for most practical scenes.\n\nIn addition, CA-LeBD is parallel for different classes, which is also a way to decrease runtime. \n\n[1] X. Chen. Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning.\n\n[2] A. Shafahi. Poison frogs! Targetedclean-label poisoning attacks on neural networks.\n\n[3] J. Zhang. Poison Ink: Robust and Invisible Backdoor Attack.\n\n[4] Nguyen. WaNet - Imperceptible Warping-based Backdoor Attack."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1696/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700215858248,
                "cdate": 1700215858248,
                "tmdate": 1700215858248,
                "mdate": 1700215858248,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]