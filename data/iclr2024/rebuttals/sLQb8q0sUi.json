[
    {
        "title": "Fair and Efficient Contribution Valuation for Vertical Federated Learning"
    },
    {
        "review": {
            "id": "G9IKBeU1jJ",
            "forum": "sLQb8q0sUi",
            "replyto": "sLQb8q0sUi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8996/Reviewer_WWnM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8996/Reviewer_WWnM"
            ],
            "content": {
                "summary": {
                    "value": "This paper seeks to assign each client a fair value in the vertical federated learning setting where clients share the same sample IDs but own different features. The paper proposes VerFedSV which consider clients\u2019 contributions at multiple time stamps during training. VerFedSV satisfy desirable properties of fairness, can be efficiently computed without model training and extra communication cost.\nFor the synchronous setting, computing VerFedSV will require the embedding of all data points. However, only the embedding for a mini-batch is available. The paper proposed how to obtain the embedding matrix through low rank matrix completion and show that error in the estimated VerFedSV is bounded.\nFor the asynchronous setting, VerFedSV can be directly and exactly computed for the mini batch. VerFedSV will reward clients with more computational resources (batch size) and frequent communication."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well-written. The content and notations are easy to follow.\n- The study of related work is comprehensive and it is clear how the paper contrasts with prior work.\n- The assumptions (e.g., low rank embedding matrix) are sound and justified (e.g., due to similarity between local data points and during successive training).\n- The paper novelly consider data valuation in the _vertical_ federated learning setting and the _asynchronous_ setting with different local computational resources."
                },
                "weaknesses": {
                    "value": "- The paper did not discuss or empirically compare to alternatives, e.g., why not request embedding for all data points or value based on the updated mini-batch or a separate validation set? The experiments lack comparison to a ground-truth (request all embeddings) and other baselines (VFL valuation methods).\n- The solution introduced in the work does not seem very novel after considering existing VFL literature and HFL data valuation work. Fan et al. (2022) have used low rank matrix completion to estimate utilities of subsets. In the introduction, it is mentioned that it is more challenging to compute Shapley value to VFL than HFL as it is not applicable (possible) to obtain the global model by concatenating private local models. The challenge has to be further clarified. Is the challenge resolved by using local embedding (as in other VFL work)?"
                },
                "questions": {
                    "value": "Questions\n1. What are the reasons not to use the asynchronous valuation approach (page 7, use mini-batch only) for the synchronous setting?\n2. Existing data valuation works (including those for HFL) usually use a separation validation set for the utility function. If a validation set is used, is the embedding matrix still needed? \n\n\nMinor suggestions\n* The Shapley value is often misspelled as Sharpley, Sharply, Shaprley etc\n* The paper can give some examples/explanation of models and $f$ for the reader to understand why the embedding is sufficient. E.g., what is $f$ for linear and logistic regression (in Sec. 3)\n* After proposition 1, the notation/arguments for the coverage number should be explained.\n* For VAFL, the client has to send the id of the embedding too.\n* Report N and T for the experiments"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8996/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8996/Reviewer_WWnM",
                        "ICLR.cc/2024/Conference/Submission8996/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8996/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698642977507,
            "cdate": 1698642977507,
            "tmdate": 1700704744400,
            "mdate": 1700704744400,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dEXTHbDdHz",
                "forum": "sLQb8q0sUi",
                "replyto": "G9IKBeU1jJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8996/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8996/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely appreciate your positive comments and valuable feedback on our paper. Following your feedback, we have made significant efforts to conduct comparison experiments. We are confident that these efforts have greatly enhanced the quality and strength of our paper. In light of this enhancement to the paper, we kindly hope that you could consider re-evaluating your score. We are confident that our work now better addresses your concerns, and we hope that the updates better reflect the contribution we aim to make to the field. We reply to each of the weaknesses and questions below. \n\n## Reply to Weakness 1 \n\nThank you for your valuable feedback. We extend the experiments to compare with alternatives including request embeddings for all data points and based on update mini-batch. \n\nWe first define a deviation metric for the upcoming comparison experiments. For client $i$, denote its percentage of VerFedSV relative to the total VerFedSV from all clients as $s_i$. These VerFedSVs are calculated using matrix completion to estimate all embeddings. Meanwhile, we compute a ground-truth scenario where full embeddings are calculated in each training round. In this scenario, the percentage of Shapley value for client i relative to the sum of all Shapley values is denoted as $s_i^{gt}$. With $M$ representing the number of clients, the metric Deviation(VerFedSV, Ground truth) is defined as $\\frac{1}{M} \\sum_{m=1}^{M} \\frac{|s_m - s^{gt}_m|}{s^{gt}_m}$. \nSimilarly, the metric Deviation(Mini-batch, Ground truth) measures the deviation of Shapley value computed with mini-batch from the ground-truth scenario. \n\nThe comparison results are shown as follows. The deviation metric is shown in mean$\\pm$std format, where mean and std are calculated from 5 independent runs. \n| | Adult | Web | Covtype | RCV1 |\n|-|-|-|-|-|\n| VerFedSV | $0.058\\pm 0.039$| $0.095\\pm 0.051$ | $0.309\\pm 0.041$ | $0.558\\pm 0.117$ |\n| Mini-batch | $0.047\\pm 0.150$ | $0.110 \\pm0.300$ | $0.319\\pm 0.057$ | $0.559\\pm 0.268$ |\n\nTo conclude, VerFedSV stably achieves low deviation since it can accurately recover the full embeddings. In contrast, the Shapley values under the Mini-batch scenario exhibit a larger variance than VerFedSV. \n\nPreparing a separate validation set under the VFL scenario is a non-trivial research topic. In the VFL scenario, the server should not possess any knowledge about the client dataset, including even the feature dimensions. The server and clients have to collaborate prudently and complicatedly to prepare and utilize the validation set while properly addressing the privacy issue. Considering this challenge, we propose using the full training set directly. This approach offers several advantages. Firstly, it utilizes the entire training set for computing Shapley value, thereby avoiding potential errors introduced by sampling mini-batches. Additionally, it ensures that the computation process of the Shapley value does not leak private information and that the resulting Shapley value remains private to each client. \n\nFor baselines, we would like to draw the reviewer's attention to SHAP [1] baseline in Section D.2. This experiment shows that VerFedSV reflects the importance of clients\u2019 local features. \nHowever, to the best of our knowledge, no other suitable baseline exists that considers both privacy and fairness simultaneously.\nActually, the SHAP baseline cannot be directly applied to the VFL task, as it requires access to all the local datasets and may breach privacy. \nMeanwhile, the baselines involving the model-independent utility would cause fairness issue in the asynchronous VFL scenario, since they rely on the client's data but ignores the computational resources contributed by the client. To explain this, we have an example in Sec. 7.2. Imagine two clients with identical datasets. Using the model-independent utility function, both clients would be evaluated as having equal contributions. However, in an asynchronous environment, Client 1 may have superior hardware and transmission power. This advantage allows for a larger batch size and facilitates more frequent communication, resulting in a higher number of asynchronous training rounds. In such a scenario, Client 1's contribution should be greater, yet the model-independent utility function fails to recognize this, leading to an unfair assessment.\n\n[2] Scott M. Lundberg, Su-In Lee. A Unified Approach to Interpreting Model Predictions. NeurIPS 2017: 4765-4774"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8996/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700615402423,
                "cdate": 1700615402423,
                "tmdate": 1700615402423,
                "mdate": 1700615402423,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G9WqzHCxBo",
                "forum": "sLQb8q0sUi",
                "replyto": "G9IKBeU1jJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8996/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8996/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "## Reply to Weakness 2 \n\nThank you for your feedback regarding the technical novelty of our learning-based basis selection approach. We understand your concerns and appreciate the opportunity to emphasize our contributions:\n\n- We addressed an important problem rarely explored in the literature, that is, how to fairly value the contributions of different clients in the field of vertical federated learning (VFL).  \n- Our proposed VerFedSV utilized the local embedding to construct the utility function. We have indeed employed techniques developed by pioneers in related fields (like local embeddings), but our work combines these techniques and applies them in a novel context. \n- VerFedSV is applicable for both synchronous and asynchronous VFL. In synchronous VFL, we utilize matrix completion to obtain the full embedding of all training sets. We demonstrate that the target embedding matrices are approximately low-rank and provide an approximation guarantee for VerFedSV.  Under the asynchronous VFL setting, we show that VerFedSV successfully reflects the strength of clients' local computational resources. \n\nTo conclude, our integrated VerFedSV provides a pioneering and exemplary framework for contribution evaluation in VFL, and this approach constitutes a solid contribution. \n\n## Reply to Question 1 \n\nIn the asynchronous setting, each client may require different amounts of time to complete a training iteration and may have varying batch sizes. However, in the synchronous setting, all clients have the same time per iteration and the same batch size. \nAs described in Section 6, our approach in the asynchronous setting involves the transmission of mini-batches from the clients to the server to update the server embeddings. There is no need to use matrix completion so that the Shapley value is proportional to the client's computational strength. \nIn the synchronous setting, we complete the full embedding, because, in this way, the entire training set is utilized for computing Shapley value, effectively avoiding potential errors from mini-batch sampling.\n\n## Reply to Question 2 \n\nPlease refer to our response in Weakness 1 regarding preparing and utilizing a separate validation set in the VFL setting. \n\n## Reply to Minor Suggestions \n\n**Misspellings** We will ensure the misspellings are thoroughly correct in the final version. \n\n**Explanation for why the embeddings are sufficient** For linear regression, $f$ is mean square error, and for logistic regression, $f$ is cross-entropy loss. The client model can be a linear model or a neural network. \nWe next explain why VerFedSV only relies on embeddings and is irrelevant to the function form of $f$ or whether the task is one-class or multi-class classification. \n\nWe first recap the one-class case and then explain the multi-class case. In one-class case, we defined the embedding matrix $\\mathcal{H}^m \\in \\mathbb{R}^{T \\times N}$ where each element $(t,i)$ is defined as $\\mathcal{H}_{t,i}^m = (h_i^m)^{(t)}$. In the context of the multi-class classification problem with $l > 2$ classes, the model parameter $\\theta_m$ is in space $\\mathbb{R}^{d^m \\times l}$. Consequently, the embedding $h_i^m = \\theta_m^\\intercal x_i^m$ is in $\\mathbb{R}^l$. Now, the embedding $\\mathcal{H}^m$ is a tensor in $\\mathbb{R}^{T \\times N \\times l}$. We reorder the tensor dimensions and lower it to a matrix $\\mathcal{H}^m \\in \\mathbb{R}^{(l\\cdot T) \\times N}$. \n\nRegarding the general function case, $h_i^m = \\theta_m^\\intercal x_i^m$ becomes $h_i^m = g(x_i^m; \\theta_m)$, where $g$ can be a neural network with weights $\\theta_m$. VerFedSV by Definition 2 in Section 4 only requires $h^i_m$ and does not depends on does not depend on the specific form of $g$. \n\n**Explain covering number** Covering number $\\mathcal{N}(\\mathcal{D},\\epsilon)$ represents the minimum number of \"balls\" or \"sets\" with radius $\\epsilon$ that are needed to cover a given dataset $\\mathcal{D}$. In other words, it quantifies the complexity of $\\mathcal{D}$. Please refer to Definition 5 in Section B for detailed formulation. \n\n**Clients send the IDs** The client will send the batch $B$ that contains the IDs of data points. We will revise it in the final version. \n\n**Report $N$ and $T$** Please refer to Table 2 in Section C for the number of training data points $N$ in different datasets. As for $T$, under the synchronous setting, it is 160, 160, 800, and 320 for each dataset, and under the asynchronous setting, it is 100. We apologize for any confusion caused and assure to be clarified in the final version."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8996/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700615490891,
                "cdate": 1700615490891,
                "tmdate": 1700615733241,
                "mdate": 1700615733241,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s2HeRvMTTL",
                "forum": "sLQb8q0sUi",
                "replyto": "G9WqzHCxBo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8996/Reviewer_WWnM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8996/Reviewer_WWnM"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed response! I have raised my score."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8996/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700704712173,
                "cdate": 1700704712173,
                "tmdate": 1700704712173,
                "mdate": 1700704712173,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "GvbVoIy2Y2",
            "forum": "sLQb8q0sUi",
            "replyto": "sLQb8q0sUi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8996/Reviewer_RX3e"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8996/Reviewer_RX3e"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a metric for contribution valuation in vertical federated learning (VFL). The method is based on federated Shapley value and is adapted to synchronous VFL via low-rank embedding matrix completion techniques and extended to asynchronous VFL. This metric reflects both the quality of the local dataset and the power of local computational resources."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper is overall well-structured and the authors made good efforts in putting this paper in the literature.\n\n2. Low-rank matrix completion of the embedding matrix resolves the issue that its updates are only partially observed in the mini-batch setting. This technique facilitates the application of Shapley value in VFL.\n\n3. The proposed method VerFedSV is theoretical grounded in this work. It is important to verify that VerFedSV satisfies the criteria of balance, symmetry, zero element, and additivity. Detailed proofs and conditions are provided.\n\n4. The authors experimentally validate the methods on real-world datasets and discussed the results."
                },
                "weaknesses": {
                    "value": "1. Given the setup, it appears that the proposed metric works well only linear models or training only the last linear layer. Although the authors mentioned that results can be extended to a multi-class classification problem and general function, little concrete explanations are provided.\n\n2. There seems be no baselines or comparison with other methods. I understand that contribution evaluation in VFL might not be well-studied before but at least Data Shapley Value can be used here."
                },
                "questions": {
                    "value": "1. In VERFEDSV, would it be inefficient to sum over all subsets $S \\subset M$?\n\n2. What is the difference between the method in section 6 and FedSV?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8996/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8996/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8996/Reviewer_RX3e"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8996/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698875846277,
            "cdate": 1698875846277,
            "tmdate": 1700718322096,
            "mdate": 1700718322096,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nCjZFyr1bc",
                "forum": "sLQb8q0sUi",
                "replyto": "GvbVoIy2Y2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8996/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8996/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely appreciate your positive comments and valuable feedback on our paper. We reply to each of the weaknesses and questions below. \n\n## Reply to Weakness 1 \n\nThank you for your valuable feedback. Our experiments indeed encompass multi-class (e.g., on Covtype) and general function cases (e.g., 2-layer perceptron model on RCV1). We will ensure providing concrete explanations in the final version. \n\nWe first recap the one-class case and then explain the multi-class case. In one-class case, we defined the embedding matrix $\\mathcal{H}^m \\in \\mathbb{R}^{T \\times N}$ where each element $(t,i)$ is defined as $\\mathcal{H}_{t,i}^m = (h_i^m)^{(t)}$. In the context of the multi-class classification problem with $l > 2$ classes, the model parameter $\\theta_m$ is in space $\\mathbb{R}^{d^m \\times l}$. Consequently, the embedding $h_i^m = \\theta_m^\\intercal x_i^m$ is in $\\mathbb{R}^l$. Now, the embedding $\\mathcal{H}^m$ is a tensor in $\\mathbb{R}^{T \\times N \\times l}$. We reorder the tensor dimensions and lower it to a matrix $\\mathcal{H}^m \\in \\mathbb{R}^{(l\\cdot T) \\times N}$. \n\nRegarding the general function case, $h_i^m = \\theta_m^\\intercal x_i^m$ becomes $h_i^m = g(x_i^m; \\theta_m)$, where $g$ can be a neural network with weights $\\theta_m$. Since our method does not depend on the specific form of $g$ and only requires $h_i^m$, it can be extended to the general function case. Admittedly, Proposition 1 on the bound of embedding matrix\u2019s $\\epsilon$-rank may not necessarily hold. However, in practice (See Section 7.1, RCV1 dataset) we still observe that embedding is approximately low-rank, so matrix completion is still an effective strategy. \n\n## Reply to Weakness 2 \n\nThank you for your feedback on the baselines. We would like to mention that data Shapley value [1] can be easily extended to the HFL scenario but not to the VFL scenario. This is because VFL's clients partition the dataset along feature dimensions, while the data Shapley value [1] was designed to assess the importance of individual data points. \n\nWe would like to draw the reviewer's attention to our extra experiment in Section D.2, where SHAP [2] is employed as a baseline to test whether VerFedSV reflects the importance of clients\u2019 local features. It is worth noting that the SHAP baseline cannot be directly applied to the VFL task, as it requires access to all the local datasets and may breach privacy. Nevertheless, the results presented in Table 5 of Section D.2 indicate that VerFedSV is indeed capable of capturing feature importance.\n\n[1] Amirata Ghorbani, James Y. Zou. Data Shapley: Equitable Valuation of Data for Machine Learning. ICML 2019: 2242-2251\n[2] Scott M. Lundberg, Su-In Lee. A Unified Approach to Interpreting Model Predictions. NeurIPS 2017: 4765-4774\n\n## Reply to Question 1 \n\nIn VerFedSV, it will be inefficient to sum over all subsets $S\\subset[M]$. However, this is a common issue in Shapley Value and well-studied in literature. Various sampling methods [1,3] are proposed for this issue. In our experiment, as mentioned in Section C, VerFedSV is computed exactly when the number of clients $M \\le 10$, and approximately using Monte-Carlo sampling when the number of clients $M > 10$. \n\nVerFedSV is efficient because it does not require model retraining. The key idea is to calculate the Shapley value for a client in each iteration of training and then aggregate these values over all iterations to determine the final contribution of the client. \n\n[3] Jiayao Zhang, Qiheng Sun, Jinfei Liu, Li Xiong, Jian Pei, Kui Ren. Efficient Sampling Approaches to Shapley Value Approximation. Proc. ACM Manag. Data 1(1): 48:1-48:24 (2023)\n\n## Reply to Question 2\n\nFedSV [4] and VerFedSV are applied in different scenarios -- HFL and VFL, respectively. Furthermore, it's worth noting that FedSV is specifically designed for the synchronousFL setting and relies on training rounds. In contrast, VerFedSV in Section 6 applies to the asynchronous FL setting. \n\n[4] Tianhao Wang, Johannes Rausch, Ce Zhang, Ruoxi Jia, Dawn Song. A Principled Approach to Data Valuation for Federated Learning. Federated Learning 2020: 153-167"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8996/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700614862148,
                "cdate": 1700614862148,
                "tmdate": 1700614862148,
                "mdate": 1700614862148,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "saJ88pgfIq",
                "forum": "sLQb8q0sUi",
                "replyto": "nCjZFyr1bc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8996/Reviewer_RX3e"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8996/Reviewer_RX3e"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reply. I have raised my score."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8996/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718309986,
                "cdate": 1700718309986,
                "tmdate": 1700718309986,
                "mdate": 1700718309986,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7h5qff6yqk",
            "forum": "sLQb8q0sUi",
            "replyto": "sLQb8q0sUi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8996/Reviewer_n6Bm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8996/Reviewer_n6Bm"
            ],
            "content": {
                "summary": {
                    "value": "This paper extends the FedSV framework by Wang et al. (2020) to vertical federated learning setting. For synchronous setting, since the server does not have access to the data embeddings to the full dataset, the authors propose an approximation algorithm based on matrix completion given the low rank assumption. For asynchronous setting, since there are no definition of training iterations for the server, the time stamps for contribution valuation is pre-determined. The author conducted experiments to demonstrate the effectiveness of the proposed technique."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The idea of using matrix completion is super interesting."
                },
                "weaknesses": {
                    "value": "The writing can be improved quite a bit. \n- For example, $d^m$ is used for denoting the feature dimension of client $m$, but $m$ here can be easily misunderstood as exponent. Better to use $d^{(m)}$. \n- Quite a few places use \"Sharpley\". \n- It's good to provide some background about matrix completion algorithm, at least in Appendix. \n- In Proposition 1, what is $D^m$? What is $\\gamma^m$? Again, $m$ can be easily misunderstood as exponent. \n- Proposition 3 can be better stated as an example instead of Proposition. \n\nI am also not too sure about the experiments.\n- What is the dimension of each dataset being used? \n- What does Table 1 supposed to tell? Is it fair to compare the Shapley value of 1 artificial client against all regular clients? \n- How to set the hyperparameters of $r$ and $\\lambda$? I think ablation study is needed."
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8996/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8996/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8996/Reviewer_n6Bm"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8996/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699067072654,
            "cdate": 1699067072654,
            "tmdate": 1700701714557,
            "mdate": 1700701714557,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1LDGqJPrBr",
                "forum": "sLQb8q0sUi",
                "replyto": "7h5qff6yqk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8996/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8996/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely appreciate the valuable suggestions you provided during the review process. Following your suggestions, we have devoted great effort to an expanded ablation study. We believe that these improvements have strengthened the paper substantially. In light of these efforts and the resulting improvements to the paper, we kindly hope that you could consider re-evaluating your score. We are confident that our work now better addresses your concerns, and we hope that the updates better reflect the contribution we aim to make to the field. We reply to each of the weaknesses and questions below.\n\n## Reply to Weakness on Writing \n\nThank you for your suggestions on writing and clarity. We will revise $d^m$ to $d^{(m)}$ to prevent potential misinterpretation and will thoroughly correct the typos. \n\nConcerning the utilization of low-rank matrix completion, we initially introduced its motivation, formulation, and application in Section 5. We will further enrich the backgrounds for its solving algorithm in the appendix, including a taxonomy of existing algorithms and more detailed explanations. \n\nIn Proposition 1, $\\mathcal{D}^{(m)}$ is the local data set for client $m$ defined in Section 3. Regarding the term $\\gamma^{(m)}$, we deeply regret any confusion it may cause. This term is defined in Section B.2 as the bound on the $L_2$ norm of the model parameters. To be specific, it is denoted as $\\gamma^{(m)} = \\max_{t \\in [T]}\\|\\theta_m^{(t)}\\|$. \n\nFinally, in line with your recommendations, we will revise the notation $\\cdot^m$ to $\\cdot^{(m)}$ and state Proposition 3 as an Example to enhance understanding. \n\n## Reply to Questions on Experiments \n\n**Question 1** For the dimension of each dataset, please refer to Table 2 (Metadata of all data sets) in the paper. \n\n**Question 2** Table 1 reveals that artificial clients, which submit randomly generated features, are assigned substantially lower Shapley values compared to regular clients providing real features. In this table, the Shapley value of one artificial client is compared against that of the regular clients. This setup\u2019s purpose is to show that artificial clients are indeed evaluated as having lower contributions. \n\n**Question 3** The hyperparameter $r$ is based on approximated $\\epsilon$-rank in our experiment. As for $\\lambda$, we observe that the performance of VerFedSV remains relatively insensitive across a wide range of $\\lambda$. To support these observations, we conduct new ablation studies as follows. \n\nWe define a deviation metric for the upcoming ablation study. For client $i$, denote its percentage of VerFedSV relative to the total VerFedSV from all clients as $s_i$. These VerFedSVs are calculated using matrix completion to estimate all embeddings. Meanwhile, we compute a ground-truth scenario where full embeddings are calculated in each training round. In this scenario, the percentage of Shapley value for client i relative to the sum of all Shapley values is denoted as $ s_{i}^{gt} $ . With $M$\nrepresenting the number of clients, the deviation metric is defined as \n$\\frac{1}{M} \\sum_{m=1}^{M} \\frac{|s_m - s^{gt}_m|}{s^{gt}_m} $. \n\nPlease refer to this [anonymous link](https://drive.google.com/file/d/14273cwj05-QrdUab_AXa8wSnswo5QHki/view?usp=drive_link) for the results. \n\nThe left part of the figure displays the deviation w.r.t the hyperparameter $r$, based on five independent runs. The median deviation of these runs is depicted by the solid line, while the shaded area represents the interquartile range (25% to 75% quantiles) of deviations. Take the Adult dataset as an example for analysis. The figure demonstrates that deviations are significant for $r<5$, since inadequate rank cannot accurately recover full embeddings. However, deviation decreases rapidly as $r$ increases and stabilizes at a low value when $r\\ge 5$. \n\nPractically, obtaining full embeddings is resource-intensive due to communication costs or limited resources on the client side. Thus, approximating $\\epsilon$ rank may be difficult. However, the server (with sufficient computational resources), after collecting the mini-batch embeddings, can test with several increasing values of $r$. The Shapley value will converge to the ground-truth value as revealed by our ablation study. \n\nThe right part of the figure illustrates the deviation w.r.t the hyperparameter $\\lambda$. It demonstrates that the performance of VerFedSV remains relatively insensitive across a wide range of $\\lambda$ and we can safely set $\\lambda=0.1$ for various datasets."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8996/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700614646419,
                "cdate": 1700614646419,
                "tmdate": 1700614646419,
                "mdate": 1700614646419,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "R1dkRX4SR0",
                "forum": "sLQb8q0sUi",
                "replyto": "1LDGqJPrBr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8996/Reviewer_n6Bm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8996/Reviewer_n6Bm"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your reply. For Table 1, I might have some misunderstanding but why not compare it with the **average** Shapley value of regular clients?"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8996/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631891447,
                "cdate": 1700631891447,
                "tmdate": 1700631891447,
                "mdate": 1700631891447,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "SdANDEig2J",
                "forum": "sLQb8q0sUi",
                "replyto": "Bpiek3RYtm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8996/Reviewer_n6Bm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8996/Reviewer_n6Bm"
                ],
                "content": {
                    "comment": {
                        "value": "In my opinion, I think a number of 0.01% does not really tell anything to me. There could be billions of regular clients which makes 0.01% a super large number. Furthermore, in data valuation, indirect evaluation metrics such as mislabeled/noisy data detection are also very common (e.g., [1, 2, 3]). I will raise the score if the authors can fix the issue in Table 1. \n\nI like the new ablation study and it's good to incorporate them into the Appendix. One question is that will the groundtruth $g_m^{GT}$ be 0 or negative? Then does the deviation metric make sense in this case?\n\nA side note about related work: Both Data Shapley and distributional Data Shapley are relatively old works (proposed in 2019 and 2020, respectively). I think it's good to review more recent advances in Shapley value-based data valuation (e.g., [1, 2, 3]). \n\n[1] Kwon, Yongchan, and James Zou. \"Beta Shapley: a Unified and Noise-reduced Data Valuation Framework for Machine Learning.\" AISTATS 2022.\n\n[2] Wang, Jiachen T., and Ruoxi Jia. \"Data banzhaf: A robust data valuation framework for machine learning.\" AISTATS 2023.\n\n[3] Wang, Jiachen T., et al. \"Threshold KNN-Shapley: A Linear-Time and Privacy-Friendly Approach to Data Valuation.\" NeurIPS 2023."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8996/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672125235,
                "cdate": 1700672125235,
                "tmdate": 1700672125235,
                "mdate": 1700672125235,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vpS1gf8W3R",
                "forum": "sLQb8q0sUi",
                "replyto": "7h5qff6yqk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8996/Reviewer_n6Bm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8996/Reviewer_n6Bm"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your response. I have raised my score. Please also update the pdf. \n\nIf all clients' Shapley value is shifted by the minimum of the Shapley value, then is $g_m^{GT}=0$ for the client with the smallest Shapley value?"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8996/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700701861326,
                "cdate": 1700701861326,
                "tmdate": 1700701883407,
                "mdate": 1700701883407,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PvxEqanZVC",
                "forum": "sLQb8q0sUi",
                "replyto": "uxhU1cQCeW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8996/Reviewer_n6Bm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8996/Reviewer_n6Bm"
                ],
                "content": {
                    "comment": {
                        "value": "If there is a $g_m^{GT}=0$, does the deviation metric still make sense?\n\nI think it's good for the author to incorporate the committed material into the updated pdf before the end of the discussion period given that the conference provides such an option. It can help the reviewer to understand these additional results in a more comprehensive way and I would then be more comfortable recommending acceptance."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8996/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700705580200,
                "cdate": 1700705580200,
                "tmdate": 1700705580200,
                "mdate": 1700705580200,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "A6pRy7zys9",
                "forum": "sLQb8q0sUi",
                "replyto": "7h5qff6yqk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8996/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8996/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. As we mentioned earlier, in the new ablation study (verifying hyper-parameter sensitivity of VerFedSV), $s_i^{gt}$ is positive, because we have this setup: clients equally partition all features and thus each client has some contributions. The deviation metric makes sense in this context. \n\nConsidering the above ablation study has already verified the conclusion on hyper-parameter sensitivity, we did not test on more general cases mentioned above (where $s_i^{gt}$ can be negative). We will add the detailed setup, intermediate result (the value of $s_i^{gt}$), and the additional results to the updated pdf now and upload it soon."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8996/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708226334,
                "cdate": 1700708226334,
                "tmdate": 1700708370849,
                "mdate": 1700708370849,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7PZr4AYmRR",
            "forum": "sLQb8q0sUi",
            "replyto": "sLQb8q0sUi",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8996/Reviewer_skLQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8996/Reviewer_skLQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new contribution valuation metric called VerFedSV for vertical federated learning. The VerFedSV metric is based on the classic Shapley value, which is a provably fair contribution valuation metric originating from cooperative game theory. The paper shows that VerFedSV satisfies many desirable properties of fairness and is quite adaptable such that it can be applied under both synchronous and asynchronous VFL settings. The paper also provides experimental results that demonstrate the effectiveness of VerFedSV in terms of fairness and efficiency."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "S1. This paper proposes a new Shapley value-based contribution valuation metric called VerFedSV for both synchronous and asynchronous vertical federated learning. \n\nS2. This paper provides a theoretical analysis of the VerFedSV metric and demonstrates its effectiveness through experimental results. \n\nS3. This paper addresses an important problem in the field of vertical federated learning, namely how to fairly value contributions from \ndifferent clients."
                },
                "weaknesses": {
                    "value": "W1. In the last paragraph of Sec. 2, it is said that model-independent utility function itself may cause some fairness issues in asynchronous scenario and cannot fully reflect a client\u2019s contribution. This claim seems not intuitive, it is better to give a concrete example to explain the fairness issues in detail.\n\nW2. There are missing related studies on Shapley Value in federated learning. Please conduct a more detailed literature survey.\n[1] Sunqi Heng et al. ShapleyFL: Robust Federated Learning Based on Shapley Value. KDD 2023. \n[2] Zhenan Fan et al. Fair and efficient contribution valuation for vertical federated learning. 2022. \n\nW3. In Sec. 3, there aren\u2019t enough explanations on the meaning of embedding h_i^m. How to compute it and how it is used to compute the loss are not mentioned either.\n\nW4. It is said in Sec. 8 that VerFedSV increases when adding identical features, which may indicate the proposed method is fragile and lacking robustness in practical use, especially in asynchronous setting.\n\nW5. There are so many typos. For example, in the first paragraph of Sec. 4, the terminology \u201cShapley\u201d is mistakenly written as \u201cSharply\u201d, \u201cShaprley\u201d, \u201cSharply\u201d for so many times."
                },
                "questions": {
                    "value": "Beyond the above weak points, I also have the following questions:\n\nQ1. Please explain the fairness issues caused by model-independent utility function in asynchronous scenario in details. It\u2019s better to give an example.\n\nQ2. Is this paper the first work with model-dependent utility functions in VFL? Are there really just two related works in VFL scenario?\n\nQ3. Please give more explanation about the meaning of embedding h_i^m and how to compute it, which is more friendly to read for people not quite familiar with VFL."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8996/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8996/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8996/Reviewer_skLQ"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8996/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699343027686,
            "cdate": 1699343027686,
            "tmdate": 1700708071345,
            "mdate": 1700708071345,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RRwgqMD0s0",
                "forum": "sLQb8q0sUi",
                "replyto": "7PZr4AYmRR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8996/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8996/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely appreciate your positive comments and valuable feedback on our paper. We have addressed your comments and made the necessary corrections to improve the paper. We would like to draw your attention to the novelty and broader impact of our research and hope that you might reconsider increasing the score based on the comprehensive merits of our research. Thank you once again for your time and valuable input. We reply to each of the weaknesses and questions below. \n\n## Reply to Weaknesses\n\n**Regarding Weakness 1:** Thank you for your insightful feedback. The model-independent utility function relies on the client's data but ignores the computational resources contributed by the client. This oversight can lead to unfairness in the asynchronous scenario. \n\nActually, we have an example in Sec. 7.2. Imagine two clients with identical datasets. Using the model-independent utility function, both clients would be evaluated as having equal contributions. However, in an asynchronous environment, Client 1 may have superior hardware and transmission power. This advantage allows for a larger batch size and facilitates more frequent communication, resulting in a higher number of asynchronous training rounds. In such a scenario, Client 1's contribution should be greater, yet the model-independent utility function fails to recognize this, leading to an unfair assessment. We provided detailed numerical results in Sec. 7.2 to support this example. Based on your valuable feedback, we will enhance the clarity of this explanation in Sec. 2 to better communicate the potential fairness issues arising from the model-independent utility function in asynchronous settings.\n\n**Regarding Weakness 2:** We appreciate your suggested references and acknowledge the importance of a comprehensive literature survey in this area. \nOur work VerFedSV is different from the existing works like ShapleyFL [1]. ShapleyFL applies Shapley Value in FL against malicious clients, while VerFedSV focuses on computing Shapley Value efficiently in the VFL scenario. \nWe will include more literature works in the final version of our paper, including but not limited to, the references you suggest, more recent works on Shapley Value and its extension to HFL and VFL. \n\n[1] Sunqi Heng et al. ShapleyFL: Robust Federated Learning Based on Shapley Value. KDD 2023 \n\n**Regarding Weakness 3:** Thank you for pointing out the need for a clearer explanation of the embedding $h_i^m$ in Sec. 3. The term $h_i^m$ represents the embedding of a local data point $x_i^m$ from the $m$-th client, transformed by the local model's parameters $\\theta_m$. For linear and logistic regression models, this embedding is computed as $h_i^m = \\langle \\theta_m, x_i^m \\rangle = \\theta_m^\\intercal x_i^m$. Then, each client uploads local embeddings $h_i^m$ to the server. The server aggregates these embeddings to obtain a global embedding $h_i$ for $i$-th data point, i.e., $h_i = \\sum_{m=1}^M h_i^m$.\nThis global embedding is employed to compute the loss, denoted by $f(h_i,y_i)$. Here, $f$ is a differentiable function. For linear regression, mean square error is employed, and for logistic regression, cross-entropy loss is employed. \n\nWe illustrated these concepts in Sec.3 and we will make sure they are more clearly explained in the final version. \n\n**Regarding Weakness 4:** We have included this particular weakness in the future work section. However, it does not mean the weakness cannot be solved. One potential solution is to implement secure statistical testing before FL training and exclude clients with extremely similar datasets. This would be an orthogonal and promising future direction. We will further clarify it in the paper. \n\n**Regarding Weakness 5:**  We sincerely apologize for the typos and will thoroughly correct them.\n\n## Reply to Questions\n\n**For Question 1**, see our reply to Weakness 1.\n\n**For Question 2**, to our best knowledge, this paper is the first work of model-dependent utility function in VFL. Regarding related works of Shapley value in the VFL, we acknowledge the rapidly evolving dynamics within this research community, so we will surely update the final version of our paper with a comprehensive review. \n\n**For Question 3**, see our reply to Weakness 3."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8996/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700614302254,
                "cdate": 1700614302254,
                "tmdate": 1700614302254,
                "mdate": 1700614302254,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ecFbzWRElc",
                "forum": "sLQb8q0sUi",
                "replyto": "LISLCdZOPS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8996/Reviewer_skLQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8996/Reviewer_skLQ"
                ],
                "content": {
                    "title": {
                        "value": "Response to author feedback"
                    },
                    "comment": {
                        "value": "I have carefully read the response and I have no further questions."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8996/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707842192,
                "cdate": 1700707842192,
                "tmdate": 1700707842192,
                "mdate": 1700707842192,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BA6S07rgxW",
                "forum": "sLQb8q0sUi",
                "replyto": "LISLCdZOPS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8996/Reviewer_skLQ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8996/Reviewer_skLQ"
                ],
                "content": {
                    "comment": {
                        "value": "I have raised my score."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8996/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708127886,
                "cdate": 1700708127886,
                "tmdate": 1700708127886,
                "mdate": 1700708127886,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]