[
    {
        "title": "The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World"
    },
    {
        "review": {
            "id": "FphlXDXcsJ",
            "forum": "c2R7ajodcI",
            "replyto": "c2R7ajodcI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1804/Reviewer_5eEK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1804/Reviewer_5eEK"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a large dataset and model for detailed VQA and captions about image regions. The data engine involves the use of a combination of localization models, contrastive vision language models, and other LLMs/VLLMs, as well as humans in the loop to verify the outputs. The resulting dataset has 1.2B regions covering a wide range of 3.5M concepts. The authors also propose a model to ingest this data and handle both discriminative/generative tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper addresses an important space of problems that existing vision and language foundational models are focused on image-level understanding, and there's a clear need to build region-level vision and language foundational model. The proposed dataset is based on the recent SA-1B dataset and extend it with semantic tags/QA pairs, and detailed caption, all of which can be useful to the community."
                },
                "weaknesses": {
                    "value": "1. It'd be helpful to get some analysis on the quality of the final data after human verification. Appendix B.3 shows the accuracy of automatic annotation is around 50-60%. How much of the error is fixed by human verification, and how much is still there? \n\n2. I'm wondering if it'd be better to set apart a high-quality split for region-level validation/testing of captioning. Existing dataset don't seem to serve this purpose very well e.g. RefCOCOg is not intended for region-level captioning. Visual genome is not a common benchmark for captioning evaluation either.\n\n3. Image-level captioning in Table 3 is helpful, but not the focus of this work in my view. To make this more complete, it might be good to add COCO captions too.\n\n4. To make a strong claim on region level understanding, I feel that the model should be able to predict regions from images rather than accepting regions as input. For example, in Table 4, it'd be more useful to have a simple ASM model that can predict regions without groundtruth box inputs.\n\n5. It'd be great to have a region-level VQA benchmark as well since the dataset includes VQA. I see the image-level VQA results in Table 10, but that does not seem to capture the uniqueness of this data.\n\n6. It'd be helpful to have some analysis on bias/fairness considerations."
                },
                "questions": {
                    "value": "See weaknesses. My main concerns are with the data quality and evaluation benchmarks based on this dataset/model."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Discrimination / bias / fairness concerns",
                        "Yes, Privacy, security and safety",
                        "Yes, Legal compliance (e.g., GDPR, copyright, terms of use)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The dataset is built on SAM-1B using LLM and VLLMs. There could be potential bias/fairness concerns."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1804/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698952919793,
            "cdate": 1698952919793,
            "tmdate": 1699636109785,
            "mdate": 1699636109785,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "jc2nId5M3e",
            "forum": "c2R7ajodcI",
            "replyto": "c2R7ajodcI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1804/Reviewer_WmLf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1804/Reviewer_WmLf"
            ],
            "content": {
                "summary": {
                    "value": "The papers presents a large-scale dataset collected using a semi-automatic data engine for open-world panoptic visual understanding. The dataset consists of 1 Billion + region annotations spanning semantic tags (3.5 Million +), question-answer pairs (3.3 billion) as well as detailed captions (1.2 billion). The paper proposes a VLLM called All-Seeing Model (ASM) trained on this dataset consisting of a location aware image tokeniser and a LLM based decoder. ASM achieves promising results on image and region-level captioning and recognition tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper does a great job explaining the details related to the dataset. The appendix contains several details that help understand the semi-automatic approach mentioned in the paper better (percentage of annotations from LLMs/VLLMs, accuracy of automatic annotations.\n- The paper presents a fairly exhaustive benchmark (VQA, OK-VQA in supplementary). The paper also attempts to evaluate the model\u2019s performance on region-based tasks like region-based visual question answering by conducting human studies.\n- The paper also presents a good summary of many factors that are responsible for improving the performance of the model such as the role of data-engineering (D.3)"
                },
                "weaknesses": {
                    "value": "- Phrase Grounding Evaluation: The proposed method also missed an opportunity to leverage the dataset to learn the ability to ground language into the image by generating the bounding boxes corresponding to the text. I would have liked to see the models performance on the phrase grounding task on Flickr30K Entities.\n- I think the paper misrepresents the state of the art in the community. For instance, the claim that current systems \u201care primarily focused on understanding images as a whole, rather than recognizing and comprehending individual instances within the scene\u201d seems ungrounded, and several state of the art systems (e.g, Unified IO, including more recent ones like KOSMOS-2) show a fairly good understanding of the image on benchmarks that test this visual grounding like referring expressions, and phrase groundings. Since the authors compare and cite KOSMOS-2, for completeness the authors should also put the proposed dataset (AS-1B) in perspective of other comparable datasets such as GRiT which consists of region annotations for 90 million images.\n- The paper uses LORA to fine-tune the LLM on various tasks COCO, VQA, etc which is different from other methods (BLIP, InstructBLIP, etc) that the method have compared against. This makes the evaluation unfair because these evaluations heavily penalise the peculiarities of the the evaluation protocol (one-word answers as opposed to natural language generation). Since methods like BLIP use a frozen LLM, it\u2019s much harder for them to conform to the expected style of answers as opposed to the ASM which adapts the LLM using LORA."
                },
                "questions": {
                    "value": "See weaknesses section"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1804/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698989127121,
            "cdate": 1698989127121,
            "tmdate": 1699636109722,
            "mdate": 1699636109722,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "HNMZEvqVx5",
            "forum": "c2R7ajodcI",
            "replyto": "c2R7ajodcI",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1804/Reviewer_Wviv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1804/Reviewer_Wviv"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors present a large dataset and model for panoptic visual understanding, collectively named the \"All-Seeing Project\".\nThe dataset (AS-1B) contains more than one billion region-text pairs, where the text comprises semantic tags, question-answer pairs, and captions.\nText in AS-1B entails a rich vocabulary of visual concepts, e.g. the authors state the presence of 3.5 million unique semantic tags.\nThe authors design a scalable, semi-automatic data collection engine to collect AS-1B --\ntheir pipeline is composed of several large vision models that generate region-text annotations,\nand human annotator to verify the correctness of the generated annotations.\nThe authors train All-Seeing Model (ASM) using their dataset and show strong empirical performance on several downstream vision-language tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "1. AS-1B is a large dataset of region-text pairs, perhaps currently the largest of its kind.\n2. The design choice of using the same images as SA-1B is excellent to mitigate ethical risks regarding copyright and privacy of users,\n   as these images are meticulously verified by Meta AI, and released with a permissible license for research.\n3. The proposed model (ASM) achieves strong empirical performance on several region-level visual understanding tasks.\n4. The design of ASM allows it to be \"composable\" in a larger system that may include localization models like SAM."
                },
                "weaknesses": {
                    "value": "**Note:**\nI wrote my review a few days ago, but it comes late due to my oversight -- apologies for the delay.\nIn the interest of time, I have updated my initial asessment to incorporate the authors' rebuttal.\nMy outstanding concerns do not require the authors to run additional experiments.\n\nI concur with the authors that the main contribution of this paper is a large-scale dataset.\nAS-1B is currently the largest dataset of its kind (to the best of my knowledge)\nand its availability will open new avenues for empirical study in the research community.\nI view the model (ASM) as an important, however secondary contribution --\nit serves as a baseline to provide a strong guarantee of data quality,\nthat training with AS-1B can yield strong empirical improvements across several task benchmarks.\n\nMy remaining concerns listed below are all geared towards ensuring a stronger guarantee of data quality\nand repsonsible release of the dataset.\n\n1. **Collection engine prone to hallucinations:**\nThe authors use large language models (LLMs) in the \"imaginator\" and \"splitter\" to produce semantic tags that are NOT conditioned on the visual content.\nThe imaginator produces plausible semantic tags that are _likely_ to occur, but not guaranteed to occur in images.\nIs there is a way to quantify the amount of hallucination by checking the response of human annotators?\nI suggest the authors to provide ample of qualitative examples in the paper showing the initial pool of semantic tags *before* they are assigned to the region proposals.\n\n2. **Redundant caption annotations:**\nThe detailed caption of a region is produced by paraphrasing three question-answer pairs.\nBased on the limited examples in the paper, the captions sound like a \"dry\" paraphrasing of the question-answer pairs (understandably so).\nI wonder if having such redundancy contributes to the uniqueness of AS-1B, or simply adds redundancy and increases the size of dataset.\n\n3. **Consider adding a datasheet:**\nThe authors should consider adding a datasheet () or a similar supplemental material outlining the characteristics of AS-1B.\nFor example, the Segment Anything paper includes a datasheet for SA-1B.\nDatasheets serve as as a medium of communication between the authors (creators of the dataset) and future works (users of the dataset).\nMany papers published in NeurIPS datasets track have datasheet templates which can be suitable in the ICLR format,\ne.g. some image-text datasets like [LAION-5B](https://arxiv.org/abs/2210.08402) and [RedCaps](https://arxiv.org/abs/2111.11431).\n\n4. **Needs a train/val split:**\nI agree with the other reviewers' assessment that the authors should consider defining a train/val split for AS-1B.\nIf the authors do not define a split, different future works will regardlessly split AS-1B in ad-hoc ways and lead to inconsistencies.\nI suggest the authors to \"hold out\" ~1% data as a validation set for the sake of consistency.\n\n6. **Missing References:**\nThe proposed All-Seeing Model is trained with both, a generative and contrastive loss, to facilitate its use for generative (e.g. captioning) and discriminative (e.g. object recognition) tasks.\nDue to the similarity in its architectural design, I believe the authors should cite a few prior works in their discussion to provide a broader context for the reader:\n\n    - [CoCa: Contrastive Captioners are Image-Text Foundation Models](https://arxiv.org/abs/2205.01917) - trains with both objects as ASM.\n    - [LiT: Zero-Shot Transfer with Locked-image text Tuning](https://arxiv.org/abs/2111.07991) - repurposes *any* image backbone to a contrastive image-text model.\n    - [Image Captioners Are Scalable Vision Learners Too](https://arxiv.org/abs/2306.07915) - trains with generative objective first, then uses LiT."
                },
                "questions": {
                    "value": "Please see weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The idea of using images from SA-1B is a major contributor to reducing the potential ethical risks arising from AS-1B. However, the paper lacks a discussion regarding the limitations of the proposed dataset and model. I suggest the authors to collect all the concerns that remain after this reviewer discussion and add a carefully worded section highlighting the limitations.\nBroadly speaking, the AS-1B dataset\n(1) is prone to the hallucinations produced by the constituent models,\n(2) contains language that is uninformative, yet sounds \"dry\" (not a weakness, but better clarify upfront),\n(3) may have annotation errors as a reasonable trade-off to reducing verification costs."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1804/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1804/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1804/Reviewer_Wviv"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1804/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1700932427451,
            "cdate": 1700932427451,
            "tmdate": 1700932427451,
            "mdate": 1700932427451,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]