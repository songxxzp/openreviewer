[
    {
        "title": "HyperFields: Towards Zero-Shot Generation of NeRFs from Text"
    },
    {
        "review": {
            "id": "RqnZwqwTzj",
            "forum": "84Hk01tFKq",
            "replyto": "84Hk01tFKq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6578/Reviewer_VyWD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6578/Reviewer_VyWD"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents \"HyperFields\", an approach designed to achieve zero-shot generation of Neural Radiance Fields (NeRFs) from textual prompts. By utilizing a dynamic hypernetwork, HyperFields is able to establish a mapping from text token embeddings, specifically derived from BERT, to the domain of NeRF parameters. The paper introduces NeRF distillation training, where individual NeRF-encoded scenes are distilled into a singular dynamic hypernetwork. The main goal of HyperFields is to efficiently generate scenes that the model has seen during its training (in-distribution), and to quickly fine-tune itself for unseen (out-of-distribution) prompts, if necessary."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(+) The paper addresses an important gap in zero-shot generation using NeRFs.\n\n(+) The presentation of the method is clear, with a well-structured explanation of the hypernetwork and NeRF distillation.\n\n(+) The paper introduces a fresh perspective on text-to-NeRF synthesis, leveraging hypernetworks, which is a less explored territory in this context."
                },
                "weaknesses": {
                    "value": "(-) **Limited experimental results**: The experimental results presented in the paper, particularly in Figure 4 and Figure 5, are quite limited. The in-distribution generalization showcased in Figure 4 uses only 9 basic shapes, and the generalization is restricted to simple uniform color translations. Figure 5, too, is restricted to basic geometry and appearance.\n\n(-) **Low quality of results**: The results presented, especially the simple geometric shapes and uniform color distributions, seem to be of low standard. There is a noticeable disparity in quality when compared to state-of-the-art techniques. Moreover, the ablation study in Figure 6 only provides a single example, which weakens the overall argument.\n\n(-) **Lack of comparative discussion**: It's important to note that recent studies, such as HyperDiffusion (ICCV 2023) and Shap-E (arXiv), have also explored the use of hyperparameters in 3D object generation, resulting in promising results. However, there is a lack of comparative discussion with these methods, which is crucial in positioning HyperFields in the current research landscape."
                },
                "questions": {
                    "value": "- In Section 4.2 (referencing Figure 5), the paper mentions, \"...with no optimization, when provided with the out-of-distribution prompt. The model chooses the semantic nearest neighbor from its training data as the initial guess for out-of-distribution prompts...\"; However, it is not clear how the model is capable of retrieving the nearest-neighbor. Could the authors provide more information on the intrinsic capabilities of the model that enable this nearest-neighbor retrieval?\n- How does HyperFields handle highly creative textual prompts?\n- How does the NeRF distillation process handle the potential loss of scene details? \n- It is important to have a discussion regarding the limitations of the proposed approach."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6578/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6578/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6578/Reviewer_VyWD"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6578/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698669235930,
            "cdate": 1698669235930,
            "tmdate": 1700738084639,
            "mdate": 1700738084639,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Vlkgdb6Nty",
                "forum": "84Hk01tFKq",
                "replyto": "RqnZwqwTzj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6578/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6578/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to R VyWD"
                    },
                    "comment": {
                        "value": "> Limited experimental results/low quality of results\n\nPlease see our results on complex scene generation in the general comment above. The quality of our results are a direct consequence of the quality of the teacher NeRFs we distill. \n\nWe provide an additional ablation demonstrating the utility of the dynamic hypernetwork, please see this [figure](https://ibb.co/KNGThj7). In this experiment we train prompts 'wooden table' and 'origami chair' together. We see that our dynamic hypernetwork can generate origami chair and wooden table, while when trained without our dynamic hypernetwork the model suffers from lack of flexibility and makes the origami chair have a geometry similar to the wooden table. This ablation is discussed in Appendix H.\n\n> Lack of comparative discussion to HyperDiffusion and Shape-E\n\nWe have updated the related works section of the paper to cite and discuss these papers. We note that HyperDiffusion is an unconditional generative model which diffuses over NeRF weights (cannot do text based scene generation), whereas our method predicts NeRF weights dynamically conditioned on the 1) text prompt, 2) the sampled 3D coordinates, and 3) the previous NeRF activations. Similarly, Shap-E diffuses over latent codes which are then mapped to weights of a NeRF MLP, and requires teacher point clouds to train. Both of these methods have the same limitations of slow inference due to denoising sampling.\n\n> Out-of-distribution nearest neighbor retrieval\n\nBERT encoded representation of the text prompt is given as input to the hypernetwork. So when an out-of-distribution prompt  'blender\u2019 is provided as input to the hypernetwork, the representation of 'blender\u2019 is similar to the representation of 'toaster\u2019--which is a shape the hypernetwork is trained on. Hence, at step zero the pre-trained hypernetwork generates weights of NeRF that render a 'toaster\u2019.\n\n> How does HyperFields handle highly creative textual prompts\n\nPlease see our results on complex scene generation in the general comment above, where we demonstrate our model\u2019s capability to generate creative textual prompts.\n\n> How does the NeRF distillation process handle potential loss of scene details?\n\nOur distillation approach provides strong supervision using the teacher NeRFs, and thus our model is able to converge to very low loss values. This translates to virtually no loss in visual quality of the training scenes predicted by the hypernetwork. See relevant figure [linked here](https://ibb.co/4mrrQBQ)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700653783068,
                "cdate": 1700653783068,
                "tmdate": 1700653783068,
                "mdate": 1700653783068,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MvXT1qNR8j",
                "forum": "84Hk01tFKq",
                "replyto": "Vlkgdb6Nty",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6578/Reviewer_VyWD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6578/Reviewer_VyWD"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for addressing the initial concerns in the rebuttal. The inclusion of additional results and ablations is appreciated and enhances the comprehensiveness of the work. However, upon reviewing these supplementary materials, I still find that the quality of the results appears rough.\n\nFurthermore, In my initial review, I did not mention ATT3D due to its very recent introduction at ICCV. However, I align with the opinions of Reviewers YDm2 and 6JFZ regarding the importance of comparing the HyperFields work with ATT3D, especially in terms of methodological novelty and result quality. After reviewing the rebuttal, my concerns in these areas persist."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738059537,
                "cdate": 1700738059537,
                "tmdate": 1700738059537,
                "mdate": 1700738059537,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8kPC8VBXvn",
            "forum": "84Hk01tFKq",
            "replyto": "84Hk01tFKq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6578/Reviewer_6JFZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6578/Reviewer_6JFZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper propose a fast way to achieve text-to-3D generation. The key idea is to design a hyper-network that predicts weights that process the spatial latent code of NeRF. The technical contribution is that the hyper-network not only looks at the text input, but also the activations of the MLP to be predicted. Experiment results are shown with only combination of color and shape instead of a full prompt (possibly due to limited computational resources). Qualitative results with very limited examples show that the technique is able to improve text-to-3D speed and achieve certain generalizability toward new concepts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The hyper-network is fast since it\u2019s free of optimization. Such feedforward approach can have computational advantages as the training cost can potentially be amortized \n- The experiment shows that the model can compose concepts in certain fashion. This helps illustrate the benefit that such model can amortize training compute to be used for many inference uses."
                },
                "weaknesses": {
                    "value": "- Current methods is trained from scratch, which might be computationally expensive. \n- The key architecture of this paper is a hyper-network that predicts the weights for the MLP. \n- A main concern regarding the result is very limited. Most of the results are shown in simple objects and compositions. \n- An other small concern is regarding the need to create a small dataset of NeRF scenes. Each NeRF can takes minutes, and this prevents it to scale to larger datasets.\n- If the model weight depends on where we sample the activations, then the generated weights can have high variance. I\u2019m a bit concerned that this means different ways to sample the points can lead to different weights, and thus leading to different performance."
                },
                "questions": {
                    "value": "- \u201cSDS loss exhibits high variances\u201d - is there any experimental/reference evidence that support it?\n- Quality: Figure 4, why is the image looks washed out? Is it because of artifacts from normalization?\n- I wonder if the main difference between ATT3D and this paper is whether the hyper-network takes activations?\n- Why do we choose three captions during training? Why can\u2019t we spread away these captions throughout different batches?\n- Maybe I\u2019ve missed it, but how do we choose different a_i\u2019s?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6578/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698812114158,
            "cdate": 1698812114158,
            "tmdate": 1699636746953,
            "mdate": 1699636746953,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "25oOTL51lE",
                "forum": "84Hk01tFKq",
                "replyto": "8kPC8VBXvn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6578/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6578/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to R 6jFZ"
                    },
                    "comment": {
                        "value": "> Main concern regarding the result is very limited\n\nPlease see our results on complex/high quality scenes in the general comment above. \n\n> Scaling NeRFs to larger datasets\n\nWe discuss in Sec 4.3 in the paper that our model is able to amortize the overhead cost of both training the NeRF dataset and distilling them into the HyperFields model. Furthermore, from just a handful of NeRFs we are able to bootstrap our model to converge faster on novel unseen prompts with minimal fine-tuning (Fig. 5), thus only aiding in the effort towards building a larger dataset. \n\n> Variance of generated weights due to sampling \n\nPlease see [figure linked here](https://ibb.co/GcLMmZW) for our experiment on the effect of the variance of the  generated weights  due to sampling. We query HyperFields on \u201ca red chair\u201d and shuffled the query rays into random mini-batches for the same viewpoint, and show the render outputs above. Note that the renders are visually identical, **even though the weights predicted by the hypernetwork are different as a result of the different mini-batch compositions**. This is because we train the HyperFields model by distilling teacher NeRFs which naturally imposes 3D view consistency constraints. Thus, our model\u2019s high expressive potential through adaptive NeRF weights does not hinder the view-consistency of scenes due to our distillation training. \n\n> SDS loss exhibits high variance \n\nThe stochasticity of the denoised output is a fundamental feature of denoising diffusion models, as the outputs of these models are dictated by randomly sampled noise over a sequence of timesteps. For diffusion models, this is a key feature which enables the substantial variation in images generated by these models. Thus, the SDS loss, which is a function of the diffusion denoiser, will similarly be stochastic and exhibit substantial variance. As an example, please see the output of Stable Diffusion after denoising **for the same timestep (980) conditioned on the same input image and text prompt (left) for two different noise samples** [figure linked here](https://ibb.co/yPdzzv0). Note that for any given denoising timestep, the predicted denoised image will vary significantly. \n\n> Figure 4 images look washed out \n\n We purposely fade some of the renders to indicate that those are the training scenes. Please see a revised version of the figure [linked here](https://postimg.cc/tYHrxNyM) where the feedforward inference scenes are outlined in black dashed lines, and the training scenes have no outline. \n\n> Difference between ATT3D and HyperFields\n\nPlease see our general comment at the top. \n\n> Why do we choose three captions during training?\n\nCan the reviewer please clarify this question? We are not sure which implementation detail this question is referring to. \n\n> How do we choose different a_i\u2019s?\n\n The a_i\u2019s are the activations of layer i of the NeRF MLP for a given sampled 3D coordinate. These activations are fed into the hypernetwork along with the conditioning text embedding to predict the weights for the NeRF MLP layer i+1. Please refer to Fig. 3 [linked here](https://postimg.cc/sBRSnw6k) for a visualization of this process."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700634637504,
                "cdate": 1700634637504,
                "tmdate": 1700634637504,
                "mdate": 1700634637504,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wvR7uLOxSf",
            "forum": "84Hk01tFKq",
            "replyto": "84Hk01tFKq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6578/Reviewer_ckby"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6578/Reviewer_ckby"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel generalizable text-to-3D framework to generate a 3D representation with neural radiance field (NeRF) given text inputs. The text inputs are processed by a pretrained BERT network to get powerful embeddings to condition the NeRF generation process. The dynamic hypernetwork is the key innovation of the design to make it succeed to generalize across different 3D shapes over various text conditions. Then a distillation loss is employed to train the NeRF image rendering process given 3D coordinates and synthesized 2D images from DreamFusion. This work will facilitate significant progress of 3D AIGC and inspire future explorations on generalizable 3D generation."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "(1) The motivation of this work is clear and the technical impact of this work are significant. Limited by the intrinsic mapping relationship between 3D coordinate and color field of NeRF, current work struggles to achieve generalizable 3D shapes with various conditional input with a unified framework. This work proposes a hypernetwork architecture, which is justified as the key innovation to learn a generalized text-to-3D mapping across different inputs.\n(2) Authors conduct extensive experiments on both in-distribution and out-of-distribution samples during testing, and results look consistently appealing.\n(3) Benefit from the use of InstantNGP, the generation is much faster than the baseline DreamFusion, which is crucial for some real-time applications such as real-time rendering and editing.\n(4) Authors have committed that they will release all the code and pretrained models, which will facilitate better reproduction and follow-up for the community."
                },
                "weaknesses": {
                    "value": "(1) There seems missing quantitative comparison between the proposed method and baseline (DreamFusion) on CLIP retrieval scores or user study, which may make the work further stronger and more convincing.\n(2) Another ablation study to conduct is to verify the effectiveness of training across multiple shapes then fine-tuning on a single shape, and compare it with a baseline that train the model on this single shape from scratch. This will demonstrate the advantage of training across a wider range of samples to learn a stronger representation between text condition and the 3D representation, by incorporating more samples during training."
                },
                "questions": {
                    "value": "I may consider further improve my rating if my concerns (listed in the weaknessed part) are well addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6578/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698814183945,
            "cdate": 1698814183945,
            "tmdate": 1699636746840,
            "mdate": 1699636746840,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WZzifIgq5K",
                "forum": "84Hk01tFKq",
                "replyto": "wvR7uLOxSf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6578/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6578/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to R ckby"
                    },
                    "comment": {
                        "value": "> ... missing quantitative comparison for baseline on CLIP retrieval scores \u2026\n\nThank you for the suggestion. In section E of the supplementary, we have user studies comparing the quality of the out-of-distribution renders of HyperFields with that of the Stable-DreamFusion baseline. Our method is consistently preferred over the baseline. Please see the table copied below. \n\n| Model | Gold Blender | Yarn Shoes | Yarn Skateboard | Plaid Skateboard | Plaid Lamppost | Strawberry with tiger stripes |\n| ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | \n| Our Method (\u2193)  | 1.30 | 1.07 | 1.32 | 1.29| 1.36 | 1.12 | \n| Best DreamFusion Baseline | 2.50 | 2.40 | 2.33 | 1.75 | 2.0 | 2.25 |\n\nPlease see the results below for the results of CLIP retrieval comparing HyperFields with StableDreamFusion baseline. We compute CLIP retrieval scores for out-of-distribution scenes in [Fig. 5](https://raw.githubusercontent.com/threedle/hyperfields/rebuttal/ood_compare_RLfix.jpg) for both Hyperfields and the Stable-DreamFusion (S) baseline, which is the NeRF model trained from scratch.  We significantly outperform the baseline when the baseline and our model are  trained for the same number of steps. We compare favorably when the baselines are trained 5x longer than our model.\n\nThe support set of scenes is a cross product of the following geometry and attribute set. \n- Geometry: {\u2018chair\u2019, \u2018pot\u2019,\u2019toaster\u2019, \u2018bench\u2019, \u2018stool\u2019, \u2018bowl\u2019, \u2018vase\u2019, \u2018couch\u2019,\u2019table\u2019}\n- Attributes: {\u2018bamboo\u2019, \u2018brick\u2019, \u2018yarn\u2019, \u2018glacier\u2019, \u2018gold\u2019, \u2018origami\u2019, \u2018plaid\u2019, \u2018rainbow\u2019, \u2018stained glass\u2019, \u2018terracotta\u2019, \u2018wooden\u2019}\n\nThe support set has 108 scenes in total. We perform CLIP retrieval on this support set.  \n\n| Model | Top-1 | Top-10 |\n| ----------- | ----------- | ----------- |\n| Stable DreamFusion (Same compute budget at our model)  | 0.12 | 0.25 |\n| Stable DreamFusion (Trained for 5x longer than our model)  | 0.62 | 0.75 | \n| HyperFields (ours)  | 0.75 | 0.87 | \n\nFor additional quantitative evaluation please refer to Table 1 in the paper. We compare the CLIP retrieval scores of the scenes generated by HyperFields for prompts it has seen during training and prompts it generated zero-shot with no optimization. Our results suggest that the quality of the zero-shot rendered novel scenes is similar to that of the quality of the scenes seen during training. For additional discussion please refer to section 4.1 in paper.\n\n> ... verify effectiveness of training across multiple shapes then fine-tuning \u2026 and compare with baseline that trains from scratch \u2026 \n\nThe effectiveness of training across multiple scenes then fine-tuning is demonstrated in [Fig. 5 in the paper](https://ibb.co/QY2TkHS).  We demonstrate that by training across multiple scenes, our model learns a meaningful map between text embeddings and NeRF, such that when fine-tuning to an out of distribution scene (prompt for which the attribute/shape tokens are not seen during training) our model converges significantly faster (~5x) than a model trained from scratch. Specifically, the comparison to the baseline model trained from scratch is given by rows 2 and 4 in [Fig. 5](https://ibb.co/QY2TkHS)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631606626,
                "cdate": 1700631606626,
                "tmdate": 1700633162729,
                "mdate": 1700633162729,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "alXHrMSMl3",
            "forum": "84Hk01tFKq",
            "replyto": "84Hk01tFKq",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6578/Reviewer_YDm2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6578/Reviewer_YDm2"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces a framework for achieving zero-shot NeRF generation from texts. This is accomplished through the training of a dynamic hypernetwork using hundreds of pre-trained NeRFs to acquire the mapping from text token embeddings to NeRF weights. Extensive experiments demonstrate the capability of the proposed method to predict in-distribution scenes in a zero-shot manner and out-distribution scenes with a few steps of fine-tuning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is clearly written and easy to follow.\n\n2. The proposed pipeline intuitively makes sense and it is interesting to see the disentanglement of different attributes learned by the proposed hypernetwork."
                },
                "weaknesses": {
                    "value": "1. My primary concern is the technical contributions of this work in comparison to the referenced ATT3D study. Specifically, while this paper clarifies the connections with ATT3D, it remains unclear what novel techniques or insights are newly introduced by this work. A more compelling justification is highly desirable.\n\n2. Furthermore, there is a lack of benchmarking against ATT3D and the reported results indicate that ATT3D may achieve much better visualization effects. This discrepancy arises because the proposed method appears to only disentangle objects with simple attributes like colors, while ATT3D's reported visualizations can manipulate higher-level attributes and behaviors, such as \"chimpanzee holding a cup.\" The authors are highly expected to conduct a benchmarking comparison with ATT3D using the same text prompt.\n\n3. Although intriguing, it remains unclear why the hypernetwork can successfully learn the disentanglement of various attributes. This may be attributed to the limited scope of text prompts and attributes during training. The authors are expected to provide more insights on this matter."
                },
                "questions": {
                    "value": "My questions are included in the weakness section. I am willing to adjust my scores if my concerns are properly addressed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6578/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6578/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6578/Reviewer_YDm2"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6578/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698914759258,
            "cdate": 1698914759258,
            "tmdate": 1700782609431,
            "mdate": 1700782609431,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AqDhbemkds",
                "forum": "84Hk01tFKq",
                "replyto": "alXHrMSMl3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6578/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6578/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to R YDm2"
                    },
                    "comment": {
                        "value": "> ATT3D\n\nPlease see our discussion of ATT3D in the general comment. \n\n> ... unclear why the hypernetwork can successfully learn the disentanglement of various attributes ....\n\nThe disentanglement is implicitly enforced through the training, as the model sees instances of the same text token (e.g. \u201cplaid\u201d) in association with different shapes (chair, toaster, table, etc.), and thus must learn the common features which define a \u201cplaid\u201d style across these different scenes. The same reasoning applies to the object/shape tokens shared across different scenes. If the model attempts to correlate any attribute token with shape token (e.g. \u201cplaid\u201d with \u201cchair\u201d), then it will be penalized when it encounters a prompt like \u201corigami chair\u201d and predicts plaid attributes for the chair, thus forcing it to disentangle the different attributes."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6578/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631353153,
                "cdate": 1700631353153,
                "tmdate": 1700726207157,
                "mdate": 1700726207157,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]