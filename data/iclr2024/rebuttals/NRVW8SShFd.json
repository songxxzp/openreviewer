[
    {
        "title": "MaskINT: Video Editing via Interpolative Non-autoregressive Masked Transformers"
    },
    {
        "review": {
            "id": "zzjM7Xo5Mw",
            "forum": "NRVW8SShFd",
            "replyto": "NRVW8SShFd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8292/Reviewer_Np2s"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8292/Reviewer_Np2s"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a two-stage video editing method, which first leverages an off-the-shelf image editing method to edit keyframes, and then performs interpolation between the edited frames. Quantitative and qualitative experiments demonstrate that MaskINT achieves comparable performance with previous methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Efficiency. The proposed method achieves comparable performance with diffusion methods, but it is much faster."
                },
                "weaknesses": {
                    "value": "- The video editing performance heavily relies on frame interpolation performance. Almost all showed results (in main submission and Supp) are simple motions, such as car translation, rhino translation. The simple motions can be easily interpolated. But for complex motions, it is difficult to perform frame interpolation, and it also suffer occlusions. Actually, in the showed man dancing case, there are obvious artifacts in arms. Also, the proposed method may suffer a lot in case of long-range video editing. Thus, the generalization ability of the proposed method is somehow limited.\n- Evaluation. There are only 11 examples in Supp, and it is difficult to judge the performance. Are the results cherry-picked? Could you give more results?"
                },
                "questions": {
                    "value": "- How about the failure cases? \n- How about long videos and complex motions?\n- Would it fail if the first-stage kerframe editing fails?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8292/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8292/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8292/Reviewer_Np2s"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8292/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698039807503,
            "cdate": 1698039807503,
            "tmdate": 1699637031293,
            "mdate": 1699637031293,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8lgSIobI4D",
                "forum": "NRVW8SShFd",
                "replyto": "zzjM7Xo5Mw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8292/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8292/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Np2s"
                    },
                    "comment": {
                        "value": "1 Generalization ability  \nWe thank Reviewer Np2s for pointing out the insufficient examples issues. Following your valuable suggestions, we further add more editing samples with diverse objects and scenes in Figure 10 to demonstrate the generalization ability of our method. \n\n2 Simple motions  \nAlthough the motions such as car translation and rhino translation seem \u201csimple\u201d, they are not easily interpolated with existing video frame interpolation methods. As shown in Table 3 and Figure 6 and also the mp4 videos in Supplementary, current state-of-the-art video frame interpolation methods still fail in many simple motions like raising the head of a camel, the fish movement, etc. The major reason is that these works mainly focus on slow-motion generation, making them less effective in handling frames with large motions. On the contrary, we are pioneering work that proposes the idea of \u201cstructure-aware frame interpolation\u201d. With the structure guidance, we can successfully reconstruct the intermediate frames with original motions. It\u2019s true that under complex motions such as the man dancing cases, our method may generate some artifacts. However, we\u2019d like to politely point out that our work mainly focuses on improving the efficiency with masked generative transformers, rather than achieving new state-of-the-art performance. Besides, the current state-of-the-art methods like TokenFlow still generate some artifacts on foot in this challenging case with even 7x inference time. \n\n\n3 Long video editing   \nSince the non-autoregressive pipeline generates all video frames simultaneously, it's challenging for it to edit an entire long video due to GPU memory limitation. Nevertheless, our framework can still be extended to generate long videos by dividing the long video into short clips and progressively performing frame interpolation within each clip. For example, given a video with 60 frames, we can select the 1st, 16th, 31st, 46th, and 60th frames as the key frames. We joint edit these five key frames together and then perform structure-aware frame interpolation within each pair of consecutive key frames. As shown in Figure 7 of Appendix, our method can still successfully generate consistent long videos with this design. The .mp4 video of these generated long videos can be found in Supplementary. \n\n4 Failure cases     \nWe also add some failure cases in Figure 11. Since we disentangle the video editing tasks into two separate stages, the final performance of the generated video depends on the keyframe editing in the first stage. In certain challenging scenarios, the attention-based key frame editing stage struggles to produce consistent frames, primarily due to the complexity of the scene or the presence of exceptionally large motion. In this case, our MaskINT can still interpolate the intermediate frames, albeit with the potential for introducing artifacts."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8292/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700189568197,
                "cdate": 1700189568197,
                "tmdate": 1700189568197,
                "mdate": 1700189568197,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7AMadlqiza",
            "forum": "NRVW8SShFd",
            "replyto": "NRVW8SShFd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8292/Reviewer_hZt7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8292/Reviewer_hZt7"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposed a two-stage video editing framework, using T2I diffusion model to edit the key frames and then interpolating between those frames. During T2I diffusion process, the paper leveraged controlnet to jointly keep the edge consistency. After that, a Masked generative transformer model called MaskINT is introduced to generate middle frames. The results show that the proposed network can accelerate generate videos compared with baseline pipelines while suffering slightly temporal and prompt consistency decrease."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed MaskINT leverage masked generative transformer to interpolate between keyframes.\n2. The inference speed outperformed the proposed video editing pipelines.\n3. MaskINT is trained on unlabeled video datasets using masked token modeling, without needing text-video pairs."
                },
                "weaknesses": {
                    "value": "1. Although the proposed MaskINT can beat other methods in speed, the method still suffers consistency degradation in both prompt and temporal domain. \n2. Noticeable degradation across key frames and interpolated frames.\n3. No related baseline comparison between video interpolation pipeline."
                },
                "questions": {
                    "value": "1. By increasing the decoding step and keyframes, the method can increase the performance in Tem-Con and Pro-Con. Can the method reach comparable qualitative results in less time by increasing those hyper parameters?\n2. The videos in supplementary seem to have heavy moir\u00e9 patterns. Why does this occur?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8292/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698745838745,
            "cdate": 1698745838745,
            "tmdate": 1699637031172,
            "mdate": 1699637031172,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GcfmORgMBx",
                "forum": "NRVW8SShFd",
                "replyto": "7AMadlqiza",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8292/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8292/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hZt7"
                    },
                    "comment": {
                        "value": "1 Comparison with video interpolation baseline   \nThank you for your suggestions. We further add quantitative comparisons in Figure 5, 8, and 9 with FILM, which is the  state-of-the-art video frame interpolation method. Noticeably, these frame interpolation works cannot intermediate frames following the original motions, due to the lack of structural motions. Besides, we also quantitatively compare our method with FILM and RIFE under the \u201creconstruction\u201d setting suggested by Reviewer gZ41. The results in Table 3 and Figure 6 also demonstrate the benefit of our method. \n\n\n2 Degradation in performance   \nWe\u2019d like to politely emphasize that our work does not aim to achieve state-of-the-art performance, but would rather focus more on the efficiency area with a better trade-off between performance and efficiency. \n\n3 Ablation study on the number of key frames and decoding steps   \nAs shown in Table 2, when further increasing the number of key frames, we can further get improvement because a better understanding of motion can be extracted from more frames. However, when further increasing the number of decoding steps, it tends to reach a saturation point. This is consistent with the conclusion in MaskGiT.  \n\n\n4 Moir\u00e9 patterns in supplementary videos   \nThanks for pointing out this issue in the quality of video visualization. The reason mainly comes from the \u201c.gif\u201d format. We save them into the \".mp4\u201d file, and this problem can be solved."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8292/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700189480242,
                "cdate": 1700189480242,
                "tmdate": 1700189480242,
                "mdate": 1700189480242,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Bwf6PjTBGF",
            "forum": "NRVW8SShFd",
            "replyto": "NRVW8SShFd",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8292/Reviewer_gZ41"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8292/Reviewer_gZ41"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new approach to structure-guided editing for videos. The proposed method is composed of two stages. In the first stage, an image-based diffusion model is leveraged, along with the cross-frame attention technique, to jointly edit a small number of key frames. In the second stage, a structure-guided non-autoregressive masked transformers model is developed for the interpolation task, aiming to propagate the information from the (edited) key frames to the intermediate frames. The experiments in the paper demonstrate the proposed method can enable temporally consistent edit propagation results while achieving better efficiency compared to existing diffusion-based approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The video editing results provided in the paper demonstrate that the non-autoregressive masked generative modelling technique, which have mostly been applied to the unconditional generation or text-condition generation so far, can be effectively adapted to the structure-conditioning generation setting.\n\nThe experiments in the paper demonstrate that the proposed method can achieve better efficiency compared to existing diffusion-based approaches."
                },
                "weaknesses": {
                    "value": "While the idea of extending the non-autoregressive masked transformer technique to structure-guided generation is technically sound, the technical contribution on the fundamental side is somewhat limited. Video editing with diffusion model via key-frame edit propagation has been widely explored. The effectiveness of masked generative model in video generation has also been well established. The key contribution of this paper, from my perspective, is in showing that it is possible to incorporate dense structure information into the masked transformer model. There are limited discussions in the paper, however, to provide insights on why such a task is difficult, what are the fundamental challenges in doing that, and why the proposed technique is a good solution for such challenges. \n\nThe discussion on the technical details is somewhat vague. In particular, it seems that the model architecture details were not elaborated.\n\nThe provided evaluation is a bit weak: \n+ I feel that the subjective comparison should be made more complete: video results were only provided (in the supplementary material) for the proposed method, not for competing methods. That makes it difficult to assess the temporal quality of the proposed method in comparison with the other methods.\n+ The comparison is not entirely fair, competing methods are all zero-shot setup, which never trains a video model. Existing video diffusion works have been shown to be effective for interpolation ([1], [2]), a fair comparison would be to compare with adapted versions of those methods to incorporate structure control signal.\n+ It seems that the provided results are all with stylized content, which tends to make it more visually tolerable to temporal inconsistencies. As the main goal of the second-stage model is to perform keyframe propagation, I think one important test that should be done is to apply the model on the reconstructive setting, i.e. perform propagation with the original keyframes instead of edited ones and assess the reconstruction quality of the intermediate frames.\n\n[1] Make-A-Video: Text-to-Video Generation without Text-Video Data. Singer et al., 2022\n\n[2] Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models. Blattmann et al., 2023"
                },
                "questions": {
                    "value": "Please find my detailed comments in the Weaknesses section. Other than that, there are a couple of questions I\u2019m curious about:\n+ Will the proposed technique works for other type of controls such as depth maps or pose map? \n+ How will the method perform in the extrapolation instead of the interpolation setting? Or in the setting where only one key frame is edited?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8292/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698813861609,
            "cdate": 1698813861609,
            "tmdate": 1699637031051,
            "mdate": 1699637031051,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fR7QFsFA7g",
                "forum": "NRVW8SShFd",
                "replyto": "Bwf6PjTBGF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8292/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8292/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Review gZ41"
                    },
                    "comment": {
                        "value": "1 Results on Video Frame Interpolation with original key frames.  \nThanks for pointing out this important evaluation setting. Following your valuable suggestions, we conduct a quantitative and qualitative evaluation of the performance of MaskINT in the reconstructive setting, where it engages in video frame interpolation using the original key frames. In this evaluation, we apply signal-to-noise ratio (PSNR), learned perceptual image patch similarity (LPIPS), and structured similarity (SSIM) to compare the interpolated frames with original video frames. We benchmark our method against two state-of-the-art Video Frame Interpolation (VFI) methods, including FILM [A] and RIFE [B]. Table 3 and Figure 6 in Appendix show that our method significantly outperforms VFI methods on all evaluation metrics, with the benefit of the structure guidance from the intermediate frames (For better visualization, we suggest you watch the .mp4 video in Supplementary). \nMoreover, even when confronted with significant motion between two frames, our approach successfully reconstructs the original video, maintaining consistent motion through the aid of structural guidance. In contrast, FILM introduces undesirable artifacts, including distorted background, multiple cat hands, and the absence of a camel's head, etc. The major reason is that current VFI models mainly focus on generating slow-motion effects and enhancing frame rate, making them less effective in handling frames with large motions. Additionally, the absence of structural guidance poses a challenge for these methods in accurately aligning generated videos with the original motion.\n\n\n2 Comparisons with [1] and [2] with additional structure control.  \nThanks for sharing these important works. However, We would like to respectfully point out that designing an architecture to explicitly introduce structural control to these works is a non-trivial task.Besides, these methods are not open-source and they use some in-house datasets to train the model, making it difficult to make any further adjustments and conduct a fair comparison. \n\n\n3 Performance with single frame.  \nWe further evaluate the performance where only the initial edited key frame is given. As shown in Table 2, when there is only one keyframe, the performance is downgraded since it\u2019s difficult to understand the motion within the video with a single reference frame. \n\n\n\n[A] Reda, Fitsum, et al. \"Film: Frame interpolation for large motion.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n\n[B] Huang, Zhewei, et al. \"Real-time intermediate flow estimation for video frame interpolation.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8292/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700189335098,
                "cdate": 1700189335098,
                "tmdate": 1700189335098,
                "mdate": 1700189335098,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]