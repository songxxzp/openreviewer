[
    {
        "title": "Adversarial Defense using Targeted Manifold Manipulation"
    },
    {
        "review": {
            "id": "DjA5L1H2Sw",
            "forum": "y3BuJotSKl",
            "replyto": "y3BuJotSKl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4256/Reviewer_18Cc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4256/Reviewer_18Cc"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a defense method named \"Target Manipulation Manifold\" (TMM) to defend against adversarial attacks in deep learning models. This method effectively defends against adversarial attacks by guiding the gradient on the target data manifold toward carefully designed trapdoors. The trapdoors are assigned an additional class label (Trapclass), making the attacks easily identifiable. Experimental results indicate that the proposed method can detect ~99% of attacks without significantly compromising clean accuracy. It also exhibits adversarial semantic preservation and robustness to non-adversarial perturbations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The primary strength of this paper lies in its novelty. The TMM approach offers a fresh perspective on adversarial attack defense, employing a unique \"trapdoor\" mechanism to detect adversarial samples. \n2. The experimental section is comprehensive, covering a variety of datasets and adversarial attack types, showcasing the superiority of this approach over existing methods.\n3. This detection algorithm avoids learning a separate attack detection model, thus maintaining semantic alignment with the original classifier.\n4. The design of TMM allows it to be easily integrated into various deep learning models without the need for significant modifications to the original model structure."
                },
                "weaknesses": {
                    "value": "1. The \"Trapclass\" filter mentioned in the paper primarily detects untargeted attacks, while the \"Entropy\" filter mainly identifies attacks that strive to minimize perturbation. Such a design might not be stable under certain specific attack strategies.\n2. It is crucial to evaluate the efficiency and speed of the algorithm when considering applying the method to large-scale datasets or real-time application scenarios. The paper doesn't seem to delve deeply into this aspect. \n3. The L function in the article combines multiple loss components, but it does not clearly state how these components interact or how their weights are balanced.\n4. The paper mentions different thresholds (such as \u03be and \u03c1) used for detecting and identifying adversarial samples. However, the selection of these thresholds appears to be based on experience rather than systematic optimization. For these hyperparameters, the article does not provide a sensitivity analysis of their impact on the results, nor does it give clear guidance on how to choose these parameters for different datasets or tasks.\n5. During the model training process, the setting of triggers is randomly selected. This raises the concern of whether some genuine data might be mistakenly identified as data containing triggers, leading to false alarms. Additionally, this random selection introduces a level of uncertainty. After numerous attempts, an adversary might be able to identify and bypass these triggers. Researchers should consider using a more stable and reliable method for setting triggers and balance the pros and cons of randomness and determinacy when designing."
                },
                "questions": {
                    "value": "Please help to check the weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4256/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698650623749,
            "cdate": 1698650623749,
            "tmdate": 1699636392750,
            "mdate": 1699636392750,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YJMQuQQBJi",
                "forum": "y3BuJotSKl",
                "replyto": "DjA5L1H2Sw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4256/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4256/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate our reviewer's time and effort to provide us with detailed feedback and raise a few interesting points. Please see below our replies to the key points raised.\n\n- _**Stability of our defense**_\n\n  Ans: To stress-test our defense we designed an adaptive attack that assumes the knowledge of the defense algorithm and the associated parameters to defeat the detection system. This should represent the worst of all the unknown attack strategies (sec 4.4). However, we showed that our detection methods still provided respectable accuracies against such extreme attack scenario.\n\n\n- _**Q. Detection speed**_\n\n   Ans: We have already covered that under the 'Computational time' in the experiment section (the last paragraph).\n\n- _**Q. Loss function components**_\n\n   Ans: Loss components are indirectly given weight by sampling the data instances corresponding to individual losses at different ratios (sec 4.1). Thus we don't need to provide individual weights in the combined loss function.\n\n- _**Q. 3.4 Threshold values of $\\rho$ and $\\xi$**_\n\n   Ans: We did specify a method to set the thresholds in sec 4.2, para 'Offline attack detection performance'. They are set to cause at most 0.5% false positive rate in the training dataset.\n\n- _**Q. Uncertainty due to randomness of the trigger**_\n\n   Ans: The randomness of the trigger (both the norm and the position) as discussed in sec 3.1 is by choice. Fig 3 shows the effect of our trigger placement. The trapring design as seen in there is only possible by randomly placing the trigger and the thick skin of the trapring is possible by using the random trigger norm. Thus they are the necessity for our method and not the negative aspects."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4256/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728436524,
                "cdate": 1700728436524,
                "tmdate": 1700728436524,
                "mdate": 1700728436524,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7MdiUVfaVi",
            "forum": "y3BuJotSKl",
            "replyto": "y3BuJotSKl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4256/Reviewer_nFtu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4256/Reviewer_nFtu"
            ],
            "content": {
                "summary": {
                    "value": "The authors present a defense method for adversarial perturbations that relies on optimizing a parameterized sum of cross-entropy losses defined over three datasets: 1) a clean dataset, 2) a noisy dataset with points sampled from an $\\epsilon$-ball around the clean points, and 3) a trapdoor dataset (i.e., a backdoored dataset armed with a patch and $y_{\\rm trap}$ label). The defenses are split among two threat models: 1) the live-attack model, in which a defender receives an adversarially-perturbed image and its label at each step of the attacker's optimization, and 2) the standard 'offline' model. They demonstrate the effectiveness of their method on four datasets in comparison to established baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- On the provided settings, the results seem quite promising--the method outperforms baselines on four datasets\n- Outside of some stylistic issues I bring up below, much of the text was clear.\n- The work combines literature in adversarial training and backdoors in an interesting way"
                },
                "weaknesses": {
                    "value": "**Style Issues:**\n\n- A lot of the tables + figures feel insufficiently described. Non-exhaustive list:\n    - The units for the FP column in the tables is unclear\n    - There are no units or explanations of Table 5.\n    - It\u2019s quite hard to see what is being shown in Figure 2 (and looks like the markings were made with pen)\n    - Figure 2 claims a 2x2 checkerboard pattern when the image shows a 4x4\n    - In a final version of the paper it would be great to add more detail to each Table.\n- If I\u2019m understanding the \u201cLoss function for trapdoor\u201d section, the authors are applying fixed patch to a random location in the image with parameterized \u2018faintness.\u2019 The explanation of this feels notation-heavy when it perhaps does not need to be. For example most of the variables in $(1 : ch, k : k+m, l : l+n)$ on page 4 are not defined at the point of introduction, and the notation is nonstandard (I think).\n    - In general, the loss function seems to be a sum of CrossEntropy Losses over three datasets, I wonder if this can be state more compactly.\n- Citations are often formatted weirdly / incorrectly. For example \u201cZhain et. al. Zhai et al. (2019)\u2026\u201d on Page 3.\n- Some terms in the Introduction like \u201cintermediate class labels\u201d are used without definition until later.\n\n**Weaknesses:**\n\n- One of the major points of confusion for me was that the threat model is unclear. In particular, it is unclear what the defender knows about the attack. Do they have an epsilon in mind? Do they know an attacker\u2019s parameters? Do they know an attacker\u2019s algorithm?\n- My initial read is that the work is not well-differentiated from adversarial training regimes. In essence they train on three datasets (via a parameterized loss) one of which is clean, and two of which are altered in traditional ways. I\u2019m looking forward to hearing from the authors on this point\u2014may be a misread on my end. However, in any case, this distinction could be better addressed.\n- The experimental section feels somewhat limited. One of the major claims of the paper is that their defense works on many attacks, but the analysis of an attacker\u2019s parameters ($\\epsilon$, learning rate, etc\u2026) is limited to a single case in the main text. It\u2019s unclear to me whether their method is robust to a higher learning rate attacker that \u2018jumps\u2019 over their trap-ring regions in the training data.\n    - Their method is based on an attacker \u2018landing\u2019 an adversarially perturbed point in the trap-ring of a training datapoint, but it feels like this inevitability can easily be countered by the attacker using a larger step-size to avoid trapdoor region all-together. This potential limitation is unaddressed.\n- Since the trap-rings are based on training data it seems that low-incidence in-distribution data is at risk of flagging an attack, raising some fairness questions. As an example, how does the model perform on dogs whose breed doesn\u2019t appear in the training set. These datapoints may not be associated with any of training data\u2019s rings.\n    - In particular, I would like to see how the method responds to CIFAR 10.1. It should maintain high performance here, but I\u2019m unsure how the distribution shift will affect the proposed method.\n    - In any case, discussion here would be great!\n- The ablations are limited. Since the method extends prior work, it would be nice to highlight exactly what performance gain their method contributes.\n- Discussion of semantics-preserving transformations is limited. I would like to see the effect of random crops or random flips. These perturbations are semantics-preserving but feel like they would fall outside of the epsilon-balls in the training data. Would these points be ignored?\n- I\u2019m struggling with the motivation behind the live-attack setting. In particular, if the attacker has access to the incremental adversarially perturbed images, couldn\u2019t they identify (in pixel space) a perturbation above a certain $\\epsilon$-threshold (which the defender chooses) that invokes a change in label. This method would not require retraining and require less compute than the proposed method.\n- One of the main defenses proposed (TMM-LA) has very little description associated with it (Page 8 bottom)."
                },
                "questions": {
                    "value": "There are a number of questions embedded into the weaknesses above. Here are some additional ones:\n\n- Doesn\u2019t Tiny ImageNet have 100K examples in the training set?\n- \u201cAlmost all attacks need to go through the trap-ring.\u201d If I\u2019m an attacker, what is preventing me from taking a learning rate high enough to jump over your trap-rings. Does the analysis of the trap rings fail when an adversarial point \u2018jumps\u2019 over the trap-ring?\n    - How wide are the trap-rings? How easy is it to land in a trap-ring?\n- What is meant by $(1 : ch, k : k+m, l : l+n)$ on page 4?\n- Please elaborate on the distinction between this and existing adversarial training literature?\n\n\n**Note:** Once the above weaknesses and questions are resolved, I'll raise my score. For the time being, too much of the proposed work was unclear to me."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4256/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4256/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4256/Reviewer_nFtu"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4256/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698720778820,
            "cdate": 1698720778820,
            "tmdate": 1699636392659,
            "mdate": 1699636392659,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IcyWohN9Mm",
                "forum": "y3BuJotSKl",
                "replyto": "7MdiUVfaVi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4256/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4256/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate our reviewer's time and effort to provide us with detailed feedback and raise a few interesting points. Please see below our replies to the key points raised.\n\n- _**On Tiny imagenet sample count**_\n\n   Ans: Sorry, it was a typo and your numbers are correct. Thank you.\n\n- _**Use of higher learning rates during attack**_\n\n\n    **Table 1**:Detection accuracies against attacks with different learning rates. \n\n      -------------------------------------------------------------\n      |   Lr   | Method| FGSM | PGD |  CW |   DF  |  PA   |   AA  |\n      |------------------------------------------------------------\n      | 1/255  | TMM-O |  100 |99.99|90.67| 99.98 | 98.50 |  100  |\n      | 1/255  | TMM-L |  100 | 100 | 100 | 99.99 | 85.24 |  100  |\n      | 2/255  | TMM-O |  100 | 100 |88.71| 99.98 | 97.31 |  100  |\n      | 2/255  | TMM-L |  100 | 100 | 100 | 98.99 | 83.09 |  100  |\n      | 4/255  | TMM-O |  100 | 100 |96.44| 98.34 | 98.49 |  100  |\n      | 4/255  | TMM-L |  100 | 100 | 100 | 99.73 | 85.17 |  100  |\n      | 8/255  | TMM-O |  100 | 100 | 100 | 99.97 | 98.57 |  100  |\n      | 8/255  | TMM-L |  100 | 100 | 100 | 99.91 | 87.01 |  100  |\n      | 16/255 | TMM-O |  100 | 100 |98.78| 99.13 | 97.09 |  100  |\n      | 16/255 | TMM-L |  100 | 100 | 100 |  100  | 92.14 |  100  |\n      -------------------------------------------------------------\n\n    Ans: As we discussed in sec 3.1, the norm of our trigger is sampled between [$\\epsilon$, 0.99], which means that the Trapring can be quite wide, sort of creating an effect of filling up the vacant spaces between the data instances with Traps. Thus, it is actually easier for the attacks to fall into the Trapring with higher learning rate (step-size) as shown in the below table.\n\n\n- _**On the threat model and defender's knowledge about attack parameters**_\n\n\n    Ans:We are extremely sorry for the confusion created due to overloading the same symbol ($\\epsilon$) for both the trigger's lowest norm ($\\epsilon_{training}$, used during training) and attack's perturbation budget ($\\epsilon_{attack}$, used during attack). Trigger's lowest norm is set as small as possible so that clean accuracy is not hampered much. In our training, we used 0.01 (2.55/255) as the minimum $\\epsilon$ for training. This means that any attack using a larger perturbation budget than 0.01 will end up in the Trapring. Attacks with smaller perturbation budget either fails because of extreme closeness requirement to the genuine data or still gets detected because even though the Traprings are enforced starting at 0.01, it starts to induce Trapring formation even before that. In short, the defender does not need to know the attacker's perturbation budget ($\\epsilon_{attack}$) and it needs to use just the minimum perturbation ($\\epsilon_{training}$) that it can use to avoid drastic reduction in the clean data performance, which can usually be very small. Thus, our method can provide defense across a wide spectrum of attack parameters.\n\n- _**Q. CIFAR10.1 dataset results**_\n\n    Ans:As suggested, we have conducted the experiment on CIFAR10.1 dataset. Given table shows the performance. We can see only 1.81% increment in the False Positive (FP) rate without any drop in the detection accuracy.\n\n      ----------------------------------------------------------------------\n      |   Dataset  | Method| FP | FGSM | PGD |  CW |   DF  |  PA   |   AA  |\n      |---------------------------------------------------------------------\n      | CIFAR10    | TMM-O |5.33|  100 |98.87|94.21| 99.98 | 98.24 |  100  |\n      | CIFAR10    | TMM-L | 0.0|  100 | 100 |99.98| 99.80 | 81.54 |  100  |\n      | CIFAR10.1  | TMM-O |7.14|  100 |98.69|96.24| 98.73 | 98.20 |  100  |\n      | CIFAR10.1  | TMM-L | 0.0|  100 | 100 | 100 | 98.99 | 83.24 |  100  |\n      ----------------------------------------------------------------------\n\n- _**TMM on the point of view of Adversarial Training**_\n\n    Ans:In the adversarial training (AT), we use adversarial samples to make the model robust. The adversarial training has two significant drawbacks: 1) the robustness obtained by AT depends on the attack methods used in the adversarial sample generation process. It has been shown that adversarial robustness against one attack method does not transfer to other attack methods. 2) AT can have large impact on the clean data accuracy as it can blur the class boundaries between genuine classes. In contrast, our method do not need to know the attack method as our design ensures universal defense and it has a least effect as instead of blurring the class boundaries it creates Traprings around the genuine data which has a very low impact on the clean data accuracy. Thus our method offers a superior choice."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4256/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728044811,
                "cdate": 1700728044811,
                "tmdate": 1700728470373,
                "mdate": 1700728470373,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "l6AP7mIcGx",
            "forum": "y3BuJotSKl",
            "replyto": "y3BuJotSKl",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4256/Reviewer_izHU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4256/Reviewer_izHU"
            ],
            "content": {
                "summary": {
                    "value": "Making a model robust requires knowledge of allowable noise threshold, which is difficult to quanitfy apriori, and still faces a harsh trade-off betwen accuracy and robustness in practice. Out-of-distribution detectors are difficult to learn for complex datasets since there is usually a benign noise coefficient in observable data. Shortcuts (or Trapdoors) are a task-specific technique for robustifying models without having to specify the allowable noise. Some drawbacks of these techniques are computational complexity and loss of alignment with the main classifier (due to requiring an extra classifier for OOD detection). The authors propose a technique named Targeted Manifold Manipulation (TMM) based on modifying the gradient flow from the manifold around each genuine data point. The key concept is to force perturbed data points to fall into a new \"Trapclass\" label instantiated as a ring in the space around the data point. \n\nThe formulation assumes there is a clean dataset which can be used for synthesizing two new datasets, the robust and trap datasets. The robust dataset is created by applying perturbation artifacts sampled from a predetermined distribution (e.g., uniform) to each sample. The trap dataset is created by overlaying patches (triggers) onto clean instances, which then force the trapped instance to classify as the newly created Trapclass. Multiple triggers are created per image to create the trap-ring effect, each varied by both spatial location coordinates and norm. The classifier observes the three datasets and is tuned by a loss function factorized by each respective dataset. The defense achieves a high detection rate and the base classifier suffers minimal impact to the accuracy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The investigated problem is relevant to the broader research community - investigating computationally feasible DNN defenses which are robust to adaptive adversaries, while preserving the benign accuracy. \n* The buildup to methodology was well-written and motivated by shortcomings of previous work. The visualizations help provide a clear intuition of the methodology to the reader.\n* The authors compared with strong white-box baseline attack (AutoAttack) and a black-box attack. There is an analysis of an adaptive attack where the adversary has knowledge of the trigger placement and Trapclass. \n* The computational complexity is better than baseline detection methods which rely on separate classifiers. \n* The authors demonstrate on a variety of small-scale data that the detection accuracy of TMM is superior to Mahalanobis and LID detectors. \n* The proposed technique does not require knowledge of the clean test sample for detection."
                },
                "weaknesses": {
                    "value": "* The trigger creation process might induce higher sample complexity compared to the baseline adversarial training techniques, since the defender must generate additional trapdoor data, in addition to the robust region data. The additional data burden isn't measured. \n* The evaluation against semantic preserving attack was weak considering previous work already formulated strong perceptual attacks beyond simple brightness and blurring modification [1, 2]. The significance would be improved if the authors compare against these techniques. \n* It isn't clear how well the method works on large-scale data such as full ImageNet, since it is necessary to produce trapdoor pattern for every image. This aspect is not well discussed in the main text, but would impact the trap ring creation since each mask must account for additional locally sparse dimensions (i.e., the trap ring may begin to suffer from curse of dimensionality), making adversarial search easier. \n* There is no comparison to standard robustification techniques such as random smoothing [3] or vanilla adversarial training [4]. Without these it is difficult to measure the impact of the work to the broader community. \n* It is still necessary to pre-define the noise threshold before training, although a higher threshold seems to imply better detection accuracy based on the supplemental results.\n* An important takeaway of [1] is that robustification in one threat model can lead to brittleness in another unseen threat model. My main concern with the defense mechanism is the over-reliance on the genuine data, which may be low quality in practice or in the worst case, suffers poisoning from an adversary. It would be valuable to know the impact of data quality on the detection accuracy, since a real system would have to receive periodic model updates. I would be willing to increase my score if the authors investigated this. \n* The description of the trapdoor mask creation (Loss function for trapdoor) was difficult to follow and could be simplified. Some of the notation seemed superfluous, e.g., trying to describe every span of coordinates and every span of values of the location-parameterized mask. IMO it would be better to just define a tensor $T \\in \\mathbb{R}^{ch\\times m\\times n}$ which is alpha-blended and applied as in Equation 1 centered at a coordinate sampled from a range. Let values in $T$ sample from a predefined range. \n\n\n[1] Perceptual Adversarial Robustness: Defense Against Unseen Threat Models. http://arxiv.org/abs/2006.12655\n\n[2] RayS: A Ray Searching Method for Hard-label Adversarial Attack. http://arxiv.org/abs/2006.12792\n\n[3] Certified Adversarial Robustness via Randomized Smoothing. http://arxiv.org/abs/1902.02918\n\n[4] Towards Deep Learning Models Resistant to Adversarial Attacks. http://arxiv.org/abs/1706.06083"
                },
                "questions": {
                    "value": "* Can the authors check TMM in the presence of a poisoning adversary?\n* What is the effect of the genuine data quality on the final detection accuracy? \n* How much extra data is necessary to train with TMM?\n* Is TMM feasible on high-scale data? Would detection accuracy degrade due to extra sparse dimensions?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4256/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698784908173,
            "cdate": 1698784908173,
            "tmdate": 1699636392565,
            "mdate": 1699636392565,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QGrBKWV0yJ",
                "forum": "y3BuJotSKl",
                "replyto": "l6AP7mIcGx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4256/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4256/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank our reviewers' efforts to raise important points and provide valuable suggestions. \n\n- _**The effect of data quality**_\n\n   **Table 1**: Detection performance for model trained with datasets having varying quality.\n\n      -----------------------------------------------------------------------------------------------\n      |          Method   | Clean Acc.|  eps  |   FP  | FGSM |   PGD  |   CW   |   DF  |  PA   | AA |\n      |-----------------------------------------------------------------------------------------------\n      |    Clean Dataset  |   94.01   | 4/255 |  5.33 |  100 |  98.87  | 94.26 | 98.24 | 99.98 | 100 |\n      |    Noisy label    |   89.41   | 4/255 |  3.40 |  100 |  98.43  | 80.21 | 89.98 | 85.57 | 100 |\n      | Poison Adversary  |   90.61   | 4/255 |  4.59 |  100 |  97.57  | 78.39 | 85.36 | 82.36 | 100 |\n      ------------------------------------------------------------------------------------------------ \n\n    Ans:We investigated the data quality issue against two different kinds of noise: label noise and poison data as in BadNet [1] on the CIFAR-10 dataset. For label noise we randomly selected 5% of the training data and then changed their labels to randomly selected other labels. For poisoning data we again randomly selected 5% of the training data, added a trigger of size 5x5 (solid blue) and made their class label as 5 ('Dog'). As we can see form the below table that even with such a high level noise the detection rates across different attacks remain useful. Poisoning data has more adverse effect than noisy labels. Even though we got high backdoor accuracy (~100%) with this poison dataset, for most of the data instances our Trapring still provided a sharper gradient, thus forcing adversarial attacks to go through the Trapring, causing detection. CW method suffered the most, but looking at the false positive (FP) rate it looks like we can tune the thresholds ($\\xi$ and $\\rho$) to improve detection at the expense of a slight increase in FP. Currently, they have been set automatically based on making 0.5% FP for the training data. Finally, 5% noise is almost at the high-end of the noise and thus for actual deployment where such noise level will be much lower, we should expect closer performance to that of the clean dataset.\n\n\n- _**Performance on high-resolution dataset**_\n\n     Ans:To get quick result we had to resort to two other moderate/high-resolution datasets, instead of the imagenet dataset that the reviewer suggested. We used Stanford car [2] dataset that contains images of size 400x400 pixels and the Youtube face (YTF) [3] dataset that contains images of size 100x100 pixels. Stanford-car consists of 8,144 train samples and 8,041 test samples and it has 196 classes. YTF consists of 1,15,470 training samples, 12,592 test sample and 1283 classes. The only adjustment we had to do is the use of larger triggers (8x8). As the below table shows, with this change our defense remained strong against such moderate/high resolution images.\n    **Table 2**: Detection performance on datasets with high-resolution images.\n\n      ---------------------------------------------------------------------------------------------------\n      |     Dataset   | Clean Acc.|  eps  |  FP  | FGSM |  PGD  |  CW   |   DF  |  PA   | AA |\n      |--------------------------------------------------------------------------------------------------\n      | Stanford Car  |   97.45   | 4/255 | 0.8  |  100 | 99.98 | 96.43 | 97.62 | 99.07 | 100 | 98.22   |\n      | Youtube Face  |   98.94   | 4/255 | 3.17 |  100 |  100  | 99.99 | 98.08 | 97.03 | 100 | 97.31   |\n      ---------------------------------------------------------------------------------------------------\n- _**Size of the extra dataset**_\n\n    Ans: The extra datasets corresponding to robust and the backdoor loss functions can be created on the fly during the batch creation and thus we don't need to create separate datasets (more in section 4.1, and Table 4 in the supplementary). However, due to the effect of so many loss functions, we needed to train our models for longer time. For example, to achieve similar level of clean data accuracy on WideResNet (width-8, depth-20) we had to train our model on CIFAR-10 for 3,000 epochs, which is roughly 3-4 times that of learning a model only on the clean data. This is one of the significant requirement of our method.\n\n[1] Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Evaluating backdooring attacks on deep neural networks.\n\n[2] https://www.cv-foundation.org/openaccess/content_iccv_workshops_2013/W19/papers/Krause_3D_Object_Representations_2013_ICCV_paper.pdf\n\n[3] arXiv preprint arXiv:1712.05526, 2017."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4256/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727026878,
                "cdate": 1700727026878,
                "tmdate": 1700727435561,
                "mdate": 1700727435561,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]