[
    {
        "title": "HyperSINDy: Deep Generative Modeling of Nonlinear Stochastic Governing Equations"
    },
    {
        "review": {
            "id": "5WWa3sZKqR",
            "forum": "B4XM9nQ8Ns",
            "replyto": "B4XM9nQ8Ns",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8230/Reviewer_Bwz8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8230/Reviewer_Bwz8"
            ],
            "content": {
                "summary": {
                    "value": "The author proposed HyperSINDy, a variant of SINDy for discovering stochastic dynamical systems. It employs a variational encoder and a sparsity promoting loss function to recover the underlying equation forms."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Originality: The author proposed a new learning framework by combining the auto-encoder and sparse-promoting loss functions for equation discovery tasks.\n\nQuality: The result for the proposed method is comprehensive and includes various ODE cases. \n\nClarity: The figure and equation are well formulated in general. The writing is self-contained to understand\n\nSignificance:  The proposed methods integrates sparse equation discovery methods deep generative modeling."
                },
                "weaknesses": {
                    "value": "The problem setting is confusing and the baseline models are not adequate. Details see the questions part."
                },
                "questions": {
                    "value": "1. Problem setting about the stochastic system. As the author mentioned, they used an alternative definition of the stochastic dynamics, e.g random ODE. However, the reviewer is confused by the benefit of such definition compared to the deterministic setting. In result part 4.1, both the mean and the STD of the underlying system are shown. However, the STD form doesn't corresponds to any dynamics and the mean is close the the true mean of the system. In such circumstances, it seems only the mean estimation is important and the std cannot be leveraged to judge the performance of the proposed model. For part 4.2, the STD result is also confusing. It is compared with the diffusion terms but it is totally different from the true diffusion term. Therefore, the reviewer wants to ask why we need to include the STD and how it can help the proposed model. \n\n2. The result needs to compare with more SOTA models and include more comprehensive metrics. There are several model combining learning based method and SINDy-like algorithm for the equation discovery tasks [1][2]. Also, by checking the Figure 2, we could find that the discovery form is different under different $\\sigma$. For a equation discovery problem, it is important to get a consistent and correct form. Therefore the reviewer suggests adding more metrics on evaluating if the proposed model can get the correct form, e.g, precision and recall metrics.   \n              \n[1] Bayesian Spline Learning for Equation Discovery of Nonlinear Dynamics with Quantified Uncertainty. \n       \n[2] Physics-informed learning of governing equations from scarce data\n  \n3. The high dimensional 10d lorenz 96 is not compared with any baselines. Moreover, the analytical form is not listed in the main manuscript. Figure 4's caption says check equation 9 but that's for lorenz 63. Lorenz 96 should like equation 12 but with the concrete forcing terms. Equation 12 indicates that all the coefficients except the forcing terms should be close to 1 or -1. However, the discovered coefficients for $x_i$ is not close to these values. Again, the reviewer doesn't understand what is the gain of reporting the STD form of the equation here. \n\n4. The methodology part is confusing. Figures 2 says $\\theta$ has 3 terms but page 8 says $\\theta$ has 2 terms. Moreover, in the lower part of Figure 2, $z$ is firstly sampled from $p_{\\theta}(z)$ then was fed into decoder $H$. However, the definition of $\\theta$ has already included $H$, making the $H$ applied to $z$ 2 times. The term \"inference model\" is commonly used for test time, but the author use it to indicate training procedure. There are all the confusing parts need to be clarified."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8230/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8230/Reviewer_Bwz8",
                        "ICLR.cc/2024/Conference/Submission8230/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8230/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698824853120,
            "cdate": 1698824853120,
            "tmdate": 1700725082180,
            "mdate": 1700725082180,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OVHfBfXPqv",
                "forum": "B4XM9nQ8Ns",
                "replyto": "5WWa3sZKqR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8230/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Weaknesses\n\n> 1. The problem setting is confusing and the baseline models are not adequate. Details see the questions part.\n\n**Response:** Please see responses below.\n\n### Questions\n> 1. Problem setting about the stochastic system. As the author mentioned, they used an alternative definition of the stochastic dynamics, e.g random ODE. However, the reviewer is confused by the benefit of such definition compared to the deterministic setting. In result part 4.1, both the mean and the STD of the underlying system are shown. However, the STD form doesn't corresponds to any dynamics and the mean is close the the true mean of the system. In such circumstances, it seems only the mean estimation is important and the std cannot be leveraged to judge the performance of the proposed model. For part 4.2, the STD result is also confusing. It is compared with the diffusion terms but it is totally different from the true diffusion term. Therefore, the reviewer wants to ask why we need to include the STD and how it can help the proposed model.\n\n**Response:** We apologize for this confusion, which we believe stems from the interpretation of the RDE used to simulate the dynamics. In each case, mean and STD refer to the parameters of the (Gaussian) distribution associated with each coefficient; trajectories are simulated by constructing sample paths from these distributions. In other words, each coefficient is effectively coupled to an independent Wiener process; the system is parameterized by an n-dimensional Wiener  process. Accordingly, each ground truth trajectory is associated with only one particular sample path for the n-dimensional Wiener process.\n\nThe STDs factor into the dynamics by specifying the level of dynamical noise that drives each coefficient. The relevance of the STD to the dynamics is most clear in the rows of Fig. 2 corresponding to $\\sigma=10$. In these cases, the ground truth trajectory (\u201cOriginal\u201d) is highly stochastic. HyperSINDy succeeds in recovering the ground truth equation along with the correct coefficient means. However, the mean (middle column) is a poor representation of the original system, which behaves much more randomly. Thus, by discovering appropriately large STD values, HyperSINDy is able to accurately represent the level of uncertainty in the system\u2019s evolution; this also enables HyperSINDy to faithfully recapitulate the dynamical behavior of the original system. E-SINDy fails to capture the appropriate level of uncertainty (as it assumes no dynamical noise); as such, it cannot faithfully recapitulate the dynamical behavior of the original system (Fig. S4).\n\n> 2. The result needs to compare with more SOTA models and include more comprehensive metrics. There are several model combining learning based method and SINDy-like algorithm for the equation discovery tasks [1][2]. Also, by checking the Figure 2, we could find that the discovery form is different under different sigma . For a equation discovery problem, it is important to get a consistent and correct form. Therefore the reviewer suggests adding more metrics on evaluating if the proposed model can get the correct form, e.g, precision and recall metrics.\n\n**Response:** We believe that this point is partially addressed by our clarification of the current problem setting as stochastic rather than merely noisy, deterministic dynamics. Nonetheless, we completely agree that comparison against another state-of-the-art method for probabilistic equation discovery, such as Bayesian Spline Learning, would improve the manuscript. We have updated the manuscript to include comparisons with this method along with additional quantitative metrics \u2013 thank you for these suggestions.\nRefer to Table S4 in the appendix for precision and recall results that complement the experiments ran for Table 1 in the main text (on Lorenz and Rossler systems generated with 10 seeds). Refer to Table S5 in the appendix for RMSE, precision, and recall comparisons between HyperSINDy and E-SINDy on multiple 10D Lorenz-96 trajectories, each generated with a different noise level. Refer to Table S6 in the appendix for RMSE, precision, and recall comparisons between HyperSINDy, E-SINDy, and Bayesian Spline Learning on multiple Lotka-Voltera trajectories simulated as RDEs with Gaussian noise on every coefficient. (response continued below)"
                    },
                    "title": {
                        "value": "Rebuttal (Part 1/2)"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700433799884,
                "cdate": 1700433799884,
                "tmdate": 1700433814455,
                "mdate": 1700433814455,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nxkA2bhQ0l",
                "forum": "B4XM9nQ8Ns",
                "replyto": "5WWa3sZKqR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8230/Reviewer_Bwz8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8230/Reviewer_Bwz8"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the detailed rebuttal"
                    },
                    "comment": {
                        "value": "I have changed my score to reflect these efforts."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725111978,
                "cdate": 1700725111978,
                "tmdate": 1700725125767,
                "mdate": 1700725125767,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8MI7biyFp6",
            "forum": "B4XM9nQ8Ns",
            "replyto": "B4XM9nQ8Ns",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8230/Reviewer_UKtm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8230/Reviewer_UKtm"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new framework HyperSINDy (Hyper sparse identification of nonlinear dynamics) to address the symbolic regression problems in high-dimensional, stochastic setting. Within a variational autoencoder, they use an encoder to learn the parameters $\\mu, \\sigma$ of the latent states $\\mathbf{z}$, and a generative model to learn $p(\\dot{\\mathbf{x}}|\\mathbf{x}, \\mathbf{z})$ where $\\mathbf{\\dot{x}}$ is parameterized by $f_\\mathbf{z}(\\mathbf{x})$. With proper choice of $f_\\mathbf{z}(\\mathbf{x})$, they build the relationship between derivatives and coefficients for addressing the task of SINDy."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper is well written.\n\nThe idea of mapping a high-dimensional, stochastic data to a low dimensional latent space and learning the coefficients through a hyper network which takes low-dimensional latent variables are novel."
                },
                "weaknesses": {
                    "value": "The capacity of $\\Theta(\\mathbf{x})$ still holds as a constraint for the performance, especially in the high-dimensional setup. It would be great if the authors could discuss the impact of the $\\Theta(\\mathbf{x})$. For example, what would the performance be if certain symbolic terms (shown up in the true equations) are missing in the dictionary in $\\Theta(\\mathbf{x})$."
                },
                "questions": {
                    "value": "Q1. What is the column of ''STD'' in Figure 2 showing? Are they showing the standard deviation of the estimates? If that is the case, plugging in the standard deviation as the coefficients in the equations are confusing.\n\nQ2. It would be great if the authors could provide more evaluation metrics for generated trajectories. Metrics like Lyapunov exponents would be helpful to see how good the performance is.\n\nQ3. How robust the performance would be across different choice of the dimension of $\\mathbf{z}$?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8230/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698857088643,
            "cdate": 1698857088643,
            "tmdate": 1699637022644,
            "mdate": 1699637022644,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "msbl34THcG",
                "forum": "B4XM9nQ8Ns",
                "replyto": "8MI7biyFp6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8230/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Weakness\n\n> 1. The capacity of (theta(x)  still holds as a constraint for the performance, especially in the high-dimensional setup. It would be great if the authors could discuss the impact of theta(x) . For example, what would the performance be if certain symbolic terms (shown up in the true equations) are missing in the dictionary theta(x).\n\n**Response:** We agree that the model is constrained by the capacity of the function library. This is true of most equation discovery methods. The polynomial basis has broad applicability; moreover, there is often a priori knowledge motivating choice of specific classes of basis functions. The SINDy framework has had widespread success with alternative libraries (e.g., Kaptanoglu et al., 2022). Our method is amenable to such alternative libraries, and thus retains the flexibility of the SINDy framework.\n\n### Questions\n\n> 1. What is the column of ''STD'' in Figure 2 showing? Are they showing the standard deviation of the estimates? If that is the case, plugging in the standard deviation as the coefficients in the equations are confusing.\n\n**Response:** Thank you for pointing out this confusion. Correct, this figure is showing the standard deviation of the estimated coefficients, although these standard deviations correspond to the noise processes that contribute to the dynamics. We have updated the figure to clarify the interpretation of the standard deviations.\n\n> 2.  It would be great if the authors could provide more evaluation metrics for generated trajectories. Metrics like Lyapunov exponents would be helpful to see how good the performance is.\n\n**Response:** Thank you for this excellent suggestion. Although we were unable to carry out a systematic analysis of the Lyupanov exponents for generated trajectories, we have incorporated two additional metrics, precision and recall (Tables S4-S6). These additional metrics enable assessment of the ability to recover the correct terms of the original equation.\n\n> 3. How robust the performance would be across different choice of the dimension of z?\n\n**Response:** Thank you for raising this important question. We have carried out additional experiments to assess performance as a function of latent dimension. Specifically, we train HyperSINDy models with varying dimensions of z on each of the 3D Stochastic Lorenz trajectories ($\\sigma = 1$, $\\sigma = 5$, $\\sigma = 10$).\n\nResults from this procedure are shown in Fig. S6. We plot the RMSE of the mean and standard deviation of discovered coefficients, as compared to ground truth. It is true RMSE can vary as a function of z dimension. It is noteworthy, however, that for each trajectory, RMSE is always lower than E-SINDy, regardless of z dimension (refer to Table 1 for E-SINDy RMSE). These results help validate our choice of z in the paper: we use 2x the state dimension for z, and this seems to have good performance at balancing RMSE of both the mean and standard deviation of discovered coefficients.\n\nReferences\n\nKaptanoglu, A.A. et al. (2022) \u2018PySINDy: A comprehensive Python package for robust sparse system identification\u2019, Journal of Open Source Software, 7(69), p. 3994. Available at: https://doi.org/10.21105/joss.03994."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700433038504,
                "cdate": 1700433038504,
                "tmdate": 1700433038504,
                "mdate": 1700433038504,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wZzoBVTBNF",
                "forum": "B4XM9nQ8Ns",
                "replyto": "msbl34THcG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8230/Reviewer_UKtm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8230/Reviewer_UKtm"
                ],
                "content": {
                    "comment": {
                        "value": "I want to thank the authors for the detailed response. I appreciate the additional experiments, understand the constraints of the function library (and also the possibility of alleviation through prior knowledge), and am also aware of the ongoing discussions."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714350764,
                "cdate": 1700714350764,
                "tmdate": 1700714350764,
                "mdate": 1700714350764,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "R2GkUMCk2n",
            "forum": "B4XM9nQ8Ns",
            "replyto": "B4XM9nQ8Ns",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8230/Reviewer_cj4i"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8230/Reviewer_cj4i"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes HyperSindy, which is a framework for modeling stochastic nonlinear dynamics. First a variational autoencoder is used to model the distribution of observed states and derivatives. Samples from the VAE are used with a hypernetwork to obtain the coefficients of the differential equations. These coefficients are combined with a function library to obtain the derivatives, allowing for the functional form of the equations to be learned. Experiments are conducted using simulated data, which show promising results."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper aims to learn both the parameters and the functional form of stochastic differential equations from data, which is a significant problem for scientific applications. The use of VAEs and hypernetworks for this problem is quite novel to my knowledge. The paper is well written and organized."
                },
                "weaknesses": {
                    "value": "Experiments are conducted in simulated environments where the simulation parameters match to the modeling assumptions (mainly around Gaussianity). I would love to see more experiments confirming the applicability of the approach to broader problems, especially with real data."
                },
                "questions": {
                    "value": "- As mentioned above, all the experiments are conducted using Gaussian distributions which match to the posterior distribution assumed for variational inference. Can authors comment on the limitations of these experiments?\n- The approach aims to learn both the functional form and parameters of the differential equations. Even though I agree that this might help with interpretability, I worry that the identifiability issues might be prominent. Do the authors expect any identifiability problems?\n- The promise of learning functional form is achieved through the function library. Are there any limitations of using such an approach?\n- What are the limitations of using a Gaussian prior with diagonal covariance for the generative model?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8230/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698874604394,
            "cdate": 1698874604394,
            "tmdate": 1699637022485,
            "mdate": 1699637022485,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hutFrR2PN3",
                "forum": "B4XM9nQ8Ns",
                "replyto": "R2GkUMCk2n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8230/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Weaknesses\n\n>1. Experiments are conducted in simulated environments where the simulation parameters match to the modeling assumptions (mainly around Gaussianity). I would love to see more experiments confirming the applicability of the approach to broader problems, especially with real data.\n\n**Response:** The insight exploited in this paper is that the standard VAE gaussian prior is well-matched to the well-studied case of stochastic dynamics involving white noise. We completely agree that extension to other kinds of noise will be an important direction for future extensions. As an initial investigation into the feasibility for such extensions, we have included a new experiment (Fig. S5) that supports the potential for HyperSINDy to model more complicated noise types (here, noise drawn from a half-Normal distribution).\n\n### Questions\n\n> 1. As mentioned above, all the experiments are conducted using Gaussian distributions which match to the posterior distribution assumed for variational inference. Can authors comment on the limitations of these experiments?\n\n**Response:** (see above response)\n\n> 2. The approach aims to learn both the functional form and parameters of the differential equations. Even though I agree that this might help with interpretability, I worry that the identifiability issues might be prominent. Do the authors expect any identifiability problems?\n\n> 3. The promise of learning functional form is achieved through the function library. Are there any limitations of using such an approach?\n\n**Response:** We agree that there can be identifiability issues in equation learning; addressing these issues generally comes at the cost of reduced flexibility. This is precisely the tradeoff reflected in these questions. Thus, specifying a function library and imposing sparsity are significant aids toward identifying a parsimonious model. Moreover, by assuming the structure of the noise, we avoid the major identifiability issues that arise when attempting to learn the form of both the deterministic and stochastic terms. These constraints each necessitate a loss of generality, though they are appealing as they retain very broad applicability.\nThe primary limitation of a function library is that it requires a priori knowledge about the dynamics, i.e. some inductive bias is necessary. However, this inductive bias is also the primary motivation behind equation discovery methods: we want to discover a sparse equation from a finite set of possible terms that could describe the system\u2019s dynamics. One alternative might be to enable a more fully data-driven search for the basis functions (e.g., use via a genetic algorithm (Schmidt and Lipson, 2009)), although such approaches are generally incur much greater computational cost.\n\n> 4. What are the limitations of using a Gaussian prior with diagonal covariance for the generative model?\n\n**Response:** One limitation is that such a prior assumes the true latent vector is built of multiple  i.i.d. random variables (diagonal covariance). However, one could certainly imagine noise processes for which this assumption does not hold. Even though the hypernetwork contains nonlinearities (and thus can model dependencies between coefficients), some systems may still be challenging to learn. Future work could explore using alternate priors, such as Gaussian Process Variational Autoencoder (Casale et al., 2018), which may better account for such dependencies.\n\nDespite this limitation, we note that the network is able to learn more complex distributions that include dependencies between coefficients. In particular, refer to Figure S5 in the appendix for results on a Lotka-Volterra system simulated with coefficients drawn from a Half-Normal distribution. Even though HyperSINDy uses a standard gaussian prior with diagonal covariance, it is able to approximate the shape of the distribution of the true non-Gaussian coefficients much better than E-SINDy. We view this experiment as a proof of principle for the flexibility provided by hypernetwork\u2019s nonlinearities: it is able to learn a mapping from latent space (a simple standard gaussian) to a much more complex coefficient space.\n\nReferences\n\nSchmidt, M. and Lipson, H. (2009) \u2018Distilling Free-Form Natural Laws from Experimental Data\u2019, Science, 324(5923), pp. 81\u201385. Available at: https://doi.org/10.1126/science.1165893.\n\nCasale, F.P. et al. (2018) \u2018Gaussian Process Prior Variational Autoencoders\u2019, in Advances in Neural Information Processing Systems. Curran Associates, Inc. Available at: https://proceedings.neurips.cc/paper_files/paper/2018/hash/1c336b8080f82bcc2cd2499b4c57261d-Abstract.html (Accessed: 19 November 2023)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700432937662,
                "cdate": 1700432937662,
                "tmdate": 1700432937662,
                "mdate": 1700432937662,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XCeHwFLiRC",
                "forum": "B4XM9nQ8Ns",
                "replyto": "R2GkUMCk2n",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8230/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your prompt follow-up. As elaborated in our latest response to Reviewer 7UM6, we stress that our manuscript does not claim the ability to explicitly map between particular RDE and SDE expressions, and our results are not contingent on the ability to carry out this transformation. Rather, we simply note that the conjugacy between the two formulations means that the results obtained in one framework are recognized to be directly pertinent to those obtained in the other (Han and Kloeden, 2017). Thus, our work retains relevance to a very broad class of problems that have been modeled as SDEs \u2013 ultimately, SDEs and RDEs are simply two alternative modeling choices for studying dynamics whose evolution involves some uncertainty.\n\nNote that the Lotka-Volterra SDE problem (Fig. 3) demonstrates the utility of our approach for a problem explicitly formulated as an SDE -- HyperSINDy captures the appropriate dynamics (as confirmed by evaluating the Kramers-Moyal coefficients) and, importantly, identifies the physical terms that are present in the drift component of the original SDE, thus supporting insights into the physical interactions in the system.\n\nWe will update the Appendix to include these clarifications, as well as an example transformation for multiplicative noises (see latest response to Reviewer 7UM6."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700445335365,
                "cdate": 1700445335365,
                "tmdate": 1700447321346,
                "mdate": 1700447321346,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MYGWiPrCKP",
            "forum": "B4XM9nQ8Ns",
            "replyto": "B4XM9nQ8Ns",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8230/Reviewer_kqcu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8230/Reviewer_kqcu"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes HyperSINDy, a method for unsupervised discovery of governing differential equations from data in the stochastic setting. HyperSINDy combines variational Bayesian inference and hypernetworks (Ha et al., 2016) to build a generative model of the data. An L0 regularization scheme based on concrete random variables is used to ensure that the final differential equation learned is sparse and interpretable. HyperSINDy outperforms the previous state of the art in both random differential equation and stochastic differential equation settings."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "- **The paper is very well written.** The presentation on the backgrounds (the SINDy framework, variational inference, L0 regularization) is very clear and the graphics help the readers better understand how the HyperSINDy framework works. There is also good discussion of the related works, i.e. ensembling methods and SDE-based approaches, which gives good motivation to the proposed method.\n- **The proposed method is novel and achieves good improvements over existing methods.** It seems that the random differential equations (RDE) approach is a pretty novel perspective, and it is very natural to combine it with generative modeling. The HyperSINDy method also achieve uniformly better mean-squared error as well as uncertainty estimation than the best existing approach.\n\nThis paper seems like a solid advancement towards solving the very important problem of data-driven discovery of interpretable stochastic governing equations. This work will have wide applications in machine learning for science."
                },
                "weaknesses": {
                    "value": "- **Experimental results on higher dimensional datasets might be a bit lacking.** One of the important claims of the advantage of HyperSINDy is that it circumvents the curse of dimensionality which hinders the performance of other methods. However, only the HyperSINDy results for one 10D system is given. It might be better if the authors can clarify how the other methods perform on this system, and/or give other examples of high dimensional systems."
                },
                "questions": {
                    "value": "- In section 3, in \"$H$ implements the implicit distribution $p_\\theta(\\mathbf{\\Xi}|\\mathbf{z})$\", why is it the \"implicit distribution?\" From my understanding, shouldn't $\\Xi_z$ just be a delta distribution (deterministic) on $H(z)$?\n- $p_\\theta(z)$ is modeled to be a standard Gaussian with diagonal covariance. Would the independence between different $z_t$ allow sudden jumps in the parameters of the system? Would it be better to model it as something like a Gaussian process?\n- Related to the last question: does the discretization step size influence the model learning result?\n- I don't think what \"E-SINDy\" stands for is ever introduced in the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8230/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8230/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8230/Reviewer_kqcu"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8230/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698994190467,
            "cdate": 1698994190467,
            "tmdate": 1699637022352,
            "mdate": 1699637022352,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vOw05SlL5u",
                "forum": "B4XM9nQ8Ns",
                "replyto": "MYGWiPrCKP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8230/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Weaknesses\n\n> 1. Experimental results on higher dimensional datasets might be a bit lacking. One of the important claims of the advantage of HyperSINDy is that it circumvents the curse of dimensionality which hinders the performance of other methods. However, only the HyperSINDy results for one 10D system is given. It might be better if the authors can clarify how the other methods perform on this system, and/or give other examples of high dimensional systems.\n\n**Response:** Thank you for raising this point. Indeed, the ability to scale to higher dimensions is a distinguishing feature of HyperSINDy. To clarify, the limited scalability of existing SDE discovery methods precludes an assessment of their performance on the 10D system. However, we agree that it would still be informative to benchmark HyperSINDy\u2019s performance against other methods for probabilistic model discovery (of deterministic ODEs) in this high-dimensional setting.\n\nTo this end, we have updated the manuscript with new experiments applying E-SINDy to the 10D Lorenz system.\nIn particular, we simulated the 10D Lorenz system with 3 different levels of noise on the forcing term ($\\sigma=0$, $\\sigma=5$, $\\sigma=10$) and trained HyperSINDy and E-SINDy on each trajectory. We evaluated the RMSE of the mean and standard deviation of the discovered coefficients, as well as precision and recall. Refer to Table S5 in the appendix for the full results. Although E-SINDy achieves slightly improved performance in the noise-free case ($\\sigma=0$), HyperSINDy outperforms E-SINDy across all metrics on  both of the higher noise levels.\n\n### Questions\n\n> 1. In section 3, in \"H implements the implicit distribution p(xi_z | z)\", why is it the \"implicit distribution?\" From my understanding, shouldn't xi_z  just be a delta distribution (deterministic) on H(z)?\n\n**Response:* We apologize for the lack of clarity \u2013 we believe this reflects the distinction between z as a random sample vs. a random variable. For a specific sample $\\hat{z}, Xi_{z} = H(z)$ is indeed deterministic. For the more general case corresponding to z as a random variable, p(Xi | z) refers to the posterior distribution of SINDy coefficients, conditioned on z. In this setting, the hypernetwork can be interpreted as an implicit distribution for p(Xi | z) (i.e. the neural network weights, biases, and architecture parameterize the distribution p(Xi | z), where z is a random variable). We use \u201cimplicit distribution\u201d to mean that we do not actually have full access to the distribution of p(Xi | z), but can still sample from it. For example, if we sample an ensemble of z vectors, then feed that ensemble into H, we are effectively sampling from p(Xi | z). We have revised this line for clarity, including a pointer to (Pawlowski et al., 2018)."
                    },
                    "title": {
                        "value": "Rebuttal (Part 1/2)"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700432466074,
                "cdate": 1700432466074,
                "tmdate": 1700432517217,
                "mdate": 1700432517217,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zkJ6FiyFVy",
                "forum": "B4XM9nQ8Ns",
                "replyto": "MYGWiPrCKP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8230/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (Part 2/2)"
                    },
                    "comment": {
                        "value": "> 2. p(z) is modeled to be a standard Gaussian with diagonal covariance. Would the independence between different z_t allow sudden jumps in the parameters of the system? Would it be better to model it as something like a Gaussian process?\n\n**Response:** Yes, this independence could allow sudden jumps in the parameters of the system. However, because HyperSINDy learns to approximate the (potentially complex) distribution of states and derivatives (i.e., q_\\phi(z|x,\\dot{x} is kept close to the prior p_theta(z)), independent samples z_t are transformed into realizations of system coefficients that are similar to those observed in the original data. In other words, over the course of training, HyperSINDy learns to utilize i.i.d. dynamical noise to mimic the dynamical behavior of the observed data \u2013 including tendency toward sudden jumps. This is an important feature distinguishing HyperSINDy from other probabilistic methods that do not explicitly model the role of dynamical noise in the generation of the observed time series.\nWe agree that Gaussian processes would provide an ideal framework for modeling the noise, particularly when moving beyond the i.i.d. noise case (see also (Casale et al., 2018; Mishra et al., 2022)). For our work, we assumed that the noise was i.i.d. in time, meaning that the independence of z_ts is a desirable modeling feature (as per the standard SDE framework). However, we note that extension to temporally correlated noise will be an important direction for future inquiry.\n\n> 3. Related to the last question: does the discretization step size influence the model learning result?\n\n**Response:** The discretization step size can influence results, although this is not an issue connected to HyperSINDy per se \u2013 rather, this relates to the more general and well-studied phenomenon of \u201cfinite-time effects\u201d emerging in the numerical analysis of continuous stochastic dynamics based upon discretely sampled time series. Algorithmically correcting for such effects has been the topic of numerous works (e.g., (Ragwitz and Kantz, 2001; Lade, 2009; Honisch, 2011; Boujo and Cadot, 2019)), including one such approach recently introduced for stochastic SINDy (Callaham et al. 2021). Recent works also introduce promising approaches toward finite-time correction in the high-dimensional setting (e.g., (Br\u00fcckner, Ronceray and Broedersz, 2020; Frishman and Ronceray, 2020)), suggesting a number of promising future directions for HyperSINDy to be extended to the theoretically continuous case.\n\n> 4. I don't think what \"E-SINDy\" stands for is ever introduced in the paper.\n\n**Response:** Thank you for pointing out this oversight! We have updated the Introduction to define E-SINDy.\n\nReferences \n\nBauer, S. et al. (2017) \u2018Efficient and Flexible Inference for Stochastic Systems\u2019, in Advances in Neural Information Processing Systems. Curran Associates, Inc. Available at: https://papers.nips.cc/paper/2017/hash/e0126439e08ddfbdf4faa952dc910590-Abstract.html\n\nBoujo, E. and Cadot, O. (2019) \u2018Stochastic modeling of a freely rotating disk facing a uniform flow\u2019, Journal of Fluids and Structures, 86, pp. 34\u201343. Available at: https://doi.org/10.1016/j.jfluidstructs.2019.01.019.\n\nBr\u00fcckner, D.B., Ronceray, P. and Broedersz, C.P. (2020) \u2018Inferring the Dynamics of Underdamped Stochastic Systems\u2019, Physical Review Letters, 125(5), p. 058103. Available at: https://doi.org/10.1103/PhysRevLett.125.058103.\n\nCasale, F.P. et al. (2018) \u2018Gaussian Process Prior Variational Autoencoders\u2019, in Advances in Neural Information Processing Systems. \n\nFrishman, A. and Ronceray, P. (2020) \u2018Learning Force Fields from Stochastic Trajectories\u2019, Physical Review X, 10(2), p. 021009. Available at: https://doi.org/10.1103/PhysRevX.10.021009.\n\nHonisch, C. (2011) \u2018Estimation of Kramers-Moyal coefficients at low sampling rates\u2019, Physical Review E, 83(6). Available at: https://doi.org/10.1103/PhysRevE.83.066701.\n\nLade, S.J. (2009) \u2018Finite sampling interval effects in Kramers\u2013Moyal analysis\u2019, Physics Letters A, 373(41), pp. 3705\u20133709. Available at: https://doi.org/10.1016/j.physleta.2009.08.029.\n\nMishra, S. et al. (2022) \u2018piVAE: a stochastic process prior for Bayesian deep learning with MCMC\u2019, Statistics and Computing, 32(6), p. 96. Available at: https://doi.org/10.1007/s11222-022-10151-w.\n\nPawlowski, N. et al. (2018) \u2018Implicit Weight Uncertainty in Neural Networks\u2019. arXiv. Available at: https://doi.org/10.48550/arXiv.1711.01297.\n\nRagwitz, M. and Kantz, H. (2001) \u2018Indispensable Finite Time Corrections for Fokker-Planck Equations from Time Series Data\u2019, Physical Review Letters, 87(25), p. 254501. Available at: https://doi.org/10.1103/PhysRevLett.87.254501.\n\nSolin, A., Tamir, E. and Verma, P. (2021) \u2018Scalable Inference in SDEs by Direct Matching of the Fokker\u2013 Planck\u2013 Kolmogorov Equation\u2019, in Advances in Neural Information Processing Systems."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700432643462,
                "cdate": 1700432643462,
                "tmdate": 1700432686901,
                "mdate": 1700432686901,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KCn6bnjL9T",
                "forum": "B4XM9nQ8Ns",
                "replyto": "vOw05SlL5u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8230/Reviewer_kqcu"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8230/Reviewer_kqcu"
                ],
                "content": {
                    "comment": {
                        "value": "I thank the authors for their detailed responses. On the implicit distribution question, I think you confused the marginal distribution p(Xi) = \\int p(Xi | z) p(z) dz that integrates out the zs (which can be approximated by sampling an ensemble of zs like you correctly pointed out), with the conditional distribution p(Xi | z) which is a delta distribution. It would be nice if the authors can fix this or provide further clarification on this point."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700607179611,
                "cdate": 1700607179611,
                "tmdate": 1700607179611,
                "mdate": 1700607179611,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "rM7BRLbMBo",
            "forum": "B4XM9nQ8Ns",
            "replyto": "B4XM9nQ8Ns",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8230/Reviewer_WJp7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8230/Reviewer_WJp7"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces HyperSINDy, a framework to model a family of special stochastic dynamics via a deep generative model of sparse, nonlinear governing equations whose parametric form is discovered from data.  HyperSINDy is built upon the combination of hypernetwork and SINDy and can learn a family of stochastic dynamics whose coefficients are driven by a Wiener process.\n\nThe main contributions of the HyperSINDy are summarized as follows: \n(1) This framework can efficiently and accurately model random differential equations (random ODEs), whose coefficients are parameterized by a Wiener process. Hence, it provides a generative modeling of stochastic dynamics when their random ODE forms are driven by white noises.\n\n(2) HyperSINDy can discover the analytical form of a sparse governing equation without a-priori knowledge. Also, by using the sparse masks, the computational complexity of HyperSINDy is scalable."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(1) The authors represent a proof of concept for this architecture, the manuscript is well written. The numerical results are convincing.\n(2) The authors of this work employ the random differential equations (random ODEs) as the library of candidate functions for SINDy. This approach is innovative and it enables the extension of SINDy from deterministic to stochastic dynamics."
                },
                "weaknesses": {
                    "value": "(1) Random differential equations are conjugate to stochastic differential equations. It is unclear how to convert a general SDEs into its random ODEs representations, for example, the Langevin type dynamics.\n(2) This manuscript lacks a comparison with other methods. \n(3) Although the authors have commented in the manuscript, it is still unclear if this HyperSINDy framework can handle complex noise terms as well as the robustness of noises."
                },
                "questions": {
                    "value": "(1) This manuscript could have been enhanced if it can provide examples of learning underdamped Langevin systems, for example, learn the harmonic oscillator under  thermal bath. \n(2) In particular, the manuscript could have been enhanced if it can provide an appendix discussion on how to construct a Random ODE representation for a general SDE.\n(3) The manuscript could have been enhanced if it can provide numerical examples when different types of noises are added to the observation data."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8230/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8230/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8230/Reviewer_WJp7"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8230/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699324391265,
            "cdate": 1699324391265,
            "tmdate": 1699637022217,
            "mdate": 1699637022217,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BoXo8be395",
                "forum": "B4XM9nQ8Ns",
                "replyto": "rM7BRLbMBo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8230/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### Weaknesses\n> 1. Random differential equations are conjugate to stochastic differential equations. It is unclear how to convert a general SDEs into its random ODEs representations, for example, the Langevin type dynamics.\n\n**Response:** Although this conjugacy is well-established, explicit transformations between SDEs and RDEs are not always straightforward. The standard procedure involves substitution via an Ornstein-Uhlenbeck process, the solution to a linear SDE. Following Han & Kloeden (2017), we may demonstrate this transformation for a scalar SDE with additive noise:\n\n$$\n\\begin{align}\n\tdX_t = f(X_t)dt + dWt\n\\end{align} $$\nbecomes\n$$\\begin{align}\n\t\\dot{Z_t} = f(Z_t + O_t) + O_t,\n\\end{align} $$\nwhere $Z_t \\coloneqq X_t - O_t$ and $O_t$ is the stationary Ornstein-Uhlenbeck process satisfying the SDE $dO_t = -O_tdt + dW_t$.\n\nA similar conjugation can work for many cases of additive or multiplicative SDEs. In general, though, this conversion is not straightforward to implement. Thus, although we appeal to RDE-SDE conjugacy in order to validate results obtained in the RDE framework, the ability to convert between these representations in practice is more challenging.\n\nOn the other hand, this challenge of converting between SDE and RDE formulations highlights unique advantages of our problem formulation. Indeed, much of the interest in RDEs is motivated from the practical advantages afforded by this formulation in certain scenarios in comparison to SDEs (e.g., (Bauer et al., 2017; Solin, Tamir and Verma, 2021)).\n\nWe have added this discussion to the Supplementary Appendix.\n\n> 2. This manuscript lacks a comparison with other methods.\n\n**Response:** Our original submission included comparisons with the leading \u201censemble SINDy\u201d method for robust equation discovery in the presence of noise, and with \u201cstochastic SINDy\u201d for a two-dimensional SDE discovery model. We have updated the manuscript to also include comparisons with Bayesian Spline Learning. Refer to Table S6 in the appendix for comparisons between HyperSINDy, E-SINDy, and Bayesian Spline Learning on a Lotka-Volterra RDE simulation.\n\n> 3. Although the authors have commented in the manuscript, it is still unclear if this HyperSINDy framework can handle complex noise terms as well as the robustness of noises.\n\n**Response:** We have performed new experiments to address the question of how our method performs under different scenarios for the noise. Refer to Figure S5 in the appendix for results comparing HyperSINDy and E-SINDy on a Lotka-Volterra system simulated as an RDE, but where each coefficient is drawn from a Half-Normal distribution. In this case, the distribution of coefficients does not match the Gaussian assumption of the latent space; however, HyperSINDy is still able model the complex coefficient distributions due to the expressivity provided by its neural network. This result establishes that the method has potential to generalize beyond the iid noise case, while leaving for subsequent work a more systematic validation in different noise settings.\n\n### Questions\n> 1. This manuscript could have been enhanced if it can provide examples of learning underdamped Langevin systems, for example, learn the harmonic oscillator under thermal bath. \n\n**Response:** Thank you for raising these possibilities for further extensions of our approach. Our updated Appendix clarifies that explicit RDE-SDE transformation is not always trivial, meaning the recasting of these canonical physics SDE problems may not be entirely straightforward; however, we agree that it represents an important future direction for broadening the scope of our framework.\n\n> 2.  In particular, the manuscript could have been enhanced if it can provide an appendix discussion on how to construct a Random ODE representation for a general SDE.\n\n**Response:** Thank you for making this suggestion, which we agree aids the interpretation of our framework. We have updated the appendix as requested (see also response to Weaknesses #1 above).\n\n> 3. The manuscript could have been enhanced if it can provide numerical examples when different types of noises are added to the observation data.\n\n**Response:** Please see response to Weaknesses #3 above.\n\nReferences\n\nBauer, S. et al. (2017) \u2018Efficient and Flexible Inference for Stochastic Systems\u2019, in Advances in Neural Information Processing Systems. Curran Associates, Inc. Available at: https://papers.nips.cc/paper/2017/hash/e0126439e08ddfbdf4faa952dc910590-Abstract.html\n\nHan, X. and Kloeden, P.E. (2017) Random Ordinary Differential Equations and Their Numerical Solution. Singapore: Springer Singapore (Probability Theory and Stochastic Modelling). Available at: https://doi.org/10.1007/978-981-10-6265-0.\n\nImkeller, P. and Schmalfuss, B. (2001) \u2018The Conjugacy of Stochastic and Random Differential Equations and the Existence of Global Attractors\u2019, Journal of Dynamics and Differential Equations, 13(2), pp. 215\u2013249."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700432112504,
                "cdate": 1700432112504,
                "tmdate": 1700432112504,
                "mdate": 1700432112504,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gRtDYH0z0H",
                "forum": "B4XM9nQ8Ns",
                "replyto": "hutFrR2PN3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8230/Reviewer_WJp7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8230/Reviewer_WJp7"
                ],
                "content": {
                    "comment": {
                        "value": "I want to first thank the response and revision from the authors. However, I feel that the current manuscript still lacks clarification of some critical points that affect the quality of the proposed scheme. \n\n(1) As mentioned by the authors, it is in general not straightforward to transform a SDE to a RODE. In fact, the transformation could be highly non-trivial and limits the application of the proposed scheme. See for example \"The shifted ODE method for underdamped Langevin MCMC. by James Foster, Terry Lyons and Harald Oberhauser\".\n(2) It is still not clear to me how to transform multiplicative noises."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700441658933,
                "cdate": 1700441658933,
                "tmdate": 1700441658933,
                "mdate": 1700441658933,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iMQUg5Z45W",
            "forum": "B4XM9nQ8Ns",
            "replyto": "B4XM9nQ8Ns",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8230/Reviewer_7UM6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8230/Reviewer_7UM6"
            ],
            "content": {
                "summary": {
                    "value": "Authors introduce a framework called HyperSINDy for modeling stochastic dynamics using a deep generative model that discovers the parametric form of sparse governing equations from data. It employs an inference model and generative model to discover\nan analytical representation of observed stochastic dynamics in the form of a random ODE (RODE). It is particularly useful for random coefficients."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Figure 1 shows the scheme of this method. It has three steps: inference mode, generative model and SINDy. It basically glue the Hypernetwork and SINDy together to tackle the random coefficient case."
                },
                "weaknesses": {
                    "value": "1. It is a typical A+B type of paper. Each part is well studied and author glue them together and demonstrate it in several simple examples. I don't think there is enough novelty here.\n\n2. All three examples are artificially made for this algorithm. All examples are corrected identified but I am not impressed unless authors are able to demonstrate some non-trivial RODE. The second example equation (11) is not even a valid example of stochastic  Lotka-Volterra. I don't know what is N(0,1) on the Right hand side means here. \n\n3. Authors have limited knowledge on RODE here in fact not all SDE can be transformed to RODE and vice versa. And in general RODE case, z is not independent with x."
                },
                "questions": {
                    "value": "If x' is not available (e.g., after training), z is sampled from the prior z \u223c p_\u03b8(z) to produce \\Xi. I don't understand this part. Please elaborate more or give an example."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8230/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8230/Reviewer_7UM6",
                        "ICLR.cc/2024/Conference/Submission8230/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 6,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8230/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699415763327,
            "cdate": 1699415763327,
            "tmdate": 1700746419431,
            "mdate": 1700746419431,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "j69rsNPvem",
                "forum": "B4XM9nQ8Ns",
                "replyto": "iMQUg5Z45W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8230/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8230/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (Part 1/2)"
                    },
                    "comment": {
                        "value": "Thank you for your comments on how the manuscript can be made more compelling. We wish to suggest that there are a couple points of confusion here \u2013 we apologize for the lack of clarity and have done our best to address these points in the revised manuscript.\n\n### Weaknesses\n> 1. It is a typical A+B type of paper. Each part is well studied and author glue them together and demonstrate it in several simple examples. I don't think there is enough novelty here.\n\n**Response:** We believe the present contribution goes well beyond a \u201ctypical A+B paper\u201d, in the sense that the two methods claimed to be simply glued together (hypernetworks (Ha, Dai and Le, 2016; Pawlowski et al., 2018) and SINDy (Brunton, Proctor and Kutz, 2016; Kaptanoglu et al., 2022) are just two pieces of an overarching conceptual and technical framework being put forward. Moreover, these two methods refer to high-level frameworks/architectures with many possible implementations (e.g., a hypernetwork is simply a neural network that predicts the weights of another network \u2013 a concept we reimagine for predicting the coefficients of an analytical expression). Thus, the integration of the two methods (which have arisen in different academic communities for very different purposes) is far from straightforward.\n\nAt a conceptual level, integration of these methods is entirely contingent on our innovative problem formulation in terms of random differential equations. This alternative formulation for stochastic dynamics enabled us to leverage the unique advantages of numerous algorithms/techniques (including SINDy and hypernetworks, but also, e.g., variational autoencoders and backpropagation-compatible L0 loss) toward our overarching goal of stochastic dynamical modeling.\n\nAt a technical level, unifying all these approaches within a common framework is a highly non-trivial task, requiring strategies to appropriately balance multiple losses relating to equation sparsity, overall model complexity, reconstruction error, and proximity to a Gaussian prior (that is, promoting discovery of a model approximating a canonical SDE).\n\nAll in all, while it can be heuristically useful to summarize the proposed approach as combining hypernetworks and SINDy, this intuitive picture belies a much more sophisticated integration of techniques and ideas. We believe that this claim is consistent with evaluations from the other reviews, who each note the novelty/innovation of the proposed framework.\n\n> 2. All three examples are artificially made for this algorithm. All examples are corrected identified but I am not impressed unless authors are able to demonstrate some non-trivial RODE. The second example equation (11) is not even a valid example of stochastic Lotka-Volterra. I don't know what is N(0,1) on the Right hand side means here.\n\n**Response:** We believe that this impression stems in part from notation differences between fields. We have modeled systems subject to white noise (thus, a general setting for stochastic dynamics), though we adopt notation more familiar in statistics/machine learning. Thus, N(0,1) denotes a Gaussian distribution with mean=0, standard deviation=1; iteratively sampling from this distribution amounts to simulating a specific realization of a driving white noise process (which is defined by random variables distributed according to N(0,1) at each time point t (Han and Kloeden, 2017). As such, our selected problems are consistent with widely used stochastic modeling problems (namely, classic SDEs) in which the source of stochasticity is taken to be a Gaussian white noise. Nonetheless, we agree that this notation can obscure connections to standard SDE representations; as such, we have rewritten equation 11 in canonical SDE matrix notation involving a 2D Wiener process (which we simulate via the Euler-Maruyama integration scheme), thus clarifying that it is indeed a valid Lotka-Volterra SDE.\n\n> 3. Authors have limited knowledge on RODE here in fact not all SDE can be transformed to RODE and vice versa. And in general RODE case, z is not independent with x.\n\n**Response:** As we understand, any finite-dimensional, ordinary SDE can be transformed to an RDE; we agree that the reciprocal is more restricted to RDEs driven by stochastic processes that are the solutions of SDEs (Han and Kloeden, 2017). To our knowledge, this reciprocity is well-established since the results of (Imkeller and Schmalfuss, 2001), and has been usefully invoked in other numerical investigations into stochastic dynamics where an RDE formulation carries practical advantages (e.g., (Bauer et al., 2017)).\nWe agree that in the general case, the noise process z(t) need not be state-independent. However, the state-independent case (i.e., an autonomous noise process) represents the most common problem setting for stochastic dynamics, and thus carries broad relevance beyond the specific models studied herein."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700431742081,
                "cdate": 1700431742081,
                "tmdate": 1700431773987,
                "mdate": 1700431773987,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "urtC6sJa8G",
                "forum": "B4XM9nQ8Ns",
                "replyto": "j69rsNPvem",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8230/Reviewer_7UM6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8230/Reviewer_7UM6"
                ],
                "content": {
                    "title": {
                        "value": "Unclear how to turn stochastic Lotka-Volterra system into RODE"
                    },
                    "comment": {
                        "value": "Authors have changed the model for the second experiment. However, in appendix F, they only mentioned how to transform the additive noise version. But the stochastic Lotka-Volterra has multiplicative noise. Does the OU transform still work here? \nDo author claim their methods can work for arbitrary SDE?"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8230/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700439684885,
                "cdate": 1700439684885,
                "tmdate": 1700439684885,
                "mdate": 1700439684885,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]