[
    {
        "title": "Pareto Deep Long-Tailed Recognition: A Conflict-Averse Solution"
    },
    {
        "review": {
            "id": "RKlErwpEtX",
            "forum": "b66P1u0k15",
            "replyto": "b66P1u0k15",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission130/Reviewer_wNAW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission130/Reviewer_wNAW"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors mainly tackle the long-tailed learning from the optimization perspective. Specially, this paper first reveals the optimization conflicts among categories in long-tailed learning, and proposes to integrate multi-objective optimization (MOO) with long-tailed learning. Temporal design on MOO, variability collapse loss and sharpness-aware minimization are then employed. Experiments on four long-tailed benchmark datasets are conducted to validate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-\tLeveraging MOO method to enhance long-tailed learning seems reasonable.\n-\tThis paper reveals the phenomena of optimization conflicts among categories, which is an importance topic in the long-tailed learning from the optimization perspective. \n-\tThe proposed method does not introduce additional computational cost.\n-\tThe paper is generally easy to follow."
                },
                "weaknesses": {
                    "value": "-\tThe paper introduces some trivial factors, e.g. sharpness-aware minimization, which makes the contribution of this paper ad-hoc. In my humble opinion, sharpness-aware minimization does not show strong connection to the proposed MOO framework, and can be integrated to almost all the long-tailed learning baselines.\n-\tThe technical contribution of this paper seems not strong. This paper mainly adopts CAGRAD with slight modification on the training schedule (temporal design). Besides, sharpness-aware minimization is directly applied and the variability collapse loss is only a regularization term on the standard deviation of the loss.\n-\tThe effectiveness of the proposed PLOT on top of baseline methods shows significant variability, see Table 1-2. However, the authors only conduct experiments on top of cRT + Mixup/MiSLAS on ImageNet-LT, Places-LT, iNaturalist, thus casting doubts on the confidence of the results.\n-\tIn Table 1, MOO underperforms in nearly half the cases when compared with baseline methods (24/54). It cannot support the claims that MOO is beneficial for long-tailed learning. However, in Table 4, the authors present the most favorable performance enhancement with cRT + Mixup, thus making the results less confident.\n-\tAccording to Theorem 4.1, the generalization bound is bounded by the weighted intra-class loss variability, i.e., $w_KM$. It raises the question of whether uniformly constraining the intra-class loss variability is the optimal solution. Besides, in the neural collapse theory, minimizing loss function (e.g. cross-entropy loss) can lead to the intra-class collapse at the optimal case, why there should be an explicit constraint on intra-class variability?\n-\tThis paper lacks background and discussions on the MOO methods, including MGDA, EPO and CAGrad. The reason for choosing CAGrad is solely performance-driven, lacking a connection to its inherent compatibility with the long-tailed learning paradigm.\n-\tIn this paper, multi-classification task is regarded as multiple binary classification tasks. Does the proposed approach yield effective results when applied to imbalanced multi-label classification tasks? The empirical study on the multi-label tasks will benefit a lot to improve the generalizability of the proposed method.\n-\tThis paper lacks disjoint analysis on the classification performance wr.t. different groups (many/medium/few).\n-\tSome results on logit adjustment are missing in Table 2.\n-\tThere is no clue that the proposed method solves the gradient conflicts issue depicted in Figure 1.\n-\tIn figure 8, it's unclear whether the proposed method actually surpass the performance of the baseline method LDAM-DRW in terms of gradient similarities. The gradient similarities of the proposed method exhibit a similar trend in the second stage.\n-\tIt is suggested to provide some qualitative results (e.g. t-SNE) to validate the effectiveness of the proposed method on the embedding space."
                },
                "questions": {
                    "value": "Please refer to the weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission130/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission130/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission130/Reviewer_wNAW"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission130/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698650044274,
            "cdate": 1698650044274,
            "tmdate": 1700633854224,
            "mdate": 1700633854224,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0BvZOnqfPD",
                "forum": "b66P1u0k15",
                "replyto": "RKlErwpEtX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission130/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission130/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer wNAW [1/3]"
                    },
                    "comment": {
                        "value": "Thank Reviewer wNAW\u2019s valuable time and comments and sorry for late response. To address your concerns, we provide pointwise responses below.\n\n> * Q1: The paper introduces some trivial factors, e.g. sharpness-aware minimization, which makes the contribution of this paper ad-hoc. In my humble opinion, sharpness-aware minimization does not show strong connection to the proposed MOO framework, and can be integrated to almost all the long-tailed learning baselines.\n\nA1: We acknowledge that SAM has been a technique commonly used to augment existing approaches. However, its effectiveness in the context of DLTR is not as significant as anticipated. While a previous study [1] demonstrated the effectiveness of incorporating SAM into certain DLTR methods, it was limited to a few baseline models (CE, LDAM, VS). However, recent research [2] indicates that SAM does not consistently improve a wide range of DLTR approaches. This suggests that the integration of SAM and DLTR requires a specific design rather than a straightforward application.\nIn our paper, we employ SAM based on the observation that the convergence of CAGrad cannot be guaranteed in DLTR scenarios, particularly for tail classes that are known to exhibit sharp minima (larger H values). Although H-Lipschitz is typically assumed to hold, it fails to do so in DLTR scenarios and must be addressed. Therefore, we utilize SAM to address the challenges posed by integrating DLTR with MOO, rather than treating it as a mere technique or trick.\n\nReference:\n\n[1] Harsh Rangwani, Sumukh K Aithal, Mayank Mishra, et al. Escaping saddle points for effective generalization on class-imbalanced data. Advances in Neural Information Processing Systems, 35: 22791\u201322805, 2022.\n\n[2] Zhipeng Zhou, Lanqing Li, Peilin Zhao, Pheng-Ann Heng, and Wei Gong. Class-conditional sharpness-aware minimization for deep long-tailed recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3499\u20133509, June 2023.\n\n> * Q2: The technical contribution of this paper seems not strong. This paper mainly adopts CAGRAD with slight modification on the training schedule (temporal design). Besides, sharpness-aware minimization is directly applied and the variability collapse loss is only a regularization term on the standard deviation of the loss.\n\nA2:  We would like to emphasize that this paper makes significant contributions in three key aspects. Firstly, we provide a comprehensive observation that identifies an intrinsic conflict issue present in current DLTR approaches during the early stages of representation learning. This novel observation sheds light on an unexplored aspect of DLTR research.\nSecondly, we propose a framework, referred to as the temporal design, which enables the application of MOO to DLTR methods, effectively addressing the aforementioned conflict issue. This framework serves as a bridge between the distinct regimes of MOO-based MTL and DLTR. Furthermore, it holds potential for further enhancement through the incorporation of advanced MOO approaches. Lastly, we introduce two novel theoretical perspectives, i.e., SAM and variability collapse loss, to tackle the convergence and generalization challenges that arise when integrating DLTR with MOO. While these designs are not entirely novel, their purpose and re-discovery within the context of MOO-based DLTR scenarios are indeed novel and well motivated, as highlighted in the original main text.\n\n> * Q3: The effectiveness of the proposed PLOT on top of baseline methods shows significant variability, see Table 1-2. However, the authors only conduct experiments on top of cRT + Mixup/MiSLAS on ImageNet-LT, Places-LT, iNaturalist, thus casting doubts on the confidence of the results.\n\nA3: We acknowledge that the improvements in Table 1-2 show variability, which is in turns aligns with our conclusion, i.e., PLOT bring more improvements to those that have more serious conflict issue since a key contribution of PLOT is just to address the conflict issue for representation learning of DLTR. Besides, we have to clarify that most of the DLTR methods haven\u2019t released their official implementations for large-scale datasets, making it hard for us to apply PLOT on them. But we would like to provide you some additional empirical evidence to demonstrate the effectiveness of PLOT on large-scale datasets in the next few days."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission130/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700056154156,
                "cdate": 1700056154156,
                "tmdate": 1700056154156,
                "mdate": 1700056154156,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XzHs6XkR5s",
                "forum": "b66P1u0k15",
                "replyto": "JZPQSFltYY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission130/Reviewer_wNAW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission130/Reviewer_wNAW"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. I have read the response and other reviews. Below are my follow-up comments.\n\nMy concern is the technical contribution of this paper. After reading the rebuttal, I appreciate that the application of SAM and the variability collapse loss in long-tailed learning is reasonable. However, the introduction of both SAM and variability collapse loss, as they are already established methods, is directly utilized in this study without significant modification.\n\n> The observed similarity in the second stage can be attributed to the strong re-weighting strategy employed by LDAM-DRW, known as DRW. This strategy effectively enhances the dominance of the tail classes, leading to the observed trend.\n\nDoes this mean that the proposed method achieves similar effect as the re-weighting strategy? If re-weighting can effectively address gradient conflicts in long-tailed situations, what then is the necessity of the proposed MOO framework?\n\n> T-SNE: As anticipated, the tail classes of the cRT + Mixup + PLOT approach exhibit increased separability compared to the vanilla approach. \n\nIn Figure 15, it's unclear where the tail class is located, and whether the tail classes show improved separability compared to the baseline method."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission130/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700625060013,
                "cdate": 1700625060013,
                "tmdate": 1700625060013,
                "mdate": 1700625060013,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qc8jNJxgMf",
                "forum": "b66P1u0k15",
                "replyto": "KkWle1Dw0H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission130/Reviewer_wNAW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission130/Reviewer_wNAW"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the response. Most of my concerns have been addressed.  However, it is suggested to incorporate more comprehensive discussions on SAM-based long-tailed learning and neural collapse literature for better clarity of the proposed method. For example, \"observations of the sharpness present in tail classes\" have already been discovered. Additionally, exploring further discussions or potential extensions in multi-label long-tailed learning appears to be a promising direction.\n\nOverall, I appreciate the detailed empirical evidence (e.g., the gradient conflict) and the perspective of multi-objective optimization, which seems to be critical for long-tailed learning. I would like to recommend acceptance and raise my score to '6'."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission130/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633818842,
                "cdate": 1700633818842,
                "tmdate": 1700633818842,
                "mdate": 1700633818842,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5NhvzCm09T",
            "forum": "b66P1u0k15",
            "replyto": "b66P1u0k15",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission130/Reviewer_KQKE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission130/Reviewer_KQKE"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers an interesting and important issue in long-tailed learning, that is the optimization conflicts. To solve this problem, the authors introduce Pareto optimization for long-tailed learning. First, the authors observe that existing widely-used fixed re-balancing strategies can uniformly lead to gradient conflict. Then, they introduce Multi-Objective Optimization (MOO) to enhance existing long-tailed learning methods. Moreover, the authors find that directly integrating MOO-based methods can lead to performance degradation. To solve this, they propose to decouple the MOO-based methods from the temporal rather than structural perspective to enhance the integration. Experimental results demonstrate that the proposed temporal MOO method can boost the baseline method and achieve an improvement by a large gap."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper studies an interesting problem, which is the optimization conflict in long-tailed learning.\n2. The authors propose to utilize MOO to solve the conflict issue, which is reasonable and makes sense.\n3. This paper is clearly written and easy to understand.\n4. The authors conduct multiple empirical studies to demonstrate the effectiveness since the results show that the proposed method can boost the performance of the baseline methods obviously.\n5. The source code is released for reproduction."
                },
                "weaknesses": {
                    "value": "1. Figure 1 may cause misunderstandings. How is Figure 1 computed? Is it cosine similarities between the mean of the gradient of different classes? Since the diagonal is red (1.0), it seems that the values represent gradient similarities. And what is the connection between \"gradient conflicts\" and \"gradient similarities\"?\n2. You mainly consider the directions of gradients. Have you considered the impacts of the L2 norms of the gradients?\n3. As you have mentioned MOO is applied during the early stages of representation learning, how to select the applied stage?\n4. The iNaturalist dataset has multiple versions. You should highlight it with \"iNaturalist 2018\"."
                },
                "questions": {
                    "value": "(This is not a question, but a suggestion). I found a very similar work that also studies long-tailed learning with multi-objective optimization [1]. Maybe you can publicly release your work considering its timeliness.\n\n[1] Long-Tailed Learning as Multi-Objective Optimization, in https://arxiv.org/abs/2310.20490"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission130/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699027535574,
            "cdate": 1699027535574,
            "tmdate": 1699635938423,
            "mdate": 1699635938423,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zsE065BZkw",
                "forum": "b66P1u0k15",
                "replyto": "5NhvzCm09T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission130/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission130/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer KQKE"
                    },
                    "comment": {
                        "value": "We are glad that Reviewer KQKE recognize our paper to be interesting and easy to follow. To address Reviewer KQKE\u2019s concerns, we provide pointwise responses below.\n\n> * Q1: Figure 1 may cause misunderstandings. How is Figure 1 computed? Is it cosine similarities between the mean of the gradient of different classes? Since the diagonal is red (1.0), it seems that the values represent gradient similarities. And what is the connection between \"gradient conflicts\" and \"gradient similarities\"?\n\nA1: Sorry for unclear description. The Figure 1 in the main text represents the cosine similarities among different classes, with the values indicating the degree of gradient similarity. As defined in **Definition 2.1**, when the cosine similarity between two gradients is less than 0, we classify them as conflicting, which is consistent with the definition in multi-task learning.\n\n> * Q2: You mainly consider the directions of gradients. Have you considered the impacts of the L2 norms of the gradients?\n\nA2: We acknowledge your concern regarding the consideration of both direction and norm in the context of our study. As depicted in **Figure 3(b)** in the main text, an increasing gradient imbalance exacerbates the issue of dominated conflicting, while non-conflicting scenarios also tend to bias the mean gradient towards the dominated one. To assess this imbalance issue, we calculate the cosine similarity between the mean gradient and the gradients of different classes, as presented in **Figure 2** in the main text. This analysis provides evidence that mainstream deep long-tailed approaches still exhibit varying degrees of imbalance issues from the perspective of gradient norm. Furthermore, we provide statistical evidence of dominated conflicts in Figure 5 in the main text to further substantiate our findings.\n\n> * Q3: As you have mentioned MOO is applied during the early stages of representation learning, how to select the applied stage?\n\nA3: The selection of the applied stage is determined by a hyper-parameter *E*; however, it is important to note that the selection is limited to a fixed set. For instance, if the long-tailed model is intended to be trained over a total of 200 epochs, we choose the applied stage *E* from the set {50, 80, 120, 140}. As part of our future directions, we aim to develop a strategy that can dynamically determine the applied stage, allowing for greater adaptability and optimization in the training process.\n\n> * Q4: The iNaturalist dataset has multiple versions. You should highlight it with \"iNaturalist 2018\".\n\nA4: We thank for your kind reminder, we will fix it in our updated version.\n\n> * Suggestion: I found a very similar work that also studies long-tailed learning with multi-objective optimization [1]. Maybe you can publicly release your work considering its timeliness.\n\nA5: We sincerely appreciate your reminder. Upon further investigation, we discovered that this paper was recently published on arXiv on October 31st. It is noteworthy that both our work and this paper share a common objective of integrating MOO and DLTR. However, it is important to highlight that our approaches follow distinct methodologies and regimes. Nonetheless, we genuinely value and appreciate your suggestion."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission130/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699694737438,
                "cdate": 1699694737438,
                "tmdate": 1699694737438,
                "mdate": 1699694737438,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "G5gptq2cLH",
                "forum": "b66P1u0k15",
                "replyto": "zsE065BZkw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission130/Reviewer_KQKE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission130/Reviewer_KQKE"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your responses.\n\n- I suggest you update Figure 1 to avoid misunderstanding. For example, you can update the caption with more explanations.\n\n- The selection of the applied stage might be a weakness. More general approaches are needed for future work.\n\n- It seems that Figure 2 is irrelevant to the norms of gradients. Figure 2 only calculates the cosine similarity. Anyway, I believe that gradient conflicts exist in long-tailed learning w.r.t cosine similarity. However, a low cosine similarity can not necessarily indicate that the class is not optimized (maybe a negative cosine similarity can). The class might still be optimized well, with the gradient norm becoming small, even if the cosine similarity maintains a high level. So we need to take a look at the gradient norms of different classes during the whole training period."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission130/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699768949046,
                "cdate": 1699768949046,
                "tmdate": 1699768949046,
                "mdate": 1699768949046,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VbB93gW8KT",
                "forum": "b66P1u0k15",
                "replyto": "N3ngAa7VJL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission130/Reviewer_KQKE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission130/Reviewer_KQKE"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for providing further responses.\n\nI think the motivation of the paper is a crucial highlight, so I suggest you enhance your insights regarding the gradient norm, and your response makes sense to me. \n\nI would like to recommend the acceptance. However, I also agree with the other reviewers' concerns, such as integrating PLOT with more state-of-the-art methods. I suggest the authors address these concerns."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission130/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699976257883,
                "cdate": 1699976257883,
                "tmdate": 1699976257883,
                "mdate": 1699976257883,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2HMUqOcBZs",
            "forum": "b66P1u0k15",
            "replyto": "b66P1u0k15",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission130/Reviewer_yXjC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission130/Reviewer_yXjC"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the problem of Deep Long-Tailed Recognition (DTLR) and highlights the importance of dynamic re-balancing to address optimization conflicts in this domain. The authors empirically demonstrate that existing DTLR methods are dominated by certain categories due to fixed re-balancing strategies, preventing them from effectively handling gradient conflicts. To address this, they introduce an approach based on multi-objective optimization (MOO) to decouple the problem from a temporal perspective, avoiding class-specific feature degradation. Their method, named PLOT (Pareto deep long-tailed recognition), was conducted on several benchmarks for evaluation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The long-tailed recognition problem studied in this paper is a fundamental task that deserves further study.\n- The paper is well organized and easy to follow.\n- The technique contributions of this paper are novel and reasonable, which are tailored for the challenges of long-tailed recognition."
                },
                "weaknesses": {
                    "value": "- My main concern with this paper is that the authors primarily demonstrate the effectiveness of their approach by augmenting existing methods with PLOT. However, they lack comprehensive performance comparisons by integrating PLOT with state-of-the-art methods like BBN, SADE, PaCo, etc. This omission makes it challenging to assess the method's effectiveness in comparison to the latest advancements in the field, raising questions about its overall impact and general applicability.\n- Another potential weakness of this paper is the insufficient coverage of long-tailed recognition methods in the references. The paper may not thoroughly discuss some classical methods in the field, which can be essential for providing a comprehensive understanding of the long-tailed recognition landscape.\n- The conclusion of the article should be written in the past tense. Additionally, the conclusions require to be added future work of this paper.\n- The authors are encouraged to carefully proofread the paper."
                },
                "questions": {
                    "value": "Please see the paper weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission130/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699081295077,
            "cdate": 1699081295077,
            "tmdate": 1699635938351,
            "mdate": 1699635938351,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CfCY0pETcy",
                "forum": "b66P1u0k15",
                "replyto": "2HMUqOcBZs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission130/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission130/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer yXjC"
                    },
                    "comment": {
                        "value": "Thank Reviewer yXjC\u2019s valuable time and comments and sorry for late response. To address your questions, we provide pointwise responses below.\n> * Q1: My main concern with this paper is that the authors primarily demonstrate the effectiveness of their approach by augmenting existing methods with PLOT. However, they lack comprehensive performance comparisons by integrating PLOT with state-of-the-art methods like BBN, SADE, PaCo, etc. This omission makes it challenging to assess the method's effectiveness in comparison to the latest advancements in the field, raising questions about its overall impact and general applicability.\n\nA1: We appreciate your valuable advice. Following your suggestion, we have incorporated an advanced long-tailed method, i.e., BBN, to further showcase the effectiveness of PLOT on CIFAR10-/100-LT datasets. And the results are presented as below.\n\nCIFAR10-LT:\n| imb. ratio | 200        | 100   | 50 |\n| ---------- | ---------- | ------- | ----- | \n| BBN        | 73.52      | 77.43 | 80.19 | \n| BBN+PLOT   | 74.34 | 78.49 | 82.44 |\n\nCIFAR100-LT:\n| imb. ratio |  200         | 100   | 50|\n| ---------- | ------------- | ----- | ------- |\n| BBN        | 36.14       | 39.77 | 45.64 |\n| BBN+PLOT | 36.21       |40.29 | 46.13 |\n\nAs observed, PLOT still benefits BBN under various imbalance ratio scenarios. \nIt should be noted that all these implementations are based on their official implementations. For fair comparison, we only show the results of PLOT augmentation on those datasets that the official code has implemented on, and both the vanilla and PLOT augmented version are run on the same three random seeds. \nFrom above results, we believe the generalization of PLOT is well demonstrated.\n\n> * Q2:  Another potential weakness of this paper is the insufficient coverage of long-tailed recognition methods in the references. The paper may not thoroughly discuss some classical methods in the field, which can be essential for providing a comprehensive understanding of the long-tailed recognition landscape.\n\nA2: As suggested, we have additional introduced more related works involving re-balancing- and ensemble experts-based works. Please refer to our updated version.\n\n> * Q3: The conclusion of the article should be written in the past tense. Additionally, the conclusions require to be added future work of this paper.\n\nA3: Thank you for your reminder, we have fixed it.\n\n> * Q4: The authors are encouraged to carefully proofread the paper.\n\nA4: Thank you for your reminder. We are diligently conducting a thorough proofreading of the entire paper. If you have any further suggestions or advice regarding the writing, we kindly request you to bring them to our attention."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission130/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700055687437,
                "cdate": 1700055687437,
                "tmdate": 1700055687437,
                "mdate": 1700055687437,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "m0NCIvLz0c",
                "forum": "b66P1u0k15",
                "replyto": "2HMUqOcBZs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission130/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission130/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer yXjC,\n\nWe sincerely thank you again for your great efforts in reviewing this paper. We have gone through your points one-by-one and tried to address them carefully. Please don\u2019t hesitate to let us know if you have any further questions. \n\nBest regards,\n\nAuthors"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission130/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700534427576,
                "cdate": 1700534427576,
                "tmdate": 1700534427576,
                "mdate": 1700534427576,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QITaKatROg",
                "forum": "b66P1u0k15",
                "replyto": "2HMUqOcBZs",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission130/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission130/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A friendly reminder for discussion"
                    },
                    "comment": {
                        "value": "Dear Reviewer yXjC,\n\nThe rebuttal phase ends today and we have not yet received feedback from you. We believe that we have addressed all of your previous concerns. We would really appreciate that if you could check our response and updated paper.\n\nLooking forward to hearing back from you.\n\nBest Regards,\n\nThe Authors of Paper 130"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission130/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700702610927,
                "cdate": 1700702610927,
                "tmdate": 1700702610927,
                "mdate": 1700702610927,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]