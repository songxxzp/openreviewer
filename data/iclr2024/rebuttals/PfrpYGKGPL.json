[
    {
        "title": "The Entity-Deduction Arena: A playground for probing the conversational reasoning and planning capabilities of LLMs"
    },
    {
        "review": {
            "id": "Vgzl2tWNrl",
            "forum": "PfrpYGKGPL",
            "replyto": "PfrpYGKGPL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3791/Reviewer_TYnB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3791/Reviewer_TYnB"
            ],
            "content": {
                "summary": {
                    "value": "The paper addresses the unreliability of Large Language Models (LLMs) when responding to ambiguous queries and emphasizes the need for intelligent agents that can ask clarifying questions. Using an entity-deducing game to evaluate LLMs' conversational skills, the authors find significant performance variations, with GPT-4 outperforming humans. The study also explores Behavior Cloning for model emulation and uses Reinforcement Learning to improve Vicuna models, aiming to better equip autonomous agents for ambiguous situations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper underscores the need for intelligent conversational agents to proactively ask questions in uncertain situations, aiding effective problem-solving across various applications.\n2. It highlights the shift from modular task management to employing Large Language Models (LLMs) for end-to-end autonomous agent development, enhancing complex task completion.\n3. The paper tackles the challenge of accurately capturing nuanced user intents, suggesting the use of entity-deducing games to evaluate and improve LLMs' conversational skills.\n4. A systematic evaluation of different LLMs is conducted, exploring enhancements using high-performing models and introducing methods to improve LLMs using game-based learning."
                },
                "weaknesses": {
                    "value": "1. The major concern is that entity-deducing game is one small problem in terms of conversational reasoning and planning capabilities of language models. I am not convinced that a single game is a full reflection of the LLM reasoning ability\n2. The experimental scale in the paper is limited. The conclusion in the paper should be drawn from at least thousands of samples."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3791/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3791/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3791/Reviewer_TYnB"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3791/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698610333049,
            "cdate": 1698610333049,
            "tmdate": 1700596262775,
            "mdate": 1700596262775,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fqakoQSMJt",
                "forum": "PfrpYGKGPL",
                "replyto": "Vgzl2tWNrl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3791/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3791/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "We are grateful to the reviewer for their valuable feedback and the insights provided. Below, we have addressed the questions and concerns raised:\n\n**Entity-deducing game is one small problem**: We completely agree that the benchmark we used does not fully reflect the reasoning ability of the LLM. Nevertheless we thinkg this problems adds to the well needed suite of test cases for measuring these characteristics of LLMs. We will revise the paper to make this point clearer. Please refer to our general response.\n\n**Experimental Scale**: We thank the reviewer for the suggestion. Please refer to the general questions for more discussion."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3791/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700255916114,
                "cdate": 1700255916114,
                "tmdate": 1700255916114,
                "mdate": 1700255916114,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2PiKpqigVj",
                "forum": "PfrpYGKGPL",
                "replyto": "fqakoQSMJt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3791/Reviewer_TYnB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3791/Reviewer_TYnB"
                ],
                "content": {
                    "title": {
                        "value": "Raise the score to boardline accept"
                    },
                    "comment": {
                        "value": "After reading the author's response, I now raise score from 5 to 6"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3791/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700596379760,
                "cdate": 1700596379760,
                "tmdate": 1700596379760,
                "mdate": 1700596379760,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Hv7SgtjCwE",
            "forum": "PfrpYGKGPL",
            "replyto": "PfrpYGKGPL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3791/Reviewer_HCV6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3791/Reviewer_HCV6"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to evaluate the ability for LLMs to generate useful clarification questions in dialogue via an entity deduction game (20 Questions). The paper evaluates several popular LLMs as well as human performance, performs some analysis and experiments (e.g., on the game strategies, performance in planning and reasoning), and also experiments with distilling policies from strong LLMs to smaller open-source models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The motivation is compelling and the task is clear. 20 Questions has been studied in prior work in the NLP community, but this seems to be the first large-scale study evaluating LLMs on the task. The experiments are also relatively clearly defined. I found research questions well-motivated and generally set up well, and they brought a lot of interesting experiments to mind that would be cool to explore more in more detail: e.g., could you somehow probe the top-N guesses directly from the model activations by training some additional probe on them? how are improvements on open-source models attributed to reasoning vs. planning (e.g. comparing the fine-tuned open source models to GPT-4 like the experiment in RQ2)?"
                },
                "weaknesses": {
                    "value": "* Some of the discussion is over-anthropomorphizing models. For example, in the introduction \"intelligent behavior has been achieved\" through what are common components of an enterprise dialogue system... it would be more precise and accurate to say that this is just traditionally how the problem of building dialogue systems, including ones that ask clarification questions, has been broken down. Some of the conclusions on why performance is strong is unsubstantiated, e.g., that GPT-4's performance is due to its zero-shot generalization ability. Wording like \"realize\" (in RQ1) also is attributing cognition to these models that I personally don't think is warranted.\n* There needs to be precise, quantifiable evaluation of the judge's performance. Without this evaluation it's impossible for a reader to understand how much of an influence its errors has on the game.\n* The evaluation set is really small! It's hard to conclude general performance on asking clarification questions when the set of target entities is so small. Ideally, we would be able to analyze performance across a number of entity features: how rare it is, how prototypical it is, how precise of an entity it is (e.g., specific breed of dog vs. dog in general), whether it's abstract or concrete, etc. With just 30 entities, it's hard to make any conclusions along these axes of difficulty. Figure 2 starts to get at this, and I think there are hints at interesting findings here (even just looking at human performance, to be honest) -- for example, umbrellas and cookie cutters are non-prototypical instances of their hypernyms in wordnet (respectively \"canopy\" and \"kitchen utensil\"). But with only 30 evaluation examples I don't think any strong conclusions could be made.\n* Details on recruiting and managing crowdsourcing for the human baseline needs to be included in the main paper. There are also no details I could find in the appendix, e.g., on pay or the crowdsourcing platform used. There appears to be a lot of noise in the human dialogues (e.g., questions in the bottom game of table 6 appear to be obviously non-optimal) and I am wondering how workers were incentivized to actually try at the game. I also think analysis comparing the human strategies and LLM strategies should be in the main paper.\n* I would have liked to see evaluation of human players on the reasoning task given a dialogue history generated by a model (and vice versa); essentially the experiment in Table 4 but replacing Vicuna 7B with a human. Since humans appear to perform worse, is this because they are worse at planning or reasoning (as defined in this paper)?\n* I think the finding that these models have consistent strategies in question asking is cool, but it's not very surprising, because these are (mostly) deterministic models after all."
                },
                "questions": {
                    "value": "* How were the entities chosen? The paper mentions they are manually curated, but from where?\n* Why are the responses different for the celebrity dataset? (\"dunno\" vs. \"maybe\")\n* What does \"retrospective perspective\" mean in Table 3 caption?\n* How do you measure uncertainty in the model's top-N predictions as mentioned in the discussion of RQ1?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Responsible research practice (e.g., human subjects, data release)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I could not find any details about crowdsourcing management, e.g. pay / incentives, recruiting, etc."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3791/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698799205172,
            "cdate": 1698799205172,
            "tmdate": 1699636336106,
            "mdate": 1699636336106,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Oga20cJemo",
                "forum": "PfrpYGKGPL",
                "replyto": "Hv7SgtjCwE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3791/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3791/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the detailed and insightful feedback. In response, we have addressed the comments as follows:\n\n**Over-anthropomorphizing**: We appreciate and agree with the reviewer's viewpoint and will tweak the language carefully to avoid the accidental over-anthropomorphization in our writing style. Note that we were not suggesting that \"intelligent behavior\" is human-like or human-level but we can see in retrospect how our presentation might be construed that way -- a careful review and rephrasing of parts of the paper should alleviate this. \n\n**Evaluating the Judge**: We acknowledge the need for better quantification of the errors made by Judge. Since obtaining the ground truth is challenging, we decided to conduct a human evaluation of Judge (GPT3.5-turbo-0613) responses. This evaluation involved non-paid volunteers from our institute who assessed the correctness of Judge's responses to questions asked given the oracle entity or celebrity name. Due to time constraints during the rebuttal, based on our current annotation progress, we were only able to evaluate 223 Judge responses on Things and 237 Judge responses on Celebrities. The human evaluators identified 7 out of 223 (3.13%) and 7 out of 237 (2.95%) responses as problematic or incorrect. A more comprehensive summary of these results will be included in our revision. \n\n**The evaluation set is small**: We agree that having a larger evaluation dataset would enable more comprehensive and detailed analysis at a finer granularity. To this end, we are currently working on expanding the evaluation by incorporating 300 examples for each dataset, across all the models being compared. We will incorporate the results of this evaluation in our general response.\n\n**Human evaluation details**: We have discussed some details of creating the human baseline in the Appendix D. We agreed that some contents needed to be moved to the main text, while some information needed to be supplemented. In this study, we did not hire any paid crowd-source workers. As discussed in Section 3, because the complexity of the multi-turn game setup, we developed an internal demo server (shown in figure 4) and relied solely on enthusiastic internal volunteers who were interested in our study. Following a 30-day trial period, 108 volunteers contributed to this study, and we successfully collected a total of 216 human game play sessions for Things and Celebrities.\n\nTo ensure the quality of the data, we manually inspected human game plays to filter out low quality data. The authors reviewed all human game play sessions, and removed sessions that do not complete the entity deduction task, contain irrelavant chit-chat, or contain nonsensical repeated questions, leaving only sensible game plays. In the end 201 sessions were selected as valid game play.\n\nWe appreciate the suggestion and will incorporate a thorough discussion comparing the strategies employed by humans and LLMs into the main text.\n\n**Evaluation of Human players on reasoning tasks**: We are currently conducting some additional experiments and will provide supplementary results shortly.\n\n**Deterministic models**: In our study, we compute statistics for each game by performing 5 repetitions for each game. We apply a sampling approach and use a temperature of 0.8 for the guesser models throughout our study. We observed that the gameplay can vary significantly, resulting in completely different outcomes. For example, the failure results presented in Table 2, 8, 12 are derived from the same model, entity, and settings."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3791/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700268350965,
                "cdate": 1700268350965,
                "tmdate": 1700268350965,
                "mdate": 1700268350965,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7ml8URdXmO",
                "forum": "PfrpYGKGPL",
                "replyto": "uaOlT7qeQb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3791/Reviewer_HCV6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3791/Reviewer_HCV6"
                ],
                "content": {
                    "comment": {
                        "value": "> We manually created several categories and then asked GPT-3.5 to generate additional categories. Next, we asked it to generate unique entities for these categories without repetition. \n\nHm, I worry about this decision -- in particular because if GPT-3.5 is already giving high prior probability to the items it's guessing, it would probably be better at guessing them than other plausible items within the categories that it may be less familiar with. Did you experiment with evaluating on items from these categories that are more adversarially chosen?"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3791/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700616503463,
                "cdate": 1700616503463,
                "tmdate": 1700616503463,
                "mdate": 1700616503463,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "K7o8SFBQZK",
                "forum": "PfrpYGKGPL",
                "replyto": "Hv7SgtjCwE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3791/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3791/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response (Cont'd)"
                    },
                    "comment": {
                        "value": "**Evaluation of Human players on reasoning tasks**:\n \nWe thank the reviewer for the valuable suggestion. While we are still awaiting the full results, we would like to share our current findings below:\n\n\n| Approach | Things | Celeb |\n|--------------------------|---------------|------------|\n| GPT-4 | 0.40\u00b10.05 | 0.48\u00b10.03 |\n| Vicuna 7B | 0.10\u00b10.04 | 0.04\u00b10.03 |\n| Human | 0.20\u00b10.04 | 0.25\u00b10.03 |\n| Vicuna 7B \u2192 Human | 0.1 | |\n| GPT-4 \u2192 Human | 0.14 | |\n| Human \u2192 Vicuna 7B | 0.11\u00b10.04 | 0.03\u00b10.01 |\n| Human \u2192 GPT-4 | 0.15\u00b10.01 | 0.18\u00b10.06 |\n\nNote that these results are based on the evaluation set in the submission for consistency. In our next update, we plan to extend the study to include a new dataset with 300 items/names. Following the setup outlined in the paper, only the last turn for each dialog trajectory is replaced. We are currently relying on non-paid volunteers for the annotation. \n\nThe findings indicate that when GPT-4 is employed for the last step reasoning in human trajectories, there is a decrease in performance. One possible explanation for this is that while GPT-4 may excel in comprehending model-generated trajectories, it may struggle to fully interpret the planning trajectory of humans based on the observed turns. This suggests that models may be more compatible with model-generated trajectories rather than human trajectories. For instance:\n\n| Turn | Human Question            | Judge Answer | GPT-4 Question   |\n|------|---------------------------|--------------|------------------|\n| 1    | Is it a VR Headset        | No.          |                  |\n| 2    | Is it made of metal       | Yes.         |                  |\n| 3    | Is it used at home?       | Yes.         |                  |\n| 4    | Is it a chair?            | No.          |                  |\n| 5    | Is it a table?            | No.          |                  |\n| 6    | Is it a faucet            | No.          |                  |\n| 7    | Is it a fridge?           | No.          |                  |\n| 8    | Is it a bed frame?        | No.          |                  |\n| 9    | Is it a window frame?     | No.          |                  |\n| 10   | Is it a stationary        | Yes.         |                  |\n| 11   | Is it a pen               | No.          |                  |\n| 12   | Is it a pencil box        | No.          |                  |\n| 13   | Is it a scisor            | No.          |                  |\n| 14   | Is it a letter opener?    | No.          |                  |\n| 15   | Does it write?            | Yes.         |                  |\n| 16   | Is it a metal pen         | No.          |                  |\n| 17   | Is it a mechanical pencil | No.          |                  |\n| 18   | Is it a printer           | Bingo!       | Is it a Stapler? |\n\nEven though it was previously established that the entity can write, GPT-4 still asked if the entity is a Stapler or not."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3791/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700725065298,
                "cdate": 1700725065298,
                "tmdate": 1700727751935,
                "mdate": 1700727751935,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "6Uf6g4arZm",
            "forum": "PfrpYGKGPL",
            "replyto": "PfrpYGKGPL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3791/Reviewer_DwoQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3791/Reviewer_DwoQ"
            ],
            "content": {
                "summary": {
                    "value": "This paper, based on the prototype of the entity deduction game, carries out a series of experimental designs. The author presents the purpose of the experiment, the process, defines the criteria for success or failure, and introduces a scoring formula for evaluation. In datasets related to entities and things, a comparison experiment of the entity deduction game was conducted on several LLMs. The results showed that GPT-4 possesses a superior ability to narrow down compared to other LLMs. Furthermore, the paper explores the possibility of enabling other weaker models to acquire this capability through reinforcement learning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The perspective of the paper is quite interesting. It identifies specific characteristics of GPT-4 in the entity deduction game task, such as superior LLM knows about backtracking or employing a good strategy to divide the potential solution space instead of simply performing enumeration."
                },
                "weaknesses": {
                    "value": "In the presentation, I believe the author should use a paragraph to summarize the unique contributions of the paper.\n1. The paper lacks novelty and robustness to pass the bar of ICLR. The experiments designed don't show much improvement compared with referenced prototype. The datasets leveraged in the experiments, both in terms of range (two categories entities and things) and quantity, have certain limitations. However, the paper lacks discussion on this aspect. Moreover, given the inherent randomness in GPT's responses, it raises the question of whether multiple repeated experiments were conducted to ensure the stability of the results.\n2. The formula introduced in this paper lacks a thorough explanation and rigor. Why pick linearity? Any insight to pick 0.02 as coefficient?\n3. The observation and conclusion from experiments seem rather preliminary. The \"entity deduction game\" and the so-called \"path planning\" requires a more strict formulation for deeper analysis. This is especially true for the criterion of what can be a good path.\n4. The ability of path planning can easily be influenced by domain knowledge. I suggest that if we want to rigorously examine planning capability, perhaps we could consider a much narrower and common sense domain, for example, having GPT guess numbers within a certain range.\nPS: I  did try this number-guessing experiment on GPT-4. GPT-4 doesn't necessarily choose the binary search as the optimal path; it often opts for a narrower range enumeration."
                },
                "questions": {
                    "value": "As stated above"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3791/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698803715763,
            "cdate": 1698803715763,
            "tmdate": 1699636336010,
            "mdate": 1699636336010,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "K06oemoffP",
                "forum": "PfrpYGKGPL",
                "replyto": "6Uf6g4arZm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3791/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3791/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "We thank the reviewer's comments and feedback on our work. Next we address the questions and concerns:\n\n**Not enough improvement over referenced LLM**: Perhaps we misunderstood the reviewer, but we feel that our methods show strong improvement over our baselines in Table 1 -- both BC trained models (V-FT* models in Table 1 -- e.g. V-FT 7B) and RL trained models (V-RLGP* models in Table 1 -- e.g. V-RLGP 7B) show more than 100% improvement over the base model (Vicuna 7B). Perhaps the reviewer is suggesting that we compare our trained models against GPT-4 ? In that case it would be unfair since the GPT-4 model is much larger than the models we trained.\n\n**Not enough diversity in the datasets and no discussion**: In section 3.1 and Appendix A, we discussed that we aim to create rich and diverse datasets. In fact, our datasets are diverse. The **Things** dataset encompasses a wide range of categories, such as objects, animals, foods, plants, vehicles, clothing, professions, materials, instruments, places, birds, sports, buildings, furniture, celestial bodies, mythical creatures, games, body parts, beverages, weather phenomena, groups, gemstones, toys, tools, patterns, appliances, and microorganisms, among others.\n\nThe **celebrities** dataset meanwhile includes names from different nationalities, eras of life, and various occupations, including entertainment, sports, politics, entrepreneurship, science, philanthropy, writing, music, military, engineering, and more. We used 980 entities for the Things dataset and 133 entities for celebrities. Additionally, we are in the process of collecting and evaluating 300 additional examples for each dataset to further expand its size. The results of this effort will be shared in our general response.\n\n**No repeated experiment**: We apologize if we didn't emphasize this enough in the paper, but we did in fact repeat our experiments. In Table 1, our main results were obtained by conducting 5 repetitions for each gameplay, which provided the standard deviation as shown in the paper. This was further discussed in Section 4.\n\n**Why linearity in reward design**: It is commonplace in the RL community to use rewards based on heuristics that assign numeric values commensurate with desriable properties, and then combine these scores linearly. For instance, similar heuristics have been used in controlling the agent's movements [4] and in task-oriented chatbots [5]. In this paper, the score we proposed combines a reward for a good game outcome with a penalty for taking longer to achieve the right answer and 0.02 is just a hyperparameter that balances these two properties. With a value of 0.02 the model is still rewarded 0.7 for answering a question correctly after the full 20 steps. Values greater than 0.02 would reduce the rewards for games that take longer, while smaller values would penalize the model less for taking a longer time. In our early exploratory experiments we found 0.02 to lead to models with higher final success rate, compared to other values. Admittedly, reward selection is an art in RL, rather than a well understood science and it is possible that there are better choices out there, both in the heuristics chosen, and how their values are computed and combined -- we would be happy to add such qualifications to the paper.\n\n**Lack path planning analysis**: In fact, we provide a comprehensive qualitative analysis of what constitutes a good or bad path for the game. In Table 2 and Tables 8, 9, and 12, we have highlighted how the models are different in their planning ability and illustrated and explained the characteristics of good \"path planning\" and identify the common failure modes that lead to inadequate planning. The criteria for a good path indeed depends on the domain and may be hard to generalize at this stage of our understanding of LLMs. However, we discussed some general observations. In Table 3 and 13, we present the criteria for successful planning, which include 1) prioritizing high-level questions before addressing specific details and enumerations, 2)being aware of the current state and asking questions to effectively bi-partition the search space, and 3) being able to recognize when the current path is incorrect and occasionally backtracking to consider previously overlooked options. All of these aspects are discussed in Section 4 of our paper, and we will make them clearer.\n\n[4] Jens, et al. *\"Reinforcement learning in robotics: A survey.\"* The International Journal of Robotics Research 32.11 (2013)\n\n[5] Bhuwan, et al. *\"Towards end-to-end reinforcement learning of dialogue agents for information access.\"* ACL (2016)."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3791/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700255356098,
                "cdate": 1700255356098,
                "tmdate": 1700255843624,
                "mdate": 1700255843624,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LzMtPhcDzl",
            "forum": "PfrpYGKGPL",
            "replyto": "PfrpYGKGPL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3791/Reviewer_oYKG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3791/Reviewer_oYKG"
            ],
            "content": {
                "summary": {
                    "value": "This research assesses the reasoning performance of Large Language Models (LLMs) using the Entity-Deduction Arena (EDA) task, in which LLMs ask a judge questions to infer an entity. According to the study, GPT-4 performs better than humans when it comes to strategic questioning. It emphasizes how well Reinforcement Learning (RL) and Behavior Cloning (BC) work to improve reasoning abilities, especially in larger models, and how well BC transfers skills from more complex models to simpler ones. The study enhances LLMs' capacity to handle unclear queries by indicating that LLMs have an underlying structure of knowledge that may be improved with training."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Innovative Testbed: The research presents the EDA as a novel testbed for assessing LLMs' strategic planning and deductive reasoning skills in handling unclear user intents through questioning to deduce entities.\n\n2. Performance Analysis: Systematically assessing different LLMs, the study finds notable variations in their performance, with more powerful models such as GPT-4 surpassing human players, highlighting the sophisticated capabilities of existing LLMs.\n\n3. Model Size Insights: Results imply that performance increase may not be primarily determined by the model's size. This disproves the notion that larger models are necessarily more effective and shows that smaller models can also gain a great deal from fine-tuning."
                },
                "weaknesses": {
                    "value": "The paper could discuss more extensively the autonomous learning capabilities of LLMs. Prior research suggests LLMs like GPT-4 may have limited autonomous planning capacity and rely heavily on well-designed heuristics. The paper's task relies solely on textual goals, which may not fully capture the challenges LLMs face with numerical or spatial reasoning, and how they adapt to diverse prompt requirements"
                },
                "questions": {
                    "value": "The paper observes that LLMs tend to fall into repetitive patterns and accumulate errors, particularly in weaker models. Can the authors elaborate on potential approaches to mitigate these undesirable behaviors? Additionally, how might these patterns impact the long-term learning and adaptability of LLMs in more complex or dynamic environments?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3791/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3791/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3791/Reviewer_oYKG"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3791/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698841685240,
            "cdate": 1698841685240,
            "tmdate": 1699636335912,
            "mdate": 1699636335912,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UrS1GFQ4KY",
                "forum": "PfrpYGKGPL",
                "replyto": "LzMtPhcDzl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3791/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3791/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response"
                    },
                    "comment": {
                        "value": "We thank the reviewer for providing us with constructive feedback and supportive comments! We address the questions in below:\n\n**Numerical or spatial reasoning**: Please also refer to our general response. We acknowledge that this paper does not address numerical or spatial reasoning, and that was intentional. We will make it clearer that we are only evaluating specific capacities of LLMs involving deductive reasoning, state tracking, fact recalling and strategical planning, under multi-turn information querying scenarios. We really like the suggestion to expand our evaluation to include more realistic tasks (e.g., spatial planning, numerical reasoning, tool-use,  and task-completion chatbot) beyond text-only objectives. To achieve that, we believe that a controlled testing environment with clear metrics and evaluation would be essential as a first step towards generalizing to diverse scenarios and prompt requirements.\n\n**Mitigation of undesirable behavior**: In our experiments, we have observed that behavior cloning (BC) training can effectively mitigate undesirable behavior and generation artifact we showed in Table 9, such as repetition. We think that the model's confidence level might play a crucial role in this mitigation. When the model is uncertain about which action to take (we can probe the model by asking its current prediction and confidence level), we have noticed a tendency for it to exhibit degeneration. By incorporating reasoning and planning capabilities learned from stronger models into weaker models through BC, we can demonstrate good trajectory and enable the model to follow a more effective strategy in generating questions. Generally, we consider these issues to be fundamental to autoregressive training via teacher forcing and believe that addressing them is essential for the effective learning and adaptability of LLMs. As far as we know, mitigating these issues for LM remains an open question for further exploration [2, 3].\n\n[2] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. *The curious case of neural text degeneration*. In ICLR, 2019.\n\n[3] Jin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang Li, and Jian Li. *Learning to break the loop: Analyzing\nand mitigating repetitions for neural text generation*. In NeurIPS, 2022."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3791/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700253348271,
                "cdate": 1700253348271,
                "tmdate": 1700253859390,
                "mdate": 1700253859390,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]