[
    {
        "title": "Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles"
    },
    {
        "review": {
            "id": "boVK2sFxyU",
            "forum": "TVDUVpgu9s",
            "replyto": "TVDUVpgu9s",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4025/Reviewer_6pMr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4025/Reviewer_6pMr"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors consider the problem of optimizing a black box function with only ranking feedback. The authors propose a novel estimator for gradient descent direction based on ranking information. They show that this estimator guarantees convergence to a stationary point. Empirical evaluations covering a diverse set of applications, including reinforcement learning and diffusion generative models, show the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. As far as I know, this is the first algorithm utilizing ranking feedback that comes with a convergence guarantee. I briefly checked the proofs and found the technique establishing the main theoretical result (lemma 1) to be interesting.\n\n2. The diversity of applications demonstrates the general applicability of the proposed algorithm. The image generation example is timely with the increasing popularity of aligning generative models with human preferences."
                },
                "weaknesses": {
                    "value": "1. While the experiments are diverse, their comprehensiveness could be improved. The image generation experiment is very limited. While the three examples presented in the main paper exhibit some improvements over the baseline model, looking at additional examples in the appendix the improvements are harder to assess. Typically for image generation, some kind of human evaluation is necessary to provide a more objective evaluation.\n\n2. Similarly, the reinforcement learning experiment compares only with a single baseline method (CMA-ES), which is not designed for RL. I think comparing with more RL-focused baselines such as [1] and [2] is more appropriate. \n\n[1]: Lee, Kimin, Laura M. Smith, and Pieter Abbeel. \"PEBBLE: Feedback-Efficient Interactive Reinforcement Learning via Relabeling Experience and Unsupervised Pre-training.\" International Conference on Machine Learning. PMLR, 2021.\n\n[2]: Christiano, Paul F., et al. \"Deep reinforcement learning from human preferences.\" Advances in neural information processing systems 30 (2017).\n\n3. There are some details missing for the empirical evaluations. For example, in section 4.2, how does a policy generate multiple episodes for querying the ranking oracle? What is the network architecture to represent a policy?"
                },
                "questions": {
                    "value": "1. Towards the end of section 3.1 on how to choose $\\mu$ in Algorithm 1, can the authors expand on how to choose $\\mu$ in practice, for example in the stable diffusion experiment?\n\n2. It is not clear how the line search budget is accounted for in Section 4.1. Is it the case that, with $m=15$ queries, gradient-based algorithms will use 10 queries to estimate $g_t$ and the final 5 queries are used for the line search?\n\n3. Section 4.1 on investigating the impact of $m$ and $k$: Equation (10) seems to be dominated by $M_1(f, \\mu)$ when $k$ and $m$ are of relatively large values. That means that the second variance is actually not the dominant quantity. Please let me know if my understanding is correct.\n\n4. In section 4.2, the authors should compare with recent preference-based RL algorithms mentioned above. Additionally, it would be useful to include achieved rewards for standard RL algorithms with scalar rewards to show the performance gap (if any) using preference-based rewards."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4025/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697498023021,
            "cdate": 1697498023021,
            "tmdate": 1699636365340,
            "mdate": 1699636365340,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fX3w03mUz9",
                "forum": "TVDUVpgu9s",
                "replyto": "boVK2sFxyU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4025/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4025/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal reponse to reviewer 6pMr (1/2)"
                    },
                    "comment": {
                        "value": "Thank you for inputting valuable feedback for this manuscript. Below is our response to your major concern.\n\n### 1. The first concern is \"The image generation experiment is very limited. While the three examples presented in the main paper exhibit some improvements over the baseline model, looking at additional examples in the appendix the improvements are harder to assess. Typically for image generation, some kind of human evaluation is necessary to provide a more objective evaluation.\"\n\nWe acknowledge that it is indeed better to include an objective evaluation of the generated images. Regrettably, since the generation process for a single image requires a real human to provide 10-20 rounds of ranking feedback,  it is neither affordable for us to generate a sufficient amount of images using ZO-RankSGD for a rigorous evaluation nor to hire a real human for evaluation. Therefore, we wish to remain the main scope of this work within providing the first theoretical and principled framework for optimization with ranking oracles, and to inspire future research on incorporating this framework with more engineering consideration. \n\n---\n\n\n\n### 2. The second concern is \"Similarly, the reinforcement learning experiment compares only with a single baseline method (CMA-ES), which is not designed for RL. I think comparing with more RL-focused baselines such as [1] and [2] is more appropriate.\"\n\nWe agree with the reviewer that the baselines [1,2] are indeed closely related to the experiment setting in section 4.2, which represents a model-based RL approach for incorporating ranking feedback. However, we chose to not include them in this manuscript for the reason explained below. \n\nThere are several major differences between the black-box optimization algorithms like CMA-ES and Zo-RankSGD and the model-based RL approach like the ones in [1,2], which make it very tricky to directly compare them experimentally. \n\nSpecifically,  as illustrated by the previous works \u201dEvolution Strategies as a Scalable Alternative to Reinforcement Learning\u201c Salimans et. al and \"Simple random search of static linear policies is competitive for reinforcement learning\" Mania et. al, the black optimization algorithm like CMA-ES aims to offer model-free and off-the-shelf policy optimization without requiring models of the system dynamics, and it has several advantages over model-based RL algorithm: It enjoys extremely high parallelism when there are a sufficient amount of CPUs and is also invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.\n\nFrom our perspective, the key difference is that the model-based RL approaches in [1,2] need to first learn a reward model after collecting a sufficient amount of human feedback before any policy optimization can happen, and, as reported in [1,2], there is usually an extra burden to leverage a lot of tricks to prevent the overfitting of such reward model. On the sharp contrary, model-free algorithms like CMA-ES and ZO-RankSGD start optimizing the policy once it receives ranking feedback. \n\nGiven these differences, it is not a trivial task to directly compare a model-based RL algorithm and a model-free RL algorithm as it should cover many different aspects: Ease of hyperparameter tuning, parallel implementation, sample efficiency, etc. \n\nTherefore, we believe that this comparison is beyond the main scope of this work and hence would like to maintain the comparison within the context of black-box and model-free optimization algorithms. We hope this explanation can help the reviewer understand our motivation for such a decision.\n\n\n\n---\n\n\n\n### 3. The third concern is \"There are some details missing for the empirical evaluations. For example, in section 4.2, how does a policy generate multiple episodes for querying the ranking oracle? What is the network architecture to represent a policy?\"\n\nSorry for the confusion. Due to the page limit, we did not include many details in this manuscript and only mentioned that we adopted the same setting as Cai et al 2022. For the question \"how does a policy generate multiple episodes for querying the ranking oracle?\", when applying a typical black-box optimization algorithm for policy optimization, a general procedure is to first add small random perturbations to the policy to obtain multiple different candidate policies, and then the multiple episodes are generated from these candidate policy. As for the network architecture, we followed Cai et al 2022 to use a simple linear policy in our experiment."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4025/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699969374188,
                "cdate": 1699969374188,
                "tmdate": 1699969374188,
                "mdate": 1699969374188,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uVVaeoG1NS",
                "forum": "TVDUVpgu9s",
                "replyto": "j8YXqkBb21",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4025/Reviewer_6pMr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4025/Reviewer_6pMr"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response"
                    },
                    "comment": {
                        "value": "I want to thank the authors for their detailed responses. However, my main concerns about the empirical evaluations still remain. I will main my score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4025/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681190147,
                "cdate": 1700681190147,
                "tmdate": 1700681190147,
                "mdate": 1700681190147,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EaKwf10gN3",
            "forum": "TVDUVpgu9s",
            "replyto": "TVDUVpgu9s",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4025/Reviewer_T8Lm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4025/Reviewer_T8Lm"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the zeroth order optimization problem where the algorithm is only allowed to query a deterministic ranking oracle. That is, given two points $x_1,x_2$, the oracle returns the point at which the value of the objective function is smaller. Under smoothness assumptions, this paper designs the ZO-RankSGD algorithm that provably converges to a local with rate $\\sqrt{d/T}$, where $d$ is the dimension. Empirically, this paper also shows that ZO-RankSGD performs competitively to ZO-SGD (where the algorithm can query the function value). It is also shown that ZO-RankSGD can be used to search over the random seeds in the diffusion model using human feedback."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-\tThis paper shows both theoretically and empirically that the proposed ZO-RankSGD algorithm has competitive performance. \n-\tThe estimator for (m,k)-ranking oracle is neat and novel, and it extends the pairwise comparison oracle in classic RLHF framework. This extension is also well-motivated by the application with diffusion model, where it is reasonable and practical to generate more than two images per round.\n-\tThis paper is well-written and easy-to-follow."
                },
                "weaknesses": {
                    "value": "-\tMy main concern about this paper is that the oracles are deterministic. However, in practical settings, even human labelers have a lot of stochasticity/inconsistency when generating feedback (see e.g. [1]). Hence there is a gap between the theory and practice even though the algorithm performs well in the diffusion model application. Since most of the prior works assume a stochastic oracle, this setting needs to be further justified.\n-\tThere is no rigorous quantitative results in real-world applications (i.e., the diffusion model). In the paper, there are only a few examples of images generated using ZO-RankSGD and human feedback.\n\n[1] Dubois, Yann, et al. Alpacafarm: A simulation framework for methods that learn from human feedback."
                },
                "questions": {
                    "value": "-\tIs the convergence rate in Corollary 1 optimal in this setting?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4025/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698363732892,
            "cdate": 1698363732892,
            "tmdate": 1699636365236,
            "mdate": 1699636365236,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7TabPj1iux",
                "forum": "TVDUVpgu9s",
                "replyto": "EaKwf10gN3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4025/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4025/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal response to reviewer T8Lm"
                    },
                    "comment": {
                        "value": "Thank you for inputting valuable feedback for this manuscript. Below is our response to your major concern.\n\n### 1. The first concern is \"My main concern about this paper is that the oracles are deterministic. However, in practical settings, even human labelers have a lot of stochasticity/inconsistency when generating feedback (see e.g. [1]). Hence there is a gap between the theory and practice even though the algorithm performs well in the diffusion model application. Since most of the prior works assume a stochastic oracle, this setting needs to be further justified.\"\n\nWe agree with you that the extension to noisy ranking oracles is indeed important, and this is why we mention that it is an important future direction in the previous manuscript. It is essential to note that, unlike the comparison oracle introduced by Cai et al. (2022), which employs flipping probabilities to represent errors in noisy comparison feedback, formulating the errors in noisy ranking feedback is not straightforward. This is the key reason why we did not consider including the results on noisy ranking feedback.\n\nAs a response to this concern, we have added some preliminary experiments for investigating the performance of ZO-RankSGD on handling noisy ranking feedback, see Appendix C.3 of the revised manuscript for more details.\n\nHere we provide a summary for these new results:\n\nTo simulate noisy ranking oracles for our preliminary experiments, we consider the scenario of directly adding Gaussian noise to the ground-truth function value, then construct the corresponding noisy ranking feedback based on the perturbed values.\n\nIn these new experiments, we found that ZO-RankSGD demonstrates resilience to additive noise across different levels of variance, consistently maintaining performance comparable to ZO-SGD. Notably, for the Rosenbrock function, ZO-RankSGD outperforms ZO-SGD, indicating superior robustness to additive noise. We speculate that this advantage stems from ZO-RankSGD relying solely on rank information for optimization, which may exhibit less variability under mild additive noise.\n\nAs one of the most important future directions, we hope to establish a well-defined formulation for the noisy ranking oracles and extend the theoretical analysis of ZO-RankSGD within this context.\n\n---\n\n### 2. The second concern is \"There is no rigorous quantitative results in real-world applications (i.e., the diffusion model). In the paper, there are only a few examples of images generated using ZO-RankSGD and human feedback.\"\n\nWe acknowledge that it is indeed better to include a quantitative evaluation of the generated images. Unfortunately, we were unable to do that for two reasons. Firstly, it is unaffordable for us to generate a sufficient amount of images using ZO-RankSGD for a rigorous evaluation, because the generation process for a single image requires a real human to provide 10-20 rounds of ranking feedback. Secondly,  there is still no widely acknowledged metric to access the images generated from Stable Diffusion. A common way in existing literature on diffusion models is to report metrics like FID for toy datasets, and then show a few examples for Stable Diffusion. In our work, FID is not a good metric as it does not reflect human preference.\n\nIn all, the main scope of this work is only to provide the first theoretical and principled framework for optimization with ranking oracles, and to inspire future research on incorporating this framework with more engineering consideration.\n\n---\n\n### 3. One question is \"Is the convergence rate in Corollary 1 optimal in this setting?\"\n\nYes, our convergence rate in Corollary 1 is optimal, according to the conclusion in \"Optimal rates for zero-order convex optimization: the power of two function evaluations\" Duchi et al."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4025/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699950283751,
                "cdate": 1699950283751,
                "tmdate": 1699950461694,
                "mdate": 1699950461694,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "P1ANX9ZDTG",
                "forum": "TVDUVpgu9s",
                "replyto": "7TabPj1iux",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4025/Reviewer_T8Lm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4025/Reviewer_T8Lm"
                ],
                "content": {
                    "title": {
                        "value": "Follow-up questions"
                    },
                    "comment": {
                        "value": "Thank you for the rebuttal. Could you specify how the noise is added to the ground-truth function for the new noisy experiments with Gaussian noise? Do you mean that every time the ranking oracle $S_f(x,\\xi_1,\\xi_2, \\mu)$ is called, there is an additive Gaussian noise $\\epsilon$ in the form $sign(f(x+\\mu\\xi_1)-f(x+\\mu\\xi_2)+\\epsilon)$?"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4025/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700515975403,
                "cdate": 1700515975403,
                "tmdate": 1700515975403,
                "mdate": 1700515975403,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YZpI2SV6ZM",
            "forum": "TVDUVpgu9s",
            "replyto": "TVDUVpgu9s",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4025/Reviewer_BfNk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4025/Reviewer_BfNk"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors introduces a novel optimization algorithm, called ZO-RankSGD, designed for solving optimization problems where only ranking oracles of the objective function are available. The authors demonstrated the effectiveness of ZO-RankSGD through both simulated and real-world tasks, such as image generation guided by human feedback. Additionally, they also explored the influence of various ranking oracles on optimization performance and offer recommendations for designing user interfaces that facilitate ranking feedback. The authors further propose future research directions that include adapting the algorithm to manage noisy and uncertain ranking feedbacks, integrating it with additional methods, and applying it to a broader range of cases beyond those with  human feedback."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Strengths:\n\n- The writing of the paper is clear and easy to follow. In particular, its concise articulation of both the problem at hand and the proposed solution makes it much easier in understanding the paper. The authors also provided a rigorous mathematical formulation of the optimization challenge and provided the underlying intuition of their algorithm. Furthermore, they proved theoretical guarantess for the convergence of ZO-RankSGD and substantiate its practical effectiveness with experimental results across diverse settings.\n\n\n- The second strength of the paper is its comprehensive experiments of how different ranking oracles affect the optimization results. The authors provide valuable insights into the design of user interfaces useful for ranking feedback and propose methods to enhance the query efficiency of the algorithm."
                },
                "weaknesses": {
                    "value": "Weakness:\n\n\n- Limited experments and unsupported claims: the evaluation of the algorithm's performance on noisy and uncertain ranking feedback is very limited. Also, the authors suggest furture direction for extending the algorithm to handle the aforementioned scenarios, however, they don't have any empirical results to support the claim. I would suggest the authors to provide at least some preliminary results for coroborate the argument.\n\n- Narrow focus: the authors only focused on image generation with human feedback, which is also a bit limited. For human feedbacks, it's more commonly used in tuning language models, though there are recent works start to explore how can we incoporate human feedbacks into improving diffusion models. The method itself is interesting, and it would be a great add if the authors can show that the method is also effective for tuning language models.\n\n\n--- \nPost rebuttal: thanks to the authors response. I think it has addressed most of my concerns. I will increase the score accordingly."
                },
                "questions": {
                    "value": "see above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "see weakness."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4025/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4025/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4025/Reviewer_BfNk"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4025/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699027009037,
            "cdate": 1699027009037,
            "tmdate": 1700843659331,
            "mdate": 1700843659331,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "T5rlYZ49DO",
                "forum": "TVDUVpgu9s",
                "replyto": "YZpI2SV6ZM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4025/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4025/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal response to reviewer BfNk"
                    },
                    "comment": {
                        "value": "Thank you for inputting valuable feedback for this manuscript. Below is our response to your major concern.\n\n### 1. The first concern is \"the evaluation of the algorithm's performance on noisy and uncertain ranking feedback is very limited ..., they don't have any empirical results to support the claim. I would suggest the authors to provide at least some preliminary results for coroborate the argument.\"\n\n\n   As a response to this concern, we have added some preliminary experiments for investigating the performance of ZO-RankSGD on handling noisy ranking feedback, see Appendix C.3 of the revised manuscript for more details.\n\n   Here we provide a summary for these new results: \n\n   First of all, It is essential to note that, unlike the comparison oracle introduced by Cai et al. (2022), which employs flipping probabilities to represent errors in noisy comparison feedback, formulating the errors in noisy ranking feedback is not straightforward. This is the key reason why we did not consider including the results on noisy ranking feedback. \n\n   To simulate noisy ranking oracles for our preliminary experiments, we consider the scenario of directly adding Gaussian noise to the ground-truth function value, then construct the corresponding noisy ranking feedback based on the perturbed values. \n\n   In these new experiments, we found that ZO-RankSGD demonstrates resilience to additive noise across different levels of variance, consistently maintaining performance comparable to ZO-SGD. Notably, for the Rosenbrock function, ZO-RankSGD outperforms ZO-SGD, indicating superior robustness to additive noise. We speculate that this advantage stems from ZO-RankSGD relying solely on rank information for optimization, which may exhibit less variability under mild additive noise. \n\n   As one of the most important future directions, we hope to establish a well-defined formulation for the noisy ranking oracles and extend the theoretical analysis of ZO-RankSGD within this context. \n\n  ---\n\n### 2. The second concern is \"Narrow focus: the authors only focused on image generation with human feedback, which is also a bit limited ... it would be a great add if the authors can show that the method is also effective for tuning language models.\"\n\n   We agree with you that testing our algorithm for language models is certainly an interesting extension, and we did think about it for once. Unfortunately, since language models generally require much more computing power than diffusion models, and our computing resources are very limited, we were unable to perform such an experiment. \n\n   While saying that, we wish to argue that not including the experiment on LLM should not be overly criticized. On one hand, from our perspective, incorporating human feedback into improving diffusion models is more urgent for now as this area is still in its infancy, while there have been extensive works on language models. On the other hand, the main scope of this work is only to provide the first theoretical and principled framework for optimization with ranking oracles, and to inspire future research on incorporating this framework with more engineering consideration.\n\n   We sincerely hope the reviewer could understand our situation and take this explanation into the evaluation of the revised manuscript."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4025/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699946946446,
                "cdate": 1699946946446,
                "tmdate": 1699946946446,
                "mdate": 1699946946446,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]