[
    {
        "title": "3D Diffuser Actor: Multi-task 3D Robot Manipulation with Iterative Error Feedback"
    },
    {
        "review": {
            "id": "cGZMdhsepQ",
            "forum": "UnsLGUCynE",
            "replyto": "UnsLGUCynE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3846/Reviewer_sG3K"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3846/Reviewer_sG3K"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes 3D Diffuser Actor, a new behavior cloning algorithm combining diffusion policy and 3D representations for multi-task robotic manipulation. Utilizing the 3D representation and the attention mechanism, the proposed method achieves new SOTA on RLBench tasks. The authors also conduct real robot experiments with the newly proposed method, showing the applicability of the diffusion-based actor."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- **Motivation is good and natural**. Diffusion policies have achieved success in fitting distributions, and introducing them into 3D is a necessary step.\n- **Good results and extensive experiments.** The results (12\\% improvements) seem to be significant, which are gained on a multi-task benchmark across diverse tasks, and showing some real robot experiments are also very necessary for such robotic manipulation agents."
                },
                "weaknesses": {
                    "value": "- **No deeper analysis about why diffusion models could help**. The ablation results only show two factors matter, but all these factors seem to be not novel and not surprising, thus deeper analysis might be necessary. I have also checked the supplementary files and the presented Figure 7 looks interesting, but why `scaled linear` is worse than `square cosine` when the former one seems to cover the original distribution better?\n- **The inference time and the denoising steps are both not clear.** The proposed method achieves 12% absolute gain, but considering the inference time of diffusion models, this gain might be not obvious in the real world, due to huge latency. Could the authors also report the wall time between different algorithms?\n- **The evaluation process is not clear**. Is the result in Table 1 the best success rate over a lot of checkpoints? And how many training \nepochs are used? How many episodes are tested during the evaluation? How many seeds are used for the main results (Table 1)?\n- **Lack of baselines in real robot experiments**. \n- **Lack of discussion and experiment comparison with recent related works such as GNFactor [2].** I think this very recent method [1] could possibly serve as a baseline and it would be good to see some direct experiment results.\n- **Lack of multi-task manipulation results in real robot experiments**. Both PerAct [1] and GNFactor [2] have shown ability to execute real-world multi-task manipulation, and it could be good to compare the multi-task performance in real robot also, considering this work is closely related to PerAct [1] and GNFactor [2].\n- **Typo** in Figure 2 (a): Acter -> Actor\n\nOverall, I tend to reject this paper with a score slightly lower than borderline,  considering the above issues for the initial review.  I would carefully consider raising my score if my questions are well addressed. \n\n[1] Shridhar, Mohit, et al. \"Perceiver-actor: A multi-task transformer for robotic manipulation.\" CoRL, 2022.\n\n[2] Ze, Yanjie, et al. \"Gnfactor: Multi-task real robot learning with generalizable neural feature fields.\" CoRL, 2023."
                },
                "questions": {
                    "value": "See `weakness` above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3846/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3846/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3846/Reviewer_sG3K"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3846/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698589382225,
            "cdate": 1698589382225,
            "tmdate": 1701061226401,
            "mdate": 1701061226401,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ZJ5PfmtsDG",
                "forum": "UnsLGUCynE",
                "replyto": "cGZMdhsepQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3846/Reviewer_sG3K"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3846/Reviewer_sG3K"
                ],
                "content": {
                    "title": {
                        "value": "Update after reading the review from  Reviewer Ws6y"
                    },
                    "comment": {
                        "value": "I also found that this paper is very similar to Act3D (it seems a lot of contents are borrowed). In addition, this work [1] seems to be also very related and in a very close style.\n\nI personally think this problem might be **much worse** than just a strong reject, if without any descent explanation from the authors.\n\n[1] ChainedDiffuser: Unifying Trajectory Diffusion and Keypose Prediction for Robotic Manipulation, https://chained-diffuser.github.io/"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699848686898,
                "cdate": 1699848686898,
                "tmdate": 1699848686898,
                "mdate": 1699848686898,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jLJAtvix6j",
                "forum": "UnsLGUCynE",
                "replyto": "cGZMdhsepQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3846/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3846/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response"
                    },
                    "comment": {
                        "value": "### 1. Further analysis on the importance of diffusion\n>No deeper analysis about why diffusion models could help. The ablation results only show two factors matter, but all these factors seem to be not novel and not surprising, thus deeper analysis might be necessary.\n\n\nMotivated by your comment and the comment of Reviewer Ws6y, and in an attempt to validate the argument regarding utility for diffusion, we conducted an additional experiment where we use a close to **identical** architecture to 3D Diffuser Actor for **regressing** the 3D location and 3D rotation of the target keypose, as opposed to the noise and the use of iterative denoising (no denoising iteration index input for  regression). We show the comparison for 3D location prediction on train and val sets ([here](https://docs.google.com/presentation/d/e/2PACX-1vTZkgzAxzw2IlVY86Ft9AmRC2UR_RBVFV8uFBdeyxeg9hdCWN7mstblmIkOQAogGR2LT1k3dsLEXcTo/pub?start=false&loop=false&delayms=3000)) and the comparison for 3D rotation prediction on train and val sets ([here](https://docs.google.com/presentation/d/e/2PACX-1vSkoF10a1x7D12Ll1xCeJ4rPSA02GzTKPsCq0yS8dJWsZYuUnFAGHjCixD_blR1_hyDBdluIqTtpbsF/pub?start=false&loop=false&delayms=3000#slide=id.g29b18ed0d7c_0_0)). \n\nAs you can see, 3D diffuser actor generalizes much better than the regression model, especially for 3D location.  Both models use the same scene encoder, language encoder, and 3D relative transformer decoder architecture, but one predicts the end-effector through iterative denoising, while the other(regression) through a single feedforward pass.\n**Our contribution**: We propose the first 3D keypose diffusion model denoised from 3D scene representations for learning 3D manipulation from demonstrations. We achieve superior performance over many prior methods in apples-to-apples comparisons in an established experimental setup on RLBench(+12% compared to state-of-the-art methods).   Our model, 3D Diffuser Actor, marries 3D representations with diffusion policies for robot manipulation, as  we describe  clearly in the abstract (*\"a framework that marries diffusion policies and 3D scene representations for robot manipulation\"*) and in the intro. In both abstract and intro  we first describe diffusion policies (in abstract: *\"Diffusion policies capture the action distribution conditioned on the robot and environment state using conditional diffusion models. They have recently shown to outperform both deterministic and alternative generative policy formulations in learning from demonstrations.\"*), then 3D policies (in abstract: *\"3D robot policies use 3D scene feature representations aggregated from single or multiple 2D image views using sensed depth. They typically generalize better than their 2D counterparts in novel viewpoints.\"*) and we say we introduce 3D diffusion policies that marry the above two: (in abstract: *\"We unify these two lines of work and present a neural policy architecture that uses 3D scene representations to iteratively denoise robot 3D rotations and translations for a given language task description.\"*) \n**The specific way this marriage is accomplished is important**:  we represent the current noisy end-effector pose estimate as a 3D token in the 3D feature cloud and featurize it  with relative 3D attentions. Since the ICLR submission deadline, **we ran the exact same model with absolute position attention (closer to what 2D diffusion policies do) on the whole PerAct benchmark and obtained an 8% worse performance**. We consider the above 3D diffusion keypose policy model to be our contribution.\n\n**Please, also see the general response in the beginning of the rebuttal.**\n\n\n\n>I have also checked the supplementary files and the presented Figure 7 looks interesting, but why scaled linear is worse than square cosine when the former one seems to cover the original distribution better?\n\nThe objective of the noising process is to cover the whole space densely, as during inference we start from pure Gaussian noise and gradually denoise, following the DDPM method. As Figure 7 shows, the scaled linear schedule captures the distribution instead of covering the whole space of possible rotations. As a result, when randomly initializing from Gaussian noise, our model gets out of distribution and cannot denoise to a valid solution."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700017449001,
                "cdate": 1700017449001,
                "tmdate": 1700017449001,
                "mdate": 1700017449001,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gFO9EmscXD",
                "forum": "UnsLGUCynE",
                "replyto": "cGZMdhsepQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3846/Reviewer_sG3K"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3846/Reviewer_sG3K"
                ],
                "content": {
                    "title": {
                        "value": "Thank the authors for response"
                    },
                    "comment": {
                        "value": "Thank the authors for detailed explanations. I have the following further questions.\n\n## 4. Real-robot experiments. \n> This is hard to do as it requires resetting the real environment to identical state across multiple models. PerAct, Act3D, RVT, InstructRL and most other works do not consider baselines in the real world experiments for that reason. You are absolutely correct, we did not have the time to do that, it is within the capabilities of our model. We leave this for future work.\n\nConsidering the time given in the discussion period (~12 days), I do not think any initial real robot experiments (e.g., one task and 10 trials) could not be conducted. Training one baseline agent might take mostly 1-2 days.\n\nAnd yes, you are right that most of the papers you mention do not compare other baselines in the real world. However, as the authors describe, the setting is closely following PerAct [1]. PerAct is not comparing with other baselines mainly because there are no competitive baselines. In addition, there are also papers in a close setting that actually do, e.g., GNFactor [2].\n\nOverall, I do not think the excuse from the authors holds.\n\n\n## 5 Comparision to GNFactor\n> Following your suggestion, we will report our performance with one camera and 20 demonstrations on these 10 tasks to be able to compare with GNFactor. Thank you for your suggestion.\n\nIt seems the updated paper does not include any new discussion or new experiments regarding this question.\n\n> Due to these discrepancies, we did not know how to judge their setup.\n\nAgain, this discrepancy can not hold as an excuse for no discussion and no experiments, since the code of GNFactor [2] is available online, and the small discrepancy (only the task and the number of demonstrations) is very common for imitation learning papers. Comparing them under the same setting is not an impossible thing, let alone with enough time for the discussion period.\n\n\nI would maintain my score, but be willing to raise my score if the problems are addressed better.\n\n[1] Shridhar, Mohit, et al. \"Perceiver-actor: A multi-task transformer for robotic manipulation.\" CoRL, 2022.\n\n[2] Ze, Yanjie, et al. \"Gnfactor: Multi-task real robot learning with generalizable neural feature fields.\" CoRL, 2023."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536095854,
                "cdate": 1700536095854,
                "tmdate": 1700559894484,
                "mdate": 1700559894484,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "K1huDRTrie",
            "forum": "UnsLGUCynE",
            "replyto": "UnsLGUCynE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3846/Reviewer_Ws6y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3846/Reviewer_Ws6y"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a framework that marries 3D scene representations and diffusion policies for imitation learning of robot manipulation tasks. The proposed scene representation is a 3D feature cloud fused from multi-view, multi-scale CLIP features using depth maps, which encodes semantics and spatial information. The diffusion policy captures the multimodality of action distribution. In the experiments, the proposed method is compared against multiple baselines on the simulated RLBench dataset, and tested on 5 real-world tasks. Ablation studies further verify the necessity of the 3D representation and the use of relational attention.\n\nNote that many of the design choices and setups in this work are largely based on Act3D [1].\n\n[1] Theophile Gervet, Zhou Xian, Nikolaos Gkanatsios, and Katerina Fragkiadaki. Act3d: Infinite\nresolution action detection transformer for robotic manipulation. CORL 2023."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper is probably the first work to combine 3D scene representations with diffusion policy for learning robot manipulation tasks. The proposed framework demonstrates good performance in both simulation and real-world experiments, and establishes a new SOTA on RLBench tasks."
                },
                "weaknesses": {
                    "value": "My initial impression of this paper is that some aspects of the method lack clarity, and certain paragraphs appear somewhat inconsistent. I found myself confused about specific technical details until I reviewed Act3D [1], a previous paper that this work heavily draws upon. \n\n**My primary concern about this work is that a portion of the technical method and writting appears to be directly borrowed from [1]. However, this relationship is not transparently acknowledged.**\n\n1. The proposed framework adopts the 3D scene representation (in a simplified form), 3D relational transformer, and training/evaluation setups from [1], which is never explicitly mentioned in the paper.\n2. The main contribution of this work lies in the use of a diffusion policy to capture multimodal action distributions. However, this aspect is not extensively discussed or thoroughly evaluated.\n3. Not only the writing style of this paper closely resemble Act3D [1], but some paragraphs have similar or even same counterparts in [1]. Detailed in ethics review."
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Many paragraphs are directly borrowed from a previous work [1], some with minor rephrase. This potentially forms plagiarism. \n\n| level of similarity | paragraph in this paper | counterpart in [1]  |\n|----------|----------|----------|\n| Rephrased  |  \u201c2D and 3D scene representations for robot manipulation\u201d in Section 2   | \"Learning robot manipulation from demonstrations\" in Section 2   |\n| Rephrased & adapted  | the first two paragraphs in Section 3.2   | Section 3   |\n| Rephrased (almost the same)  | \"Scene and language encoder\" in Section 3.2 | \"Visual and language encoder\" in Section 3 |\n| Partially the same | \"Baselines\" and \"Evaluation metrics\" in Section 4 | \"Baselines\" and \"Evaluation metric\" in Section 4.1 |\n\n[1] Theophile Gervet, Zhou Xian, Nikolaos Gkanatsios, and Katerina Fragkiadaki. Act3d: Infinite\nresolution action detection transformer for robotic manipulation. CORL 2023."
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3846/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3846/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3846/Reviewer_Ws6y"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3846/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698761260441,
            "cdate": 1698761260441,
            "tmdate": 1699636342674,
            "mdate": 1699636342674,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Cx9oYQTiDI",
                "forum": "UnsLGUCynE",
                "replyto": "K1huDRTrie",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3846/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3846/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response"
                    },
                    "comment": {
                        "value": "### 1. Relationship to Act3D\n\n>My primary concern about this work is that a portion of the technical method and writting appears to be directly borrowed from [1]. However, this relationship is not transparently acknowledged.\n\n\n**Please, see the general response in the beginning of the rebuttal.**\n\n\n> The proposed framework adopts the 3D scene representation (in a simplified form), 3D relational transformer, and training/evaluation setups from [1], which is never explicitly mentioned in the paper.\n\n\n**Please, see the general response in the beginning of the rebuttal.**\n\n\n\n\n\n### 2. Discussion on multimodality and contribution\n>The main contribution of this work lies in the use of a diffusion policy to capture multimodal action distributions. However, this aspect is not extensively discussed or thoroughly evaluated.\n\n\n\nMotivated by your comment and the comment of Reviewer sG3K, and in an attempt to validate the argument regarding utility for diffusion, we conducted an additional experiment where we use a close to **identical** architecture to 3D Diffuser Actor for **regressing** the 3D location and 3D rotation of the target keypose, as opposed to the noise and the use of iterative denoising (no denoising iteration index input for  regression). We show the comparison for 3D location prediction on train and val sets ([here](https://docs.google.com/presentation/d/e/2PACX-1vTZkgzAxzw2IlVY86Ft9AmRC2UR_RBVFV8uFBdeyxeg9hdCWN7mstblmIkOQAogGR2LT1k3dsLEXcTo/pub?start=false&loop=false&delayms=3000)) and the comparison for 3D rotation prediction on train and val sets ([here](https://docs.google.com/presentation/d/e/2PACX-1vSkoF10a1x7D12Ll1xCeJ4rPSA02GzTKPsCq0yS8dJWsZYuUnFAGHjCixD_blR1_hyDBdluIqTtpbsF/pub?start=false&loop=false&delayms=3000#slide=id.g29b18ed0d7c_0_0)). \n\nAs you can see, 3D diffuser actor generalizes much better than the regression model, especially for 3D location.  Both models use the same scene encoder, language encoder, and 3D relative transformer decoder architecture, but one predicts the end-effector through iterative denoising, while the other(regression) through a single feedforward pass.\n\n**Our contribution**: We propose the first 3D keypose diffusion model denoised from 3D scene representations for learning 3D manipulation from demonstrations. We achieve superior performance over many prior methods in apples-to-apples comparisons in an established experimental setup on RLBench(+12% compared to state-of-the-art methods).   Our model, 3D Diffuser Actor, marries 3D representations with diffusion policies for robot manipulation, as  we describe  clearly in the abstract (*\"a framework that marries diffusion policies and 3D scene representations for robot manipulation\"*) and in the intro. In both abstract and intro  we first describe diffusion policies (in abstract: *\"Diffusion policies capture the action distribution conditioned on the robot and environment state using conditional diffusion models. They have recently shown to outperform both deterministic and alternative generative policy formulations in learning from demonstrations.\"*), then 3D policies (in abstract: *\"3D robot policies use 3D scene feature representations aggregated from single or multiple 2D image views using sensed depth. They typically generalize better than their 2D counterparts in novel viewpoints.\"*) and we say we introduce 3D diffusion policies that marry the above two: (in abstract: *\"We unify these two lines of work and present a neural policy architecture that uses 3D scene representations to iteratively denoise robot 3D rotations and translations for a given language task description.\"*) \n**The specific way this marriage is accomplished is important**:  we represent the current noisy end-effector pose estimate as a 3D token in the 3D feature cloud and featurize it  with relative 3D attentions. Since the ICLR submission deadline, **we ran the exact same model with absolute position attention (closer to what 2D diffusion policies do) on the whole PerAct benchmark and obtained an 8% worse performance**. We consider the above 3D diffusion keypose policy model to be our contribution.\n\n\n**Please, also see the general response in the beginning of the rebuttal.**"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700017333497,
                "cdate": 1700017333497,
                "tmdate": 1700017333497,
                "mdate": 1700017333497,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "taxoVHAUHa",
                "forum": "UnsLGUCynE",
                "replyto": "Cx9oYQTiDI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3846/Reviewer_Ws6y"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3846/Reviewer_Ws6y"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for their time and efforts to prepare the rebuttal and revise the paper.\n\nThe paper looks less similar to Act3D after the authors rewrite some of the paragraphs and explicitly acknowledge borrowing technical contents from it. However, as the scene representation it uses can be trivially adapted from Act3D, replacing the regression policy head in Act3D with a diffusion model is an incremental contribution.\n\nThe extra experimental results reveal that the diffusion head generalizes better than the regression head, which makes sense to me. But as a major claim of the paper is \"handling multi-modality\", it will be nice to explicitly demonstrate the multi-modality robot behavior qualitatively or quantitatively. A good example can be found in Figure 4 of the diffusion policy paper [1].\n\n[1] Chi, Cheng, et al. \"Diffusion policy: Visuomotor policy learning via action diffusion.\" RSS 2023.\n\nI will keep my rating as it is."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700620986195,
                "cdate": 1700620986195,
                "tmdate": 1700620986195,
                "mdate": 1700620986195,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "d4BqPQKZ7z",
            "forum": "UnsLGUCynE",
            "replyto": "UnsLGUCynE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3846/Reviewer_7nbv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3846/Reviewer_7nbv"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes 3D Diffuser Actor, a transformer-based behavior-cloning method that combines the power of diffusion policies and 3D scene representations. The model tokenizes multi-view camera observations, language instructions, and the history proprioception information. A 3D relative transformer models the diffusion process, processing these tokens to predict the next gripper pose over several diffusion steps. The experimental results showcase the remarkable performance of the proposed method, outperforming several strong baselines in the RLBench environments."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The experimental results are exceptionally strong, demonstrating a substantial improvement over various recently proposed baselines (from late 2022 to mid-2023)."
                },
                "weaknesses": {
                    "value": "1. The method's description lacks clarity, particularly concerning crucial components of the architecture. The appendix does not sufficiently clarify these ambiguities either.\n2. The backbone of the proposed network is extremely similar to Act3D, including using multi-view image input, using the pyramid network for feature exaction, the generation of the 3D feature cloud, and the 3D relative transformer. I understand that Act3D is a very recent work, however, as the authors are already aware of Act3D, proper discussion regarding the relationship between this work and Act3D should be addressed."
                },
                "questions": {
                    "value": "1. When building the 3D scene feature cloud, the authors claim, `We associate every 2D feature grid location in the 2D feature maps with a depth value, by averaging the depth values of the image pixels that correspond to it.` What is a `grid` here? Is each 2D feature map from the feature pyramid network separated into an NxN grid akin to ViT?\n2. When generating the 3D feature cloud, how does the method handle cases where multiple points (pixels) from different views correspond to the same 3D location? Are the features averaged in such instances?\n3. What constitutes a visual token in this context? Are individual 3D points considered tokens?\n4. The performance of the proposed method is notably poor in close jar and sort shape. Could the authors provide a detailed failure analysis to shed light on these issues?\n5. Given that diffusion policies suggest the utility of diffusing multiple action steps into the future, has the method been evaluated with multiple keypoint steps in the diffusion process?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3846/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3846/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3846/Reviewer_7nbv"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3846/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698778950900,
            "cdate": 1698778950900,
            "tmdate": 1699636342588,
            "mdate": 1699636342588,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UDMdmbpy5T",
                "forum": "UnsLGUCynE",
                "replyto": "d4BqPQKZ7z",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3846/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3846/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response"
                    },
                    "comment": {
                        "value": "### 1. Same backbone as Act3D\n> The backbone of the proposed network is extremely similar to Act3D, including using multi-view image input, using the pyramid network for feature exaction, the generation of the 3D feature cloud, and the 3D relative transformer. I understand that Act3D is a very recent work, however, as the authors are already aware of Act3D, proper discussion regarding the relationship between this work and Act3D should be addressed.\n\n**Please, see  the general response in the beginning of the rebuttal regarding the relationship between this work and Act3D.** \n\n### 2. Explanation of 3D feature cloud\n> When building the 3D scene feature cloud, the authors claim, We associate every 2D feature grid location in the 2D feature maps with a depth value, by averaging the depth values of the image pixels that correspond to it. What is a grid here? Is each 2D feature map from the feature pyramid network separated into an NxN grid akin to ViT?\n\nAn image encoder extracts a $H \\times W \\times C$ feature map from each camera view, where $H$, $W$, and $C$ denotes the height, width and channel dimension of the feature map.  Each $1 \\times 1 \\times C$ feature vector (we call it token) in the feature map corresponds to a fixed-size patch in the input image. We can thus lift a 2D feature token to 3D by averaging the 3D locations of all pixels within the patch. The 3D locations of pixels are obtained by combining the 2D pixel locations and depth values (x, y, d) using the camera intrinsics and pinhole camera equation. We updated the manuscript and Figure 1 to explain the feature extraction and 2D-3D lifting procedure more clearly.\n\n> When generating the 3D feature cloud, how does the method handle cases where multiple points (pixels) from different views correspond to the same 3D location? Are the features averaged in such instances?\n\nWe found that having two points with identical 3D locations rarely happens, so we did not explicitly handle this case. Although some merging could be applied, such as averaging, we adopted a more general solution and simply let attention focus on the important features.\n\n>What constitutes a visual token in this context? Are individual 3D points considered tokens?\n\nEach  feature vector from the 2D feature maps lifted to 3D is a 3D token. \n\n\n\n### 3. Predicting further in the future\n>Given that diffusion policies suggest the utility of diffusing multiple action steps into the future, has the method been evaluated with multiple keypoint steps in the diffusion process?\n\nThank you for your suggestion. Although in this version we only predict the next keypose, our model could indeed support the prediction of multiple keyposes and it would be interesting to explore that in the future as a way to plan. Nonetheless, we would like to emphasize that our diffusion model predicts the next subgoal to reach, not low-level actions directly. Instead, once a subgoal keypose has been predicted, then a low-level planner is assigned with predicting a sequence of actions that connect the current pose and the predicted keypose. This decomposition has been followed by previous approaches as well (e.g. C2F-QA, PerAct, Act3D). This is different from what related works on diffusion policies do, which is to predict low-level actions directly."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700017247895,
                "cdate": 1700017247895,
                "tmdate": 1700017247895,
                "mdate": 1700017247895,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ayugMukOcP",
            "forum": "UnsLGUCynE",
            "replyto": "UnsLGUCynE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3846/Reviewer_2s3N"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3846/Reviewer_2s3N"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a framework called \"3D Diffuser Actor,\" combining diffusion policies and 3D scene representations to enhance robot manipulation. Along with 3D scene features aggregated from single or multiple 2D image views using sensed depth, these policies enable improved generalization over 2D counterparts. The new model\u2019s architecture uses 3D scene representations to iteratively rectify robot 3D rotations and translations given a language description of a task. The experiments demonstrate the efficacy of the proposed framework by outperforming previous benchmarks in learning from demonstrations, both in simulated environment and real world."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors conduct a thorough evaluation of their method as they evaluate their method in both simulated environment and real world, and comparing them with existing strong baselines. A thorough evaluation have us better understand the proposed model and their actual performance."
                },
                "weaknesses": {
                    "value": "1. The scientific contribution of this paper is unclear. Diffusion model could not be the contribution of this paper, and adopting diffusion mode for trajectory generation is not a new idea (see [a]). \n2. The figures in this paper lack sufficient illustrative value and information. For instance, Figure 1 appears to show the model taking a multi-view image as input, yet the caption indicates it uses a 3D scene feature cloud. A clear connection between these two elements would greatly improve understanding."
                },
                "questions": {
                    "value": "Your diffusion model appears to produce only the target pose of the action, after which a trajectory is generated using the MoveIt planner given the initial joint state and target joint state. Does the robot execute the entire trajectory directly until the end, or does it update the target pose using the diffusion model and regenerate the trajectory after each forward step?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3846/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3846/Reviewer_2s3N",
                        "ICLR.cc/2024/Conference/Submission3846/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3846/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698977567875,
            "cdate": 1698977567875,
            "tmdate": 1700732373874,
            "mdate": 1700732373874,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "14Uul3tytY",
                "forum": "UnsLGUCynE",
                "replyto": "ayugMukOcP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3846/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3846/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors' Response"
                    },
                    "comment": {
                        "value": "### 1. Contribution\n> The scientific contribution of this paper is unclear. Diffusion model could not be the contribution of this paper, and adopting a diffusion model for trajectory generation is not a new idea (see [a]).\n\n\n\n\n**Our contribution**: We propose the first 3D keypose diffusion model denoised from 3D scene representations for learning 3D manipulation from demonstrations. We achieve superior performance over many prior methods in apples-to-apples comparisons in an established experimental setup on RLBench(+12% compared to state-of-the-art methods).   Our model, 3D Diffuser Actor, marries 3D representations with diffusion policies for robot manipulation, as  we describe  clearly in the abstract (*\"a framework that marries diffusion policies and 3D scene representations for robot manipulation\"*) and in the intro. In both abstract and intro  we first describe diffusion policies (in abstract: *\"Diffusion policies capture the action distribution conditioned on the robot and environment state using conditional diffusion models. They have recently shown to outperform both deterministic and alternative generative policy formulations in learning from demonstrations.\"*), then 3D policies (in abstract: *\"3D robot policies use 3D scene feature representations aggregated from single or multiple 2D image views using sensed depth. They typically generalize better than their 2D counterparts in novel viewpoints.\"*) and we say we introduce 3D diffusion policies that marry the above two: (in abstract: *\"We unify these two lines of work and present a neural policy architecture that uses 3D scene representations to iteratively denoise robot 3D rotations and translations for a given language task description.\"*) \n**The specific way this marriage is accomplished is important**:  we represent the current noisy end-effector pose estimate as a 3D token in the 3D feature cloud and featurize it  with relative 3D attentions. Since the ICLR submission deadline, **we ran the exact same model with absolute position attention (closer to what 2D diffusion policies do) on the whole PerAct benchmark and obtained an 8% worse performance**. We consider the above 3D diffusion keypose policy model to be our contribution.\n**Please, also see the general response in the beginning of the rebuttal.**\n\n\n### 2. Explanation of 3D feature cloud\n> The figures in this paper lack sufficient illustrative value and information. For instance, Figure 1 appears to show the model taking a multi-view image as input, yet the caption indicates it uses a 3D scene feature cloud. A clear connection between these two elements would greatly improve understanding.\n\nAn image encoder extracts a $H \\times W \\times C$ feature map from each camera view, where $H$, $W$, and $C$ denotes the height, width and channel dimension of the feature map.  Each $1 \\times 1 \\times C$ feature token in the feature map corresponds to a fixed-size patch in the input image. We can thus lift a 2D feature token to 3D by averaging the 3D locations of all pixels within the patch. The 3D locations of pixels are obtained by combining the 2D pixel locations and depth values (x, y, d) using the camera intrinsics and pinhole camera equation. We updated the manuscript to explain the feature extraction and 2D-3D lifting procedure more clearly. We also **updated Figure 1** to illustrate the lifting process.\n\n### 3. Frequency of keypose prediction\n> Your diffusion model appears to produce only the target pose of the action, after which a trajectory is generated using the MoveIt planner given the initial joint state and target joint state. Does the robot execute the entire trajectory directly until the end, or does it update the target pose using the diffusion model and regenerate the trajectory after each forward step?\n\nOnce a keypose is predicted, a low-level planner is tasked to predict a trajectory between the current and the predicted pose. This trajectory is executed open-loop until the end. We found that this was sufficient for all 18 tasks we focus on. At the same time, it\u2019s more efficient computationally, since the denoising process is sparsely called."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700017167798,
                "cdate": 1700017167798,
                "tmdate": 1700017167798,
                "mdate": 1700017167798,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EvxfqJatc2",
                "forum": "UnsLGUCynE",
                "replyto": "ayugMukOcP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3846/Reviewer_2s3N"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3846/Reviewer_2s3N"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to thank the authors for their comprehensive responses, touching up of the figures.\n\nThe overall writing of this paper is more clear. However, after going through all the comments (from the authors and other reviewers), I am still not convinced by the claimed contributions. The proposed method is more of an assemble of existing techniques without providing adequate sharp insight the of manipulation problem. \n\nI will downgrade my rating as the authors does not provide adequate strong argument for their contributions and evidence for the advantages of the proposed method in real world.\n\nPlus, I also change my rating of the presentation of this paper from 2 to 3."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3846/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732227007,
                "cdate": 1700732227007,
                "tmdate": 1700732429385,
                "mdate": 1700732429385,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]