[
    {
        "title": "Nonparametric Classification on Low Dimensional Manifolds using Overparameterized Convolutional Residual Networks"
    },
    {
        "review": {
            "id": "5a2SgllTFi",
            "forum": "L3yJ54gv3H",
            "replyto": "L3yJ54gv3H",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2580/Reviewer_BAzp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2580/Reviewer_BAzp"
            ],
            "content": {
                "summary": {
                    "value": "The authors provide a nonparametric analysis of ConvResNeXts---a generalisation of deep convolutional residual networks. In particular, the authors derive approximation and estimation bounds when the target function is in the Besov class, that is the class of functions supported on a low-dimensional differential manifold and having controlled smoothness. In particular, the estimation result shows that ConvResNeXts achieves a rate of error convergence close to the minimax rate."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper makes a good combination of several ideas on neural network construction and generalization estimation. The central results are clear and easy to locate. Most claims are well-supported and section 4 is extremely useful in understanding the proofs."
                },
                "weaknesses": {
                    "value": "The lack of a strong comparison with previous works makes it difficult to appreciate the impact of the result. The adaptivity to the data distribution is a well-known property of fully connected networks of any depth (even in the kernel regime), which makes me doubt the necessity of developing a theory of ConvResNeXts. What insights do we get from the analysis that requires using ConvResNeXts?\n\nSecondly, I found the definition of Besov spaces too dense and, hence, difficult to understand. Perhaps a few examples would help, as well as a more detailed account of how such spaces generalise Sobolev and Holder spaces, which are known to a wider audience."
                },
                "questions": {
                    "value": "1. The brief definition of ConvResNeXts at the end of the second paragraph does not suffice to understand the architecture.\n\n2. Adding a quick definition of Besov spaces when they are first mentioned in the third paragraph would improve readability. It is not reasonable that a reader unfamiliar with these spaces should wait until the conclusions to read a comment on their nature. In addition, the paragraph explaining besov spaces in the conclusions would benefit from further clarification, e.g. some way to understand why Holder and Sobolev is contained in Besov and concrete support to vague claims such as 'Besove spaces can capture important features such as edges'."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2580/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2580/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2580/Reviewer_BAzp"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2580/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698743581859,
            "cdate": 1698743581859,
            "tmdate": 1699636195343,
            "mdate": 1699636195343,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0awlYws6NA",
                "forum": "L3yJ54gv3H",
                "replyto": "5a2SgllTFi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2580/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2580/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Response to Reviewer BAzp"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nThank you for taking the time to review the ICLR paper. I would like to address the question you've raised with the following response.\n\n**Weakness 1: The lack of a strong comparison with previous works makes it difficult to appreciate the impact of the result. The adaptivity to the data distribution is a well-known property of fully connected networks of any depth (even in the kernel regime), which makes me doubt the necessity of developing a theory of ConvResNeXts. What insights do we get from the analysis that requires using ConvResNeXts?**\n\nWe are not sure whether the reviewer is referring to the analysis based on the neural tangent kernel. Chen and Xu (2020, arXiv 2009.10683) show that the neural tangent kernel of deep feedforward networks essentially corresponds to a function class induced by the Laplacian kernel. Though the theoretical analysis relies on the eigendecomposition of the corresponding reproducing kernel Hilbert space, which depends on the underlying data distribution, the obtained statistical rates of convergence are not necessarily adaptive to the intrinsic dimension of the Riemannian manifolds considered in this paper, and therefore still suffer from the curse of dimensionality. For example, Bach (2014, arXiv 1412.8690) only proves the adaptivity to the linear subspace, not more general manifolds. Moreover, to estimate Besov functions optimally, it is essential that the method uses a different kernel \"bandwidth\" parameters for each local neighborhood. This is something that DNNs in the NTK regime are not capable of (more of this in our Answer 2!).  \n\nExisting results on neural networks, such as Chen et al. (2021), do show that feedforward networks can adapt to the low dimensional manifolds. However, their results are not applicable to overparameterized neural networks considered in this paper. \n\nA recent paper (Zhang and Wang, 2023) highlights that weight-decay and overparameterized DNNs in parallel blocks can be optimal for Besov functions due to the weight-decay induced sparsity, but it was left as an open problem whether the parallel blocks were crucial. They also did not consider adapting to the manifold.  The architectural insight we proved in this paper is that we don't necessarily need parallel blocks when we have residual connections, and that we can exchange Depth $M$ and Width $N$ as long as their product remains the same.  To say it differently, while we did not show that the Convolutional layers are essential, we did provide new insight into why \"residual connection\" and \"parallel blocks' ' in ResNeXts are useful in both approximation and generalization. \n\n**Weakness 2: I found the definition of Besov spaces too dense and, hence, difficult to understand. Perhaps a few examples would help, as well as a more detailed account of how such spaces generalize Sobolev and Holder spaces, which are known to a wider audience.**\n\nDefining the Besov space is essential for illustrating the merits of overparameterized NN over kernels. Kernel ridge regression with any kernel choices was proven to be suboptimal for estimating functions in the Besov class from the work of Donoho, Liu, MacGibbon in 1990. This includes NTKs. As for Sobolev and Holder classes, they are smaller function classes that do not include functions with heterogeneous smoothness.  We have included a new Appendix A which illustrates what ``functions with heterogeneous smoothness'' means concretely and clarifies the relationship between Holder, Sobolev and Besov classes.  We hope this addresses your concern.\n\n**Q1: The brief definition of ConvResNeXts at the end of the second paragraph does not suffice to understand the architecture.**\n\nA1:  We have introduced the architecture of ConvResNeXts in Section 2.3 and also see Figure 1(b) for illustration. In the updated paper, we have added a pointer to Section 2.3 and Figure 1(b) in the introduction to enhance the accessibility of our ConvResNeXt architecture description.\n\n**Q2: Adding a quick definition of Besov spaces when they are first mentioned in the third paragraph would improve readability. In addition, the paragraph explaining besov spaces in the conclusions would benefit from further clarification.**\n\nA2: We have added a quick introduction of Besov spaces in the third paragraph of the introduction to improve readability in the updated version. Thanks for the suggestion! Moreover, we have added a detailed comparison among different functional spaces in the appendix due to the space limit. Roughly speaking, the Besov space contains functions with heterogeneous smoothness while Holder and Sobolev classes contain functions with homogeneous smoothness. ConvResNeXts can achieve a near optimal convergence rate for estimating functions in Besov classes, while this is not achievable by kernel methods. This separation does not exist in smaller function classes such as Sobolev and Holders because they are more homogeneously smooth."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708520624,
                "cdate": 1700708520624,
                "tmdate": 1700708520624,
                "mdate": 1700708520624,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1B7LbFmAPr",
            "forum": "L3yJ54gv3H",
            "replyto": "L3yJ54gv3H",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2580/Reviewer_cTm8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2580/Reviewer_cTm8"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the capacity of Convolutional residual networks to approximate and estimate smooth (Besov) functions on smooth manifold. The paper focuses on the ConvResNext architecture, a convolutional network with residual connections and parallel modules. The paper shows that these networks can approximate arbitrary smooth target functions supported on a smooth manifold, without suffering from the curse of dimensionality, i.e. with no exponential dependence on the extrinsic dimension of the problem. The work also studies an estimation result, giving a generalization bound for ConvResNext architecture trained on smooth functions on manifolds."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper provides novel generalization and approximation results on residual convolutional networks fitting smooth functions on manifolds."
                },
                "weaknesses": {
                    "value": "While both the approximation and generalization results seem novel, it is not clear to me whether the derived bounds are interesting in the context of learning with convolutional networks. In particular, it seems that the approximation results hold just as well for standard feedforward networks, and the main contribution of this work is transforming these bounds from densely connected networks to convolutional networks. The work relies on a result stating that any feedforward network can be reformulated as a convolutional network. Given this result, this just means that any property that holds for feedforward networks holds to some extent for a conv-nets. The property of approximating smooth functions on a manifold seems to be just a particular case where this argument applies. Conv-nets are typically used in cases where they display significant benefits over densely connected feedforward, and I believe this should be reflected in the theoretical results. I think the authors should clearly state the following:\n1) Which of the results in the paper also apply to feedforward networks?\n2) For the results that apply to feedforward networks, which of them are novel to this work? E.g., is the approximation bound on smooth functions on a manifold has been already established for feedforward networks?\n3) In which results there is an actual benefit for using a convolutional networks compared to feedforward dense networks?\n\nAnother more minor issue is with the presentation of the approximation and generalization bounds. If I understand correctly, some of the constants in the bounds have exponential dependence on the intrinsic dimension of the manifold, and it is interesting to show this dependence explicitly.\n\nMinor:\n- Definition 6: there seems to be a typo, with some unclosed bracket.\n- Bottom of page 5: \"hyperparameters parameters\" should be just \"hyperparameters\"?\n- Top of page 7: \"our This\""
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2580/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698936749747,
            "cdate": 1698936749747,
            "tmdate": 1699636195280,
            "mdate": 1699636195280,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Q4m7PHLJ02",
                "forum": "L3yJ54gv3H",
                "replyto": "1B7LbFmAPr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2580/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2580/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Response to Reviewer cTm8 (Part 1)"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nThank you for providing a thoughtful review of our paper. We appreciate your feedback and would like to address the points you have raised in a rebuttal.\n\n**Q1: Which of the results in the paper also apply to feedforward networks?**\n\nI believe there might be misunderstanding here. Our work focuses on ConvResNexts, which includes the popular ConvResNet as a special example. Our techniques also work for (non-convolutional) ResNets and ResNeXts. However, our results are not directly applicable to feedforward networks without residual connections.\n\n**Q2: For the results that apply to feedforward networks, which of them are novel to this work? E.g., has the approximation bound on smooth functions on a manifold been already established for feedforward networks?**\n\nSimilar results have been established in Chen et. al. (2019) for feedforward networks but without **overparameterization** or **adaptivity to the Besov class**. An intermediate result from our theory does apply to overparameterized feedforward networks with residual connections. The decomposition from feedforward networks to convolutional networks enables us to further investigate the depth and width tradeoff in the ConvResNeXts architectures.\n\nThe novelty of our theorems is that we consider a **more complex** model structure, ResNeXt, where there are $N$ residual blocks and each of them has $M$ parallel paths with **identity connections**. In comparison, existing work only focuses on dense-connected feedforward neural networks. Moreover, our network can be **overparameterized** by manipulating either a large $M$, a large $N$, or both. This stands in stark contrast to the constraints in Zhang and Wang (2022), where $M$ must be large and $N=1$, and in Liu et al. (2019), where $M=1$ and $N$ must be properly upper bounded. Even though our network is highly overparameterized, we prove that it can efficiently learn the ground-truth function when trained with weight decay. However, classical statistical theories cannot explain this phenomenon due to the huge variance of the neural network class. \n\nIn addition, compared to Zhang and Wang (2022), our paper considers the problem where the ground-truth Besov function is supported on a low-dimensional **manifold**. We develop a close-to-optimal convergence rate which only depends on the intrinsic dimension of the manifold, thus circumventing the curse of the ambient dimensionality. \n\n**Q3: In which results is there an actual benefit for using convolutional networks compared to feedforward dense networks?**\n\nConvolutional networks (convnets) have long exhibited advantages over feedforward networks, particularly in vision tasks. Our study focused on non-parametric regression and classification with data on a smooth manifold, an abstraction of vision problems where images reside on a \"natural image\" manifold. Our results show they both ConvResNet, (feedforward) ResNet and whether or not there are parallel blocks as in ResNeXt,  they all could work, and one can replace width with depth as long as $MN$ remains sufficiently large.  As for whether the convnet is better or worse than the feedforward net, it really depends on which manifold and what kind of functions on the manifold we are estimating.\n\nOne important property that helps us prove the convergence rate is the **sparsity**, which is induced by the block-wise architecture in a ResNeXt. Feedforward neural networks do not have such an architecture, and typically do not have sparsity after training. Because of that, to the best of our knowledge, it has been completely untouched on whether feedforward neural networks without cardinality constraints have the same or better sample efficiency than ResNexts in learning functions in Besov space, not to mention Besov functions on a manifold.\n\nConsidering the popularity of ConvResNets, the theoretical study on ConvResNets or ConvResNeXts is quite limited. We are not aware of any existing results on the approximation and statistical theories for such practical networks. Our work attempts to bridge the gap by establishing a close-to-optimal statistical rate of convergence. Though we are not able to show the benefits of ConvResNeXts, our work should still be considered as a substantial contribution, as our setting is very close to real applications in several aspects: Practical architectures with **overparameterization**, **training with weight decay** and **the data exhibiting low dimensional structures**."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708236848,
                "cdate": 1700708236848,
                "tmdate": 1700708265214,
                "mdate": 1700708265214,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uBwGQXC4T3",
            "forum": "L3yJ54gv3H",
            "replyto": "L3yJ54gv3H",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2580/Reviewer_CZj1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2580/Reviewer_CZj1"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies weight decay regularized training of overparameterized convolutional neural network through the lens of nonparametric classification. In particular, the authors consider ConvResNeXts, which cover ConvResNets, and show that such training induced sparsity. This explains why overparameterized neural networks generalize. The authors then prove that the estimation error of ConvResNeXts of functions supported on a smooth manifold deponds on their ambient dimensions. Thus, the curse of dimensionality does not occur."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is well-written, and the problem addressed is important to the community.\n- I like the fact that the authors study overparameterization of ConvResNets, although not directly."
                },
                "weaknesses": {
                    "value": "- Since Theorem 4 depends on finding the global optimizer, I wonder if this is too strong of an assumption."
                },
                "questions": {
                    "value": "See Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2580/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2580/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2580/Reviewer_CZj1"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2580/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699327731368,
            "cdate": 1699327731368,
            "tmdate": 1699636195188,
            "mdate": 1699636195188,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "L4cAKTRKmJ",
                "forum": "L3yJ54gv3H",
                "replyto": "uBwGQXC4T3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2580/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2580/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Response to Reviewer CZj1"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nThank you for taking the time to review the ICLR paper and for your positive feedback. I'm grateful for your engagement and would like to address the question you've raised with the following response.\n\n**Weakness: Since Theorem 4 depends on finding the global optimizer, I wonder if this is too strong of an assumption.**\n\nWe concede that some existing work that studies the effect of overparameterization does study the optimization dynamics and prove statistical learning bounds for the solution that optimization algorithms converge to, e.g., those in the NTK regime.  We love these works as they actively unroll! On the other hand, none of these techniques thus far are applicable to our problem. Our Appendix A illustrates why it is provably impossible for NTK to be optimal for estimating functions in Besov classes. So in short, we gained something quite nice by giving up on end-to-end analyzing the optimization algorithm and analyzing the regularized ERM. This approach to decouple learning and optimization has its origin in the pioneering work of Vapnik, Bartlett, and many other learning theorists. We essentially made the same choice for the interest of getting a more fine-grained (in our opinion, the correct way to approach overparameterization) learning theory. We hope the reviewer could see the complementary nature of our work and how it pushes along a different direction in the shared goal of understanding deep learning.\nFinally, we have added empirical studies to illustrate that while finding global optimal solutions may not be tractable, our theory does match the outcome of training ConvResNeXt using SGD  (see Appendix B) in mildly overparameterized regime with representation learning --- something that the NTK theory falls short on.\n \nThanks again for your appreciation, and I trust that this response adequately addresses your query. Should you have any further feedback or questions, please don't hesitate to share them. Your perspective is invaluable to us."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707744420,
                "cdate": 1700707744420,
                "tmdate": 1700707744420,
                "mdate": 1700707744420,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KPeqHMvuzc",
            "forum": "L3yJ54gv3H",
            "replyto": "L3yJ54gv3H",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2580/Reviewer_QcEh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2580/Reviewer_QcEh"
            ],
            "content": {
                "summary": {
                    "value": "This paper develops the approximation and estimation theory of Convolutional Residual Neural Networks. This paper shows that ConvResNeXt networks can well adapt to a smooth function on a low-dimensional manifold both in approximation and generalization. The authors provide theoretical justification for the overparameterization of such networks as well as training such networks with weight decay."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The authors perform strong, rigorous analysis and develop interesting theoretical guarantees for ConvResNeXt, a well-known network structure that has enjoyed remarkable performance on real applications. This paper is a great contribution to the understanding of such network structure. \n- The ideas of proof are novel and interesting to me."
                },
                "weaknesses": {
                    "value": "- The paper does not provide a direct comparison with prior research on different NN architectures and function spaces. It's not clear to me how this work different from prior ones, and what are the theoretical advantages of your network structure. \n- The paper is restricted to binary classification with empirical logistic loss. Are your findings extendable to other losses? \n- The paper is not written in clear language. Some sentences are unfinished, for example the 4th line on page 7. \n- No numerical experiments are conducted to support the theoretical findings."
                },
                "questions": {
                    "value": "- For remark 1, it's not clear to me why there is only a small number of non-trivial blocks. It is also unclear to me why weight decay plays a crucial role here. \n- Why is there no curse of dimensionality in your problem setting? Is this because of the ConvResNeXt structure, the Besov function space or other assumptions? \n- The authors claim that ConvResNeXts can 'efficiently learn the function without suffering from the curse of dimensionality'. What does 'efficiency' here mean, sample efficiency or computational efficiency? \n- The authors claim that ConvResNexts learn Besov functions with a better convergence rate close to the minimax rate, which is significantly faster than NTK methods. What is the intuition behind this discovery?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concerns."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2580/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2580/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2580/Reviewer_QcEh"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2580/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699425201188,
            "cdate": 1699425201188,
            "tmdate": 1699636195117,
            "mdate": 1699636195117,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KBQ2ByHoLJ",
                "forum": "L3yJ54gv3H",
                "replyto": "KPeqHMvuzc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2580/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2580/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Response to Reviewer QcEh (Part 1)"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nThank you for your valuable evaluation of our manuscript. We provide responses to your questions and comments in the following.\n\n\n**Weakness 1: The paper does not provide a direct comparison with prior research on different NN architectures and function spaces. It's not clear to me how this work differs from prior ones, and what are the theoretical advantages of your network structure.**\n \nNote that we have already compared our results with related works in Section 1. Here we summarize the difference again as follows:\n\n(1) **Neural network architecture**: In contrast to prior works, exemplified by Zhang and Wang (2022), which predominantly focus on parallel feedforward neural networks, our investigation centers on the intricacies of stacked ConvResNeXts. This architectural choice introduces a significantly more complex nested function form, presenting us with the challenge of addressing novel issues in bounding the metric entropy --- this is one of our major contributions. Specifically, we adopted a more advanced method that leverages the Dudley's chaining of the metric entropy (via critical radius / local Gaussian/Rademacher complexity, Bartlett et al., 2005), which resulted in a tighter bound even for the much simpler setting of Zhang and Wang (2022).\n\nReference:\n\nP. L. Bartlett, O. Bousquet, and S. Mendelson. Local Rademacher complexities. Annals of Statistics, 33(4):1497\u20131537, 2005.\n\n\n(2) **Overparameterization regime**: Our network architecture comprises $N$ residual blocks, each containing $M$ paths. Consequently, the network can be overparameterized by manipulating either a large $M$, a large $N$, or both. This stands in stark contrast to the constraints in Zhang and Wang (2022), where $M$ must be large and $N=1$, and in Liu et al. (2021), where $M=1$ and $N$ must be properly upper bounded. Even though our network is highly overparameterized, we prove that it can efficiently learn the ground-truth function when trained with weight decay.  However, classical statistical theories cannot explain this phenomenon due to the huge variance of the neural network class.\n\n(3) **Manifold setting**: Compared to Zhang and Wang (2022), our paper considers the case when the ground-truth Besov function is supported on a low-dimensional manifold and we develop a close-to-optimal convergence rate which only depends on the intrinsic dimension of the manifold, thus circumventing the curse of the ambient dimensionality. \n\n\nWork | Neural Architecture | Overparameterized | Low dimensional manifolds | Function Class |Remark\n|---|---|---|-------|---|---| \nZhang and Wang (2022) | Parallel FNNs | Y | N | Besov | Not used in practice\nLiu et al. (2021) | ConvResNets | N | Y | Besov | Difficult to train without overparameterization\nChen et al. (2019)| FNNs | N | Y | Holder | Difficult to train due to the cardinality constraints, not used in practice\nThis work | ConvResNeXts | Y | Y | Besov | Include ConvResNets as a special case\n\nThe **advantages** of ConvResNeXts include that it provides a flexible way to design overparameterized networks where either the number of residual blocks or the number of paths in each block can be large. In addition, the skip-layer connections in ConvResNeXts effectively mitigates the vanishing or exploding gradient issues. Notably, the practical training of ConvResNeXts proves to be more straightforward compared to simpler structures such as FNNs, resulting in outstanding performance across various real-world applications, notably in the domain of image recognition.\n\n**Weakness 2:  The paper is restricted to binary classification with empirical logistic loss. Are your findings extendable to other losses?** \n\nOur findings can be generalized to encompass any loss function that exhibits Lipschitz continuity concerning the model's output, including but not limited to the cross-entropy loss. While our paper predominantly relies on the logistic loss for the sake of clarity and simplicity, the applicability of our results extends to a broader range of loss functions.\n\n**Weakness 3: Typo.**\n\nThanks for pointing out the typo. We have corrected it in the latest updated version.\n\n**Weakness 4: No numerical experiments are conducted to support the theoretical findings.**\n\nWe conducted a numerical simulation where we learn a regression function that is supported on the low-dimensional space, and show that ConvResNeXts outperforms other methods with a relatively small model complexity. The results of ConvResNeXts are almost dimension-independent due to the representation learning that helps identify the low-dimensional manifold, while standard non-parametric methods, such as kernel ridge regression and Gaussian processes deteriorate quickly as the ambient dimension gets bigger. We have added the simulation to Appendix B in the updated paper."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2580/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707394189,
                "cdate": 1700707394189,
                "tmdate": 1700707414211,
                "mdate": 1700707414211,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]