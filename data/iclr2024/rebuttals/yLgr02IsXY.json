[
    {
        "title": "AMPipe: Accelerating MoE Model Training with Intra-Block Pipelining"
    },
    {
        "review": {
            "id": "YT4WcTXWh2",
            "forum": "yLgr02IsXY",
            "replyto": "yLgr02IsXY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3434/Reviewer_4gRQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3434/Reviewer_4gRQ"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose AMPIPE, an optimization technique that improves the training speed of MoE models with pipeline parallelism. AMPIPE splits the attention computation and MLP computation into smaller chunks and schedules them to overlap with the all2all communication, reducing the cricial path in comparison to training an MoE model with pipeline parallelism enabled. The authors evaluate AMPIPE on several GPT-MoE models and show that it can achieve higher throughput than an existing MoE system Tutle without affecting convergence."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper applies chunking to both Attention and MLP layers in a Transformer block to create a better pipeline scheme that exploits the overlapping potential between attention, MLP, and all2all communication in MoE models.\n- The paper demonstrates the effectiveness of AMPIPE on low-performance interconnects."
                },
                "weaknesses": {
                    "value": "1. The technical novelty of the work is limited. Particularly, the idea of breaking GeMM computation into smaller chunks and overlapping those chunks with communication collectives has been previously studied, such as in Tutle. The main contribution of the work is to extend that to the Attention layer in an MoE model.\n\n2. There is a big misconnection between the motivation of the work and the evaluation. Pipeline parallelism for MoE models is usually used for massive MoEs (e.g. hundreds of billions or even trillion-scale models) when the model cannot fit on aggregated GPUs with data parallelism, expert parallelism, and tensor parallelism. However, the tested MoE models are relatively small, with the largest one having only 12 layers and a hidden dimension 1600. This raises questions in terms of whether pipeline parallelism is even needed for this scale of models, i.e., the authors might have compared with a very sub-optimal configurations that are unnecessarily slow or have compared with configs that do not need pipeline parallelism. Indeed, from Figure 8, we see that as the model size increases, the gap between AMPIPE and Tutle decreases. To be more convincing, the paper should either test larger-scale models where pipeline parallelism is actually needed or use stronger baselines that sweep over different parallelism combinations. \n\n3. The paper lacks many important implementation details. In particular, while the projection of Q, K, V can be chunked along the sequence dimension, the softmax operation in the attention calculation requires all tokens in a sentence to calculate the normalized attention score. Therefore, it does not seem to be correct to claim that chunking Q does not affect the output of attention calculation unless additional synchronization is added. Also, the paper overlooks the normalization operations in its analysis, which are crucial for Transformer effectiveness and efficiency (as they may introduce additional synchronization that affects the pipeline's schedule. Unfortunately, the paper has been very vague on how these important operators are handled, e.g., in terms of how those operations are handled, the paper simply says \"these operations are also amendable to chunking\", but how and how would they affect the pipeline efficiency? \n\n4. The experiment setup is sub-optimal. The model is evaluated on a single node with 4xA10G and 8xA800, but uses a pipeline depth of 4, which seems to be unnecessarily deep. The paper does not justify this choice of parallelism config and why this is a reasonable baseline. For the given model and hardware, is the parallelism config really a good choice in practice? It is not very difficult to improve a weak baseline, and not choose the best-performing baseline for the given model and hardware undermines the attractiveness of the proposed method."
                },
                "questions": {
                    "value": "1. If DP + EP + TP already produces a configuration that provides much faster training speed why bother using pipeline parallelism?\n\n2. Do the results in Figure 8 include normalization operations, such as softmax, layer norm, etc.\n\n3. The paper simulates low data communication speed by adding artificial delays to all-to-all communication. How would the proposed technique perform on real commodity GPU clusters?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3434/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698413267541,
            "cdate": 1698413267541,
            "tmdate": 1699636295558,
            "mdate": 1699636295558,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "krnEPYsy5a",
                "forum": "yLgr02IsXY",
                "replyto": "YT4WcTXWh2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3434/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3434/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your insightful and constructive feedback, which has helped us improve our paper!\n\n**Weaknesses 1**: The technical novelty of the work is limited. Particularly, the idea of breaking GeMM computation into smaller chunks and overlapping those chunks with communication collectives has been previously studied, such as in Tutle. The main contribution of the work is to extend that to the Attention layer in an MoE model.\n\n**Response**: We appreciate your insights regarding the technical novelty of our work. Indeed, while the MoE communication and computation pipeline have been well explored in numerous previous studies (e.g., Tutel[1], FasterMoE[2], Lina[3], Mpipemoe[4]), our work uniquely tackles the MoE-attention bottleneck within the entire transformer block. This perspective, we argue, transcends mere incremental advancement, offering novel contributions to the field of Mlsys research.\n\n**Weaknesses 3 & Question 2**: The paper lacks many important implementation details...\n\nDo the results in Figure 8 include normalization operations, such as softmax, layer norm, etc.\n\n**Response**: Your question regarding the implementation details is important. Indeed, the results presented in Figure 8 encompass all elements within the transformer blocks, including both softmax and layernorm operations.\n**Normalization operations will not affect the integrity** of AMPipe. AMPipe's algorithm should be exactly the same as the original PyTorch implementation. We assure you of the correctness of our algorithm and will provide more comprehensive details in our revised manuscript. Regarding the concerns raised:\n\n(1) **Normalization Outside Softmax**: In modern LLM architectures like [GPT-2](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L367C24-L367C33), [OPT](https://github.com/huggingface/transformers/blob/main/src/transformers/models/opt/modeling_opt.py#L260C40-L260C49), [LLaMA](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L635C32-L635C44), and [Falcon](https://github.com/huggingface/transformers/blob/main/src/transformers/models/falcon/modeling_falcon.py#L732C28-L732C37), normalization operations (e.g., RMSNorm[5] and LayerNorm[6]) are typically applied **token-wise**. Consequently, when we chunk the sequence tokens into shards while retaining the integrity of each single token, the output remains unaffected.\n\n(2) **Normalization Inside Softmax**: These operations are conducted on a **query-wise** basis, meaning that each query token undergoes normalization across the entire kv length. Thus, dividing the whole query sequence into chunks also does not affect the output as we maintain the integrity of each query token's computation.\n\nThus, both the normalization inside and outside softmax will not introduce additional synchronization that affects the pipeline's schedule.\n\nAdditionally, the implementation of FlashAttention-v2[7], which includes two loops \u2014 the outer loop iterating over the query length and the inner loop handling softmax normalization across the kv length \u2014 demonstrates that chunking the outer loop does not impact the results. This method maintains the integrity of the output compared to the original attention implementation, providing further evidence for point (2).\n\nFurthermore, the Blockwise Parallel Transformer's[8] approach of splitting sequence along the query length also substantiates that such an operation does not compromise the model output, even with normalization operations. This approach offers validation for both points (1) and (2)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3434/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491724537,
                "cdate": 1700491724537,
                "tmdate": 1700491724537,
                "mdate": 1700491724537,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3y01oiO2e0",
            "forum": "yLgr02IsXY",
            "replyto": "yLgr02IsXY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3434/Reviewer_wCG9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3434/Reviewer_wCG9"
            ],
            "content": {
                "summary": {
                    "value": "This work proposes AMPipe, a method to accelerate Mixture-of-Experts (MoEs) training when the sequence length is long. The basic idea is to split the sequence dimension into blocks so that the computation of one block can be overlapped with the communication of the next block."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Training large language models for long sequence length is a timely and important topic.\n- The design of computation-communication overlap is sound."
                },
                "weaknesses": {
                    "value": "For me, this paper reads more of a technical report or technical blog introducing a nicely designed engineering effort. The major contribution is a chunking method along the sequence dimension to overlap the computation of FlashAttention and the All-to-All communication of MoEs for two consecutive chunks, which has limited novelty and takes barely one page (i.e., Section 3) to illustrate. I am afraid it may fail to provide significant insights to the community."
                },
                "questions": {
                    "value": "(1) To support training on extremely long sequence length, memory is also important to consider. I would like to know whether AMPipe saves memory.\n\n(2) Sequence parallel and tensor parallel are also important techniques to support lengthy sequences by amortizing the memory onto different GPUs. It would be interesting to see how to integrate AMPipe with them.\n\n(3) Table 1 does not report the number of layers, the number of experts, and the size (number of parameters) of each model.\n\n(4) According to the specification of A800, the NVLink bandwidth should be 400GB/s. It should be elaborated how the data communication speed is limited to 20GB/s in Section 4.3 (e.g., by transmitting 20x of data volume?) Moreover, although the intention to simulate commodity communication bandwidth is nice to have, I am afraid limiting the NVLink bandwidth to 1/20 of the origin (from 400GB/s to 20GB/s) is a good choice since the computing power (i.e., flops) of A800 is quite performant. \n\n(5) If I understand well, AMPipe requires token-level communication in MoE layers, so I would like to ask whether it can be applied to sequence- or task-level MoEs?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3434/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698573704251,
            "cdate": 1698573704251,
            "tmdate": 1699636295484,
            "mdate": 1699636295484,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kFLLwK54WC",
                "forum": "yLgr02IsXY",
                "replyto": "3y01oiO2e0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3434/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3434/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your thoughtful feedback on our work. We appreciate the opportunity to clarify aspects of our research and methodology.\n\n**Question 1**: To support training on extremely long sequence length, memory is also important to consider. I would like to know whether AMPipe saves memory.\n\n**Response**: Your question regarding memory efficiency is pertinent. Building on the FlashAttention [1][2], AMPipe achieves linear memory usage with increasing sequence length. In our empirical evaluations, we observed no additional memory usage compared to current solutions, thus efficiently supporting training on extremely long sequences.\n\n**Question 2**: Sequence parallel and tensor parallel are also important techniques to support lengthy sequences by amortizing the memory onto different GPUs. It would be interesting to see how to integrate AMPipe with them.\n\n**Response**: AMPipe is theoretically compatible with data, tensor, and sequence parallelism[3]. We have already integrated it with tensor parallelism in our latest version, and details of this integration will be included in the appendix of a forthcoming revision. Regarding sequence parallelism, its integration is non-trivial. A more dedicated design is necessary to effectively combine current sequence parallelism computation and communication with the MoE computation and communication, aiming to achieve enhanced performance.\n\n**Question 3**: Table 1 does not report the number of layers, the number of experts, and the size (number of parameters) of each model.\n\n**Response**: Regarding the suggestion to add the number of layers and parameters in Table 1, we would like to clarify that our experiments in Sections 4.1 and 4.2 focus on speedup comparison within a transformer block instead of the whole model, emphasizing head and dimension settings. In this experiment, we use data and expert parallelism, and each GPU has one expert (varying expert numbers will only change local MLP, which is not a bottleneck in our evaluations). The base, medium, large, and xl transformer blocks have 7M, 13M, 20M, and 31M parameters, respectively. Additionally, we conducted an end-to-end experiment, as detailed in Section 4.3 and Table 2, using a cluster with eight A800 GPUs. We believe that our setting can reflect its real-world training potential to some extent.\n\n**Question 4**: According to the specification of A800, the NVLink bandwidth should be 400GB/s. It should be elaborated how the data communication speed is limited to 20GB/s in Section 4.3 (e.g., by transmitting 20x of data volume?) Moreover, although the intention to simulate commodity communication bandwidth is nice to have, I am afraid limiting the NVLink bandwidth to 1/20 of the origin (from 400GB/s to 20GB/s) is a good choice since the computing power (i.e., flops) of A800 is quite performant.\n\n**Response**: We appreciate your input on our bandwidth emulation method. We emulated lower bandwidth by transmitting 20x the data volume. While Tutel[4] tested their speedup on high GPU-inter connections (up to 1,600Gbps), it's common to train models on powerful GPUs like the A100 with lower network bandwidths (e.g., AWS's A100 server has 50GB/s[5] and Google Cloud's A100 server has 12.5GB/s[6]). Our aim was to simulate a realistic training environment under our limited budget, recognizing that it may not perfectly match all real-world deployments. However, the trend observed should be reflective of most scenarios.\n\n**Question 5**: If I understand well, AMPipe requires token-level communication in MoE layers, so I would like to ask whether it can be applied to sequence- or task-level MoEs?\n\n**Response**: AMPipe is adaptable to both sequence-level and task-level MoEs. Given that AMPipe effectively serves the finer-grained token-level communication, it can also be applied to these larger-grained scenarios. While it is particularly well-suited to token-level communication in MoEs, which involves shuffling tokens across different devices, it can also accommodate larger-grained communication in sequence or task-level MoEs, where tokens from the entire sequences or tasks are sent to the same experts/devices. Many LLM-based MoE models and systems (e.g., Tutel[4], FasterMoE[7], Lina[8]) utilize token-level communication, and our empirical experiments demonstrate AMPipe's effectiveness in this token-level scope."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3434/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490947209,
                "cdate": 1700490947209,
                "tmdate": 1700490947209,
                "mdate": 1700490947209,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BNm7LSAje4",
            "forum": "yLgr02IsXY",
            "replyto": "yLgr02IsXY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3434/Reviewer_aNzs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3434/Reviewer_aNzs"
            ],
            "content": {
                "summary": {
                    "value": "Using the observation that the transformer attention layer can be chunked, AMPipe notes that the chunked computation can be pipelined with other work being done in a transformer network. Namely, the A2A data movement of MoE networks is can be dispached in chunks and overlapped with the chunked attention computation. This is similar to and builds on the Intra-MoE pipelining done by the tutel paper."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The work shows how pipelining the all2all MoE communication with attention computation can improve throughput by 1.4x over training with no pipelining (where the pipelining in tutel shows improvement of 1.3x)."
                },
                "weaknesses": {
                    "value": "- When taking into account the tutel baseline, AMPipe provides a 7% improvement at best? (1.4x vs 1.3x speedup). This is effectively low impact.\n- The work does not show how this scales to large gpu clusters (where the majority of MoE training happens given MoE networks are relatively large).\n- while novel, the novelty is limited to combining tutel based pipelineing with pipelining based on chunked attention computation (an already existing concept)\n- This is generally more useful for long context length settings. But the majority of pretraining happens at seq len = 2k or 4k and long context length training only happens for a small subset of the total training time. This meta-point lowers the overall impact of works targeting improvements at really long context lengths."
                },
                "questions": {
                    "value": "- Can a variant of figure 8 be creates which combines the fwd and bwd pass? this allows the reader to see the total end to end speedup at different degrees instead of just seeing it in table 2 in tabular form."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3434/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698731722048,
            "cdate": 1698731722048,
            "tmdate": 1699636295408,
            "mdate": 1699636295408,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KoYWlV3O3g",
                "forum": "yLgr02IsXY",
                "replyto": "BNm7LSAje4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3434/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3434/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your insightful and constructive feedback, which has been instrumental in refining our paper.\n\n**Weakness 1**. When taking into account the tutel baseline, AMPipe provides a 7% improvement at best? (1.4x vs 1.3x speedup). This is effectively low impact.\n\n**Response**: Your observation about AMPipe's 7% improvement over the Tutel baseline is appreciated. However, it's important to note that the speedups in Figure 7 vary with different model/sequence settings. The maximum speedup compared to Tutel is indeed higher, as illustrated by the 1.4x vs 1.23x difference. Furthermore, in the context of training large LLMs on thousands of GPUs, which require months of training time and millions of dollars[1], even a 10% speedup can translate into substantial financial benefits, underscoring the practical significance of the AMPipe.\n\n**Weakness 2**. The work does not show how this scales to large gpu clusters (where the majority of MoE training happens given MoE networks are relatively large).\n\n**Response**: We appreciate your insight regarding scalability. We acknowledge that our evaluation, conducted on a smaller cluster, may not fully reflect the cases of training LLMs on real-world, large GPU clusters. We recognize that the communication bandwidth in larger clusters differs significantly from that in smaller setups. However, our present approach, encompassing diverse models, sequence settings, and low-bandwidth emulation, aims to replicate a range of real-world scenarios. We believe that performance in large GPU cluster environments will align with our experimental results to some extent, although perfect emulation may not be feasible.\n\n**Weakness 3**: while novel, the novelty is limited to combining tutel based pipelineing with pipelining based on chunked attention computation (an already existing concept)\n\n**Response**: Thank you for pointing out our work's conceptual scope. While there are numerous studies on MoE communication and computation pipeline with various methods(e.g., Tutel[2], FasterMoE[3], Lina[4], Mpipemoe[5]), our work distinctively addresses the MoE-attention bottleneck from a **whole transformer block perspective**. Our approach, we believe, is more than incremental work, offering valuable insights for future Mlsys research.\n\n**Weakness 4**: This is generally more useful for long context length settings. But the majority of pretraining happens at seq len = 2k or 4k and long context length training only happens for a small subset of the total training time. This meta-point lowers the overall impact of works targeting improvements at really long context lengths.\n\n**Response**: We acknowledge the prevalent trend of pretraining at shorter sequence lengths, a focus of prior MoE system works (e.g., Tutel, FasterMoE, Lina, Mpipemoe). However, AMPipe shows benefits in training with longer sequences, aligning with the emerging trend of longer sequences in LLM designs. We need to admit that addressing long context lengths is obtaining the importance, even though current methods often allocate a moderate portion of training time to them.\n\n**Question**: Can a variant of figure 8 be creates which combines the fwd and bwd pass? this allows the reader to see the total end to end speedup at different degrees instead of just seeing it in table 2 in tabular form.\n\n**Response**: We appreciate your suggestion to revise Figure 8 to combine forward and backward passes. We will incorporate this enhancement in our revised version. However, as discussed in the paper, Figure 8 is a variant of Figure 7 across 56 benchmarks. Pipeline degree is a configuration for AMPipe, and the case with the highest speedup is more important (i.e., pipeline degree=4 case in Figure 7). \n\n\n[1] https://blogs.nvidia.com/blog/what-are-large-language-models-used-for/\n\n[2] Changho Hwang, Wei Cui, Yifan Xiong, Ziyue Yang, Ze Liu, Han Hu, Zilong Wang, Rafael Salas,\nJithin Jose, Prabhat Ram, et al. Tutel: Adaptive mixture-of-experts at scale. Proceedings of\nMachine Learning and Systems, 5, 2023.\n\n[3] Jiaao He, Jidong Zhai, Tiago Antunes, Haojie Wang, Fuwen Luo, Shangfeng Shi, and Qin Li. Fastermoe: modeling and optimizing training of large-scale dynamic pre-trained models. In Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming,\npp. 120\u2013134, 2022.\n\n[4] Jiamin Li, Yimin Jiang, Yibo Zhu, Cong Wang, and Hong Xu. Accelerating distributed {MoE}\ntraining and inference with lina. In 2023 USENIX Annual Technical Conference (USENIX ATC\n23), pp. 945\u2013959, 2023b.\n\n[5] Zheng Zhang, Donglin Yang, Yaqi Xia, Liang Ding, Dacheng Tao, Xiaobo Zhou, and Dazhao\nCheng. Mpipemoe: Memory efficient moe for pre-trained models with adaptive pipeline parallelism. In 2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS),\npp. 167\u2013177. IEEE, 2023."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3434/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490514303,
                "cdate": 1700490514303,
                "tmdate": 1700490514303,
                "mdate": 1700490514303,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EhTe2FG1TE",
            "forum": "yLgr02IsXY",
            "replyto": "yLgr02IsXY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3434/Reviewer_Ai23"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3434/Reviewer_Ai23"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a method to speedup MoE inference and training by moving the pipeline parallelism to begin earlier in the layer's execution.  Specifically, the attention operation is split over queries along the sequence dimension into chunks which can be ran separately, before the same pipelining is applied to the feed forward networks in MoE layer.  This introduces a significant increase in throughput of 10-40% across many different configurations.  Code is provided where they add their AMPipe to the MegatronLM code."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* MoE is becoming very relevant for the current scaling of models as dense models are impractical at the trillion parameter scale, which makes this work relevant.\n* Code is provided that is an extension of the MegatronLM code.  I skimmed their AMPipe implementation and it seems fairly simple which makes it easier to integrate into existing systems.\n* They test on a variety of model scales and context lengths"
                },
                "weaknesses": {
                    "value": "* This work is conceptually very incremental, as they are simply moving the parallelism earlier in the layer's execution.  I am not familiar enough with the related work to evaluate how significant this work is in relation to others.\n\nNevertheless, I am voting to accept this paper as the quantitative speedups are significant and the method appears effective and simple enough to implement, which is very important in practice."
                },
                "questions": {
                    "value": "**Setup** \"As a baseline, we constructed MoE models based on GPT-3\u201d\n* GPT-3 is not an open source model, I believe you mean GPT-2, considering the description of the model in Table 1\n\n**Table 1** I think it would be useful to the reader to add the # of layers and # of parameters to compare the different models\n\n\u201cWith an implementation consisting of more than 1k lines of code (LoCs)\u201d\n* The fact that you provided code is great, but lines of code is not an informative metric.  The optimum number of lines of code is the minimum necessary to have performant and readable code.\n\nIt is worth explicitly defining what \"pipe degree\" is rather than keeping it implicit.  Perhaps say that pipe degree = n and chunk length = N / n, etc.\n\nTypos: \n\u201cunofficially reported to use MoE paradigm.\u201d - - - > \u201cunofficially reported to use the MoE paradigm.\u201d\nFor MoE, sharding the input batch along seqeunce dimension\u201d - - > \u201cFor MoE, sharding the input batch along sequence dimension"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3434/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698812962661,
            "cdate": 1698812962661,
            "tmdate": 1699636295287,
            "mdate": 1699636295287,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "LfVwOOw9Qh",
                "forum": "yLgr02IsXY",
                "replyto": "EhTe2FG1TE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3434/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3434/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your insightful and constructive feedback, which has helped us improve our paper.\n\n**Question**: Setup \"As a baseline, we constructed MoE models based on GPT-3\"\n\n**Response**: Thanks for pointing out our mistake. Our baseline is constructed on the open-sourced GPT-2, not GPT-3. This will be duly corrected in our revised manuscript.\n\n**Question**: Table 1 I think it would be useful to the reader to add the # of layers and # of parameters to compare the different models\n\n**Response**: Regarding the suggestion to add the number of layers and parameters in Table 1, we would like to clarify that our experiments in Sections 4.1 and 4.2 focus on speedup comparison within a transformer block instead of the whole model, emphasizing head and dimension settings. In this experiment, we use data and expert parallelism, and each GPU has one expert (varying expert numbers will only change local MLP time, which is not a bottleneck in our evaluations). The base, medium, large, and xl transformer blocks have 7M, 13M, 20M, and 31M parameters, respectively. Additionally, we conducted an end-to-end experiment, as detailed in Section 4.3 and Table 2, using a cluster with eight A800 GPUs. \n\n**Question**: \"With an implementation consisting of more than 1k lines of code (LoCs)\" The fact that you provided code is great, but lines of code is not an informative metric. The optimum number of lines of code is the minimum necessary to have performant and readable code.\n\n**Response**: Thanks for your constructive advice. We agree with your point on the metric of lines of code. Our revision will better reflect the emphasis on the performance and readability of the code rather than the quantity of code.\n\n**Question**: It is worth explicitly defining what \"pipe degree\" is rather than keeping it implicit. Perhaps say that pipe degree = n and chunk length = N / n, etc.\n\n**Response**: Thank you for highlighting the need to define \"pipe degree\" more explicitly. We confirm that your understanding is correct: with a pipe degree of n and an original sequence length of N, each sequence chunk will have a length of N/n. This definition will be included in the revised manuscript for clarity.\n\n**Question**: Typos: \"unofficially reported to use MoE paradigm.\" - - - > \"unofficially reported to use the MoE paradigm.\" For MoE, sharding the input batch along seqeunce dimension\" - - > \"For MoE, sharding the input batch along sequence dimension\n\n**Response**: Thanks for your diligence in identifying typographical errors. These will be corrected to ensure clarity and professionalism in our presentation."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3434/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700490055214,
                "cdate": 1700490055214,
                "tmdate": 1700490055214,
                "mdate": 1700490055214,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "F8XtMG0Jy6",
                "forum": "yLgr02IsXY",
                "replyto": "LfVwOOw9Qh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3434/Reviewer_Ai23"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3434/Reviewer_Ai23"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors 1"
                    },
                    "comment": {
                        "value": "I thank the authors for their response.  I maintain my score and recommendation."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3434/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700610159061,
                "cdate": 1700610159061,
                "tmdate": 1700610159061,
                "mdate": 1700610159061,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]