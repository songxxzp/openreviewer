[
    {
        "title": "MaGIC: Multi-modality Guided Image Completion"
    },
    {
        "review": {
            "id": "GGg7CqEjtS",
            "forum": "o7x0XVlCpX",
            "replyto": "o7x0XVlCpX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4753/Reviewer_VxR9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4753/Reviewer_VxR9"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a multi-modal approach for image completion with LARGE missing regions. The different modalities, such as depth, edge, sketch, pose, provide complementary information for plausible completion. The approach does not require training."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Dealing with LARGE missing regions is a critical task in image completion. This topic is of broad interest in the ML and image processing community.\n2. The idea of leveraging multiple resources is nice though not ground-breaking novel. Making is scalable and flexible is the key, which is solved by two stage approach: modality oriented conditional network and across-modality blending.\n3. The approach is integrated into the diffusion process neatly and training-free.\n3. The paper is very well written and easy to follow, with good illustrations. \n4. The results are convincing with well-planned experiments, which also demonstrate good image generation results beyond completion"
                },
                "weaknesses": {
                    "value": "1. I'm not fully convinced that different image channels/features, such as depth, sketch, edge, could be called modality.\n2. The fair comparison is not easy since most SOTA are not considering multiple resources in the same time. It'd be nice to share some insight into this, and share failure cases."
                },
                "questions": {
                    "value": "As in weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4753/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698791759848,
            "cdate": 1698791759848,
            "tmdate": 1699636457727,
            "mdate": 1699636457727,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "H0vwpO1p1C",
                "forum": "o7x0XVlCpX",
                "replyto": "GGg7CqEjtS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for the time, thorough comments, and valuable suggestions. We are pleased that you acknowledged our novel idea, the very well-written paper, and our convincing experiments.\n\n> **Q1**: I'm not fully convinced that different image channels/features, such as depth, sketch, edge, could be called modality.\n\n**A1**: Sorry for any confusion caused. We fully understand that in certain fields, 'modality' usually refers to forms with greater differences, such as text, video, and audio, rather than sketch and canny edge. However, in our context, we adhere to the terminology and conventions established by preceding studies [1-3].\n\n`[1]` Toward multimodal image-to-image translation. NeurIPS. 2017.\n\n`[2]` Unbiased multi-modality guidance for image inpainting. ECCV. 2022.\n\n`[3]` Multimodal Conditional Image Synthesis with Product-of-Experts GANs. ECCV. 2022.\n\n> **Q2**: The fair comparison is not easy since most SOTA are not considering multiple resources in the same time. It'd be nice to share some insight into this, and share failure cases.\n\n**A2**: We deeply appreciate your observation regarding the difficulty in constructing a fair comparison, given the absence of baselines with similar multi-conditioning features. In response, we have included an analysis of failure cases in Figure 8 (Appendix) in our updated version.\n\nPlease let us know if there are further questions. Thanks again!"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700378131968,
                "cdate": 1700378131968,
                "tmdate": 1700378174397,
                "mdate": 1700378174397,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ps3fiLheOR",
                "forum": "o7x0XVlCpX",
                "replyto": "GGg7CqEjtS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A friendly reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nI would like to send a kind reminder. Has our response addressed your concerns? The reviewer discussion period is nearing its end, and we eagerly await your reply. Your suggestions and comments are invaluable to the community. Thank you!\n\nBest, The authors"
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700628933151,
                "cdate": 1700628933151,
                "tmdate": 1700628933151,
                "mdate": 1700628933151,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "vLkE1vQGhl",
            "forum": "o7x0XVlCpX",
            "replyto": "o7x0XVlCpX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4753/Reviewer_jMgg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4753/Reviewer_jMgg"
            ],
            "content": {
                "summary": {
                    "value": "MaGIC or Multi-modality Guided Image Completion can merge text, canny edge, sketch, segmentation, depth, pose, or any arbitrary combination as guidance for image completion. The authors aim to design a framework that is scalable and flexible. MaGIC has two core components -- a modality-specific U-Net (MCU-Net), and a consistency modality blending (CMB). The MCU-Net will be individually fine-tuned under each single modality, in the first stage. Then, to achieve multi-modality guidance, the CMB algorithm flexibly aggregates guidance signals from any combination of previously learned MCU-Nets. The MCU-Net is similar to T2I-Adapter, composed of a standard U-Net denoiser from the pre-trained Stable Diffusion (SD) and an encoding network which injects a single modality guidance into the U-Net to attain single-modality guidance. The CMB leverages guidance loss to gradually narrow the distances between intermediate features from SD pre-trained U-Net and multiple MCU-Nets during denoising sample stage. This ensures that the SD U-Net features do not deviate too much from the original feature distribution during multi-modality guidance. CMB is training-free and allows for the flexible addition or removal of guidance modalities, avoiding cumbersome re-training and preserving the feature distribution of the original SD U-Net denoiser.\n\nTo verify MaGIC, the authors conduct experiments on image inpainting, outpainting, and editing using the COCO, Places2, and in-the-wild data."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "I like the extension of classifier guidance to multiple modalities that too training-free. Similar techniques has been explored in other single-modality context like in Sketch-Guided Text-to-Image Diffusion Models, but extending to multi-modal case is a nice extension.\n\nThe qualitative comparisons are very intuitive (especially with T2I-Adapter and ControlNet). The overall presentation is reasonable and easy to follow.\n\nThe authors included substantial appendix sections, detailing several architectural details like the design of MCU-Net."
                },
                "weaknesses": {
                    "value": "While this is an interesting piece of work, I have some big gripes (please let me know if I understood it wrong):\n\nIn related work section (page 3 last paragraph), the authors give the impression that T2I-Adapter (and ControlNet) \"fails to simultaneously use multi-modality as guidance\". This is completely wrong. I understand that T2I-Adapter do not explicitly train jointly for multiple modalities, but they can combine multiple modalities (see section 4.3.2 in T2I-Adapter).\n\nSecond, the authors need to provide a solid justification why a simple \"feature-level addition\" mentioned in Page-6 paragraph-2 is not good. T2I-Adapter (in broad terms) does exactly that. I would need more detailed experiment and intuition comparing both T2I-Adapter and ControlNet for multi-modality case.\n\nThe argument given by authors \"denoiser is trained solely on the distribution of $$\\hat{F}_{c} = F_{enc} + F_{c}$$ \" makes even less sense when you consider ControlNet with its zero-convolutions. In ControlNet we get plausible generation from the starting iterations, thanks to zero-convolutions and with training the conditioning branch becomes good. Hence, the distribution mismatch should not really be an issue to begin with.\n\nApart from my major concerns above, there are some minor corrections/concerns:\n1. I think Eq. 4 should be $$ l (\\hat{F}_{C}, F_{*}) = \\frac{1}_{L} \\sum_{l=0}^{L} \\sum_{i=1}^{N} \\delta_{c} || \\hat{F}_{c}^{l} - F_{*}^{l} ||_{2}^{2} $$ (Note: subscript is C and not c?)\n\n2. Typos e.g., 2nd last paragraph just after equation 2 \"Denoising\"\n\n3. The MCU-Net is basically T2I-Adapter. I do not see any reason to have a new name for it (only to rebrand something and create more confusion). On the other hand, given MCU-Net is same as T2I-Adapter, the only merit of this paper is CMB."
                },
                "questions": {
                    "value": "Since the only contribution of this paper is CMB, I would suggest to have a very detailed comparison with respect to T2I-Adapter, ControlNet, and many more (for multiple modalities).\n\nApart from just a few qualitative results and some incremental metrics improvement, why do you think Converse Amplification (or simply a variant of classifier guidance) a better approach than zero-convolutions with ControlNet?\n\nAlso, can you add some failure cases of CMB? This is important to give a better idea of where ControlNet lacks and where CMB lacks (I understand CMB can be coupled with ControlNet or T2I-Adapter)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4753/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4753/Reviewer_jMgg",
                        "ICLR.cc/2024/Conference/Submission4753/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4753/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698900493722,
            "cdate": 1698900493722,
            "tmdate": 1700739686197,
            "mdate": 1700739686197,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8zZuot4oQv",
                "forum": "o7x0XVlCpX",
                "replyto": "vLkE1vQGhl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (part 1/2)"
                    },
                    "comment": {
                        "value": "Thank you for the time, thorough comments, and nice suggestions. We are pleased to clarify your questions step-by-step.\n\n> **Q1**: In related work section (page 3 last paragraph), the authors give the impression that T2I-Adapter (and ControlNet) \"fails to simultaneously use multi-modality as guidance\". This is completely wrong. I understand that T2I-Adapter do not explicitly train jointly for multiple modalities, but they can combine multiple modalities (see section 4.3.2 in T2I-Adapter). Second, the authors need to provide a solid justification why a simple \"feature-level addition\" mentioned in Page-6 paragraph-2 is not good. T2I-Adapter (in broad terms) does exactly that.\n\n**A1**: Thank you for your thorough review. We acknowledge that the T2I-Adapter represents an outstanding contribution to controllable image generation, exhibiting exponential performance in single-modality guidance tasks.\nHowever, our claim that it \"**fails** to simultaneously use multi-modality as guidance\" remains valid. \nThe T2I-Adapter, as outlined in its paper, implements multimodal guidance through a straightforward feature-level addition approach, while showcasing results with bimodal guidance. \nHowever, theoretically, this feature-level addition leads to a distribution mismatch with the pre-trained model, a phenomenon illustrated in Figure 7 of our updated paper. This implies that with an increased number of guiding modalities (more than two), such straightforward strategy can drastically perturb the distribution, resulting in distorted outputs. \nEmpirically, our hypothesis is corroborated in Appendix Table 4 and Figure 9 of our updated version. In Table 4, our approach shows improvements of 230.30%, 71.40%, and 37.07% in image quality metrics P-IDS, U-IDS, and FID, respectively, compared to the multimodal guidance of T2I-Adapter. \nQualitatively, Figure 9 demonstrates that while the T2I-Adapter aligns well with the shape and layout of multiple guiding modalities, it produces images lacking in realism. The loss of high-frequency details and the failure to restore content in unmasked regions are due to the simultaneous addition of features from four modalities during inference, which does not match the distribution of the T2I-Adapter's backbone U-Net.\n\n> **Q2**: I would need more detailed experiment and intuition comparing both T2I-Adapter and ControlNet for multi-modality case.\n\n**A2**: Thanks for your suggestion. As highlighted by Reviewer VxR9, the absence of multimodal guided image completion methods makes it challenging to conduct fair comparative experiments. ControlNet and T2I-adapter are specialist in various single modality image generations, and we implement a latent-level blending to enable them to perform image completion. This way ensures maximum fairness in comparing the best multimodal guided image completion methods currently available.\nWe have put the Table 4 (Appendix) for detailed quantitative experiments with T2I-Adapter and ControlNet in our new version. Additionally, in the Figure 9 (Appendix), we also provide qualitative comparison with T2I-Adapter under multi-modal conditioning. This supports our findings in Section 4.2, where T2I-adapter* and ControlNet* are observed to fall short in preserving spatial consistency."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700377924078,
                "cdate": 1700377924078,
                "tmdate": 1700377924078,
                "mdate": 1700377924078,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "hkhYH3plp7",
                "forum": "o7x0XVlCpX",
                "replyto": "vLkE1vQGhl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (part 2/2)"
                    },
                    "comment": {
                        "value": "> **Q3**: The argument given by authors \"denoiser is trained solely on the distribution of $\\hat{F_c} = F_{enc} + F_c$\" makes even less sense when you consider ControlNet with its zero-convolutions. In ControlNet we get plausible generation from the starting iterations, thanks to zero-convolutions and with training the conditioning branch becomes good. Hence, the distribution mismatch should not really be an issue to begin with.\n\n**A3**: Thanks for raising such an interesting question,  and we would like to provide a comprehensive answer addressing two crucial aspects. \n\n1. **The function of zero-convolution:** Our understanding of zero-convolution aligns with its purpose in ControlNet for stabilizing the initial training process. This technique avoids introducing \"noise\" to deep features generated by production-ready weights, as discussed in ControlNet. In both T2I-Adapter and ControlNet, the modality features are directly added to the features of the original Stable Diffusion, which is trained for conditional generation, and that is why we said the denoiser is trained on $\\hat{F_c} = F_{enc} + F_c$. Consequently, we disagree with there exists any direct correlation between the zero-convolution and training distribution.\n2. **The origin of distribution mismatch:** Our primary focus resolves around the process of combining separately trained condition modules (such as adapter in T2I-Adapter) to support multi-modality input in a training-free manner. Analogous to how modality features might act as noise during the initial training phase for a well-established Stable Diffusion, we posit that features produced by separately trained conditional modules could similarly be perceived as noise by each other. For example, suppose the trained networks supports segmentation ($\\hat{F_{seg}} = F_{enc} + F_{seg}$) and depth $\\hat{F_{dep}} = F_{enc} + F_{dep}$ modalities individually, the combined features $F_{seg}+F_{dep}$ could be noise inputs for both networks, leading to distribution mismatch. \n   We apologize for any confusion in our initial manuscript concerning this question and welcome further discussion to clarify these aspects.\n\n\n> **Q4.1**: I think Eq. 4 should be $\\ell(\\hat{F_C}, F_*) = \\frac{1}{L} \\sum_{l=0}^{L} \\sum_{i=1}^{N} \\delta_{c} || \\hat{F_c^l} - F_*^{l} ||_{2}^{2}$ (Note: subscript is C and not c?)\n\n**A4.1**: Thanks for your nice suggestion. We have revised the equation 4 as:\n\n$\\ell(\\hat{F_\\mathcal{C}}, F_*) = \\frac{1}{L} \\sum_{l=0}^L \\sum_{c\\in \\mathcal{C}} \\delta_{c} \\| \\hat{F_c^l} - F_{*}^l\\|_2^2$\n\n> **Q4.2**: Typos e.g., 2nd last paragraph just after equation 2 \"Denoising\"\n\n**A4.2**: Thanks for your detailed review. We have corrected the typo in our new version.\n\n>**Q4.3**: The MCU-Net is basically T2I-Adapter. I do not see any reason to have a new name for it (only to rebrand something and create more confusion). On the other hand, given MCU-Net is same as T2I-Adapter, the only merit of this paper is CMB.\n\n**A4.3**: Sorry for confusion. It's important to clarify that MCU-Net is distinct from the T2I-Adapter; we cannot simply use the T2I-Adapter as our backbone for single modality guided image completion. Our work involves gathering multi-modality datasets and training MCU-Net with masked images. This training enables our model to adeptly fill masked regions while ensuring harmony with the unmasked areas. This way significantly contributes to our method's superior performance over baseline models, as evidenced by our experimental results.\n\n> **Q5**: I would suggest to have a very detailed comparison with respect to T2I-Adapter, ControlNet, and many more (for multiple modalities).\n\n**A5**: Thanks. We have included both quantitative and qualitative comparisons with respect to T2I-adapter and ControlNet. Please refer to Table 4 and Figure 9 in our updated version.\n\n> **Q6**: Apart from just a few qualitative results and some incremental metrics improvement, why do you think Converse Amplification (or simply a variant of classifier guidance) a better approach than zero-convolutions with ControlNet?\n\n**A6**: Thank you. As **A3** clarified, zero-convolution differs significantly from Converse Amplification in both motivation and implementation. If there are any further questions or concerns, please feel free to discuss them.\n\n> **Q7**: Also, can you add some failure cases of CMB? This is important to give a better idea of where ControlNet lacks and where CMB lacks (I understand CMB can be coupled with ControlNet or T2I-Adapter).\n\n**A7**: Thank you for your thoughtful suggestion. We have included the failure cases in Figure 8 in our revised version.\n\nPlease let us know if there are further questions. Again, thanks!"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700378050392,
                "cdate": 1700378050392,
                "tmdate": 1700379668124,
                "mdate": 1700379668124,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ySRWpWe6lr",
                "forum": "o7x0XVlCpX",
                "replyto": "vLkE1vQGhl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A friendly reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nI would like to send a kind reminder. Has our response addressed your concerns? The reviewer discussion period is nearing its end, and we eagerly await your reply. Your suggestions and comments are invaluable to the community. Thank you!\n\nBest, The authors"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700628906748,
                "cdate": 1700628906748,
                "tmdate": 1700628906748,
                "mdate": 1700628906748,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pTA3v1J2Lb",
                "forum": "o7x0XVlCpX",
                "replyto": "vLkE1vQGhl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Update on comparing T2I-Adapter with MCU-Net"
                    },
                    "comment": {
                        "value": "Thank you again for your thoughtful review and comments. Following our discussion with Reviewer c8RF, we have further updated the qualitative experiments comparing T2I-Adapter with MCU-Net in the latest version (see Appendix A.3, Figures 10 and 11). This additional experiment supplements Figure 5, demonstrating that the T2I-adapter lacks awareness of the unmasked region, hence failing to maintain spatial consistency during image completion \u2013 a key contribution of our MCU-Net. Additionally, we'd like to gently point out that the main emphasis of our paper is centered on train-free multimodal guidance. We hope this clarification on the novelty of our work addresses your concerns and averts any potential reason for rejection. \n\nIf your issues have been satisfactorily resolved, we kindly request you to consider not rating our work negatively. We deeply value your feedback and thank you for your time and attention."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694188901,
                "cdate": 1700694188901,
                "tmdate": 1700694188901,
                "mdate": 1700694188901,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "n4ts49lX10",
                "forum": "o7x0XVlCpX",
                "replyto": "vLkE1vQGhl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Authors"
                    },
                    "comment": {
                        "value": "As the phase of author-reviewer discussions draws to a close, we are so pleased to note your recognition of our efforts and the raised scores. \n\nWe are grateful for the valuable suggestions you posed and appreciate the time and effort you devoted to the review process."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740690538,
                "cdate": 1700740690538,
                "tmdate": 1700740720684,
                "mdate": 1700740720684,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OLfDpGaEy0",
            "forum": "o7x0XVlCpX",
            "replyto": "o7x0XVlCpX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4753/Reviewer_c8RF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4753/Reviewer_c8RF"
            ],
            "content": {
                "summary": {
                    "value": "The paper MaGIC: Multi-modality Guided Image Completion introduces a novel framework for image completion that supports various guidance modalities, such as text, edge, sketch, and more. The proposed method, MaGIC, enables flexible and scalable multi-modality guidance without the need for retraining the model. The paper demonstrates consistent improvements in image quality over existing approaches."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**Innovative and Flexible Approach** The paper addresses the challenging problem of multi-modality-guided image completion. It proposes a new simple training-free procedure, allowing for various guidance modalities, such as text, edge, sketch, segmentation, depth, and pose. \n\n** Large Consistent Gains** The paper shows consistent and significant improvements over state-of-the-art approaches, particularly in image quality."
                },
                "weaknesses": {
                    "value": "**Clarity and Typos** The paper is challenging to follow and contains multiple typos, which can impede understanding. Improved clarity in the presentation and thorough proofreading would enhance the paper's quality.\n\n**Non-standard Update Scheme** The update scheme presented in equation (5) appears inhomogeneous, as it involves gradient descent with respect to $z_t$ but updates $z'_{t-1}$. This choice could be a reasonable heuristic but is not discussed or justified, which leaves questions about its validity.\n\n**Lack of Quantitative Evaluation** The paper only qualitative results without quantitative evaluation metrics when compared with recent baselines such as ControlNet and T2I-adapter. In particular, the performances with respect to ControlNet should be carefully assessed.  \n\n**Inadequate Dataset and Modality Descriptions** The datasets used and the conditioning modalities are briefly presented. A more detailed description of the datasets, along with the rationale for their selection, would be beneficial. \n\n**Missing Ablations** A more in-depth exploration of the impact of the weights $\\delta_c$ in equation (4) would offer valuable insights. The stability of these parameters is critical as their tuning could rapidly be cumbersome.\n\n**Inconsistent Results** The results in Table 3.b appear to be inconsistent, with FID scores not following the expected pattern. This raises questions about the efficiency of the CMB method and its need for complex hyperparameter tuning. In particular, for a fixed P=30, FID(Q=1)>FID(Q=10)>FID(Q=5)."
                },
                "questions": {
                    "value": "I wonder why the authors did not provide CLIP score evaluation as well as reconstruction performances."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4753/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4753/Reviewer_c8RF",
                        "ICLR.cc/2024/Conference/Submission4753/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4753/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699374216933,
            "cdate": 1699374216933,
            "tmdate": 1700737478197,
            "mdate": 1700737478197,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ti4idZ7bpP",
                "forum": "o7x0XVlCpX",
                "replyto": "OLfDpGaEy0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (part 1/2)"
                    },
                    "comment": {
                        "value": "Thanks for your constructive suggestions. Your endorsement of our method and experiments gives us significant encouragement. Here are our clarifications.\n\n> **Q1**: The paper is challenging to follow and contains multiple typos, which can impede understanding. Improved clarity in the presentation and thorough proofreading would enhance the paper's quality.\n\n**A1**: We are sorry for this and truly appreciate your feedback on improving our paper's clarity and addressing the typos. We have corrected the expression and typos in our new version.\n\n> **Q2**: The update scheme presented in equation (5) appears inhomogeneous.\n\n**A2**: Our approach aligns with what classifier-free guidance (CFG) [1] does and it makes common sense in this field. The core idea is to manually design reasonable loss and impose its gradient on latent via backpropagation. This process allows for the adjustment of the latent space towards generating the desired output. Please refer to applications [2-4] of CFG if you need further clarification. If there are any further questions or confusion, we are open to discussing them.\n\n`[1]` Classifier-Free Diffusion Guidance. Arxiv. 2022.\n\n`[2]` Freedom: Training-free energy-guided conditional diffusion model. Arxiv. 2023.\n\n`[3]` Training-free layout control with cross-attention guidance. Arxiv. 2023.\n\n`[4]` Unite and Conquer: Plug & Play Multi-Modal Synthesis using Diffusion Models. CVPR. 2023.\n\n> **Q3**: The paper only contains qualitative results without quantitative evaluation metrics when compared with recent baselines such as ControlNet and T2I-adapter. \n\n**A3**: Thanks for your valuable suggestion. We have added the Table 4 (Appendix) for quantitative evaluation with ControlNet and T2I-Adapter in our updated manuscript. Additionally, in the Figure 9 (Appendix), we also provide qualitative comparison with T2I-Adapter under multi-modal conditioning.\n\n>  **Q4**: The datasets used and the conditioning modalities are briefly presented. A more detailed description of the datasets, along with the rationale for their selection, would be beneficial.\n\n**A4**: We are sorry for the confusion of the datasets. But we actually have included a detailed description of the datasets for each modality in Appendix Section A.1 due to the space limitations.\n \nWe chose the COCO dataset for its inclusion of manually-crafted segmentation. However, due to its relatively smaller image count compared to LAION, we opted for LAION to facilitate image filter. For instance, we retained only those images containing body to acquire pose maps, thereby preventing instances of empty guidance in the paired dataset. Our data selection and the underlying rationale align with those of ControlNet and T2I-Adapter, whose methodologies can be referenced for further information.\n\n> **Q5**: A more in-depth exploration of the impact of the weights $\\delta_c$ in equation (4) would offer valuable insights. The stability of these parameters is critical as their tuning could rapidly be cumbersome.\n\n**A5**: Thank you for your feedback. As suggested, we have conducted ablation study on $\\delta_c$ and the results are shown in the table below:\n\n| $\\delta_c$ | FID$\\downarrow$ | PickScore$\\uparrow$/% |\n| :----------- | :---------------- | :---------------------- |\n| 1.0          | 37.65\u00b10.22        | 49.57\u00b10.17              |\n| 0.1          | 37.75\u00b10.25        | 50.98\u00b10.37              |\n| 0.5          | 37.27\u00b10.26        | 50.96\u00b10.15              |\n| 2.0          | 37.93\u00b10.18        | 49.18\u00b10.16              |\n\nIn our initial manuscript, due to resource constraints and setting $\\delta_c$ to 1 is trivial, we had not conducted this part of the experiment. However, the results suggest that adjusting the value within a practical range (0-1) yields improved results. This finding holds promise and offers valuable insights for future practical applications. Thanks again for the valuable suggestion."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700377335158,
                "cdate": 1700377335158,
                "tmdate": 1700377335158,
                "mdate": 1700377335158,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dTDwmBnfCI",
                "forum": "o7x0XVlCpX",
                "replyto": "OLfDpGaEy0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal (part 2/2)"
                    },
                    "comment": {
                        "value": "> **Q6**: The results in Table 3.b appear to be inconsistent, with FID scores not following the expected pattern. This raises questions about the efficiency of the CMB method and its need for complex hyperparameter tuning. In particular, for a fixed P=30, FID(Q=1)>FID(Q=10)>FID(Q=5).\"\n\n**A6**: Thank you for your detailed review. When Q is sufficiently large, the latent will match the conditioning modality. However, in the case of multimodal conditioning, a large Q can lead to stiff competition among multiple modalities for the latent. This competition can naturally lead to worse performance, as seen in metrics like FID. To clarify, we've emphasized in the revision that Table (b) is evaluated under multimodal conditioning. \n\nAnd we acknowledge the reviewer's concern regarding hyperparamter tuning. But our work targets training-free scenarios with introducing merely two additional hyperparameter. It is not excessively burdensome compared to training a large, multimodal guided model.\n\n> **Q7**: I wonder why the authors did not provide CLIP score evaluation as well as reconstruction performances.\n\n**A7**: PickScore, a **CLIP-based** metric, aligns more closely with human judgments, which is why we chose it. FID, a **reconstruction** metric, together with U-IDS and P-IDS, evaluates image quality and aligns well with human perception, making it suitable for our purposes. PSNR and SSIM, being **pixel-wise** metrics, are not ideal for **editing** or large mask settings [1,2]. Nevertheless, based on your suggestion, we have included Table 5 in the Appendix for a quantitative evaluation with ControlNet and T2I-Adapter in terms of CLIP score, and additional reconstruction metrics PSNR, SSIM, and LPIPS.\n\n`[1]` Large scale image completion via co-modulated generative adversarial networks. ICLR. 2021.\n\n`[2]` Mat: Mask-aware transformer for large hole image inpainting. CVPR. 2022.\n\nPlease let us know if there are further questions. Thanks again!"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700377616664,
                "cdate": 1700377616664,
                "tmdate": 1700377616664,
                "mdate": 1700377616664,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1zrICZK4xt",
                "forum": "o7x0XVlCpX",
                "replyto": "OLfDpGaEy0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4753/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4753/Authors"
                ],
                "content": {
                    "title": {
                        "value": "A friendly reminder"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nI would like to send a kind reminder. Has our response addressed your concerns? The reviewer discussion period is nearing its end, and we eagerly await your reply. Your suggestions and comments are invaluable to the community. Thank you!\n\nBest, The authors"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700628878952,
                "cdate": 1700628878952,
                "tmdate": 1700628878952,
                "mdate": 1700628878952,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IOVQezoVyX",
                "forum": "o7x0XVlCpX",
                "replyto": "1zrICZK4xt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4753/Reviewer_c8RF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4753/Reviewer_c8RF"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Authors,\n\nThank you for providing a comprehensive rebuttal. It has clarified some of my concerns. Here are my responses to specific points:\n\n1. **Concerning the guidance loss:** The choice to have different variables for the input and the output of the denoiser confused me. It appears clearer to me now. I believe this explanation could be beneficial to include in the paper.\n\n2. **Concerning the evaluations:** I salute the effort for bringing in additional experiments. The performance gains observed seem convincing.\n\n3. **Concerning the tuning of $\\delta_c$:** I acknowledge the clarification about the constant choice of $\\delta_c$ for all c. However, I would recommend explicitly mentioning this choice in the paper. The extra experiments tend to show that the performances are stable with respect to this parameter but it is difficult to really conclude on this point as the variation of $\\delta_c$ was performed on a small range and with constant values for all weights. \n\n4. **Question on the Novelty:** I tend to agree with Reviewer jMgg that the MCU-Net appears very similar to T2I-adapter. However, if I understand correctly, the MCU-Nets are trained as denoisers? A more explicit discussion on this point could be beneficial. In particular, could the authors be more specific on the exact training loss used for the MCU-Net? Would it be correct to say that the proposed method, in a nutshell, is given by multiple classifiers guidance with, instead of classifiers, modality-specific denoisers? This is not a good or a bad thing, I just want to be sure of my understanding."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700665200963,
                "cdate": 1700665200963,
                "tmdate": 1700665200963,
                "mdate": 1700665200963,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cDaQqgEeXQ",
                "forum": "o7x0XVlCpX",
                "replyto": "OLfDpGaEy0",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4753/Reviewer_c8RF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4753/Reviewer_c8RF"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the previous answer which helped me understand your method.\n\nIn Fig. 10 and 11, are the completions given by the modality-specific MCU-Net or the denoiser $\\theta^*$ guided by the MCU-Net?\n\nIf I understand correctly, the performance gains brought by MAGIC come with the burden of training multiple modality-specific diffusion models which may require a large amount of data as well as high compute capacities while T2I-adapter is designed to be light-weight as it only attends to biased the encoding features. From these considerations, I have the feeling that the experimental set-up described in A.1. only describes the CMB sampling. Some insights on this point would be beneficial and could help understand if there are specific regimes where T2I-Adapter, ControlNet, or MAGIC  are expected to perform better."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732346244,
                "cdate": 1700732346244,
                "tmdate": 1700737282737,
                "mdate": 1700737282737,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1z9QBCKibD",
                "forum": "o7x0XVlCpX",
                "replyto": "cDaQqgEeXQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4753/Reviewer_c8RF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4753/Reviewer_c8RF"
                ],
                "content": {
                    "comment": {
                        "value": "Overall I am satisfied with the authors' rebuttal and I am thus willing to increase my raise."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4753/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737413071,
                "cdate": 1700737413071,
                "tmdate": 1700737413071,
                "mdate": 1700737413071,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]