[
    {
        "title": "A Study of Generalization in Offline Reinforcement Learning"
    },
    {
        "review": {
            "id": "uCsMQK7Mqt",
            "forum": "3w6xuXDOdY",
            "replyto": "3w6xuXDOdY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5112/Reviewer_qttr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5112/Reviewer_qttr"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of generalization for popular offline RL algorithms. By carrying out extensive experiments of state-of-the-art offline RL algorithms (IQL, CQL, DT, etc) on procgen and webshop environments, this paper presents the findings that although offline RL methods outperform BC in training environments with suboptimal data, they fail to generalize as well to testing environments similar to training environments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper carries out extensive experiments on procgen and another more realistic environment webshop, covering most state-of-the-art offline rl methods such as CQL, IQL and BCQ. \n\nThe problem of generalization of offline reinforcement learning is important yet relatively scarcely touched.\n\nThe paper is well-written and easy-to-follow. The format of using red text boxes makes it easy to capture important takeaways."
                },
                "weaknesses": {
                    "value": "I have concerns that some of the offline RL baselines might not be tuned appropriately. For example, in the procgen environments, there seems to be a large gap between BC and CQL both in the train and test environment. However, CQL contains a weighted combination of TD-learning loss and behavior cloning loss so a proper tuning of the weights should make CQL at least comparable to BC.\n\nAlthough procgen is a relatively popular benchmark, it is not the primary benchmark for those offline RL methods. They are most extensively tested on continuous control tasks such as D4RL (https://arxiv.org/abs/2004.07219), so authors should explain why they did not conduct experiments in the setting of continuous control. For example, the different test environments can be attained by modifying the environment parameters such as gravity.\n\nThe conclusion of the paper is derived mostly through empirical observations. It would be important to also have theoretical understandings of why offline RL does not work as well as BC in terms of generalization, similar to the analysis done in https://arxiv.org/pdf/2204.05618.pdf.\n\nI am willing to raise my score if those concerns can be solved."
                },
                "questions": {
                    "value": "Why IQL cannot be used in the setting of webshop? The expectile regression part should not depend on the number of actions for each state and as long as the expectile regression can be implemented it should be fine?\n\nIn section 4.5, is it possible to extend the plot a bit to show around how much data the learning curve stops to grow? It seems that we also need more than 3 data points to draw a valid conclusion regards to the trend.\n\nIn Figure 3, why is the blue line sometimes higher than the red line? Isn't the data collected by expert PPO?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5112/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5112/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5112/Reviewer_qttr"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5112/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698124509683,
            "cdate": 1698124509683,
            "tmdate": 1699636503229,
            "mdate": 1699636503229,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YfCB26gsGB",
                "forum": "3w6xuXDOdY",
                "replyto": "uCsMQK7Mqt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5112/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5112/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response from authors"
                    },
                    "comment": {
                        "value": "We are grateful for your valuable feedback and hope our answers below will address your remaining concerns. We are pleased to see that you found our paper to be \u201cwell-written, easy-to-follow\u201d and the topic important. \n\n> I have concerns that some of the offline RL baselines might not be tuned appropriately.\n\nWe have included thorough details on all the parameters that we had swept for tuning all our baselines in **Appendix G.2**\n\nTo summarize, for Procgen:\n\n        \u201cFor BC, we performed a sweep over the batch size \u2208 {64,128,256,512} transitions and learning rate \u2208 {5e \u2212 3, 1e \u2212 4, 5e \u2212 4, 6e \u2212 5}. For BCQ, CQL, and IQL, which use a DQN-style training setup (i.e., they have a base model and a frozen target model) (Mnih et al., 2013), in addition to the hyperparameters mentioned for BC, we swept over whether to use polyak moving average or directly copy weights, in the latter case, the target model update frequency \u2208 {1, 100, 1000} and in the former case, the polyak moving average constant \u03c4 \u2208 {0.005, 0.5, 0.99}. For BCQ, we also swept over the threshold value for action selection \u2208 {0.3, 0.5, 0.7}. For CQL, we swept over the CQL loss coefficient, which we refer to as cql alpha in our codebase, \u2208 {0.5, 1.0, 4.0, 8.0}. Finally, for IQL, we sweep over the temperature \u2208 {3.0, 7.0, 10.0} and the expectile weight \u2208 {0.7, 0.8, 0.9}.\t\t\n\t\t\n        For sequence modelling algorithms, DT and BCT, we sweep over the learning rate and batch size mentioned above, as well as the context length size \u2208 {5, 10, 30, 50}. For DT, we also sweep over the return-to-go (rtg) multiplier \u2208 {1, 5}. We follow similar approach in (Chen et al., 2021) to set the maximum return-to-go at inference time by finding the maximum return in the training dataset for a particular game and then multiplying by either 1 or 5 depending on the rtg multiplier value. We also use the default value of 0.1 for dropout in DT and BCT from (Chen et al., 2021).\u201d\n\n**For CQL, we swept over 4 (batch size) * 5 (learning rate) * 3 (update freq.) * 3 (tau)  * 4 (cql alpha)=720 different combinations**, with each combination trained and evaluated thrice for different random seed, amounting to a total of **2160 training runs for tuning CQL**. Similar is the case for other baselines.\n\n**We have also run a more comprehensive sweep for 10M expert dataset and have updated Table 2 in the Appendix accordingly. In WebShop too we ran a similar comprehensive sweep which is mentioned in Appendix G.2.2.**\n\nIn conclusion, **we believe we\u2019ve done extensive HP tuning with similar budgets for all our baselines.** \n\n> D4RL (https://arxiv.org/abs/2004.07219), so authors should explain why they did not conduct experiments in the setting of continuous control. For example, the different test environments can be attained by modifying the environment parameters such as gravity.\n\nThank you for such an insightful question. First of all, **we would like to emphasize that D4RL is not a standard benchmark for testing generalization. Procgen was chosen because it is one of the most commonly used benchmarks for this purpose in the online RL setting**, and therefore we wanted to advocate its use for offline setting as well. Similarly, WebShop is an equally challenging generalization benchmark in a different domain which is relevant for real-world applications (particularly of language models interacting with the web, which is a timely subject of wide interest to the community).\n\nMoreover, the offline RL methods that we evaluated are advertised for both, continuous as well as discrete domains. BCQ, CQL and DT especially have achieved state-of-the-art results on Atari at the time of their papers\u2019 acceptance.\n\n**Our research, on the other hand, emphasizes the importance of testing offline RL methods across a diverse range of tasks. While these methods are often presented as universally applicable, identifying scenarios where they falter is crucial. This approach not only highlights the limitations of current methods but also encourages the development of more robust and versatile solutions.**"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5112/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700212304357,
                "cdate": 1700212304357,
                "tmdate": 1700212304357,
                "mdate": 1700212304357,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "j91kNwCXGu",
                "forum": "3w6xuXDOdY",
                "replyto": "tOmlVNHNwx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5112/Reviewer_qttr"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5112/Reviewer_qttr"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the rebuttal"
                    },
                    "comment": {
                        "value": "Thanks authors for the rebuttal. I do understand the common use of procgen to study the generalization of reinforcement learning algorithms and emphasize with the authors about the limited computational resources for more experiments. However, I still think the proposed experiments and theoretical understandings are important to further support the conclusion of the paper. Therefore, I decide to keep my score of weak accept."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5112/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700517514500,
                "cdate": 1700517514500,
                "tmdate": 1700517514500,
                "mdate": 1700517514500,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DL9PeUyjFB",
            "forum": "3w6xuXDOdY",
            "replyto": "3w6xuXDOdY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5112/Reviewer_4tcZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5112/Reviewer_4tcZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper study and benchmark the generalization abilities of existing offline RL algorithms, revealing several interesting and helpful conclusions regarding to current offline RL learning researches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The problem studied in this paper is important and interesting. Existing offline RL benchmarks indeed require more practical metrics for evaluation.\n2. The experiments conducted in this paper seem solid and abundant. The conclusions made sound convincing.\n3. Hyperparameters are provided and the results seem reproducible."
                },
                "weaknesses": {
                    "value": "1. The algorithms included are a bit limited. Methods like model-based learning [1-2], curriculum imitation [3], and other methods are not involved. More useful conclusions can be made when the benchmarking algorithms are expanded.\n\n2. Benchmark included is a bit limited. Most of the results are concluded from a simulated benchmark ProcGen, only a small part of experiments are conducted on the real-world dataset \"WebShop\". The author may consider expanding their tested benchmark to more real-world problems as introduced in [4]. \n\n[1] Yu T, Thomas G, Yu L, et al. Mopo: Model-based offline policy optimization[J]. Advances in Neural Information Processing Systems, 2020, 33: 14129-14142.\n[2] Yu T, Kumar A, Rafailov R, et al. Combo: Conservative offline model-based policy optimization[J]. Advances in neural information processing systems, 2021, 34: 28954-28967.\n[3] Liu M, Zhao H, Yang Z, et al. Curriculum offline imitating learning[J]. Advances in Neural Information Processing Systems, 2021, 34: 6266-6277.\n[4] Qin R J, Zhang X, Gao S, et al. NeoRL: A near real-world benchmark for offline reinforcement learning[J]. Advances in Neural Information Processing Systems, 2022, 35: 24753-24765."
                },
                "questions": {
                    "value": "1. Will the author open-source their benchmark and evaluation codes?\n2. Can the author explain more about the setup of Webshop? What is the state/action/reward/objective/transition of this problem? An example would be better (can lie in the appendix).\n3. Can you explain how you fine-tune these algorithms and how the results are selected? It is thorny and important to select the best model in practice."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5112/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698448812397,
            "cdate": 1698448812397,
            "tmdate": 1699636503138,
            "mdate": 1699636503138,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cPjeaGswlD",
                "forum": "3w6xuXDOdY",
                "replyto": "DL9PeUyjFB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5112/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5112/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response from authors (1/2)"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer for providing us with valuable feedback. We are glad that the reviewer found our paper\u2019s results to be \u201cimportant and interesting\u201d and our experiments to be \u201csolid and abundant\u201d. \n\n> The algorithms included are a bit limited. Methods like model-based learning [1-2], curriculum imitation [3], and other methods are not involved. More useful conclusions can be made when the benchmarking algorithms are expanded.\n\nOur paper aims to evaluate the most commonly used offline RL algorithms on a new benchmark that probes their generalization abilities (which is an understudied problem in the literature). We believe we have accomplished this by benchmarking BCQ, CQL, IQL, DT, BCT, BC etc., which have a combined 3000+ citations and are competitive on a wide range of commonly used offline RL benchmarks [1]. In contrast, the methods you referenced are rather specialized and don\u2019t seem to be standard baselines in common offline RL papers. There are a lot of offline RL algorithms and we cannot possibly implement, tune, and evaluate all of them on a new benchmark (especially if we want to do justice to all these methods via extensive HP tuning using similarly large budgets to ensure fair comparisons\u2014which we do for our current baselines in Appendix G). **Since we had to draw the line somewhere, we decided to include only the most versatile, effective, and widely used offline learning methods according to the literature.** \n\n**Our goal was to create a benchmark that is accessible and easy to use by the community with simple and unified implementations** in order to speed up progress. Therefore, **we have open-sourced our code and datasets, and we welcome contributions from everyone to integrate and tune their baselines on our benchmark.**\n\n[1]: Tarasov, D., Nikulin, A., Akimov, D., Kurenkov, V., & Kolesnikov, S. (2022). CORL: Research-oriented deep offline reinforcement learning library. arXiv preprint arXiv:2210.07105.\n\n> Most of the results are concluded from a simulated benchmark ProcGen, only a small part of experiments are conducted on the real-world dataset \"WebShop\". The author may consider expanding their tested benchmark to more real-world problems as introduced in [4].\n\nWhile we covered a significant portion of our experiments on Procgen in the main paper, **we covered similar experiments conducted with the WebShop dataset in Appendix N of our submission**. Here we present a comprehensive assessment on various sizes of the WebShop dataset (i.e. 100, 500, 1000, 1500, 5000, 10000 episodes), as well as study the impact of suboptimal (collected from IL policy) versus expert data (collected from humans) in WebShop. These experiments are also crucial in demonstrating the robustness and adaptability of our approach in a real-world context.\n\nFurthermore, we want to emphasize that **our research is highly relevant and timely, particularly with the increasing importance of LLM-based agents interacting with the internet**. The Procgen benchmark, which learns directly from high-level images in an applied domain (games), and the WebShop environment, which is a realistic simulator of e-commerce websites, both are highly relevant and practical. Specifically, our experiments with WebShop showcase generalization to real-world scenarios, including interactions with actual websites like Amazon.\n\n**While NeoRL [4] does not assess generalization to new levels/instructions, the WebShop simulator on the other hand allows a realistic emulation of web interactions, thus enabling the agents to generalize to real-world scenarios like the Amazon website**. We argue that this is a substantial step towards real-world applicability. \n\nWe do not currently plan to expand the benchmark to other domains for the purpose of this submission. However, we are open to exploring this in the future and will aim to make it easy for others to contribute additional datasets since **we will be open-sourcing our codebase. Both our repositories can be adapted to other similar environments i.e. image-based/state-based environments in the Procgen repository, and text-based environments in the WebShop repository.**\n\n> Will the author open-source their benchmark and evaluation codes?\n\nWe would like to clarify that both, **the code and the datasets, are already uploaded as zip files as part of the supplementary material here and we will definitely open-source the code upon acceptance**.\n\n**We answer the remaining questions in the next comment.**"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5112/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700211891409,
                "cdate": 1700211891409,
                "tmdate": 1700211891409,
                "mdate": 1700211891409,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TcOQvtvhFB",
                "forum": "3w6xuXDOdY",
                "replyto": "cPjeaGswlD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5112/Reviewer_4tcZ"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5112/Reviewer_4tcZ"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your reply"
                    },
                    "comment": {
                        "value": "1. \"the methods you referenced are rather specialized and don\u2019t seem to be standard baselines in common offline RL papers\", I disagree, why model-based and imitation-based methods is specialized? \"draw the line somewhere\", what line? It is too subjective.\n\n2. Thanks for the explanation of the WebShop dataset.\n\n3. About model selection. It seems that you are using the test time effect to select the best checkpoint, this may not be feasible in real-world problems, I think there should be a practical way to make a fair comparison (by fair I mean not only the best results are reported)\n\n4. Thanks for your commitment to open-source, I really appreciate that.\n\n5. Do not push me to increasing the score, which should be a natural thing after my concerns are eased, and is not your job."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5112/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700619914415,
                "cdate": 1700619914415,
                "tmdate": 1700619914415,
                "mdate": 1700619914415,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "LlaOE1CrLv",
            "forum": "3w6xuXDOdY",
            "replyto": "3w6xuXDOdY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5112/Reviewer_HieC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5112/Reviewer_HieC"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the generalization abilities of offline RL algorithms across different environments. In particular, it introduced a collection of offline RL datasets of different sizes and skill-levels from the Procgen and WebShop environments. Experiments show that existing offline RL methods perform significantly worse than online RL on both train and test environments. Additional experiments show that an increase in data diversity improves generalization while an increase of the size of training data does not."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper investigates an important problem in offline RL.\n\n- It introduced a collection of offline RL datasets of different sizes and skill-levels from the Procgen and WebShop environments.\n\n- The experiments are thorough.\n\n- The writing is clear and easy to follow."
                },
                "weaknesses": {
                    "value": "- The novelty of the study is somewhat restricted. Given that many existing offline RL algorithms do not inherently prioritize generalization ability in their design, so the current experimental results are mostly within expected outcomes.\n\n- There are many duplicate references: \"Leveraging procedural generation to benchmark reinforcement learning\", \"Offline q- learning on diverse multi-task data both scales and generalizes\", \"Deep residual learning for image recognition.\", \"The nethack learning environment.\"\n\n- This paper primarily serves as a summary of empirical observations, and no specific solutions are proposed to address the generalization issue. IMHO, there is a lack of deeper understanding of the generalization issue of offline RL agents and the take aways for readers from this work is limited."
                },
                "questions": {
                    "value": "I appreciate the authors' efforts to investigate an important question in offline RL. However, upon reviewing the current draft, I find the perspective presented to be somewhat one-sided. It is essential to acknowledge that the overall conclusions drawn in the paper might be confined to the specific benchmark dataset being utilized. There have been notable instances demonstrating the impressive generalization capabilities of offline RL agents [1] [2]. Consequently, it would be prudent to avoid overly definitive statements in this paper, given these successful examples.\n\nMoreover, I would like to highlight a gap in the current work\u2014there is a lack of more in-depth analysis to elucidate the key question: \"why certain offline RL agents [1][2] exhibit superior generalization skills while others do not?\"\n\nA more profound theoretical analysis might be instrumental in explaining these discrepancies.\n\n[1] (Agarwal et al., 2020) An Optimistic Perspective on Offline Reinforcement Learning\n\n[2] (Kumar et al., 2023) Offline Q-Learning on Diverse Multi-task Data Both Scales and Generalizes"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5112/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5112/Reviewer_HieC",
                        "ICLR.cc/2024/Conference/Submission5112/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5112/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698524716816,
            "cdate": 1698524716816,
            "tmdate": 1700659069809,
            "mdate": 1700659069809,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "UdI4BQWxwk",
                "forum": "3w6xuXDOdY",
                "replyto": "LlaOE1CrLv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5112/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5112/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response from authors"
                    },
                    "comment": {
                        "value": "Thank you for providing valuable and constructive feedback which greatly improves our work. We were glad you found our \u201cexperiments thorough\u201d and \u201cwriting clear and easy-to-follow\u201d.  Below we address the points raised in the same order: \n\n> novelty is restricted. Given that many existing offline RL algorithms do not inherently prioritize generalization ability in their design, so the current experimental results are mostly within expected outcomes.\n\nWe respectfully disagree with this assessment of our contributions. As far as we know, **our paper is the first to introduce a benchmark specifically focused on evaluating the generalization of offline RL algorithms.** The absence of such a benchmark limits our understanding of these algorithms' real-world applicability, and our work strives to bridge this gap. In addition, we believe our results are novel in that this is the **first paper to demonstrate that online RL > behavioral cloning > sequence modeling > offline RL approaches when it comes to generalization to new tasks**. Even if our results weren\u2019t surprising, **we believe it is important to show this empirically via a thorough comparison study that others can later reproduce and build on since, as we show in the paper, details such as the data diversity or size can lead to different results**. Moreover, our findings emphasize the need for more robust algorithms suitable for practical applications and establish a standardized benchmark for the equitable assessment of these methods across diverse scales and qualities. Hence, **our contributions consist of: the creation of multiple datasets from two different domains, along with the extensive evaluation of multiple algorithms, and a clear evaluation protocol**.\n\n> many duplicate references\n\nThank you for pointing this out! We have now rectified the references.\n\n> paper primarily serves as a summary of empirical observations, and no specific solutions are proposed to address the generalization issue\n\nWhile we agree that specific techniques (either new or existing ones such as regularization) could increase performance of these offline RL, the **purpose of our work isn\u2019t to improve existing offline RL algorithms but rather to thoroughly evaluate *existing* ones as they are typically used in the literature (and in practice)**. \n\n> Consequently, it would be prudent to avoid overly definitive statements in this paper, given these successful examples\u2026\u2026 Moreover, I would like to highlight a gap in the current work\u2014there is a lack of more in-depth analysis to elucidate the key question: \"why certain offline RL agents [1][2] exhibit superior generalization skills while others do not?\"\n\nWe acknowledge the points you raised and we will revise our manuscript to address them. We agree that our conclusions should be more cautiously phrased, emphasizing that they are based on evaluations within specific domains. Indeed, there's a need for further research to study the reason behind the good/bad generalizability of these methods. \n\nFurthermore, we would like to re-emphesize the following:\n1. Paper [1] doesn't have any experiments on generalization to new environments so it is **not a relevant comparison to our work.**\n2. This is also mentioned in the **\u201cExtended Related Works\u201d section in Appendix D of our submission**, i.e. methods like [2] study how to generalize skills across different Atari games, using a dataset of about 40 games (and [1] only studies online generalization to the same game). In contrast, our research is about how well different levels or instructions within the same game can be handled, and we do this within the Procgen as well as the WebShop framework. Moreover, **our dataset and benchmark is designed to be memory-efficient, making it easier for the academic community to use without needing many GPUs. This is unlike the approaches in [2], which require a large number of TPUs and are practically infeasible to reproduce in most of the academic settings**.\n\nWe hope our responses above have adequately addressed your queries and that you will consider increasing your score. Please let us know if there is anything else preventing you from recommending acceptance of our paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5112/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700211432862,
                "cdate": 1700211432862,
                "tmdate": 1700211432862,
                "mdate": 1700211432862,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yxtcBNLTxF",
                "forum": "3w6xuXDOdY",
                "replyto": "t7p7YTvqhT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5112/Reviewer_HieC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5112/Reviewer_HieC"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the reply. The responses addressed most of my concerns and I will raise my rating to 6."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5112/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700659055267,
                "cdate": 1700659055267,
                "tmdate": 1700659055267,
                "mdate": 1700659055267,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "iCHuc8fBSU",
                "forum": "3w6xuXDOdY",
                "replyto": "LlaOE1CrLv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5112/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5112/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer"
                    },
                    "comment": {
                        "value": "We are grateful for your consideration of our rebuttal and thank you for your decision to raise the score. If there are any further issues or questions, we would be happy to provide additional information."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5112/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700672236029,
                "cdate": 1700672236029,
                "tmdate": 1700672254688,
                "mdate": 1700672254688,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ZCsOn4gxhk",
            "forum": "3w6xuXDOdY",
            "replyto": "3w6xuXDOdY",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5112/Reviewer_f7uZ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5112/Reviewer_f7uZ"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a benchmark for evaluating generalization in offline learning. Based on the benchmark, state-of-the-art offline policy learning algorithms, including BC, sequence modeling approaches, and offline RL algorithms, are tested. The results show that all the offline learning methods perform worse than online RL in both train and test environments. The results also reveal that BC is stronger to generalize to new environments than other offline learning methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper presents new results on the generalization of offline learning. As we know, it is important to understand the generalization ability of offline learning methods in order to apply offline methods to real-world problems. Although not very surprising, this paper first confirms that offline RL and sequence modeling approaches can be struggling to generalize to new environments.\n2. The results may have a broad impact on the community. Indeed, we can no longer ignore the generalization problem of existing offline learning methods. So, more investigation is needed. The results may also have an impact on our choice of offline learning methods in application scenarios.\n3. The experimental results are sufficient and convincing. In the experiments, multiple sequence modeling methods and multiple offline RL algorithms are included, two kinds of games are tested, and multiple settings are tested. \n4. The new benchmark is new and an important contribution."
                },
                "weaknesses": {
                    "value": "1. The paper does not discuss in depth the root causes of the generalization problem. I think it would be a great credit to the paper if the authors could share some thoughts on why.\n2. There are minor problems:\n- The color of the lines in Figure 2(a) is wrong.\n- The results on Leaper are missing in Figure 11 and Figure 12."
                },
                "questions": {
                    "value": "Can the authors share some thoughts on why offline RL does not generalize well?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5112/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5112/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5112/Reviewer_f7uZ"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5112/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698552092005,
            "cdate": 1698552092005,
            "tmdate": 1699636502931,
            "mdate": 1699636502931,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Gs6JEnkx89",
                "forum": "3w6xuXDOdY",
                "replyto": "ZCsOn4gxhk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5112/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5112/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response from authors"
                    },
                    "comment": {
                        "value": "We are grateful for your valuable feedback and hope our answers below will address your remaining concerns. We are pleased to see that you found our \u201cresults to be impactful\u201d and \u201cour benchmark an important contribution\u201d.\n\n> some thoughts on why offline RL does not generalize well?\n\nWe believe one reason for which offline RL methods lag behind BC is that all the offline RL approaches employ a risk-averse strategy whereby if they are encouraged to not take actions that they haven\u2019t seen during training. However, given that we are testing generalization to new environments, **all the states the agent encounters at test time are new so offline RL methods will be averse to taking any of those actions, likely defaulting to a rather suboptimal policy.** In contrast, BC doesn\u2019t have any such constraints so it simply uses its learned representations to decide what action is best to take by \u201clooking at the observation closest to the test state and picking the best action\u201d. **If BC is able to learn good enough state representations from its training data, it should be able to also learn a good enough policy that generalizes to new environments reasonably well.** \n\nOn the question of **why all offline learning approaches lag behind online RL ones, we believe the reason lies in the fact that online RL collects and learns from its own data, thus seeing a much more diverse set of states than BC or offline RL approaches where the set of states is fixed by the dataset.** Given that at test time, all the states are new for the agent, having seen a wider range of states can help the agent learn better representations and have a better idea of what\u2019s the best action in new situations. As our experiments in section 4.4 show, training on more diverse data can greatly improve the generalization of offline learning methods. Yet, as our results in section 4.5 show, simply training on data from multiple PPO checkpoints is not enough since these are sparsely sampled throughout training so they cannot span the entire space (it\u2019s also difficult to know exactly how training dynamics play into the diversity of the data collected which could be a good direction for future work to further investigate). \n \n**We\u2019ve updated the paper with a more in-depth discussion of this topic \u2013 see Appendix E \u201cDiscussion\u201d**, however, we believe more work is needed to test these hypotheses in order to reach a better understanding of these results.\n\nIt's important to note that our findings, which may seem surprising, underscore the significance of our benchmark. The divergence in results emphasizes the need for further research in this domain. Our results highlight the complexity of offline RL generalization and cautions against assuming that insights from Atari datasets (which is one of the most commonly used offline RL datasets) will directly transfer. Moreover, we would like to emphasize that **our results also align with other literature like [1], [2],[3] that show similar results i.e. BC > offline RL wrt generalization.**\n\n[1]: Gulcehre, C., Paine, T. L., Srinivasan, S., Konyushkova, K., Weerts, L., Sharma, A., ... & de Freitas, N. (2023). Reinforced Self-Training (ReST) for Language Modeling. arXiv preprint arXiv:2308.08998.\n[2]: Piterbarg, U., Pinto, L., & Fergus, R. (2023). NetHack is Hard to Hack. arXiv preprint arXiv:2305.19240.\n[3]: Hambro, E., Raileanu, R., Rothermel, D., Mella, V., Rockt\u00e4schel, T., K\u00fcttler, H., & Murray, N. (2022). Dungeons and Data: A Large-Scale NetHack Dataset. Advances in Neural Information Processing Systems, 35, 24864-24878.\n\n\n> There are minor problems:\nThe color of the lines in Figure 2(a) is wrong.\nThe results on Leaper are missing in Figure 11 and Figure 12.\n\nWe thank the reviewer for pointing out these errors. We have now fixed Figure 2(a).\n**In Figures 11 and 12, our underlying dataset had 0 return, since the PPO policy itself did not perform well on that level. Hence, all the offline RL baselines too had 0 return which is the reason why the plots look empty as if the results are missing.** To clarify this, we have added a note in the captions of Figures 11 and 12\n\nWe hope our responses above have successfully addressed your concerns. Please let us know if you have any outstanding questions."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5112/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700211174240,
                "cdate": 1700211174240,
                "tmdate": 1700211174240,
                "mdate": 1700211174240,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]