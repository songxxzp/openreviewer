[
    {
        "title": "Empirical Likelihood for Fair Classification"
    },
    {
        "review": {
            "id": "EtJMwudW1d",
            "forum": "GACjMj1MS1",
            "replyto": "GACjMj1MS1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4204/Reviewer_umPv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4204/Reviewer_umPv"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes to use the covariance as a proxy for the fairness and develop the confidence region of the covariance vector using empirical likelihood. In this way, a confidence region (with the significant level alpha being used for the fairness constraint) can be used to provide a more interpretable way to trade-off between accuracy and fairness (DP)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The proposed approach is technically sound and does seem to have the potential to replace the covariance based measure in Zafar et al., 2017 and become a more general measurement for DP.\n\n2. Comparing results from Table 3 and 4, the proposed algorithm does have a positive impact on both DP and EO on the ACS PUMS datasets including the OOD cases."
                },
                "weaknesses": {
                    "value": "The proposed method itself is technically sound and has good potential. Most of my concerns are regarding the evaluation of the method in the experiments.\n\n1. The simulation results in Table 2 do not seem to be a fair comparison. I would suggest adding more granularity for Zafar et al. with 0<c<0.1.\n\n2. In Table 3, increasing alpha from 0.05 to 0.5 does not seem to affect the performance much (both in training and testing). \n\n3. The proposed algorithm should be tested on more commonly used fairness datasets such as [1] or the ones from UCI Machine Learning Repository (https://archive.ics.uci.edu/)--- Adult Census Income, German Credit, etc.\n\n4. Only one baseline is compared in the experiments. I would suggest the authors to test against other baseline approaches optimizing for DP.\n\n[1] Ding, Frances, Moritz Hardt, John Miller, and Ludwig Schmidt. \"Retiring adult: New datasets for fair machine learning.\" Advances in neural information processing systems 34 (2021): 6478-6490."
                },
                "questions": {
                    "value": "1. Can you provide more simulation results in Table 2 for Zafar et al. with 0<c<0.1? E.g. c=0.01, 0.02, ... Meanwhile, can alpha take values of 1? Or what will happen when alpha is close to 1, e.g. alpha = 0.99?\n\n2. In Table 3, increasing alpha from 0.05 to 0.5 does not seem to affect the performance much (both in training and testing). Is it still the case when alpha is increased to e.g. 0.9?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4204/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4204/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4204/Reviewer_umPv"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4204/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697645003276,
            "cdate": 1697645003276,
            "tmdate": 1699636387045,
            "mdate": 1699636387045,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ENCpKr3ceD",
                "forum": "GACjMj1MS1",
                "replyto": "EtJMwudW1d",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4204/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4204/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your helpful comments.\n\nResponse to Weakness 1\uff1aWe have included results for Zafar et al. (2017) with $0<c<0.1$ in our response to Question 1.\n\nResponse to Weakness 2: The impact of increasing $\\alpha$ has been discussed in our response to Question 2.\n\nResponse to Weakness 3: We have applied our method to these datasets. In Section 5.1 of our paper, we use the data (Ding et al., 2021) to show the robust of our method to distribution shifts. In Section 5.2, we use the German credit dataset to test our method by considering two sensitive features simultaneously. We kindly request the reviewer to revisit these sections for additional insights.\n\n    Ding, F., Hardt, M., Miller, J., & Schmidt, L. (2021). Retiring adult: New datasets for fair machine learning. In Advances in Neural Information Processing Systems. Curran Associates, Inc.\n    \nResponse to Weakness 4\uff1aOur experiments have been expanded by comparing with a method that directly optimizes DP (Weerts et al., 2023). Please refer to our global response for the new experimental results.\n\n    Weerts, H.; Dud\u00b4\u0131k, M.; Edgar, R.; Jalali, A.; Lutz, R.; Madaio, M. Fairlearn: Assessing and improving fairness of ai systems. Journal of Machine Learning Research, 24(257):1\u20138, 2023.\n\nResponse to Question 1:  In the following table, we present the outcomes of Zafar et al. (2017) with $0<c<0.1$. Notably, none of these results exhibit superior performance in both accuracy and fairness metrics when compared with our method. Our method achieves a higher (or equal) accuracy and a higher fairness than Zafar et al. (2017): $\\alpha=0.5$ vs. $c=0.035$, $\\alpha=0.1$ vs. $c=0.095$, $\\alpha=0.2$ vs. $c=0.095$. \n\n| c     | ACC  | F1    | p%      | DP    | EO    |\n|-------|-------|-------|--------|--------|-------|\n| 0.015 | 0.832 | 0.833 | 89.941 | -0.016 | 0.109 |\n| 0.035 | 0.835 | 0.836 | 86.465 | 0.038  | 0.149 |\n| 0.055 | 0.842 | 0.842 | 83.077 | 0.050  | 0.188 |\n| 0.075 | 0.843 | 0.843 | 81.346 | 0.065  | 0.207 |\n| 0.095 | 0.842 | 0.840 | 78.548 | 0.094  | 0.240 |\n\nAs $\\alpha$ is closer to 1, the constraint becomes more strict. Theoretically, $\\alpha$ can take the value of 1. When $\\alpha=1$, the length of the confidence interval becomes 0, which means the confidence interval only includes one point 0. We show the result when $\\alpha=0.99$ in the following table.\n\n | $\\alpha$     | ACC  | F1    | p%      | DP    | EO    |\n|-------|-------|-------|--------|--------|-------|\n| 0.99 | 0.825 | 0.827 | 92.308 | -0.029 | 0.083 |\n\n Response to Question 2: The initial glance may suggest only marginal fairness enhancement when increasing $\\alpha$ from 0.05 to 0.5. However, when assessing the balance between accuracy loss and fairness improvement, we have a more detailed perspective. We calculate the average AC, DP, and EO reductions across the eight states. On average, AC decreases 0.0022, DP decreases 0.0150, and EO decreases 0.0185 for the training data. The ratios of fairness improvement to accuracy loss are more significant, with values of 0.0150/0.0022=6.78 for DP and 0.0185/0.0022=8.34 for EO. For the test data, AC decreases 0.0019, DP decreases 0.0156, and EO decreases 0.0132 on average, with ratios 0.0156/0.0019=8.28 for DP, 0.0132/0.0019=7.01 for EO.\n \nIn addition, we increase $\\alpha$ to 0.9 as shown in the following table. We compare the results between $\\alpha=0.05$ and $\\alpha=0.9$. On average, AC decreases 0.0028, DP decreases 0.0207, and EO decreases 0.0224 for the training data. The ratios are 0.0207/0.0028=7.28 for DP and 0.0224/0.0028=7.86 for EO. For the test data, AC decreases 0.0027, DP decreases 0.0214, and EO decreases 0.0194 on average, with ratios 0.0214/0.0027=7.79 for DP, 0.0194/0.0027=7.07 for EO. These findings underscore the effective role of $\\alpha$ in balancing the accuracy and fairness, where fairness gains are substantial comparing to the minor accuracy loss.\n \n |         |  |  $\\alpha$ |   =     |     0.9   |            ||\n|-------|-------------|-------------------|--------|--------|------------|------------|\n|    |       ID       | (training)     |        | OOD     |   (test)  |        |\n| State | ACC          | DP                | EO     | ACC     | DP         | EO     |\n| CA                | 0.7239      | 0.1967            | 0.2259 | 0.7282 | 0.2020     | 0.2111 |\n| TX                | 0.7330      | 0.2195            | 0.2605 | 0.7337 | 0.1874     | 0.1956 |\n| DE                | 0.7290      | 0.2186            | 0.2882 | 0.7375 | 0.2595     | 0.2538 |\n| NV                | 0.7152      | 0.1874            | 0.1948 | 0.7312 | 0.1611     | 0.1518 |\n| KY                | 0.7451      | 0.1413            | 0.1464 | 0.7181 | 0.0759     | 0.1225 |\n| FL                | 0.7376      | 0.2083            | 0.2402 | 0.7295 | 0.1546     | 0.1669 |\n| MO                | 0.7455      | 0.1399            | 0.2025 | 0.7220 | 0.0917     | 0.1319 |\n| NE                | 0.7297      | -0.0411           | 0.0317 | 0.7109 | 0.0354     | 0.0972 |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4204/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700283027382,
                "cdate": 1700283027382,
                "tmdate": 1700283382781,
                "mdate": 1700283382781,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qTVA2ZCKod",
            "forum": "GACjMj1MS1",
            "replyto": "GACjMj1MS1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4204/Reviewer_Rn9g"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4204/Reviewer_Rn9g"
            ],
            "content": {
                "summary": {
                    "value": "Based on the covariance fairness measure by Zafar et al. (2017), the authors propose a fairness constraint based on a statistical likelihood test. Specifically, they formulate a constraint that requires that a statistical test with level alpha does not reject $H_0$: the classifier is fair."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I think that the idea of using a likelihood statistic for enforcing fairness is novel and an interesting approach. The paper is well written and supported by the required theoretical developments."
                },
                "weaknesses": {
                    "value": "I think the main limitation of this work is the experimental evaluation. I understand that the method is based on Zafar's covariance fairness measure, and comparing to that is very valuable. However, since this seminal work in 2017, there has been a lot of development in the community regarding fairness regularizers or constraints. Comparing the results of your approach to some of these more recent works would be of interest."
                },
                "questions": {
                    "value": "Some minor questions and comments:\n- On the bottom of page 3, you state that a system of $m$ constraints and $d$ parameters with $m\\geq d$ can only have the solution $\\theta=0$. If I understand correctly $m$ is the number of sensitive attributes and $d$ is the number of features. Does it make sense that $m\\geq d$?\n- In equation (8), is $\\lambda$ a decision variable of the minimization problem? \n- Am I correct in my analysis that (8) is nonconvex?\n- I think Figure 1 is very hard to understand. Should we compare Figure 1 with Figure 2 and observe that the ID results are more representative of the OOD resutls than in Figure 2?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4204/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4204/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4204/Reviewer_Rn9g"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4204/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698658631163,
            "cdate": 1698658631163,
            "tmdate": 1699636386971,
            "mdate": 1699636386971,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TaK93Rw3QC",
                "forum": "GACjMj1MS1",
                "replyto": "qTVA2ZCKod",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4204/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4204/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your helpful comments.\n\nResponse to Weakness: We have significantly expanded our experiments by comparing with the method (Weerts et al. 2023) in our global response. The results showcase our EL method is superior to the compared approach.  We kindly request the reviewer's attention to the new experimental results provided in our global response.\n\n    Weerts, H.; Dud\u00b4\u0131k, M.; Edgar, R.; Jalali, A.; Lutz, R.; Madaio, M. Fairlearn: Assessing and improving fairness of ai systems. Journal of Machine Learning Research, 24(257):1\u20138, 2023.\n\nResponse to Question 1: $m$ is the number of sensitive attributes and $d$ is the number of non-sensitive features. There are some cases where $m\\geq d$.  To illustrate this situation, consider the Ricci 20 dataset (Le et al., 2022). This dataset encompasses five features \"race\", \"position\", \"written\", \"oral\", and the derived feature \"combine\" representing a composite score based on \"written\" and \"oral\", and one label \"promoted\". If we only consider the four original features and study the discrimination of race and position on the promotion, we have two sensitive features \"race\" and \"position\" and two non-sensitive features \"written\" and \"oral\". Then $m=d=2$.\n\n    Le Quy, T., Roy, A., Iosifidis, V., Zhang, W., & Ntoutsi, E. (2022). A survey on datasets for fairness-aware machine learning. WIREs Data Mining and Knowledge Discovery, 12(3), e1452.\n\n Response to Question 2: Yes. In equation (8), $\\lambda$ is a decision variable of the minimization problem.\n \n Response to Question 3: The optimization problem in equation (8) is nonconvex. We can obtain a local minimum when dealing with nonconvex problems. Our empirical experiments confirm that this approach yields satisfactory performance. Achieving a global minimum in nonconvex problems is indeed a demanding task, and it falls outside the scope of our focus in this paper.\n\n Response to Question 4: The in-distribution (ID) results represent the training outcomes, and the out-of-distribution (OOD) results indicate the performance across 49 other states during the testing. Figure 2 (adapted from Figure 3 of Ding et al. (2021)) shows the accuracy of the training data (ID) is much higher than the accuracy among all the test data (OOD), which underscores a marked separation between the in-distribution and out-of-distribution results. In contrast, our results in Figure 1, depict OOD outcomes distributed around the ID results in a random pattern without a marked separation between ID and OOD. The key point in comparison between Figure 1 and Figure 2 is that Figure 2 demonstrates a marked separation between in-distribution (ID) and out-of-distribution (OOD) results, while Figure 1 not. We can say that the ID results are more representative of the OOD results in Figure 1 than in Figure 2."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4204/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700282442962,
                "cdate": 1700282442962,
                "tmdate": 1700282442962,
                "mdate": 1700282442962,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ESHyfkk4to",
                "forum": "GACjMj1MS1",
                "replyto": "TaK93Rw3QC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4204/Reviewer_Rn9g"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4204/Reviewer_Rn9g"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your comments"
                    },
                    "comment": {
                        "value": "Thank you for your clarifications. The new experiments are insightful and show that the proposed method performs well.\nSome follow-up questions on your answers to my questions:\nQ1: So you assume the protected attribute is not part of the feature vector?\nQ2: Thank you for the clarification. I suggest adding $\\lambda$ under the $\\min$, like $\\theta$, to avoid confusion.\nQ3: That makes sense. Is there a straight-forward way to apply your methodology to (S)GD type algorithms (i.e. Neural Networks)\nQ4: Thank you, that is clear now."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4204/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700474064524,
                "cdate": 1700474064524,
                "tmdate": 1700474064524,
                "mdate": 1700474064524,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nOHkz1Kdi7",
                "forum": "GACjMj1MS1",
                "replyto": "qTVA2ZCKod",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4204/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4204/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to new comments"
                    },
                    "comment": {
                        "value": "Thanks for your helpful feedback.\n\nResponse to Q1: Yes. The protected attribute is excluded from the feature vector in our methodology, aligning with the approach employed by Zafar et al. (2017) and Zink and Rose (2020).\n\n    Zafar, M.B., Valera, I., Rogriguez, M.G. &amp; Gummadi, K.P.. (2017). Fairness Constraints: Mechanisms for Fair Classification. Proceedings of the 20th International Conference on Artificial Intelligence and Statistics.\n\n    Zink A, Rose S. 2020 . Fair regression for health care spending. Biometrics. 76(3):973-982.\n\nResponse to Q2: Thanks for your suggestion. We have added $\\lambda$ under \"min\" in our revised paper.\n\nResponse to Q3:  Yes. our methodology can be applied to ((S)GD) type algorithms, including Neural Networks. To implement this, one can leverage packages like tensorflow_constrained_optimization (tfco), specifically designed for integrating gradient-based learning algorithms such as SGD or Adam for solving constrained optimization problems. The tfco package provides flexibility in choosing different gradient-based learning algorithms, making it compatible with our proposed methodology.\n\nResponse to Q4: We are glad the explanation is clear now. If you have any further questions or need additional clarification, feel free to ask."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4204/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700494146162,
                "cdate": 1700494146162,
                "tmdate": 1700494186239,
                "mdate": 1700494186239,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "T2UGLQwtTk",
                "forum": "GACjMj1MS1",
                "replyto": "nOHkz1Kdi7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4204/Reviewer_Rn9g"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4204/Reviewer_Rn9g"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for clarifying. I would like to retain my score."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4204/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700506190412,
                "cdate": 1700506190412,
                "tmdate": 1700506190412,
                "mdate": 1700506190412,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "02GSwbDB0S",
            "forum": "GACjMj1MS1",
            "replyto": "GACjMj1MS1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4204/Reviewer_YzKS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4204/Reviewer_YzKS"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a framework to ensure fairness of classifiers while handling the uncertainty from point estimates using finite samples. In particular, formulating fairness in terms of the covariance between sensitive attributes and the decision boundary (Zafar et al. 2017), the authors propose using the empirical likelihood method to derive a confidence region for the covariance vector. Empirical evaluation using simulation as well as real-world data shows that the proposed approach can balance the accuracy-fairness tradeoff and be robust to distribution shifts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The consideration of uncertainty in fairness assessment is interesting, and addresses a problem that is underexplored in existing approaches.\n\nThe application of an EL-based estimator for fairness using confidence intervals is a novel contribution. The authors suggest that this approach could be applied to other fairness criteria, which could increase the impact of this method broadly.\n\nThe paper is overall well-written and technically sound as far as I can tell."
                },
                "weaknesses": {
                    "value": "While the comparison to Zafar et al. (2017) is very thorough, no other fair learning methods are considered in the empirical evaluation. In particular, comparing group fairness metrics such as DP and EO against baselines that directly optimize those notions could serve to more strongly justify the proposed approach using covariance-based fairness constraints. \n\nI was a bit confused about the discussion at the end about applying EL to other fairness criteria. If indeed extending to non-linear measures is straightforward, it is unclear why the current method only considers covariance as a fairness proxy.\n\nThe motivation for the covariance-based fairness constraints was not very compelling. The authors suggest that determining the threshold for accuracy-fairness tradeoff becomes easier with the proposed approach. However, it is still unclear what the appropriate confidence region is for a given group fairness notion (whether a soft constraint to bound the violation or as a hard constraint)."
                },
                "questions": {
                    "value": "1. How does the proposed method compare against Zafar et al. (2017) under distribution shifts?\n\n2. Is the discussion about EL for general fairness measures new or known results? If it is the former, there needs to be more details and proofs. \n\n3. I was not sure about the significance of Figure 1. Also, should the legends ID and OOD evaluations be switched in Figures 1 and 2? Figure 2 was also extremely hard to read."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4204/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699085430283,
            "cdate": 1699085430283,
            "tmdate": 1699636386913,
            "mdate": 1699636386913,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gWEPZ1oZDB",
                "forum": "GACjMj1MS1",
                "replyto": "02GSwbDB0S",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4204/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4204/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your helpful comments.\n\nResponse to Weakness 1\uff1aOur experiments have been expanded by comparing with a method that directly optimizes DP (Weerts et al., 2023). Please refer to our global response for the new experimental results.\n\n    Weerts, H.; Dud\u00b4\u0131k, M.; Edgar, R.; Jalali, A.; Lutz, R.; Madaio, M. Fairlearn: Assessing and improving fairness of ai systems. Journal of Machine Learning Research, 24(257):1\u20138, 2023.\n    \nResponse to Weakness 2\uff1aWe can extend EL to other fairness criteria using similar techniques as presented in our paper. However, the extension involves carefully finding the influence functions and deriving the estimating equations of the specific fairness criteria, which serves as the foundation of the logarithm of the EL ratio converging to the $\\chi^2$ distribution. Our current paper represents a preliminary exploration of applying EL to the fairness metrics. We leave the extension of EL to other fairness criteria as a future project.\n\nResponse to Weakness 3\uff1aThe tradeoff indicator $\\alpha$ in our framework has a bounded range from 0 to 1, while covariance is unbounded. In this sense, it is easier to determine the threshold by our method. The choice of the confidence region depends the preference of the user's accuracy and fairness. Our constraint is formed by imposing $0$ included in the confidence region of the covariance at a user-determined significant level. The inherent mechanism of the confidence region allows the violation of the covariance as $0$, controlled by $\\alpha$. In this sense, our constraint can be seen as a soft constraint.\n\nResponse to Question 1: Our method performs better than the method in Zafar et al. (2017) under distribution shifts. We conducted experiments using datasets characterized by explicit distribution shifts (Ding et al., 2021), and the results are shown below. In each row of the following table, our method performs better in both accuracy and fairness than Zafar et al. (2017).\n\n|       |  Zafar|  et al.| (2017) |        | EL    | based  |fairness|        |\n|-------|-------|--------|--------|--------|-------|--------|--------|--------|\n| State | $c$   | ACC    | DP     | EO     | $\\alpha$ | ACC | DP     | EO     |\n| CA    | 0.007 | 0.7284 | 0.2036 | 0.2125 | 0.50  | 0.7284 | 0.2034 | 0.2123 |\n| TX    | 0.020 | 0.7348 | 0.2009 | 0.2110 | 0.01  | 0.7348 | 0.1983 | 0.2106 |\n| TX    | 0.006 | 0.7340 | 0.1909 | 0.1993 | 0.50  | 0.7340 | 0.1905 | 0.1988 |\n| TX    | 0.003 | 0.7338 | 0.1893 | 0.1978 | 0.70  | 0.7338 | 0.1892 | 0.1975 |\n| NV    | 0.020 | 0.7319 | 0.1686 | 0.1598 | 0.50  | 0.7319 | 0.1657 | 0.1542 |\n| NV    | 0.015 | 0.7317 | 0.1665 | 0.1595 | 0.50  | 0.7319 | 0.1657 | 0.1542 |\n| NV    | 0.009 | 0.7314 | 0.1642 | 0.1549 | 0.70  | 0.7314 | 0.1634 | 0.1535 |\n| FL    | 0.008 | 0.7304 | 0.1605 | 0.1774 | 0.30  | 0.7305 | 0.1605 | 0.1771 |\n\nThe following table shows the results of Zafar et al. (2017) whe $c=0.01$ under distribution shifts. The fairness metrics differ significantly between the training data and test data, which indicates the method is not robust to distribution shifts.\n\n|       |   ID        |results  |(training)|OOD    |results |(test)  |\n|-------|-------------|---------|---------|--------|--------|--------|\n| State | ACC         | DP      | EO      | ACC    | DP     | EO     |\n| CA    | 0.7247      | -0.2286 | -0.1996 | 0.7286 | 0.2053 | 0.2130 |\n| TX    | 0.7341      | -0.2640 | -0.2193 | 0.7343 | 0.1936 | 0.2038 |\n| DE    | 0.7288      | -0.2882 | -0.2191 | 0.7378 | 0.2620 | 0.2620 |\n| NV    | 0.7161      | -0.1969 | -0.1893 | 0.7314 | 0.1645 | 0.1564 |\n| KY    | 0.7458      | -0.1495 | -0.1443 | 0.7197 | 0.0823 | 0.1273 |\n| FL    | 0.7396      | -0.2451 | -0.2123 | 0.7307 | 0.1619 | 0.1795 |\n| MO    | 0.7459      | -0.2043 | -0.1434 | 0.7239 | 0.1084 | 0.1427 |\n| NE    | 0.7306      | -0.0336 | 0.0182  | 0.7123 | 0.0426 | 0.1056 |\n\n    Ding, F., Hardt, M., Miller, J., & Schmidt, L. (2021). Retiring adult: New datasets for fair machine learning. In Advances in Neural Information Processing Systems. Curran Associates, Inc.\n    \nResponse to Question 2: The extension of EL for general fairness measures can be considered as new results, as explained in our response to Weakness 2. We leave the extension as a future project.\n\nResponse to Question 3:  The legends ID and OOD evaluations should be switched in Figure 1. We apologize for our carless errors. We have corrected the errors in our revised paper. The in-distribution (ID) results represent the training outcomes, and the out-of-distribution (OOD) results indicate the performance across 49 other states during testing. Figure 2 (Ding et al., 2021) shows the accuracy of the training data is much higher than the accuracy among all the test data, which underscores a marked separation between the ID and OOD results. In contrast, our results in Figure 1 depict OOD outcomes distributed around ID results in a random pattern without a marked separation between ID and OOD."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4204/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700281152015,
                "cdate": 1700281152015,
                "tmdate": 1700281152015,
                "mdate": 1700281152015,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]