[
    {
        "title": "Learning to Count without Annotations"
    },
    {
        "review": {
            "id": "ANCbwejrjD",
            "forum": "DAs9X4mCpu",
            "replyto": "DAs9X4mCpu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5616/Reviewer_1EYE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5616/Reviewer_1EYE"
            ],
            "content": {
                "summary": {
                    "value": "This manuscript targets on few shot counting model training without annotations. Specifically, the authors build a synthetic dataset to provide supervision signal to the object counter, and utilize a DINO and Vision Transformer based architecture to make prediction of density map."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. This manuscript is sound in making adequate explaination to the results and experimental analysis;\n2. The writing quality of this manuscript is ok to make me get the points."
                },
                "weaknesses": {
                    "value": "1. The motivation of this work is poor. I am still confused on why we should build such a sythetic dataset from others to get some supervision signal to train a unsupervised counter. \n2. From the data perspective, these generated data are without double to be filled with artefact and the solution in this manuscript is just the copy-paste, whose contribution is limited. \n3. The counting model utilized in this manuscript is not totally original, which seems to be the DINO + ViT. \n4. It is evident that the authors omitted some methods in unsupervised counter (Completely Self-Supervised Crowd Counting via Distribution Matching-ECCV22), or foundation model based method (Can SAM Count Anything? An Empirical Study on SAM Counting) & (Training-free Object Counting with Prompts)."
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5616/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698738938986,
            "cdate": 1698738938986,
            "tmdate": 1699636580019,
            "mdate": 1699636580019,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "X4yw6Ngjbf",
            "forum": "DAs9X4mCpu",
            "replyto": "DAs9X4mCpu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5616/Reviewer_21rs"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5616/Reviewer_21rs"
            ],
            "content": {
                "summary": {
                    "value": "This paper focuses on the unsupervised object-counting task that does not require any manual annotations. To this end, the authors construct \u201cSelfCollages\u201d, images with various pasted objects as training samples, that provide a rich learning signal covering arbitrary object types and counts. Experiments on the counting dataset demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The unsupervised counting task is a challenging task, and it is appealing to see the authors propose a practical way.\n\nThe proposed method even outperforms simple baselines and generic models such as FasterRCNN and DETR."
                },
                "weaknesses": {
                    "value": "1. The experiments are not convincing. There are two pioneering works (CrowdCLIP[1] and CSCCNN ) that also focus on the unsupervised counting task. However, the authors do not discuss or compare with them. I would like to see a comprehensive comparison.\n2. The evaluated FSC-147 dataset is not very challenging. I suggest the authors try to conduct experiments on the crowd datasets, which are usually dense and challenging. Compared with CrowdCLIP[1] and CSC-CCNN[2] will make the paper more solid.\n3. It is better to add a subsection to discuss the weakly/semi-supervised counting methods that also reduce the annotation cost.\n4. The motivation for pasting different images on top of a background is not clear.\n5. Do the authors try to utilize other cluster algorithms unless the K-means\uff1f\n\n[1] CrowdCLIP: Unsupervised Crowd Counting via Vision-Language Model. CVPR 2023.\n[2] Completely self-supervised crowd counting via distribution matching. ECCV 2022."
                },
                "questions": {
                    "value": "see weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5616/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698757997825,
            "cdate": 1698757997825,
            "tmdate": 1699636579922,
            "mdate": 1699636579922,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    },
    {
        "review": {
            "id": "aNJJTPbUrf",
            "forum": "DAs9X4mCpu",
            "replyto": "DAs9X4mCpu",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5616/Reviewer_HmkX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5616/Reviewer_HmkX"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a method for counting objects without annotations. It leverages DINO and N-cut to extract object patches and then randomly places them into a background image, allowing for the acquisition of localization labels without annotation. Additionally, it trains a counting model based on CounTR but with the DINO backbone to count objects."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The authors propose a method to generate synthetic data for object counting and implement an unsupervised object counting approach.\n2. They utilize the DINO backbone to create a counting model similar to CounTR."
                },
                "weaknesses": {
                    "value": "1. The approach of creating synthetic data by copying segmentation results from one image to another is a well-known technique in segmentation [1]. However, this paper applies it to object counting.\n2. The trained model's performance is not satisfactory, particularly in FSC-147 high, which is the primary objective of counting dense and small objects.\n3. The motivation for the counting task is to abstract information from dense scenes that detection models struggle with, particularly partial and occluded objects. However, the proposed method does not effectively handle partial or occluded objects, which contradicts the motivation of the counting task.\n\n[1] Ghiasi, Golnaz, et al. \"Simple copy-paste is a strong data augmentation method for instance segmentation.\" CVPR, 2021."
                },
                "questions": {
                    "value": "1. How does the model perform if fine-tuned on FSC-147?\n2. How does the model perform on a specific dataset without retraining? For instance, previous methods have conducted adaptation on the CARPK dataset.\n3. Why is $n_{max}$ set as a very small value ($n_{max} = 20$)? A higher count might be more suitable for a counting model since powerful detection models can handle it in sparse scenes.\n4. Although SC-147 is split into low/medium/high in experiments, the overall performance should also be reported in the corresponding tables and sections.\n5. Table 5 seems unfair. $n_{max}$ in UnCo is 20, making the trained model more suited to FSC-147 low (8-16 objects). CounTR is trained on the entire FSC-147 dataset, with counts ranging from 7 to 3731. The domains are different, and for a fair comparison, CounTR should be trained using only samples from FSC-147-low.\n6. The average baseline in Table 1 seems unfair, particularly for FSC-147 low. Specifically, if the estimation density is 0, the MAE is the average GT count in FSC-147 low (8-16), which is much smaller than 37."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5616/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5616/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5616/Reviewer_HmkX"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5616/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698897948517,
            "cdate": 1698897948517,
            "tmdate": 1699636579830,
            "mdate": 1699636579830,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": []
    }
]