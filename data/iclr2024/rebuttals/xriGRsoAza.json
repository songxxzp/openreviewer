[
    {
        "title": "Inherently Interpretable Time Series Classification via Multiple Instance Learning"
    },
    {
        "review": {
            "id": "ebJKsudcJ3",
            "forum": "xriGRsoAza",
            "replyto": "xriGRsoAza",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3547/Reviewer_Yc2u"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3547/Reviewer_Yc2u"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new framework that leverages Multiple Instance Learning to make deep learning TSC models inherently interpretable without compromising predictive performance. The authors evaluate MILLET on 85 UCR TSC datasets and show that it produces sparse explanations quickly and of higher quality than other interpretability methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The strengths of this paper include:\n\n1.\tIntroducing a new MILLET framework that makes deep learning TSC models inherently interpretable by leveraging the MIL approach without compromising predictive performance. In particular, the authors proposed exploring four MIL (attention, instance, additive and conjunctive) pooling methods to increase interpretability while replacing GAP. Moreover, including positional encoding ensures the modelling of time series constraints.\n2.\tProposing a new synthetic dataset called WebTraffic to explore the MILLET concept and evaluate the inherent interpretability of their models. The authors compared the four proposed MIL pooling approaches for MILLET with GAP on their WebTraffic dataset. Each pooling method is applied to the FCN, ResNet, and InceptionTime backbones.\n3.\tEvaluating MILLET on 85 UCR datasets and showing that it produces sparse explanations quickly and of higher quality than other interpretability methods. The authors found that \nwhile Conjunctive InceptionTime is the best approach for balanced accuracy (outperforming the HC2 and Hydra-MR SOTA methods), it is not quite as strong on the other metrics. However, it remains competitive, and for each backbone, using MILLET improves performance across all metrics. Moreover, the authors found that the Conjunctive has the best interpretability performance."
                },
                "weaknesses": {
                    "value": "The major weaknesses are summarized below:\n\n1.\tThe paper does not provide a detailed comparison of MILLET with other state-of-the-art TSC models. Although the authors claim that \u201cWe design three MILLET DL models by adapting existing backbone models that use GAP: FCN, ResNet, and InceptionTime. While extensions of these methods and other DL approaches exist (see Foumani et al., 2023), we do not explore these as none have been shown to outperform InceptionTime (Middlehurst et al., 2023).\u201d the application of MILLET to other models and further comparisons with other state of the art TSC is missing and is relevant better to measure the effectiveness and generalizability of the proposed approach.\n\n2.\tThe paper does not provide a detailed comparison with other TSC interpretability methods. (e.g.LIME)\n\n3.\tThe paper does not provide a detailed algorithm complexity analysis. While the authors provide information on the run time of MILLET (see E.3), a more detailed analysis of the algorithm complexity would help establish its scalability and feasibility in large-scale applications."
                },
                "questions": {
                    "value": "1.\tHow does MILLET perform with other TSC models?\n2.\tHow does MILLET compare with other TSC interpretability methods?\n3.\tCan you provide a more detailed analysis of the algorithm complexity?\n4.\tHave you considered the potential impact of the choice of hyperparameters on the performance of MILLET?\n5.\tHave you considered the potential impact of class imbalance on the performance of MILLET?\n\nAfter reading the author's rebuttal and discussions I am more incline to accept the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3547/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3547/Reviewer_Yc2u",
                        "ICLR.cc/2024/Conference/Submission3547/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3547/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698137126370,
            "cdate": 1698137126370,
            "tmdate": 1700642716268,
            "mdate": 1700642716268,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "FpYJpFNV91",
                "forum": "xriGRsoAza",
                "replyto": "ebJKsudcJ3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3547/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3547/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Yc2u"
                    },
                    "comment": {
                        "value": "Thank you for your time in reviewing our work. Please see our response to your review below in addition to the general rebuttal given above.\n\n1. *Application of MILLET to other TSC methods (Weakness 1 / Question 1)*  \nIn the work we focused on applying MILLET to the deep learning family of TSC methods. As stated in the paper, \u201cwe choose to focus on DL approaches due to their popularity, strong performance, and scope for improvement\u201d (Section 2). We agree that extending MILLET to other families of TSC methods is a worthwhile area of further study, but believe that our analysis against the current versions of SOTA methods is sufficiently comprehensive to demonstrate the effectiveness of MILLET. While MILLET should still be applicable to other TSC methods, it is not as trivial as applying it to other deep learning methods and warrants standalone future work. We feel that our time during the rebuttal is best served delivering more insights into MILLET for deep learning methods, as this was the main focus of the work.\n2. *Other interpretability methods (Weakness 2 / Question 2)*  \nThank you for the suggestion of comparing with LIME. With LIME, we expect the analysis to uncover a similar issue as to what we discovered with SHAP: LIME will be very expensive to run as it makes many repeated forwarded passes of the model. We will include a discussion of the alternative interpretability methods in a new section in the Appendix.\n3. *Time complexity analysis (Weakness 3 / Question 3)*  \nWe have begun conducting time complexity analysis of the pooling methods, and will expand Appendix E.3 to include these results. This will go beyond run time and number of forward passes, rather looking at actual time complexity and how the pooling methods scale with the number of classes and time series length. Please see the general rebuttal for initial results.\n4. *Impact of hyperparameter choice (Question 4)*  \nAs discussed in Appendix B.2, we chose fixed hyperparameter values based on those used to train the original backbone models (i.e. no hyperparameter tuning was used). This was done to ensure a fair comparison and provide a set of default hyperparameter values for use in derivative works. We will happily expand Appendix B.2 to mention the impact of hyperparameter choice. For example, MILLET had varying convergence rates for different datasets: sometimes it converged very quickly (so a lower LR might be better), but sometimes it took a long time to converge, so more epochs may be required. Scheduled/adaptive LR might be appropriate in some cases.\n5. *Impact of class imbalance (Question 5)*  \nAs mentioned in your review, MILLET was the best approach in our evaluation of balanced accuracy performance. This suggests it is more robust to class imbalance than other methods. This was also without any specific focus on optimising for class imbalance, e.g. weighted cross entropy loss could be used during training to further improve performance on imbalanced datasets. However, further analysis of the effect of class imbalance would indeed be insightful. We will expand Appendix E.1 to include this. Our initial results have shown that performance improvements are demonstrated for MILLET on datasets with a high level of imbalance compared to InceptionTime, HC2, and Hydra-MR."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700245359884,
                "cdate": 1700245359884,
                "tmdate": 1700245359884,
                "mdate": 1700245359884,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VKO3LahVTo",
                "forum": "xriGRsoAza",
                "replyto": "FpYJpFNV91",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3547/Reviewer_Yc2u"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3547/Reviewer_Yc2u"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "I am satisfied with the authors' response. The focus on hyperparameters, time complexity and discussing the impact of class imbalance should have priority over the application of MILLET to other TSC ML approaches that may require future work. However, the hyperparameter tuning may influence the performance of the proposed approach. The authors should provide evidence of that. \nBased on the time complexity analysis, I expect to find a general discussion summarising the best choice (on average) as a trade-off between computation effort and predictive performance."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700329688212,
                "cdate": 1700329688212,
                "tmdate": 1700329688212,
                "mdate": 1700329688212,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XUXgvTx8G1",
                "forum": "xriGRsoAza",
                "replyto": "WI7JjK2zAP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3547/Reviewer_Yc2u"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3547/Reviewer_Yc2u"
                ],
                "content": {
                    "title": {
                        "value": "Re-response"
                    },
                    "comment": {
                        "value": "Although the authors did not provide a sensitive analysis of hyperparameters, I am still satisfied with the author's response."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642538529,
                "cdate": 1700642538529,
                "tmdate": 1700642538529,
                "mdate": 1700642538529,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "thBfylEXdt",
            "forum": "xriGRsoAza",
            "replyto": "xriGRsoAza",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3547/Reviewer_uQF7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3547/Reviewer_uQF7"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents MILLET a model for Multiple Instance Learning for Locally Explainable Time series classification."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "+ The paper is well written and all the choices are justified and carefully explained\n+ The experimentation is deep\n+ The proposal is novel (to the best of my knowledge)\n+ The references are updated"
                },
                "weaknesses": {
                    "value": "- I would have appreciated a comparison with LIME or with LIMESegments\n- I would have appreciated a comparison against ROCKET or MiniROCKET at least as competitor for the TSC task. Further usage of MILLETS also for ROCKET will completely fulfill the purpose of proposing this approach as a model-agnostic one.\n- To fully understand the paper the reader is constrained to refer to the Supplementary Material. A suggestion is to save some space and anticipate in the main paper some of the details of the Supplementary Material.\n- Experiments with the synthetic dataset should have been performed by varying the number of records, time stamps, classes."
                },
                "questions": {
                    "value": "Questions can be derived from the weaknesses part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3547/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698227070856,
            "cdate": 1698227070856,
            "tmdate": 1699636308827,
            "mdate": 1699636308827,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7buzuaNvjJ",
                "forum": "xriGRsoAza",
                "replyto": "thBfylEXdt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3547/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3547/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer uQF7"
                    },
                    "comment": {
                        "value": "Thank you for your time in reviewing our work. Please see our response to your review below in addition to the general rebuttal given above.  \n\n1. *Other interpretability methods (Weakness 1)*  \nThank you for the suggestion of comparing with LIME and LIMESegments. For LIME, we expect the analysis to uncover a similar issue as to what we discovered with SHAP: LIME will be very expensive to run as it makes many repeated forward passes of the model. We will include a discussion of the alternative interpretability methods in a new section in the Appendix.\n2. *Application of MILLET to ROCKET (Weakness 2)*  \nIn the work we focused on applying MILLET to the deep learning family of TSC methods. As stated in the paper, \u201cwe choose to focus on DL approaches due to their popularity, strong performance, and scope for improvement\u201d (Section 2). We agree that extending MILLET to other families of TSC methods is a worthwhile area of further study, but believe that our analysis against the current versions of SOTA methods is sufficiently comprehensive to demonstrate the effectiveness of MILLET. Convolutional methods such as ROCKET often use a different form of pooling to deep learning methods: while deep learning employs GAP, convolutional models use alternative approaches such as max pooling and proportion of positive values (PPV) pooling. While MILLET should still be applicable to these methods, it is not as trivial as applying it to other deep learning methods and warrants standalone future work. We feel that our time during the rebuttal is best served delivering more insights into MILLET for deep learning methods, as this was the main focus of the work.\n3. *Understanding requires supplementary material (Weakness 3)*  \nWe appreciate there are a lot of references to the appendix throughout the work. We attempted to signpost the relevant appendices as much as possible to aid clarity and indicate where readers should look for further detail. We are happy to improve the clarity of the main body in our revised submission.\n4. *Synthetic dataset should have varied the number of records, etc. (Weakness 4)*  \nOne of the strengths of our synthetic dataset (WebTraffic) is indeed that it facilitates precise control of dataset properties (dataset size, time series length, number of classes, etc.). However, in this work we relied on the 85 UCR datasets to cover a broad range of TSC domains and properties, but WebTraffic could certainly be used for this purpose. We hope that it may be used in derivative works to further investigate interpretable TSC."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244578727,
                "cdate": 1700244578727,
                "tmdate": 1700244578727,
                "mdate": 1700244578727,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "p3JRWyIoim",
                "forum": "xriGRsoAza",
                "replyto": "7buzuaNvjJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3547/Reviewer_uQF7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3547/Reviewer_uQF7"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your reply."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645286018,
                "cdate": 1700645286018,
                "tmdate": 1700645286018,
                "mdate": 1700645286018,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4UnilFr9aX",
            "forum": "xriGRsoAza",
            "replyto": "xriGRsoAza",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3547/Reviewer_pSU6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3547/Reviewer_pSU6"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a novel intrinsically interpretable deep learning model. The authors framed time series classification as a Multiple Instance Learning (MIL) which can highlight the most influential time points in the outcome of the model. This method employs a various techniques such as attention, instance pooling, additive pooling, and conjunctive pooling across an ensemble of deep methods where each method offer different interpretability."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "Nicely written. Well evaluated. Novelty."
                },
                "weaknesses": {
                    "value": "I was not able to identify any weakness."
                },
                "questions": {
                    "value": "Overall, this paper could be a significant algorithmic contribution and I think the authors done amazing job on presenting it. I wonder if the method can be applied to other domains."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3547/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698644393437,
            "cdate": 1698644393437,
            "tmdate": 1699636308750,
            "mdate": 1699636308750,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wcB3cdKREG",
                "forum": "xriGRsoAza",
                "replyto": "4UnilFr9aX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3547/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3547/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pSU6"
                    },
                    "comment": {
                        "value": "Thank you for your time in reviewing our work. Please see our response to your review below in addition to the general rebuttal given above.  \n\nWe believe there could be some scope for applying our work in other machine learning paradigms (we assume this the \u2018other domains\u2019 that you are referring to \u2013 please correct us if this is not the case!). The existing pooling methods we study (instance, attention, and additive) are applicable in other machine learning paradigms (e.g. MIL for vision), so our novel pooling method (conjunctive) should also be applicable elsewhere. We will mention this in a new appendix section."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244057115,
                "cdate": 1700244057115,
                "tmdate": 1700244057115,
                "mdate": 1700244057115,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Oskwhx6xpX",
                "forum": "xriGRsoAza",
                "replyto": "wcB3cdKREG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3547/Reviewer_pSU6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3547/Reviewer_pSU6"
                ],
                "content": {
                    "comment": {
                        "value": "That was exactly what I meant. I check other reviews and your response. In my humble opinion this is a great paper and deserve to be publish."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689176562,
                "cdate": 1700689176562,
                "tmdate": 1700689176562,
                "mdate": 1700689176562,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "K6OGLddFsF",
            "forum": "xriGRsoAza",
            "replyto": "xriGRsoAza",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3547/Reviewer_NeUd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3547/Reviewer_NeUd"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a framework based on Multiple Instance Learning to enhance the interpretability of time series classification models. The framework, MILLET, proposes passing the extracted feature embeddings from any backbone (e.g. FCN, ResNet) through a positional encoding, dropout, and a final pooling layer. Depending on the pooling structure, the method can make the underlying model more interpretable in some settings while also improving performance in others. The authors propose a new pooling method, conjunctive pooling, specifically for time series. MILLET is evaluated through the construction of 12 new models on 85 UCR datasets and a newly introduced synthetic dataset for interpretability evaluation. This approach represents a novel application of MIL to TSC and offers improved interpretability in various domains."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Thanks to the authors for their submission: it contains useful research that shows good research practices while explaining an interesting and novel idea within multivariate time series classification and interpretability. The results of this work will be informative to other researchers and are significant in improving our understanding of applying deep learning methods with time series. Some specific strengths of this research:\n\n- The Multiple Instance Learning presented in this work has more general applications within time series classification than previous work and provides a more robust evaluation of the benefits and drawbacks across a range of tasks, both synthetic and real.\n- MILLET model design adds very little complexity to existing models while contributing improved interpretability. It is flexible enough to work with any backbone model (FCN, ResNet, InceptionTime, and more) while maintaining performance. \n- The proposed synthetic dataset, WebTraffic, provides a helpful contribution to the task of benchmarking time series interpretability. With the ability to scale up to large sizes and the replication of a common time-series use-case it could be a helpful foundation to build-on in the future.\n- Performance of Conjunctive Pooling shows improvements across the class of neural network models for time series classification."
                },
                "weaknesses": {
                    "value": "While the results and paper are generally strong, there are a few areas for improvement particularly as regards to interpretability claims:\n\n- Lack of comparison against previous benchmarks for saliency maps for feature attribution in time series classification from TSR [1], DynaMask [2], WinIT [3], and TimeX [4]. All of these works provide additional synthetic datasets for evaluating interpretability methods and show performance against more general methods like Feature Occlusion and Integrated Gradients. While MILLET seems like to improve on such methods due to the better computational efficiency, it is not thoroughly evaluated in the paper, except in counting the number of forward passes in the difference between SHAP, CAM, and MILLET.\n\n- Interpretability evaluation metrics. It is not clear that AOPCR and NDCG@n can be strictly ported over to the time-series setting. For example, as pointed out, with NDCG@n the time points in the middle of a region of missing data may be considered important by the ground truth, but may not be highlighted by the interpretability method, instead the beginning or end may be highlighted. These scores may be weighted differently for similar outcomes. More discussion around the impact of this is relevant to researchers.\n \n[1] https://arxiv.org/pdf/2010.13924.pdf \n[2] https://arxiv.org/pdf/2106.05303.pdf \n[3] https://arxiv.org/pdf/2107.14317.pdf \n[4] https://arxiv.org/pdf/2306.02109.pdf"
                },
                "questions": {
                    "value": "These questions will help clarify my understanding of the paper. Some of these could benefit from additional analysis in the paper itself:\n\n1/ What is the author\u2019s intuition for the added performance of conjunctive pooling over other pooling methods?\n2/ In Figure 6, are the x\u2019s referring to different pooling methods for MILLET or multiple runs of the Conjunctive pooling model?\n3/ One of the most interesting things about the WebTraffic dataset is the different signatures grounded in real-world patterns. The authors note that the Conjunctive InceptionTime model identifies regions around Spikes and only the start and end of Cutoffs. Does classifier or pooling selection change how the interpretability functions or performs across these various class types? Can this tell us anything more about how the Conjunctive Pooling functions or why it performs better?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3547/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698825588840,
            "cdate": 1698825588840,
            "tmdate": 1699636308651,
            "mdate": 1699636308651,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Pzz9IvV8ys",
                "forum": "xriGRsoAza",
                "replyto": "K6OGLddFsF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3547/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3547/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer NeUd"
                    },
                    "comment": {
                        "value": "Thank you for your time in reviewing our work. Please see our response to your review below in addition to the general rebuttal given above.\n\n1. *Other interpretability methods (Weakness 1)*  \nThank you for the references to additional interpretability metrics for TSC. We will include a discussion in a new appendix section contrasting our approach with these methods.\n2. *Time complexity analysis (Weakness 1 cont.)*  \nWe have begun conducting time complexity analysis of the pooling methods, and will expand Appendix E.3 to include these results. This will go beyond run time and number of forward passes, rather looking at actual time complexity and how the pooling methods scale with the number of classes and time series length. Please see the general rebuttal for initial results.\n3. *Interpretability evaluation metrics (Weakness 2)*  \nWe will include a discussion of additional evaluation metrics in a new appendix section and contrast them with AOPCR and NDCG@n.\n4. *Intuition as to why conjunctive pooling is more effective (Question 1)*  \nIn Section 3.2, when we define conjunctive pooling, we state \u201c\u2026 [conjunctive pooling] is expected to benefit performance as the attention and classifier heads are trained in parallel rather than sequentially, i.e. the classifier cannot rely on the attention head to alter the time point embeddings prior to classification, making it more robust.\u201d Through our new time complexity analysis, we find that conjunctive pooling is also more efficient than additive pooling (the most similar pooling method) when the number of classes is less than the size of the extracted features. This is always true in this work as the feature embeddings are of length 128. \n5. *Figure 6 (Question 2)*  \nEach x in Figure 6 refers to a different pooling method (Instance, Additive, or Conjunctive). Attention pooling is omitted from this diagram as it performs poorly. We will happily modify the caption to clarify this.\n6. *Performance across classes in WebTraffic (Q3)*  \nThrough analysis of the confusion matrix for Conjunctive InceptionTime on the WebTraffic dataset, we found that the model has relatively consistent performance across all classes, i.e. there isn\u2019t one particular class where it fails drastically compared to the other classes. The class where it does the worst is the peak class; occasionally missing peaks (identifying them as class 0 \u2013 None). Further investigation found the interpretations are sometimes still able to identify the correct region, despite the prediction being wrong. This suggests conjunctive pooling is useful for investigating incorrect predictions. We will include an additional appendix section that provides this analysis."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700243862518,
                "cdate": 1700243862518,
                "tmdate": 1700243862518,
                "mdate": 1700243862518,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tONMQqNeDN",
                "forum": "xriGRsoAza",
                "replyto": "Pzz9IvV8ys",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3547/Reviewer_NeUd"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3547/Reviewer_NeUd"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks to the authors for their response to our reviews. The responses helped answer the three questions I highlighted. \n\nIn response to Q2, I appreciate the clarification. It is not clear to me why Attention pooling is omitted simply because of poor performance? Perhaps this result should be included as it contrasts with the similar (even best accuracy) performance to other pooling methods on the WebTraffic dataset as shown in the appendix. \n\nThe response to Q3, and the updated appendix section on conjunctive pooling's performance with the WebTraffic dataset was useful in bringing to light some of the nuances of interpreting the results of these interpretability methods. The separation of examples of finding correct regions with incorrect predictions and the completely incorrect predictions was interesting, and as mentioned, suggests the method could be useful for conducting such investigations and there may be other evaluation metrics to capture this dynamic.\n\nI appreciate the authors time in conducting the time complexity analysis and adding it to appendix E.3, as well as the discussion around evaluation."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3547/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700669280815,
                "cdate": 1700669280815,
                "tmdate": 1700669280815,
                "mdate": 1700669280815,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]