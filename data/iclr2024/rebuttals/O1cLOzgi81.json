[
    {
        "title": "MoDA: Mixture of Domain Adapters for Parameter-efficient Generalizable Person Re-Identification"
    },
    {
        "review": {
            "id": "9WUPA5KyTl",
            "forum": "O1cLOzgi81",
            "replyto": "O1cLOzgi81",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission220/Reviewer_jvA8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission220/Reviewer_jvA8"
            ],
            "content": {
                "summary": {
                    "value": "The paper focuses on the domain generalization problem of person reID. It proposes a mixture of domain adapters to perform parameter-efficient DG reID. The authors argue that the proposed method is parameter-efficient."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "None"
                },
                "weaknesses": {
                    "value": "1. The proposed method lacks novelty. As the author said, the proposed method is an integration of CLIP and adapter. The author just uses some hot techniques in the reid domain. I do not see any insight beyond CLIP and Adapter. I do not think this kind of paper can be accepted in any way.\n2. The parameter-efficient setting may be meaningless in the DG setting; The number of tunable parameters is not a main concern in the DG reid domain. The author argue that, in the DG setting, the number of parameters of classifier heads is very large. As seen in Table 4, the sum of IDs is still very small. I have trained with 100,000 IDs, no problem exists. Also, I am not sure why you focus on this strange aspect. Fewer parameters do not mean a higher training speed and less VRAM used. I do not think the setting and the intuition of this paper is reasonable.\n3. The writing is too poor; The writing of the whole paper is very poor. Also there are some typos: \u201cblcok-aware \u2014> block-aware\u201d. A thorough proofreading is needed. \n4. Not good experiments; in Table 6, your experimental results are much lower than others. I see the tunable parameters are small. But I do not think it is meaningful. Also, have you compared yours with the prompting methods?"
                },
                "questions": {
                    "value": "N/A"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission220/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission220/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission220/Reviewer_jvA8"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission220/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697906807344,
            "cdate": 1697906807344,
            "tmdate": 1699635947920,
            "mdate": 1699635947920,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OsHDMM2UTC",
                "forum": "O1cLOzgi81",
                "replyto": "9WUPA5KyTl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission220/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission220/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer jvA8"
                    },
                    "comment": {
                        "value": "Thanks for your valuable comments firstly.\n\n**Weakness 1 & 4.** Concerning about the novelty and originality, we argue that our contribution lies not only in adopting these technologies to the Domain generalizable Person ReID domain, but also in introducing a novel block-aware voting network to exploit the text feature to utilize the cross-modality information. \n\nWe believe that using both representations from text and vision feature space could strengthen the robustness of the final feature in some way. We understand the need for a detailed analysis and experimental demonstration. We will delve into this matter and provide a robust explanation in our further work.\n\n**Weakness 2.** The most significant benefit of PEFT methods comes from the reduction in memory and storage usage. We understand that this may not be a main concern in the DG ReID domain. However, it indeed helps us to scale to a larger model in downstream tasks with a much fewer tunable parameters and VRAM. And it can also alleviate the catastrophic forgetting of large models when we fine-tuning them with a downstream datasets. So, we still think it is reasonable and valuable to exploit the PEFT methods to ReID tasks including DG ReID.\n\n**Weakness 3.** We will undertake a rigorous proof-reading and ensure an immaculate grammar and improved fluency in the revised paper.\n\nThank you again for your detailed and constructive feedback. We sincerely take your criticisms and will improve our paper in the revision.\n\nBest Regards."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission220/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675059180,
                "cdate": 1700675059180,
                "tmdate": 1700675059180,
                "mdate": 1700675059180,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u4cC9Q1rbu",
                "forum": "O1cLOzgi81",
                "replyto": "OsHDMM2UTC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission220/Reviewer_jvA8"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission220/Reviewer_jvA8"
                ],
                "content": {
                    "comment": {
                        "value": "Many thanks for the authors' reply. It is a clear reject."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission220/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675338170,
                "cdate": 1700675338170,
                "tmdate": 1700675338170,
                "mdate": 1700675338170,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7f0w3Pjc08",
            "forum": "O1cLOzgi81",
            "replyto": "O1cLOzgi81",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission220/Reviewer_DXS9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission220/Reviewer_DXS9"
            ],
            "content": {
                "summary": {
                    "value": "In this work authors present a method that can handle the domain generalizable person re-identification. Inspired by the previous works in this area, authors designed a network in which they leverage the CLIP Re-ID model with a set of domain specific adapters as well as a domain generalizable adapter termed as Mixture of domain adapters (MoDA) that can efficiently scale up to larger backbones models. Another advantage of this approach is that the expert opinions are mixed even at intermediate stages so that the model can leverage finer information mixing."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed approach is reasonable and clearly outlined. Despite sharing similarities with previous works, authors managed to design expert mixing strategy that results in a more parameter efficient architecture when scaling to larger backbone models for variety of source domains. Additionally, the proposed method achieves competitive results to that of state-of-the-art models when tested in equal environment under the same evaluation protocols."
                },
                "weaknesses": {
                    "value": "There is no detailed discussion on the obtained results beside merely stating the method achieves better results. It is expected that some insight to be offered as why to the proposed method performs better or worse compared to certain methods. While it is important to know the method has a better accuracy, it is equally as important to know why and how that improvement occurs. Additionally, would be possible for authors to provide some failure cases and some future direction that the designed framework can be enhanced?"
                },
                "questions": {
                    "value": "Could authors provide some explanation on why the proposed method performs slightly better than META under protocol one but META performs better by a good margin under protocol 2?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission220/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698794341394,
            "cdate": 1698794341394,
            "tmdate": 1699635947832,
            "mdate": 1699635947832,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "9jdQGMbC9T",
                "forum": "O1cLOzgi81",
                "replyto": "7f0w3Pjc08",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission220/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission220/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer DXS9"
                    },
                    "comment": {
                        "value": "Firstly, we appreciate your thoughtful analysis and the time taken to critically review our work. \n\n**Weekness 1.** Regarding your comment on the discussion of the obtained results, we understand the necessity of providing detailed insights into why and how the proposed method performs better or worse compared to certain methods. In the revised version, we plan to include a more comprehensive analysis, along with a section discussing the reasons behind the improvements. \n\n**Weekness 2 & Question 1.** We will discuss more about the analysis of our experiment results and limitation of our method. And we will try to provide some important insights and contribute to the future direction of our research.\n\nAnd thanks for your positive feedback on the clarity of our proposed approach and its efficiency, which really motivates us to further improve our paper. Thank you again for your time and insightful comments and look forward to further discussion.\n\nBest Regards"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission220/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700671672431,
                "cdate": 1700671672431,
                "tmdate": 1700671756321,
                "mdate": 1700671756321,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "l4kQiKBvZt",
                "forum": "O1cLOzgi81",
                "replyto": "9jdQGMbC9T",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission220/Reviewer_DXS9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission220/Reviewer_DXS9"
                ],
                "content": {
                    "title": {
                        "value": "Final rating"
                    },
                    "comment": {
                        "value": "Based on the other reviewers comments and the provided responses by authors, I do not believe that the manuscript is ready for publication in its current form, a more critical analysis of the results and further explanation could improve the quality of the paper and provide more insight into the proposed framework. Therefore, I keep my original rating.\n\nBest,\nDXS9"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission220/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700674266504,
                "cdate": 1700674266504,
                "tmdate": 1700674266504,
                "mdate": 1700674266504,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BQ6CN1G9QI",
            "forum": "O1cLOzgi81",
            "replyto": "O1cLOzgi81",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission220/Reviewer_QbiD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission220/Reviewer_QbiD"
            ],
            "content": {
                "summary": {
                    "value": "This study focuses on Domain Generalizable ReID (DG ReID) and introduces a new method called Mixture of Domain Adapters (MoDA) to overcome the problem that the model parameters are too large. The proposed MoDA method incorporates Global-Adapter, Export Adapter, Voting Network, and the CLIP technique for DG ReID, resulting in competitive results compared to state-of-the-art methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1)\tThe authors exploit the Global-Adapter, Export Adapter, and Voting Network for DG ReID task."
                },
                "weaknesses": {
                    "value": "1)\tIn Tables 1 and Table 2, the bolded font is misleading, and the performance of the ACL and META methods outperforms the proposed method. In particular, the model parameter of ACL is not much larger than that of the proposed method, but the performance of ACL is much higher than that of this paper. \n2)\tAs shown in Table 3, the Global-Adapter, Export-Adapter, and Voting Network proposed in this paper did not bring significant performance improvement, which makes me doubt the effectiveness of the method proposed in this paper. \n3)\tAbbreviations that appear for the first time need to indicate the full name. For example, in the third line of the abstract, the DG ReID appears for the first time without giving a full description. \n4)\tThe GELU in Figure 2 is not given a full description, as well as in the text, which can confuse the reader. \n5)\tThe use of mathematical symbols is not rigorous enough. The i in Formula 1 represents both the image and the index, which is easy to cause ambiguity. \n6)\tIn Formula 3a, the definition of MA is not stated. \n7)\tIn Formula 5, the definition of d_p and d_a are not stated. \n8)\tThe font size in all the figures is too small. \n9)\tEnglish writing needs improvement. \n10)\tSome grammatical errors. In the top line of section 3.2, \u201cdelineates\u201d -> \u201cdelineate\u201d"
                },
                "questions": {
                    "value": "Why does this paper not directly use images as the input to the model, but increase the optimization operation of text tokens? The implication of text tokens needs to give a more detailed analysis and experimental demonstration."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Discrimination / bias / fairness concerns"
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission220/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698890916368,
            "cdate": 1698890916368,
            "tmdate": 1699635947745,
            "mdate": 1699635947745,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "S66IdKQMsV",
                "forum": "O1cLOzgi81",
                "replyto": "BQ6CN1G9QI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission220/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission220/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer QbiD"
                    },
                    "comment": {
                        "value": "First of all, sincerely thanks for your time and valuable comments.\n\n**Weakness 1.** Regarding the confusion with Tables 1 and 2, it is our mistake for the potential misunderstanding arising from the bolded font. In our upcoming revision, we will clarify the numberings, and ensure a more suitable representation of the comparative performances.\n\n**Weakness 2.** Actually, we will concatenate the aggregated feature and the global feature branch feature as the final representation to perform the inference, which is inspired by META[1]. It is unclear in the Table 3 and it causes the confusion about the mixture of adapters and the concatenation of features. And then it makes the effectiveness of our proposed method and modules not as prominent. And in the revision, we will provide a clearer distinction and a more detailed explanation to strengthen our improvement. \n\n**Weakness 3-10.** Concerning the font size in our figures, typos, English grammar and writing style, we truly appreciate your patience. And we will undertake a rigorous proof-reading and ensure an immaculate grammar and improved fluency in the revised paper.\n\n**Question 1.** Actually, the model only uses images as the input both at training and inference stage. The reason why we extraly use the text tokens is that we hope to leverage a **cross-modality** approach to enhance the generalization ability of DG ReID. We believe that using both representations from text and vision feature space could strengthen the robustness of the final feature in some way.  We understand the need for a detailed analysis and experimental demonstration. We will delve into this matter and provide a robust explanation in our further work.\n\nThank you again for your comprehensive feedback , and we believe it will lead to significant improvements in our manuscript. Grateful for your time and effort.\n\nBest Regards.\n\n[1] Boqiang Xu, Jian Liang, Lingxiao He, and Zhe Sun. Mimic embedding via adaptive aggregation: Learning generalizable person re-identification. In European Conference on Computer Vision, 2021."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission220/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700673790009,
                "cdate": 1700673790009,
                "tmdate": 1700673790009,
                "mdate": 1700673790009,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "qi72aJZ7DW",
            "forum": "O1cLOzgi81",
            "replyto": "O1cLOzgi81",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission220/Reviewer_GQT9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission220/Reviewer_GQT9"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a Mixture of Domain Adapters (MoDA) framework for DG ReID, which imports CLIP and adapter to achieve the parameter efficiency during the scaling up of MOE."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Although CLIP and adapter are not new, the voting network using of the CLIP-trained ID-specific tokens to save the inference time is a reasonable and interesting idea."
                },
                "weaknesses": {
                    "value": "1. The improvements in Table.1/2 is slight. The performance is even lower than SOTA. As the advantage of the proposed MoDA is based on the tunable params. I suggest the author to provide a comparison with the same level of tunable params, to see whether the performance can beat other SOTAs.\n2. In Eq.11, I do not get the meaning of the \u0393+agg and \u0393+expert. They are two hardest positive samples, NOT two distances as described, right?\n3. What is the difference between \"Mixture of Expert Adapters\" and \"w/o Global Adapter in Mixture of Experts and Global\". Moreover, in my opinion, the \"Mixture of Expert Adapters\" contains no global adapter, what's the meaning of \"w/o Global Adapter\" in \"Mixture of Expert Adapters\"?\n4. Where is Table.5 in Section4.4?\n5. Many mistakes in the writing, such as \"The expter branch\" in Fig.3 title."
                },
                "questions": {
                    "value": "I don't like the writing of this paper, but the idea is reasonable. So I prefer to borderline."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission220/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698906872803,
            "cdate": 1698906872803,
            "tmdate": 1699635947667,
            "mdate": 1699635947667,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5iqI8VKomS",
                "forum": "O1cLOzgi81",
                "replyto": "qi72aJZ7DW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission220/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission220/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer GQT9"
                    },
                    "comment": {
                        "value": "Firstly, thank you for taking the time to review our work and for your insightful comments.\n\n**Weakness 1.** Regarding the performance shown in Tables 1 and 2, we acknowledge your suggestion about a comparison with the same level of tunable parameters. We believe that this recommendation will make the comparisons fairer and will emphasize parameter-efficiency advantages of our proposed method.\n\n**Weakness 2.**  Actually, \u0393+agg and \u0393+expert are hardest positive distances but not the specific samples. As we expect the aggregated features to be as similar as possible to those derived by domain expert, so we utilize this loss function by **minimizing the difference of the distances** between them and their respective hardest positive/negative sample.\n\n**Weakness 3.** Actually, we will concatenate the aggregated feature and the global feature branch feature as the final representation to perform the inference, which is inspired by META[1]. \n\n(a) \"Mixture of Expert Adapters\" means that we only mix the expert adapters by the voting network in every block without global branch, and only concatenate the aggregated feature and global feature at last as we mentioned above.\n\n(b) \"Mixture of Expert Adapters\" means that we mix the expert adapters and global adapter by the voting network in every block and we still concatenate the aggregated feature and global feature at last. \n\nAnd \"w/o Global Adapter\" means that we only use the aggregated feature as final representation without concatenating with the global feature. And it applies to \"w/o Expert Adapters\" as well. \n\nIt is unclear in the current version and it causes the confusion about **the mixture of adapters** and **the concatenation of features**. And in the revision, we will provide a clearer distinction and a more detailed explanation.\n\n**Weakness 4.** It seems like a typo and we actually mean the Table 3 in Section4.4. We will fix it in our revised version.\n\n**Weakness 5** **&** **Question 1.** We appreciate you pointing out typos and writing mistakes. We will thoroughly proofread the paper to avoid such errors in our revised version.\u00a0\n\nFinally, thank you for your positive comment about the worthiness and strengths of our idea. We will diligently revise the paper and make it more readable, understandable, and highlight our contributions. Thank you again for your time and look forward to further discussion.\n\n[1] Boqiang Xu, Jian Liang, Lingxiao He, and Zhe Sun. Mimic embedding via adaptive aggregation: Learning generalizable person re-identification. In European Conference on Computer Vision, 2021."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission220/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700670438537,
                "cdate": 1700670438537,
                "tmdate": 1700671788263,
                "mdate": 1700671788263,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QEWmOPIQCy",
            "forum": "O1cLOzgi81",
            "replyto": "O1cLOzgi81",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission220/Reviewer_ZKXT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission220/Reviewer_ZKXT"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose a novel MoE-based Domain Generalizable ReID method called \"Mixture of Domain Adapters\" (MoDA) to address the challenges faced by existing DG ReID methods. MoDA leverages Adapter-tuning and CLIP in a parameter-efficient manner to mitigate the need for extensive fine-tuning of backbone, classifier head, and expert parameters. Their approach aims to handle the large number of person IDs typically encountered in DG ReID, allowing for better scalability to larger vision models. Many experiments demonstrate that MoDA achieves competitive or even superior results compared to state-of-the-art methods while requiring significantly fewer tunable parameters."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors of this paper have frozen the weights of CLIP and integrated the \"Mixture of Domain Adapters\" (MoDA) module for fine-tuning. They applied this technique to ReID and achieved promising experimental results. The performance seems good."
                },
                "weaknesses": {
                    "value": "This paper encompasses several technologies, including CLIP, PEFT (Adapter), MoE, etc., but none of these were introduced by the authors themselves. Instead, existing technologies were adapted for use in the Re-ID domain. The authors claim, \"To the best of our knowledge, our work is the first one to exploit CLIP to DG Person ReID and also the first one to attempt PEFT methods, such as adapter, for DG Person ReID.\" However, CLIP-ReID [1] and Clip-adapter [2] have already explored these aspects. Furthermore, AdaMix [3] and MixDA [4] have also applied MoE adapter techniques. Utilizing these technologies in the Re-ID field does not necessarily constitute a novel contribution or innovation. It is hoped that the authors can introduce more of their own design elements and emphasize improvements upon prior work.\n\nPerhaps visualizing the relationship between MoE adapters and specific domain or person characteristics could be beneficial. For instance, in Re-ID tasks, conducting a visual analysis to understand when and under what circumstances certain adapters are activated and whether any patterns emerge could provide valuable insights.\n\nIn terms of writing, the paper reads more like a technical report combining elements MoE, CLIP, and Adapter based PEFT without a clear focus or coherence. It is suggested to start with a specific problem within DG Re-ID as the motivation, introduce the proposed solution, and emphasize the uniqueness of this approach. The method should not merely be a simple combination of existing techniques, and authors should highlight the distinctive contributions. \n\nThere is room for improvement in the style of the figures and tables as well.\n\n[1] Li Siyuan, Li Sun, and Qingli Li. CLIP-ReID: exploiting vision-language model for image re-identification without concrete text labels. AAAI. 2023.\n[2] Gao P., Geng S., Zhang R., Ma, T., Fang R., Zhang Y., Li H., and Qiao Y. Clip-adapter: Better vision-language models with feature adapters. arXiv preprint arXiv:2110.04544.\n[3] Wang, Yaqing, et al. Adamix: Mixture-of-adapter for parameter-efficient tuning of large language models. arXiv preprint arXiv:2205.12410.\n[4] Diao, Shizhe, et al. Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models Memories. arXiv preprint arXiv:2306.05406."
                },
                "questions": {
                    "value": "see weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission220/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699324805024,
            "cdate": 1699324805024,
            "tmdate": 1699635947600,
            "mdate": 1699635947600,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PbxZhSjyN4",
                "forum": "O1cLOzgi81",
                "replyto": "QEWmOPIQCy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission220/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission220/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer ZKXT"
                    },
                    "comment": {
                        "value": "Thanks for your insightful and valuable comments firstly.\n\n**Weakness 1.** Concerning about the novelty and originality, we argue that our contribution lies not only in adopting these technologies to the Domain generalizable Person ReID domain, but also in introducing a novel block-aware voting network to exploit the text feature to utilize the cross-modality information. \n\nIndeed, both Adamix[1] and MixDA[2] incorporate the ideas of adapters and MoE. However, in Adamix, they actually employ distinct FC-Down and FC-Up layers within one single adapter structure and primarily utilize a randomly routing mechanism. On the other hand, MixDA also does not leverage a cross-modal voting network approach. And neither of these methods specifically address domain generalization or exploitation of CLIP.\n\nWe will further clarify our own design elements and improvement in the revised version of the paper more clearly.\n\n**Weakness 2.** Visualizing the relationship between MoE adapters and specific domain or person characteristics is really a valuable suggestion, which may help us to analyze our method and help readers better understand the effectiveness of our approach. We will incorporate this valuable suggestion by utilizing methods, such as T-SNE, in our revised paper and we believe insights from this analysis can enhance our contribution further.\n\n**Weekness 3.** Regarding the clarity and coherence of the paper, we appreciate your feedback. We will diligently revise the manuscript starting from the specific problem within DG Re-ID, introducing the solution we propose, and emphasizing our unique approach and its contributions.\n\nLastly, we acknowledge that the figures and tables presentation style could be improved. We plan to polish those elements to render them more comprehensible and visually appealing.\n\nThank you again for your constructive comments and we look forward to further discussion!\n\nBest regards.\n\n\n[1] Wang, Yaqing, et al. Adamix: Mixture-of-adapter for parameter-efficient tuning of large language models. arXiv preprint arXiv:2205.12410. \n\n[2] Diao, Shizhe, et al. Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models Memories. arXiv preprint arXiv:2306.05406."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission220/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666428507,
                "cdate": 1700666428507,
                "tmdate": 1700671800155,
                "mdate": 1700671800155,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]