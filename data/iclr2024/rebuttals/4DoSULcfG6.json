[
    {
        "title": "Chameleon: Increasing Label-Only Membership Leakage with Adaptive Poisoning"
    },
    {
        "review": {
            "id": "xnmxL4FtBP",
            "forum": "4DoSULcfG6",
            "replyto": "4DoSULcfG6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3924/Reviewer_JKci"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3924/Reviewer_JKci"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a data poisoning attack, called Chameleon, to enhance the privacy leakage due to label only membership inference attacks. It first shows that current attacks that aim to enhance privacy leakage via poisoning are not effective in label-only MIA threat model. The attack fails because after poisoning both the IN and OUT models misclassify the target samples. To improve the attack efficacy, Chameleon tailors the number of replicas of poisoning samples for each challenge sample. The paper shows that such poisoning significantly improves label-only MIA accuracy especially at low FPRs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Chameleon idea is elegant and easy to implement\n- Intuition and other aspects of the attack are well explained"
                },
                "weaknesses": {
                    "value": "- Chameleon is an expensive attack\n- I am not sure how will such attack be useful in practice due to the computations involved\n- Some parts of the paper need to improve presentation, e.g., theoretical attack and figure 1"
                },
                "questions": {
                    "value": "The attack proposed is very elegant in that it is easy to implement and outperforms prior attacks. Also, the explanation of the attack is  clear and easy to understand. The paper also does a fair job in evaluating their proposed attack. Overall I think this is a good paper, but  I have the following concerns:\n\nAttack computation cost and utility:\n- Chameleon is an expensive attack given the number of models one has to train to find the right number of poisoning sample replicas. Can authors discuss the compute cost involved? I didn\u2019t see any discussion in the main paper.\n- Given the high computation cost and the fact that modern ML model architectures are generally huge, I wonder where will this attack be useful? Which type of adversaries can afford it? It will be good to clearly discuss these aspects.\n\nSome concerns about the evaluations\n- For C100, Chameleon adds on average 0.6 replicas of poisoning samples, which means there are 40% data which need no poisoning. This means MIAs without any poisoning should work well. But this is not reflected in Table 1 results. Clarify.\n- Minor: Given that modern ML systems have generally very large and multimodal models, it might be useful to have evaluations on large and/or multimodal models.\n\nClarity of paper:\n- Figure 1 is not readable: I could not understand what it is trying to convey. Please clarify\n- Theoretical attack section currently does not clearly explain what is the attack and why this analysis is needed if the same conclusions can be drawn from empirical analysis."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3924/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3924/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3924/Reviewer_JKci"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3924/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698608212121,
            "cdate": 1698608212121,
            "tmdate": 1699636352777,
            "mdate": 1699636352777,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kkDD6xwIWW",
                "forum": "4DoSULcfG6",
                "replyto": "xnmxL4FtBP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3924/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3924/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their insightful comments. We address the main concerns below.  In the paper, we marked the modified text in blue, for ease of identification.\n\n**Attack computation cost and utility:**\n\nWhile we acknowledge that shadow model training incurs a cost, it is crucial to note that our attack trains a fixed number of shadow models, independent of the number of challenge points. In our experiments for CIFAR-10, inferring membership over 500 challenge points simultaneously, our attack trains a total of 80 shadow models and achieves a 20.8x TPR(@1%FPR) improvement over the Decision-Boundary (DB) attack. Despite the training cost, it still remains notably lower than that of state-of-the-art confidence based membership inference attacks like LiRA (Carlini et al., 2022) and TruthSerum (Tramer et al., 2022), which typically require 128 shadow models for optimal performance.\n\nAdditionally, by careful selection of hyperparameters in Algorithm 2, our attack achieves a 12.1x TPR(@1%FPR) improvement over the DB attack while requiring to train only 16 shadow models. In Appendix D, we provide a detailed cost analysis associated with running our attack along with end-to-end running time comparison between ours and DB attack. Interestingly, our computationally constrained variant, using 16 models, nearly matches the runtime of the DB attack while significantly outperforming it in terms of TPR (12.1x improvement).\n\n\n**Evaluation Concerns:**\n\n- **CIFAR-100:** Table 1 displays TPR values at low FPRs, with False Positives in CIFAR-100 coming from 60% of the challenge points that require poisoning to succeed in the MI test. Existing Decision-Boundary attack indeed demonstrates a strong performance (as shown in Table 2) but on average case metrics with AUC and MI accuracy being 84% and 81%, respectively. Notably, these metrics are high because of the 40% of challenge points that succeed the MI test without requiring poisoning.\n\n\n- **Large Models:** We evaluated our attack on a larger model (VGG-16) with 138 million trainable parameters. In this scenario we observed an increase in MI leakage with respect to the smaller ResNet-18 model. Results for this setup are given in Section 4.4 (Page 9) and Table 3 (Appendix A.3, Page 13).\n\n**Clarity:**\n\n- **Figure 1:** The objective of Figure 1 is to show how two distinct challenge points require different numbers of poisoned replicas to create a separation between  the IN and OUT model confidences. We have updated Figure 1 to make it more readable.\n\n\n- **Theoretical/Optimal Attack:** We aim to formulate an optimal attack that maximizes the TPR at a fixed FPR of x%, under a specified list of assumptions (detailed in Appendix B.1).  Our objective of constructing the optimal attack is two-fold:\n\n    - To understand how the maximum attainable TPR (@x%FPR) is affected when we vary the number of poisoned replicas in the training set.\n    - To analyze if our Chameleon attack follows the optimal attack\u2019s behavior or deviates from it under poisoning.\n\nWe observe that the maximum attainable TPR improves and then declines as we add more poisoned replicas in the training set  indicating that excessive poisoning in\nthe label-only scenario adversely impacts the maximum achievable TPR. A similar behavior is also observed when we run the Chameleon attack on real data showing that it closely aligns with the optimal attack behavior. We have revised Section 3.3 for improved clarity on the optimal attack and its objectives.\n\nWe hope our response has addressed the concerns expressed in the review. We will be happy to provide additional clarifications if needed."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3924/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700089470335,
                "cdate": 1700089470335,
                "tmdate": 1700089470335,
                "mdate": 1700089470335,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fLuYyqVPAg",
                "forum": "4DoSULcfG6",
                "replyto": "kkDD6xwIWW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3924/Reviewer_JKci"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3924/Reviewer_JKci"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response!"
                    },
                    "comment": {
                        "value": "- Time complexity: Thanks for clarifying. The part that talks about optimized algorithm for attack is not visible in the main body of the paper and I think it would be good if authors give Algorithm 2, Appendix C, and Appendix D more visibility in the main body as this is the algorithm that probably will be useful not the Algorithm 1. \n\n- Evaluation concern: \n1. Cifar100: Is this your hypothesis or you know that all the false positives are from 60% of the points? \n2. For larger models, why have you increased number of training epochs? Wouldn't that lead to unnecessary overfitting?\n\n- Clarity:\n2. Theoretical attack: If I understand correctly, optimal attack should always outperform other attacks, including your attack. Why is this not the case here?\n\n- **New concerns**:\n1. How do you choose y' when poisoning? Do you randomly pick some label other than true label? What is the impact of choice of y' on performances of your and baseline MIAs? If you haven't already, please add this somewhere in the paper.\n\n2. Evaluation setup:  Given that label only attacks are designed to be more practical, how can attackers have the data from exactly the same distribution of the original training data? This is a strong assumption, and hence, can you provide some evaluations where attacker cannot have data from exactly the same distribution?\n\n3. Finally, I see that you discuss DP as a defense, but it is tough to achieve good privacy-utility trade-offs with DP. So, I was wondering if you can comment on (or even better show) how would these attacks work with simpler defenses, e.g., regularization or per-example gradient clipping? \n\n4. Also, can you provide train and test accuracies of models you are attacking? Before and after poisoning? This is important because if for 500 challenge points, attacker introduces D_p of size 2000 (~10% of training data), and if that leads to poor model performances, such model will never be deployed, and hence, will not be available for attacker to query. \n\n5. If I understand correctly, this is a targeted label-only MIA where you report results only on the set of challenge points. If yes, how have you ensured significance of the results reported? Do you repeat the experiments before reporting results?"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3924/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700425108703,
                "cdate": 1700425108703,
                "tmdate": 1700425108703,
                "mdate": 1700425108703,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ZkNlonuMDX",
                "forum": "4DoSULcfG6",
                "replyto": "1fp3qOQJX4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3924/Reviewer_JKci"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3924/Reviewer_JKci"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the response!"
                    },
                    "comment": {
                        "value": "Thanks for the detailed response. I think most of my concerns are addressed, except about the assumptions about the adversary's knowledge of distribution of the training data. I understand that prior works do not consider real-world use cases either, but I feel it should be included in newer works as this one."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3924/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700613503189,
                "cdate": 1700613503189,
                "tmdate": 1700613503189,
                "mdate": 1700613503189,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5XRzf8pXrR",
            "forum": "4DoSULcfG6",
            "replyto": "4DoSULcfG6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3924/Reviewer_ECEy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3924/Reviewer_ECEy"
            ],
            "content": {
                "summary": {
                    "value": "The key contribution of this paper is to present a poisoning strategy to enhance the success of label-only membership inference attacks. The paper first shows that an existing poisoning regime negatively impacts the label-only attack's success and proposes a new way to calibrate the number of poisoning points to inject. And then, the paper proposes a way to construct shadow models and perform membership inference. In evaluation, the paper demonstrates that poisoning can increase the TPR by an order of magnitude while preserving the model's performance. The paper also analyzes the impact of attack configurations and further tests if (and also shows) DP reduces the attack success."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The paper presents a new poisoning attack for enhancing label-only MIs.\n2. The paper shows the poisoning can increase the attack success by 18x.\n3. The paper is well-written"
                },
                "weaknesses": {
                    "value": "1. The poisoning seems to be a straightforward adaptation of Tramer et al.\n2. The proposed label-only MI seems to be impractical.\n3. (Sec 3.3) The claim about \"theoretical\" attack is unclear.\n\n\nOverall comments:\n\nI agree it is a nice extension of existing work (Tramer et al.) to label-only settings. At the same time, this attack itself and the poisoning strategy are not surprising. So, my impression of this paper is slightly below the acceptance threshold. But if there are surprising factors that I've missed, I am not willing to fight for rejection.\n\n\n**[Straightforward Extension]**\n\nOf course, existing poisoning could not work well against an adversary who only observes hard labels. The adversary cannot \"exploit\" the impacts of poisoning until there is a change in the target's label. If too many are injected, the attacker may not know whether the target is a member. So, in the label-only settings, the key is to calibrate the number of poisoning samples. It is therefore not surprising in Section 3.2 that an \"adaptive\" poisoning strategy is needed.\n\n\n**[Practicality of This Poisoning]**\n\nHowever, I believe that choosing the right threshold $t_p$ is more challenging than shown in this paper. The paper assumes that the adversary can know the \"underlying distribution.\" \n\nBut considering that the label-only attacks are for studying the practicality in the \"true black-box\" settings (e.g., hard-labels), I wonder how well this attack can perform when there's a slight distributional difference between the training data an adversary uses and the victim's. Indirectly, the ablation study shows the proposed label-only attack is a bit sensitive to the choice of a poisoning threshold. \n\nIn practical scenarios, when a practitioner wants to check the risks of \"practical\" label-only membership leakage, the proposed attack may not be a useful one to use.\n\n\n**[Theoretical Attacks (Sec 3.3)]**\n\n(1) In most cases, the theoretical analysis means the best possible attack that an adversary can perform under a specific attack configuration. But I am not sure whether the paper presents the same.\n\n(2) I am a bit unclear on how the paper theoretically analyzes the impact of poisoning samples on the leakage. It depends on many factors, such as the training data and/or the choice of a model and a training algorithm.\n\nI think the section could a bit mislead readers."
                },
                "questions": {
                    "value": "My questions are in the detailed comments in the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No concern about the ethics."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3924/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699154794182,
            "cdate": 1699154794182,
            "tmdate": 1699636352699,
            "mdate": 1699636352699,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AZ5g55lRsm",
                "forum": "4DoSULcfG6",
                "replyto": "5XRzf8pXrR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3924/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3924/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the careful reading of the paper, and the detailed comments. We address the main concerns below.  In the paper, we marked the modified text in blue, for ease of identification.\n\n**Straightforward Extension:**\n\nThe Truth Serum attack by Tramer et al. does not work in label-only settings, and we had to add several new components to account for this challenging threat model: (1) We introduce a new adaptive poisoning strategy that is critical for the attack's success in label-only settings; (2) We carefully craft queries based on our proposed concept of membership neighborhood to improve the attack\u2019s success. Both of these ideas are novel and have not been used in prior work on membership inference. In more detail, our contributions are as follows:\n\n- **Poisoning Strategy:**\n    - **Single challenge point:** Algorithm 1 is designed to adaptively calibrate the number of  poisoned replicas k such that the label for the given challenge point flips with respect  to the OUT model, creating a separation between IN and OUT models.\n\n    - **Multiple challenge points:** The first crucial observation we make is that each challenge point  requires a different  number of poisoned replicas k to create a separation between the IN and OUT models. In Fig. 5(a), we show that fixing the same k across all challenge points does not yield a high TPR.  In Appendix C, we then discuss that trivially repeating Algorithm 1 for each challenge point makes our attack impractical, as the number of shadow models to be trained scales up with the number of challenge points. Consequently, we propose Algorithm 2, which tackles this drawback and trains a fixed number of shadow models, independent of the number of challenge points. \n\n- **Membership Neighborhood:** We introduce a systematic approach for selecting close neighbors for each challenge point by reusing the shadow models trained in the poisoning stage. With only 64 queries per challenge point, our attack is >39x more query-efficient than the prior Decision Boundary attacks. Figure 6(b) demonstrates how using heuristic augmentations instead of our systematic approach for querying the target model can cause a significant drop in the TPR.\n\n**Theoretical/Optimal Attack** (Appendix B): \n\nThe analysis indeed builds an optimal attack that maximizes TPR at a fixed FPR of x% for a given attack configuration. The attack configuration (as detailed in Appendix B) uses the following assumptions:\n\n- The posterior distribution of the model parameters given the training data satisfies Equation (2). This is a common assumption made by prior MI works Sablayrolles et al. (2019) and Ye et al. (2022)  when modeling MI leakage.\n- We also assume the 0-1 loss function is used in order to bound the total loss in  Equation (2). \n- Finally, we assume that under poisoning, the model parameter selection for classifying  a specific challenge point z_1 is solely dependent on z_1,  its membership m_1 and the poisoned dataset D_p. This assumption is based on the findings from TruthSerum (Tramer et al., 2022), where empirical observations revealed that multiple poisoned models aimed at inferring the membership of z_1 had very similar logit (scaled confidence) scores on challenge point z_1. This observation implied that a poisoned model\u2019s prediction on z_1 was largely influenced only by the presence/absence of the original point and the poisoned dataset D_p.  \n\nWe have also revised Section 3.3 to improve clarity of our optimal attack and its objectives, under the attack configuration mentioned above.\n\n**Practicality of Poisoning:**\n\nWe adhere to the same data distribution assumption that has been followed in the MI literature (including prior label-only attacks): Shorki et al., 2017, Yeom et al., 2018, Leino et al., 2020, Choo et al., 2021, Li et al., 2021,  Carlini et al., 2022, Tramer et al, 2022 and Wen et al., 2023.\n\nWe hope our response has addressed the main concerns expressed in the review. We will be happy to provide additional clarifications if needed."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3924/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700089213342,
                "cdate": 1700089213342,
                "tmdate": 1700089213342,
                "mdate": 1700089213342,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "cd9KEWGCAU",
            "forum": "4DoSULcfG6",
            "replyto": "4DoSULcfG6",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3924/Reviewer_uopX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3924/Reviewer_uopX"
            ],
            "content": {
                "summary": {
                    "value": "This paper targets at Membership Inference (MI), in which an attacker seeks to determine whether a particular data sample was included in the training dataset of a model. In contrast to the most of work in this area, the paper considers a less favorable setting: the attacker has access only to the predicted label on a queried sample, instead of the confidence level. I think this is an important problem, which should be interesting to the communities of both DP and privacy attack. To address this challenge, the paper proposes a new\nattack Chameleon that leverages adaptive data poisoning to achieve better accuracy than the previous work."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper proposes a new attack Chameleon that leverages adaptive data poisoning to achieve better accuracy than the previous work.\n2. The paper observes an interesting phenomenon: for different challenge point, the sweet spot of the number of samples needed in the data poisoning is different. The paper also proposes a theory to reflect this phenomenon.\n3. Various experiments have shown the advantages of the new method."
                },
                "weaknesses": {
                    "value": "Although the attack and the observation is interesting, I think the paper has the following weak points:\n\n1. Time complexity. Clearly from Algorithm 1, to run the adaptive poisoning, the attacker has to run the training model much more times than the baseline algorithms, making the proposed algorithm less practical. However, the paper touches little about this topic, and does not provide any comparison in the experiment section. I think this information is crucial for the readers to better understand and appreciate the proposed algorithm.\n\n2. Multiple challenge points. Usually in practice, the attacker needs to attack multiple challenge points instead of the only one. Although the paper briefly discusses this in the appendix, I think it is far from enough. Specifically, Algorithm 2 is just a simple generalization of Algorithm 1, neglecting many interesting and important problems due to more than one challenge points. For example, the problem of time complexity becomes even worse. Furthermore, due to the correlations of different challenge points, it is not clear how Algorithm 2 performs. Considering an extreme case when there are two challenge points opposing each other, it is possible after k_max iterations, the algorithm can not find meaningful k_i for both points simultaneously.\n\n3. Clarity (minor points). The paper needs to improve the clarity. For example, many definitions are used without being defined, e.g., LIRA, challenge point, in+out model. It is better to provide those definitions in the preliminary to make the paper more self-contained."
                },
                "questions": {
                    "value": "Please refer to the section above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3924/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3924/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3924/Reviewer_uopX"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3924/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699207947782,
            "cdate": 1699207947782,
            "tmdate": 1699636352625,
            "mdate": 1699636352625,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pSUjrftOAe",
                "forum": "4DoSULcfG6",
                "replyto": "cd9KEWGCAU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3924/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3924/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their comments and suggestions.  We resonate strongly with the comment on the importance of label-only membership inference in private ML, and we hope that our paper provides a step forward in this area.  We address the main concerns below.  In the paper, we marked the modified text in blue, for ease of identification.\n\n**Time Complexity:**\n\nWhile we acknowledge that shadow model training incurs a cost, it is crucial to note that our attack trains a fixed number of shadow models, independent of  the number of challenge points. In our experiments for CIFAR-10, inferring membership over 500 challenge points simultaneously, our attack trains a total of 80 shadow models and achieves a 20.8x TPR(@1%FPR) improvement over the Decision-Boundary (DB) attack. Despite the training cost, it still remains notably lower than that of state-of-the-art confidence based membership inference attacks like LiRA (Carlini et al., 2022) and TruthSerum (Tramer et al., 2022), which typically require 128 shadow models for optimal performance.\n\nAdditionally, by careful selection of hyperparameters in Algorithm 2, our attack achieves a 12.1x TPR(@1%FPR) improvement over the DB attack while requiring to train only 16 shadow models. In Appendix D, we provide a detailed cost analysis associated with running our attack along with end-to-end running time comparison between ours and DB attack. Interestingly, our computationally constrained variant, using 16 models, nearly matches the runtime of the DB attack while significantly outperforming it in terms of TPR (12.1x improvement).\n\n**Multiple Challenge Points:**\n\nWe would like to clarify that Algorithm 2 (Appendix C) does address the drawbacks highlighted by the reviewer.  Algorithm 2 indeed handles multiple challenge points simultaneously while training only a fixed number of shadow models, independent of the number of challenge points, and also captures the potential interactions between different challenge points and their respective poisoned replicas. We have updated Appendix C to explicitly elaborate on the impracticality of a straightforward extension of Algorithm 1 (also pointed out by the reviewer) and how Algorithm 2 can be used as an efficient alternative to tackle these drawbacks. \n\n**Clarity:**\n\nThank you for highlighting this issue. To ensure the paper is as self-contained as possible, we have added definitions of the terms used throughout the paper in sections 2 and 3. \n\nWe hope our response has addressed the main concerns expressed in the review. We will be happy to provide additional clarifications if needed."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3924/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700088884555,
                "cdate": 1700088884555,
                "tmdate": 1700088884555,
                "mdate": 1700088884555,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]