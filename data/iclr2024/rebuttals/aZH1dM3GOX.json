[
    {
        "title": "Multi-Task Reinforcement Learning with Mixture of Orthogonal Experts"
    },
    {
        "review": {
            "id": "EVyAb2wi78",
            "forum": "aZH1dM3GOX",
            "replyto": "aZH1dM3GOX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2589/Reviewer_duFe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2589/Reviewer_duFe"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies learning a good representation for multi-task setting. Their definition of a good representation acroos tasks crucially relies on the idea of capturing unique and common properties shared by all the tasks. To do this they solve a constrained optimization problem that orthogonalizes the representations generated by a mixture of experts by applying the Gram-Schmidt orthogonalization technique. This results in ensuring independence between the representations. They draw a nice connection between their orthogonal representations to the Stiefel manifold, a particular type of Riemannian manifold where the inner product is defined. In fact they define a Steifel Contextual MDP (SC-MDP) in definition 4.2 that maps any arbitrary state $s$ to a vector representation in Steifel Manifold (consequently these vector representations are orthonormal). They learn this mapping function $\\phi$ (called experts) from the data and while doing so concurrently enforce a constraint so that the learned representations are orthonormal (see eq(1)). Finally, to enforce diversity across experts, they apply the Gram-Schmidt process to orthogonalize the k-representations. They generate the final RL policy by combining these representations with a linear parametrization. Finally, they conduct several empirical evaluation of their algorithm in several standard RL benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) Learning a diverse, meaningful, and common representation across several tasks is an important area of research.\n2) Their approach is somewhat novel where they enforce as they draw this nice connection between the Stiefel manifold and orthonormal representations (in the Stiefel manifold) leads to diverse and meaningful representations across tasks.\n3) Their approach is simple and leads to good empirical performance in some benchmarks.\n4) The Mixture of Experts (MOE) is their main competitor which employs a mixture of experts without enforcing diversity. Principally they show that as you increase the number of experts the average return of MOE is less than MOORE. They also show how task-specific weights align with representations."
                },
                "weaknesses": {
                    "value": "1) While they draw some connection to the Stiefel manifold, they do not analyze theoretically any properties of their representations that might give some meaningful insight as to why these orthonormal representations are useful.\n2) The paper is not clear about the computation cost of the constrained optimization problem in eq(1).\n3) The paper lacks some more experimental discussion to prove conclusively that the orthonormal representations are helping over their closest competitor MOE."
                },
                "questions": {
                    "value": "1) There is no discussion in the paper regarding the computational cost of the constrained optimization in (1). How hard is it to solve and enforce this constraint? \n2) I understand that your approach enhances the diversity of feature representation which in turn leads to a good exploration of the state space. How do you ensure the balance between exploration and exploitation? Do you rely on the PPO and SAC to do a standard exploration/exploitation with the diverse learnt features?\n3) The discussion/implementation details between single and multi-head version of MOORE is missing. This seems important because in MT7 (figure 2) MOORE is outperformed by PC-grad and MTPPO? Why?\n4) It was vaguely referred to in section 5.1 (below figure 4) that MOORE has a faster convergence rate than other baselines. Can you elaborate on this? Do you believe that diverse representations (the key thrust of this paper) lead to a faster convergence?\n5) The ablation study is somewhat confusing. Observe that the ablation study is done on single-head architecture and you already show in MT7 (figure 1) that MOE is outperformed by MOORE. However, PC-grad and MTPPO outperform MOORE in figure 1 (single head). So it is not clear whether even PC-grad and MTPPO are learning a diverse set of representations. Can you elaborate on this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "Not applicable."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2589/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2589/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2589/Reviewer_duFe"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2589/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698457682070,
            "cdate": 1698457682070,
            "tmdate": 1699636196647,
            "mdate": 1699636196647,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "knEQdKPRMk",
                "forum": "aZH1dM3GOX",
                "replyto": "EVyAb2wi78",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2589/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2589/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Computation cost of enforcing the constraint\n\nInstead of solving the expensive constrained optimization problem at each time step, we take into account the hard constraint by applying the Gram-Schmidt process on the generated representations from the experts to orthogonalize them, hence satisfying the constraint at each time step. The time complexity of the Gram-Schmidt process is T = $O(k^{2}\\times d)$, where $d$ is the representation dimension and $k$ is the number of experts. \n\n> Exploration and Exploitation\n\nMOORE does not influence the exploration-exploitation trade-off. Instead, the main focus is extracting diverse representations of common objects and skills shared across the tasks. We rely on the RL algorithm (SAC, PPO) to handle the exploration aspect. \n\n> Single-head and Multi-head\n\nThe discussion of single-head and multi-head is highlighted in the appendix (A.1.2). In the revised version, we enhanced the discussion in the Appendix and added a clear sentence in section 4.2 of the methodology. Here, we will provide a clear distinction between the single-head and multi-head architectures. In multi-task, there are two common architectures: single-head, and multi-head. The difference between them is highly dependent on how we condition our network. In single-head architecture, the context $c$ (e.g. task-id) is usually concatenated with the input and fed to the network. Since MOORE has a task-agnostic representation block, we concatenate the context $c$ with the output of the representation block (aggregated representation) $v_{c}$. On the contrary, the multi-head consists of multiple task-specific output modules $f_{\\theta} = [f_{\\theta_{1}}, .., f_{\\theta_{|\\mathcal{C}|}}]$ where the context $c$ is responsible for selecting the corresponding task-specific output module $f_{\\theta_{c}}$. This is also valid for the baselines.\n\n> Figure 2 discussion\n\nGiven the multiple evidence of the effectiveness of MOORE in MiniGrid, MetaWorld MT10, and the new results in MetaWorld MT50, we believe we have demonstrated the superiority of our approach compared to baselines on large-scale MTRL problems. We believe that the single case where MOORE performs worse is negligible.\n\n> Fast convergence\n\nIn MTRL, sharing representations is key to learning multiple tasks using the same resources. Learning representations and skills shared across tasks can help in solving difficult tasks. We believe the MOE baseline eventually learns independent experts at the end of the training, yet it requires more samples to do so. It needs to focus on learning diverse experts and avoid any representation collapse while enhancing the quality of representation for every expert. In MOORE, we force diversity through the application of the Gram-Schmidt process, hence the network will only focus on learning high-quality representations for each expert. We argue this is the reason why our approach exhibits high sample efficiency in comparison to the other baselines.\n\n> Stiefel manifold and theoretical analysis\n\nStiefel manifold has been well-studied in machine learning [1,2]. Our goal in proposing the SC-MDP formulation is to connect the RL literature to the machine learning one that investigate the importance of the Stiefel manifold. For instance, [2] use the Stiefel manifold to alleviate the catastrophic forgetting in the continual learning scenario. We believe that our study could be of inspiration for future investigations on continual learning.\n\n> More discussion about the results and the importance of the orthogonality\n\nOur experiments show that MOORE consistently outperforms MOE on the challenging MTRL tasks of MiniGrid and MetaWorld MT10. Nevertheless, in the revised version of the paper, we now provide new results on the very challenging MetaWorld MT50, where MOORE again outperforms the MOE, even by a large margin. We believe that this can convince the Reviewer about the superiority of MOORE w.r.t. MOE and about the importance of diversity in the representation.\n\n> Single-head and ablation studies\n\nIn Fig. 4, we do the ablation study on MT7 since we can evaluate MOORE on a large number of experts before reaching the case where we have an expert for each task ($k=7$). Besides, we select the single-head architecture due to time reasons, since the experiments are faster than the multi-head ones. As the Reviewer points out, MOORE performs slightly worse than the baseline in this setting. However, we consider this lower performance to be negligible, especially considering the superiority of MOORE in all the other settings. \n\n[1] Huang, Lei, et al. \"Orthogonal weight normalization: Solution to optimization over multiple dependent stiefel manifolds in deep neural networks.\" AAAI (2018).\n\n[2] Chaudhry, Arslan, et al. \"Continual learning in low-rank orthogonal subspaces.\" NeurIPS (2020)."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700179091358,
                "cdate": 1700179091358,
                "tmdate": 1700179091358,
                "mdate": 1700179091358,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3Qrqj1oChI",
                "forum": "aZH1dM3GOX",
                "replyto": "knEQdKPRMk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2589/Reviewer_duFe"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2589/Reviewer_duFe"
                ],
                "content": {
                    "title": {
                        "value": "Answer to Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your rebuttal and answering some of my questions. \n- Thank you for clarifying the computational complexity. One of my concerns from your section 5.1.1 is that as you increase the number of experts you get a better performance against MOE (fig 5) at the cost of the computational complexity. My guess is that this is also dependent on the task at hand. How do you choose the right number of experts?\n- Thank you for clarifying the exploration-exploitation question.\n- I understand the difference between single-head vs multi-head architecture. My main concern is that in MT7 (figure 2) MOORE is outperformed by PC-grad and MTPPO? Why? Is this something to do with PPO? \n- Thanks for conducting the additional experiment on MT50."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700566500012,
                "cdate": 1700566500012,
                "tmdate": 1700566500012,
                "mdate": 1700566500012,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WR296u03mr",
            "forum": "aZH1dM3GOX",
            "replyto": "aZH1dM3GOX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2589/Reviewer_78VR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2589/Reviewer_78VR"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a novel approach for representation learning in Multi-Task Reinforcement Learning (MTRL) known as MOORE (Mixture Of Orthogonal Experts). The core idea is to share representations across tasks in an orthogonal manner to promote diversity and prevent representations from collapsing. MOORE utilizes the Gram-Schmidt process to shape a shared subspace of representations derived from a mixture of experts. This approach ensures diversity by pushing these representations to exist within the Stiefel manifold, a Riemannian manifold known for its orthogonality properties. The authors validate their method on two MTRL benchmarks: MiniGrid and MetaWorld, and report state-of-the-art performance on the latter."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The idea of orthogonalizing shared representations is novel and aligns with the goal of maximizing representation capacity in MTRL.\n\nLeveraging the properties of the Stiefel manifold for enforcing orthogonality is innovative and could have implications beyond MTRL.\n\nThe authors have demonstrated strong empirical results, notably achieving state-of-the-art performance on the MetaWorld MT10-rand tasks."
                },
                "weaknesses": {
                    "value": "The author used MoE and Transfer-MoE as its baseline. As far as I know, there are other MoE baselines, like PMOE [1], why the author didn't compare them in experiments.\n\nIn environments like DoorKey, as the author showed in Fig. 13, MOORE is worse than some other baselines, why and how this happened needs more discussion.\n\n[1] Ren, Jie, et al. \"Probabilistic mixture-of-experts for efficient deep reinforcement learning.\" arXiv preprint arXiv:2104.09122 (2021)."
                },
                "questions": {
                    "value": "See Weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2589/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2589/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2589/Reviewer_78VR"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2589/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698770541236,
            "cdate": 1698770541236,
            "tmdate": 1699636196529,
            "mdate": 1699636196529,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sbSoNUGtSP",
                "forum": "aZH1dM3GOX",
                "replyto": "WR296u03mr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2589/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2589/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Other MOE baselines\n\nThank you for the reference. In this work, we focused on MTRL algorithms like CARE [1] and PaCo which are the state-of-the-art MTRL baselines. Also, they can be considered as MOE baselines since CARE [1] uses a mixture of state encoders (experts) as we do, and PaCo [2] learn a mixture of policies or critics. Our MOE baseline utilizes the same architecture as our approach except for the Gram-Schmidt process stage; thus, we can assess the importance of enforcing diversity across the experts.\n\n> Fig. 13, MOORE is worse than some other baselines, why and how this happened needs more discussion.\n\nGiven the multiple evidence of the effectiveness of MOORE in MiniGrid, MetaWorld MT10, and the new results in MetaWorld MT50, we believe we have demonstrated the superiority of our approach compared to baselines on large-scale MTRL problems. We believe that the single case where MOORE performs worse is negligible.\n\n[1] Sodhani, Shagun, et al. \"Multi-task reinforcement learning with context-based representations.\" ICML (2021).\n\n[2] Sun, Lingfeng, et al. \"PaCo: Parameter-Compositional Multi-Task Reinforcement Learning.\" NeurIPS (2022)."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700178910846,
                "cdate": 1700178910846,
                "tmdate": 1700178910846,
                "mdate": 1700178910846,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "3GmPEpynYJ",
            "forum": "aZH1dM3GOX",
            "replyto": "aZH1dM3GOX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2589/Reviewer_stvx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2589/Reviewer_stvx"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose MOORE, a method for learning a common state representation using a mixture of orthogonal experts to be combined with task-specific weights for Multi-Task Reinforcement Learning. Building upon a Block Contextual MDP, a Stiefel-Contextual MDP is defined that formally allows for a compositional policy, taking advantage from a latent state representation retrieved from a mixture of experts. In various empirical studies, this representation is shown to improve the performance of PPO in various discrete tasks, and the performance of SAC in various continuous control tasks. Furthermore, ensuring orthogonality of experts via the Gram-Schmidt Process yields even improved results."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper tackles a relevant problem and provides a well-motivated solution approach. Overall, the paper is well structured and the presented considerations are theoretically sound. A wide variety of tasks, domains,  action-, and state-spaces, are used for evaluation, using a sufficient amout of baselines and sensible ablations."
                },
                "weaknesses": {
                    "value": "Overall, the clarity of the paper should be improved. Concretely: \n\n- The introduction does not state whether the proposed method is used for learning in a multi-task scenario or to curate diverse tasks for learning. Moreover, providing a motivation why a common state representation across tasks is needed could be useful. Also, the empirical studies could be included with the contributions listed in the introduciton.\n- In Section 4, in addition to the visualization of the mixture of experts in Fig. 1, a visualizing the parts of the compositional policy $\\pi(a|s)=(V_sw_c)^T\\theta$ an their connection would be helpful. To further elaborate on the intertwinement of the components and the proposped process, showcasing an exemplary algorithm might be useful (Section 4.2).\n- Regarding the experiments: I assume MOORE to be used in conjuction with PPO for the MiniGrid results (5.1) and with SAC for the MetaWorld results (5.2). Yet this should be clearly stated in the paper. Furthermore, if not introduced in related work, used baselines should be described (e.g., MTPPO, MTSAC, PCGrad, ...). To improve the overall readability, the Figure placement could be improved.  \n- Limitations should be stated \n\nRelated work should be extended with regards to work on mixtures of experts for reinforcmeent learning. \nAlso, the baselines used later on should be briefly introduced and compared. Further related work regarding diversity-based RL (e.g., B. Eysenbach, A. Gupta, J. Ibarz, and S. Levine, \u2018Diversity is All You Need: Learning Skills without a Reward Function\u2019, in International Conference on Learning Representations, 2019.) could also be helpful. \n\nFinally, to foster reproducibility, the authors should consider open-sourcing their implementations upon publication."
                },
                "questions": {
                    "value": "From my understanding, orthogonality is induced via the Gram-Schmidt Process. Could you elaborate on the necessity of the additional hard constraint introduced in Eq. 1?\n\nRegarding the experimental results, why is the performacne of PPO only reported as a horizontal line?\n\nHow many experts are used for the Meta World evaluations? How does the number of experts impact the scalabiltiy of the proposed approach? Further elaborations on the computational overhead introduces, as well as possible limitations might be more insightful, than the presented remarks regarding interpretability. \n\nMinor Comments:\n- p. 2: To improve readability, the contributions could be displayed as a list. \n- p. 2: $\\pi$ is defined in a deterministic but used in a stochastic manner."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2589/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2589/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2589/Reviewer_stvx"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2589/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698787175392,
            "cdate": 1698787175392,
            "tmdate": 1699636196465,
            "mdate": 1699636196465,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aSS5PyBYoH",
                "forum": "aZH1dM3GOX",
                "replyto": "3GmPEpynYJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2589/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2589/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> The relation between the hard constraint and the Gram-Schmidt process\n\nThe constrained optimization problem in Eq. 1 shows our objective mathematically. However, we realize the hard constraint by applying the Gram-Schmidt process on the generated representations from the experts to orthogonalize them, hence satisfying the constraint. We modified the main paper adding two sentences clarifying this part. Thank you for pointing this out.\n\n> PPO horizontal line\n\nWe included the single-task performance as a dashed line for two reasons. First, for the sake of clarity of the plots since we have many baselines. Second, it's common in the multi-task Learning literature to consider the single-task performance as a performance bound either as an upper bound in [1] or as a lower bound in [2], hence we care more about the final performance of the single-task agent.\n\n> The number of experts k\n\nFor MiniGrid, the number of experts $k$ (generating the $k$ representations) is $2$, $3$, and $4$ for MT3, MT5, and MT7, respectively. On the other hand, the number of experts is $4$ and $6$ for MT10-rand and MT50-rand of the MetaWorld benchmark, respectively. We have clarified that point in the revised version of the paper.\n\n> How does the number of experts impact the scalabiltiy of the proposed approach?\n\nIn Fig. 4, we showed the effect of increasing the number of experts regarding the performance. On average, we can clearly see that the performance is increasing much faster than the MOE baseline. However, this comes with a cost. The more experts we use, the higher the time complexity and the memory requirement. So, there is definitely a trade-off between the representation capacity and the time and memory requirements. \n\n> The computation and memory requirments\n  \nThe difference between MOORE and MOE is in the Gram-Schmidt stage where we orthogonalize the $k$-representations. The time complexity of the Gram-Schmidt process is $T = O(k^{2}\\times d$) [3,4], where $d$ is the representation dimension and $k$ is the number of experts. MOORE and MOE can be considered as a soft-MOE because they both compute the whole $k$ representations from all the experts and then aggregate them. MOORE and MOE have the same memory requirement of storing the parameters of the mixture of experts. The memory requirement is also similar in PaCo; however, in the MetaWorld experiments, we exhibit better performance using fewer experts. This shows that diversity plays an important role in maximizing the representation capacity. In the revised version, we have provided a clear discussion about the computation and memory requirements in Appendix C.\n\n> To improve readability, the contributions could be displayed as a list.\n  \n  Thank you for this comment. We have updated the contribution paragraph in the revised version by adding an index for each contribution."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700178748605,
                "cdate": 1700178748605,
                "tmdate": 1700178748605,
                "mdate": 1700178748605,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "a01FVTtKbf",
                "forum": "aZH1dM3GOX",
                "replyto": "3GmPEpynYJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2589/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2589/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Introduction modifications and motivation\n\nIn this work, our goal is to learn a set of diverse representations that cover the common objects, skills, and properties shared across tasks. We want to remark that sharing knowledge is known to be key to the success of the MTRL algorithm, as discussed in multiple works in the MTRL literature [1,2,5,6].\n\n> Empirical results in the contribution list\n\nThank you for your suggestion. We have added the empirical results regarding the state-of-the-art performance of MOORE on MetaWorld as part of the contribution list.\n\n> Enhancing Figure 1\n\nThank you for your feedback. In the revised version, we have updated the visual illustration of MOORE in Figure 1 by demonstrating in detail the computation happening inside the output head. \n\n> MOORE and the RL algorithms\n\nWe appreciate your comment. We stated that fact in Appendix A.1.2 and A.2.2. However, we have also clarified that point in Section 5 in the revised version of the paper.\n\n> Baseline\n\nAll baselines are well-known in the MTRL literature. We provided a brief description of each baseline in the paper. Moreover, in the revised version, we have added the implementation details about MTPPO and MTSAC in Appendix A.1.2 and A.2.2 since they are a central part of our approach and the other related baselines. \n\n> Figure placement\n\nThank you. We have enhanced the figure placement in the revised version of the paper.\n\n> Limitations should be stated\n    \nMOORE is considered as a soft-MOE algorithm because it requires computing the whole $k$ representations from all experts and then aggregating them, compared to the sparse selection of one expert as done in the sparse-MOE algorithms. Thus, MOORE has the limitation of potentially suffering from high time complexity, resulting in a trade-off between the representation capacity and time complexity. In this work, our goal is to make use of the whole representational space spanned by all the diverse experts and consider them as a set of basis, letting the task-encoder produces the task-specific weights to aggregate those bases. As a future work, we can investigate the possibility of selecting a few orthogonal experts like in the sparse-MOE algorithms. In the revised version of the paper, we have stated this limitation in the conclusion. \n    \n> Related works improvement\n\nThank you for the additional reference. Considering the focus on MTRL in our work, we focus on reviewing related works on a mixture of experts in the domain of MTRL. Nevertheless, we have added additional references about the mixture of experts and diversity in RL in the revised version of the paper.  \n\n> Open-sourcing the implementation\n\nWe plan to open-source our implementation upon acceptance.\n\n\n> $\\pi$ is defined in a deterministic but used in a stochastic manner.\n\nYes, in the preliminaries, we used a deterministic form of the policy for simplicity. However, we agree with the Reviewer that presenting it in the stochastic form can enhance the clarity of the paper. In the revised version of the preliminaries, we have defined the policy in the stochastic setting.\n\n[1] Sodhani, Shagun, et al. \"Multi-task reinforcement learning with context-based representations.\" ICML (2021).\n\n[2] D'Eramo, Carlo, et al. \"Sharing knowledge in multi-task deep reinforcement learning.\" ICLR (2019).\n\n[3] Golub, Gene H., and Charles F. Van Loan. Matrix computations. JHU press, (2013).\n\n[4] Mashhadi, Peyman Sheikholharam, et al. \"Parallel orthogonal deep neural network.\" Neural Networks (2021).\n\n[5] Yu, Tianhe, et al. \"Gradient surgery for multi-task learning.\" NeurIPS (2020)\n\n[6] Hessel, Matteo, et al. \"Multi-task deep reinforcement learning with popart.\" AAAI (2019)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700178862377,
                "cdate": 1700178862377,
                "tmdate": 1700178862377,
                "mdate": 1700178862377,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XUUCLd441J",
                "forum": "aZH1dM3GOX",
                "replyto": "a01FVTtKbf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2589/Reviewer_stvx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2589/Reviewer_stvx"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarifications and providing a revised version."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700734245059,
                "cdate": 1700734245059,
                "tmdate": 1700734245059,
                "mdate": 1700734245059,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "jU9vFYccDG",
            "forum": "aZH1dM3GOX",
            "replyto": "aZH1dM3GOX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2589/Reviewer_pAYF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2589/Reviewer_pAYF"
            ],
            "content": {
                "summary": {
                    "value": "The paper aims to learn shared representations for multi-task reinforcement learning. The authors propose that these representations should be as diverse as possible. They define a contextual MDP called the Stiefel Contextual Markov Decision Process (SC-MDP) where the states are projected into the Stiefel manifold. The property of a Stiefel manifold is all elements can be represented as k-orthonormal representations i.e. representations that comprise k orthonormal vectors, each of dimension d. They impose a hard constraint on the optimization using the Gram-Schmidt process. They achieve competitive results on minigrid and metaworld."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(1) The use of the Gram-Schmidt process to enforce a hard constraint of orthonormality on the optimization is interesting. \n\n(2) If the vectors are orthonormal, the state will be represented using k diverse vectors, providing a greater span of the state space. This bears fruit as empirically, they perform better when the vectors are orthonormal i.e. when the hard constraint is applied.\n\n(3) They introduce the SC-MDP formulation where each state can be mapped to an element in the Stiefel manifold."
                },
                "weaknesses": {
                    "value": "(1) Please provide an algorithm box. I am not sure why these different vectors are called experts. Is it for every step of the convex optimization, you perform the Gram-Schmidt orthonormalization like projected gradient descent? Or do you perform the normalization towards the end? \n\n(2) Say you obtain $U^* = [u_1, u_2, ..., u_k]$ by minimizing $J(\\Theta)$. Are these k vectors linearly independent? Gram-Schmidt process works on linearly independent vectors. Moreover, the vectors that you get after the process, $V^*= [v_1, v_2, ..., v_k]$ span the same space as $U^*$ but the $w_c$ for both will be different. I think these confusions can be cleared if you provide an algorithm box.\n\n(3) You mention that MOORE even outperforms the single task performance is a few cases. But in those cases, most baselines also outperform the single task performance.\n\n(4) Please address the questions in the following section."
                },
                "questions": {
                    "value": "(1) What do you mean by frames in Definition 4.1?\n\n(2) How do you parameterized the critic? You have only mentioned the parameterization of actor.\n\n(3) Explain MT-PPO - How is PPO adapted for multi-task RL?\n\n(4) What do you mean by single-head and multi-head architectures? Do you refer to single-head and multi-head attention?\n\n(5) For transfering experts, you write \"we transfer experts learnt on MT3 to MT5 (MT3 $\\rightarrow$ MT5) and on MT5 to MT7 (MT5 $\\rightarrow$ MT7) \". But in the next sentance you write \"First, we train on base tasks (intersection of two scenarios), and then we transfer the learnt experts on novel tasks (the difference between the two scenarios)\". Both statements sort of contradict each other."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2589/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2589/Reviewer_pAYF",
                        "ICLR.cc/2024/Conference/Submission2589/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2589/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698816864380,
            "cdate": 1698816864380,
            "tmdate": 1700648959008,
            "mdate": 1700648959008,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "4clLGRHGJI",
                "forum": "aZH1dM3GOX",
                "replyto": "jU9vFYccDG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2589/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2589/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> What do you mean by frames in Definition 4.1?\n\nWe use the term orthonormal $k$-frame, in the definition, to represent a set of orthonormal vectors. If the reviewer prefers, we can replace the term \"frames\" with \"vectors\".\n\n> How do you parameterized the critic? You have only mentioned the parameterization of actor.\n\nWe parameterize the critic similar to the actor. Only in MetaWorld, instead of mapping the state only, we map the state-action by the mixture of experts, followed by the application of the Gram-Schmidt process to orthogonalize the representations. Then, the task-encoder generates the task-specific weights $w_{c}$ given the context $c$ (e.g., as task-id). The task-specific representation $v_{c}$ of the state-action can be computed using the task-specific weight $w_{c}$ and the set of representations $V_{s,a}$. The $Q$-value is then calculated either using a single-head architecture where we concatenate the task-id with the task-specific representation as an input to a single output module $f_{\\theta}$, or using a multi-head architecture where the output module consists of multiple task-specific output modules where the corresponding output module is selected using the context $c$ (e.g., as task-id). In the revised version, we provide Alg. 1 and Alg. 2 of the MOORE forward function for the Actor and of the critic, respectively. \n\n> Explain MT-PPO - How is PPO adapted for multi-task RL?\n \nWe adapt PPO to the MTRL setting by training the actor (policy) and critic (value function) by minimizing their corresponding loss functions averaged on transitions sampled from all tasks. We have added the PPO adaptation details in Appendix A.1.2.\n\n> Single-head and Multi-head\n\nThe discussion of single-head and multi-head is highlighted in Appendix A.1.2. In the revised version, we enhanced the discussion in the Appendix and added a clear sentence in section 4.2 of the methodology. Here, we provide a clear distinction between the single-head and multi-head architectures. In multi-task, there are two common architectures: single-head, and multi-head. The difference between them is highly dependent on how we condition our network. In single-head architecture, the context $c$ (e.g. task-id) is usually concatenated with the input and fed to the network. Since MOORE has a task-agnostic representation block, we concatenate the context $c$ with the output of the representation block (aggregated representation) $v_{c}$. On the contrary, the multi-head consists of multiple task-specific output modules $f_{\\theta} = [f_{\\theta_{1}}, .., f_{\\theta_{|\\mathcal{C}|}}]$ where the context $c$ is responsible for selecting the corresponding task-specific output module $f_{\\theta_{c}}$. This is also valid for the baselines. \n \n> Transfer learning study\n\nWe agree that this part needs more elaboration. We have added Appendix B.2 in the revised version of the paper to elaborate more on the experimental details of the transfer learning study. In our transfer learning study, we assess the transfer capability of our approach in utilizing the diverse representations, learned on a set of *base* tasks, for a set of *novel* but related tasks. We evaluate our approach, MOORE, against the MOE baseline on MiniGrid. We refer to the transfer learning adaptation of our approach as **Transfer-MOORE**, and **Transfer-MOE** for the MOE baseline.\n\nWe conducted two experiments based on the sets of tasks defined on MiniGrid (MT3, MT5, and MT7). We show the empirical results on two transfer learning scenarios where we transfer a set of experts learned on MT3 to MT5 (MT3 $\\rightarrow$ MT5), and on MT5 to MT7 (MT5 $\\rightarrow$ MT7). It is worth noting that MT3 is a subset of MT5, and MT5 is a subset of MT7. We consider the intersection between every two sets (MT3 and MT5 or MT5 and MT7) as base tasks while considering the difference as novel tasks. For instance, in the MT3$\\rightarrow$MT5 scenario, the base tasks are LavaGap,  RedBlueDoors, and Memory (common for MT3 and MT5), while having DoorKey, and MultiRoom as novel tasks (only in MT5).\n\nTransfer-MOORE learns the base tasks and then transfers the learned mixture of experts for learning the novel ones. While learning the novel tasks, the mixture of experts is frozen but the task encoder and output head are being trained. On the contrary, MOORE is only trained on novel tasks from scratch. This also holds for MOE and Transfer-MOE. In this study, we are employing a multi-head architecture, the output head consists of completely decoupled task-specific output modules. Therefore, we train only the task encoder and the task-specific output modules for the novel tasks from scratch, keeping the mixture of experts frozen and the base task-specific heads too."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700178560676,
                "cdate": 1700178560676,
                "tmdate": 1700178560676,
                "mdate": 1700178560676,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "sewHBWTCbX",
                "forum": "aZH1dM3GOX",
                "replyto": "jU9vFYccDG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2589/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2589/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> Algorithm box\n\nTo clear any ambiguity, we provide now an algorithm box in the appendix of the revised version, regarding the adaptation of the actor (Algorithm 1) and critic (Algorithm 2) using MOORE. We consider the state encoders as experts. At each step, the generated representations $U$ from the experts (state encoders) are orthogonalized by applying the Gram-Schmidt process to satisfy the hard constraint in Eq. 1. We aggregate the orthogonal representations $V$ using the task-specific weights $w_{c}$. \n\n> Are these k vectors linearly independent? Gram-Schmidt process works on linearly independent vectors.\n\nWe agree that the input vectors to the Gram-Schmidt process should be linearly independent. In this work, we cannot guarantee the independence of the vectors before the process; however, we have not encountered any instability during learning. We believe that the reason is that the Gram-Schmidt is a differentiable process and the gradients are actually contributing to learning the experts, hence favoring the independence of the input vectors. \n\n> You mention that MOORE even outperforms the single task performance is a few cases. But in those cases, most baselines also outperform the single task performance.\n\nHere, we highlighted that MOORE outperforms the single-task performance with a **significant margin**. The Reviewer correctly points out that also the other baselines manage to outperform the single-task performance in the same setting, although we can observe that they do it by a lower margin than ours. We agree that we should have stated this point clearer and we have done it in the revised version."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700178630988,
                "cdate": 1700178630988,
                "tmdate": 1700178630988,
                "mdate": 1700178630988,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ycj0YT5E8B",
                "forum": "aZH1dM3GOX",
                "replyto": "sewHBWTCbX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2589/Reviewer_pAYF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2589/Reviewer_pAYF"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks a lot for your clarifications. However, there are still some concerns.\n\n(1) You don't need to replace the term \"frames\" with \"vectors\" but be sure to explain the term you are using. In Linear Algebra, the term \"vectors\" is more common\n\n(2) In MTPPO, are you conditioning the policy and value function on the task id?\n\n(3) The transfer learning pipeline is clear now. But if MT3 is a subset of MT5, doesn't the intersection tasks simply mean MT3? If so, you don't need to even write the term intersection set as it only adds confusion.\n\n(4) I am still concerned about the use of Gram Schmidt process as there is no way to ensure that the input vectors $U_s$ are linearly independent. From the algorithm box, you are simply using a network to generate $U_s$ and applying the Gram-Schmidt process on top of it and hoping that by backpropagation, as the process is differentiable, it will train the network $h_\\phi$ to generate linearly independent vectors. I am not fully convinced why this will work. Even from your comment, it looks like you are also not very sure if the input vectors are in fact linearly independent. \n\n(5) You say that MOORE outperforms the single-task performance by a significant margin. Agreed. But the baselines also outperform by a good margin. The curves look very close to each other. So how much is the margin? The only reason I am concerned about this is that mathematically, as the input might not be linearly independent, the produces experts are not orthogonal and so want to see if empirically, the proposed solution provides significant improvement."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700512322260,
                "cdate": 1700512322260,
                "tmdate": 1700512322260,
                "mdate": 1700512322260,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0kX64MONuk",
                "forum": "aZH1dM3GOX",
                "replyto": "jU9vFYccDG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2589/Reviewer_pAYF"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2589/Reviewer_pAYF"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks a lot for the additional clarifications and results. I see that the improvements of MOORE over PCGrad are small. Moreover, if the Gram-Schmidt process does not generate orthogonal vectors as their inputs are linearly dependent, it might not always lead to instability in training, it will just lead to poor outputs. Empirically, the method might overperform the baselines, still, if applied correctly, i.e. if the vectors are indeed orthogonal, the results can be better.  \n\nI believe that the idea of using Gram Schmidt process is interesting and novel and has some empirical improvements over the baselines so I have increased my score."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700648919521,
                "cdate": 1700648919521,
                "tmdate": 1700649032928,
                "mdate": 1700649032928,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EuWG5533s7",
            "forum": "aZH1dM3GOX",
            "replyto": "aZH1dM3GOX",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2589/Reviewer_gg3H"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2589/Reviewer_gg3H"
            ],
            "content": {
                "summary": {
                    "value": "The authors introduce a novel representation learning method for multi-task reinforcement learning (MTRL) that promotes orthogonal representations to enhance diversity. Initially, they present a Stiefel Contextual Markov Decision Process (SC-MDP) to interpret the orthogonal representation spaces. Following this, the authors propose a constrained optimization problem that ensures orthogonality in the representations and employ the Gram-Schmidt process to address the constraints. Empirical results demonstrate that their proposed method, MOORE, consistently surpasses previous MTRL algorithms across two MTRL benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper proposes an interesting approach to promote the diversity of representation with Gram-Schmidt process and shows empirical improvements on two MTRL benchmarks."
                },
                "weaknesses": {
                    "value": "The paper doesn't introduce a method that is fundamentally distinct from the approach in prior work[1], which utilizes a representative parameter set {$\\phi_1,...,  \\phi_k$} and task-specific weight $w_c$ derived from the task-id. The proposed structure for the policy appears to be merely different combinations of the existing learnable parameters discussed in paper[1]. While Paco[1] employs a linear combination of the representative parameters with the task-specific weight as the policy parameters, this paper adopts the same strategy to compute an additional input for policy inputs combined with the state. Moreover, although the authors argue that orthogonality results in more diverse representations, there's an absence of theoretical backing or ablation studies to corroborate this claim. Given that the vector $k$-representations $U_s$ is contingent upon the state $s$, the authors cannot guarantee representational diversity at the task level, even if state-level orthogonality is ensured. Additionally, the paper does not address issues related to complexity and memory requirements. Storing a set of parameters for representation implies that the proposed \"MOORE\" method could demand significant memory and computational resources. Finally, the authors should benchmark their method against the MT50 benchmark from Meta-World, as all their experiments are conducted on a limited task set size.\n\n[1] Sun, Lingfeng, et al. \"PaCo: Parameter-Compositional Multi-Task Reinforcement Learning.\" Advances in Neural Information Processing Systems 35 (2022): 21495-21507."
                },
                "questions": {
                    "value": "1. The Gram-Schmidt process can yield different vectors based on which vector is chosen as the first. How do the authors determine the initial vector for the Gram-Schmidt process?\n\n2. Can you provide the results for the MT50 benchmark?\n\n3. How many representations $k$ are used in each experiment?\n\n4. Could you elucidate the distinction between MOORE and Transfer-MOORE?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2589/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2589/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2589/Reviewer_gg3H"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2589/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698840531239,
            "cdate": 1698840531239,
            "tmdate": 1699636196270,
            "mdate": 1699636196270,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "y9IWmkStdL",
                "forum": "aZH1dM3GOX",
                "replyto": "EuWG5533s7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2589/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2589/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> How do the authors determine the initial vector for the Gram-Schmidt process?\n\n- Thank you for raising this point. In MOORE, we consider always the first expert's representation as the initial vector for the Gram-Schmidt process. We agree that the process will yield different set of orthonormal vectors depending on the initial selected vector. We argue that it does not matter in our case since the representations are actually generated from the mixture of experts which are being trained.\n- In the Appendix D of the revised version, we have added an ablation study on MT5 of MiniGrid where we utilize $3$ experts. We provided the variations of MOORE based on the initial vector selected for the Gram-Schmidt process. The study shows that the performance is almost the same while varying the selected initial vector, supporting our claim that the initial vector does not matter since the experts are being trained.\n\n> Can you provide the results for the MT50 benchmark?\n\nWe agree about the importance of testing MOORE on MT50, and we apologize for not having done it in the original submission due to lack of time. We have added the comparison between MOORE and the MOE baseline on MT50 for 10 epochs in the experimental section of the revised version. The experiments are still running to obtain results for 20 epochs, which we plan to publish in a new revision in the upcoming days, in order to be able to compare with the already available results of the other baselines in [1]. Notably, as shown in the new Figure 6b, the performance of MOORE obtained after 10 epochs is already superior than the state-of-the-art of PaCo [1], i.e., $57.3%$. Moreover, our performance has been achieved using only $6$ experts in comparison to the state-of-the-art PaCo which uses $20$ experts.\n\n>How many representations k are used in each experiment?\n\nFor MiniGrid, the number of experts $k$ (generating the $k$ representations) is $2$, $3$, and $4$ for MT3, MT5, and MT7, respectively. On the other hand, the number of experts is $4$ and $6$ for MT10-rand and MT50-rand of the MetaWorld benchmark, respectively. We have clarified that point in the revised version of the paper.\n\n>Could you elucidate the distinction between MOORE and Transfer-MOORE?\n\nIn our transfer learning study, we assess the transfer capability of our approach in utilizing the diverse representations, learned on a set of *base* tasks, for a set of *novel* but related tasks. We evaluate our approach, MOORE, against the MOE baseline on MiniGrid. We refer to the transfer learning adaptation of our approach as **Transfer-MOORE**, and **Transfer-MOE** for the MOE baseline. Transfer-MOORE learns the base tasks and then transfer the learned mixture of experts for learning the novel ones. While learning the novel tasks, the mixture of experts is frozen but the task encoder and output head are being trained. On the contrary, in this study, MOORE is only trained on the novel tasks from scratch. This also holds for MOE and Transfer-MOE. We also added a subsection in the Appendix A.4 elaborating more on the experimental details of the transfer learning study.\n\n>Similarity to Paco.\n\nPaCo [1] interpolates a task-specific policy from a subspace of policies spanned by a set of parametric policies while utilizing task-specific weights $w_{c}$. Hence, PaCo searches for the task-specific policy in the parametric space, not in the feature/representation space. On the other hand, MOORE utilizes a set of experts (state-encoders for actor or state-action-encoders for critic) to generate a set of orthogonal representations that spans a representation subpsace where each task can interpolate the relevant representation. Using the task-id is not crucial in MOORE, nor in PaCo. It is just an example on how we can interpolate task-specific information. For instance, CARE [2] uses the task description instead of the task-id while doing attention mechanism to produce set of representations. Moreover, PaCo applies some tricks (loss-maskout and w-reset) to stabilize the MTRL training process, however, our approach does not employ any tricks.\n \n>Theoretical backing or ablation studies to corroborate this claim. Given that the vector \n$k$-representations $U_{s}$ is contingent upon the state $s$, the authors cannot guarantee representational diversity at the task-level, even if state-level orthogonality is ensured.\n\nThank you for raising this interesting point. We want to clarify that we are focusing on representing each state by a set of diverse representations, and we agree that we have not targeted the diversity at the task-level. Nevertheless, as shown in the experiemental section, the orthogonality condition succeeds to make the representations at the state-level diverse and outperforming the other baselines. In addition, in Fig. 4, we can deduce that the diversity manages to increase the state representation capacity as we increase the number of experts in comparison to not enforcing diversity."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700178142698,
                "cdate": 1700178142698,
                "tmdate": 1700178142698,
                "mdate": 1700178142698,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rdKzc5Dh44",
                "forum": "aZH1dM3GOX",
                "replyto": "EuWG5533s7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2589/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2589/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">Complexity and memory requirements\n\n- The difference between MOORE and MOE is in the Gram-Schmidt stage where we orthogonalize the k-representations. The time complexity of the Gram-Schmidt process is $T = O(k^{2}\\times d$) [3,4], where $d$ is the representation dimension and $k$ is the number of experts. \n- MOORE and MOE can be considered as a soft-MOE because they both compute the whole $k$ representations from all the experts and then aggregate them.\n- MOORE and MOE have the same memory requirement of storing the parameters of the mixture of experts. The memory requirement is also similar in PaCo; however, in the MetaWorld experiments, we exhibit better performance using fewer experts. This shows that diversity plays an important role in maximizing the representation capacity. In the revised version, we have provided a detailed discussion about the computation and memory requirements in Appendix C.\n\n[1] Sun, Lingfeng, et al. \"PaCo: Parameter-Compositional Multi-Task Reinforcement Learning.\" NeurIPS (2022).\n\n[2] Sodhani, Shagun, Amy Zhang, and Joelle Pineau. \"Multi-task reinforcement learning with context-based representations.\" ICML (2021).\n\n[3] Golub, Gene H., and Charles F. Van Loan. Matrix computations. JHU press, (2013).\n\n[4] Mashhadi, Peyman Sheikholharam, et al. \"Parallel orthogonal deep neural network.\" Neural Networks (2021)."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2589/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700178217269,
                "cdate": 1700178217269,
                "tmdate": 1700178217269,
                "mdate": 1700178217269,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]