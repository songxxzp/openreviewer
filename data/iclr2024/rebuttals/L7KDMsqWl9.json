[
    {
        "title": "HHD-Ethiopic: A Historical Handwritten Dataset for Ethiopic OCR with Baseline Models and Human-level Performance"
    },
    {
        "review": {
            "id": "MdowP1Nl1P",
            "forum": "L7KDMsqWl9",
            "replyto": "L7KDMsqWl9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6196/Reviewer_tpDS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6196/Reviewer_tpDS"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new dataset and baseline models for historical handwritten Ethiopic OCR. The contributions of this paper are as follows:\n(1) A new dataset that contains about 80,000 text-line images from 18th to 20th century manuscripts, with multiple annotations and human-level performance benchmarks.\n(2) Three types of classical text recognition methods, including transformer-based methods, attention-based methods, and CTC-based methods, are tested on the new dataset.\n(3) The authors compare the baseline methods with human-level performance, showing their superiorities and weaknesses."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. This paper introduces a new dataset for historical handwritten Ethiopic OCR. \n2. Some baseline methods are evaluated on this dataset."
                },
                "weaknesses": {
                    "value": "The contributions of this paper are limited. The reasons are:\n1. This paper is a dataset paper, without new technical methods.\n2. The academic challenges of this dataset are not representative. The impact of this new benchmark seems limited."
                },
                "questions": {
                    "value": "I suggest the authors provide a new solution for this new dataset."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6196/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698214007017,
            "cdate": 1698214007017,
            "tmdate": 1699636674831,
            "mdate": 1699636674831,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "l5HnETjPWT",
                "forum": "L7KDMsqWl9",
                "replyto": "MdowP1Nl1P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6196/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6196/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer  tpDS"
                    },
                    "comment": {
                        "value": "Thank you for your comments.\n\n[**This paper is a dataset paper, without new technical methods**]:  Yes, the  paper is a datasets paper and our goal is to  create a historical handwritten dataset for a low-resourced script and provide benchmark results with SOTA OCR  models. In addition we compare the human-machine performance based on evaluation results from several human annotators and the baseline models and we believe this contribution is a good fit for \u201c**dataset and benchmark**\" themes (https://iclr.cc/Conferences/2024/CallForPapers) of ICLR-2024.\n\n[**The academic challenges of this dataset are not representative**]: The experimental evaluation results of this dataset, involving several SOTA OCR models and human performance, indicate limited recognition performance. Therefore, this dataset presents challenges and can be valuable for further investigation of new methods to address such challenges. If you have a particular aspect you are curious about, let us know.\n\n\n[**The impact of this new benchmark seems limited**]: To the best of our knowledge, this is the first sizable historical handwritten dataset of the Ethiopic script, characterized by a unique syllabic writing system, low-resource availability, and complex orthographic diacritics. We believe that the human-level performance and results of the SOTA OCR models reported in this work can be utilized as benchmarks for various state-of-the-art OCR models."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6196/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700315374628,
                "cdate": 1700315374628,
                "tmdate": 1700315374628,
                "mdate": 1700315374628,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EBVkiTER2u",
            "forum": "L7KDMsqWl9",
            "replyto": "L7KDMsqWl9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6196/Reviewer_6Yog"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6196/Reviewer_6Yog"
            ],
            "content": {
                "summary": {
                    "value": "The article presents a new database of lines of handwritten text from historical documents written in Ethiopian script. This database consists of more than 80,000 lines of text from 1,700 pages. Two test sets were defined, one set with the same distribution as the training set (IID test) and another derived from completely disjoint manuscripts (OOD test). The performance of human transcribers was evaluated. Several handwriting recognition models were trained and evaluated on the test sets. One model outperformed humans on the IID set. The database and training codes are available online."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "- a large new database available for the Ethiopian script\n- the code is available to reproduce the experiments\n- an interesting proposal for an IID and OOD test sets"
                },
                "weaknesses": {
                    "value": "- full pages are not provides, which prevents the evaluation of full page models, which are currently the best performing models, compared with cascade models (line detection + HTR), which accumulate errors\n- no reference to https://journals.openedition.org/jtei/4109 \n- no reference or comparison to the models available in Transkribus https://readcoop.eu/model/ethiopic-classical-ethiopic-scripts-from-ethiopia-and-eritrea/\n- tesseract supports Ethiopic but is neither mentioned nor tested. As the script is not cursive, a test of tesseract would be possible. \n- the models tested have never been evaluated on standard handwriting recognition bases. How do they compare with standard libraries such as Transkribus, pylaia and trOCR?\n\nIn conclusion, the article presents an interesting resource for HTR, but makes no new contribution either experimentally or methodologically."
                },
                "questions": {
                    "value": "- no measure of inter-annotator agreement: it seems that annotation is difficult, judging by the poor performance of humans. How correct is the annotation? What is the variance of the annotation?\n- no details are given about the manuscripts: where do they come from, how were they chosen, how many are there?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6196/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698703024384,
            "cdate": 1698703024384,
            "tmdate": 1699636674726,
            "mdate": 1699636674726,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zoslDjugYu",
                "forum": "L7KDMsqWl9",
                "replyto": "EBVkiTER2u",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6196/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6196/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer 6Yog"
                    },
                    "comment": {
                        "value": "Thank you for reviewing our work. Your input has helped us improve the paper. We've addressed your questions and comments below.\n\n[**full page are not provided to evaluate line detection+HTR**]: line detection + recognition is one way of OCRing. in literature, the cascaded detection and recognition task  is most common in scene text recognition. Our focus was to create segmented text-line images  for the recognition task and the annotation was done for lines images. However, we will keep updating the dataset for detection and recognition tasks in the future. \n\n[**include reference paper done for Ethiopic**]: we included the paper in our reference list.\n\n[**no reference or comparison with the already available Ethiopic_Eritrea  HTR model in Transkribus**] This model is released in the form of webApp, and cannot be retrained. Furthermore, the authors do not provide an API to easily upload samples and download results. Therefore, we limited ourselves to evaluating its performance manually by uploading a few with a samples manuscript image from our test dataset and comparing the results with those of our model. The Ethiopic_Eritrea  HTR model from the Transkribus model archives a CER of~65.2%  while our Hopt-attn-CTC model archives 17.82 for this sample specific test manuscript. Link for demo:https://github.com/ethopic/hhd-ethiopic-I/blob/main/Hopt_attn_CTC_vs_Transkribu.ipynb\n\n[**the models tested have never been evaluated on standard handwriting recognition bases. How do they compare with standard libraries such as Transkribus, pylaia and trOCR?**] \nWe have assessed eight methods, including SOTA OCR techniques validated on Latin, Chinese, and Amharic scripts as documented in the literature, under the same computational constraints and resources. Our focus is on models that are comparatively more lightweight than those requiring extensive computing resources or subscriptions. Additionally, we have incorporated the evaluation results of TrOCR (refer to Table 3 in the revised version). TrOCR, being a model with a large number of parameters, is fine-tuned for fewer epochs. This approach is not chosen because it requires more time for training and inference, leading us to prioritize smaller, more efficient networks. Similarly, the Ethiopic_Eritrea Handwritten Text Recognition (HTR) model in Transkribus, which you recommended and we have evaluated, is developed using PyLaia. It's important to note that models in Transkribus are not free and require a subscription for extensive HTR tasks. They also require a stable internet connection. Furthermore, the need to upload manuscripts for the Transkribus models raises ethical concerns.\n\n[**Tesseract model**], Tesseract is an OCR framework designed for hundreds of scripts including Ethiopic. We evaluated the model proposed for Amharic on a sample test set. The results being so poor, this model cannot be considered state of the art. For this reason we did not include it as part of our baselines. \n\n[**inter-annotator agreement of annotators**]  As discussed in section 3.1 and Appendix B, each text-line image in the training set is annotated individually. In contrast, the test sets undergo expert review, and we evaluate the ground-truth text differences before and after the review in CER (see section 4.1.)\n\n[**details about the manuscripts**] As discussed in appendix B, the text line images are extracted from manuscripts between the 18th and 20th century; collected from ENALA covering 7 different books of cultural and religious contents (~ 1300 pages), while the remaining is from the internet."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6196/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700314972501,
                "cdate": 1700314972501,
                "tmdate": 1700314972501,
                "mdate": 1700314972501,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "HfMhq2OEj3",
            "forum": "L7KDMsqWl9",
            "replyto": "L7KDMsqWl9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6196/Reviewer_HsR1"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6196/Reviewer_HsR1"
            ],
            "content": {
                "summary": {
                    "value": "This paper works on historical handwritten Ethiopic script recognition. It provides introduction to the character system contained in the Ethiopic and lists some major challenges in this task at first. Then, it introduces a new OCR dataset for historical handwritten Ethiopic script HHD-Ethiopic, which consists of roughly 80,000 annotated text-line images and compares the human-level recognition performance with some state-of-the-art OCR models on the proposed dataset. The main contributions of this paper are as follow:\n\u2022\tThis new dataset is the first sizable dataset for handwritten Ethiopic text-image recognition and it can encourage further research on Ethiopic script recognition.\n\u2022\tThe author assessed the human-level performance of multiple participants in HHD-Ethiopic dataset to establish a baseline for comparison with machine learning models.\n\u2022\tThe author evaluate several state-of-the-art OCR methods on the HHD-Ethiopic dataset."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThis paper collects and analyzes a new historical handwritten Ethiopic dataset, which is benefit to future research.\n2.\tThis paper compare several popular OCR methods on the proposed new dataset. And it tries performing a fair comparison between human and machine performance on historical handwritten Ethiopic scripts recognition task."
                },
                "weaknesses": {
                    "value": "1.\tThe structure of Table 2 and its description in the bottom of page 6 is not match: According to the Table, the Test-set-I is the IID data, which should be Annot-V with 26.56% CER and 24.56% NED; the Test-set-II is the Annot-VI. And it is better to add a row displaying the average performance. \n2.\tBecause of the error in Section 4.1, the Figure 5 and the conclusion inferred from it are also wrong (i.e. HPopt-Attn-CTC cannot surpass human performance on Test-set-II).\n3.\tAccording to the previous two reasons, I think the experiments of the paper are insufficient. \n4.\tThis work did not propose a new approach to recognize with the specific Historical handwritten Ethiopic script."
                },
                "questions": {
                    "value": "Researchers often compare the performance of recognition methods for other languages using the recognition accuracy. Why not include this metric?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6196/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6196/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6196/Reviewer_HsR1"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6196/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698822564224,
            "cdate": 1698822564224,
            "tmdate": 1699636674594,
            "mdate": 1699636674594,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "RWner0C4bn",
                "forum": "L7KDMsqWl9",
                "replyto": "HfMhq2OEj3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6196/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6196/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer HsR1"
                    },
                    "comment": {
                        "value": "We appreciate your valuable feedback. Thank you.\n\n[**Correction on structure of table 2 and add a row for average performance**]: we have corrected it (the middle horizontal line is moved four steps down, and  inserted one additional row for average performance of each group. Furthermore, we have updated the corresponding values of CER and NED in the text.\n\n[**Mismatch form section 4.1, the Figure 5 and the conclusion inferred from it are also wrong**]: After fixing the table structure in Section 4.2 (Table 2), the remaining inferences drawn from this table are now accurate.\n\n[** According to the previous two reasons, I think the experiments of the paper are insufficient**]: Although we have previously shared the source code, trained models, and dataset for reproducibility, an \u201c\\hline\u201d mistake in the LaTeX table code caused confusion in a section of the paper derived from this table. We have now corrected this error and the experiments are corrected.\n\n[**New approach for historical handwritten Ethiopic script**]  as mentioned in the introduction section, there is lack of historical handwritten dataset for Ethiopic, the main goal of this paper is to create the dataset and evaluate it with SOTA OCR models. In addition, we evaluate human-level performance as a benchmark.\n\n[**Why not include accuracy as a metric?**] We are using standard metrics (e.g  e.g https://arxiv.org/pdf/1909.07741.pdf) for sequence to sequence comparisons, where 3 types of errors occur at the character level: insertions, substitutions, and deletions. The 1-CER (CER=Character Error Rate) is as close as it gets to an accuracy (taking into account the 3 types of possible errors). At the sentence level, the accuracy is usually zero because there are usually a few character-level mistakes. Of course, if a language model or lexicon would be integrated in our system to mitigate some of the mistakes, the sentence-level error would go down, but this is no in the scope of this research."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6196/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700314646082,
                "cdate": 1700314646082,
                "tmdate": 1700314646082,
                "mdate": 1700314646082,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YqWFn7Skwd",
            "forum": "L7KDMsqWl9",
            "replyto": "L7KDMsqWl9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6196/Reviewer_idkx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6196/Reviewer_idkx"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a new dataset for historical handwritten Ethiopic script. The dataset contains approximately 80k text lines extracted from scanned document from the 18th to the 20th century. The paper describes the challenges of Ethiopic script, and the data collection and annotation process. Baseline recognition results are reported as character error rates, evaluating a number of state-of-the-art techniques in handwriting recognition, as well as the human error rate on the proposed test splits."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "First, the paper presents a new dataset for an under-resourced script, made of historical documents: this is quite valuable for a more general inclusion of all languages and eventually allow to process more archives. \nThis seems to be even more valuable given the high error rate of human readers for these documents, even when they are familiar with Ethiopic characters. Systems derived from that dataset could therefore be a good help for archivists. \nFinally, the paper provides some baselines with existing handwriting recognition methods.\nThe paper is globally well written and easy to follow."
                },
                "weaknesses": {
                    "value": "There are maybe sometimes too many details in the text, or repeated statements, which could be shortened in favor of either more analysis of the data, a more detailed description of the challenges of Ethiopic scripts (e.g. some parts of App. A are very interesting and could fit in the main part of the paper).\nThe fact that some datasets exist for Ethiopic script (described in App. B) should appear in Section 2, where the reader is let to think that no such dataset exist. Moreover, it would be interesting to see how the models presented in the experiment section would perform on these other datasets, or to include the models used in these other papers in the baseline, as they might address the challenges of Ethiopic script. (for example, Abdurhaman et al. report a CER of less than 2% on their dataset)\nIn a paper proposing a new dataset, I would expect more statistical analysis or the proposal of a method to address the specific challenges of the new dataset. In particular, the human error rate seems quite high, which at the same time makes the dataset interesting but begs the question of the quality of the ground-truth. How would the human performance be with access to reference material? Do the annotator make the same errors or different ones? Do the model make the same mistakes as the human evaluation? For the training set labeling, is there a measure of, for example, inter-annotator agreement?\nThe paper is interesting and the proposal of a new dataset valuable, but it could be more suited to other venues like DAS, ICFHR, ICDAR or maybe ICML"
                },
                "questions": {
                    "value": "Why are CRNN, ASTER, SVTR, etc. models worse than the CTC ones despite being larger? In appendix C we learn that they were not trained to the end due to the lack of resources, but that makes the result confusing, if not misleading if they are understood as baselines.\n\nIn the first test set, did you make sure that the same writer or document do not appear in the training and test set?\n\nMinor remarks:\n  - the format of citations needs to be fixed\n  - the \"remark\" column in Table I is not necessary\n  - Sec. 3.1: \"we have generate\" -> generated\n  - p5. CTC, attention and transformers are put in parallel when they correspond to different things (loss, mechanism, model)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6196/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698835918025,
            "cdate": 1698835918025,
            "tmdate": 1699636674471,
            "mdate": 1699636674471,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "s43XYctSSA",
                "forum": "L7KDMsqWl9",
                "replyto": "YqWFn7Skwd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6196/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6196/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reviewer [idkx]"
                    },
                    "comment": {
                        "value": "Thank you for taking the  time and effort to reviewing our paper.  We address your concerns and questions as follows.\n\n[**Content rearrangement**] we reorganized contents in appendix A, B moving a portion to section 1 and 2 of the main paper (updates are highlighted )\n\n[**Evaluate models proposed in baseline papers on other Ethiopic datasets**] we evaluate our HHD-ethiopic dataset using models proposed for Ethiopic script by Dikubab et al. (2022) and Belay et al. (2019a) and present the result in Table 3. However, neither the source code nor pretrained models are  provided by Abdurahman et al. (2021) and it is difficult to reproduce the results and use the model to evaluate our dataset. In addition, the dataset of Abduraman is a word level dataset extracted from just 1574 lines of modern Amharic handwritten texts, which follow different writing structures (Example in modern ethiopic writing space is a separator while both space and  \u201ctwo dots\u201d are used as a separator in historical scripts\u2013see Figure 11 in appendix section).  To add more, we evaluated our dataset using a similar CNN-RNN-CTC architecture proposed by Abdurahman et al. (2021), although we haven't accessed the implementation details provided by Abdurahman (See Table 3 in the main paper).\n\n[**Question on human performance and details of the annotation**]. We provided the details of the annotation process (in section 3.1 and appendix B, phase I&II) and human-level performance evaluation (in section 3.2 and appendix B, phase III).  To make it more clear, below is the answer for each question:\n >**How would the human performance be with access to reference material?** Those individuals participated in the human-level performance have no access for  references. The only individuals having access to the reference are people participating in Ground-truth annotation and their supervisors ( see  section 3.1 and appendix B, phase I).\n\n>**Do the annotators make the same errors or different ones?**  Annotators make different errors in the case of tests, as each text line is annotated by more than one person. However, in the training set, due to the cost associated with annotating each line by many annotators, each text-line image was annotated by an individual.\n\n>**Do the model make the same mistakes as the human evaluation?**  Based on our observations during the experiment and the sample predictions shown in Figure 7, the mistakes differ. Both models and humans made substitution errors, but the model tends to have a higher number of insertions and deletions.\n\n>**For the training set labeling, is there a measure of, for example, inter-annotator agreement?**  For the training set, since each text-line is annotated by an individual annotator we haven\u2019t measured inter-annotator agreement for the training set. However we measure the agreement between the supervisors and the expert for the test set (see the revised version of the main paper, section 4.1 highlighted in yellow).\n\n[**The paper is interesting and the proposal of a new dataset valuable, but it could be more suited to other venues like DAS, ICFHR, ICDAR or maybe ICML**],  Thanks for the suggestion. We  found that the \"datasets and benchmarks\" theme at ICLR 2024 aligns well with our work, and that's why we're interested in submitting to this venue.\n\n[**Why are CRNN, ASTER, SVTR, etc. models worse than the CTC ones despite being larger? In appendix C we learn that they were not trained to the end due to the lack of resources, but that makes the result confusing, if not misleading if they are understood as baselines**].  As discussed in Appendix C.2, we conducted runs for all models with a limited wall time, less than 24 hours, regardless of their complexity. The initial CTC-based models, with smaller parameters, underwent more epochs, whereas models with larger parameters could not. Furthermore, we implemented the CTC-based model using optimized hyperparameter values through Bayesian optimization (refer to section 3.2 of the main paper), resulting in improved outcomes.\n\n[**In the first test set, did you make sure that the same writer or document does not appear in the training and test set?**].  Text-line image samples in the IID test sets are randomly selected from the distribution using sklearn train-test split. For OOD, only 18th-century documents are chosen without mixing the same documents.\n\n[**Minor remarks**] We corrected and highlighted the changes."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6196/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700314179863,
                "cdate": 1700314179863,
                "tmdate": 1700314179863,
                "mdate": 1700314179863,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]