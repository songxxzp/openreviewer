[
    {
        "title": "MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models"
    },
    {
        "review": {
            "id": "JFUQRNd1YQ",
            "forum": "a7eIuzEh2R",
            "replyto": "a7eIuzEh2R",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1493/Reviewer_M8P4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1493/Reviewer_M8P4"
            ],
            "content": {
                "summary": {
                    "value": "This paper propose a new benchmark for evaluating LLMs' mapping and navigation abilities (MANGO) by constructing 53 mazes (language-described walkthrough) from textgames and questions asking the LLMs to find a destination or infer a route. Extensive filtering and human examination are applied to ensure the data quality. Chain-of-Thoughts and prompt engineering are considered for LLMs. A range of latest LLMs including GPT-3.5-Turbo, GPT-4, Llama-2, and RWKV are evaluated accordingly to the success rate of the models in responding to the destination and route finding questions. Experiments show that GPT-3.5 and GPT-4 achieve the best results while still performing poorly on hard questions and occasionally hallucinate nonexistent locations or edges. Analysis also demonstrates that LLMs with better mapping and navigation capabilities can better solve relevant downstream tasks, suggesting potential in addressing other embodied navigation tasks."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Investigating the mapping and navigation capabilities of LLMs is an emergent and practical problem in embodied AI. As a researcher in this field, I am aware that extensive efforts have been devoted to understand and reason about the 3D space, which can greatly facilitate many functions such as explainable localization, path planning, and human intervention in agent navigation. As a result, I am very happy to see the benchmark proposed in this paper which I believe can benefit relevant research. In particular,\n- The MANGO dataset is large, it is of an appropriate complexity and suitable for evaluating the LLMs; the selected mazes have clear and traversable structures and the walkthrough are described by rich texts, the spatial positions and agent's actions are nicely integrated, and the proposed destination and route finding questions are clear and effective to reflect the LLMs understanding and reasoning.\n- The proposed data has been carefully filtered and examed, especially with the help of human annotators, to ensure the accuracy of text descriptions/questions and traversable paths.\n- Dataset statistics, examples, and visualizations are clearly presented in this paper.\n\nBesides, this paper benchmarks the most recent (and popular) LLMs including GPT-3.5, GPT-4, Llama-2, and RWKV on MANGO, and performs comprehensive analyses on their resulting success/failure cases. \n- Important questions such as \"what makes those mazes challenging?\" are nicely investigated through quantifying factors such as number of locations and number of imputed edges, showing valuable insights of the LLMs understanding.\n- Critical issues such as \"LLMs occasionally hallucinate nonexistent locations or edges\" and \"non-GPT models with careful prompt tuning still suffer high chance of failing\" have been found, which might guide and inspire future relevant research.\n- It was good to see the experiments in Section 3.4 about evaluation on a downstream navigation task - a relatively simple case but it is a nice start (can be improved, consider how it might link to practical navigation in the real-world). \n\nOverall, this paper was an enjoyable read to me. It is well-motivated, it is technically sound, it introduces a novel and useful benchmark. The paper is also very nicely-written, to me, almost all information are clearly presented."
                },
                "weaknesses": {
                    "value": "1. The proposed MANGO constructs a simplified text-world, its connection to real-world navigation and mapping of embodied agents is unclear. Specifically,\n    - It assumes a known environment but many real-world navigation is only partially-observed.\n    - The structures of spaces and agents' actions in MANGO are very simple, whereas in the real-world they are often very diverse and complex.\n    - It only provides text data and it is hard to extend to visual inputs (consider the emerging large VLMs for addressing similar problems).\n\n2. This paper does not discuss any limitation and it is unclear how MANGO can be extended to more practical scenarios in the future."
                },
                "questions": {
                    "value": "Please address my concerns mentioned in the Weaknesses.\n\nSome questions below are not critical to my evaluation. \n1. Section 2.2 and Appendix: I might overlooked this somewhere but I didn't find clear explanation on why slightly different data is applied to evaluate different LLMs?\n2. Apart from the proposed metrics for DF and RF, do the authors think some navigation-oriented measurement might be helpful? e.g., Success weight by Path Length (SPL) (On Evaluation of Embodied Navigation Agents. Anderson et al., 2018).\n3. Any results on experimenting with different prompts for the LLMs? And any insight on how to write those prompts?\n4. The results shown in Tables are from a single-run of the LLMs or from multiple runs and averaged?\n\nFor the others, instead of just responding Yes/No, I hope the authors can share their thoughts that might help further improve this paper.\n1. The authors mentioned the drawbacks of using unique IDs for locations (e.g., L01, L02, L03, ...), but it is important in real world because sometimes a space is hard to label with a clear name or there might be many same type of rooms in a building. I wonder how would the results change if IDs instead of names are used in the experiments. I also wonder some commonsense might help in practical navigation (e.g., a kitchen is likely to be on the first floor next to the living room) so a clear name might be helpful.\n2. The walkthougt contains detailed descriptions of the observations at each location, how would the results change if those descriptions are removed?\n3. Many large Vision-Language Models (VLMs) (e.g., the lastest GPT-4V) have been considered in addressing mapping and navigation problems, with egocentric image or top-dowm map inputs the models have very rich and less ambiguous information than only describing the world with language. I wonder how would VLMs impact the research presented in this paper.\n4. What about tunning LLMs on MANGO, e.g., using adaptor for low compute cost, would the results become much better?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1493/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698546952926,
            "cdate": 1698546952926,
            "tmdate": 1699636078313,
            "mdate": 1699636078313,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NPffZEzD3b",
                "forum": "a7eIuzEh2R",
                "replyto": "JFUQRNd1YQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1493/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1493/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal 1 of 2"
                    },
                    "comment": {
                        "value": "Thank you very much for your supportive review. We are thankful that you acknowledge the value of the work. In our [General Response]s, we addressed the major concerns of the other reviewers, which also answers some of your questions. Here we try to address your remaining questions. \n\n> This paper does not discuss any limitations and it is unclear how MANGO can be extended to more practical scenarios in the future.\n\nWe thought about the limitations of this work but didn't include them in the submission since ICRL didn't ask for them. We agree that it is a good idea to add such a section and we will do so in the camera-ready version! Thank you! \n\nOur Limitations section will discuss the key differences between our benchmark and more realistic settings (e.g., simple actions) as well as how MANGO can be extended to more practical scenarios. \n\nA way to directly extend MANGO is to enrich its spatial and structural configurations on top of the current maps: \n- add spatial notions (e.g., distance in meters, area in square meters) such that one would need complex movements to achieve a target (e.g., not \"north\" but \"north 3 meters\"); \n- add notions of facing directions and rotations such that one would need to turn to switch facing directions. \n\nThis extension is straightforward but non-trivial and it is an interesting and useful future direction. \n\n> The proposed MANGO constructs a simplified text-world, its connection to real-world navigation and mapping of embodied agents is unclear. Specifically, it assumes a known environment but many real-world navigation is only partially-observed.\n\nOur MANGO environments are partially-observed as well: at each step, an LLM only sees the current view and \"remembers\" what it has seen in the past (i.e., walkthrough) but has no access to the full map. Further, MANGO requires navigational reasoning due to the existence of imputed edges (see Sec 2.2 in paper). \n\n> The structures of spaces and agents' actions in MANGO are very simple, whereas in the real-world they are often very diverse and complex. \n> It only provides text data and it is hard to extend to visual inputs (consider the emerging large VLMs for addressing similar problems). \n\nYes, our benchmark is more simplistic than real settings. Please see [General Response - Practicality and Impact] for our detailed discussion about this. \n\nWe will also discuss this limitation in the new Limitations section. \n\n> Section 2.2 and Appendix: I might overlooked this somewhere but I didn't find a clear explanation on why slightly different data is applied to evaluate different LLMs? \n\nBecause each LLM may read a walkthrough of a slightly different length due to their technical (e.g., tokenizer) differences. When the walkthrough gets shorter, some questions may become unanswerable since they involve locations and paths that are no way inferred from the given shorter walkthrough. \n\n> Apart from the proposed metrics for DF and RF, do the authors think some navigation-oriented measurement might be helpful? e.g., Success weight by Path Length (SPL) (On Evaluation of Embodied Navigation Agents. Anderson et al., 2018).\n\nThis is a sweet idea! Thank you. \nWe will add it in the camera-ready version. \n\n> Any results on experimenting with different prompts for the LLMs? And any insight on how to write those prompts?\n\nWe only lightly tuned the prompts. \n\nIn general, we follow the guidelines from CoCoGen [1] and CodeIE [2] to write the instructions, formulating the prompts in a structured format and asking the LLM to generate structured outputs. \n\nThere are small wording variations across models (e.g., \"nodes\" better for some models, but \"locations\" better for others; cases; !!! for emphasizing; etc). \n\nWe will publish all our prompts and experiment logs after the paper is published. \n\n> The results shown in Tables are from a single-run of the LLMs or from multiple runs and averaged? \n\nSingle run, but we set the temperature to be 0 for reproducibility."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549070775,
                "cdate": 1700549070775,
                "tmdate": 1700549070775,
                "mdate": 1700549070775,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7SqbyKiK5U",
                "forum": "a7eIuzEh2R",
                "replyto": "JFUQRNd1YQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1493/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1493/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal 2 of 2"
                    },
                    "comment": {
                        "value": "> The authors mentioned the drawbacks of using unique IDs for locations (e.g., L01, L02, L03, ...), but it is important in real world because sometimes a space is hard to label with a clear name or there might be many same type of rooms in a building. I wonder how would the results change if IDs instead of names are used in the experiments. \n\nBy default, IDs are latent: they are not given in the walkthrough; one can only see them by reading the source code. \n\nWe agree that it is an interesting idea to include IDs in the walkthrough and see how results change. The results may be improved for the reasons you mention; the results may be hurt because tracking many non-descriptive IDs doesn't seem to be a trivial task for LLMs. It is indeed interesting to try it. \n\nWe also agree that real-world locations may be hard to label, and we encountered such settings even in the games during data annotation. That is why we took care to resolve the location names (see A.2) such that similar but distinct locations can be distinguished. We chose to do so because we believed descriptive names are more LLM-friendly, but we might be wrong. \n\nDo you think it is a reasonable improvement of our work if we do the following?  \n- Release a version of our data that includes IDs in the walkthrough; \n- Evaluate some models on a small set of the test cases with IDs. \n\n> I also wonder some commonsense might help in practical navigation (e.g., a kitchen is likely to be on the first floor next to the living room) so a clear name might be helpful. \n\nYes, we agree. Though our maps are taken from fictional games, many plots reflect realistic settings over certain time periods, such as 905 (1990's, typical offices and residential houses), detective (1974, London, a typical British mansion), and ludicorp (2000's, a typical large office building). But we didn't find a clear trend in performance: GPTs got high scores on 905, but medium to low scores on ludicorp.  \n\n> The walkthrough contains detailed descriptions of the observations at each location, how would the results change if those descriptions are removed?\n\nThat is what we investigated in the \"simplified setting\"; please see Sec 3.1 and 3.2 on page-5. Those results are labeled with the tag \"-S\" in Tab-1: the changes are small and there is no clear trend. \n\n> Many large Vision-Language Models (VLMs) (e.g., the latest GPT-4V) have been considered in addressing mapping and navigation problems, with egocentric image or top-down map inputs the models have very rich and less ambiguous information than only describing the world with language. I wonder how would VLMs impact the research presented in this paper.\n\nBy establishing this benchmark, we aim to make a focused contribution that complements the work on vision-based mapping and navigation. \n\nPlease see [General Response - Practicality and Impact] for more details. \n\n> What about tunning LLMs on MANGO, e.g., using adaptor for low compute cost, would the results become much better?\n\nYes, we think it will be better. \n\nThe primary goal of our MANGO benchmark is to function as a testbed for 0-shot ability of LLMs in mapping and navigation. In other words, it is test-only. We regard this as the most interesting thing to track since 0-shot transferability is a fundamental property of intelligence. In this regard, we are similar to other test-only benchmarks such as MMLU [3].  \n\nBut we agree that it is interesting to investigate how low-cost-adapting can improve the performance. In particular, an interesting investigation is how an LLM can learn from a few mazes and then transfer to the others. We will list this as a future work in our camera-ready. \n\nReferences: \n\n[1] Madaan et al. EMNLP 2022. Language Models of Code are Few-Shot Commonsense Learners. \n\n[2] Li et al. ACL 2023. CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors. \n\n[3] Hendrycks et al. ICLR 2021. Measuring Massive Multitask Language Understanding."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549100926,
                "cdate": 1700549100926,
                "tmdate": 1700549100926,
                "mdate": 1700549100926,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s2IDmzDJ40",
                "forum": "a7eIuzEh2R",
                "replyto": "JFUQRNd1YQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1493/Reviewer_M8P4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1493/Reviewer_M8P4"
                ],
                "content": {
                    "title": {
                        "value": "Final Rating"
                    },
                    "comment": {
                        "value": "Thank the authors for the very detailed response (and the new results)! Thanks for the great work! Most of my concerns have been nicely addressed.\n\nThe part that I am still not convinced about is the discussion related to visual models. As a researcher in Embodied AI+LMs, I feel the rebuttal overclaims the practical usefulness of language (and text-only LMs) in mapping and navigation. MANGO is indeed orthogonal to vision-language multimodal LLMs, but we still need to understand its practical impact on real-world applications. This is why I keep highlighting real-world scenarios in my review and raised points such as partially observable, IDs for locations, extension to visual inputs, etc.\n\nNevertheless, I still think this paper is a good start, and it will benefit many emerging research that integrates LLMs in embodied AI systems for planning and high-level decision-making. I will keep my rating as an Accept. \n\nFor the questions:\n```\nDo you think it is a reasonable improvement of our work if we do the following?\n(1) Release a version of our data that includes IDs in the walkthrough;\n(2) Evaluate some models on a small set of the test cases with IDs.\n```\n(1) If it is not too much of a workload, you could, just in case future research might want to use it. \n(2) Maybe necessary. But if you are going to run this, consider using \"Bathroom 01, Bathroom 02, Bathroom 03, ...\" (a room label + ID without the descriptive marks)."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700620570638,
                "cdate": 1700620570638,
                "tmdate": 1700620570638,
                "mdate": 1700620570638,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "O5cQwlraaz",
                "forum": "a7eIuzEh2R",
                "replyto": "LznAFhRPVa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1493/Reviewer_M8P4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1493/Reviewer_M8P4"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks! Great to hear that! I just noticed that I wrote a typo for (2), which should be \"unnecessary\" :) Good luck with the paper!"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700629269232,
                "cdate": 1700629269232,
                "tmdate": 1700629269232,
                "mdate": 1700629269232,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pfPVV3Ta84",
            "forum": "a7eIuzEh2R",
            "replyto": "a7eIuzEh2R",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1493/Reviewer_fKBg"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1493/Reviewer_fKBg"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to evaluate the mapping and navigating abilities of large language models (LLMs) by proposing a new dataset called MANGO, which comprises 53 mazes taken from Zork-I. The LLMs are given a walkthrough as input and tasked with completing two types of tasks: destination-finding and route-finding. The study evaluates GPT-3.5, GPT-4, LLaMa, and RWKV models on this dataset and provides an analysis of the results for GPT models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is well-written, with clear explanations of dataset construction and experiments.\n2. The study focuses on evaluating the mapping and navigating abilities of LLMs, which are important for both natural language processing and robotics. Many current robotics benchmarks overlook these challenges, using high-level functions like navigate_to(target_location) as an atomic operation. This paper highlights the challenges of these tasks and proposes a new dataset to test the abilities of LLMs."
                },
                "weaknesses": {
                    "value": "1. The proposed dataset does not effectively test mapping and navigating abilities, as the samples can be easily converted into a graph with locations as nodes and directions as relations. This is too simplistic for most real-world robotics scenarios, which involve more complex object and position relationships. For example, the robots in a house, or robots (cars) on the street may be facing much more complex scenes.\n2. The simplicity of the current dataset means it could be solved by an agent translating natural language into <source, path, destination> triples, then using code or search libraries. The natural language is generated by patterns, making natural language understanding easy. This paper may have limited research impact, as future studies might follow the path of the GSM8K dataset, using methods such as PAL or LLMs with code interpreters as tools."
                },
                "questions": {
                    "value": "1. Can this task be addressed using traditional search algorithms like depth-first-search or breadth-first-search?\n2. It is suggested that the authors test additional LLMs, particularly those pre-trained on code and fine-tuned on instructions, to provide a more in-depth analysis."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1493/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1493/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1493/Reviewer_fKBg"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1493/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698674067494,
            "cdate": 1698674067494,
            "tmdate": 1700795637207,
            "mdate": 1700795637207,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wq8CN1ga3M",
                "forum": "a7eIuzEh2R",
                "replyto": "pfPVV3Ta84",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1493/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1493/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your review. We have answered some of your questions in the [General Response]s. Here we try to address your remaining concerns. \n\n> It is suggested that the authors test additional LLMs, particularly those pre-trained on code and fine-tuned on instructions, to provide a more in-depth analysis.\n\nWe did this for you! Please read [General Response - New Results] for details. \n\n> The proposed dataset does not effectively test mapping and navigating abilities, as the samples can be easily converted into a graph with locations as nodes and directions as relations. This is too simplistic for most real-world robotics scenarios, which involve more complex object and position relationships. For example, the robots in a house, or robots (cars) on the street may be facing much more complex scenes.\n> The simplicity of the current dataset means it could be solved by an agent translating natural language into <source, path, destination> triples, then using code or search libraries. The natural language is generated by patterns, making natural language understanding easy. \n> Can this task be addressed using traditional search algorithms like depth-first-search or breadth-first-search?\n\nWe agree that our benchmark is simpler than your examples, but it is still challenging for best-to-date LLMs and functions as a testbed where interesting analysis can be done. \n\nBut please note that it can not be easily converted into a symbolic graph since language-to-structure mapping is not easy to learn. Please read [General Response - Have We Evaluated Strong Methods?] for more discussion."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548836976,
                "cdate": 1700548836976,
                "tmdate": 1700548836976,
                "mdate": 1700548836976,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DJ6IJGK3Pz",
                "forum": "a7eIuzEh2R",
                "replyto": "wq8CN1ga3M",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1493/Reviewer_fKBg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1493/Reviewer_fKBg"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate the additional results obtained with an LLM that has been pre-trained on code and fine-tuned on instructions, which provides new insights.\n\nNevertheless, I have concerns regarding the challenge of mapping unstructured text to structured forms. Your dataset focuses solely on the source location, action direction, and target location, which primarily demands the ability for named entity recognition, a task considerably simpler than First-Order Logic. By prompting the LLM to translate the text into (S, P, D) triplets, we can address these issues using a search algorithm. To better illustrate my concerns, I suggest you could prompt the LLM to translate a walkthrough path into (S, P, D) triplets and evaluate its accuracy. I believe this would more accurately reflect the complexity of your dataset."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700619773192,
                "cdate": 1700619773192,
                "tmdate": 1700619773192,
                "mdate": 1700619773192,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JBzxS0AFca",
                "forum": "a7eIuzEh2R",
                "replyto": "at7b6jk4KP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1493/Reviewer_fKBg"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1493/Reviewer_fKBg"
                ],
                "content": {
                    "comment": {
                        "value": "Can you try this setting:\nInput one step in the \"example 1\" to ChatGPT, then prompt it to generate a (S, P, D) triplet. Please provides five examples to ChatGPT as few-shot demonstration."
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644688724,
                "cdate": 1700644688724,
                "tmdate": 1700644688724,
                "mdate": 1700644688724,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TGag4mx6jj",
            "forum": "a7eIuzEh2R",
            "replyto": "a7eIuzEh2R",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1493/Reviewer_eNST"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1493/Reviewer_eNST"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a new benchmark dataset for evaluating the mapping and navigation abilities of large language models. The authors construct 53 mazes from a suite of textgame. Given a walkthrough that visits every location in the maze, the LLM is tested with a suite of synthetically generated destination-finding and route-finding questions. The authors test with three families of LLMs, RWKV, LLAMA-2 and GPT, as well as LLMs of different sizes and notice clear performance gap in terms of model capabilities. They further conduct deep comparison between GPT 3.5 and GPT 4 on a range of controlled experiments which helps understand the factors that affect model performance, as well as the association with downstream tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The data collection protocol is carefully designed and clearly stated in the paper. Making it easy to follow. The design of the task is elegant, challenging for models, yet rather straight forward for humans. The collection effort is non-trivial and the authors have carefully cleaned the data. \n2. The author conducts thorough comparison between GPT 3.5 and GPT 4 and extensive controlled experiments to help understand the model performance and how it associates with different features of the task."
                },
                "weaknesses": {
                    "value": "1. While the experiments in the paper do demonstrate that the task is challenging for LLMs, it is unclear for me how the proposed benchmark differs from all other datasets in the literature. In particular:\n    1. Is it necessary to derive the dataset from real textgames? Would it work to use pure synthetic data, similar to how some of the symbolic reasoning datasets such as SCAN are created.\n    2. How this compares to datasets used in embodied AI and NL navigation such ALFRED?\n    4. Does the dataset reveal strength/weakness of the LLMs that is overlooked on other datasets?\n2. The authors conduct extensive experiments on GPT-3.5 and GPT-4 which is very helpful. However, instead of focusing on models that already perform good, I feel the paper could benefit from more experiments on:\n    1. Why the other models perform much worse than GPT models, although they have demonstrated great performance on other tasks.\n    2. Why for models like RWKV and LLAMA the size does not seems to affect the performance much.\n    3. Are specific model designs, such as attention, position embedding, instruction tuning, affect the performance?\n3. In the paper the LLMs are prompted to directly generate the solution, with small amount of COT reasoning. This is actually different from how human solve the task, where oftentimes we need to parse the walkthrough and draw the map first. This is also how many embodied agent/NL navigation works have built up the system. I would expect a baseline on this direction to have much better performance."
                },
                "questions": {
                    "value": "1. Is there comparison between the proposed benchmark with pure synthetic generated mazes to demonstrate the value of construct the dataset from real textgames?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1493/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1493/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1493/Reviewer_eNST"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1493/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699000322162,
            "cdate": 1699000322162,
            "tmdate": 1699636078139,
            "mdate": 1699636078139,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "j2w7H2ViHB",
                "forum": "a7eIuzEh2R",
                "replyto": "TGag4mx6jj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1493/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1493/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your review. We have answered some of your questions in the General Responses. Here we try to address your remaining concerns. \n\n> need to parse the walkthrough and draw the map first\u2026 I would expect a baseline on this direction to have much better performance. \n> small amount of COT reasoning\u2026 parse the walkthrough and draw the map first\u2026 a baseline on this direction to have much better performance.\n\nPlease see [General Response - Have We Evaluated Strong Methods?] in [General Response]s. \n\n> how the proposed benchmark differs from all other datasets in the literature. \n> How does this compare to datasets used in embodied AI and NL navigation such as ALFRED? \n> Does the dataset reveal strength/weakness of the LLMs that is overlooked on other datasets?\n\nPlease see [General Response - Novelty]: basically, our MANGO focuses on an important dimension---i.e., mapping and navigation---which is different from the foci of other datasets. \n\n> Is it necessary to derive the dataset from real textgames? Would it work to use pure synthetic data, similar to how some of the symbolic reasoning datasets such as SCAN are created.\n> Is there comparison between the proposed benchmark with pure synthetic generated mazes to demonstrate the value of construct the dataset from real textgames?\n\nPlease see [General Response - Practicality and Impact]. \n\n> Why the other models perform much worse than GPT models, although they have demonstrated great performance on other tasks. Why for models like RWKV and LLAMA the size does not seems to affect the performance much. Are specific model designs, such as attention, position embedding, instruction tuning, affect the performance?\n\nWe would love to analyze these, and please see [General Response - New Results]. \n\nOur new results include the effects of different tuning methods."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548813817,
                "cdate": 1700548813817,
                "tmdate": 1700548813817,
                "mdate": 1700548813817,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Es8mAoTfAr",
                "forum": "a7eIuzEh2R",
                "replyto": "j2w7H2ViHB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1493/Reviewer_eNST"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1493/Reviewer_eNST"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the detailed response and additional results, they addressed some of my concerns regarding model performance. However, I would like to remain my original score of weak reject, as while I appreciate the contribution of the proposed work, I feel it still needs further improvement for conference competitive as ICLR.\n1. Discussion / experiment with existing datasets & synthetically generated data are not sufficient. While some of the synthetic  dataset like SCAN has a simple format/action space. It can be composed into complex tasks that require deep reasoning and mapping skill of the model. Text games provides nice and user friendly textual description of the environment, but I am not sure if that is necessary for testing the mapping/navigation ability of the model. It is unclear whether the challenge for the model lies in abstracting the map / environment from the NL description, or in memorize / trace the map during reasoning. The former is where rich NL descriptions from text games would help, while the latter can potentially be tested with other methods. To justify the necessity of proposed method, I feel experiment on this front is needed.\n2. I appreciate the thoughts in \"Have We Evaluated Strong Methods?\". It would make the paper much stronger if those assumptions are verified via experiments."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700616204794,
                "cdate": 1700616204794,
                "tmdate": 1700616204794,
                "mdate": 1700616204794,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rsVIvxlzko",
                "forum": "a7eIuzEh2R",
                "replyto": "3dKywiN9hM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1493/Reviewer_eNST"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1493/Reviewer_eNST"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the prompt response.\n\nFirst I want to clarify that I am just using SCAN as an example of cases where synthetically generated examples could still be challenging and good at examine specific capability of the model. Still, if the focus is to examine the map and navigation ability of LLM, why not synthetically generate examples, what is the advantage of using real text games with extra processing? I feel experiment support on this front is needed. Most language navigation and embodied AI benchmarks are generated programmatically, what are the things that work for them, but not in the case here?\n\nRegarding stronger methods, my intention is actually the last point you mentioned \n>a map would only be useful if it was correct and it was used in a symbolic search algorithm ... the questions would have become trivial using traditional search algorithms if a correct structured representation was built for a map. However, **building such a representation---i.e., parsing a walkthrough into a map, is a very difficult task in the first place since it is generally a very challenging task to map unstructured text to structured forms**\n\nThere are different ways of handling mapping and navigation with LLM, and parsing unstructured text to structured forms is one way. Without experiment, it is hard to say whether this is more challenging than the prompting method used in the paper, or it will solve the task much better.\n\n>Our MANGO benchmark can be used to evaluate if an LLM can build such a structured representation internally\n\nSimilarly, I feel it is equally important to evaluate whether the LLM can build such representation internally, or parse and handle it with external tools.\n\nWith all the different benchmarks we already have today to evaluate different aspects of the LLM, more highlights on the key differentiating factor of the proposed dataset and its practical implications would make the submission stronger."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700638908106,
                "cdate": 1700638908106,
                "tmdate": 1700638908106,
                "mdate": 1700638908106,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "yvCfpSL4ig",
            "forum": "a7eIuzEh2R",
            "replyto": "a7eIuzEh2R",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1493/Reviewer_14NH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1493/Reviewer_14NH"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a benchmark that evaluates the mapping and navigation abilities of LLMs. The proposed benchmark covers 53 mazes as well as evaluation strategies taken from text games dataset and modified to suit the requirement for the benchmark tasks. As base model performance is poor, hence it is claimed for future scope of research. Also, the authors promise to release the data and code. The authors claim their work as a first to measure the mapping and navigation abilities of LLMs. However, the novelty and hardness of the work done is not established.\nSuggestions:\nAn experiment might be comparing trained human performance in similar task vs a data trained LLM."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper does a good work of curating a benchmark for text games based navigation and mapping. \nThe metrics of evaluation for two identified tasks, namely DF and RF questions (destination, route) gives an head start to reuse existing datasets posing them for a different problem."
                },
                "weaknesses": {
                    "value": "I will like to see the benchmark performance for random responses (within a class bound) to verify the amount of information gained by base model. \nNeed some explaination regarding para before 3.4 in terms of pilot experiments.\nRelated work should be broken into sub-sections of research topics - the current norm - for ease of readibility.\nThe purpose of Fig. 6 which is too much info is not clear.\nIn page 7, what makes the maze challenging needs some results to support the text descriptions below.\nThe use cases and applicability in real life scenarios like robotics is not well established, requesting to look to the plethora of work in embodied intelligence and adapting the problem in that regard."
                },
                "questions": {
                    "value": "How will the system evolve if vision language based models like CLIP need to be tested - as that is more practical?\nHow fruitfull are game environments to real life human occupied or indutstrial environments? Is the transfer easily equitable?\nHow are the easy and hardness of the DF, RF questions come up to? How does it vary with dataset characteristic changes?\nWhat are the runtimes for the experimental evaluations? \nAre any subset minival of the dataset available for checking the model performance quickly? \nAlso, why restriction to GPT based models only?\nHow is ambiguity in location and maps resolved? Any technical relation with length of text description?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1493/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699261935655,
            "cdate": 1699261935655,
            "tmdate": 1699636078051,
            "mdate": 1699636078051,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "jYo4jRLN2U",
                "forum": "a7eIuzEh2R",
                "replyto": "yvCfpSL4ig",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1493/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1493/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you very much for your review. We have answered some of your questions in the [General Response]s. Here we try to address your remaining concerns. \n\n> The use cases and applicability in real-life scenarios like robotics is not well established, requesting to look to the plethora of work in embodied intelligence and adapting the problem in that regard.\n> How fruitful are game environments to real life human occupied or industrial environments? Is the transfer easily equitable? \n> How will the system evolve if vision language based models like CLIP need to be tested - as that is more practical? \n\nThere is a lot of interest in the robotics community in using language as a medium decomposing tasks into a sequence of subtasks that better match the primitives available to robots. We see mapping and navigation as one such example. Indeed, our results may not be immediately applicable to real-world embodied agents. However, as people in the robotics community increasingly rely on available LLMs for reasoning, there is merit in understanding the capabilities of the off-the-shelf models. \n\nWe have similar and other relevant arguments in [General Response - Practicality and Impact]. \n\nFor our discussion on vision language models like CLIP, please see our [General Response - Have We Evaluated Strong Methods?]. \n\n> benchmark performance for random responses (within a class bound) to verify the amount of information gained by the base model. \n\nWe had random results in an earlier version but removed them because they were too bad. We can add them back to the appendices, but we could assure you that those numbers are nearly 0 compared to the results of the LLMs. \n\n> Need some explanation regarding para before 3.4 in terms of pilot experiments. \n\nThat entire paragraph is explaining the pilot experiments, isn't it? \n\n\nIf what you'd like are the detailed numbers, we can surely add them in the appendices of the camera-ready version. \n\nOtherwise, can you be specific about what precisely in this paragraph needs clarification so we can make more targeted revisions? Thank you very much! \n\n> Related work should be broken into sub-sections of research topics - the current norm - for ease of readability. \n\nBreaking into subsections with headings is a nice idea and we know many papers do it. \n\nIn our Related Work, each paragraph is about a different field or topic; it just does not have a boldfaced heading. This way is also widely adopted by many well-written (and even award-winning) papers. \n\nWe prefer our current style because it reads more smoothly (i.e., not being interrupted by headings). \n\nThis taste of style is very subjective. Perhaps a different writing style should not be considered as a \"weakness\"? \n\n> The purpose of Fig. 6 which is too much info is not clear. In page 7, what makes the maze challenging needs some results to support the text descriptions below. \n\nFig-6 is similar to Fig-3 but its metric is reasoning accuracy. The design of Fig-3 is explained in Sec 3.3. Results supporting page-7 are in Tab-2, which is referred to in page-7 (\"Table 2 displays the regression results.\"). \n\n> How are the easy and hardness of the DF, RF questions come up to?\n> How does it vary with dataset characteristic changes? \n\nPlease find the definitions of \"easy\" and \"hard\"in Sec 2.2. They depend on \"edges that are not covered in the prefix\" of the walkthrough. Sec 2.2 also explains how an easy question may become hard once the walkthrough gets longer, as well as how to determine if it is easy or hard based on the current walkthrough (see the definition of EASY label). \n\n> What are the runtimes for the experimental evaluations? \n\nFor GPT-3.5 and GPT-4, the runtimes are primarily constrained by the speed limitation of OpenAI API calls. Completing both tasks for all mazes on two 48GB A6000 GPUs takes: \n- 2 days for the series of Llama models; \n- 3 days for RWKV. RWKV is slower because it reads longer walkthroughs. \n\nWe will add these details to the camera-ready version. \n\n> Are any subset minival of the dataset available for checking the model performance quickly?\n\nWe will publish all the data and our experiment logs once the paper is published. \n\n> Also, why restriction to GPT based models only? \n\nBy \"GPT based\", do you mean \"Transformer\" or \"decoder-only\" or \"OpenAI GPT\"? \n\nDecoder-only models become predominant because they can generate texts and they are efficient to train and do inference with. Encoder-only models can not generate; encoder-decoder models are not efficient. \n\nWe evaluate the strongest-to-date LLMs and they are all decoder-only. \n\nMost of them are Transformer-based. RWKV is an RNN-Transformer hybrid. \n\nOnly two models are OpenAI GPTs. We also evaluated Llamas and RWKV. \n\n> How is ambiguity in location and maps resolved? \n\nA.2 and A.3 are about location and move resolutions."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1493/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700548780960,
                "cdate": 1700548780960,
                "tmdate": 1700548780960,
                "mdate": 1700548780960,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]