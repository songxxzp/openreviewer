[
    {
        "title": "Language-Interfaced Tabular Oversampling via Progressive Imputation and Self-Authentication"
    },
    {
        "review": {
            "id": "XJhgMjNFBO",
            "forum": "8F6bws5JBy",
            "replyto": "8F6bws5JBy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6977/Reviewer_tYE6"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6977/Reviewer_tYE6"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a way to synthesize minority tabular samples using large language models in order to handle class-imbalance. A self-authentication framework is desiged that uses language models to predict the label of the generated sample and sifts out ill-converted samples. Experiments are conducted on a variety of datasets and imbalance ratios."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Class-imbalance learning for tabular data is a under-exploited area. This paper makes a meaningful attempt.\n- Synthesizing minor classess with large language models seems to be new.\n- The proposed method is simple to implement. The conversion from tabular to textual representation and the self-authentification with LLM prediction can generalize to different LLMs."
                },
                "weaknesses": {
                    "value": "- My biggest concern is on the reliability of those minor samples generated by LLMs. LLMs are shown to easily generate counter-factual texts. For the tabular data, although the authors use a self-authentification framework to filter out samples with incorrect prediction, there is no guarantee that the remaining samples are semantically meaningful. Considering the different feature columns, I doubt that current LLMs can truly understand the numeric feature values and their relative relation. \n- The language interfacing requires column feature names. In many tabular data, feature names may not have meaningful semantics such as an abbreviation. It would be hard for LLMs to understand those features."
                },
                "questions": {
                    "value": "- To understand LLM as a self-authenticator, I wonder the accuracy of directly using LLM to make classification on original data."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6977/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6977/Reviewer_tYE6",
                        "ICLR.cc/2024/Conference/Submission6977/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6977/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698690924124,
            "cdate": 1698690924124,
            "tmdate": 1700675238766,
            "mdate": 1700675238766,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kqOo5endcM",
                "forum": "8F6bws5JBy",
                "replyto": "XJhgMjNFBO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6977/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6977/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer tYE6 (1)"
                    },
                    "comment": {
                        "value": "We sincerely appreciate the reviewer tYE6\u2019s constructive feedback. We address the reviewer\u2019s concerns below.\n\n---\n\n### [W1, W2. Understanding the semantics of feature names.]\n\nAs the reviewer has mentioned, the performance of LLMs may vary based on the semantic meaning of column feature names, as observed in [1]. However, even in the absence of column names, the language model does not break down in performance but rather is capable of processing the given data via its general sequential modeling capability originating from the pretrained weights when it is fine-tuned on the tabular dataset. For example, [2] leverages such capabilities to process time series data, which do not contain linguistic meaning.\n\nThis capability is further observed in our main experiments. In the datasets used in our experiments, there are cases where column names do contain semantic meaning (e.g., Shoppers, Diabetes), contain semantics but lack context (e.g. Default), contains abbreviations (e.g., Sick, Obesity), or do not contain any meaning (e.g. Satimage). Below, we have listed selected column (feature) names for each dataset:\n\nDefault: ['LIMIT_BAL', 'PAY_2', 'BILL_AMT1', 'PAY_AMT1', 'PAY_AMT2', \u2026]\n\nShoppers: ['ExitRates', 'PageValues', 'SpecialDay', 'Month', 'VisitorType', \u2026]\n\nDiabetes: [\u2019Glucose\u2019, \u2018BloodPressure\u2019, \u2018Insulin\u2019, \u2018BMI\u2019, \u2018Age\u2019, \u2026]\n\nSick: [\u2019TSH\u2019, \u2018T3\u2019, \u2018TT4\u2019, \u2018T4U\u2019, \u2018FTI\u2019, \u2026]\n\nObesity: [\u2019FAVC\u2019, \u2018NCP\u2019, \u2018CAEK\u2019, \u2018CH2O\u2019, \u2018FAF\u2019, \u2026 ]\n\nSatimage: [Aattr, Cattr, A1attr, D10attr, D28attr, \u2026]\n\nEven presented with a wide and versatile range of semantics levels for feature names, the main experiments in Section 4 show that our method surpasses other methods in the oversampling task. In Appendix C.1.1 of the revised manuscript, we provide the feature-wise histogram visualizations of synthetic samples w.r.t. the real distribution, where our method exceeds other baselines in modeling individual columns of varying levels of semantics (e.g. PAY_2, Month, T4U, D10attr, Cattr), even for datasets such as Satimage where feature names are devoid of meaning.\n\n---\n\n### [W1. Reliability of the synthesized samples.]\n\nSince the backbone tabular language model(TLM) of LITO is first fine-tuned on a given tabular dataset, the TLM can successfully learn the given tabular data and the relative relations of columns, as shown in [1, 3]. However, as the reviewer has mentioned, recent observations in language models have shown that they can generate counterfactual texts. This phenomenon is exacerbated when the availability of minority-class samples is low, which is verified in our experimental observation where a TLM baseline(GReaT) produces samples that intrude into the distribution of other classes even when it is prompted to generate a certain minority class, which can be considered as counterfactual generation. This is in fact why we employ self-authentication to filter out the counterfactual samples. Moreover, our importance-aware imputation method only re-samples the important columns, keeping other columns in the real data distribution - reducing counterfactual noise.\n\nVisualizations in Appendix C.1.2 show that our method effectively reduces counterfactual generation(intrusion into other classes) compared to baselines.  Quantitative analyses in Appendix C.2 indicate that LITO produces meaningful samples(coverage) while suppressing counterfactual samples(spillage).\n\n---\n\n### [Q1. classification accuracy of the language model on original data.]\n\nIn the table below, we report the classification performance of the backbone TLM on the test dataset:\n\n| | F1 | bACC |\n| --- | --- | --- |\n| Default | 50.41 | 52.82 |\n| Shoppers | 60.63 | 62.35 |\n| Sick | 67.51 | 68.20 |\n| Diabetes | 54.24 | 53.02 |\n| Obesity | 51.58 | 52.14 |\n| Satimage | 75.15 | 76.21 |\n\n---\n\n[1] Dinh et al. \"LIFT: Language-interfaced fine-tuning for non-language machine learning tasks.\" NeurIPS 2022.\n\n[2] zhou et al. \u201cOne Fits All: Power General Time Series Analysis by Pretrained LM.\u201d NeurIPS 2023.\n\n[3] Borisov et al. \"Language models are realistic tabular data generators.\" ICLR 2023."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6977/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700484786750,
                "cdate": 1700484786750,
                "tmdate": 1700484817078,
                "mdate": 1700484817078,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8Cwy613pPd",
                "forum": "8F6bws5JBy",
                "replyto": "kqOo5endcM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6977/Reviewer_tYE6"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6977/Reviewer_tYE6"
                ],
                "content": {
                    "title": {
                        "value": "Reply to author rebuttal"
                    },
                    "comment": {
                        "value": "I appreciate the authors' response on my previous concerns. I am still not quite convinced by the explaination \"...but rather is capable of processing the given data via its general sequential modeling capability\" and the example with time series data. To me, time series data have an inherent causality between previous and current timesteps, while columns of tabular data are disordered. \n\nGiven the provided evidence that the propose method can exceed other baselines and reduce counterfactual generation, I decided to raise my score to be positive."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6977/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700675204747,
                "cdate": 1700675204747,
                "tmdate": 1700675204747,
                "mdate": 1700675204747,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "aId060Mwe8",
            "forum": "8F6bws5JBy",
            "replyto": "8F6bws5JBy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6977/Reviewer_gyRK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6977/Reviewer_gyRK"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the class imbalance problem in tabular data processing. Specifically, one major drawback of class imbalance in tabular data is that the minority data is not sufficient to help learn a good predictor. In order to solve this problem, the authors propose to leverage the language model to synthesize minority data. Specifically, there are three major parts, first, the minorities are generated by conditioning on their class labels; then, some incorrect examples are rejected through self-authentication; finally, the authors propose an adaptive oversampling by progressively imputation. Through extensive quantitative experiments, the proposed method shows a larger performance improvement than other baseline methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper is well-written and well-organized.\n- Experimental validations are extensive and effective over other methods.\n- The minority generation can effectively solve the class imbalance problem."
                },
                "weaknesses": {
                    "value": "- There have already been many methods that leverage language models to augment data. Using a similar strategy to solve the class imbalance problem seems to be a trivial contribution to me. Could the authors identify why the proposed method could be a novel contribution?\n- Whether the generation of minority data could help the learning performance is still unclear to me. It would be better if the quality of the synthesized data could be carefully evaluated. Moreover, how many examples are rejected during self-authentication? It would further improve the quality of this paper if a proper ablation study were conducted.\n- How can different language models influence the performance of the proposed method? Could the author try it on GPT-3 or ChatGPT?\n- Minor: The Algorithm is taking up too many blank spaces."
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "No ethics concers"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6977/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6977/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6977/Reviewer_gyRK"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6977/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698915517159,
            "cdate": 1698915517159,
            "tmdate": 1700666652675,
            "mdate": 1700666652675,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "79BqqvPKoJ",
                "forum": "8F6bws5JBy",
                "replyto": "aId060Mwe8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6977/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6977/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gyRK (1)"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer gyRK for their insightful comments. We address the reviewer's concerns below.\n\n---\n\n### [W1. Novel contribution of the proposed method.]\n\nWhile data augmentation strategies via language models are on the rise, our work focuses on whether language models are practically applicable for tabular data generation under class-imbalanced situations, which are frequent in the tabular domain. To this end, we propose a novel oversampling method with two components: progressive importance-aware imputation and self-authentication. In comparison to prior works, our paper introduces novel contributions in both its findings and methodology, which include:\n\n1. Experimental observations (Section 4.2, 4.3, and Appendix C of the revised manuscript) show that naive oversampling with a tabular language model(GReaT) fine-tuned on imbalanced data to oversample minority class samples results in low-quality samples that degrade downstream classification performance(as in samples that overlap into the distributions of other classes), due to the scarcity of minority class samples.\n2. To filter these samples and enhance sample quality, we propose self-authentication, which utilizes the language model itself. Our experiments in Section 4.4 show that self-authentication is an effective component for increasing the quality of the generated samples, which is reflected in the increase in downstream classification performance.\n3. We propose progressive importance-aware imputation, a novel borderline sampling strategy that integrates the domain characteristics of tabular data and the capabilities of language models.\n    * Unlike other data modalities(such as vision, speech, \u2026) where class-relevant information is spread across features, the features of tabular data each hold distinctive information, where there exists columns important to the given task, and also relatively unimportant columns[1]. However, previous borderline sampling strategies such as [2, 3] manipulate all features simultaneously - introducing potential noise in the unimportant columns thereby degrading sample quality.\n    * Using this domain characteristic, we propose to iteratively select, puncture, and re-impute the important columns, using the conditional sampling capability of the language model. This ensures the feature quality of the unimportant columns as they stay in the real distribution, while important columns are resampled for diversity.\n    * To identify the instance-wise important columns,  we propose to leverage the self-attention maps of the language model to measure the feature importance."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6977/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700484191759,
                "cdate": 1700484191759,
                "tmdate": 1700484314413,
                "mdate": 1700484314413,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9Yf9V1Z8qS",
                "forum": "8F6bws5JBy",
                "replyto": "aId060Mwe8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6977/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6977/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gyRK (2)"
                    },
                    "comment": {
                        "value": "### [W2. Quality of the synthesized data.]\n\nAs observed in our main experiments(Section 4.2, 4.3) of the main paper, the classification performance increase of various downstream machine learning classifiers (logistic regression, xgboost, ... etc.) augmented with LITO oversampling reflects that our method generates quality samples that are effective in increasing classification performance compared to other baselines.\n\nTo further examine the qualities of the synthesized data, we undertake both qualitative and quantitative analysis of synthesized samples. The detailed figures and descriptions are provided in Appendix C.1 and C.2 of the revised manuscript.\n\nIn our qualitative analysis, following the methodology of previous work[2], we present **feature-wise histograms** and **UMAP**[4] **visualization** plots for synthesized samples for various datasets, which are included in Appendix C.1. These histograms reveal that LITO's column features more closely match the actual minority distribution than other comparison baselines, demonstrating the quality of the samples synthesized with our method. Additionally, we construct UMAP diagrams to visually compare the manifolds occupied by both the training data and the synthetic minorities created by our method and other baselines. The results demonstrate the ability of LITO to accurately represent the true minority distribution while avoiding intrusion into the distribution of other classes.\n\nTo quantitatively assess the quality of synthesized samples, we employ the Coverage Score[5] and measure the overlap of synthetic minority samples with the real minority samples. However, focusing solely on coverage w.r.t. the real minority samples may neglect the fact that naive high coverage can lead to overlap with the majority class distributions, reducing the effectiveness of minority oversampling and adversely affecting the performance of downstream classification tasks. Consequently, we also calculated the coverage score between the synthesized minorities and real majority samples, a metric we refer to as *\"spillage\"* temporarily. For relative reference, we also measure the natural spillage of real minority data into the real majority data and report its difference. The provided table contrasts the coverage and spillage scores of LITO with those of recent baselines. LITO demonstrates a balance of high coverage and low spillage, whereas GReaT exhibits significant spillage and SOS has limited coverage. These findings suggest that our approach not only generates a diverse set of samples but also maintains their quality. This pattern is also observed in the UMAP visualizations in Appendix C.1.\n\n| imb = 100 | Default |  |  | Shoppers |  |  | Sick |  |  | Diabetes |  |  |\n| --- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | Coverage(\u2191) | Spillage(\u2193) | \u0394Spill | Coverage | Spillage | \u0394Spill | Coverage | Spillage | \u0394Spill | Coverage | Spillage | \u0394Spill |\n| Real | 1 | 0.533 | - | 1 | 0.345 | - | 1 | 0.074 | - | 1 | 0.599 | - |\n| SOS | 0.608 | 0.203 | 0.330 | 0.769 | 0.213 | 0.131 | 0.929 | 0.466 | 0.392 | 0.375 | 0.553 | 0.046 |\n| GReaT | 0.859 | 0.883 | 0.350 | 0.969 | 0.884 | 0.539 | 0.943 | 0.457 | 0.383 | 0.798 | 0.931 | 0.332 |\n| LITO | 0.951 | 0.656 | 0.123 | 0.870 | 0.440 | 0.095 | 0.936 | 0.485 | 0.411 | 0.676 | 0.376 | 0.226 |\n\n| imb = 100 | Obesity |  |  | Satimage | (*LITO-C) |  |\n| --- | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | Coverage | Spillage | \u0394Spill | Coverage | Spillage | \u0394Spill |\n| Real | 1 | 0.325 | - | 1 | 0.126 | - |\n| SOS | 0.657 | 0.161 | 0.164 | 0.338 | 0.045 | 0.082 |\n| GReaT | 0.847 | 0.392 | 0.067 | 0.630 | 0.400 | 0.273 |\n| LITO | 0.926 | 0.386 | 0.061 | 0.672 | 0.108 | 0.019 |\n\nTo further address the reviewer\u2019s concerns, we record the acceptance ratios of self-authentication. The table below shows the sample acceptance ratio for the datasets used in the main experiment. As the imbalance ratio increases, the acceptance ratio is lower, maintaining the sample quality.\n\n| Imb. ratio | Diabetes | Default | Shoppers | Sick  | Obesity | Satimage |\n| --- | :---: | :---: | :---: | :---: | :---: | :---: |\n| 10 | 55.48 | 20.28 | 51.30 | 93.31 | 67.24 | 41.98 |\n| 100 | 1.04 | 13.83 | 12.95 | 14.72  | 10.61 | 8.28 |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6977/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700484305571,
                "cdate": 1700484305571,
                "tmdate": 1700496128961,
                "mdate": 1700496128961,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tiOlQrIfAf",
                "forum": "8F6bws5JBy",
                "replyto": "aId060Mwe8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6977/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6977/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer gyRK (3)"
                    },
                    "comment": {
                        "value": "### [W3. Using different language models e.g. GPT-3 or ChatGPT.]\n\nSince the general capabilities of language models increase with their number of parameters, their capability to model tabular data can also increase with model size, as observed in [6, 7]. Since our method utilizes a language model backbone, the performance of our method can be further improved when equipped with a larger language model. This is observed in Section 4.5 of the main paper, where we adopt LITO to ChatGPT-3.5-turbo via in-context learning (ICL) and conduct experiments on the Diabetes dataset for imbalance ratio 20. We briefly summarize the results in the table below:\n\n| Method | F1 | bAcc |\n| --- | --- | --- |\n| Vanilla | 49.54 | 54.29 |\n| SMOTE | 61.06 | 61.28 |\n| BSMOTE | 59.77 | 60.46 |\n| CTGAN | 55.31 | 55.50 |\n| SOS | 49.80 | 54.39 |\n| GReaT(DistillGPT2) | 49.44 | 49.59 |\n| Ours(DistillGPT2) | 63.04 | 63.20 |\n| Ours(ChatGPT) | **67.37** | **66.88** |\n\nExperimental results show that utilizing larger language models results in a further performance increase of our method.\n\n---\n\n[1] Grinsztajn et al. \"Why do tree-based models still outperform deep learning on typical tabular data?.\"\u00a0NeurIPS 2022.\n\n[2] Kim et al. \"SOS: score-based oversampling for tabular data.\" KDD 2022.\n\n[3] Han et al. \u201cBorderline-smote: a new over-sampling method in imbalanced data sets learning.\u201d ICIC 2005.\n\n[4] McInnes et al. UMAP: Uniform Manifold Approximation and Projection. Journal of Open Source Software, 2018.\n\n[5] Naeem et al. \"Reliable fidelity and diversity metrics for generative models.\" ICML 2020.\n\n[6] Dinh et al. \"LIFT: Language-interfaced fine-tuning for non-language machine learning tasks.\" NeurIPS 2022.\n\n[7] Borisov et al. \"Language models are realistic tabular data generators.\" ICLR 2023."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6977/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700484441014,
                "cdate": 1700484441014,
                "tmdate": 1700484441014,
                "mdate": 1700484441014,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kdy22ANL1Y",
                "forum": "8F6bws5JBy",
                "replyto": "tiOlQrIfAf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6977/Reviewer_gyRK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6977/Reviewer_gyRK"
                ],
                "content": {
                    "title": {
                        "value": "Reply to author rebuttal"
                    },
                    "comment": {
                        "value": "Thanks for the thorough reply to all my questions. The explanations are reasonable and additional experiments show promising results. Hence, I decided to raise my score to 6."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6977/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700666625063,
                "cdate": 1700666625063,
                "tmdate": 1700666625063,
                "mdate": 1700666625063,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "jETvjC1xWH",
                "forum": "8F6bws5JBy",
                "replyto": "aId060Mwe8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6977/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6977/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your valuable insights and reconsideration of scores. We are pleased that the explanations and additional experiments have addressed the reviewer\u2019s concerns. We sincerely appreciate your time in reviewing our response and paper."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6977/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726438110,
                "cdate": 1700726438110,
                "tmdate": 1700726438110,
                "mdate": 1700726438110,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "68NGuovFxK",
            "forum": "8F6bws5JBy",
            "replyto": "8F6bws5JBy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6977/Reviewer_puAc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6977/Reviewer_puAc"
            ],
            "content": {
                "summary": {
                    "value": "This paper targets at the problem that tabular data usually struggles with class-imbalance problem.\nTo deal with this issue, this paper introduces an algorithm which oversamples the tabular data in a self-authentication manner.\nSpecifically, the algorithm synthesizes the minority class samples by progressively masking the important features of the majority class samples and imputing them towards the minority distribution.\nThe experiments show that the algorithm is able to generate reliable minority which can boost the ML classifier performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The writing is good enough to make readers easily understand the paper. For example, Figure 1 and algorithm covers most of the details in the algorithm, which can make readers briefly know the whole algorithm in a short period.\"\n- The targeted problem, minority in the tabular data, is interesting and still an under-presented problem in the community. Improving this problem can make a large improvement on the ML community."
                },
                "weaknesses": {
                    "value": "- The main concern is that the oversampling algorithm is based on masking. While some of the information is changed during masking and self-authentication, most of the information still remain and suggests the algorithm just repeat some similar data and limit the diversity of data. Have you tried to analysis the label distribution between real negatives, synthesized negatives, and real positives?\n- Another concern s about that the novelty of the proposed method. While the claimed contribution in this paper include \"data generation\" with \"self-authentication\", the similar ideas have appeared in some literature, like [1] and [2]. Could the authors state the difference between this work and the previous?\n\n[1] Language Models are Realistic Tabular Data Generators, ICLR 2023"
                },
                "questions": {
                    "value": "- As mentioned in the weakness section, have you tried to analysis the label distribution between real negatives, synthesized negatives, and real positives? Can this algorithm bring the dataset diversity which can bring the benefit to the downstream task?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6977/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699182750014,
            "cdate": 1699182750014,
            "tmdate": 1699636815599,
            "mdate": 1699636815599,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WoegUlkMJw",
                "forum": "8F6bws5JBy",
                "replyto": "68NGuovFxK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6977/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6977/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank reviewer puAc for constructive feedbacks. We address the reviewer's concerns below.\n\n---\n\n### [W1, Q1. Diversity of the generated synthetic minority samples.]\n\nIn order to examine the diversity of the synthetic minority samples generated by LITO(ours), we conducted additional qualitative and quantitative analyses, where detailed figures and descriptions are added in Appendix C of the revised manuscript.\nFor qualitative analyses, we follow previous work[1] and plot the **individual feature-wise histograms** and **UMAP**[2] **visualizations** of synthesized minority samples for multiple datasets. The resulting figures are presented in Appendix C.1.1 and C.1.2, respectively. The individual feature-wise histograms show that the column features generated by LITO best resemble the ground truth minority distribution compared to other baselines, showcasing sampling diversity and quality. Also, we plot UMAP diagrams to visually observe the manifolds occupied by the training data and synthetic minorities generated by our method and other baselines. As seen in the figures, our method effectively covers the ground truth minority distribution while refraining from intruding into the distribution covered by other classes.\n\nTo quantitatively measure the diversity of the synthesized minority samples compared to the ground truth minority distribution, we measure the Coverage Score[3] between the real minority samples and synthesized minority samples (Appendix C.2). However, measuring coverage **only** against the real minority samples neglects the quality aspect of the synthetic samples, as a distribution with high coverage only may simply \u2018spill\u2019 into the distribution of majority class samples, degrading minority oversampling quality thereby lowering downstream classification performance. To this end, we also measure the Coverage score between **real majority samples** and synthesized minorities (for convenience, we temporarily term this as *\"spillage\"*). For relative reference, we also measure the natural spillage of real minority data into the real majority data and report its difference. The table below shows the coverage and spillage scores of LITO and recent competing baselines. LITO exhibits high coverage while maintaining low spillage, while GReaT shows high spillage, and SOS shows low coverage. These results indicate that our method generates diverse samples while maintaining quality. This tendency is also visually confirmed in the UMAP visualizations in Appendix C.1.2.\n\n| imb = 100 | Default |  |  | Shoppers |  |  | Sick |  |  | Diabetes |  |  |\n| --- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | Coverage(\u2191) | Spillage(\u2193) | \u0394Spill | Coverage | Spillage | \u0394Spill | Coverage | Spillage | \u0394Spill | Coverage | Spillage | \u0394Spill |\n| Real | 1 | 0.533 | - | 1 | 0.345 | - | 1 | 0.074 | - | 1 | 0.599 | - |\n| SOS | 0.608 | 0.203 | 0.330 | 0.769 | 0.213 | 0.131 | 0.929 | 0.466 | 0.392 | 0.375 | 0.553 | 0.046 |\n| GReaT | 0.859 | 0.883 | 0.350 | 0.969 | 0.884 | 0.539 | 0.943 | 0.457 | 0.383 | 0.798 | 0.931 | 0.332 |\n| LITO | 0.951 | 0.656 | 0.123 | 0.870 | 0.440 | 0.095 | 0.936 | 0.485 | 0.411 | 0.676 | 0.376 | 0.226 |\n\n| imb = 100 | Obesity |  |  | Satimage | (*LITO-C) |  |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | Coverage | Spillage | \u0394Spill | Coverage | Spillage | \u0394Spill |\n| Real | 1 | 0.325 | - | 1 | 0.126 | - |\n| SOS | 0.657 | 0.161 | 0.164 | 0.338 | 0.045 | 0.082 |\n| GReaT | 0.847 | 0.392 | 0.067 | 0.630 | 0.400 | 0.273 |\n| LITO | 0.926 | 0.386 | 0.061 | 0.672 | 0.108 | 0.019 |\n\nWe also analyze the label distribution of synthesized samples. Since we cannot know the oracle true labels for synthetic samples, we approximately infer their labels using an XGBoost classifier trained using the full training data. Since the test accuracy of the XGBoost classifier is not 100%, there exist potential discrepancies. The table below shows the inferred label distribution: the percentage of synthesized samples produced with our method and competing baselines that are predicted as the minority class, where our method surpasses or is on par with other baselines.\n\nXGBoost bAcc: 71.19(Default), 78.83(Shoppers), 82.61(Sick), 68.23(Diabetes)\n\n| imb = 100 | Default | Shoppers | Sick | Diabetes |\n| --- | :---: | :---: | :---: | :---: |\n| SOS | 42.38 | 62.59 | 66.01 | 41.24 |\n| GReaT | 40.78 | 56.35 | 68.40 | 39.94 |\n| Ours | 46.51 | 61.77 | 70.84 | 50.5 |\n\nFinally, as observed in our main experiments(Section 4.2, 4.3) of the main paper, the increase in classification performance of various downstream machine learning classifiers (logistic regression, xgboost, ... etc.) trained with LITO-oversampled data also reflects the quality and diversity of the synthetic minority samples generated by our method, compared to other baselines."
                    },
                    "title": {
                        "value": "Response to Reviewer puAc (1)"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6977/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483352283,
                "cdate": 1700483352283,
                "tmdate": 1700495709653,
                "mdate": 1700495709653,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RPoeYEIalX",
                "forum": "8F6bws5JBy",
                "replyto": "68NGuovFxK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6977/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6977/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer puAc (2)"
                    },
                    "comment": {
                        "value": "### [W2. Novelty of the proposed method.]\n\nAlthough [4] has demonstrated the possibility of language models for the generation of tabular data, our work focuses on the practical applicability of such tabular language models - whether they can be effectively adopted for oversampling to remedy the class-imbalance problem, which is a frequent and substantial issue in the tabular domain. To this end, we propose a novel sampling strategy comprised of two components: progressive importance-aware imputation and self-authentication. Compared to previous works, our proposed work contains the following novel contributions:\n\n1. Our experimental observations, including Section 4.2, 4.3, and Appendix C of the revised paper, show that naive oversampling with a tabular language model(GReaT) fine-tuned on imbalanced data to oversample minority class samples results in ill-generated samples (for example, synthetic samples that intrude into distributions of other classes) that degrade downstream classification performance, due to the scarcity of minority class samples.\n2. To filter out such samples, we propose self-authentication, which utilizes the capabilities of the language model itself. Our experiments in Section 4.4 show that self-authentication serves as an effective filter for maintaining the quality of the tabular samples generated by a tabular language model.\n3. We propose progressive importance-aware imputation, a novel borderline sampling strategy that combines and leverages the characteristics of the tabular domain and the capabilities of language models simultaneously.\n    * Unlike other data modalities(e.g. vision, speech) where class-relevant information is spread across features, the column features of tabular data each hold distinctive information, where there exist columns important to the given task, and also relatively unimportant columns[5]. However, previous borderline sampling strategies such as [1, 6] manipulate all features simultaneously - introducing potential noise in the unimportant columns thereby degrading sample quality.\n    * Inspired by this characteristic, we propose to select, puncture, and re-impute the important columns iteratively, using the conditional sampling capability of the language model. This ensures the feature quality of the unimportant columns as they stay in the real distribution, while important columns are resampled for diversity.\n    * We propose to utilize the inherent self-attention maps of the language model to identify and measure the importance of column features instance-wise.\n\n---\n\nDue to possible technical issues with the website, citation [2] mentioned by the reviewer in the review text is not visible to us. To this end, we politely request the reviewer to re-inform us of the name of the paper for citation [2] in the review.\n\n---\n\n[1] Kim et al. \"SOS: score-based oversampling for tabular data.\" KDD 2022.\n\n[2] McInnes et al. UMAP: Uniform Manifold Approximation and Projection. Journal of Open Source Software, 2018.\n\n[3] Naeem et al. \"Reliable fidelity and diversity metrics for generative models.\" ICML 2020.\n\n[4] Borisov et al. \"Language models are realistic tabular data generators.\" ICLR 2023.\n\n[5] Grinsztajn et al. \"Why do tree-based models still outperform deep learning on typical tabular data?.\"\u00a0NeurIPS 2022.\n\n[6] Han et al. \u201cBorderline-smote: a new over-sampling method in imbalanced data sets learning.\u201d ICIC 2005."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6977/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483800727,
                "cdate": 1700483800727,
                "tmdate": 1700484325513,
                "mdate": 1700484325513,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "M5ODqaKS4f",
                "forum": "8F6bws5JBy",
                "replyto": "RPoeYEIalX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6977/Reviewer_puAc"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6977/Reviewer_puAc"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks authors for the response. The new results show the diversity of the generated synthetic minority samples. I will maintain my current rating for acceptance."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6977/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700700865492,
                "cdate": 1700700865492,
                "tmdate": 1700700865492,
                "mdate": 1700700865492,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1eLsmhRnk4",
                "forum": "8F6bws5JBy",
                "replyto": "68NGuovFxK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6977/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6977/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for reviewing our response. We sincerely appreciate your time and effort in reviewing our paper and giving constructive feedback."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6977/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726340104,
                "cdate": 1700726340104,
                "tmdate": 1700726340104,
                "mdate": 1700726340104,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]