[
    {
        "title": "Graph Positional and Structural Encoder"
    },
    {
        "review": {
            "id": "6O0wzoFAub",
            "forum": "yJdj2QQCUB",
            "replyto": "yJdj2QQCUB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4049/Reviewer_rYLm"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4049/Reviewer_rYLm"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces the graph positional and structural encoder (GPSE), which is the first attempt to create a graph encoder that can generate rich PSE representations for GNNs. GPSE can learn representations for a variety of PSEs and is notably adaptable. When trained on a large molecular graph dataset, it can be applied successfully to different datasets, even those with differing distributions or modalities. The study shows that GPSE-enhanced models can either improve performance or match the results of models using explicitly computed PSEs. This contribution suggests the potential for large pre-trained models in extracting graph positional and structural information, positioning them as strong alternatives to current methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is well structured, easy to follow.\n- The introduced GPSE model stands out for its simplicity, consistently delivering results either superior to or on par with hand-crafted PSEs.\n- A commendable effort has been made in terms of the ablation studies, providing comprehensive insights into the proposed model."
                },
                "weaknesses": {
                    "value": "1. **Comparison with related work:** While the paper does touch upon the LSPE (Dwivedi et al. 2021) \u2013 which shares similarities in learning positional representations along with the prediction task \u2013 a more detailed comparison is crucial to identify the unique contributions of the present work.\n2. **Clarity on computational complexity:** A major limitation of most hand-crafted PSEs is their high complexity on large graphs. The paper could benefit from a deeper dive into GPSE's computational complexity, particularly when compared against the often complex hand-crafted PSEs and other encoding strategies.\n3. **Missing baseline method:** Throughout the experiments, GPSE is compared against individual PSEs. Considering GPSE is designed to capture multiple PSE representations, a comparison against a combined baseline which concatenates various PSEs \u2013 akin to those used in GPSE's pre-training \u2013 would offer a more fair evaluation.\n\n**Minor points**\n- **Assumptions about Audience:** The introduction might be challenging for those new to the field, given the use of specific terms like \"1-WL bounded expressiveness\" and \"over-squashing\" without much elucidation.\n\n_Reference:_ \n\nDwivedi, Vijay Prakash, et al. \"Graph Neural Networks with Learnable Structural and Positional Representations.\" ICLR 2021."
                },
                "questions": {
                    "value": "- Q1: For downstream tasks, is the pretrained GPSE kept frozen or is it finetuned? Would fine-tuning the GPSE improve its performance? It is noteworthy that, in other modalities like NLP and computer vision, fine-tuning pretrained models often outperforms freezing pretrained models.\n\nI will be happy to increase my rating if the authors can address all my concerns and questions."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4049/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4049/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4049/Reviewer_rYLm"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4049/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698670685096,
            "cdate": 1698670685096,
            "tmdate": 1700641537592,
            "mdate": 1700641537592,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "augitKYbgB",
                "forum": "yJdj2QQCUB",
                "replyto": "6O0wzoFAub",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4049/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4049/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer rYLm"
                    },
                    "comment": {
                        "value": "> Comparison with related work: While the paper does touch upon the LSPE (Dwivedi et al. 2021) \u2013 which shares similarities in learning positional representations along with the prediction task \u2013 a more detailed comparison is crucial to identify the unique contributions of the present work.\n\nWe thank the reviewer for raising the question. We kindly refer the reviewer to our global response, \u201cRelation between GPSE and LPSE,\u201d for a more in-depth explanation of the difference between the two.\n\n> Clarity on computational complexity: A major limitation of most hand-crafted PSEs is their high complexity on large graphs. The paper could benefit from a deeper dive into GPSE's computational complexity, particularly when compared against the often complex hand-crafted PSEs and other encoding strategies.\n\nWe appreciate the reviewer\u2019s constructive comment regarding the computational complexity of GPSE. As the reviewer has suggested, we added a detailed analysis with an empirical study demonstrating the computational benefit of using GPSE instead of directly computing hand-crafted PSEs. As a concrete example, computing all PSEs on MolPCBA took about four hours, while GPSE only took about four minutes. We kindly refer the reviewer to the global response, \u201cComputational complexity of GPSE,\u201d for more details, as well as Appendix [H] for results and additional visualizations.\n\n> Missing baseline method: Throughout the experiments, GPSE is compared against individual PSEs. Considering GPSE is designed to capture multiple PSE representations, a comparison against a combined baseline which concatenates various PSEs \u2013 akin to those used in GPSE's pre-training \u2013 would offer a more fair evaluation.\n\nWe thank the reviewer for bringing up this valid point. We kindly refer the reviewer to our global response, \u201cComprehensive comparison against AllPSE baselines,\u201d for more detailed information about the newly added baseline as suggested. In brief, we observed that GPSE outperforms or matches the performance of AllPSE in nearly all of our primary benchmarks.\n\n> Assumptions about Audience: The introduction might be challenging for those new to the field, given the use of specific terms like \"1-WL bounded expressiveness\" and \"over-squashing\" without much elucidation.\n\n\nThe WL-test is explained along with visual support in Appendix E, in discussion of how GPSE breaks symmetries associated with the test. Over-squashing and over-smoothing are similarly explained in Appendix C: Theory details. We do agree with the reviewer, however, that connections to these sections should be clarified to redirect the reader. To this end, we have re-adjusted the beginning of Section 2.2 to refer to Appendix C as well as C.1, where we further discuss the connections between over-smoothing/over-squashing and GPSE. We have similarly referred to Appendix E in the beginning of Section 3.3, and reformatted Appendix E to further elucidate the WL test and its implications.\n\n> Q1: For downstream tasks, is the pretrained GPSE kept frozen or is it finetuned? Would fine-tuning the GPSE improve its performance? It is noteworthy that, in other modalities like NLP and computer vision, fine-tuning pretrained models often outperforms freezing pretrained models.\n\nWe thank the reviewer for the insightful comments. We note that all GPSE results presented in the main paper are obtained with a GPSE model with frozen weights (no fine-tuning). On the flip side, we experimented with fine-tuning but no concrete conclusion about whether it is helpful was drawn. Specifically, we have tried fine-tuning GPSE on the two primary molecular property benchmarking tasks: ZINC and PCQM4Mv2. As we have presented in Appendix Table F.5, fine-tuning could help in certain cases, but no consistent pattern shows the superiority of fine-tuning.\n\nFine-tuning pre-trained models is a vast topic, and how to effectively do so for better downstream performance in our specific setting is an under-explored area. Thus, an exciting future avenue is to study the optimal way to fine-tune GPSE to further adapt to specific downstream datasets.\n\n---\nWe sincerely appreciate the reviewer's constructive feedback and have diligently addressed each concern raised in our revision. We remain open to further discussion and are ready to address any additional concerns the reviewer may have. In light of these comprehensive revisions, we kindly ask the reviewer to reevaluate our work, hoping that the enhancements made positively impact the overall quality and merit of our paper."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4049/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700289751981,
                "cdate": 1700289751981,
                "tmdate": 1700289751981,
                "mdate": 1700289751981,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Z5t7SEvsZP",
                "forum": "yJdj2QQCUB",
                "replyto": "augitKYbgB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4049/Reviewer_rYLm"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4049/Reviewer_rYLm"
                ],
                "content": {
                    "title": {
                        "value": "Response to the rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your detailed response, which has addressed most of my concerns. Therefore, I am leaning towards acceptance."
                    }
                },
                "number": 21,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4049/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700641514855,
                "cdate": 1700641514855,
                "tmdate": 1700641514855,
                "mdate": 1700641514855,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5AuMLUAWd2",
            "forum": "yJdj2QQCUB",
            "replyto": "yJdj2QQCUB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4049/Reviewer_7nJh"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4049/Reviewer_7nJh"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes GPSE, a framework that can be used to pretrain an encoder in order to _learn_ a diverse set of positional and structural encodings, given an input graph augmented with random node features. The authors show good empirical results on a diverse set of downstream tasks when using the pretrained encoder to extract the encodings."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Results on some datasets are promising, like those obtained on ZINC."
                },
                "weaknesses": {
                    "value": "The authors have __not__ shown the advantage of learning the encodings instead of computing them directly on the dataset of downstream task of interest. In each table the direct competitor should be the GNN predictor directly augmented with __all__ the positional and structural encodings (that are learned by GPSE). For example, Table 2 should include \"GPS + LapPE + ElstaticPE + EigValSE + RSWE + HKdiagSE + CycleSE\". Comparing only to \"GPS + LapPE\" or \"GPS + RWSE\" is unfair, and no conclusion can be drawn from it as it might be that simply using one of the other encodings lead to better results, so the role of the pretraining is less clear.\n\nIt is unclear how the authors deal with sign and basis ambiguity. Consider for example LapPE. When training GPSE, what is the sign of the target LapPE? What about the basis?"
                },
                "questions": {
                    "value": "Please clarify the importance of learning the encodings. Otherwise the conclusion is that using all those encodings is better than using no encoding, and and it is unclear what is the impact of computing them directly on the input graph instead of learning them in a pretraining step that uses MolPCBA.\nAlso consider the problem of sign and basis ambiguity."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4049/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4049/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4049/Reviewer_7nJh"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4049/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698695128333,
            "cdate": 1698695128333,
            "tmdate": 1700682631724,
            "mdate": 1700682631724,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3jVWP0ymUb",
                "forum": "yJdj2QQCUB",
                "replyto": "5AuMLUAWd2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4049/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4049/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 7nJh"
                    },
                    "comment": {
                        "value": "> Table 2 should include \"GPS + LapPE + ElstaticPE + EigValSE + RSWE + HKdiagSE + CycleSE\". Comparing only to \"GPS + LapPE\" or \"GPS + RWSE\" is unfair, and no conclusion can be drawn from it as it might be that simply using one of the other encodings lead to better results, so the role of the pretraining is less clear.\n\nWe appreciate the reviewer for bringing up this valid concern regarding a more proper baseline of concatenating all PSEs. We kindly refer the reviewer to our global response, \u201cComprehensive comparison against AllPSE baselines,\u201d for more detailed information about the newly added baseline as suggested. In brief, we observed that GPSE outperforms or matches the performance of AllPSE in nearly all of our primary benchmarks.\n\n> It is unclear how the authors deal with sign and basis ambiguity. Consider for example LapPE. When training GPSE, what is the sign of the target LapPE? What about the basis?\n\nWe thank the reviewer for the question regarding the sign and basis ambiguity issues [1] for LapPE. We have described our approach in Appendix A (Positional and structural encoding tasks detail). To summarize, we tackled the sign ambiguity issue by taking the **absolute values** of the eigenvectors. On the other hand, we did not explicitly resolve the basis ambiguity issue. However, since we only use the eigenvectors associated with the *first four smallest non-trivial eigenvalues*, there is a significantly lower chance of encountering degenerate eigenvalues (the root cause of basis ambiguity) compared to learning against the full set of eigenvectors.\n\nWe further note that *current known strategies from the literature for tackling the sign and basis ambiguity do not apply to our problem setting*. In [1], the goal was to utilize the eigenvectors in a way that is sign and basis invariant. In our problem setting, however, we aim to *learn against the PSEs*, thus requiring an **objective function** designed to be sign and basis invariant. Both invariances are invalid for standard regression objective functions such as MSE, MAE, and Cosine Similarity losses. Although we acknowledge the value of tackling this specific problem in a future project, this is ultimately out of this project's scope, as we primarily aim to demonstrate the possibility of learning a compressed representation that captures multiple PSEs.\n\n[1] Lim, Derek, et al. \"Sign and Basis Invariant Networks for Spectral Graph Representation Learning.\" The Eleventh International Conference on Learning Representations. 2022.\n\n> Please clarify the importance of learning the encodings. Otherwise the conclusion is that using all those encodings is better than using no encoding, and and it is unclear what is the impact of computing them directly on the input graph instead of learning them in a pretraining step that uses MolPCBA. Also consider the problem of sign and basis ambiguity.\n\nWe thank the reviewer for the insightful suggestions. Please refer to our earlier reply for more information regarding the **performance benefit** of using GPSE compared to the AllPSE counterparts.\n\nFurthermore, GPSE demonstrates a notable **computational benefit**, with significantly shorter computation time on large datasets like MolPCBA than computing all PSEs exactly. Please refer to the global response, \u201cComputational complexity of GPSE,\u201d for a more in-depth discussion, as well as Appendix [H] for results and additional visualizations.\n\nFinally, as discussed in the reply above, we have dealt with the sign ambiguity of LapPE by taking the absolute values. Meanwhile, basis ambiguity remains an open problem due to the unique challenge of designing a general-purpose objective function that is basis invariant.\n\n---\nWe sincerely appreciate the reviewer's constructive feedback regarding the AllPSE baseline and sign/basis ambiguity concern. We believe that our replies have sufficiently addressed the concerns raised. We remain open to further discussion and are ready to address any additional concerns the reviewer may have. In light of these comprehensive revisions, we kindly ask the reviewer to reconsider their initial evaluation and increase their score accordingly if no lingering questions remain."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4049/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700289680174,
                "cdate": 1700289680174,
                "tmdate": 1700289680174,
                "mdate": 1700289680174,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "512Eie01Ii",
                "forum": "yJdj2QQCUB",
                "replyto": "5AuMLUAWd2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4049/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4049/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Friendly Reminder to Reviewer 7nJh for Open Discussion"
                    },
                    "comment": {
                        "value": "Dear Reviewer 7nJh,\n\nWe sincerely appreciate your dedication to reviewing our work and the valuable insights provided. In response to your comments, we have thoroughly addressed each concern in our rebuttal. As the deadline for open discussion is quickly approaching, we kindly remind you to review our efforts and share any further feedback or questions you may have. Your additional input is crucial for us to refine our work effectively. We eagerly await your response and thank you again for your invaluable contributions to this process.\n\nBest regards,\n\nThe Authors"
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4049/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668082587,
                "cdate": 1700668082587,
                "tmdate": 1700668317551,
                "mdate": 1700668317551,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "F1lMG29mtQ",
                "forum": "yJdj2QQCUB",
                "replyto": "512Eie01Ii",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4049/Reviewer_7nJh"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4049/Reviewer_7nJh"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the time spent in answering my questions. \n\nI have particularly appreciated the additional baseline, which was necessary to assess the importance of learning the encodings. For future work, I would recommend adding baseline experiments with all possible combinations of these encodings for a comprehensive comparison.\n\nRegarding the sign and basis ambiguity, I believe the answer does not address my concerns. While I can understand leaving the basis ambiguity for future work, I believe that addressing the sign problem in a more principle manner would significantly strengthen the paper. It is well known that taking the absolute value seriously degrades the expressive power of the encodings [1] so I would encourage the authors to think of alternatives. For example, maybe you could learn a sign-invariant function of the eigenvectors instead of the (absolute values of the) eigenvectors, in the spirit of the literature tackling sign and basis ambiguity. \n\nI will increase my score to 5. The weaknesses related to sign and basis ambiguity, and the intrinsic hand-crafted nature of the paper prevents me to fully support this work. I think the usage of: (1) residual connections, (2) gating mechanisms (3) virtual nodes make the paper a bit too much feature-engineered. The justification of these choices is intuitive but certainly not formal. \n\n\n[1] Dwivedi et al., 2020. Benchmarking Graph Neural Networks"
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4049/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700682603219,
                "cdate": 1700682603219,
                "tmdate": 1700682603219,
                "mdate": 1700682603219,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "47iVG3xRpc",
                "forum": "yJdj2QQCUB",
                "replyto": "5AuMLUAWd2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4049/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4049/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 7nJh"
                    },
                    "comment": {
                        "value": "Thank you for your response and for acknowledging our efforts in incorporating the new AllPSE baseline into our study.\n\nWe concur with your suggestion that additional combinations of PSE could enrich our results. In line with this, we have already incorporated the most significant binary combination, LapPE+RWSE, into our global replies. We are **actively working on integrating more combinations** and will ensure these results are included in the camera-ready version of our paper.\n\nRegarding the remaining concern about sign ambiguity, we understand that our original experiments have not satisfactorily resolved the issue. We have therefore conducted a new set of experiments to demonstrate this aspect further. Particularly, we pretrained four different versions of GPSE using 5% MolPCBA subset (due to time constraints): (1) GPSE-abs takes absolute value of the LapPE (default setting in our paper), (2) GPSE-noabs do not take absolute value of the LapPE, (3) GPSE-signinvar uses a sign invariant loss function for LapPE by taking the minimum of the losses from both signs, (4) GPSE-SignNet uses a randomly initialized SignNet [1] model to generate sign invariant features as the training target for GPSE. We tested the four variations on two molecular (ZINC-subset, PCQM4Mv2-subset) and two transfer benchmarks (Peptides-func, Peptides-struc). Our results indicate that using the default absolute handling of LapPE results in similar or better performance than other handlings, indicating the **effectiveness of using the absolute LapPE for training GPSE**. Given that it is possible that not using absolute LapPE results in better performance than default GPSE, such as in Peptides-func, we acknowledge the value of further exploring optimal ways to handle LapPE sign ambiguities in future work. That said, our results here proved that the default handling using absolute value is sufficient.\n\nLastly, we respectfully differ from the viewpoint that our methodological choices lack formal justification as suggested by the reviewer. We would like to indicate that the overall reasons that drive most architectural decisions behind GPSE are fairly straight-forward: (1) Learning PSEs that can encode long-distance relationships require deep GNNs to avoid under-reaching, (2) Deep GNNs invariably suffer from over-smoothing and over-squashing. Our proposed architecture is composed of well-established tools in tackling these issues; we additionally provide  **detailed formal proofs in Appendix C** of our paper, providing theoretical underpinnings for these choices. Therefore, we believe our architectural choices are well-justified from both the perspective of theory and ties to prior work on graph bottlenecks, as well as empirically, as the robustness of our results suggests.\n\nWe appreciate your valuable feedback and hope our additional justifications have addressed your concerns further. Considering the additional experiments and clarifications provided, we kindly ask the reviewer to reevaluate the score of our submission.\n\n|            | **ZINC (subset)** | **PCQM4Mv2 (subset)** | **Peptides-struct** | **Peptides-func** |\n| :------- | :-------: | :-------: | :-------: | :-------: |\n|            | **MAE \u2193** |  **MAE \u2193** |  **MAE \u2193** | **AP \u2191** |\n| GPSE-abs (default) | **0.0957 \u00b1 0.0044** | **0.1216 \u00b1 0.0002** | **0.2516 \u00b1 0.0018** | 0.6584 \u00b1 0.0042 |\n| GPSE-noabs           | 0.1051 \u00b1 0.0046 | 0.1229 \u00b1 0.0006 | 0.2554 \u00b1 0.0025 | **0.6687 \u00b1 0.0119** |\n| GPSE-signinvar      | 0.1116 \u00b1 0.0072 | 0.1243 \u00b1 0.0004 | 0.2594 \u00b1 0.0019 | 0.6619 \u00b1 0.0097 |\n| GPSE-SignNet        | 0.1035 \u00b1 0.0052 | 0.1232 \u00b1 0.0006 | 0.2568 \u00b1 0.0020 | Not run yet |\n\n[1] Lim, Derek, et al. \"Sign and Basis Invariant Networks for Spectral Graph Representation Learning.\" The Eleventh International Conference on Learning Representations. 2022."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4049/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738297001,
                "cdate": 1700738297001,
                "tmdate": 1700738587928,
                "mdate": 1700738587928,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7kd1xPqDvZ",
            "forum": "yJdj2QQCUB",
            "replyto": "yJdj2QQCUB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4049/Reviewer_vPPW"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4049/Reviewer_vPPW"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces the Graph Positional and Structural Encoder (GPSE), an approach for training graph encoders to effectively capture positional and structural encodings (PSEs) within graphs. \nThese encodings are pivotal for identifying node roles and relationships, a challenge accentuated by the inherent absence of a standard node ordering in graph structures. \nGPSE shows the capability to learn and apply these PSEs across a wide range of graph datasets, with potential to match or even surpass the performance of explicitly computed PSEs. \nThe encoder stands out for its adaptability and proficiency in various graph dimensions and types.\n\n\nThe paper details a method to train a Message Passing Neural Network (MPNN) as a graph encoder, focusing primarily on the extraction of positional and structural representations from query graphs based solely on their structure. \nThis extraction utilizes a set of PSEs in a predictive learning framework. \nPost training, the encoder can adeptly apply these PSEs to augment models (both MPNNs and Transformers) across various datasets. \nThe design includes diverse PSE types such as Laplacian eigenvectors and eigenvalues, electrostatic and random walk structural encodings, heat kernel structural encodings, and cycle counting graph encodings. \nThese PSEs provide insights into a node's relative position (positional encodings) and the local connectivity patterns (structural encodings), enhancing the understanding of graph structures."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This study presents a noteworthy contribution to the field of graph neural networks, particularly in its architectural and methodological approach. \nThe originality of the paper lies in its integration of PSEs within GNNs, targeting the specific challenges of graph representation learning. \nWhile the concept might not completely redefine the foundational theories of graph neural networks, it innovatively combines existing ideas to enhance the representation and processing of graph structures, demonstrating a balanced level of originality.\n\nIn terms of quality and clarity, the paper is methodically sound, though it might benefit from a more in-depth exploration of its theoretical underpinnings. \nThe clarity of presentation is commendable, with technical details and concepts explained in a manner that strikes a balance between depth and accessibility. \nThe significance of the research, while notable, appears more confined to immediate practical applications rather than setting a new paradigm. Nevertheless, its potential impact in improving model performance in various graph-related tasks."
                },
                "weaknesses": {
                    "value": "I've encountered a potential issue concerning the anonymity of the authors in the submission, specifically related to the URL (specifically, the owner of Google Drive is not anonymized) in the code that the authors submitted, which might violate the double-blind review process. While I cannot confirm if the names found in the URL indeed belong to the authors, I wanted to raise this as a potential concern for the integrity of the review process. This concern is based on the `Anonymous Url` at the top of this page and the `Source code submission` section in the Author Guide. \n\n---\n\nThe research primarily excels in engineering advancements for graph neural networks, focusing more on practical architectural solutions than on extending theoretical foundations. \nAlthough these innovations address key issues in GNNs, they potentially add complexity and computational overhead that aren't thoroughly examined.\n\nThe model's performance, when combined with GPSE, is less impressive, considering its complexity, which includes 20 MPNN layers, MLP heads for each PSE, a gating mechanism, and a virtual node.\n\nFor fair comparison in Table 2 and Table 3, GPSE should be compared not only with single combinations like GPS+LapPE or GPS+RWSE, but also with a more comprehensive set-up, including GPS+LapPE+ElstaticPE+RWSE+EigValSE+HKdiagSE+CycleSE, with simple projection layers for each PSE type.\n\nIn Table 4, GPSE appears to be less advantageous compared to the SSL `Self-supervised pre-trained (Hu et al., 2020b)` model. \n(Hu et al. outperform GPSE on 3 out of 5 datasets by some margins)"
                },
                "questions": {
                    "value": "Could you please verify whether the owner of the Google Drive link is the author?\n(if you want to check the name that I found, then I will leave the name in the comment)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4049/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698798089571,
            "cdate": 1698798089571,
            "tmdate": 1699636368481,
            "mdate": 1699636368481,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "c7iiwVRGYG",
                "forum": "yJdj2QQCUB",
                "replyto": "L1BTSnRyHG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4049/Reviewer_vPPW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4049/Reviewer_vPPW"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you to the authors for their responses. I feel sorry for any unintended concern, but as a reviewer, I felt obligated to report this issue.\n\n[1] To clarify for AC and the reviewers, when I opened the Google Drive link, I could immediately see the owner's name on the right side of my browser without any additional steps (NOT by searching for more detailed information). So, my discovery of the author's name was merely by clicking the link provided by the authors.\n\n\n[2] Also, by clicking for additional information on the file from the provided link, one can still access the email (AuthorID@msu.edu, which is used in publications) and affiliation information (Michigan State University).\nAdditionally, the name I discovered doesn't seem to be a simple alias. It clearly shares the same family name as the author, with a slightly altered given name. I was able to make this connection because the author used the same alias on GitHub, and their linked Twitter account was posting about the content of the paper.\n\n\n\nUnlike the point [1], I leave the judgment of point [2] to AC as it involved an additional step (clicking for information of the file) and further searching (appearing at the top in incognito mode searches). As both a reviewer and a researcher, I express my regret for the necessity of reporting this.\n\n\n\n*I recommend using an anonymous cloud storage service or creating an anonymous account for Google Drive to host data.*"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4049/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699988100070,
                "cdate": 1699988100070,
                "tmdate": 1700030739553,
                "mdate": 1700030739553,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "5MKHSHtPDh",
                "forum": "yJdj2QQCUB",
                "replyto": "5AuMLUAWd2",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4049/Reviewer_vPPW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4049/Reviewer_vPPW"
                ],
                "content": {
                    "comment": {
                        "value": "Regardless of the anonymity issue that I raised, I would also like to discuss the point made by Reviewer 7nJh and Reviewer rYLm, since positional and structural encodings are significant topics in recent graph learning.\n\nI have a same concern with Reviewer 7nJh and Reviewer rYLm about the fair comparison on Table 2 as noted in my review:\n>For fair comparison in Table 2 and Table 3, GPSE should be compared not only with single combinations like GPS+LapPE or GPS+RWSE, but also with a more comprehensive set-up, including GPS+LapPE+ElstaticPE+RWSE+EigValSE+HKdiagSE+CycleSE, with simple projection layers for each PSE type.\n\nAdditionally, to provide a detailed analysis, it would be beneficial to include the experimental results in Appendix for each combination of encodings (such as GPS+LapPE+ElasticPE, GPS+LapPE+RWSE, GPS+RWSE+CycleSE, ...).\n\nI hope that the comments have been helpful in clarifying the contribution of this paper, and I look forward to the authors' response."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4049/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700136512670,
                "cdate": 1700136512670,
                "tmdate": 1700136512670,
                "mdate": 1700136512670,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1l1NIZptnm",
                "forum": "yJdj2QQCUB",
                "replyto": "7kd1xPqDvZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4049/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4049/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to reviewer vPPW (1/2)"
                    },
                    "comment": {
                        "value": "> The research primarily excels in engineering advancements for graph neural networks, focusing more on practical architectural solutions than on extending theoretical foundations. Although these innovations address key issues in GNNs, they potentially add complexity and computational overhead that aren't thoroughly examined.\n\nWe thank the reviewer for recognizing the engineering advancements of GPSE and its practical contributions to the community. We understand the importance of providing a more thorough analysis of GPSE\u2019s computational complexity to improve its practical impact further. We kindly refer the reviewer to our global response, \u201cComputational complexity of GPSE,\u201d for more detailed information regarding the newly added analysis. In short, we have demonstrated and explained the computational benefit of using GPSE instead of computing all exact PSEs.\n\n> The model's performance, when combined with GPSE, is less impressive, considering its complexity, which includes 20 MPNN layers, MLP heads for each PSE, a gating mechanism, and a virtual node.\n\nTo avoid any potential confusion, we would like to underline that these listed components are *of the GPSE model* that are used to generate latent PSEs *in inference mode*, and do not pertain to the downstream models (GPS/GIN/GCN etc.) used. All GPS experiments have been run with 3 to 10 layers, while GCN and GraphSAGE for the node-level tasks use only 3 message-passing layers, for example. The MoleculeNet experiments (Table 4) are similarly conducted with a 5-layer GINE, identical to Sun et al. (2022). In short, the models we use to obtain the results are appropriate for their respective benchmarking settings.\n\nThere is of course a limitation of how much PSEs by themselves can help improve performance, which is bound by how much positional information is useful for the given dataset and task at hand. This also depends on the PSEs that are learned; with more powerful PSEs, we can expect the benefits of GPSE also compound. For the scope of our paper, we would like to emphasize that **we consistently match or beat individual PSEs as well as their combinations (please note the added AllPSE experiments)**, an important result by itself.\n\nFinally, despite the suggested complexities, **GPSE is nevertheless much more efficient than the alternative of explicitly computing PSEs**. We have conducted extensive studies on computational runtimes on both (a) increasing number of graphs, and (b) increasing graph sizes while keeping the number of graphs constant, and demonstrated that GPSE has both better wall-clock times and scaling properties on average than individual PSEs, and their combinations. We kindly refer the reviewer to our global response \u201cComputational complexity of GPSE\u201d as well as Appendix [H] for results and additional visualizations.\n\n> For fair comparison in Table 2 and Table 3, GPSE should be compared not only with single combinations like GPS+LapPE or GPS+RWSE, but also with a more comprehensive set-up, including GPS+LapPE+ElstaticPE+RWSE+EigValSE+HKdiagSE+CycleSE, with simple projection layers for each PSE type.\n\nWe appreciate the reviewer for bringing up this valid concern regarding a more proper baseline of concatenating all PSEs. We kindly refer the reviewer to our global response, \u201cComprehensive comparison against AllPSE baselines\u201d, for more detailed information about the newly added baseline as suggested. In brief, we observed that GPSE outperforms or matches the performance of AllPSE in nearly all of our primary benchmarks."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4049/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700289525545,
                "cdate": 1700289525545,
                "tmdate": 1700289525545,
                "mdate": 1700289525545,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "X9dPnJdZhm",
                "forum": "yJdj2QQCUB",
                "replyto": "7kd1xPqDvZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4049/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4049/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to reviewer vPPW (2/2)"
                    },
                    "comment": {
                        "value": "> In Table 4, GPSE appears to be less advantageous compared to the SSL Self-supervised pre-trained (Hu et al., 2020b) model. (Hu et al. outperform GPSE on 4 out of 5 datasets by some margins)\n\nWe first highlight that the Hu et al. results in fact report the *best of 5 distinct self-supervised pre-trained methods*; we opted to show only the best for each dataset for compactness. Our original impression was that the table became overcrowded and detracted from the main points of our results regarding information transfer to downstream tasks. We thank the reviewer for pointing this out, as the current setup may be misleading in that it puts the SSL methods in Hu et al., 2020b in a better light than they actually are. We have therefore added the expanded version of Table 4 to Appendix [I]. \n\nAdditionally, we have updated our GPSE results in Table 4 after conducting more hyperparameter search on the *mixing ratio between GPSE and the original graph features* - a standard parameter to tune in GPS. Previously, we did not optimize the GPSE performance on the MoleculeNet benchmarks as we did not intend to imply a direct comparison between SSL and GPSE: **The two approaches are complementary** to each other and the key conclusion we wanted to draw was that **GPSE also shows strong ability to transfer knowledge** from large pre-train datasets to the downstream task. While we still think that was the primary intention, we also understand the reviewer\u2019s concern about GPSE appearing to be less advantageous to SSL on paper. With these latest results, *GPSE matches or outperforms the combined SSL on 7/8 MoleculeNet datasets*, and overall are consistently more powerful than any single SSL method.\n\nFinally, we want to reiterate the difference in GPSE\u2019s and SSL\u2019s abilities to transfer knowledge. SSL methods are constrained to transfer knowledge between graphs that have the exact format: node features and edge features. This is due to the nature of SSL expecting the same input graph format between pre-train and downstream datasets. On the flip side, GPSE by design, is more flexible: it can transfer knowledge to *any* graph structured data, as it only uses graph connectivity for the computation, not node/edge features. This flexibility makes the obtained competitive results against SSL methods even more impressive.\n\n> Could you please verify whether the owner of the Google Drive link is the author? (if you want to check the name that I found, then I will leave the name in the comment)\n\nWe greatly appreciate the reviewer's suggestions and good intentions regarding using anonymized cloud storage solutions to anonymize our work for review more strictly. We acknowledge the concern about the ownership of the Google Drive link provided in our submission, as discussed in a main thread. That said, we commit to strictly adhering to the recommended practices of using anonymized cloud storage solutions in all our future submissions. We assure the reviewer that maintaining the integrity of the double-blind review process is of utmost importance to us, and any lapse in anonymity was entirely unintentional. We hope this addresses the reviewer's concern and appreciate their understanding in this matter.\n\n---\nWe sincerely thank the reviewer for their thoughtful and insightful comments, which we have found immensely helpful in enhancing the soundness of our work. In response, we have diligently addressed all the points raised, ensuring that each concern has been thoroughly considered and acted upon. Nevertheless, we remain open to further discussion and are fully prepared to conduct additional experiments if required to refine our work further. Given our comprehensive response to the feedback provided, we kindly request the reviewer to reevaluate our manuscript, hoping that these enhancements will positively impact our paper's overall quality and merit."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4049/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700289568077,
                "cdate": 1700289568077,
                "tmdate": 1700289568077,
                "mdate": 1700289568077,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "nKm5jvwzZ9",
                "forum": "yJdj2QQCUB",
                "replyto": "7kd1xPqDvZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4049/Reviewer_vPPW"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4049/Reviewer_vPPW"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the detailed responses from the authors. \n\nI recognize the importance of this research topic, which led me to carefully read the authors' rebuttal as well as the comments from other reviewers. \n\nRegarding the comparison with SSL, I fully understand the authors' response. \n\nFor other aspects such as complexity and efficiency, the additional information provided in the Global Replies about inference time suggests that GPSE is efficient. \n\nYet, when considering the training time, it still raises questions about its practicality (since graph sizes in experiments are mostly small, the comparison based on graph size was evaluated more from a scalability perspective than practicality or efficiency). \n\nWith these factors in mind, I would like to discuss further with other reviewers whether the complex computations of GPSE are justifiable. \n\nFor the analysis, it would be beneficial to conduct comparative analyses with various combinations beyond GPS+AllPSE (I am aware of the time constraints of this review process, so this suggestion is intended for future improvements).\n\nThis is essential since the paper seems to hold more value in engineering aspects rather than theoretical ones, and a detailed approach would provide the readers with rich information.\n\n*(Regarding the issue of anonymity, I would like to continue the discussion with AC.)*"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4049/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588783352,
                "cdate": 1700588783352,
                "tmdate": 1700588783352,
                "mdate": 1700588783352,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "eQcmKQeUJl",
            "forum": "yJdj2QQCUB",
            "replyto": "yJdj2QQCUB",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4049/Reviewer_6S8q"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4049/Reviewer_6S8q"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces GPSE a method to extract position and structural encodings (pse) from graphs. It can also be applied to other graph modelling tasks without the explicit need to compute the pse. The methos adopts a self-supervision style to train GPSE comprised of an encoder module which extracts the features and then passes through the decoder (MLP) to obtain the final embeddings. The loss is calculated against each of the six pse which is the target. For training, they use the MolPCBA dataset and test it against different molecular and node classification datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The problem of solving for transferable position and structural encoding is of high importance and relevance in the graph machine learning area.\n- The paper shows extensive experiments on a variety of different datasets to validate the method of transferability across different graph types."
                },
                "weaknesses": {
                    "value": "- The GPSE architecture is unclear in section 2.2, even after re-reading the appendix and the section multiple times. It would have been easier to follow if the approach was explained in a step-by-step manner and aided with equations. \n- The contributions in the paper seem to be limited as many of the methods have been adopted similarly but is unclear how are they different from the existing approach. (please refer to questions)"
                },
                "questions": {
                    "value": "- Section 2.2 talks more about the general effect of over-smoothing and squashing in graph neural networks, but it is not clear what this has to do with the method of learning effective position encoding for graphs. Why is it relevant to the problem?\n- In Table 4 GPSE shows better performance on MoleculeNet dataset where it is mentioned that GPSE is at a comparative disadvantage but is this due to the nature of the dataset or the importance of structural and position to the specific dataset? It will be interesting to observe what will be the effect of GINE+{LapPE/RWSE} vs GPSE. \n- Why are results from other datasets like (ClinTox MUV HIV) not included in Table 4 as GraphLoG is evaluated on these datasets? \n- Why are GNN models used as a baseline for testing the extreme OOD node-classification benchmarks (table 6) instead it will be interesting to see how transformer+GPSE will perform on the dataset as they don't have the inductive bias of GNNs. \n- The paper [1] has a similar research question and method to GPSE, can the authors help me understand the difference? \n- For SSL the methods are generally pre-trained and then fine-tuned on down-stream tasks, whereas in GPSE the base model GINE is augmented with learned pse and trained from scratch, how does this comparison seem to stand out? The comparison to the baseline of GINE from Hu et al. (2020b) is fair, but how can it be compared to SSL techniques? \n\n\n[1] GRAPH NEURAL NETWORKS WITH LEARNABLE STRUCTURAL AND POSITIONAL REPRESENTATIONS (ICLR 22)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4049/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4049/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4049/Reviewer_6S8q"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4049/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698798983697,
            "cdate": 1698798983697,
            "tmdate": 1699636368400,
            "mdate": 1699636368400,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "o6i9gdiQdr",
                "forum": "yJdj2QQCUB",
                "replyto": "eQcmKQeUJl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4049/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4049/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 6S8q (1/4)"
                    },
                    "comment": {
                        "value": "> The GPSE architecture is unclear in section 2.2...\n\nWe thank the reviewer for suggesting improving the clarity of the GPSE\u2019s computation. We have added a more detailed formulation of the GPSE model with equations to Appendix B.1. In brief, GPSE takes input node features as random noise generated from the standard Gaussian distribution, which is then linearly projected to the hidden dimension. It then passes the processed features through a 20-layer GatedGCN with virtual nodes. Finally, the processed hidden representations are projected to predict one of the PSEs via a two-layer MLP.\n\n> The contributions in the paper seem to be limited as many of the methods have been adopted similarly but is unclear how are they different from the existing approach.\n\nWe thank the reviewer for raising this question. While we disagree with the statement that \u201csimilar methods have already been adopted and that this limits the contributions of the paper\u201d, we understand that this likely stems from the confusion in the scope of GPSE and LPSE papers, which are completely distinct as we have addressed in our global response, \u201cRelation between GPSE and LPSE\u201d. We would nevertheless like to further elucidate our contributions and overall novelty of our method here.\n\nOur novelty lies in the idea that we can **pre-train a positional encoder from random features using a standard MPNN** \u2013 a task that is *novel in the literature with no prior attempt* to the best of our knowledge. We not only show that our learned PSEs consistently and significantly outperform conventional PSEs across a wide range of datasets and tasks, but also are much **faster to compute** and have better scaling properties (please see Appendix H for the new computational cost and scaling experiments). Additionally, we show that GPSE is extremely generalizable, a property that is inherently absent in conventional PSEs. As mentioned in the paper, a scalable and generalizable PSE has huge implications for building efficient foundational models for graph data.\n\nFinally, we would like to demonstrate that designing an MPNN that is suitable for this task is also **technically challenging**, even if many submodules we employed are readily available from the literature: As indicated by our ablation studies, typical GNNs (such as GCN or GIN) or network depths (<5 layers) are completely unsuitable for learning PSEs. Instead, we found that gated message passing (GatedGCN) and unusually deep networks are essential for this task. Furthermore, we have (1) discussed these design decisions in Section 2.2, with *theoretical justifications* in Appendix C, (2) showed *evidence supporting the effectiveness* of our choices in Table 1, Figure 2 and Tables F.1 and F.3, and (3) demonstrated the effectiveness of the GPSE model on specific *1-WL toy examples* in appendix E. These extensive discussions we provided and the ablation studies we conducted are not just supplemental information. They stand testament to the effectiveness of GPSE and its ability to capture rich positional and structural representations of graphs in a unified framework for augmenting downstream prediction models."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4049/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700289294674,
                "cdate": 1700289294674,
                "tmdate": 1700289459060,
                "mdate": 1700289459060,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aVqAPcY7Ov",
                "forum": "yJdj2QQCUB",
                "replyto": "eQcmKQeUJl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4049/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4049/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 6S8q (2/4)"
                    },
                    "comment": {
                        "value": "> Section 2.2 talks more about the general effect of over-smoothing and squashing in graph neural networks, but it is not clear what this has to do with the method of learning effective position encoding for graphs.\n\nWe thank the reviewer for raising the question regarding the relevance of over-smoothing/-squashing in our problem setup. We note that these problems are essential to overcome to *effectively learn the positional and structural encodings*, especially for those that require **global views of the graph**. For example, the Laplacian eigenvector corresponding to the first non-trivial eigenvalue, also known as the Fiedler vector, corresponds to the solution of the graph min-max cut problem. Intuitively, this problem requires accessing the global view of the entire graph as it, colloquially, aims to partition the entire graph into two parts with minimal connections.\n\nA straightforward solution to incorporating more global information into the model is by *stacking more message-passing layers* to increase the receptive field and thus effectively expose the model to information beyond the local structure. However, simply stacking more message-passing layers easily leads to the **over-smoothing** problem, where the messages of each node become increasingly uniform as the number of layers increases. Our usage of the gating mechanism, along with residual connection, effectively mitigates this issue while still exposing the model to more non-local information.\n\nMeanwhile, the model may still have difficulty incorporating global information, even after fixing the over-smoothing issue and stacking more layers due to **over-squashing**. Informally, over-squashing can be understood as the difficulty in losslessly sending messages between two nodes across the network. This difficulty is primarily because there are only a few possible routes between the two nodes compared to all other available routes to each of the nodes. We mitigate this problem using a *virtual node* that serves as the global information exchange hub to enable global information exchange, bypassing the \u201cfew routes\u201d limitation. Our ablation study shown in Figure 2 showed empirical evidence about the effectiveness of our solutions to over-smothering/-squashing.\n\nTo summarize, the over-smoothing and over-squashing problems stem from the requirement of the GPSE model to access global information of the graph effectively, which is necessary for some of the positional encodings we aim to learn. We have also incorporated the above explanations into Appendix C to clarify this relationship, and have adjusted Section 2.2 to refer to this section so that the connection is clearer in the main paper as well."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4049/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700289343938,
                "cdate": 1700289343938,
                "tmdate": 1700289343938,
                "mdate": 1700289343938,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aLiXCVR7TF",
                "forum": "yJdj2QQCUB",
                "replyto": "eQcmKQeUJl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4049/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4049/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Reviewer 6S8q (3/4)"
                    },
                    "comment": {
                        "value": "> Why are GNN models used as a baseline for testing the extreme OOD node-classification benchmarks (table 6) instead it will be interesting to see how transformer+GPSE will perform on the dataset as they don't have the inductive bias of GNNs. \n\nClassical GNNs like GCN and GraphSAGE were used as baselines for the OOD node-classification benchmarks because they are known to outperform graph Transformers and related models (e.g. GPS) on large, homophilic graphs typically used for node classification, for reasons we elaborate on below. Nevertheless, we have run experiments with both graph Transformer and GPS models, we report the results below, the full table is also added to Appendix F, table F.7. In both cases, we see that GPSE leads to clear improvements, even though the overall performance falls short of GCN/SAGE/GATEv2, still leading architectures for large graphs.\n\nGraph Transformers typically have quadratic complexity in the number of nodes to employ global attention between node pairs. These computational requirements become prohibitively large for graphs with nodes in the order of thousands. Additionally, graph Transformers are known to run into over-smoothing issues much more quickly than MPNNs, which leverage the sparsity of these graphs. Especially for homophilic large graphs, the inductive biases of MPNNs are therefore typically preferable to graph Transformers for both theoretical and empirical reasons.\n\nBuilding scalable graph Transformers is expectedly a major area of interest in graph learning. One simple and effective approach to improve the scalability of both MPNNs and graph Transformers is neighbor sampling, where a subset of neighbors (or nodes in the case of a Transformer) are employed for message-passing. However, we may expect neighbor sampling to simulate the information flow of a graph Transformer only approximately, leading to some loss of information in the process; this is also implied by the comparatively weak performance of the Transformer-based models.\n\n| ****                   | ****      | **arXiv**             | **Proteins**          |\n|------------------------|-----------|--------------------------------|--------------------------------|\n|                        | **+GPSE?** | **ACC %** | **AUROC \\%** |\n| GCN                    | NO    | 71.74 \u00b1 0.29                   | 79.91 \u00b1 0.24                   |\n|                        | YES    | 71.67 \u00b1 0.12                   | 79.62 \u00b1 0.12                   |\n| SAGE                   | NO    | 71.74 \u00b1 0.29                   | 80.35 \u00b1 0.07                   |\n|                        | YES    | 72.19 \u00b1 0.32                   | 80.14 \u00b1 0.22                   |\n| GAT(E)v2               | NO    | 71.69 \u00b1 0.21                   | 83.47 \u00b1 0.13                   |\n|                        | YES    | 72.17 \u00b1 0.42                   | 83.51 \u00b1 0.11                   |\n| Transformer            | NO    | 57.00 \u00b1 0.79                   | 73.93 \u00b1 1.44                   |\n|                        | YES    | 59.17 \u00b1 0.21                   | 74.67 \u00b1 0.74                   |\n| GPS                    | NO    | 70.60 \u00b1 0.28                   | 69.55 \u00b1 5.67                   |\n|                        | YES    | 70.89 \u00b1 0.36                   | 72.05 \u00b1 3.75                   |\n\n> The paper [1] has a similar research question and method to GPSE, can the authors help me understand the difference? \n\nWe kindly refer the reviewer to our global response, \u201cRelation between GPSE and LPSE,\u201d for a more in-depth explanation of the difference between the two."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4049/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700289423161,
                "cdate": 1700289423161,
                "tmdate": 1700289423161,
                "mdate": 1700289423161,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9Qm0Eu4oBf",
                "forum": "yJdj2QQCUB",
                "replyto": "eQcmKQeUJl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4049/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4049/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply to 6S8q (4/4)"
                    },
                    "comment": {
                        "value": "> For SSL the methods are generally pre-trained and then fine-tuned on down-stream tasks, whereas in GPSE the base model GINE is augmented with learned pse and trained from scratch, how does this comparison seem to stand out? The comparison to the baseline of GINE from Hu et al. (2020b) is fair, but how can it be compared to SSL techniques?\n\nWe thank the reviewer for the question regarding fine-tuning the GPSE to adapt to the specific downstream task. We have indeed tried this idea on two primary molecular property benchmarking tasks: ZINC and PCQM4Mv2. As we have presented in Appendix Table F.5, fine-tuning could help in certain cases, but there is no consistent pattern showing the superiority of fine-tuning. However, fine-tuning pre-trained models is a vast topic, and how to effectively do so for better downstream performance in our specific setting is an under-explored area. Thus, an exciting future avenue is to study the optimal way to fine-tune GPSE to further adapt to specific downstream datasets. \n\n---\nWe extend our gratitude for the valuable feedback provided by the reviewer. The insights and suggestions have been essential in guiding the improvements to our manuscript. We have carefully addressed each point raised and are prepared for any necessary further discussions or additional experiments. With these revisions, we kindly ask the reviewer to reevaluate their initial assessment and increase their score if deemed appropriate."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4049/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700289449805,
                "cdate": 1700289449805,
                "tmdate": 1700289449805,
                "mdate": 1700289449805,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dYWUXRkzWe",
                "forum": "yJdj2QQCUB",
                "replyto": "eQcmKQeUJl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4049/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4049/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Friendly Reminder to Reviewer 6S8q for Open Discussion"
                    },
                    "comment": {
                        "value": "Dear Reviewer 6S8q,\n\nWe are immensely grateful for your thorough review and the valuable feedback on our work. In light of your comments, we have meticulously addressed each point in our rebuttal. As the time frame for open discussion is nearing its close, we kindly request your attention to our responses and welcome any further feedback or queries you might have. Your timely and detailed feedback is essential for us to ensure the quality and completeness of our revisions. We greatly appreciate your continued guidance and look forward to your response soon.\n\nBest regards,\n\nThe Authors"
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4049/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668246708,
                "cdate": 1700668246708,
                "tmdate": 1700668328272,
                "mdate": 1700668328272,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]