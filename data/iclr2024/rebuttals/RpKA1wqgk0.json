[
    {
        "title": "MetaFormer with Holistic Attention Modelling Improves Few-Shot Classification"
    },
    {
        "review": {
            "id": "349NJ7uaZu",
            "forum": "RpKA1wqgk0",
            "replyto": "RpKA1wqgk0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9345/Reviewer_xXZf"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9345/Reviewer_xXZf"
            ],
            "content": {
                "summary": {
                    "value": "A coherent and lightweight framework MetaFormer is proposed to improve the Vision Transformer performance in meta-learning. The framework contains a sample-level attention module (SAM) and a task-level attention module (TAM). The SAM enables the consistency of attended features across samples in a task whilst the TAM regularizes the learning of features for the current task by attending to a specific task in the pool. Extensive experiments are conducted on four commonly used FSL benchmark datasets and the new SOTA is achieved with a marginal increase in computational cost."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-- The paper adds inter-sample and inter-task attention modules to the original Vision Transformer for meta-learning. These ideas are not novel whilst the implementation of them in such a way is somewhat novel.\n\n-- The presentation is generally good but lacks clarity for some details (see weaknesses).\n\n-- Thorough experiments on the in-domain and cross-domain settings provide valuable references to the community of FSL."
                },
                "weaknesses": {
                    "value": "-- The authors use the terms context and target sets instead of support and query sets which are usually employed in FSL literature. This makes the presentation harder to understand.\n\n-- The main concern is the use of test samples during learning which makes it a transductive learning method. It might be that I misunderstand the training details of the model but it is unclear to me if M unlabelled query samples are used in any way before final prediction using Eq(7).\n\n-- The autoregressive inference setting employs the information from the test data. It is unfair to compare with those under the true inductive learning setting.\n\n-- It is unclear what properties of the learned task embedding have. A suggestion would be to analyze the task embedding space somehow to give intuitive insights for better understanding. For example, one can sample N tasks from the meta-testing data and M tasks from the meta-training data and compute the NxM distance matrix and take a closer look at some exemplar rows/columns to see what kind of tasks are similar in the learned task embedding space."
                },
                "questions": {
                    "value": "1. In tables 1 and 2,  why are the bottom two groups of methods separated whilst they use the same backbone ViT-Small?\n\n2. If I misunderstand the method regarding the transductive setting, is it possible to adapt the framework to the transductive setting and how?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9345/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698221199414,
            "cdate": 1698221199414,
            "tmdate": 1699637175826,
            "mdate": 1699637175826,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pWqvqAxzn4",
                "forum": "RpKA1wqgk0",
                "replyto": "349NJ7uaZu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9345/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9345/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xXZf (Part 1)"
                    },
                    "comment": {
                        "value": "We extend our appreciation for your  constructive feedback on our manuscript. Below, we address each of your points comprehensively. Should there be any additional queries or clarifications needed, please feel free to let us know.\n\n#### Q1: The terms context and target sets instead of support and query sets\n> Thanks for your suggestion. We have revised our manuscript to use \"support\" and \"query\" sets to align with more FSL works and enhance clarity. Our initial choice of \"context\" and \"target\" aligns with previous meta-learning works of [1-4],  where \"context\" set provide conditions for task adaptation and \"target\" set serves evaluation targets of the adapted model. \n>\n> [1] Zintgraf, et al. \u201cFast Context Adaptation via Meta-Learning.\u201d In ICML, 2019\n>\n> [2] Rusu, et al. \u201cMeta-learning with latent embedding optimization.\u201d In ICLR, 2019\n>\n> [3] Requeima, et al. \u201cFast and \ufb02exible multi-task classi\ufb01cation using conditional neural adaptive processes.\u201d In NeurIPS, 2019.\n>\n> [4] Patacchiola, et al. \u201cContextual Squeeze-and-Excitation for Efficient Few-Shot Image Classification.\u201d In NeurIPS, 2022\n\n#### Q2: Whether the proposed method is inductive or transductive\n> - We would like to humbly clarify that the proposed MetaFormer with its meticulously designed sample causal mask **flexibly and efficiently accommodates both inductive and transductive settings**. This also constitutes one of our **core contributions**.\n>    - When configuring the sample causal mask as the inductive variant (refer to Figure 2(a) for details), we instantiate MetaFormer-I. This design choice with **query interactions blocked**, coupled with the inherent Layer Normalization in vision transformers, ensures **independent predictions for each query sample**, adhering strictly to inductive learning principles. \n>    - When configuring the sample causal mask as the autoregressive variant (refer to Figure 2(b) for details), we instantiate **MetaFormer-A**. This represents a **coherent and efficient transductive implementation** capable of extracting support and query feature embeddings **in a single feedforward pass**. To the best of our knowledge, MetaFormer-A stands out as the **first pure transformer-backed method for transductive few-shot image classification**.  It eliminates the need for extra time-consuming components suh as label propagation in GNNs [1] or specialized loss functions [2], offering simplicity and efficiency. \n> - We sincerely appreciate the reviewer's great suggestion to include more comparisons validating the superiority of our transductive version, MetaFormer-A. In response,\n>   - We conduct additional comparisons, pitting MetaFormer-A against other transductive methods. The results, in the table below, demonstrates its superior performance with a 79.41% accuracy without introducing extra components like label propagation in GNNs [1] or specialized loss functions [2], establishing it as a **compelling choice for seamless transitioning from the inductive setting**.\n>   \n>     | Method        | Backbone |miniImageNet 5-way 1-shot |\n>     | ------------- | -------- |------------- |\n>     | TPN [1]       | ResNet   | 59.46        |\n>     | ADV [2]       | WRN      | 74.63        |\n>     | MetaFormer-A  | ViT-Small | **79.41**       |\n>\n>    - We further conduct a comparative analysis employing a naive transductive approach, wherein constraints on query interactions as MetaFormer-A impose are omitted, and the sampling size ($r$) is set to be equal to the number of query samples. This method is analogous to the one described in [3]. The following table results showcase the **superiority of our autoregressive version MetaFormer-A**.\n> \n>      | Method        | Backbone |miniImageNet 5-way 1-shot |\n>      | ------------- | -------- |------------- |\n>      | naive transductive | ViT-Small | 76.00    |\n>      | MetaFormer-I  | ViT-Small | 75.78       |\n>      | MetaFormer-A  | ViT-Small | **79.41**       |\n> \n>\n> [1] Liu, et al. \"Learning to propagate labels: Transductive propagation network for few-shot learning.\" arXiv preprint arXiv:1805.10002, 2018.\n> \n> [2] Huang, et al. \"Improving Task-Speci\ufb01c Generalization in Few-Shot Learning via Adaptive Vicinal Risk Minimization.\" In NeurIPS, 2022\n> \n> [3] Hou, et al. \"Cross attention network for few-shot classification.\" In NeurIPS, 2019\n\n#### Q3: Visualization analysis of learned task embeddings\n\n> Thank you for your insightful suggestion. We conduct a qualitative analysis of learned task probe vectors across different tasks randomly sampled from meta-train (M=3) and meta-test sets (N=3). As shown in Appendix J Fig. 7, the visualization effectively reveals the **efficacy of these vectors in capturing task relationships**. For example, we observe a higher similarity in task features among tasks involving car tires, dogs, and long-legged animals. This demonstrates MetaFormer's capability in discerning and utilizing task semantics."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9345/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700529933739,
                "cdate": 1700529933739,
                "tmdate": 1700529933739,
                "mdate": 1700529933739,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TSVHVSVuS5",
                "forum": "RpKA1wqgk0",
                "replyto": "349NJ7uaZu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9345/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9345/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xXZf (Part 2)"
                    },
                    "comment": {
                        "value": "#### Q4. In tables 1 and 2, why are the bottom two groups of methods separated whilst they use the same backbone ViT-Small?\n\n> We  apologize for any potential misunderstandings. Our intention in distinguishing between these two groups of methods is to **highlight the effectiveness of MetaFormer when employed in conjunction with diverse training paradigms**. \n>  - FewTURE [1] represents a pioneering meta-learning approach specifically tailored for self-supervised pre-trained Vision Transformers (ViT). \n>  - The SMKD [2] method adopts a transfer learning approach based on self-distillation. \n> \n> The results obtained from our experiments consistently showcase MetaFormer's ability to enhance task adaptation across both of these distinct methodological groups.\n> \n> [1] Hiller, et al. \"Rethinking generalization in few-shot classification.\" In NeurIPS, 2022.\n>\n> [2] Lin, et al. \"Supervised masked knowledge distillation for few-shot transformers.\" In CVPR, 2023"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9345/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700529979843,
                "cdate": 1700529979843,
                "tmdate": 1700529979843,
                "mdate": 1700529979843,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "NdzPIn7rlg",
                "forum": "RpKA1wqgk0",
                "replyto": "349NJ7uaZu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9345/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9345/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We would love to hear back from Reviewer xXZf"
                    },
                    "comment": {
                        "value": "Hi Reviewer xXZf,\n\nWe would like to follow up to see if our response addresses your concerns or if you have any further questions. We would really appreciate the opportunity to discuss this further if our response has not already addressed your concerns.\n\nThank you again!"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9345/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633978122,
                "cdate": 1700633978122,
                "tmdate": 1700633978122,
                "mdate": 1700633978122,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "RySiyREFCR",
                "forum": "RpKA1wqgk0",
                "replyto": "NdzPIn7rlg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9345/Reviewer_xXZf"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9345/Reviewer_xXZf"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the reply, much appreciated. The transductive learning method does not seem to perform comparably well with SOTA though the more advanced ViT backbone is employed."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9345/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661462554,
                "cdate": 1700661462554,
                "tmdate": 1700661462554,
                "mdate": 1700661462554,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "sWQVfdwBjg",
            "forum": "RpKA1wqgk0",
            "replyto": "RpKA1wqgk0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9345/Reviewer_xYfp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9345/Reviewer_xYfp"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed MetaFormer, a new Transformer-based method for meta-learning. To improve the efficiency of self-attention in meta-learning, the authors propose to distengle the computation into three different dimensions: Task Attention, Spatial Attention and Sample Attention. Compared to existing state-of-the-arts, the proposed method achieves better performance across different datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1.  The motivation of this paper is clear: most exisiting meta-learning frameworks only show effectiveness in convolutional neural networks, as Transformer is prevailing these days, it is meaningful to validate and adapt this architecture in meta-learning as well.\n2. The proposed method achieves clearly better performance than SOTA methods.\n3. This paper is easy to follow. The figures well illustrate the framework of the proposed MetaFormer."
                },
                "weaknesses": {
                    "value": "1. Recent large-scale pretrained vision foundation models, such as CLIP and SAM, have demonstrated superior zero-shot performance on image classification and visual grounding. In this context, one of my primary concerns is that the problem setting in this paper is not sufficiently significant. For example, datasets such as miniImageNet and CIFAR-FS may provide insights into the performance of meta-learning frameworks on small datasets, but they cannot accurately reflect performance on large-scale open-vocabulary datasets.\n\n2. From a technical perspective, the proposed holistic attention mechanism is novel in meta-learning. However, in general, the decoupling of self-attention computation into multiple dimensions is not new in the literature [1, 2]. For example, in video processing, TimeSformer [2]  has proposed to separate the spatial and temporal attention in a single block.\n\n3. The paper does not demonstrate any efficiency gains from the proposed attention design. This is unconvincing, as one of the primary motivations described in the introduction is to reduce the computational cost of attention in ViTs when adapting for Meta-learning.\n\n[1] Ho, Jonathan, et al. \"Axial attention in multidimensional transformers.\" arXiv preprint arXiv:1912.12180 (2019).\n\n[2] Bertasius, Gedas, Heng Wang, and Lorenzo Torresani. \"Is space-time attention all you need for video understanding?.\" ICML. Vol. 2. No. 3. 2021."
                },
                "questions": {
                    "value": "Can the authors report the FLOPs and inference speed, memory cost in Table 1, 2, 3? Or at least the Table 3."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9345/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9345/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9345/Reviewer_xYfp"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9345/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698488965799,
            "cdate": 1698488965799,
            "tmdate": 1699637175719,
            "mdate": 1699637175719,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CKk0I9rnHu",
                "forum": "RpKA1wqgk0",
                "replyto": "sWQVfdwBjg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9345/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9345/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xYfp (Part 1)"
                    },
                    "comment": {
                        "value": "We appreciate very much your constructive comments on our paper. Please kindly find our response to your comments below, and all revisions made to the paper are highlighted in red for your ease of reference. We hope that our response satisfactorily addresses the issues you raised. Please feel free to let us know if you have any additional concerns or questions.\n\n#### Q1. Recent large-scale pretrained vision foundation models, such as CLIP and SAM, have demonstrated superior zero-shot performance. In this context, one of my primary concerns is that the problem setting in this paper is not sufficiently significant. Datasets such as miniImageNet and CIFAR-FS cannot accurately reflect performance on large-scale open-vocabulary datasets.\n> - Our paper is **expressly designed to enhance the few-shot performance of pre-trained vision foundation models** on downstream tasks, as underlined in the Introduction (last sentence in the first paragraph).\n>   - Pre-trained vision model: In our previous experiment, we adopt the **ViT pre-trained by DINO** as the pre-trained vision model for meta-training. Extensive empirical results robustly validate the effectiveness of our proposed method in enhancing few-shot learning performance on the pre-trained model.\n>   - Datasets: Despite the relatively modest scale of miniImageNet ad CIFAR-FS, our performance on these datasets **offers valuable insights into open-vocabulary datasets.** We follow the practice in FewTURE by pre-training DINO on only the meta-training set of the two datasets. This mirrors real-world scenarios where open-vocabulary downstream datasets are often out of the distribution of the pre-training dataset.\n> - **Existing literature highlights the potential for further enhancing CLIP's few-shot performance on downstream tasks utilizing few-shot techniques** [3-5]. Many of these innovative designs and key insights originate from meta-learning experiments conducted on small datasets. For instance, the cache model in [3] is akin to matching networks [6], the context conditional prompt in [4] is inspired by task-specific vectors [7], and the observation that refined channels focus more on foreground regions [5] correlates with findings in [8] about varying channel importance across tasks.\n> - To demonstrate the **compatibility of the proposed MetaFormer with larger pre-trained vision foundation models such as CLIP** and the effectiveness on open-vocabulary datasets, we adapt our method to the CLIP model with ViT-B/16 for advancing its few-shot performance in downstream tasks.\n    >   - Baselines: We compare with **(1)** zero-shot CLIP, **(2)**  TiP-Adapter [3], a state-of-the-art method that adopts the shot feature and its corresponding label for the key and value of the cache model as the classifier head, **(3)** the variant TiP-Adapter-F [3] that fine-tunes the key in the cache model, and **(4)** the variant TiP-Adapter-F with more layers fine-tuned, being comparable to ours in the number of parameters.\n    >   - Datasets: We follow [3] and evaluate on the challenging open-vocabulary datasets, EuroSAT [9] and ISIC [10].\n    >   - Training and evaluation protocol: We adopt the episodic approach as described in [6] to construct tasks: **(1)** during training, both support and query sets are sampled from the train set; **(2)** during evaluation,  we strictly follow the TiP-Adapter [3] pipeline for sampling the support set from the train set and the query set from the test set to construct a task for evaluation. This is reasonable as there are no new classes in the test set.\n>   - Implementation: Our implementation follows TiP-Adapter [3], integrating its cache model as the auxiliary classifier head. We exclusively fine-tune the SAM modules and the head while keeping the visual and textual encoders of CLIP frozen. Evaluation employs pre-trained word embeddings of a single prompt, \u201ca photo of a [CLASS].\u201d Further implementation details are available in Appendix I.\n>   - Results: As presented in the table below, highlight noteworthy insights: **(1)** CLIP pre-trained on large-scale web-crawled image-text pairs struggles with downstream datasets exhibiting a substantial domain gap, such as the medical dataset of ISIC; **(2)** adapting CLIP with a downstream dataset is pivotal to improved performance, with a caution against the risks of overfitting with excessive parameter adaptation; **(3)** our method significantly enhances Zero-shot CLIP on EuroSAT by 42.76% and ISIC by 43.81%, and its adaptation ability also **surpasses Tip-Adapter by a large margin**. \n>  - We sincerely thank the reviewer for the constructive comments. We have also incorporated these discussion into Appendix I."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9345/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528685432,
                "cdate": 1700528685432,
                "tmdate": 1700528685432,
                "mdate": 1700528685432,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "re7RLmVOOd",
                "forum": "RpKA1wqgk0",
                "replyto": "sWQVfdwBjg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9345/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9345/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xYfp (Part 2)"
                    },
                    "comment": {
                        "value": "#### Q2. From a technical perspective, the proposed holistic attention mechanism is novel in meta-learning. However, in general, the decoupling of self-attention computation into multiple dimensions is not new in the literature [1, 2]\n> - We  would like to underscore the following novel and meaningful technical innovations made to the field of few-shot learning.\n>   - The decoupling of temporal and spatial attention has indeed been explored in video transformers [1, 2]. However, it is crucial to highlight that our consideration of the sample-to-sample relationship in few-shot learning presents a unique challenge distinct from the frame-to-frame relationship in videos, i.e., query samples have to be differentiated from support ones. Our introduction of **sample causal masks** serves an effective solution to address the challenge.\n>      - Notably, a mere adjustment in the design of these sample causal masks allows our method to **flexibly accommodate both inductive and autoregressive inference**. \n>      - We substantiate the effectiveness of the proposed masks in **Appendix D Table 5c**, concluding that a lack of effective constraints between support and query samples (see Appendix D Fig 6 for details of the ablated masks of within-support and support-query) results in sub-optimal performance. \n>   - The proposed task attention module (TAM) with **novel knowledge encoding and consolidation mechanisms** also contributes to few-shot learning within the context of vision transformers. Detailed insights into TAM's contributions are provided in response to reviewer kzi2 Q2. \n> - While we acknowledge that our proposed technique may not be characterized as groundbreaking in a broader sense, it is important to note its wide impact to the domain of few-shot learning. In particular:\n>   - The matching of local structural patterns between samples has long proven effective in few-shot learning [2, 3], though preceding endeavors relying on CNNs such as DeepEMD [2] are very slow. For example, DeepEMD demands 8 hours to evaluate 2000 5-way 5-shot episodes) [3]. We first **enable this pattern matching across both samples and tasks within the ideally fitting framework of ViT in a highly efficient manner** (requiring only 3.5 minutes). Notably, the success of ViT plays a pivotal role in explaining our markedly improved performance than prior CNN-based methods (miniImageNet 5-way 1-shot: Ours: 75.78% / DeepEMD: 65.91%).\n>   - Our decoupling of spatial and sample attention **makes the proposed method seamlessly compatible with recent state-of-the-art pre-trained vision transformers**, as mentioned in the Introduction, further enhancing their few-shot learning performances. \n>     - Our current experiments have already demonstrated the compatibility and effectiveness when working with the pre-trained model of **DINO**;\n>     - During the response period, we have also applied our approach to **CLIP**, achieving remarkable success by outperforming the SOTA method by 16.82%. Please find details to the response to Q1. \n>\n> In conclusion, we hope that these clarifications underscore the significance of our work in the field of meta-learning, and have cited/discussed these two works in the main text.\n> \n> [1] Ho, Jonathan, et al. \"Axial attention in multidimensional transformers.\" arXiv preprint arXiv:1912.12180 (2019).\n> \n> [2] Bertasius, Gedas, Heng Wang, and Lorenzo Torresani. \"Is space-time attention all you need for video understanding?.\" ICML. Vol. 2. No. 3. 2021."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9345/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528878600,
                "cdate": 1700528878600,
                "tmdate": 1700528878600,
                "mdate": 1700528878600,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "o2inDswajQ",
                "forum": "RpKA1wqgk0",
                "replyto": "sWQVfdwBjg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9345/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9345/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer xYfp (Part 3)"
                    },
                    "comment": {
                        "value": "#### Q3 and Q4. Efficiency gains in FLOPs and inference speed, memory cost \n\n> We appreciate the reviewer's invaluable feedback, and delve into a meticulous comparative analysis of computational efficiency. \n> - This assessment encompasses (1) MetaFormer, (2) a naive implementation of sample-to-sample interactions without decoupling the spatial attention and sample attention, denoted as MetaFormer-naive, and (3) the selection of two state-of-the-art and representative methods with comparable parameter counts from the meta-learning and transfer-learning domains as our primary comparison objects (4) the additional computational ablation analysis in Table 3.\n>\n> - We conduct the evaluation on an NVIDIA RTX A6000 GPU, wherein we report performance metrics, including **inference-time GFLOPs, latency, and memory usage**, specifically for 5-way 1-shot and 5-way 5-shot scenarios on the miniImageNet dataset.\n> - The tables below distinctly signify that \n>   - MetaFormer significantly **reduces computational complexity in stark contrast to MetaFormer-naive**, the naive implementation of sample-to-sample interactions.\n>   - Compared to other baselines, MetaFormer exhibits not only **higher accuracies** attributable to comprehensive sample-to-sample interactions, but also **remarkably superior computational efficiency**.\n>   - Different combinations of these modules reveal **trade-offs between model complexity and performance**, as well as the benefits of each module in enhancing the model's capabilities.\n>   \n> We have incorporated this detailed comparative analysis into Appendix E Table 6 in the revised manuscript, providing a robust understanding of the computational efficiency landscape and affirming the advantages of MetaFormer over baselines in efficiency.\n> | Method | GFLOPs | 5w1s Acc. | 5w1s Infer. GPU Memory | 5w1s Infer. Speed [ms] | 5w5s Acc. | 5w5s Infer. GPU Memory | 5w5s Infer. Speed [ms] |\n> | --- | --- | --- | --- | --- | --- | --- | --- |\n> | FewTURE  | 5.01 | $68.02 \\pm 0.88$ | 3304M | $77.35 \\pm 0.47$ | $84.51 \\pm 0.53$ | 6482M | $111.22 \\pm 1.27$ |\n> | SMKD-Prototype | 12.58 | $74.28 \\pm 0.18$ | 4288M | $137.58 \\pm 0.66$ | $88.82 \\pm 0.09$ | 4666M | $171.37 \\pm 0.78$ |\n> | MetaFormer-naive | 602.40 | N/A | 20.85G | $417.05 \\pm 0.51$ | N/A | 30.86G | $659.94 \\pm 1.05$ |\n> | MetaFormer-I | 4.88 | $75.78 \\pm 0.71$ | 3661M | $67.65 \\pm 0.78$ | $90.02 \\pm 0.44$ | 5887M | $105.72 \\pm 1.06$ |\n>\n> | SAM | TAM | Add. Params. | GFLOPs | 5w1s Acc. | 5w1s Infer. GPU Memory | 5w1s Infer. Speed [ms] | 5w5s Acc. |5w5s Infer. GPU Memory | 5w5s Infer. Speed [ms] |\n> | --- | --- | --- | --- | --- | --- |--- |--- | --- |--- |\n> | $\\checkmark$ | $\\checkmark$ | $+3.57 \\mathrm{M}$ | 4.88 | $75.78 \\pm 0.71$ | 3661M | $67.65 \\pm 0.78$ | $90.02 \\pm 0.44$ | 5887M | $105.72 \\pm 1.06$ |\n> | $\\checkmark$ | $\\times$     | $+2.01 \\mathrm{M}$ | 4.77 | $74.64 \\pm 0.76$ | 3602M | $67.56 \\pm 0.81$ | $87.53 \\pm 0.47$ | 5878M | $103.93 \\pm 0.84$ |\n> | $\\times$ | $\\checkmark$     | $+1.56 \\mathrm{M}$ | 4.68 | $73.63 \\pm 0.75$ | 3590M | $54.51 \\pm 0.49$ | $87.76 \\pm 0.52$ | 5868M | $88.07 \\pm 0.79$ |"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9345/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700529626832,
                "cdate": 1700529626832,
                "tmdate": 1700529626832,
                "mdate": 1700529626832,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "vhblFOWvQm",
                "forum": "RpKA1wqgk0",
                "replyto": "sWQVfdwBjg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9345/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9345/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We would love to hear back from Reviewer xYfp"
                    },
                    "comment": {
                        "value": "Hi Reviewer xYfp,\n\nWe would like to follow up to see if our response addresses your concerns or if you have any further questions. We would really appreciate the opportunity to discuss this further if our response has not already addressed your concerns.\n\nThank you again!"
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9345/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633898005,
                "cdate": 1700633898005,
                "tmdate": 1700633898005,
                "mdate": 1700633898005,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7ZwERzHyKC",
            "forum": "RpKA1wqgk0",
            "replyto": "RpKA1wqgk0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9345/Reviewer_5apc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9345/Reviewer_5apc"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors propose a ViT-based few-shot learning framework namely MetaFormer. Starting from vanilla ViT, the authors first propose Sample-leel Attention Module to reduce the computation cost and better intra-task interaction, and then propose Task-level Attention Module to enhance inter-task interactions for better feature representation. MetaFormer achieves promising performance on various few-shot learning benchmarks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The motivation of \"decoupling space-level and sample-level attention\" is intuitive. Since this design can effectively reduce the computation cost meanwhile reduce sufficient unrelated attention values to ensure the quality of output features. \n\n2. The performance of MetaFormer is promising. And the visualization results demonstrate the effectiveness of proposed methods. \n\n3. This paper is well written and easy to reproduce."
                },
                "weaknesses": {
                    "value": "1. Some critical ablation studies are lacked. e.g., the nubmer of task probe vectors (why choosing 1 for ViTs) and the size of knowledge pool (which number is better). Besides, for Table 3, the authors could introduce more variants (e.g., vanilla ViT with more layers) to support that: the performance improvement is from SAM / TAM but not more parameters. \n\n2. Though the authors claim that using proposed holistic attention mechanism can significantly reduce the computation complexity, the authors still need to provide essential FLOPS / latency statistic to support the merit. For example, reporting baseline method and MetaFormer under 5-way 5-shot setting. \n\n3. More questions regrading the details of the paper, please see Question section for detail."
                },
                "questions": {
                    "value": "1. In line 1 of page 5, the authors claim that the complexity of O((NK + M)^2 + L^2). Nevertheless, both NK+M and L cannot be omitted in the decoupled attention, therefore it mighte be O(L(NK+M)^2 + (NK+M)L^2). The authors could recheck the complexity and ensure the correctness of the manuscript. \n\n2. As shown in Eqn. 5, the tokens in knowledge pool are updated by direct addition without averaging. The authors could discuss the performance between with and without averaging during pool consolidation."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9345/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9345/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9345/Reviewer_5apc"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9345/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698605881015,
            "cdate": 1698605881015,
            "tmdate": 1699637175609,
            "mdate": 1699637175609,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "x6uiAziHJv",
                "forum": "RpKA1wqgk0",
                "replyto": "7ZwERzHyKC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9345/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9345/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5apc (Part 1)"
                    },
                    "comment": {
                        "value": "Thank you sincerely for your thoughtful feedback on our work. Below, we have provided a detailed explanation for your concerns as follows. Please do not hesitate to let us know if you have any further questions.\n\n#### Q1. Ablation studies to support that: the performance improvement is from SAM / TAM but not more parameters\n> We have thoroughly examined the factors contributing to our performance improvement, concluding that the improvement is attributed to SAM/TAM rather than a mere expansion of parameters. This assessment is articulated through two primary perspectives.\n> - Comparison with other baselines with equivalent or more parameters: In the updated Tables 1 and 2, we observe that FewTURE/HCTransformers, despite possessing a larger number of parameters, markedly lags behind the proposed MetaFormer.\n>\n> - Ablation studies\n>   - We conduct the analysis by comparing with an ablated version, achieved by **naively augmenting the number of layers in ViT-Small to make it comparable with the proposed MetaFormer**. The results presented in the following table and in Appendix F Table 7 substantiate that merely increasing parameters cannot fully address the challenges inherent in few-shot learning. In fact, such augmentation may even elevate the risk of overfitting.\n>      \n>     | Method                           | Backbone             | Backbone Params  | Total Params    | 1-shot           | 5-shot            |\n>     | -------------------------------- | -------------------- | ---------------- | --------------- | ---------------- | ----------------- |\n>     | vanilla ViT with more layers     | ViT-Small            | $21\\mathrm{M}$   | $25.2 \\mathrm{M}$ | $69.75 \\pm 0.71$ | $84.12 \\pm 0.56$  |\n>     | MetaFormer-I (Ours)              | ViT-Small            | $21\\mathrm{M}$   | $24.5 \\mathrm{M}$ | $75.78 \\pm 0.71$ | $90.02 \\pm 0.44$  |\n>     | MetaFormer-A (Ours)              | ViT-Small            | $21\\mathrm{M}$   | $24.5 \\mathrm{M}$ | $79.41 \\pm 0.73$ | $91.21 \\pm 0.44$  |\n>   - In **Table 3**, we present an ablation study where we remove either TAM or SAM, both of which yield a noticable performance drop. This underscores the indispensable contributions of both TAM and SAM to the overall effectiveness of our model.\n>   - In **Appendix D Table 5c**, We provide additional evidence of the effectiveness of the proposed SAM, by maintaining the number of parameters and only varying the masks.  The ablated masks of within-support and support-query (see Appendix D Fig 6 for details) manifest sub-optimal performance, further validating that SAM with the inductive mask works not because of the introduction of extra parameters.\n>   - We have also followed the reviewer's suggestion by investigating the impact of increasing the number of task probe vectors and pool size.  The results, as illustrated in the tables below, indicate that (1) our model is **not sensitive to the number of task probe vectors**, and (2) **a sufficiently diverse but compact knowledge pool** (rather than the largest pool) leads to improvements in performance.\n>\n>|  The number of probe vectors | miniImageNet 5-way 1-shot |\n>    | ------------- | ------------------------- |\n>    | 1  |    $75.78 \\pm 0.71$       |\n>    | 4  |    $75.28 \\pm 0.73$       |\n>    | 8  |    $75.44 \\pm 0.72$       |\n>    | 16  |   $75.58 \\pm 0.72$       |\n>\n>    | The size of knowledge pool |tieredImageNet 5-way 5-shot |\n>    | -------------  |------------------------- |\n>    | 10  | $90.40 \\pm 0.49$       |\n>    | 50  | $91.44 \\pm 0.53$       |\n>    | 100 | $89.92 \\pm 0.54$       |"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9345/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528405614,
                "cdate": 1700528405614,
                "tmdate": 1700528405614,
                "mdate": 1700528405614,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pEtWEwFpss",
                "forum": "RpKA1wqgk0",
                "replyto": "7ZwERzHyKC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9345/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9345/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5apc (Part 2)"
                    },
                    "comment": {
                        "value": "#### Q2. Essential FLOPS / latency statistic to support the merit of reducing computational complexity. For example, reporting baseline method and MetaFormer under 5-way 5-shot setting.\n> We appreciate the reviewer's invaluable feedback, and delve into a meticulous comparative analysis of computational efficiency. \n> - This assessment encompasses (1) MetaFormer, (2) a naive implementation of sample-to-sample interactions without decoupling the spatial attention and sample attention, denoted as MetaFormer-naive, and (3) two other state-of-the-art methods.\n> - We conduct the evaluation on an NVIDIA RTX A6000 GPU, wherein we report performance metrics, including **inference-time GFLOPs and latency**, specifically for 5-way 1-shot and 5-way 5-shot scenarios on the miniImageNet dataset.\n> - The table below distinctly signifies that \n>   - MetaFormer significantly **reduces computational complexity in stark contrast to MetaFormer-naive**, the naive implementation of sample-to-sample interactions.\n>   - Compared to other baselines, MetaFormer exhibits  not only **higher accuracies** attributable to comprehensive sample-to-sample interactions, but also **remarkably superior computational efficiency**.\n>   \n> We have incorporated this detailed comparative analysis into Appendix E Table 6 in the revised manuscript, providing a robust understanding of the computational efficiency landscape and affirming the advantages of MetaFormer over baselines in efficiency.\n>\n>| Method | GFLOPs | 5-way 1-shot Acc. | 5-way 1-shot Infer. Speed [ms] | 5-way 5-shot Acc. | 5-way 5-shot Infer. Speed [ms] |\n>| --- | --- | --- | --- | --- | --- |\n>| FewTURE | 5.01 | $68.02 \\pm 0.88$ | $77.35 \\pm 0.47$ | $84.51 \\pm 0.53$ | $111.22 \\pm 1.27$ |\n>| SMKD-Prototype | 12.58 | $74.28 \\pm 0.18$ | $137.58 \\pm 0.66$ | $88.82 \\pm 0.09$ | $171.37 \\pm 0.78$ |\n>| MetaFormer-naive | 602.40 | N/A | $417.05 \\pm 0.51$ | N/A | $659.94 \\pm 1.05$ |\n>| MetaFormer-I | 4.88 | $75.78 \\pm 0.71$ | $67.65 \\pm 0.78$ | $90.02 \\pm 0.44$ | $105.72 \\pm 1.06$ |\n\n#### Q3. The complexity should be O(L(NK+M)^2 + (NK+M)L^2).\n> We appreciate the reviewer for pointing out this typo. We have revisited and corrected the complexity calculation in the manuscript based on your suggestion.\n\n#### Q4. The performance between with and without averaging during pool consolidation in Eqn. 5\n> We conducted supplementary experiments to explore the impact of \"averaging\". As illustrated in the table below, the **performance difference between consolidation with averaging and without averaging is relatively marginal**. This can be elucidated by the nature of the cosine similarity-based score function employed during knowledge retrieval, which remains not influenced by the magnitude of the stored vectors in the pool. To this end, we adopt the addition without averaging.\n> \n>| Method          | miniImageNet 5-way 1-shot |\n>| --------------- | ------------------------- |\n>| with averaging    |        $75.78 \\pm 0.71$   |\n>| without averaging |        $75.57 \\pm 0.72$   |"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9345/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528496221,
                "cdate": 1700528496221,
                "tmdate": 1700528496221,
                "mdate": 1700528496221,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Y9mzUaUVYr",
                "forum": "RpKA1wqgk0",
                "replyto": "7ZwERzHyKC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9345/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9345/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We would love to hear back from Reviewer 5apc"
                    },
                    "comment": {
                        "value": "Hi Reviewer 5apc,\n\nWe would like to follow up to see if our response addresses your concerns or if you have any further questions. We would really appreciate the opportunity to discuss this further if our response has not already addressed your concerns.\n\nThank you again!"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9345/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633861094,
                "cdate": 1700633861094,
                "tmdate": 1700633861094,
                "mdate": 1700633861094,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8Tn06lJB6l",
            "forum": "RpKA1wqgk0",
            "replyto": "RpKA1wqgk0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission9345/Reviewer_kzi2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission9345/Reviewer_kzi2"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces MetaFormer, a ViT-based framework, designed to excel in the domain of few-shot image classification. It splits attention mechanisms into two key phases: intra-task and inter-task interactions. Intra-task interactions are handled by the Sample-level Attention Module (SAM), which models sample relationships within tasks. For inter-task interactions, the Task-level Attention Module (TAM) is introduced to learn task-specific probe vectors and retrieve relevant semantic features from previous tasks, building a dynamic knowledge pool. MetaFormer demonstrates good performance across a wide range of benchmarks, including those related to few-shot learning and cross-domain tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1)  The concept behind the proposed Metaformer is very simple and straightforward.\n2)  The proposed Metaformer delivers superior quantitative results on extensive few-shot learning benchmarks."
                },
                "weaknesses": {
                    "value": "1) Fundamentally, the technical contributions concerning sample-level attention and task-level attention presented in this work are not groundbreaking. For instance, the approach of decoupling attention (as detailed in section 3.2) to alleviate computational complexity is a well-established practice, particularly within the domain of video transformers.\n2) A more thorough examination of related research is warranted. For example, it would be insightful to delve into the distinctions between the inter-task attention module in [1] and the proposed TAM module, even if [1] is rooted in the continual learning community.\n3) It would enhance the clarity of Tables 1 and 2 to incorporate columns displaying the number of parameters for each backbone model, as opposed to segregating this information in the ablation study section. Such an adjustment would facilitate a more straightforward assessment of whether the observed improvements in numerical performance can be attributed to an augmented parameter count.\n\n[1] Continual learning with lifelong vision transformer, CVPR 2022"
                },
                "questions": {
                    "value": "It would be beneficial to incorporate more in-depth discussions concerning prior research in the realm of meta-learning that incorporates vision transformers as their foundational architecture. For instance, when the authors highlight that \"the majority of existing methods are specially tailored for CNNs and thus fail to translate effectively to vision transformers,\" it would be valuable to provide a more comprehensive explanation of the limitations of existing approaches when they are applied in conjunction with transformers."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission9345/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9345/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission9345/Reviewer_kzi2"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission9345/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698733594412,
            "cdate": 1698733594412,
            "tmdate": 1699637175497,
            "mdate": 1699637175497,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "PnjiZDl9J2",
                "forum": "RpKA1wqgk0",
                "replyto": "8Tn06lJB6l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9345/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9345/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kzi2 (Part 1)"
                    },
                    "comment": {
                        "value": "We sincerely thank the reviewer for providing valuable feedback. We detail our response below point by point. Some experimental results have been updated in the revised paper, and any modifications made to the paper are highlighted in red for your convenience. Please kindly let us know whether you have any further concerns.\n\n#### Q1. Technical contributions concerning sample-level attention and task-level attention \n\n> - We acknowledge the reviewer's scrutiny of our technical contributions and would like to emphasize that our work indeed introduces novel and meaningful innovations to the field of few-shot learning.\n>   - The decoupling of temporal and spatial attention has indeed been explored in video transformers [1]. However, it is crucial to highlight that our consideration of the sample-to-sample relationship in few-shot learning presents a unique challenge distinct from the frame-to-frame relationship in videos, i.e., query samples have to be differentiated from support ones. Our introduction of **sample causal masks** serves an effective solution to address the challenge.\n>      - Notably, a mere adjustment in the design of these sample causal masks allows our method to **flexibly accommodate both inductive and autoregressive inference**. \n>      - We substantiate the effectiveness of the proposed masks in **Appendix D Table 5c**, concluding that a lack of effective constraints between support and query samples (see Appendix D Fig 6 for details of the ablated masks of within-support and support-query) results in sub-optimal performance. \n>   - The proposed task attention module (TAM) with **novel knowledge encoding and consolidation mechanisms** also contributes to few-shot learning within the context of vision transformers. Detailed insights into TAM's contributions are provided in response to Q2. \n> - While we acknowledge that our proposed technique may not be characterized as groundbreaking in a broader sense, it is important to note its wide impact to the domain of few-shot learning. In particular:\n>   - The matching of local structural patterns between samples has long proven effective in few-shot learning [2, 3], though preceding endeavors relying on CNNs such as DeepEMD [2] are very slow. For example, DeepEMD demands 8 hours to evaluate 2000 5-way 5-shot episodes) [3]. We first **enable this pattern matching across both samples and tasks within the ideally fitting framework of ViT in a highly efficient manner** (requiring only 3.5 minutes). Notably, the success of ViT plays a pivotal role in explaining our markedly improved performance than prior CNN-based methods (miniImageNet 5-way 1-shot: Ours: 75.78% / DeepEMD: 65.91%).\n>   - Our decoupling of spatial and sample attention **makes the proposed method seamlessly compatible with recent state-of-the-art pre-trained vision transformers**, as mentioned in the Introduction, further enhancing their few-shot learning performances. \n>     - Our current experiments have already demonstrated the compatibility and effectiveness when working with the pre-trained model of **DINO**;\n>     - During the response period, we have also applied our approach to **CLIP**, achieving remarkable success by outperforming the SOTA method by 16.82%. Please find details to the response to Q1 of Reviewer xYfp. \n>\n> In conclusion, we appreciate the reviewer's critical evaluation of our work, and we hope that these clarifications underscore the significance of our technical contributions to the field of few-shot learning. \n>   \n>\n>[1] Bertasius, Gedas, Heng Wang, and Lorenzo Torresani. \u201cIs space-time attention all you need for video understanding?.\u201d ICML. Vol. 2. No. 3. 2021.\n>\n>[2] Zhang, Chi, Yujun Cai, Guosheng Lin, and Chunhua Shen. \"Deepemd: Few-shot image classification with differentiable earth mover's distance and structured classifiers.\" CVPR, pp. 12203-12213. 2020. \n>\n>[3] Kang, Dahyun, Heeseung Kwon, Juhong Min, and Minsu Cho. \"Relational embedding for few-shot classification.\" ICCV, pp. 8822-8833. 2021."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9345/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528060093,
                "cdate": 1700528060093,
                "tmdate": 1700528060093,
                "mdate": 1700528060093,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3es5m7bhqn",
                "forum": "RpKA1wqgk0",
                "replyto": "8Tn06lJB6l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9345/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9345/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kzi2 (Part 2)"
                    },
                    "comment": {
                        "value": "#### Q2. Distinctions between the inter-task attention module in [1] and the proposed TAM module, even if [1] is rooted in the continual learning community.\n\n> We appreciate the reviewer's great suggestion, and have incorporated the following discussions into the revised manuscript.\n> \n> - The TAM module proposed in our study distinguishes itself from the inter-task attention (IT-att) in [1] on several crucial fronts.\n>   - **Problem setting**: as acknowledged by the reviewer, TAM is rooted in the domain of *few-shot learning*, where the paramount concern is facilitating *knowledge transfer* between tasks. <u>In contrast</u>, IT-att in [1] is grounded in *continual learning*, where the primary focus lies in mitigating *catastrophic forgetting*.\n>   - **Task embedding**: while both TAM and IT-att [1] seemingly adopt a learnable embedding for each task, TAM utilizes it to represent *the knowledge specific to the current task*.  <u>In contrast</u>, IT-att stores *all past knowledge* in it through regularization-based consolidation mentioned below.\n>   - **Encoding of knowledge from other tasks**: Owing to disparate problem settings, TAM maintains *a knowledge pool that stores an array of task-dependent embeddings*. <u>In contrast</u>, IT-att [1] keeps a record of *a single key and a single bias*. \n>   - **Consolidation mechanism**: Leveraging our knowledge pool, we consolidate the current task probe vector by *averaging it with the most relevant vector in the pool* (refer to Eq.(5)). <u>In contrast</u>, IT-att, which is designed to address forgetting, employs *importance-based regularization* to enforce proximity of the current task embedding and previous one. \n> - Thus, the task interaction in TAM exhibits **greater flexibility and expressiveness**, aligning more closely with the objective of knowledge transfer in few-shot learning. \n>   - We further substantiate this claim through an ablation study, wherein we implement IT-att in our setting. The results, reported below and in Appendix G Table 8, demonstrate that our proposed **TAM consistently outperforms IT-att by approximately 1.3%** in the 5-way 5-shot setting on miniImageNet.\n> \n>      | Method             |   5-way 5-shot on miniImageNet |\n>      | ------------------ |  ------------------------- |\n>      | IT-att   [1]          |        $88.70 \\pm 0.50$     |\n>      | TAM                |        $90.02 \\pm 0.44$     |\n>\n>[1] Wang, et al. \"Continual learning with lifelong vision transformer.\" In CVPR, 2022.\n\n\n\n#### Q3. Clarity of Tables 1 and 2 to incorporate columns displaying the number of parameters for each backbone model\n\n> - We appreciate this great suggestion, and in response, we have incorporated additional columns in Table 1 and 2 to present **the number of parameters (backbone + model-related)** for each method in the revision.\n> - We would like to humbly highlight that the observed improvements in performance are **not solely attributed to an augmented parameter count**, as supported by the following empirical evidence.\n>   - In the updated tables, we observe that FewTURE/HCTransformers, despite possessing a larger number of parameters, markedly lags behind the proposed MetaFormer.\n>   - We also conduct a comparative analysis with an ablated version, achieved by naively augmenting the number of layers in ViT-Small to make it comparable with the proposed MetaFormer. The results presented in the following table substantiate that merely increasing parameters cannot fully address the challenges inherent in few-shot learning. In fact, such augmentation may even elevate the risk of overfitting.\n>\n>   \n> | Method                           | Backbone             | Backbone Params  | Total Params    | 1-shot           | 5-shot            |\n>| -------------------------------- | -------------------- | ---------------- | --------------- | ---------------- | ----------------- |\n>| vanilla ViT with more layers     | ViT-Small            | $21\\mathrm{M}$   | $25.2 \\mathrm{M}$ | $69.75 \\pm 0.71$ | $84.12 \\pm 0.56$  |\n>| MetaFormer-I (Ours)              | ViT-Small            | $21\\mathrm{M}$   | $24.5 \\mathrm{M}$ | $75.78 \\pm 0.71$ | $90.02 \\pm 0.44$  |\n>| MetaFormer-A (Ours)              | ViT-Small            | $21\\mathrm{M}$   | $24.5 \\mathrm{M}$ | $79.41 \\pm 0.73$ | $91.21 \\pm 0.44$  |\n>\n>[1] Hiller, et al. \"Rethinking generalization in few-shot classification.\" In NeurIPS, 2022.\n>\n>[2] Lin, et al. \"Supervised masked knowledge distillation for few-shot transformers.\" In CVPR, 2023"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9345/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528114986,
                "cdate": 1700528114986,
                "tmdate": 1700528114986,
                "mdate": 1700528114986,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WLXrUr9JxF",
                "forum": "RpKA1wqgk0",
                "replyto": "8Tn06lJB6l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9345/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9345/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer kzi2 (Part 3)"
                    },
                    "comment": {
                        "value": "#### Q4. More in-depth discussions concerning prior research in the realm of meta-learning that incorporates vision transformers as their foundation architectures.\n\n> - Our claim that \"existing meta-learning methods are specially tailored for CNNs and thus fail to translate effectively to vision transformers\" is grounded in our empirical observations during the development of our proposed framework and supported by the findings in [5].\n>   - Initially, we had planned to **adapt FiLM**, a technique commonly employed in CNN-based meta-learning for task adaptation through conditioned batch normalization [2, 3], into **layer normalization layers of ViT for task conditioning**. Unfortunately, our experiments reveal **a performance drop** when ViT was applied with FiLM, as shown in the following table. \n>   - The work of **[5] also showcases the inferiority of FiLM when naively applied to ViT**, as compared to their proposed task conditioning method tailored specifically for ViT (which involves only conditioning the attention block with a bias), particularly on large-scale Meta-Dataset.\n>   - Such failures are attributed to the substantial differences between the two backbones [6].\n>     \n> - We posit that the challenge of architectural inconsistency partially accounts for the **limited research in the realm of meta-learning grounded on ViT**. Another key challenge is the increased parameter requirement of ViT. FewTURE, as expounded in the Related Work section, is the pioneering work that tailors to ViT via inner-loop token importance reweighting, and addresses the second challenge via pre-training with DINO on the meta-training dataset. Our approach, empowering sample-to-sample and task-to-task interaction, further improves the accuracy substantially.\n> - To avoid misunderstanding, we have corrected the original claim to and incorporated these discussions into our supplemental material.\n> \n> | Method       | Backbone  | miniImageNet 5-way 1-shot |\n>| ------------ | --------- | ------------------------- |\n>| Vanilla ViT  | ViT-Small | $69.03 \\pm 0.71$          |\n>| ViT + FiLM   | ViT-Small | $58.75 \\pm 0.73$          |\n>| MetaFormer-I | ViT-Small | $75.78 \\pm 0.71$          |\n>| MetaFormer-A | ViT-Small | $79.41 \\pm 0.73$          |\n>\n> [1] Han-Jia, et al. \"Few-shot learning via embedding adaptation with set-to-set functions.\" In CVPR, 2020.\n>\n> [2] Requeima, et al. \u201cFast and \ufb02exible multi-task classi\ufb01cation using conditional neural adaptive processes.\u201d In NeurIPS, 2019.\n>\n> [3] Oreshkin, et al. \u201cTADAM: task dependent adaptive metric for improved few-shot learning.\u201d In NeurIPS, 2018\n>\n> [4] Hiller, et al. \"Rethinking generalization in few-shot classification.\" In NeurIPS, 2022.\n>\n> [5] Xu, et al. \u201cExploring E\ufb03cient Few-shot Adaptation for Vision Transformers.\u201d In TMLR, 2023\n>\n> [6] Raghu, Maithra, et al. \"Do vision transformers see like convolutional neural networks?.\" In NeurIPS, 2021."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9345/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700528141241,
                "cdate": 1700528141241,
                "tmdate": 1700528141241,
                "mdate": 1700528141241,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "xUWddQyiDh",
                "forum": "RpKA1wqgk0",
                "replyto": "8Tn06lJB6l",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission9345/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission9345/Authors"
                ],
                "content": {
                    "title": {
                        "value": "We would love to hear back from Reviewer kzi2"
                    },
                    "comment": {
                        "value": "Hi Reviewer kzi2,\n\nWe would like to follow up to see if our response addresses your concerns or if you have any further questions. We would really appreciate the opportunity to discuss this further if our response has not already addressed your concerns.\n\nThank you again!"
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission9345/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633763606,
                "cdate": 1700633763606,
                "tmdate": 1700633763606,
                "mdate": 1700633763606,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]