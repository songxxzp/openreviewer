[
    {
        "title": "Out-Of-Distribution Detection With Smooth Training"
    },
    {
        "review": {
            "id": "WdcM4fVWTa",
            "forum": "iqAbdT35hE",
            "replyto": "iqAbdT35hE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1837/Reviewer_c4yF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1837/Reviewer_c4yF"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the issue of label smoothing in OOD detection. It first identifies the cause of overconfidence prediction is cross-entropy loss in neural networks. It then proposes a new training scheme of label smoothing, SMOT, for perturbed inputs. SMOT proposes to train models on the confidence of different areas of mask-out regions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. It applied label smoothing beyond training data where it demonstrated labeling smoothing on ID data is not enough.\n2. The proposed training method without using auxiliary OOD dataset, but perturbation of masking is very promising where OOD auxiliary dataset is not available.  \n3. The supplementary sections justify many decisions made in the main paper, such as why masking is chosen. The paper is generally complete and clear."
                },
                "weaknesses": {
                    "value": "The discussions/conclusions from Theorem 1 and 2 are too abrupt and not very obvious, such as the paragraph just before the Sec.3.2. More explanations are needed, especially for Theorem 2. It makes it less self-contained."
                },
                "questions": {
                    "value": "1. What are the intuitions/interpretations of $\\sqrt{\\frac{C}{n}}$ in Equ(4), and the equations of equations of theorem 1 and 2? The connection between the equations and the implications is not clear. \n2. Can SMOT use the updated model trained on the fly to get the mask for perturbation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1837/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1837/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1837/Reviewer_c4yF"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1837/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697692720404,
            "cdate": 1697692720404,
            "tmdate": 1699636113689,
            "mdate": 1699636113689,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xGYQAyP16u",
                "forum": "iqAbdT35hE",
                "replyto": "WdcM4fVWTa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1837/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1837/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">**Q1:** What are the intuitions/interpretations of $\\sqrt(C/n)$ in Equ(4), and the equations of equations of theorem 1 and 2? The connection between the equations and the implications is not clear.\n\n**A1:** We apologize for the confusion. $n$ is the number of training samples and $C$ is a uniform constant. When the number of the training samples is large enough, this term tends to 0 such that the risk of empirical predictor $\\mathbf{f}_{\\boldsymbol{\\theta}_S}$  can approximate to the optimal risk. \nTheorem 1 states that under the right conditions, the model is likely to be overconfident in the ID data. Theorem 2 states that when the model is overconfident in the ID data, it will also be overconfident in the OOD data which have a small distribution discrepancy with ID data. We'll go into more detail in the revised version.\n\n>**Q2:** Can SMOT use the updated model trained on the fly to get the mask for perturbation?\n\n**A2:** Thanks for your kind suggestion, we perform the experiment you described. We still train 300 epochs, in the first 100 epochs we perform the standard training and then we use the model in training to obtain the CAM to perform the SMOT training. The results are as follows:\n|                         | Avg FPR95 | Avg AUROC |\n|-------------------------|-----------|-----------|\n| msp                     | 51.13     | 91.27     |\n| SMOT trained on the fly | 25.05     | 95.44     |\n\nIt is shown what you described works well! Thanks again for your insightful comments. We will add these results and discussions to our revision."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1837/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737039472,
                "cdate": 1700737039472,
                "tmdate": 1700737039472,
                "mdate": 1700737039472,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Z2K6rY6ZFG",
            "forum": "iqAbdT35hE",
            "replyto": "iqAbdT35hE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1837/Reviewer_m2VV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1837/Reviewer_m2VV"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes SMOT, a smooth training algorithm for OOD detection. SMOT is based on the heuristic that masking out certain features from the input image should correspondingly leads to decrease in the network's prediction confidence. Specifically, SMOT leverages CAM together with random-thresholding to determine the masking region, and the soft label (or essentially prediction confidence encoded in the training target) is determined according to the threshold (which is related to the area of the masking region if I understand correctly). Experiments on CIFAR-10/100 and ImageNet-200 show that SMOT exhibits (moderate) performance improvements over certain existing methods."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The manuscript is in general clearly written."
                },
                "weaknesses": {
                    "value": "### Theoretical Investigation\n\nI find that Sec. 3.1 is somewhat hard to follow. The message / motivation it tries to convey is unclear to me. See specific comments or questions below.\n\n1. The conclusion of Theorem 1 is \"given a sufficient amount of training data and a small optimal risk, ..., the issue of over-confidence for ID data is highly probable to arise\". However, the equation is only related to \"over-confidence\" (which I assume refers to excessive maximum softmax probability according to Eq. 2) when the loss is exactly cross-entropy loss. If we use label smoothing as the loss (although it is later empirically shown not to work), then there won't be over-confidence by looking at Eq. 4.\n\n2. My same argument could be applied to Theorem 2 as well. Furthermore, I can't see how exactly the \"over-confidence in OOD data\" is reflected in Theorem 2. More elaboration and clarification is necessary.\n\n3. The concluding paragraph under Theorem 2 makes me lost again. Why we want to \"access real OOD data to reduce the distribution discrepancy during training\"? What does it mean to \"reduce the distribution discrepancy\" (the $d(\\theta)$ in Theorem 2?) between ID and OOD? Meanwhile, why suddenly \"limited training ID data\", \"overfitting\", and \"the failure of ID classification\" become issues for OOD detection?\n\n4. Lastly, where are the proofs of the Theorems (or where are the references if they were proved by existing works)?\n\n\n\n### Design of SMOT\n\n1. Eq. 9 seems a little arbitrary. Why using a temperature-scaled exponential function? What's the intuition behind it? Why (t - 255)? What is the value range of t?\n\n### Experiments\n\n1. One limitation of the experiments and presented results is the fact that all considered OOD datasets are far-OOD which are easier to be detected. I expect to see more results on near-OOD splits (e.g., CIFAR-100 or Tiny ImageNet for CIFAR-10, SSB or NINCO for ImageNet), which are more likely to translate to real-world where the OOD images can be extremely similar to ID images.\n\n2. The baseline selection seems a bit arbitrary. How does SMOT compare with recent top-performing methods (e.g., ASH [1] as identified by OpenOOD [2])? Also, a highly relevant baseline is missing (see below \"Related Work\" for details).\n\n3. Why the training budget and learning rate scheduler is different between base models and SMOT models? Specifically, base models are trained for 200 epochs, while the \"final model\" with the proposed SMOT loss is trained for 300 epochs). Meanwhile, the base model adopts a step-wise learning rate decay schedule, while the final model uses the more advanced cosine decay. Is this a fair comparison, especially given that both longer training and sophisticated scheduler exactly benefit OOD detection (Table 5 in [3])?\n\n4. Lastly, an important ablation study that I believe should be included is how SMOT compares with random masking / cropping. This would better justify SMOT's design of leveraging CAM to determine the masking region.\n\n### Related Work\nSec. 5 should be more thorough and informative. Specifically, notice that the general idea of using corrupted / perturbed images associated with soft labels has been explored in at least two works in the field of OOD detection [4, 5]. Among these, [4] is in particular relevant to this work. I put up a table below making high-level comparison between [4] and this work.\n\n|      | soft target |  perturbation  | needs a pre-trained model? | \n|------|-------------|---|------|\n| [4]  |  $y_\\epsilon=(1-\\epsilon)\\cdot y + \\epsilon / K \\cdot u$ (see their Eq. 3)  |   image corruptions defined by ImageNet-C   | a pre-trained classifier for determining $\\epsilon$ |\n| SMOT |  $y_\\epsilon=(1-\\epsilon)\\cdot y + \\epsilon / K \\cdot u$ (Eq. 5 in this work)   |  masking  | a pre-trained classifier for generating CAM mask |\n\nFrom the above table, it is not obvious what advantages SMOT can offer over [4] (e.g., less compute, not requiring a pre-trained model). Therefore, I believe that [4] should be not only referenced but also included as an actual baseline to show that CAM-based masking and the associated method for assigning $\\epsilon$ value are better than the designs of [4].\n\n### Format\nThe references are inserted absurdly (e.g. \"ResNet18 He et al. (2016)\") which I believe are not in the most appropriate format. There are also some mis-formatting, e.g. \"Eq.equation 8\" in \"Training details\".\n\n------------------------------------------------------\n[1]  Extremely simple activation shaping for out-of-distribution detection\n\n[2] OpenOOD v1.5: Enhanced Benchmark for Out-of-Distribution Detection \n\n[3] Open-Set Recognition: A Good Closed-Set Classifier Is All You Need\n\n[4] Bridging In- and Out-of-distribution Samples for Their Better Discriminability\n\n[5] Mixture Outlier Exposure: Towards Out-of-Distribution Detection in Fine-grained Environments"
                },
                "questions": {
                    "value": "Please see the questions in Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1837/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1837/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1837/Reviewer_m2VV"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1837/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698451930129,
            "cdate": 1698451930129,
            "tmdate": 1700755715412,
            "mdate": 1700755715412,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "OImfCBmIeB",
                "forum": "iqAbdT35hE",
                "replyto": "Z2K6rY6ZFG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1837/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1837/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank you for your constructive comments! Please find our responses below.\n\n>**Q1:** The conclusion of Theorem 1 is \"given a sufficient amount of training data and a small optimal risk, ..., the issue of over-confidence for ID data is highly probable to arise\". However, the equation is only related to \"over-confidence\" (which I assume refers to excessive maximum softmax probability according to Eq. 2) when the loss is exactly cross-entropy loss. If we use label smoothing as the loss (although it is later empirically shown not to work), then there won't be over-confidence by looking at Eq. 4.\n\n**A1:** Thanks for your valuable comments.\nTheorem 1 states that networks trained on ERM principle are likely to be overconfident in ID data. We do not discuss the vanilla label smoothing here. We agree that label smoothing can alleviate the problem of overconfidence in neural networks. However, this is more of a conclusion drawn from experimental results. It is hard to follow the your conclusion \"*If we use label smoothing as the loss, then there won't be over-confidence by looking at Eq. 4*\". In fact, label smoothing is not widely used in OOD detection. Our experimental results also demonstrate that vanilla label smoothing does not improve the model's OOD detection performance. We intuitively believe that this may be due to the fact that label smoothing is applied to raw ID samples, which is contrary to the goal of OOD detection. \n\n>**Q2:** I can't see how exactly the \"over-confidence in OOD data\" is reflected in Theorem 2. More elaboration and clarification is necessary.\n\n**A2:** We apologize for the misunderstanding.\nTheorem 2 states that for any OOD sample $x$, the upper bound of the risk that the well-trained model \n $\\mathbf{f}_{\\boldsymbol{\\theta}_S}$ misassigns it to the label space of the ID samples is the right-hand term of the inequality in Theorem 2 (I'm sorry, but openreview doesn't seem to be able to compile that equation).\n\n>**Q3:** The concluding paragraph under Theorem 2 makes me lost again. Why we want to \"access real OOD data to reduce the distribution discrepancy during training\"? What does it mean to \"reduce the distribution discrepancy\" (the $d(\\theta)$ in Theorem 2?) between ID and OOD? Meanwhile, why suddenly \"limited training ID data\", \"overfitting\", and \"the failure of ID classification\" become issues for OOD detection?\n\n**A3:** Thanks for your insightful comments.\n\nIn the OE approach, we are allowed to use surrogate OOD data to regularize the model, i.e., to allow the model to have low confidence on this data. However, it is clear that the surrogate OOD data has distributional differences from the real OOD data encountered during testing. Here REDUCE the distribution discrepancy refers to reducing the distribution discrepancy between the surrogate OOD data and the real OOD data.\n\n\n>**Q4:** Lastly, where are the proofs of the Theorems (or where are the references if they were proved by existing works)?\n\n**A4:** The proofs are in APPENDIX A. We will highlight it in our revision. Thanks for your kind suggestion.\n\n>**Q5:** Eq. 9 seems a little arbitrary. Why using a temperature-scaled exponential function? What's the intuition behind it? Why (t - 255)? What is the value range of t?\n\n**A5:** Thanks for pointing out this potentially confusing problem. We design the smoothing parameter function based on the following three principles\uff1a\n\n- The smoothing parameter should be a monotonically decreasing function of the masking threshold, because the more regions are masked, the smoother its label should be.\n- When no region is masked, the smoothing parameter should be (or close to) 0.\n- When all regions are masked, the smoothing parameter should be (or close to) 1.\n\nThe designed temperature-scaled exponential function satisfies the above rules and can simply adjust the steepness of the function by one parameter $T$. Of course, other more complex functions or learnable functions can be considered, and this is our future work. $t$ is the masking threshold, ranging from 0 to 255, corresponding to the values in the generated heat map. We set $(t-255)$ because when the masking threshold $t$ is 255, the image will not be masked, and we should use hard label, i.e., $\\epsilon = 0$."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1837/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700737245717,
                "cdate": 1700737245717,
                "tmdate": 1700738325018,
                "mdate": 1700738325018,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "alcwVUJNrB",
            "forum": "iqAbdT35hE",
            "replyto": "iqAbdT35hE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1837/Reviewer_2XDU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1837/Reviewer_2XDU"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes label smoothing training framework for OOD detection. The authors use the CAM to identify the regions that have a strong correlation to the true label, and generate a masked input image and corresponding soft label for smooth training. Extensive experiments show that the smooth training strategy greatly improves the OOD performance with different score functions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The proposed smooth training (SMOT) strategy, where soft labels are applied to the perturbed inputs, is technical sound to relieve overconfidence problem.\n2. The image masking and label smoothing strategy is quite novel and makes sense.\n3. The paper is well structured and in good presentation and writing."
                },
                "weaknesses": {
                    "value": "1. It is a little bit expensive to use CAM for identifying those label-correlated regions. I would like to see the OOD detection performance with the randomly generated masks. For example, randomly masking 30%-70% of the image for smoothing training.\n2. The proposed SMOT utilizes data augmentation for OOD detection. Therefore, the author should introduce and compare more related methods that investigate the effectiveness of data augmentation in OOD detection. I believe there have been many papers that exploring data augmentation for Calibration or OOD detection [1,2]\n3. The SMOT framework is similar to the Outlier Exposure (OE) framework, the author should also compare the proposal with other Outlier exposure (OE) based methods, and discuss the advantages compared with the OE framework.\n\n[1] RankMixup: Ranking-Based Mixup Training for Network Calibration\n[2] OUT-OF-DISTRIBUTION DETECTION WITH IMPLICIT OUTLIER TRANSFORMATION"
                },
                "questions": {
                    "value": "1. What model is used in Table 2/3/4?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1837/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698722464998,
            "cdate": 1698722464998,
            "tmdate": 1699636113528,
            "mdate": 1699636113528,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "tPwpRgKnXf",
                "forum": "iqAbdT35hE",
                "replyto": "alcwVUJNrB",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1837/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1837/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank you for your constructive comments! Please find our responses below.\n>**Q1:** It is a little bit expensive to use CAM for identifying those label-correlated regions. I would like to see the OOD detection performance with the randomly generated masks. For example, randomly masking 30%-70% of the image for smoothing training.\n\n**A1:** Thanks to your kind suggestion. Following your constructive comments, we have added the experiment, with the randomly generated masks. More detailed results and disucssions can be found in **A3** to **Reviewer H2kX**. \n\n>**Q2:** The proposed SMOT utilizes data augmentation for OOD detection. Therefore, the author should introduce and compare more related methods that investigate the effectiveness of data augmentation in OOD detection. I believe there have been many papers that exploring data augmentation for Calibration or OOD detection.\n\n**A2:** Thank you for your kind suggestion. \n\nFor the first paper RankMixup[1] you mentioned, it does have similarities to our work at the top level of thought. It argues that mixed samples should have lower confidence and that the higher the mixing, the lower the confidence. In our work, we think that the less complete the sample, the lower the confidence. We take inspiration from the human perspective and design our algorithm accordingly. We believe that our approach more closely aligns with how humans perceive the world However, since RankMixup is a relatively new piece of work, the authors have not published its code for the time being and we have not compared it with SMOT. As for the other mentioned work DOE[2], it uses a min-max learning scheme-searching to synthesize OOD data that leads to worst judgments and learning from such OOD data for uniform performance in OOD detection. The biggest difference between us and them is that we make a smooth excess between ID data and OOD data. We compare DOE and SMOT on CIFAR10. The result is as follows:\n|      | FPR95 | AUROC |\n|------|-------|-------|\n| DOE  | 5.15  | 98.78 |\n| SMOT | 15.42 | 97.15 |\n\nAs can be seen from the experimental results, SMOT is not as good as DOE. However, this is not a fair comparison. DOE belongs to the Outlier Exposure (OE) framework and requires surrogate OOD data. while SMOT does not require any additional data. We will add a discussion of these outstanding works in the revised version.\n\n>**Q3:** The SMOT framework is similar to the Outlier Exposure (OE) framework, the author should also compare the proposal with other Outlier exposure (OE) based methods, and discuss the advantages compared with the OE framework.\n\n**A3:** We agree with your point. Our approach is indeed similar to OE. Masked samples can be viewed as OOD samples. \n\nWe would like to highlight the differences between ours and OE. Specificaly, we do smoothing between ID samples and OOD samples. The more ID samples are masked, the more it is considered an OOD sample. This makes the transition between ID and OOD smoother. Also, our OOD samples are created by simply masking ID samples without the need for an external dataset. We compared our method with some OE methods with the following results:\n|       | FPR95 | AUROC |\n|-------|-------|-------|\n| OE[3]    | 12.41 | 97.85 |\n| MixOE[4] | 13.55 | 97.59 |\n| SMOT  | 15.42 | 97.15 |\n\n\nAs can be seen from the experimental results, SMOT is not as good as OE and MixOE. We would like to note that it's not a fair comparison. \n\nThanks agin for your constructive comments. We will add the results and discussions to the revised paper.\n\n[1]: RankMixup: Ranking-Based Mixup Training for Network Calibration\n\n[2]: OUT-OF-DISTRIBUTION DETECTION WITH IMPLICIT OUTLIER TRANSFORMATION\n\n[3]:Deep anomaly detection with outlier exposure\n\n[4]:Mixture Outlier Exposure: Towards Out-of-Distribution Detection in Fine-grained Environments"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1837/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736665026,
                "cdate": 1700736665026,
                "tmdate": 1700736665026,
                "mdate": 1700736665026,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "iDIKu0KKmP",
            "forum": "iqAbdT35hE",
            "replyto": "iqAbdT35hE",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1837/Reviewer_H2kX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1837/Reviewer_H2kX"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new training strategy called Smooth Training (SMOT) to improve out-of-distribution (OOD) detection performance. The key idea is to apply label smoothing to perturbed inputs rather than original inputs during training. Specifically, the authors randomly mask label-relevant regions of input images identified by class activation maps. The labels for these masked images are softened proportional to the size of the masked regions. This forces the model to output lower confidence for partial inputs, widening the gap between in- and out-of-distribution examples."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* The proposed smooth training strategy is intuitive and simple to implement, requiring only small modifications to the standard training procedure.\n* Thorough theoretical analysis is provided on how the commonly used cross-entropy loss leads to overconfidence, and how smooth training can mitigate this issue.\n* Comprehensive experiments on CIFAR and ImageNet-200 benchmarks demonstrate SMOT consistently improves OOD detection across different base models, scoring functions, and datasets. Improvements are also shown when fine-tuning CLIP.\n* Ablation studies validate the efficacy of key components like the label smoothing function and masking threshold sampling distribution."
                },
                "weaknesses": {
                    "value": "* Although smooth training enhances Out-Of-Distribution (OOD) detection, there's a minor decrease in in-distribution accuracy compared to conventional training. An in-depth exploration of this trade-off could be beneficial.\n* The authors employ class activation maps to pinpoint label-relevant regions for masking, necessitating a pre-trained model. Studying other perturbation techniques that don't require a pre-trained model could expand the method's applicability.\n* Further analysis could be devoted to how the results are sensitive to variations in hyperparameter settings. For instance, how essential is the use of a CAM heatmap to guide masking? What's the optimal way to establish the relationship between mask size and label smoothing hyperparameter ?"
                },
                "questions": {
                    "value": "see weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1837/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698815460807,
            "cdate": 1698815460807,
            "tmdate": 1699636113453,
            "mdate": 1699636113453,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qWnRQr3iNS",
                "forum": "iqAbdT35hE",
                "replyto": "iDIKu0KKmP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1837/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1837/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We sincerely thank you for your constructive comments! Please find our responses below.\n\n>**Q1:** Although smooth training enhances Out-Of-Distribution (OOD) detection, there's a minor decrease in in-distribution accuracy compared to conventional training. An in-depth exploration of this trade-off could be beneficial.\n\n**A2:** Thanks for your suggestion. \n\nSmooth training does lead to a minor decrease (from 95.12% to 94.54%) in classification accuracy of ID samples. Following your suggestion, we further explore the trade-off between OOD detection performance and ID classification performance on CIFAR-10. We do this by varying the values of the hyperparameters $\\lambda$. The experimental results are as follows:\n|               | Texture |       | SVHN  |       | iSUN  |       | Places |       | LSUN  |       | Average |       |        |\n|---------------|---------|-------|-------|-------|-------|-------|--------|-------|-------|-------|---------|-------|--------|\n|               | FPR95   | AUROC | FPR95 | AUROC | FPR95 | AUROC | FPR95  | AUROC | FPR95 | AUROC | FPR95   | AUROC | ID ACC |\n| w/o smot      | 58.59   | 88.59 | 55.71 | 91.92 | 50.80 | 91.80 | 57.85  | 88.70 | 32.71 | 95.33 | 51.13   | 91.27 | 95.12  |\n| $\\lambda = 0.01$ | 35.93   | 94.25 | 18.92 | 96.82 | 25.74 | 96.01 | 46.27  | 91.26 | 12.38 | 97.96 | 27.78   | 95.26 | 94.72  |\n| $\\lambda = 0.05$ | 27.44   | 95.15 | 12.15 | 97.57 | 14.27 | 97.38 | 31.79  | 93.73 | 2.01  | 99.35 | 17.53   | 96.63 | 94.56  |\n| $\\lambda = 0.1$  | 23.01   | 96.24 | 8.27  | 98.21 | 12.27 | 97.89 | 31.52  | 93.88 | 2.04  | 99.56 | 15.42   | 97.15 | 94.54  |\n| $\\lambda = 0.3$  | 37.92   | 93.91 | 26.92 | 95.81 | 28.84 | 95.67 | 45.06  | 91.79 | 21.16 | 96.81 | 31.98   | 94.79 | 94.22  |\n| $\\lambda = 0.5$  | 26.24   | 95.65 | 21.05 | 96.32 | 19.68 | 96.89 | 43.83  | 91.96 | 14.09 | 97.67 | 25.97   | 95.70 | 94.50  |\n| $\\lambda = 1$    | 31.96   | 94.46 | 27.23 | 95.72 | 17.25 | 97.07 | 42.03  | 91.85 | 8.53  | 98.15 | 25.5    | 95.45 | 94.02  |\n\nThe experimental results show that the ID ACC of models trained with SMOT is lower than that of models trained with normal cross-entropy loss, and that larger $\\lambda$ usually leads to lower ID ACC, but not necessarily to higher AUROC. In practice, we need to choose an appropriate $\\lambda$ so that the model maintains both high ID classification ability and good OOD detection ability.\n\n\n>**Q2:** The authors employ class activation maps to pinpoint label-relevant regions for masking, necessitating a pre-trained model. Studying other perturbation techniques that don't require a pre-trained model could expand the method's applicability.\n\n**A2:** Thank you for your kind suggestion. \n\nIn addition to using masking as perturbation function, we also use adding gaussian noise to inputs as perturbation function, which does not require additional pre-trained models. We report the experimental results in Table 9 in the Appendix:\n|             | Texture |       | SVHN  |       | iSUN  |       | Places |       | LSUN  |       | Average |       |\n|-------------|---------|-------|-------|-------|-------|-------|--------|-------|-------|-------|---------|-------|\n| Method      | FPR95   | AUROC | FPR95 | AUROC | FPR95 | AUROC | FPR95  | AUROC | FPR95 | AUROC | FPR95   | AUROC |\n| MSP         | 58.59   | 88.59 | 55.71 | 91.92 | 50.80 | 91.80 | 57.85  | 88.70 | 32.71 | 95.33 | 51.13   | 91.27 |\n| SMOT(mask)  | 23.01   | 96.24 | 8.27  | 98.21 | 12.27 | 97.89 | 31.52  | 93.88 | 2.04  | 99.56 | 15.42   | 97.15 |\n| SMOT(noise) | 41.40   | 93.05 | 36.78 | 94.64 | 27.91 | 95.87 | 44.64  | 91.40 | 23.40 | 96.77 | 34.68   | 94.35 |\n\nIt is shown that adding noise can also improve the OOD detection performance of the model."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1837/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736167806,
                "cdate": 1700736167806,
                "tmdate": 1700736167806,
                "mdate": 1700736167806,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MVAFNm7jSP",
                "forum": "iqAbdT35hE",
                "replyto": "iDIKu0KKmP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1837/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1837/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">**Q3:** Further analysis could be devoted to how the results are sensitive to variations in hyperparameter settings. For instance, how essential is the use of a CAM heatmap to guide masking? What's the optimal way to establish the relationship between mask size and label smoothing hyperparameter?\n\n**A3:** We apologize that we missed the important experiment of comparing with random masking. \n\nAs suggested by you and other reviewers, we add this experiment on CIFAR10. Since the images in CIFAR10 is of 32\\*32, we divide the image into 64 small 4\\*4 squares, and in each loop, we sample a probability $p$ from the distribution of $Beat(\\alpha, \\beta)$, let each small square be masked with probability $p$, and then smooth the label. The  smoothing parameter is designed as $\\epsilon(p) = (exp(p/T) - 1)/(exp(1/T)-1)$. We conducted experiments with different $\\alpha, \\beta$ and T. Results are as follows(The numbers in the table are the average AUROC/the average FPR95.):\n| $(\\alpha, \\beta)$    | $T=10$        | $T=1$         | $T=0.3$       | $T=0.1$       |\n|-----------|-------------|-------------|-------------|-------------|\n| (1,1)     | 92.91/39.02 | 93.76/35.14 | 93.62/36.78 | 92.08/44.14 |\n| (2,2)     | 91.50/37.48 | 91.27/42.00 | 90.30/41.09 | 91.47/45.07 |\n| (2/3,2/3) | 89.38/44.34 | 90.97/41.33 | 89.31/46.17 | 92.24/41.27 |\n| (5,2)     |91.79/43.34 | 90.95/44.55 | 91.59/46.87 | 91.79/43.34 |\n| (2,5)     | 90.17/37.44 | 89.00/42.54 | 90.12/40.23 | 92.28/42.02 |\n| 50,20)    | 91.23/45.54 | 93.28/36.55 | 85.33/54.66 | 91.83/42.89 |\n| (20.50)   | 89.64/35.99 | 87.47/49.29 | 86.64/45.58 | 92.56/39.57 |\n\nAs can be seen from the experimental results, when CAM is not used to guide masking, it does not work well and in many cases has lower performance than using simple cross-entropy loss. This is because masking randomly does not necessarily result in a change in the label of the image. It doesn't make sense to soften the label when we mask the background\n\nAs for how to establish a relationship between the masking size and label smoothing hyperparameter, this is really a major drawback of SMOT at the moment. In our paper, we simply use a temperature-scaled exponential function with a adjustable temperature. A better approach might be to get the relation between masking size and smoothing hyperparameter from priori knowledge or to learn one such relation. This is our future research direction."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1837/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700736565748,
                "cdate": 1700736565748,
                "tmdate": 1700736565748,
                "mdate": 1700736565748,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]