[
    {
        "title": "Adam through a Second-Order Lens"
    },
    {
        "review": {
            "id": "eIUa5Pn1Ay",
            "forum": "CVldG5ohCy",
            "replyto": "CVldG5ohCy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5299/Reviewer_2iRB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5299/Reviewer_2iRB"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes AdamQLR, which is a modification of the Adam optimizer and tries to adapt some heuristics used in the K-FAC optimizer for Adam, such as Levenberg Marquardt damping (equation (2) in the paper) and learning rate selection (equation 3 in the paper), both based on a truncated second order Taylor expansion of the function computed by the neural network at the current parameters $\\theta_t$ (equation 1 in the paper). The authors perform experiments on 6 tasks (Rosenbrock, UCI Energy/Protein, Fashion-MNIST, SVHN and CIFAR-10) and they compare two versions of their work (Adam QLR Tuned/Untuned) against a few popular optimizers in the literature (SGD Minimal/Full, Adam, K-FAC)."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "Below I enumerate the strengths of the paper:\n1. clearly written and easy to understand\n2. code is provided\n3. optimal hyper-parameters are clearly stated in Table 2\n4. ablation study for Levenberg-Marquardt heuristic"
                },
                "weaknesses": {
                    "value": "The paper lacks novelty and originality because it only combines Adam and K-FAC in a facile way and thus doesn't provide better results for most of the tasks mentioned, such as UCI Energi/Protein, Fashion-MNIST and CIFAR-10 (in these cases, K-FAC and/or original Adam are better than the proposed method because the generalization performance VS runtime is not competitive as stated in the abstract).\n\nThe evaluation was performed on small tasks and I believe that the usage of Rosenbrock function not adding any value to the paper since the tasks that involve Neural Networks are much more complicated. The paper does not have any tables that contains accuracies for classification tasks and it is extremely difficult to figure out what the final accuracies are only by looking at the plots. In the end, it is unfortunate to say that the paper does not meet the novelty and originality requirements for ICLR."
                },
                "questions": {
                    "value": "1. how does AdamQLR behave on NLP tasks?\n2. how do you compute the curvature matrix C that is used to update the learning rate $\\alpha$ and the damping factor $\\lambda$? In the manuscript you state that the overhead is only one additional forward pass, while we all know that computing Hessian-vector-products requires an additional backward pass (which, of course, implies a forward pass in the first place)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5299/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5299/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5299/Reviewer_2iRB"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5299/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698338522441,
            "cdate": 1698338522441,
            "tmdate": 1699636530367,
            "mdate": 1699636530367,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "H7iJ5wTLxm",
                "forum": "CVldG5ohCy",
                "replyto": "eIUa5Pn1Ay",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5299/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5299/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 2iRB"
                    },
                    "comment": {
                        "value": "Thank you for your efforts reviewing our submission \u2013 we appreciate your time.\n\nW1: Since AdamQLR is closely related to its Adam base, we would not expect AdamQLR to display dramatic performance improvements over Adam. The major contribution of our work is that an _Untuned_ version of AdamQLR performs comparably to a _Tuned_ version of Adam. This indicates a significant cost saving which does not appear on our figures: that of performing hyperparameter optimisation. We will update our paper to make this narrative clearer.\n\nWhile we appreciate our combination of Adam and K-FAC may seem \u201cfacile\u201d at first glance, we emphasise our motivating observation that the heuristics used in K-FAC are _essential_ to its ability to perform stable training, rather than providing slight quality improvements. This is a very different relationship from the use of e.g. weight decay in SGD, despite both being \u2018heuristics\u2019. From this perspective, we argue an evaluation of these techniques applied to Adam is of interest and value to the ML community, and extends beyond a trivial augmentation of Adam.\n\nW2: Even though our experiments in Figure 2 aren\u2019t as large-scale as some other applications, we think they strike a balance between being large enough to display features of the neural network training problem, but small enough to easily perform multiple repetitions of the experiments to reinforce our results. The Rosenbrock function is included largely as a proof-of-concept, but we think it\u2019s valuable to visualise the trajectory followed by AdamQLR as compared to our benchmarks.\n\nWe will add tables of final performance figures to the paper, though we note Figure 4 (Appendix B.1.1) plots accuracy evolutions for our experiments on classification tasks.\n\nTo our knowledge, our submission is the first work to apply heuristics from second-order optimisation to a gradient-based method such as Adam. It addresses the relevant question of how these heuristics might aid first-order algorithms, especially considering their fundamental importance to second-order techniques. Further, it describes how the tuned performance of a widely-used optimiser (Adam) can be replicated with AdamQLR without the cost of hyperparameter optimisation. Recalling the ICLR guidance that \u201ca lack of state-of-the-art results does not by itself constitute grounds for rejection\u201d, we feel our work is sufficiently novel and original to be of interest to the ICLR community. We would welcome further discussion to help us understand the root of your disagreement.\n\nQ1: Brief experiments during development suggested our algorithm was a poor match for Penn Treebank trained on the GPT-2 model, so we did not pursue these further. We suspect this may be related to the common observation that transformer models display quite different optimisation dynamics to other settings \u2013 indeed, the varying preference for SGD or Adam in particular circumstances has been long-noted in the community. We will add an updated version of this setting to our paper, which we hope will resolve the ambiguity.\n\nQ2: Thank you for pointing out this misstatement. We do indeed compute the curvature matrix $\\mathbf{C}$ using the Jacobian-vector product trick, and our implementations may be inspected in the functions `learning_rate_from_hessian()` and `learning_rate_from_fisher()` from `optimisers.py` in the Supplementary Material. These apply a forward-mode Jacobian-vector product to the gradient-producing function, and a forward pass through the gradient is of course a backwards pass through the model. We will update this line accordingly.\n\nWe were genuinely surprised by your rating, and would be keen to engage in further discussion to better understand your concerns about our work."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5299/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700174485195,
                "cdate": 1700174485195,
                "tmdate": 1700174485195,
                "mdate": 1700174485195,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cisHfHcoI7",
                "forum": "CVldG5ohCy",
                "replyto": "H7iJ5wTLxm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5299/Reviewer_2iRB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5299/Reviewer_2iRB"
                ],
                "content": {
                    "comment": {
                        "value": "I would like to give more justification for my rating, point by point with citation from your manuscript and I would appreciate your feedback on each of them.\n\n**About computational efficiency of AdamQLR**. I believe that adapting the heuristics of K-FAC to Adam is not a natural approach since Adam is a popular optimizer especially for its simplicity: the covariance matrix of the gradient is used as a proxy for the Hessian matrix, which is supposed to be diagonal, making it easy to compute by squaring the gradient entries, providing a computationally efficient algorithm. I believe that estimating full Hessian information (via the vector products with an additional backward pass) just to compute the learning rate for Adam (that uses the assumption of diagonal Hessian, as described above) just doesn't make sense to me. Moreover, calling this approach computationally-efficient is fundamentally wrong since it incurs  that additional backward pass because, as you have also stated in the paper, Adam is already an adaptive learning rate optimizer.\n\n\n\n> **We propose a variation of damping based on Adam\u2019s internal curvature estimates which, when applied to Adam\u2019s update proposals, outperforms classical damping from e.g. K-FAC**\n\nThis statement is unclear to me. How do you measure which damping scheme is better and from which point of view, what is the metric based on which you compare these?\n\n\n\n> **We might ask if accepting first-order methods\u2019 inaccurate curvature models and applying second-order stability techniques would blend the computational efficiency and optimisation accuracy of each**\n\nFrom your paper I understand that you ran standard K-FAC and I would be interested in whether you tried K-FAC without these heuristics that you inputted to Adam. I believe this should have been a first step in the research flow.\n\n\n\n**Momentum for K-FAC**. In your paper you skipped the momentum heuristic from K-FAC and state that Adam already has a momentum correction. Indeed, Adam uses momentum for the gradient (similar to how SGD applies it), but K-FAC uses momentum term in such a way that the quadratic approximation M is minimized, which is completely different. Can you please elaborate more on this particular topic, since it is also an heuristic of K-FAC.\n\n\n\n**Learning rate clipping**. When you rescale the learning rate, you clip it to $\\alpha_\\text{max}$, which is another heuristic that you introduce. After a quick look at the K-FAC paper, learning rate clipping is not mentioned in the original paper. I believe that blending this heuristic with the ones from K-FAC leads to unfair comparison\n\n\n\n**Other evaluation flaws**. I believe that it is not fair to compare different optimizers with different batch-sizes, since this parameter yields different number of optimization steps. This also requires manual scaling of initial learning rate to account for the gradient noise in the stochastic gradient.\n\n\n\n**AdamQLR increases learning rate**. It is known that in the context of stochastic optimization the gradient is noisy, depending on the batch size. The learning rate schedules are designed to decay the learning rate over the course of optimization, converging to zero by the end of training. From SGD convergence analysis we know that by decaying the learning rate we alsodecay the term that depends on the gradient noise which contributes to increasing the upper bound for the convergence (at least in SGD analysis). The simple fact that AdamQLR increases the learning rate might be a problem for why this technique does not yield good results.\n\n**Tuning**. It seems to me that your approach needs a lot of tuning in order to make it work, which is an indication that the method is not numerically stable.\n\n**Manuscript inconsistencies**. There are some inconsistencies in the information from abstract and introduction which I do not agree with and they are related to all the points that I mentioned in this comment, backed by the observations in the next comment. This is what I meant when I first said that it doesn't meet the ICLR standards."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5299/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700516693438,
                "cdate": 1700516693438,
                "tmdate": 1700516693438,
                "mdate": 1700516693438,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "UMelrxPs3C",
                "forum": "CVldG5ohCy",
                "replyto": "H7iJ5wTLxm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5299/Reviewer_2iRB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5299/Reviewer_2iRB"
                ],
                "content": {
                    "comment": {
                        "value": "I am continuing by pointing out some inconsistencies between the abstract/introduction and the results in Figure 2, which I use to justify my score for the paper.\n\n**AdamQLR VS other optimizers**\n1. UCI Energy, Figure 2a:\n- **train loss**: K-FAC < Adam < Tuned AdamQLR < Untuned AdamQLR < SGD (Minimal / Full)\n- **test loss**: K-FAC has lowest (you could have zoomed in on the interval 0-50 seconds)\n2. UCI Protein, Figure 2b:\n- **train loss**: K-FAC << Tuned AdamQLR < Adam < Untuned AdamQLR < SGD (Minimal / Full)\n- **test loss**: same relationship as for the train loss\n3. Fashion-MNIST, Figure 2c:\n- **train loss**: K-FAC is by far the best, while AdamQLR (Tuned and Untuned) and original Adam have similar trajectories\n- **test loss**: K-FAC is the best in the first 5 seconds, then Tuned AdamQLR is better than Adam and Untuned AdamQLR\n- ** test accuracy**: K-FAC is the best, while Tuned/Untuned AdamQLR and Adam are all similar\n4. SVHN, Figure 2d:\n- **train loss**: here, Untuned AdamQLR is better than all other optimizers in the first ~75s of the training. However, I do not know why there are so many large and frequent decreases in the training loss, compared to the other optimizers, can you please explain that? To me it seems like the learning rate decay is performed more often than for the other optimizers (or is it from the automatic learnign rate adjustment?)\n- **test loss**: Untuned AdamQLR is the best in the first 50s of the training, while it is almost outperformed by SGD\n- **test accuracy**: Untuned AdamQLR is similar to SGD\n5. CIFAR-10, Figure 2e:\n- **train loss**: K-FAC < Untuned AdamQLR < Tuned AdamQLR < SGD < Adam\n- **test loss**: K-FAC decreases the test loss by a lot in the first 250s and is much better than Tuned and Untuned AdamQLR\n- **test accuracy**: Tuned AdamQLR is better by the time point 1200s\n6. ImageNet, Figure 6:\n- **validation accuracy**: Untuned Adam is better than AdamQLR and SGD\n- **test accuracy**: same as for the validation accuracy"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5299/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700517042437,
                "cdate": 1700517042437,
                "tmdate": 1700517042437,
                "mdate": 1700517042437,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "pII48hCvqc",
            "forum": "CVldG5ohCy",
            "replyto": "CVldG5ohCy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5299/Reviewer_MsHa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5299/Reviewer_MsHa"
            ],
            "content": {
                "summary": {
                    "value": "This work refers to Adam and proposes to adaptively adjust the learning rate. Specifically, the authors utilize $\\rho$ to denote the ratio between the difference of true loss function $f()$ and the difference of second-order estimation $M()$. Then the authors refine the estimated Hessian matrix through $\\lambda$ according to $\\rho$. Finally, the learning rate is then computed by minimizing $M(\\theta - \\alpha d)$."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1.\tThe method makes sense.\n\n2.\tExtensive experiments show the effectiveness of the method."
                },
                "weaknesses": {
                    "value": "1.\tI wonder how to get the matrix $C$ in Eq. 1.\n\n2.\tWhat is the principle of setting $\\omega_{dec}$ and $\\omega_{inc}$, and why $\\lambda$ is adjusted when $\\rho$ larger than 3/4 or smaller than 1/4?"
                },
                "questions": {
                    "value": "Please see the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5299/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698836783920,
            "cdate": 1698836783920,
            "tmdate": 1699636530270,
            "mdate": 1699636530270,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2TdwRIXuYH",
                "forum": "CVldG5ohCy",
                "replyto": "pII48hCvqc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5299/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5299/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer MsHa"
                    },
                    "comment": {
                        "value": "Thank you for your efforts reviewing our submission \u2013 we appreciate your time.\n\nW1: The curvature matrix $\\mathbf{C}$ denotes any estimate of objective curvature we may wish to use. AdamQLR only uses this matrix in the context of a product with an arbitrary vector. Using the Jacobian-vector product trick (Pearlmutter, 1994; reference in paper), we can compute such products with the Fisher information matrix (which we use) or the Hessian matrix (a common alternative) without having to compute or store the whole matrix. This allows us to compute the exact matrix-vector products according to the definition of our chosen curvature matrix. The functions `learning_rate_from_hessian()` and `learning_rate_from_fisher()` in `optimisers.py` from our Supplementary Material contain our implementations in JAX. We briefly mention this approach in the final paragraph of Section 3.3, and will expand our explanation to make this clearer.\n\nW2: In principle, we seek $\\omega_\\textrm{dec}$ and $\\omega_\\textrm{inc}$  to be sufficiently close to 1 to avoid destabilising the optimiser with dramatically-varying damping, but sufficiently far from 1 to allow prompt rectification of any undesired behaviour from the optimiser. We note that a major thrust of our contribution is that an _Untuned_ version of AdamQLR (using the default hyperparameters we suggest) performs comparably to a _Tuned_ version of Adam, which avoids placing any burden of selecting these factors on the end-user \u2013 we will update our paper to make this clearer. In any case, our sensitivity study in Figure 13 (Appendix B.2.5) suggests AdamQLR is robust to the setting of these factors.\n\nOur thresholds for $\\rho$ in Equation 2 were taken directly from the original K-FAC settings (Martens and Grosse, 2015; reference in paper), since we considered them to be components of the damping strategy we were porting to Adam. Their purpose is to detect when the loss change predicted by the model is much larger than the actual change, and intervene quickly to apply damping to mitigate the model error. The intention is to bias the decision towards decreasing damping if the optimisation is proceeding stably, since in the vicinity of a local minimum excessive damping would slow down convergence. We will add a more verbose discussion of this phenomenon to the paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5299/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700174303752,
                "cdate": 1700174303752,
                "tmdate": 1700174303752,
                "mdate": 1700174303752,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "0rRrqfLZEX",
            "forum": "CVldG5ohCy",
            "replyto": "CVldG5ohCy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5299/Reviewer_mFeq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5299/Reviewer_mFeq"
            ],
            "content": {
                "summary": {
                    "value": "In this paper authors propose some symbiosis of two optimization methods: Adam and K-FAC. They combine damping and learning rate selection techniques from K-FAC and use it inside Adam algorithm. The resulting algorithm, called AdamQLR, is then evaluated on different regression and classification tasks."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. Lots of numerical experiments.\n2. Clear description of algorithm modification.\n3. Good description of the motivation of the heuristics, adopted from K-FAC.\n4. Description of the experimental setup and hyperparameter search space."
                },
                "weaknesses": {
                    "value": "From theoretical point of view, the result seems insignificant. You took some heuristics, that improve the model, and moved it to another model. There is no evidence, that it should work better in theory. From practical point of view, as far as I understand, the number of hyperparameters increased: $\\beta_1, \\beta_2, \\varepsilon$ for Adam vs $\\beta_1, \\beta_2, \\varepsilon, \\lambda$ for AdamQLR (or even $w_{dec}, w_{inc}$ instead of $\\lambda$."
                },
                "questions": {
                    "value": "Rosenbrock function example seems unfair, because you use Hessian there, what do you think?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5299/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5299/Reviewer_mFeq",
                        "ICLR.cc/2024/Conference/Submission5299/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5299/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698838978687,
            "cdate": 1698838978687,
            "tmdate": 1700596767552,
            "mdate": 1700596767552,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hhxAO948BS",
                "forum": "CVldG5ohCy",
                "replyto": "0rRrqfLZEX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5299/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5299/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer mFeq"
                    },
                    "comment": {
                        "value": "Thank you for your efforts reviewing our submission \u2013 we appreciate your time.\n\nW: We agree our work has a greater empirical focus than theoretical. We believe valuable contributions can be made from both perspectives, and the clear intuition behind our method partly substitutes a more theoretically robust (but less practically useful) analysis.\n\nWhile the features we port from K-FAC are heuristics, we feel this slightly undersells their importance, because in our experience these heuristics are _essential_ to K-FAC making progress in training without diverging, rather than being nice-to-haves which incrementally polish the performance of an already-strong underlying algorithm. A main theme of our work is thus to investigate the possibility that these heuristics can play a more fundamental role in optimisation. We will revise our paper to make this point more clearly.\n\nWe do indeed introduce additional damping hyperparameters for AdamQLR. However, a major thrust of our contribution is that the _Untuned_ version of AdamQLR performs comparably to the _Tuned_ version of Adam, so we claim it is reasonable to leave these hyperparameters at their default values in practice. Indeed, the fact that damping is adaptive means AdamQLR can effectively correct its own hyperparameters to an extent. We will update our paper to place a greater emphasis on this contribution.\n\nQ: Unlike our neural network tasks, the Rosenbrock function does not output a probability distribution, so we cannot compute a Fisher information matrix. AdamQLR supports any curvature estimate, so we use the Hessian matrix as the next-best option. We note that Hessian-vector products allow multiplication by large Hessians at similar cost to the Fisher-vector products we use in the other training tasks, so there is not a computational disparity between the Rosenbrock experiment and our other evaluations. If by \u201cunfair\u201d you mean a different issue, could you please clarify?"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5299/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700174195031,
                "cdate": 1700174195031,
                "tmdate": 1700174195031,
                "mdate": 1700174195031,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lra14YOMRF",
                "forum": "CVldG5ohCy",
                "replyto": "wXoSxc6bT7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5299/Reviewer_mFeq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5299/Reviewer_mFeq"
                ],
                "content": {
                    "title": {
                        "value": "Highlights of changes"
                    },
                    "comment": {
                        "value": "Please, if it is possible highlight all the changes that you made in the original paper."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5299/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700561045902,
                "cdate": 1700561045902,
                "tmdate": 1700561045902,
                "mdate": 1700561045902,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JpLAwwChkL",
                "forum": "CVldG5ohCy",
                "replyto": "0rRrqfLZEX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5299/Reviewer_mFeq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5299/Reviewer_mFeq"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your comments and changes!\nI have been looking through the revised version more carefully, and I've got several remarks.\n1. You say, that your main motivation is to show that untuned version of AdamQLR performs similarly to tuned Adam. However you do not provide the results for untuned Adam. Maybe it performs similarly.\n2. Again, you do not provide the results for untuned K-FAC. Thus, we do not know, how it performs on these tasks.\n3. FashionMNIST: you say, that K-FAC overfits much earlier, compared to other methods. But It achieves the best test accuracy faster, then any other method. So it is a wrong conclusion: if we stop all the methods earlier, and not when K-FAC starts overfitting, it will be the best. \n4. Actually, K-FAC performs the best in most of the experiments. And, if you say, that your main motivation is not to provide SOTA method, but to provide a method, which untuned version has comparable performance with tuned Adam or K-FAC, then again see points 1 and 2.\n4. The section about batch size seems weird, since obviously the bigger batch size, the better convergence of the algorithm, since it narrows the area of convergence of stochastic gradient-based method, which can be seen from convergence rate of SGD [1].\n4. You only provide performance results against time, which seems not enough, and it is better to provide also performance against epochs.\n4. When we look at experiments on bigger models and datasets (ImageNet, Penn Treebank), we see, that proposed method is outperformed by all the others. Taking into account my points 1-4, it seems unfair to say, that untuned AdamQLR performs on the same level as tuned Adam on any task.\n\n[1] Gower, Robert Mansel, et al. \"SGD: General analysis and improved rates.\" International conference on machine learning. PMLR, 2019."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5299/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700596070549,
                "cdate": 1700596070549,
                "tmdate": 1700596665605,
                "mdate": 1700596665605,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0IKQiWZMPb",
                "forum": "CVldG5ohCy",
                "replyto": "dUg5I6foi1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5299/Reviewer_mFeq"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5299/Reviewer_mFeq"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks, but actually I meant highlight with color in the revised PDF."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5299/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700657464263,
                "cdate": 1700657464263,
                "tmdate": 1700657464263,
                "mdate": 1700657464263,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "27d3ATJfbC",
            "forum": "CVldG5ohCy",
            "replyto": "CVldG5ohCy",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5299/Reviewer_BW7Y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5299/Reviewer_BW7Y"
            ],
            "content": {
                "summary": {
                    "value": "This paper tried to combine the first-order method (such as Adam) with the second-order methods, such as K-FAC. More specifically, the authors propose a novel optimizer AdamQLR: combining damping and learning rate selection techniques of K-FAC. The experimental results illustrate that the proposed method AdamQLR can achieve competitive generalisation performance and training efficiency."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The idea of combining first-order and second-order methods is very interesting. In addition, the research direction is also very important. \n2. The proposed method is very easy to understand. I think we should pay more attention to second-order method and improve its efficiency."
                },
                "weaknesses": {
                    "value": "1. I think the main results are from the figure 2. But the figure is not very clear for me, maybe you can list the training loss, test loss, convergence steps, and generalization gap ( |training_loss - test_loss| ) in a table. From this figure. I'm not very clear whether the proposed method can solve the overfitting issue and improve the generalization. So I think you can analyze the generalization gap. \n\n2. The experimental results are not very strong for me. Although the proposed method can achieve fast convergence and lower test loss, their performance is still too close. In addition, you try to analyze training loss and test loss in figure 2. But loss value is not a great metric for classification tasks and I think you should show the accuracy. \n\n3. The training task is too simple and the results on complex tasks (such as ImageNet) is not very strong."
                },
                "questions": {
                    "value": "1. Loss value in figure 2 is not great enough to compare the performance of different methods and maybe you should provide the accuracy value."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5299/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699364701442,
            "cdate": 1699364701442,
            "tmdate": 1699636530090,
            "mdate": 1699636530090,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vwkAsWbEHA",
                "forum": "CVldG5ohCy",
                "replyto": "27d3ATJfbC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5299/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5299/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BW7Y"
                    },
                    "comment": {
                        "value": "Thank you for your efforts reviewing our submission \u2013 we appreciate your time.\n\nW1/Q1: Figure 2 is indeed the crux of our results section, and our claims about generalisability are based on comparisons from these graphs. We will add a table of final results as you suggest.\n\nW2: We show classification accuracies in Figure 4 (Appendix B.1.1), since we agree there is reason to be interested in both loss and accuracy performance. We will update the main body to point out this figure more clearly. Regarding the performance similarities: since we apply second-order heuristics to the core Adam algorithm, we don\u2019t expect to massively beat it \u2013 our main contribution is that the _Untuned_ version of AdamQLR performs comparably to the _Tuned_ version of Adam, meaning AdamQLR (Untuned) can be applied with dramatically reduced computational effort.\n\nW3: Even though the tasks in Figure 2 aren\u2019t as large-scale as some other applications, we think they strike a balance between being large enough to display features of the neural network training problem, but small enough to easily perform multiple repetitions of the experiments to reinforce our results. The reviewer may also be interested in the Penn Treebank/GPT-2 results we will be adding in response to other feedback."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5299/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700174105801,
                "cdate": 1700174105801,
                "tmdate": 1700174105801,
                "mdate": 1700174105801,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]