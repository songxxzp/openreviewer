[
    {
        "title": "Differential Model Scaling using Differential Topk"
    },
    {
        "review": {
            "id": "9GhNVAQkTp",
            "forum": "lQhh1sbfzp",
            "replyto": "lQhh1sbfzp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1064/Reviewer_L4n8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1064/Reviewer_L4n8"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes DMS which can find improved structures and outperforms state-of-the-art NAS methods. It demonstrated improved performance on image classification, object detection and LLM."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The proposed method is sound and straightforward."
                },
                "weaknesses": {
                    "value": "- The image classification baselines are too weak. baselines should have 90%+ top-1 accuracy.\n- Shown in Table 6, the performance gain is marginal."
                },
                "questions": {
                    "value": "- accuracy vs MACs Plot with Table 6. The performance gain seems marginal from the table.\n- explain how this loss_resource is designed."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1064/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698608195906,
            "cdate": 1698608195906,
            "tmdate": 1699636032871,
            "mdate": 1699636032871,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8n2aqurvj8",
                "forum": "lQhh1sbfzp",
                "replyto": "9GhNVAQkTp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1064/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1064/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer L4n8"
                    },
                    "comment": {
                        "value": "Dear reviewer,\n\nThank you for your comments. We have addressed all of them below.\n\n> W1 \"The image classification baselines are too weak. baselines should have 90%+ top-1 accuracy.\"\n\nThanks for your suggestion. We agree with you that a strong baseline is very important. \n\nHowever, in this paper, our main target is to compare the search efficiency of our method and other search methods. Hence, we select our baselines from the related work for a fair comparison. All of the most related work, including ScaleNet, ModelAmplification, and so on, use EfficientNet as their baselines.\n\nSince we think our baselines are suitable for our target rather than too weak. We will consider using stronger baselines in our future work if we need to build a SOTA image classification model.\n\n> W2 \"Shown in Table 6, the performance gain is marginal.\", \n> Q1 \"accuracy vs MACs Plot with Table 6. The performance gain seems marginal from the table.\"\n\nSorry for the confusion. There are two reasons why our performance gain seems 'marginal'.\n\n1. Compared with the Multi-Shot and One-shot methods, our methods have a much lower search cost, up to several tens, even hundreds of times lower. Therefore, our performance gain is not marginal because we may use much lower search costs to achieve a higher performance.\n2. Compared with zero-shot NAS methods, they all use much stronger training recipes than ours, such as distillation and mixup. Therefore, the real improvement gain is more significant than the table shows.\n\nHere, we provide a new table to make the comparison clearer (We only list some methods that may be confusing; please refer to Table 5 in our revised paper for full comparison.)\n\n| Method                   | NAS Type  | Top1     | MACs  | Params | Cost |\n| ------------------------ | --------- | -------- | ----- | ------ | ---- |\n| Zen-score (+)            | ZeroShot  | 78.0     | 0.41G | 5.7M   | Low  |\n| **DMS-EN-B0 (ours)**     | Gradient  | **78.5** | 0.39G | 6.2M   | Low  |\n| Zen-score \uff08+\uff09          | ZeroShot  | 79.1     | 0.60G | 7.1M   | low  |\n| ModelAmplification-EN-B1 | MultiShot | 79.9     | 0.68G | 8.8M   | High |\n| **DMS-EN-B1 (ours)**     | Gradient  | **80.0** | 0.68G | 8.9M   | Low  |\n| ScaleNet-EN-B2           | OneShot   | 80.8     | 1.6G  | 11.8M  | High |\n| Zen-score (+)            | ZeroShot  | 80.8     | 0.9G  | 19.4M  | Low  |\n| ModelAmplification-EN-B2 | MultiShot | 80.9     | 1.0G  | 9.3M   | High |\n| BigNAS-XL                | OneShot   | 80.9     | 1.0G  | 9.5M   | High |\n| **DMS-EN-B2 (ours)**     | Gradient  | **81.1** | 1.1G  | 9.6M   | Low  |\n\n(+) means the model is trained using much stronger training tricks, such as distillation and mixup. As some methods do not report their search costs, we use \"High\" and \"Low\" to provide a rough comparison.\n\nThanks for your comments. We updated the table (Note the index is 5 in our revised paper, while it's 6 in the original paper).\nWe also added an accuracy vs MACs plot to the paper, notated as Figure 3 in our revised paper.\n\n> Q3 \"explain how this loss_resource is designed.\n\nResource constraint loss is widely used in the field of NAS and pruning [1,2]. We just follow the common practice in the field.\n\nTake constraining the number of parameters of a fully connected layer as an example. Suppose the layer has $f_{in} \\in R$ input features and $f_{out} \\in R$ output features. The structure parameters for its input features and output features are $a_{in} \\in [0,1]$ and $a_{out} \\in 0,1$, respectively. The resource constraint loss for the layer is defined as $f_{in} \\times a_{in} \\times f_{out} \\times a_{out}$. The resource constraint loss of the total model is the sum of all parameter losses of all layers.\n\nWe also detailed the resource constraint loss in Appendix A.1.2. Please refer to it for more details.\n\n[1] Li, Yanyu, et al. \"Pruning-as-Search: Efficient Neural Architecture Search via Channel Pruning and Structural Reparameterization.\"  \n[2] Gao, Shangqian, et al. \"Disentangled differentiable network pruning.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n\n\nThanks for your comments again. If you have any further comments, please let us know.\nBesides, we want to emphasize the value of our method. We build a general and flexible NAS method. It can improve various tasks stably with a reasonable search cost, while prior methods are usually designed for a specific task and use much more resources. Our method pushes the boundary of NAS and makes it more practical for real-world applications.\nTherefore, we think our method is valuable and worth publishing. I hope you can raise our paper's score if our responses are convincing. Thank you very much."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1064/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700127313627,
                "cdate": 1700127313627,
                "tmdate": 1700127313627,
                "mdate": 1700127313627,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1sIXneV5RU",
                "forum": "lQhh1sbfzp",
                "replyto": "9GhNVAQkTp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1064/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1064/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sincerely expecting feedback from reviewer L4n8."
                    },
                    "comment": {
                        "value": "Dear reviewer:\n\nThanks for your constructive comments. We have posted our responses to your comments. We expect your feedback about whether our responses address your concerns, or if you have any further questions. We are glad to answer them and improve our paper.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1064/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547587424,
                "cdate": 1700547587424,
                "tmdate": 1700547587424,
                "mdate": 1700547587424,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Ns26NCriux",
            "forum": "lQhh1sbfzp",
            "replyto": "lQhh1sbfzp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1064/Reviewer_9aAa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1064/Reviewer_9aAa"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a new method called Differential Model Scaling (DMS) for optimizing network architectures, resulting in improved performance across diverse tasks. The study addresses the inefficiency of existing Neural Architecture Search (NAS) methods and proposes DMS as a more efficient alternative for searching optimal width and depth in networks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper introduces a novel, fully differentiable model scaling method, addressing a fundamental challenge in neural network architectures.\n-  The developed search algorithm efficiently identifies optimal network structures, potentially reducing computational costs in architecture search.\n-  The paper is well-written, with a clear and accessible style that enhances understanding, making it broadly accessible to the scientific community."
                },
                "weaknesses": {
                    "value": "- The paper does not provide the code or the implementation details of the proposed method, which makes it difficult to reproduce and verify the results.\n- The paper does not explain how the layerwise mask affects the channel-wise mask in the differential topk. It is unclear how the two masks interact and whether they can be jointly optimized in an efficient way.\n- The paper lacks a proper control experiment to isolate the effect of the differential topk from other factors, such as the network architecture, the learning rate, and the data augmentation. It is possible that some of the improvements are due to these factors rather than the proposed method.\n- The paper introduces too many hyperparameters for the differential topk, such as the temperature, the sparsity ratio, and the regularization coefficient. The paper does not provide a systematic analysis of how these hyperparameters affect the performance and the stability of the method. It is also unclear how to choose these hyperparameters for different tasks and architectures.\n- The paper's ablation study is not comprehensive enough to demonstrate the advantages of the proposed method. The paper only compares the differential topk with a uniform scaling baseline, but does not compare it with other model scaling methods, such as compound scaling or progressive scaling. The paper also does not show how the differential topk performs on different network layers, such as convolutional layers or attention layers."
                },
                "questions": {
                    "value": "- In Section 3.1.3, you use a moving average to update the layerwise mask. What is the motivation and benefit of this technique? How does it affect the convergence and stability of the optimization?\n- In Section 3.1.3, you adopt an L1-norm regularization term for the channel-wise mask. Why did you choose this norm over other alternatives, such as L2-norm or entropy? How does the choice of norm influence the sparsity and diversity of the channel-wise mask?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1064/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1064/Reviewer_9aAa",
                        "ICLR.cc/2024/Conference/Submission1064/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1064/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698684319897,
            "cdate": 1698684319897,
            "tmdate": 1700650844261,
            "mdate": 1700650844261,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ut8pOxKcbJ",
                "forum": "lQhh1sbfzp",
                "replyto": "Ns26NCriux",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1064/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1064/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9aAa (Part 1/3)"
                    },
                    "comment": {
                        "value": "Dear reviewer:\n\nThanks for your comments. Below are our responses to address your concerns.\n\n> W1 \"The paper does not provide the code or the implementation details of the proposed method, which makes it difficult to reproduce and verify the results.\"\n\nConsidering the double-blind review policy, we can not open-source our code before the paper is accepted. However, we will open-source our code after the paper is accepted.\n\n> W2 \"The paper does not explain how the layerwise mask affects the channel-wise mask in the differential topk. It is unclear how the two masks interact and whether they can be jointly optimized in an efficient way.\"\n\nThe layerwise mask and the channel-wise mask are independent.\nFor example, we have a $layer$ and an input $x$. We use $m_{L_i} \\in [0,1]$ to denote the layerwise mask and $m_{C}\\in [0,1]^{N}$ for the channel-wise mask.\nThe forward process is as follows: $y= m_C \\times x+m_{L_i} \\times layer(m_C \\times x)$.\nAfter searching, we will prune layers and channels according to the layerwise and channel-wise masks, respectively.\n\nWe updated Appendix A.1.1 to make it more clear.\n\n> W3 \"The paper lacks a proper control experiment to isolate the effect of the differential topk from other factors, such as the network architecture, the learning rate, and the data augmentation. It is possible that some of the improvements are due to these factors rather than the proposed method.\"\n\nWe think we have controlled the factors you mentioned.\nFor architecture, we evaluate our method always with the same architecture as our baselines.\nFor learning rate and data augmentation, without extra explanation, we use the same training setting for our method and the baseline methods.\nWe detail the training setting in Appendix a.6. If you think where we have not controlled the factors, please let us know."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1064/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700127166457,
                "cdate": 1700127166457,
                "tmdate": 1700127166457,
                "mdate": 1700127166457,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "pUOsv13XHl",
                "forum": "lQhh1sbfzp",
                "replyto": "Ns26NCriux",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1064/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1064/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9aAa (Part 2/3)"
                    },
                    "comment": {
                        "value": "> W4 \"The paper introduces too many hyperparameters for the differential topk, such as the temperature, the sparsity ratio, and the regularization coefficient. The paper does not provide a systematic analysis of how these hyperparameters affect the performance and the stability of the method. It is also unclear how to choose these hyperparameters for different tasks and architectures.\"\n\nSorry for the confusion. We think our hyperparameters are not too many, as there are only two hyperparameters that need to be turned for different tasks and architectures, and the other hyperparameters are fixed.\n\nWe detail our hyperparameters as follows:\n\nWe first introduce the fixed hyperparameters.\n\n1. Decay for Taylor importance. Taylor importance[1,2] is a well-known method to measure the importance of elements, and the decay of moving average is also widely used in the literature. Therefore, we directly use the value from prior work.\n2. Temperature of our diffenretial topk. The temperature is used to polarize [3] the mask of elements. This is also a widely used approach for pruning. Directly selecting a value that can polarize the mask of elements is enough. Thanks to our importance normalization, the temperature can be directly computed by closed-form, detailed in section 3.1.2. Therefore, it's also fixed.\n3. Supernet size and sparsity ratio. In our opinion, the supernet size and sparsity ratio are not hyperparameters. They are set according to the user's demand and their resource amount.\n\nOnly two hyperparameters need to be turned for different tasks and architectures.\n1. Resource constraint loss weight, denoted by $\\lambda_{resource}$\n2. Learning rate for structure parameters denoted by $lr_{structure}$\n   \nThey are used to control the update of the structure parameters. The update value of a structure parameter is computed by $lr_{strucutre}\\times (g_{task} + \\lambda_{resource}\\times g_{resource})$, where $g_{task}$ and $g_{resource}$ is the gradient of structure parameters with respect to the task loss and resource constraint loss,\nWe provide an ablation study as follows:\n\n| - | 5e-2 | 5e-3 | 5e-4 | 5e-5 |\n|--|------|------|------|------|\n| 0.1 | / | / | / | / |\n| 1 | 73.0 | 73.1 | 72.9 | / |\n| 10 | 72.5 | 72.2 | 72.6 | 70.9 |\n\nThe head row is the $lr_{structure}$, while the head column is the $\\lambda_{resource}$. '/' indicates that the model cannot reach our target resource constraint.\nObviously, \n\n1. Smaller $\\lambda_{resource}$ is better, as long as the model can reach the target resource constraint. Smaller $\\lambda_{resource}$ means that the task loss takes more control of the update of the structure parameter.\n2. When $\\lambda_{resource}$ is small, the model is not sensitive to the change of $lr_{structure}$. When $\\lambda_{resource}$ is large, a relatively large $lr_{structure}$ is better. This is because reaching the target resource constraint can reduce the influence of the resource constraint loss, as resource constraint loss is zero when the model reaches the target resource constraint.\n\nTherefore, the setting of $\\lambda_{resource}$ and $lr_{structure}$ is not difficult. We first fix $lr_{structure}$ and turn $\\lambda_{resource}$ to a small value and ensure the model can reach the target resource constraint. Then, we turn $lr_{structure}$ to a relatively large value, which makes the model reach the target resource constraint in the first hundreds of iterations. Only observing the resource decrease in the first epoch is enough to set these two hyperparameters.\n\nCompared with other NAS methods, our method uses fewer hyperparameters. For example, ModelAmplification must turn at least five hyperparameters for different tasks and models.\n\nWe added the ablation study to Appendix A.5.4 in our revised paper.\n\n[1] Molchanov, Pavlo, et al. \"Importance estimation for neural network pruning.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019.  \n[2] Humble, Ryan, et al. \"Soft masking for cost-constrained channel pruning.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.  \n[3] Neuron-level Structured Pruning using Polarization Regularizer"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1064/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700127220219,
                "cdate": 1700127220219,
                "tmdate": 1700127220219,
                "mdate": 1700127220219,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "40FuNRYJX8",
                "forum": "lQhh1sbfzp",
                "replyto": "Ns26NCriux",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1064/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1064/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 9aAa (Part 3/3)"
                    },
                    "comment": {
                        "value": "> W5.1 \"The paper's ablation study is not comprehensive enough to demonstrate the advantages of the proposed method. The paper only compares the differential topk with a uniform scaling baseline, but does not compare it with other model scaling methods, such as compound scaling or progressive scaling.\"\n\nThanks for your suggestion. We don't compare with compound scaling and progressive scaling in our ablation study because we have already compared them with our method in our main experiments. Where EfficientNet is a compound scaling method, and ModelAmplification is a progressive scaling method. Besides, we have also compared our method with other model scaling methods, such as differential scaling (JointPruning).\nTherefore, we think it is not necessary to compare with other scaling methods in our ablation study.\n\n> W5.2 \" The paper also does not show how the differential topk performs on different network layers, such as convolutional layers or attention layers.\"\n\nWe apologize for our unclear description.\nWe apply our differential topk to different layers by multiplying the mask, outputed by our differential topk operators, with the input to the layer.\n\nFor convolutional layers, suppose the input is $X \\in \\mathbb{R}^{B \\times C \\times H \\times W}$, and the mask is reshaped as $m \\in \\mathbb{R}^{1 \\times C \\times 1 \\times 1}$, $X \\times m$ works as the new input to the layer.\n\nFor an attention layer, we search the head dims of q k v and the number of heads. Suppose our supernet has $H$ heads and $D$ dims in each head. We have a mask for qk head dim with $m_{qk}\\in R^{1\\times 1 \\times 1 \\times D}$, a mask for v head dim with $m_{v}\\in R^{1\\times 1 \\times 1 \\times D}$, and a mask for number of heads $m_{head}\\in R^{1\\times H \\times 1 \\times 1}$. Suppose the sequence length is $L$, and the q k v for self-attention is $Q,K,V \\in R^{B \\times H \\times L \\times D}$. We compute the output of the self-attention by $softmax(\\frac{Q'K'^T}{\\sqrt{}})V'$, where $Q'=Q\\times m_{qk}\\times m_{head}, K'=K\\times m_{qk}\\times m_{head}, V'=V\\times m_{v}\\times m_{head}$\n\nWe added above content to Appendix A.1.1\n\n> Q1 \"In Section 3.1.3, you use a moving average to update the layerwise mask. What is the motivation and benefit of this technique? How does it affect the convergence and stability of the optimization?\"\n\nThe moving average is used to make element importance measures more stable. As shown in the following table.\nWith more stable element importance, we can get a better result.\n| Measure Method | Top-1 |\n|----------------|-------|\n| Taylor without moving average | 72.5 |\n| Taylor with moving average | **73.1** |\n\nTaylor importance with moving average is a common technique in the pruning field. Please refer to [1,2] for more details.\nWe also added the ablation study to Appendix A.5.3 in our revised paper.\n\n\n[1] Molchanov, Pavlo, et al. \"Importance estimation for neural network pruning.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019.  \n[2] Humble, Ryan, et al. \"Soft masking for cost-constrained channel pruning.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n\n> Q2 \"In Section 3.1.3, you adopt an L1-norm regularization term for the channel-wise mask. Why did you choose this norm over other alternatives, such as L2-norm or entropy? How does the choice of norm influence the sparsity and diversity of the channel-wise mask?\"\n\nSorry for the confusion. We did not use L1-norm regularization term in the paper. We mentioned that \"L1-norm\" can be an alternative as an importance measure method. The \"L1-norm\" refers to a well-known channel importance measure method, which computes the L1-norm of each filter as the importance score. Please refer to [1] for more details.\n\n[1] Li, Hao, et al. \"Pruning filters for efficient convnets.\" arXiv preprint arXiv:1608.08710 (2016).\n\nThanks for diving into our paper and providing the constructive comments. We have revised our paper according to your suggestions. Please feel free to contact us if you have any further questions.\nFurthermore, we want to emphasize the value of our work. Our method. It's a general and flexible NAS method that can be applied in real-world scenarios. It improves performance stably, has only two hyperparameters that need turning, and costs much fewer resources than other NAS methods. Hence, we think our work is valuable and deserves to be published.\nIf our responses have addressed your concerns, we would appreciate it if you could consider raising the score of our paper. Thank you for your time and consideration."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1064/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700127258566,
                "cdate": 1700127258566,
                "tmdate": 1700127258566,
                "mdate": 1700127258566,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BIPqfhLNAg",
                "forum": "lQhh1sbfzp",
                "replyto": "Ut8pOxKcbJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1064/Reviewer_9aAa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1064/Reviewer_9aAa"
                ],
                "content": {
                    "title": {
                        "value": "About W2"
                    },
                    "comment": {
                        "value": "I am still confused about W2. From your equation, $m_{L_i}\\times layer(m_{C}\\times x)$, I think the update of $m_{L_i}$ is related to that of $m_{C}$."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1064/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700483274162,
                "cdate": 1700483274162,
                "tmdate": 1700483274162,
                "mdate": 1700483274162,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w2ty7N0WQp",
                "forum": "lQhh1sbfzp",
                "replyto": "40FuNRYJX8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1064/Reviewer_9aAa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1064/Reviewer_9aAa"
                ],
                "content": {
                    "title": {
                        "value": "About Q1 & Q2"
                    },
                    "comment": {
                        "value": "I noticed that you didn't directly address the question at hand. In the study titled 'Zero-cost Proxies for Efficient NAS,' the authors explore various pruning-based metrics used to assess the importance of neurons, each exhibiting distinct performance characteristics. Some of these proxies include:\n\nPlain: $S(\\theta) = \\frac{\\partial \\mathcal{L}}{\\partial \\theta} \\circ \\theta$\nSNIP: $S(\\theta) = \\left| \\frac{\\partial \\mathcal{L}}{\\partial \\theta} \\circ \\theta \\right|$\nGRASP: $S(\\theta) = -\\left( H \\frac{\\partial \\mathcal{L}}{\\partial \\theta} \\right) \\circ \\theta$\nFisher: $S(z) = \\sum_{z_i \\in Z} \\left( \\frac{\\partial \\mathcal{L}}{\\partial z_i} \\right)^2$\nSynFlow: $R = \\prod_{\\theta_i \\in \\Theta} \\left| \\theta_i \\right|$, $S(\\theta) = \\left( \\frac{\\partial R}{\\partial \\theta} \\right) \\circ \\theta$\n\nAmong these proxies, Taylor (Plain) is similar to Plain. My question is why you chose Taylor (Plain) instead of other proxies such as SynFlow, which are considered to be more accurate."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1064/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700484977879,
                "cdate": 1700484977879,
                "tmdate": 1700484977879,
                "mdate": 1700484977879,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qMxP3uWso5",
                "forum": "lQhh1sbfzp",
                "replyto": "Ns26NCriux",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1064/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1064/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responds to \"About Q1 & Q2\""
                    },
                    "comment": {
                        "value": "Sorry, we misunderstood your question.\n\nIn our view, your question can be divided into two parts.\n1. Why do we use Taylor importance with moving average as the element importance measure method?\n\nAnwser: Our reason is simple. Our main contribution is to propose a method to search the number of elements, rather than measuring the importance of each element. Hence, we directly select a **simple and commonly used** element importance metric, Taylor importance, as the element importance measure method. \n\n2. What's the advantage of Taylor importance with moving average, compared with other metrics?\n\nAnwser: \n\nTo be honest, we do not conduct a comprehensive comparison between different metrics, as this is not our main contribution. As a comparison, ZiCo must compare with other metrics, as they propose a new metric.\n\nBesides, For structural pruning, some work argues that the difference between different importance metrics is not significant [1]. Note the conclusion of Zico about the importance metrics is not suitable for us, as the search granularity is different. ZiCo modifies the importance metrics to select models, while we search structure elements (channels/layers).\n\nAlthough we do not conduct a comprehensive comparison, we will still provide an ablation study to compare different metrics.\n\n[1] Huang, Zhongzhan, et al. \"Rethinking the pruning criteria for convolutional neural network.\" Advances in Neural Information Processing Systems 34 (2021): 16305-16318."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1064/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700495609065,
                "cdate": 1700495609065,
                "tmdate": 1700551504813,
                "mdate": 1700551504813,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aMiLwgAz37",
                "forum": "lQhh1sbfzp",
                "replyto": "zesVr7XXwW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1064/Reviewer_9aAa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1064/Reviewer_9aAa"
                ],
                "content": {
                    "title": {
                        "value": "Increase Rating"
                    },
                    "comment": {
                        "value": "Sorry for the late reply. I appreciate your efforts in responding to my questions. I have decided to increase the rating."
                    }
                },
                "number": 24,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1064/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700651028140,
                "cdate": 1700651028140,
                "tmdate": 1700651028140,
                "mdate": 1700651028140,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WLt7EyFY3m",
            "forum": "lQhh1sbfzp",
            "replyto": "lQhh1sbfzp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1064/Reviewer_EkDu"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1064/Reviewer_EkDu"
            ],
            "content": {
                "summary": {
                    "value": "The paper discusses the challenges of manually designing network architectures and the nondifferentiable limitations of existing Neural Architecture Search (NAS) methods. To address these issues, the authors propose Differential Model Scaling (DMS), which offers increased efficiency in searching for optimal width and depth configurations in DNNs. DMS allows for direct and fully differentiable modeling of both width and depth, making it easy to optimize. The authors evaluate DMS across various tasks, including image classification, object detection, and language modeling, using different network architectures such as CNNs and Transformers. The results consistently demonstrate that DMS can find improved network structures and outperforms state-of-the-art NAS methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. This paper proposes a differentiable top-k method, which could be used to select channels or layers in DNNs. The design of differentiable top-k  method is skillful and  meaningful. With normalized importance factors, a learnable parameter $\\alpha$ is used to select elements.   \n2. The whole DMS method merged the task loss and cost loss, With the guidence of cost loss, the DMS can search for efficient models.  \n3. Various experiments demonstrates the superiority of DMS over existing NAS methods. The pruning experiments  presents the method is better than SOTA pruning methods."
                },
                "weaknesses": {
                    "value": "1. Different element importance methods are not studied. Some comparisions should be presented to underscore the DMS method.  \n2. More types of cost losses should be considered, such as latency or memory cost. Latency is a superior indicator compare to FLOPs.   \n3. As far as I know, gumbel top-k method is also differentiable, why you develop a new differentiable top-k methd?    \n4. The open source of the code will help to understand the paper."
                },
                "questions": {
                    "value": "Please see the weaknesses.   \nBesides, in p.5, the authors demonstrate that \"Intuitively, $c_{i}^{'}$ indicates the portion of $c$ values larger than $c_{i}$.\". Here should be \"smaller\" instead of \"larger\"."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1064/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698723984723,
            "cdate": 1698723984723,
            "tmdate": 1699636032719,
            "mdate": 1699636032719,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WAT1xjL6Oa",
                "forum": "lQhh1sbfzp",
                "replyto": "WLt7EyFY3m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1064/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1064/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer EkDu"
                    },
                    "comment": {
                        "value": "Dear review:\n\nThanks for your constructive comments. We address your comments as follows:\n\n> W1: \"Different element importance methods are not studied. Some comparisons should be presented to underscore the DMS method.\"\n\nThanks for your suggestion. Here, we provide a new ablation study to show the influence of element importance.\n\n| Measure Method | Top-1 |\n|----------------|-------|\n| Index | 72.3 |\n| Taylor, without moving average | 72.5 |\n| Taylor, with moving average | **73.1** |\n\n\"Index\" means assigning an importance value to each element according to its index in the sequence statically. For example, we assign 0 to the first element, and 1.0 to the last.\n\nObversely, Taylor importance works better than index importance, as it is able to detect the importance of each element dynamically. Moving averaging further makes Taylor importance more stable.\n\nWe also added the ablation study to Appendix A.5.3 in our revised paper.\n\n> W2: \"More types of cost losses should be considered, such as latency or memory cost. Latency is a superior indicator compare to FLOPs.\"\n\nThanks for your suggestion. In this paper, we use two different resource constraint losses, including Flops loss and number of parameters loss.\n\nIn this paper, our main target is to show the high search efficiency of our method rather than design a model for a specific platform. Therefore, we think any resource indicator is acceptable as long as we use the same resource indicator as our baselines for fair comparison.\n\nSpecifically, for memory cost constraint, we think it's similar to the number of parameters. Hence, we think there is no need to add a memory cost constraint to our paper.\n\nFor latency constraint, We agree that latency is a useful indicator. However, the relation between latency and structure hyperparameters is not easy to model. It means latency is not easy to be converted to a differentiable loss. We think it's also an interesting research direction. We will take it in future research.\n\nTherefore, we think the constraints on flops and the number of parameters are enough to evaluate our method, and other constraints should be left for future research, such as some work that focuses on a specific hardware platform.\n\n\n> W3: \"As far as I know, gumbel top-k method is also differentiable, why you develop a new differentiable top-k methd?\"\n\nWe are sorry for the confusion.\nGumbel top-k [1,2] is completely different from our method. It may be confusing because we use the same term, \"top-k\", in our paper.\n\nGumbel top-k is learned to sample 'k' different elements from a set of n elements, where 'k' is a constant.\nOur differential topk is learning the 'k' itself, which is a learnable parameter.\nGumbel top-k cannot be used to search the width and depth of a network, while our method can.\n\nWe also talk about another method, \"Gumbel softmax\", which is also usually used in pruning and NAS [3].\n\"Gumbel softmax\" is able to work as a learnable gate to select elements for width and depth. However, as we described in our paper, it models depth and width hyperparameter searching as an element selection problem, making it hard to optimize.\n\n[1] Tan, Haoxian, et al. \"Mutually-aware Sub-Graphs Differentiable Architecture Search.\" arXiv preprint arXiv:2107.04324 (2021).  \n[2] Li, Jiaoda, Ryan Cotterell, and Mrinmaya Sachan. \"Differentiable subset pruning of transformer heads.\" Transactions of the Association for Computational Linguistics 9 (2021): 1442-1459.  \n[3] Herrmann, Charles, Richard Strong Bowen, and Ramin Zabih. \"Channel selection using gumbel softmax.\" European Conference on Computer Vision. Cham: Springer International Publishing, 2020.\n\n\n> W4: \"The open source of the code will help to understand the paper.\"\n\nConsidering the double-blind review policy, we can not open-source our code before the paper is accepted. We will open-source our code after the paper is accepted.\n\n> Q5: \"Besides, in p.5, the authors demonstrate that \"Intuitively, $c_i'$ indicates the portion of $c$ values larger than $c_i$ Here should be \"smaller\" instead of \"larger\".\n\nThanks for your correction. We have updated the paper to correct it.\n\nThanks for your effort in reviewing our paper. We have addressed all your comments in above. If you have any further questions, please let us know."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1064/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700126935940,
                "cdate": 1700126935940,
                "tmdate": 1700126935940,
                "mdate": 1700126935940,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TIlFdH2hy5",
                "forum": "lQhh1sbfzp",
                "replyto": "WLt7EyFY3m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1064/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1064/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sincerely expecting feedback from reviewer EkDu."
                    },
                    "comment": {
                        "value": "Dear reviewer:\n\nThanks for your constructive comments. We have posted our responses to your comments. We expect your feedback about whether our responses address your concerns, or if you have any further questions. We are glad to answer them and improve our paper.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1064/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547557193,
                "cdate": 1700547557193,
                "tmdate": 1700547557193,
                "mdate": 1700547557193,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "seAPIHqQa8",
            "forum": "lQhh1sbfzp",
            "replyto": "lQhh1sbfzp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1064/Reviewer_nwwR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1064/Reviewer_nwwR"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new method for differentiable architecture search using a new differentiable top-k operator. Elements (units, blocks, filters, any grouping of parameters) of the network are assigned importance parameters, $c$, that depend on a moving average of the [Taylor importance][taylor]. A learnable threshold $a$ is used to select $k$ elements whose $c$ exceed the threshold. A tradeoff in performance versus capacity is achieved by computing the model resource usage from $c$ and $a$ then constructing a loss function:\n$$\n \\text{loss}_{\\text{resource}} = \\log \\frac{r_c}{r_t} \\text{ if } r_c > r_t \\text{ else } 0\n$$\nwhere $r_c$ is the current resource consumption and $r_t$ is the target.\n\nDuring stochastic gradient optimization, $a$ will be pushed to maintain resource consumption at the desired level, while providing some slack for the model to still learn to perform the task.\n\nThe effectiveness of this method is tested in experiments on image classification on ImageNet, image detection on COCO and a large language model finetuning task.\n\n[taylor]: https://arxiv.org/abs/1906.10771"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The main contributions of the paper are the empirical results: outperforming [ZiCo][] by 0.7% with the same search time (0.4 GPU days). \nThis result appears to be well tested and therefore the paper achieves this goal. Similar results also support the method empirically on COCO and language model finetuning.\n\nThe authors describe the key difference between this work and similar architecture search methods is that it provides a differentiable and direct way to approach architecture search. In other words, other works allow a differentiable selection of which elements to include but do not allow easy optimization of how many elements to include.\n\nArchitecture search is a significant area of research and this paper submits a new method for consideration.\n\n[zico]: https://arxiv.org/abs/2301.11300"
                },
                "weaknesses": {
                    "value": "In Section 3.2 the authors mention that this method bears some resemblance to pruning works, \"Our DMS follows a pipeline similar to training-based model pruning.\" This implies that the model should be compared to pruning based methods in experiments. However, the comparisons appear to be made to NAS methods, such as [JointPruning][]. A comparison to state of the art sparse methods, such as [RIGL][] would make the experiments more robust.\n\nThe function they have constructed for optimization is smooth but saturates outside of the active regions illustrated in Figure 2. This may cause vanishing gradient information. Any experiment to investigate whether this happens during training, or why it doesn't happen would be valuable.\n\nThe relationship between resource constraint and the top-k parameters is in the appendix but it's extremely important to the overall algorithm.\n\nThe top-k operator as described leads the reader to assume $k$ would be fixed but in practice it's not constrained and $k$ can be any value. Really it's just a binary mask that has a soft constraint to sum to a low enough value to meet the resource constraints.\n\n[rigl]: https://arxiv.org/abs/1911.11134\n[jointpruning]: https://ieeexplore.ieee.org/document/9516010"
                },
                "questions": {
                    "value": "How sensitive is the method to the element importance measure $c$? It's computed as a moving average with a specific hyperparameter. It seems like the gradient estimate of $a$ depends on this being stable.\n\nIn Section 3.2 you say \"Compared with training-based pruning, our method eliminates the need for time-consuming pretraining since we think searching from scratch is more efficient thatn from a pretrained model...\". How does that save resources? Typically pretrained models are available for free, but training one yourself from scratch is extremely expensive? I don't understand what Table 4 means because the rows and colums refer to search and retraining but one can't have both a pretrained model and a model that is retrained?\n\nThe gains over prior architecture search methods seem to be relatively minor, such as 0.7% accuracy on ImageNet. What would be your argument against this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1064/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1064/Reviewer_nwwR",
                        "ICLR.cc/2024/Conference/Submission1064/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1064/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698784321743,
            "cdate": 1698784321743,
            "tmdate": 1700596451379,
            "mdate": 1700596451379,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "cmMiDjrsFj",
                "forum": "lQhh1sbfzp",
                "replyto": "seAPIHqQa8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1064/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1064/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nwwR (Part 1/3)"
                    },
                    "comment": {
                        "value": "Dear reviewer:\n\nThanks for your helpful comments. Here are our responses to your comments.\n\n> W1: \"In Section 3.2 the authors mention that this method bears some resemblance to pruning works, \"Our DMS follows a pipeline similar to training-based model pruning.\" This implies that the model should be compared to pruning based methods in experiments. However, the comparisons appear to be made to NAS methods, such as JointPruning. A comparison to state of the art sparse methods, such as RIGL would make the experiments more robust.\"\n\nThanks for your suggestion. We compared our method with SOTA structure pruning methods in our original paper, detailed in Appendix A.4. Our method also outperforms SOTA structure pruning methods. \n\nThere are two types of pruning methods: unstructural pruning and structural pruning. Unstructural pruning methods, such as RIGL, prune the model by setting the weights to zero. Structural pruning methods, such as our method, prune the model by removing the whole dimensions of the weight tensor. The two types of pruning methods have different pruning granularity. Unstructural pruning needs special hardware support to achieve acceleration, while structural pruning can be accelerated by general hardware. Therefore, RIGL can not be compared with our method directly. Please refer to [1] for more details.\n\n\nBesides, we want to talk about the boundary between structure pruning and NAS. In fact, they are not two independent fields. The classic NAS method, darts [2], is very similar to pruning, and the pruning work, EagleEye[3], also searches model architectures.\nOur method can be interpreted as a structure pruning method and a NAS method simultaneously. Propose it as a nas method because we want to emphasize our method's high structure search efficiency rather than discovering better initialization from pretrained models.\n\n[1] Li, Hao, et al. \"Pruning filters for efficient convnets.\" arXiv preprint arXiv:1608.08710 (2016).  \n[2] Liu, Hanxiao, Karen Simonyan, and Yiming Yang. \"Darts: Differentiable architecture search.\" arXiv preprint arXiv:1806.09055 (2018).  \n[3] Li, Bailin, et al. \"Eagleeye: Fast sub-net evaluation for efficient neural network pruning.\" Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part II 16. Springer International Publishing, 2020.  \n\n\n> W2: \"The function they have constructed for optimization is smooth but saturates outside of the active regions illustrated in Figure 2. This may cause vanishing gradient information. Any experiment to investigate whether this happens during training, or why it doesn't happen would be valuable.\"\n\nThrough our deduction and observation, we think gradient vanishing does not happen in our method. We show our deduction as follows.\n\nFigure 2 illustrates the gradient of $a$ with respect to element mask $m_i$. Here, we denote it as $\\frac{\\partial m_i}{\\partial a}$. It's indeed that $\\frac{\\partial m_i}{\\partial a}$ may equal to 0 when $m_i$ is close to 0 or 1. But the gradient of $a$ with respect to the $task\\_loss$ is $\\sum_{i=0}^{N}{\\frac{\\partial task \\_loss}{\\partial m_i}\\frac{\\partial m_i}{\\partial a}}$. As shown in Figure 2, $\\frac{\\partial m_i}{\\partial a}$ will not be 0 for all $i \\in [0,N]$. Therefore, the gradient of $a$ has a low probability of being 0.\n\nBesides, we also do not observe gradient vanishing in our experiments. Therefore, we think gradient vanishing does not happen in our method.\n\n\n> W3: \"The relationship between resource constraint and the top-k parameters is in the appendix but it's extremely important to the overall algorithm.\"\n\nWe apologize for the inconvenience. As many papers have used the resource constraint[1,2], we move the section about the resource constraint to the appendix when the paper is too long.\n\nThanks for your advice. We will consider moving the section to the main paper.\n\n[1] Li, Yanyu, et al. \"Pruning-as-Search: Efficient Neural Architecture Search via Channel Pruning and Structural Reparameterization.\"  \n[2] Gao, Shangqian, et al. \"Disentangled differentiable network pruning.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.\n\n\n> W4: \"The top-k operator as described leads the reader to assume would be fixed but in practice it's not constrained and can be any value. Really it's just a binary mask that has a soft constraint to sum to a low enough value to meet the resource constraints.\"\n\nWe are sorry for the confusion. We agree that the 'k' is usually fixed.\nThe reason we use the term \"top-k\" is because we just follow prior work. For example, [1] uses \"soft top-k\" in their method with an unfixed 'k'.\nWe will consider more carefully when using the term \"top-k\" in the future.\n\n[1] Gao, Shangqian, et al. \"Disentangled differentiable network pruning.\" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1064/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700126812456,
                "cdate": 1700126812456,
                "tmdate": 1700126812456,
                "mdate": 1700126812456,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "aobqjtscLn",
                "forum": "lQhh1sbfzp",
                "replyto": "seAPIHqQa8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1064/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1064/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nwwR (Part 2/3)"
                    },
                    "comment": {
                        "value": "> Q1: \"How sensitive is the method to the element importance measure $a$? It's computed as a moving average with a specific hyperparameter. It seems like the gradient estimate of $a$ depends on this being stable.\"\n\nYes, the moving average is used to make the training more stable.\nHere, we provide an ablation study of the element importance measure.\n\n| Measure Method | Top-1 |\n|----------------|-------|\n| Index | 72.3 |\n| Taylor importance without moving average | 72.5 |\n| Taylor importance with moving average | 73.1 |\n\n\"Index\" means assigning an importance value to each element according to its index in the sequence statically. For example, we assign 0 to the first element, 1.0 to the last.\nAs shown in the above Table, the Taylor importance and the moving average improve the performance.\nAs Taylor importance computes importance by mini-batch, the moving average is used to make the importance measure more stable, also making the gradient computation of $a$ more stable.\n\nWe also added the ablation study to our revised paper in Appendix A.5.3.\n\n\n> Q6: \"In Section 3.2 you say \"Compared with training-based pruning, our method eliminates the need for time-consuming pretraining since we think searching from scratch is more efficient thatn from a pretrained model...\". How does that save resources? Typically pretrained models are available for free, but training one yourself from scratch is extremely expensive?\"\n\nHere, we compare the pipeline of our method with standard pruning methods as follows.\n| Method | Pruning | Ours |\n|--------|---------|------|\n| Stage 1 | Pretrain | No Need |\n| Stage 2 | Prune | Search |\n| Stage 3 | Fintune | Retrain |\n\nWe guess that you may think the fintune stage of pruning methods is more efficient than the Retrain stage (train from scratch) of our method. However, most structural pruning methods directly use the same training setting as pretraining in fintuning stage [1,2]. Because this setting achieves the best performance.\nTherefore, our method is more efficient by omitting the most resource-consuming pretraining stage and does not introduce any extra cost.\n\nBesides, the pretrained models are not free but expensive. They are free only because someone has paid for them and shared them with us.\nImagine two situations:\n\n1. We want to search for a model with a big private dataset.\n2. We want to design a new model architecture.\n\nIn both situations, we can not find any free pretrained model.\n\nHere, we provide the performance and search cost comparison between searching with pretrained models and without pretrained models.\n\n| Supernet | $Iinit_{search}$ | $cost_{pretrain}$ | $cost_{search}$ | $cost_{total}$ | Top-1 |\n|----------|------------------|-------------------|-----------------|----------------|-------|\n| ResNet-50 | Random | 0 | 41 | 41 | 73.1 |\n| ResNet-50 | Pretrain | 410 | 41 | 451 | 73.8 |\n| ResNet-152 | Random | 0 | 116 | 116 | **74.6** |\n\n(The unit of cost is $G MACs \\times Epochs$)  \nSearching without pretrained models achieves 0.8\\% higher top-1 accuracy and only uses nearly 1/4 of the total cost of searching with pretrained models.\nAs a result, we think our method is more efficient than standard pruning methods in both performance and cost.\n\n[1] Li, Bailin, et al. \"Eagleeye: Fast sub-net evaluation for efficient neural network pruning.\" Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part II 16. Springer International Publishing, 2020.  \n[2] Wang, Huan, and Yun Fu. \"Trainability preserving neural structured pruning.\" arXiv preprint arXiv:2207.12534 (2022).\n\n> Q7 \"I don't understand what Table 4 means because the rows and columns refer to search and retraining, but one can't have both a pretrained model and a model that is retrained?\"\n\nWe apologize for the confusion. We update the table as follows:\n| Supernet | $Iinit_{search}$ | $Init_{retrain}$ | $cost_{pretrain}$ | $cost_{search}$ | $cost_{total}$ | Top-1 |\n|----------|------------------|------------------|-------------------|-----------------|----------------|-------|\n| ResNet-50 | Random | Random | 0 | 41 | 41 | 72.6 |\n| ResNet-50 | Pretrained | Random | 410 | 41 | 451 | 72.5 |\n| ResNet-50 | Random | Searched | 0 | 41 | 41 | 73.1 |\n| ResNet-50 | Pretrained | Searched | 410 | 41 | 451 | 73.8 |\n\nThis table shows different initialization schemes for the search stage and retrain stage. The search stage searches a structure, and then the structure is retrained. Hence, in the retrain stage, the model is initialized randomly or with the weights from the search stage. So there is no pretrained initialization in the retrain stage."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1064/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700126850494,
                "cdate": 1700126850494,
                "tmdate": 1700126850494,
                "mdate": 1700126850494,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "dEojD01zl9",
                "forum": "lQhh1sbfzp",
                "replyto": "seAPIHqQa8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1064/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1064/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nwwR (Part 3/3)"
                    },
                    "comment": {
                        "value": "> Q8 \"The gains over prior architecture search methods seem to be relatively minor, such as 0.7% accuracy on ImageNet. What would be your argument against this?\"\n\nThere are two reasons.\n1. The comparison is unfair, as Zico uses a stronger training script, such as distillation and mix-up. Therefore, the real improvement should be larger.\n2.  Our main target is not to provide SOTA ImageNet performance but to provide a general and flexible architecture search method.  Our method is almost compatible with most network architectures and training processes. Therefore, we only change the depth and width of our baselines and do not use any tricks to improve performance. For our method, improving various tasks stably with limited search costs and limited hyperparameter tuning is more critical, as it is more important for real-world applications. Note, as far as we know, we are the first method that achieves 2.0% performance improvement on yolo-v8, a high-performance and lightweight model.\n\nTherefore, we think 0.7% improvement is not minor.\n\n\nThank you for your comments again. We are looking forward to hearing more feedback from you. \nIf you have any further questions, please feel free to contact us. We are active until the rebuttal deadline.\nBesides, we want to emphasize the value of our work. Our method is novel and effective, and it outperforms the state-of-the-art NAS methods using much fewer resources. It pushes the boundary of NAS and makes it more practical. Our work is valuable and deserves to be introduced to the community.\nWould you consider raising the score if our responses have resolved your concerns? Thank you!"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1064/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700126869570,
                "cdate": 1700126869570,
                "tmdate": 1700126869570,
                "mdate": 1700126869570,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qfMsDnZpir",
                "forum": "lQhh1sbfzp",
                "replyto": "seAPIHqQa8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1064/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1064/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sincerely expecting feedback from reviewer nwwR."
                    },
                    "comment": {
                        "value": "Dear reviewer:\n\nThanks for your constructive comments. We have posted our responses to your comments. We expect your feedback about whether our responses address your concerns, or if you have any further questions. We are glad to answer them and improve our paper.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1064/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547508327,
                "cdate": 1700547508327,
                "tmdate": 1700547508327,
                "mdate": 1700547508327,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WoM1cU9tOs",
                "forum": "lQhh1sbfzp",
                "replyto": "seAPIHqQa8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1064/Reviewer_nwwR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1064/Reviewer_nwwR"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal Response"
                    },
                    "comment": {
                        "value": "I apologise for not replying sooner.\n\nW1: I missed Appendix A.4 when reading this paper for the first time. It does appear that the method compares well in these experiments.\n\nW2: Figure 2 does show regions of zero gradient. Other methods that use this kind of relaxation add noise (most famously Gumbel softmax) to smooth out the gradient (with noise sometimes it will land in the high gradient region). This appears to involve a fixed mask, perhaps the noise from SGD is enough to maintain gradient information. If, as you say, \"we also do not observe gradient vanishing in our experiments\" then it may be worthwhile adding those results, for example a gradient hook showing the norm of the gradient passing through the soft mask versus the gradient norm due to the resource loss would be valuable. It seems like the method requires these two parts to be balanced carefully via $\\lambda_{resource}$.\n\nQ1: I find it concernging that the Taylor importance is not critical to the performance of the method. Why would it still work within 1% accuracy without it? The importance signal tells the model which parts are important. Is the method in this analogous to [Nested Dropout](https://proceedings.mlr.press/v32/rippel14.html), ie the network reacts to the importance of different elements prescribed by the mask and not the other way around?\n\nQ6: I don't understand how you can end up with a trained model here without training it; you're comparing pretrained models to your search method and the cost is 116 for the larger model using your search method? And it performs better? I apologize I don't understand what is happening here and I will update my confidence in my review.\n\nQ7: See response to response Q6 above.\n\nQ8: Unfortunately, I can't conclude that the method would have worked better if it had been trained differently. It would be necessary to replicate ZiCo and train it without the extra tricks they used for a fair comparison."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1064/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700596418421,
                "cdate": 1700596418421,
                "tmdate": 1700596915971,
                "mdate": 1700596915971,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4KSZ70Ab45",
            "forum": "lQhh1sbfzp",
            "replyto": "lQhh1sbfzp",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1064/Reviewer_AHK9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1064/Reviewer_AHK9"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces Differential Model Scaling (DMS) to increase the efficiency of width and depth search in networks. The differential top-k introduces by the method to model structural hyperparameters in direct and differentiable manner lays the foundation of this approach. The method is evaluated fairly exhaustively on different image classification architectures like EfficientNet-B0, DeiT on ImageNet. Furthermore the method is also evaluated on myriad tasks like object detection on COCO and language modelling (with Llama-7B model). The proposed method achieves significant improvements over different NAS baselines and some handcrafted architectures."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The approach presented is very novel and well motivated. \n- Experimental evaluation (across different scales, applications, model variants) is exhaustive. The paper also ablates the initialisation scheme of the architecture thoroughly. The search time comparison between different methods is also provided, thus showing the compute savings of the method. \n- The presentation is clear and the paper is well written.\n- The contribution of the paper is very significant especially since it scales NAS methods to realistic search spaces."
                },
                "weaknesses": {
                    "value": "- Search time comparison in some cases seems unfair/confusing (refer to questions)\n- Since the problem is cast as a NAS problem the search spaces used are not the ones very traditional to NAS (refer to questions)\n- The search needs to be repeated for every resource constraint and obtaining a Pareto-Front of objectives might be very expensive (unlike methods like OFA[1]  which directly approximate the whole Pareto-front)"
                },
                "questions": {
                    "value": "- Search time comparison -> Since the observation from section 5 show that initialization from pre-trained models is very useful for differential scaling, did the authors include this in the search time computation. If a method relies on pre-trained models, then ideally the pre-training cost is a part of the total cost incurred. Could the authors clarify the intialization scheme used in each of the tables ie. table 1,2,3.\n- In the appendix the authors compare with one-shot methods like OFA [1] . The comparison in my opinion is unfair since the search is performed on different search spaces. Could the authors evaluated the method on the exact same search space as OFA? This would help differentiable the gains of the search-space v/s the method itself? Similarly  could a comparison be made with the AutoFormer [2] by evaluating the method on its exact search space [2]?\n\nI am willing to increase my score if my concerns are addressed as I believe this is a very interesting and impactful work. \n\n[1] Cai, H., Gan, C., Wang, T., Zhang, Z. and Han, S., 2019. Once-for-all: Train one network and specialize it for efficient deployment. arXiv preprint arXiv:1908.09791.\n\n[2]Chen, M., Peng, H., Fu, J. and Ling, H., 2021. Autoformer: Searching transformers for visual recognition. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 12270-12280)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1064/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1064/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1064/Reviewer_AHK9"
                    ]
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1064/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698917950200,
            "cdate": 1698917950200,
            "tmdate": 1699636032562,
            "mdate": 1699636032562,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5fqR4jQMmc",
                "forum": "lQhh1sbfzp",
                "replyto": "4KSZ70Ab45",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1064/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1064/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to  Reviewer AHK9"
                    },
                    "comment": {
                        "value": "Dear reviewer:\n\nThanks for your nice comments and suggestions. We have revised the paper according to your suggestions. The following are our responses to your comments.\n\n> W1: \"Search time comparison in some cases seems unfair/confusing (refer to questions)\"  \n> Q1: \"Search time comparison -> Since the observation from section 5 show that initialization from pre-trained models is very useful for differential scaling, did the authors include this in the search time computation. If a method relies on pre-trained models, then ideally the pre-training cost is a part of the total cost incurred. Could the authors clarify the intialization scheme used in each of the tables ie. table 1,2,3.\"\n\nWe are sorry for the confusion. \nTo clarify, we did not load pretrained models by default. We only load pretrained models in Table 3 (LLM experiments), Table 4 (ablation study), and Table 8 (comparison with other pruning methods). Therefore, we did not include the pretraining cost in Table 1 for our method as we did not use pretraining, and we think the comparison is fair for other methods.\nWe chose this setting in experiments because, based on our ablation study, we found searching from a large, randomly initialized supernet reaches higher accuracy but lower cost than searching from a small pretrained supernet. Details are in the table below.\n\n| Supernet | $Iinit_{search}$ | $cost_{pretrain}$ | $cost_{search}$ | $cost_{total}$ | Top-1 |\n|----------|------------------|-------------------|-----------------|----------------|-------|\n| ResNet-50 | Random | 0 | 41 | 41 | 73.1 |\n| ResNet-50 | Pretrain | 410 | 41 | 451 | 73.8 |\n| ResNet-152 | Random | 0 | 116 | 116 | **74.6** |\n\nWe have updated this table to section 5 to make our conclusion more clear.\n\n> W2: \"Since the problem is cast as a NAS problem the search spaces used are not the ones very traditional to NAS (refer to questions)\"  \n> Q2: \"In the appendix the authors compare with one-shot methods like OFA [1] . The comparison in my opinion is unfair since the search is performed on different search spaces. Could the authors evaluated the method on the exact same search space as OFA? This would help differentiable the gains of the search-space v/s the method itself? Similarly could a comparison be made with the AutoFormer [2] by evaluating the method on its exact search space [2]?\"\n\nThere are two main differences between the search space of our method and that of OFA and Autoformer.\n\n1. What is more fine-grained in OFA's search space: We (and Autoformer) only search for the depth and width of networks, while OFA also searches for the resolution and kernel size. This is because we want to build a general method that can be applied to NLP and CV tasks. The resolution and kernel size are not necessary for NLP tasks.\n2. What is more fine-grained in our search space: We can search width and depth with step 1 due to our high search efficiency. However, OFA and Autoformer have to use large steps and min-max limitations to compress their search space. This is an advantage of our method, as our search space is easier to define and much larger.\n\nOur current submission uses the default setting directly, causing different search spaces. To make the comparison more fair, we select Autoformer-T as our baseline and use the exact same supernet and search space from Autoformer-T. The performance and estimated search cost are shown below.\n\n| Method | Search Space | MACs | Top-1 | $Cost_{search}$ |\n|--------|--------------|------|-------|-----------------|\n| Autoformer-T | AutoFormer | 1.3 G | 74.7 | > 25 GPU days |\n| DMS | Autorormer | 1.3 G | **75.2** | 2 GPU days |\n\nUsing Autoformer's search space, our method outperforms Autoformer-T by 0.5%  with less than 10% of the search cost. It proves the gain of our search method is from itself.\n\nWe have also added this ablation study to Appendix A.5.2 in our revised paper.\n\n> W3: The search needs to be repeated for every resource constraint and obtaining a Pareto-Front of objectives might be very expensive (unlike methods like OFA[1] which directly approximate the whole Pareto-front).\n\nYes, we agree with your opinion. Our method is not suitable for searching for the Pareto-Front.\n\nHowever, we want to emphasize our target is to build a general and flexible NAS method. There are several situations where our method is more suitable than OFA.\n\n1. Searching for a small number of models, which is the usual case in the real world. \n2. Searching extremely large models. For example, we prepare to train an LLM with 100B parameters using all our GPUs for 30 days. It's impossible to train a supernet (by OneShot NAS, like OFA), which needs much over 30 days, such as 60 or even 300 days. But using much less than 30 days, such as 1 day or 3 days , to search for it with our method is acceptable.\n\nLast, thank you for your helpful reviews! Looking forward to hearing back from you!"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1064/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700126520630,
                "cdate": 1700126520630,
                "tmdate": 1700126520630,
                "mdate": 1700126520630,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MAb4E20whq",
                "forum": "lQhh1sbfzp",
                "replyto": "4KSZ70Ab45",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1064/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1064/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Sincerely expecting feedback from reviewer AHK9."
                    },
                    "comment": {
                        "value": "Dear reviewer:\n\nThanks for your constructive comments. We have posted our responses to your comments. We expect your feedback about whether our responses address your concerns, or if you have any further questions. We are glad to answer them and improve our paper.\n\nBest,\n\nAuthors"
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1064/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700547469925,
                "cdate": 1700547469925,
                "tmdate": 1700547469925,
                "mdate": 1700547469925,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]