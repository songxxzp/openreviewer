[
    {
        "title": "BIRB: A Generalization Benchmark for Information Retrieval in Bioacoustics"
    },
    {
        "review": {
            "id": "tXQ71Bq1Sg",
            "forum": "ybiwT2yP1c",
            "replyto": "ybiwT2yP1c",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6091/Reviewer_sjxb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6091/Reviewer_sjxb"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a benchmark (BIRB) centered on the retrieval of bird vocalizations. It contains multiple passively-recorded datasets and a baseline system for these tasks are proposed. The benchmark aims for the direction of ML models' robustness and generalization ability."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper proposes a new series of datasets, covering various aspects of challenging ML problems, especially in the field of acoustic data shift.\n2. A pipeline for performance evaluation is also proposed. The authors find that increasing the size of networks does not translate to improved generalization, which is an interesting phenomenon. \n3. Various models are tested and benchmarked on the datasets."
                },
                "weaknesses": {
                    "value": "While the topic of this paper is intriguing, I have some concerns that lowers my score:\n1. The paper claimed the finding of \"increasing the size of networks does not translate to improved generalization\". However, no reason for this phenomenon is given.\n2. Many evaluation datasets are proposed. Does each of these datasets emphasize something (e.g. Label shift influence, long-tail distribution)? If not, they lack of research potential to be set as a part of benchmark dataset. A good example for constructing evaluation set would be ImageNet-C[1].\n3. Throughout the paper, no example dataset sample is provided to give intuition of how the dataset is constructed.\n\n[1] Benchmarking Neural Network Robustness to Common Corruptions and Perturbations. ICLR 2019"
                },
                "questions": {
                    "value": "I also have the following questions:\n1. Is it a reasonable practice to use existent public datasets to construct one's own dataset? Are the credits properly given?\n2. What does BIRB stand for? Apologies in advance if I missed it in the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The authors have discussed ethical concerns."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6091/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697667283656,
            "cdate": 1697667283656,
            "tmdate": 1699636656841,
            "mdate": 1699636656841,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Gth22ndmJH",
                "forum": "ybiwT2yP1c",
                "replyto": "tXQ71Bq1Sg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6091/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6091/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sjxb"
                    },
                    "comment": {
                        "value": "We thank Reviwer sjxb for their feedback. We hope the following addresses the weaknesses and questions raised in their review.\n\n> The paper claimed the finding of \"increasing the size of networks does not translate to improved generalization\". However, no reason for this phenomenon is given.\n\nThis is a good question. Prior works have also made similar observations (for instance \"A Unified Few-Shot Classification Benchmark to Compare Transfer and Meta Learning Approaches\"). Further investigation is necessary, but our intuition is that either a) scaling up capacity is not an effective strategy in challenging transfer tasks, or b) due to class imbalance, we are in a medium-to-low data regime for a significant proportion of the species that is not supportive of larger network architectures.\n\nThis finding highlights the importance of our objective of designing a benchmark that is informed by a realistic problem setting. Standard academic benchmarks are akin to unit tests for deep learning approaches: methods developed using those benchmarks can and do overlook important failure cases in real-world applications, which can only be uncovered through \"integration test\" benchmarks like BIRB.\n\nWe will incorporate this discussion in the updated manuscript.\n\n> Does each of these datasets emphasize something (e.g. Label shift influence, long-tail distribution)? If not, they lack of research potential to be set as a part of benchmark dataset.\n\nEach of the datasets maps to one of the tasks we describe in Section 4.3, each of which aligns to one or more challenges that we measure by evaluating on that dataset. While the alignment is not necessarily one-to-one, the datasets are diversified across geographical locations and species distributions, which helps establish generalizability in the benchmark results. We also note that the \"Disentangling Label and Covariate Shift\" subsection in Section 4.3 does present ablations that directly disentangles the various learning challenges presented by BIRB. The methodology we present there can be followed by users of the benchmark to study the effect of those challenges in isolation.\n\n\"Integration\" benchmarks like BIRB are important to deep learning researchers for the reasons outlined in our response to Weakness 1. Our benchmark is intended to drive fundamental research in a way that is directly beneficial to ecological researchers and conservationists, whose modelling challenges do not neatly decompose into different research subfields. Transfer learning, domain adaptation/generalization, and class-imbalanced learning applications all have the opportunity to demonstrate effectiveness on BIRB, and we believe that the fact that such advances can directly translate to real-world impact will be encouraging to many researchers in those fields.\n\n> Throughout the paper, no example dataset sample is provided to give intuition of how the dataset is constructed.\n\nTo clarify, are you referring to an example as it goes through the data processing pipeline, or do you mean more explicitly the modality/format of an example? We'd be glad to provide a more explicit description and/or additional modality to illustrate this more clearly--please let us know which component you were referring to."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700163529308,
                "cdate": 1700163529308,
                "tmdate": 1700163529308,
                "mdate": 1700163529308,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DvgvCXBPPH",
                "forum": "ybiwT2yP1c",
                "replyto": "tXQ71Bq1Sg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6091/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6091/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sjxb"
                    },
                    "comment": {
                        "value": "Regarding the two questions raised in the review:\n\n> Is it a reasonable practice to use existent public datasets to construct one's own dataset? Are the credits properly given?\n\nThere is precedent in constructing \"datasets of datasets\" benchmarks; refer for instance to \"A Large-scale Study of Representation Learning with the Visual Task Adaptation Benchmark\" or \"Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples\". We agree that proper credit attribution is important in those cases, which we do through citations in Table 1. We will clarify in the text that we are making use of data collected as part of previous works to help avoid any ambiguity, and we are happy to incorporate any further suggestion on how to make that clear. \n\nWe also note that the datasets we employ are designed for ecologically themed research and are not usable off-the-shelf in machine learning applications. They do not prescribe any specific evaluation procedure; the evaluation protocol we propose is our own contribution. The datasets were also prepared by a variety of different groups, which required significant effort on our part to achieve data consistency across datasets. Preprocessing details (for reproducibility) are outlined in Appendix 1, which we summarize here:\n\n- Resolving significant differences in the bird taxonomies used by various datasets\n- Correctly and consistently aligning labels in various input formats\n- Extracting fixed length slices from file-level labels/annotations using peak-finding (Xeno-Canto queries)\n- Converting timeboxed annotations in soundscape data into fixed length labeled slices\n\n> What does BIRB stand for? Apologies in advance if I missed it in the paper.\n\nThank you for pointing this out. We will clarify by expanding the acronym (Benchmark for Information Retrieval in Bioacoustics) the first time it is used in the main text. We will also add a footnote to an online article (https://www.audubon.org/news/when-bird-birb-extremely-important-guide) to provide context on the origin of the word."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700163693058,
                "cdate": 1700163693058,
                "tmdate": 1700163693058,
                "mdate": 1700163693058,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "krtOcBUwso",
                "forum": "ybiwT2yP1c",
                "replyto": "Gth22ndmJH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6091/Reviewer_sjxb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6091/Reviewer_sjxb"
                ],
                "content": {
                    "title": {
                        "value": "Comment"
                    },
                    "comment": {
                        "value": "Thank the authors for their detailed responses and I really appreciated it. The response has fully resolved my concerns in: \n1. The usage (intention) of the constructed test sets. \n2. The validity of the proposed dataset.\n3. The full name for BIRB.\n\nFor the third weakness, I meant the modality/format of an example. Sorry for the ambiguity.\n\nThe only left concern is the presentation of the paper (also mentioned by other reviewers), and could be improved from the following aspects (from my opinion):\n1. Careful claims: Every conclusion should be well supported explicitly, especially with the prerequisite \"we find\". \n2. Better introduction to the research topic.\n3. A brief guide for benchmark usage. Since one of the paper's goal is to encourage and inspire machine learning & deep learning researchers to look into the field of bioacoustics, this would be important.\n\nGiven the current version of the paper, I am keeping my score for now. However, I believe this research work could be impactful and I encourage the authors to revise the paper as much as they can."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700192958377,
                "cdate": 1700192958377,
                "tmdate": 1700192958377,
                "mdate": 1700192958377,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "uPan8PGM5W",
            "forum": "ybiwT2yP1c",
            "replyto": "ybiwT2yP1c",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6091/Reviewer_sG4k"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6091/Reviewer_sG4k"
            ],
            "content": {
                "summary": {
                    "value": "This paper is benchmarking a much more complicated evaluation scenario for modern machine learning algorithm. Specifically, it designs a information retrieval task based on bird vocalizations. Several existing public datasets are involved to create this benchmark. Accordingly, the author also provide several baselines for this benchmark. The machine learning generalization ability and robustness property are expected to be evaluated based on this real-world fashion benchmark."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The research topic is attractive to me. Considering a real practical scenario for current machine learning field is reasonable and necessary.\n\n2. The collected datasets are sufficient and provided baselines cover some recent popular methods."
                },
                "weaknesses": {
                    "value": "I mainly concern the presentation of this paper. Given the current machine learning field not familiar with the bird-based bioacustics, more background information should be included before introducing the benchmark. Similarly, the benchmark itself is also unclear. The task should be better to illustrate with a figure and the figure of baseline system is not informative. Some relevant information in supplementary may be moved into the main draft to elaborate the benchmark. In addition, for the experimental analysis, corresponding discussion and visualizations are necessary for a better description."
                },
                "questions": {
                    "value": "Please refer to the weakness for my concerns and most of the paper format should be improved for a better illustration to readers. I recognize the research contribution of this paper but current draft is a bit unclear for a good publication. The author may want to significantly revise the paper draft to clarify this benchmark work."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6091/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698719407248,
            "cdate": 1698719407248,
            "tmdate": 1699636656724,
            "mdate": 1699636656724,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aFALwdfzFx",
                "forum": "ybiwT2yP1c",
                "replyto": "uPan8PGM5W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6091/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6091/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sG4k"
                    },
                    "comment": {
                        "value": "We thank Reviewer sG4k for their feedback! We address some concerns and include follow-up questions so we can better understand the concerns.\n\n> Similarly, the benchmark itself is also unclear. The task should be better to illustrate with a figure and the figure of baseline system is not informative. \n\nWould you be able to clarify the aspects which are unclear so we can address these either with clarifications in our response or improve the presentation of the paper? We care about accessibility of this system and reader understanding!\n\n> Some relevant information in supplementary may be moved into the main draft to elaborate the benchmark. In addition, for the experimental analysis, corresponding discussion and visualizations are necessary for a better description.\n\nAre there any pieces included in the supplementary that you found particularly helpful to highlight in the main paper? For the sake of providing sufficient introduction to the setting, the dataset descriptions, benchmark setup and protocol details, suite of evaluation results and analyses, etc., there is a lot of content and discussion to cover and we\u2019re trying to strike a balance between providing enough information in the main paper and opportunities for the reader to learn more by referencing the Appendix where we elaborate on all of these areas.\n\n> Given the current machine learning field not familiar with the bird-based bioacustics, more background information should be included before introducing the benchmark. \n\nSimilarly to the previous point, if you have any specific suggestions on what you found or would find helpful, please do let us know!"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700076282488,
                "cdate": 1700076282488,
                "tmdate": 1700076282488,
                "mdate": 1700076282488,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "1Rf3CqjYCE",
            "forum": "ybiwT2yP1c",
            "replyto": "ybiwT2yP1c",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6091/Reviewer_N4MM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6091/Reviewer_N4MM"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces BIRB, a benchmark for bioacoustics, addressing challenges in machine learning generalization. BIRB focuses on a retrieval task where models are trained on an upstream dataset and tested on retrieving vocalizations from a different corpus. It assesses out-of-distribution generalization, few-shot learning, and robustness to class imbalances. The benchmark provides a baseline system using a nearest-neighbor search for efficient evaluation. BIRB has practical implications for bioacoustics research and large-scale data processing, offering a real-world and complex evaluation platform for machine learning models in bioacoustics."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper introduces the BIRB benchmark, which is a novel and comprehensive benchmark designed for evaluating model generalization in the field of bioacoustics. The paper's originality lies in its approach to formulating retrieval tasks for bird vocalizations, capturing real-world complexities, and addressing generalization challenges specific to this domain. \n2. Moreover, this work's strength lies in its use of publicly available, high-quality bioacoustic datasets, such as Xeno-Canto and passively collected soundscape datasets. \n3. This paper's primary contribution is in the proposed benchmark, which opens up opportunities for researchers to explore and innovate in the field of bioacoustics. While the proposed method (baseline) is not the central focus, it provides a practical starting point for conducting retrieval tasks within the benchmark."
                },
                "weaknesses": {
                    "value": "1. While the paper primarily focuses on introducing the benchmark, it could benefit from more innovative approaches or methodologies for retrieval tasks. I think it's important to inspire researchers with novel ideas for addressing the challenges in bioacoustics, beyond providing a baseline model.\n2. The paper could be enhanced by a more extensive comparative analysis of different approaches or models for the tasks presented in the benchmark. Currently, it provides results for a set of baseline models but does not explore alternative methodologies.  \n3. This work provides detailed results but could improve the interpretation of these results. For instance, it mentions that there's a significant difference in performance between deep models trained on XC upstream data and models pre-trained on AudioSet, but it doesn't delve into the reasons behind this discrepancy or suggest potential solutions.\n4. The paper employs ROC-AUC as its primary evaluation metric. While this is common in information retrieval tasks, it would be beneficial to consider additional evaluation metrics that are specific to bioacoustics and relevant to the benchmark's objectives."
                },
                "questions": {
                    "value": "1.  The paper mentions performance differences between models pre-trained on AudioSet and those trained on XC upstream data. Could the authors offer more insights into why this divergence occurs and what it might imply for domain adaptation in bioacoustics?\n2. Are there any specific experiments, ablations, or investigations that the authors plan to conduct in the future with the benchmark, beyond the preliminary baseline models presented in this paper?\n3. Are there specific implications for domain adaptation, representation learning, or transfer learning from the challenges in bioacoustics and the proposed benchmark?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6091/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698754159587,
            "cdate": 1698754159587,
            "tmdate": 1699636656621,
            "mdate": 1699636656621,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SDPBk8oOD5",
                "forum": "ybiwT2yP1c",
                "replyto": "1Rf3CqjYCE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6091/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6091/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer N4MM"
                    },
                    "comment": {
                        "value": "We thank Reviewer N4MM for their thoughtful feedback and questions! We address these concerns and questions.\n\n> [The benchmark] could benefit from more innovative approaches or methodologies for retrieval tasks. I think it's important to inspire researchers with novel ideas for addressing the challenges in bioacoustics, beyond providing a baseline model.\n\nThe ICLR 2024 call for papers explicitly mentions datasets and benchmarks in the list of relevant topics. As such, our contributions in designing an evaluation protocol which closely aligns with the real-world needs of ecological researchers and conservationists and demonstrating that it represents a significant and multi-faceted challenge stand on their own. In the evaluation framework we present, we encapsulate naturally occurring challenges, align these with important ML problem areas, and disentangle the influences of the underlying challenges into a comprehensive suite of tasks. Furthermore, in our initial empirical investigation and evaluation, we characterize the extent to which each provides challenges for the models we benchmark. Lastly, we provide inspiration to researchers by outlining concrete follow-up directions by enumerating our findings and open problems that stem from these. \n\n> The paper could be enhanced by a more extensive comparative analysis of different approaches or models for the tasks presented in the benchmark.\n\nWithin the extensive comparative analysis that we've presented in this work, are there any specific approaches or models that you think would be important to include in providing this first set of comprehensive evaluations, results, and analyses?\n\n> In regards to Weaknesses 3 and Question 1:\n\nA meaningful insight from our work reveals the importance of using an appropriate learned representation in this framework when addressing bioacoustics retrieval. In particular, the model trained on XC upstream is more relevant and applicable to this task, providing a rich feature space that translates well to downstream tasks, much more so than the AudioSet dataset which includes many unrelated or irrelevant acoustics data.\nFor domain adaptation, our conclusion is that we benefit from training on data that is more directly related than data that is less so. More explicitly, bird vocalizations provide a more useful representation for detecting other bird vocalizations than the broad and less relevant classes included in AudioSet.\n\n> [It] would be beneficial to consider additional evaluation metrics that are specific to bioacoustics and relevant to the benchmark's objectives.\n\nROC AUC is also common for bioacoustics tasks; refer for instance to Stowell et al. (2022): \"Performance is measured using standard metrics such as accuracy, precision, recall, F-score, and/or area under the curve (AUC or AUROC)\". Appendix A.3.2 provides a justification for our choice of metric. We will include a statement in the main paper to highlight this point.\n\n> Are there any specific experiments, ablations, or investigations that the authors plan to conduct in the future with the benchmark, beyond the preliminary baseline models presented in this paper?\n\nAs we touched on in response to your concerns in Weaknesses #1, our thorough investigation has provided meaningful open problems (in the context of foundational ML research and bioacoustics-related fields) and future work within this setting, as we have outlined in Section 6: Conclusion. Our group continues to explore problems within this space and are particularly interested in building self-supervised approaches that leverage large quantities of unlabeled and in-domain data.\n\n> Are there specific implications for domain adaptation, representation learning, or transfer learning from the challenges in bioacoustics and the proposed benchmark?\n\nImplications include:\n1. Using a relevant and general dataset for learning a rich and transferable feature space is a powerful approach. This is apparent from our comparison between XC models and AudioSet pre-trained models.\n2. There is value in combining the baseline approaches we presented with domain adaptation techniques in building robust and generalizable retrieval models. This motivates the continued exploration of the intersection between domain adaptation, transfer learning, and bioacoustics, given that bioacoustics provides an inherently more complex backdrop than many academic domain adaptation settings.\n3. As we previously stated, bioacoustics represents a rich playground to address fundamental ML questions but is underserved when focusing only on the canonical settings that are often explored, i.e. computer vision and language tasks."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700075655549,
                "cdate": 1700075655549,
                "tmdate": 1700075655549,
                "mdate": 1700075655549,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4NbqVESMwZ",
                "forum": "ybiwT2yP1c",
                "replyto": "kY7Em27PUP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6091/Reviewer_N4MM"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6091/Reviewer_N4MM"
                ],
                "content": {
                    "title": {
                        "value": "Response to the Authors"
                    },
                    "comment": {
                        "value": "Thank you for your responses. I appreciate your efforts to address my comments and questions. However, I still have some reservations about your paper, especially regarding the novelty and significance of your approaches and methodologies. I think your paper would benefit from more innovative and inspiring ideas for tackling the challenges in bioacoustics, beyond providing a benchmark and some baseline models. Therefore, I am not convinced to increase my score at this point. I hope you can accept my feedback and improve your paper in the future."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700661786509,
                "cdate": 1700661786509,
                "tmdate": 1700661786509,
                "mdate": 1700661786509,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gmriqxJ2Hm",
            "forum": "ybiwT2yP1c",
            "replyto": "ybiwT2yP1c",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6091/Reviewer_SRto"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6091/Reviewer_SRto"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a benchmark to measure the generalization capabilities of bird vocalization detection models.\nThe benchmark is composed of existing datasets in the field, one large-scale upstream (i.e. training) dataset and 7 small-scale downstream (evaluation) datasets, which overall evaluate generalization against several challenges, such as domain shift, label shift, limited data and class imbalance.\nModels are trained on the upstream dataset and used as \"embedding models\" (i.e. feature extractors) to solve retrieval tasks using a few labeled instances on the downstream datasets.\n7 recent baseline models are evaluated on this benchmark, including linear models trained on handcrafted features and deep embedding models with EfficientNet, Conformer and Transformer architectures.\nResults show that domain-shift is one of the most challenging generalization factor."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The benchmark is a nice real-world application on measuring generalization in the field of bioacoustics, which could be interesting for practitioners.\n- Evaluation protocol is simple to follow/apply, i.e. models are used as feature extractors and the task is simple retrieval.\n- A decent number of baselines already evaluated in this setting."
                },
                "weaknesses": {
                    "value": "## Concerns on the evaluation protocol\n- The evaluation benchmark retrieves relevant samples from a downstream dataset given a query, and it allows reusing class embeddings for queries learned during training. I'm not sure to get this point. If the label of a query is known, then what is the point of retrieving samples from a dataset? We already know the answer.\n- Regardless, it would be nice to report results where none of the class embeddings from training time is used when evaluating the models. Because, one of the generalization aspects posed by the datasets is domain-shift, and it is not clear how to measure domain-shift when query embeddings come from training dataset (rather then being embedded at test time using downstream dataset images).\n\n## Comments on datasets\n- I wonder if there is a universal class taxonomy which encapsulates all the classes in all the datasets. For instance, in standard vision datasets (like ImageNet, MS-COCO), classes come from different ontologies/granularities and matching those classes is far from being trivial. To measure \"label shift\", it is essential to know which classes have been seen during training vs at test time (i.e. seen and unseen classes).\n- Also, when measuring label shift, what is the semantic relation (due to their granularity) between seen and unseen bird classes? \n- Is it possible to evaluate domain-shift while fixing label-shift? For instance, seen and unseen classes being equal, whereas recordings being focal vs passive. \n\n## Comments on the paper overall\n- Given that this is a benchmark paper, and that not everybody is super familiar with the domain (bioacoustics), I would expect a more direct and clear explanation of the task being solved, the types of input given to the network, etc. In that sense the paper is not easy to understand."
                },
                "questions": {
                    "value": "I would like the authors to address the weaknesses I listed above.\nMy main concerns are related to soundness of the evaluation protocol."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6091/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699019575501,
            "cdate": 1699019575501,
            "tmdate": 1699636656504,
            "mdate": 1699636656504,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "M2frIgxlk6",
                "forum": "ybiwT2yP1c",
                "replyto": "gmriqxJ2Hm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6091/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6091/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer SRto"
                    },
                    "comment": {
                        "value": "We thank Reviewer SRto for their thoughtful feedback! We address the Reviewer\u2019s questions, concerns, and feedback below. \n\n> If the label of a query is known, then what is the point of retrieving samples from a dataset?\n\nThe term \"query\" is used in the \"retrieval\" sense of the word: the query is not the example we wish to label, but rather the example (or handful of examples) which is a representative of a species\u2019 vocalization that we seek to retrieve in the unlabeled corpus. This represents a practical use case where a practitioner might say: \"I have an example of a Canadian goose vocalization (query) and want to find other examples of this species/vocalization type in my unlabeled dataset/field recording (search corpus).\u201d\n\n> It would be nice to report results where none of the class embeddings from training time is used when evaluating the models.\n\nCan you clarify what you mean by class embeddings from training time? \nIf we understand correctly, here is some clarification. No examples (audio) used in evaluation are included at training time. This includes both the query examples and search corpora for each dataset/task. Furthermore, we exclude entire subsets of classes from being presented at training time.\n\n> I wonder if there is a universal class taxonomy which encapsulates all the classes in all the datasets.\n\nIn the empirical evaluation presented in this work, we produce a \"standard\" class taxonomy that encapsulates all the classes across every dataset we evaluate on, which is analogous to what you suggest and describe. It is worth noting that there is no universal taxonomy used across the field and that numerous class taxonomies exist and are updated every 6 or 12 months. Additional details on the topic of class taxonomies are included in the section titled \"Labels & Species Codes\" in Appendix A.3.1.\n\n> To measure \"label shift\", it is essential to know which classes have been seen during training vs at test time (i.e. seen and unseen classes). \n\nWe align with Moreno-Torres et al. (2012)'s definition of label shift (or prior probability shift): a problem for which P(Y) changes between the training and test datasets but P(X | Y) remains the same. In the experiment on disentangling label and covariate shift all classes are seen during training, but their marginal probabilities change in moving from the training set to the \"evaluation (label-shifted)\" set. In that setting there are no classes that are unseen at training time. \n\n> When measuring label shift, what is the semantic relation (due to their granularity) between seen and unseen bird classes?\n\nIs your question in regards to a specific setting that we evaluate, as in Section 4.3 Generalization Results?\nOverall, the consideration of semantic relation is irrelevant/independent when measuring label shift. It is, however, relevant when considering performance on low resource classes (those with minimal representation at training), as in the case of the Artificially Rare task and matters when considering generalization to such classes. Additionally,\n1. The ablation study includes all classes observed (\"seen\") at training time and investigates the impacts of label shift.\n2. There is a component of label shift in the Artificially Rare task, and all bird classes within that collection are included at training time.\n3. In the \"Performance in mixed conditions\" task, there is a combination of bird classes included at training time and those which are not present due to overlap with the Heldout Region sets.\n\n> Is it possible to evaluate domain-shift while fixing label-shift?\n\nAbsolutely! Refer to Figure 3: the \"evaluation (label-shifted)\" dataset is drawn from the same XC dataset as the training set, and therefore is not subject to domain shift, while the \"Pennsylvania, USA\" dataset is subject to domain shift (focal to passive). The \"evaluation (label-shifted)\" dataset's marginal class distribution is constructed specifically so as to match the marginal class distribution of the Pennsylvania dataset.\nComparing the two allows us to assess the residual effect of domain shift while controlling for label shift.\nNote that it's not feasible to instead subsample the Pennsylvania dataset so that its marginal class distribution matches that of XC for two reasons: 1) some classes have a zero marginal probability in the Pennsylvania dataset, and 2) the Pennsylvania dataset is considerably smaller than XC, and evaluation noise would be unacceptably large if we were to subsample it."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700074468672,
                "cdate": 1700074468672,
                "tmdate": 1700074647595,
                "mdate": 1700074647595,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "bqs59icepS",
                "forum": "ybiwT2yP1c",
                "replyto": "gmriqxJ2Hm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6091/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6091/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer SRto"
                    },
                    "comment": {
                        "value": "> Given that this is a benchmark paper, and that not everybody is super familiar with the domain (bioacoustics), I would expect a more direct and clear explanation of the task being solved, the types of input given to the network, etc.\n\nThank you for this feedback, we agree that we want to provide a helpful \"table setting\" for folks who aren't familiar with bioacoustics and provided overview in the Introduction and more in-depth explanation into this setting in the Appendix A.1 Why Avian Bioacoustics and A.2 Related Work. While we do describe in the main paper the base model training and inputs, along with the task description and breakdown thoroughly in the main paper, we can imagine that there would be value in having a high-level summary of all the components in either a figure, table, or brief highlight. Do you think that would help assure the reader has a clear sense of the elements of this framework? We would like to update the main paper to provide a bit more clarity for the reader."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700074493749,
                "cdate": 1700074493749,
                "tmdate": 1700074660459,
                "mdate": 1700074660459,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QDS0r2oOaQ",
                "forum": "ybiwT2yP1c",
                "replyto": "ARSWmqg5rw",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6091/Reviewer_SRto"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6091/Reviewer_SRto"
                ],
                "content": {
                    "title": {
                        "value": "Note on the updated paper"
                    },
                    "comment": {
                        "value": "Thanks for updating the paper. It would have been nice to color all changes / new material in the updated version so that they can easily be tracked."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700707814741,
                "cdate": 1700707814741,
                "tmdate": 1700707814741,
                "mdate": 1700707814741,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "XHOaG4YMBF",
                "forum": "ybiwT2yP1c",
                "replyto": "gmriqxJ2Hm",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6091/Reviewer_SRto"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6091/Reviewer_SRto"
                ],
                "content": {
                    "title": {
                        "value": "Review reply for the response"
                    },
                    "comment": {
                        "value": "Thanks for the response. However, my confusion still remains. For this reason I'd like to keep my initial score.\n\n* As I mentioned in my review,\n> The evaluation benchmark retrieves relevant samples from a downstream dataset given a query, and it allows reusing class embeddings for queries learned during training. I'm not sure to get this point.\nI see no clarification regarding the use of class embeddings for \"known/seen\" classes. How can the identity of a sample (i.e., its class) already be known at test time? It can be predicted of course, but not mentioned in text, if I'm not mistaken.\n\n* I strongly disagree with this argument: \n> Overall, the consideration of semantic relation is irrelevant/independent when measuring label shift\n\nSemantic relation between classes/labels well impact transferability across them."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6091/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700708623686,
                "cdate": 1700708623686,
                "tmdate": 1700708679370,
                "mdate": 1700708679370,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]