[
    {
        "title": "Kalman Filter Online Learning from non-Stationary Data"
    },
    {
        "review": {
            "id": "CLdHsmXnBz",
            "forum": "ZzmKEpze8e",
            "replyto": "ZzmKEpze8e",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5320/Reviewer_VM8e"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5320/Reviewer_VM8e"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies the challenging problem of online continual learning and introduces an approach based on the Kalman filter (KF).  This paper models non-stationary as parameter drift based on linear stochastic dynamics and employs KF to estimate the posterior distribution of adaptable model parameters. Additionally, the paper discusses updates to the forgetting coefficient of KF. The experimental results demonstrate the superior performance of the proposed method compared to previous online continual learning methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper studies a practical and interesting problem: online continual learning, emphasizing non-stationary. It explores the application of  KF in classification problems by updating the forgetting coefficient. \n\nThe proposed method, which utilizes a Kalman filter-based approach for prior and posterior estimation of model parameters, presents a reasonable solution to tackle the (non-stationary)  online learning problem.\n\nOverall, the paper is well-structured and effectively communicates the details of the proposed method.\n\nFurthermore, in the experimental evaluation, the proposed method demonstrates superior performance compared to previous approaches, as reported in the paper."
                },
                "weaknesses": {
                    "value": "1) This paper could benefit from a comparison with related works that utilize the Kalman filter for online continual learning and adaptation tasks, as seen in [1]. Such a comparison or discussion regarding the difference between the proposed method and [1] would enhance the paper's comprehensiveness.\n\n2) The paper assumes that the ground-truth model follows a linear predictor based on features,  followed by adapting the last layer of the neural network using KF. The results in Table 1 suggest that the \"Backbone finetuning\" method outperforms \"No backbone finetuning (Purely linear model).\" This observation raises questions about the validity of the assumption regarding linear stochastic dynamics.  It is essential to engage in an in-depth discussion regarding the correctness and practicality of these assumptions.\n\n3) In the evaluation of CLOC in section 4.2.2, it's crucial to consider the real-time computational cost for each algorithm, as emphasized in [2]: In the evaluation of the proposed method could be a prudent step.\n\n[1] A. Abuduweili, et al, \u201cRobust online model adaptation by extended kalman filter with exponential moving average and dynamic multi-epoch strategy\u201d, L4DC 2020.   \n[2] Bornschein, et al. \u201cSequential learning of neural networks for prequential mdl.\u201d ICLR, 2022."
                },
                "questions": {
                    "value": "1) How does the proposed method compare to related Kalman filter-based techniques for online continual learning and adaptation, such as KF in  [1]?\n\n2) Does the assumption concerning linear stochastic dynamics align with the outcomes presented in Table 1, given that the \"Backbone finetuning\" method outperforms the \"No backbone finetuning (Purely linear model)\"?\n\n3) What are the real-time computational costs associated with each algorithm?\n\n[1] A. Abuduweili, et al, \u201cRobust online model adaptation by extended kalman filter with exponential moving average and dynamic multi-epoch strategy\u201d, L4DC 2020."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5320/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5320/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5320/Reviewer_VM8e"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5320/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698641987634,
            "cdate": 1698641987634,
            "tmdate": 1699636533944,
            "mdate": 1699636533944,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6G2oe0gqSp",
                "forum": "ZzmKEpze8e",
                "replyto": "CLdHsmXnBz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5320/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5320/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to VM8e: part 1"
                    },
                    "comment": {
                        "value": "We would like to thank reviewer VM8e for the feedback. Please find our detailed answer below.\n\n> This paper could benefit from a comparison with related works that utilize the Kalman filter for online continual learning and adaptation tasks, as seen in [1]. Such a comparison or discussion regarding the difference between the proposed method and [1] would enhance the paper's comprehensiveness. How does the proposed method compare to related Kalman filter-based techniques for online continual learning and adaptation, such as KF in [1]?\n\nWe would like to thank the reviewer for suggesting this paper. Another relevant paper is also based on Extended Kalman Filter (EKF) from Chang et al., which we discuss in our paper. The drawback of the paper you proposed and this other EKF work, consists in fact that these approaches scale poorly with problem dimensionality. EKF from Chang et al. has complexity $O(PC^2)$, where P is the number of parameters and C is the number of classes, so it is not  applicable in situations where C is large such as CLOC. Note that our method has complexity $O(mK+m^2)$ on top of online SGD complexity (or $O(PC+P^2)$  using Chang et al notation) so in practice runs as fast as online SGD which has $O(P K)$ cost.  Further, the paper you mentioned in [1], uses an ordinary EKF approach with additional modifications, which scales as $O(K^3+PK^2+P^2K)$. It is easy to see from the algorithm 2, where $H_t$ is the matrix of size $K \\times P$ and $P_t$ is the matrix of size $P \\times P$. Line 2 requires $H_t*P_{t-1}*H_t^T$ multiplication which is done in $O(KP^2+K^2 P)$. After that we need to invert the matrix of size $K \\times K$ which takes $O(K^3)$. Therefore the total running complexity is $O(K^3+PK^2+P^2K)$. For large $K$ the term is dominated by $O(K^3+PK^2)$ and for large $P$ it is dominated by $O(P^2K)$. Moreover, the method requires us to store the matrix $P$ as well as to store (during computations) $H_t$, this results in $O(P^2+KP)$ storage complexity. Note that in Neural Networks settings, P is of the order of millions / billions. Our algorithm requires us to store $O(m^2+Km)$ which corresponds to the last layer covariance matrix and per-class mean vectors. Therefore, our algorithm is significantly cheaper than this variant, both in terms of computation time and memory. Even if we were to apply this approach to the last layer, we would still get a very large complexity of $O(K^3+mK^2+m^2 K)$, which would make this approach less practical than ours. Based on this thinking, comparing to the proposed approach might not be very informative, since we are also primarily interested in scalability. \n\n> The paper assumes that the ground-truth model follows a linear predictor based on features, followed by adapting the last layer of the neural network using KF. The results in Table 1 suggest that the \"Backbone finetuning\" method outperforms \"No backbone finetuning (Purely linear model).\" This observation raises questions about the validity of the assumption regarding linear stochastic dynamics. It is essential to engage in an in-depth discussion regarding the correctness and practicality of these assumptions. Does the assumption concerning linear stochastic dynamics align with the outcomes presented in Table 1, given that the \"Backbone finetuning\" method outperforms the \"No backbone finetuning (Purely linear model)\"?\n\nWe formulate the method assuming this linear model, but later on, we indeed, violate this assumption. Nevertheless, we still use the KF updates assuming the linear model and we see that in practice this works very well. That said, what we are doing is not completely unreasonable. If the model is fully linear, then the KF updates are EXACT, meaning that we have true posterior over the last layer at time t. If the model parameters change (thetas or gammas), the updates stop being exact, but become approximate and we in fact propagate the approximate posterior. Such a setting is well studied and is known as online Bayesian learning. Without explicitly saying that, this is in fact what is happening in our case when the model parameters change. We will add an explicit comment about it in the text."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5320/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678510260,
                "cdate": 1700678510260,
                "tmdate": 1700678510260,
                "mdate": 1700678510260,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "zMohxacymp",
                "forum": "ZzmKEpze8e",
                "replyto": "CLdHsmXnBz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5320/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5320/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to VM8e: part 2"
                    },
                    "comment": {
                        "value": "> In the evaluation of CLOC in section 4.2.2, it's crucial to consider the real-time computational cost for each algorithm, as emphasized in [2]: In the evaluation of the proposed method could be a prudent step. What are the real-time computational costs associated with each algorithm?\n\nThis is an important point of making the model efficient real-time. However, the real-time computational costs are a function of algorithm complexity as well as the efficient implementation. Our algorithm has an additional  $O(m^2)$ computational overhead on top of online SGD, i.e. online SGD in the final layer has cost $O(Km)$ while our method has  $O(Km+m^2)$ where K is the number of classes and m is the dimensionality of the last layer. The term $O(m^2)$ comes from a matrix multiplication needed because Kalman Filter updates a $m \\times m$ covariance matrix over the weights. This is a very small additional overhead on top of the online SGD, and in fact if $K$ is much larger than $m$ the term $O(Km)$ dominates  and the cost of online SGD and our method is roughly the same. We do not aim to provide an efficient implementation of our method for real-time applications, but this could be easily incorporated into an efficient real-time implementation of Online SGD."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5320/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700678522560,
                "cdate": 1700678522560,
                "tmdate": 1700688136186,
                "mdate": 1700688136186,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Pq3UnC9rP8",
            "forum": "ZzmKEpze8e",
            "replyto": "ZzmKEpze8e",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5320/Reviewer_NkkH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5320/Reviewer_NkkH"
            ],
            "content": {
                "summary": {
                    "value": "The authors present an application of online Kalman filter inference within dealing with concept drift in deep learning. There is no theoretical analysis, but the computational results on variously batched and randomized classification datasets (CIFAR-100) and CLOC are promising."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The experiments show a clear improvement over a number of methods that could be seen as the state of the art, incl:\nNo backbone finetuning (Purely linear model)\nBackbone finetuning\nBackbone finetuning with Replay\nOnline SGD\nOnline SGD + Replay\nER++ (Ghunaim et al., 2023).\n\nThe paper is well written, both interms of derivation of the method and explaining the expertiments."
                },
                "weaknesses": {
                    "value": "The improvement over ER++ is marginal. \n\nThe experiments do not test against the best possible Kalman filter (in terms of the system matrices), or some restarted version of the best possible Kalman filter."
                },
                "questions": {
                    "value": "Have you considered extending the work of Kozdoba et al (https://doi.org/10.1609/aaai.v33i01.33014098) showing that Kalman filters can be approximated arbitrarily closely using ARMA models? This could reduce the runtime of O(m^2) to O(d) for recursion depth of d. \n\nHow exactly do you envision to use \"reparametrize the integral to be an expectation under the standard normal and then apply Monte Carlo\"? Plausibly, the scaleability of Monte Carlo could be a constraint? Could you present the statistical performance and runtime of the precomputing and the actual application of the method parametrized by S, as in the O(KS) or O(Km + m^2)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5320/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698689128375,
            "cdate": 1698689128375,
            "tmdate": 1699636533822,
            "mdate": 1699636533822,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mtq8zowWeM",
                "forum": "ZzmKEpze8e",
                "replyto": "Pq3UnC9rP8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5320/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5320/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to NkkH"
                    },
                    "comment": {
                        "value": "We would like to thank reviewer NkkH for feedback. Please find our detailed response below.\n\n> The improvement over ER++ is marginal.\n\nWe would like to highlight that we compared our method to ER++ in two scenarios: stationary CIFAR-100 and CLOC (highly non-stationary classification problem). We saw marginal improvement over ER++ in stationary CIFAR-100, see Table 1. In fact, we didn't see much performance difference across methods. In the text we argued that this is due to the fact that this problem is stationary, therefore there is not much benefit in modeling non-stationarity. After that, we compared our method to ER++ on CLOC, see Figure 3. From this Figure we can see that our method offers a significant improvement over ER++. We hope this clarifies that our method is indeed significantly better than ER++ in the non-stationary regime that we most care about. \n\n> The experiments do not test against the best possible Kalman filter (in terms of the system matrices), or some restarted version of the best possible Kalman filter.\n\nWe are not sure what the reviewer means by \"best possible Kalman filter\". Please note that our paper is not about finding the best possible Kalman Filter nor to do Kalman Filter (KF) research. Our contribution can be summarized as follows. We propose to certain type  of KF on the last layer of the Neural Network, that crucially is fully scalable in the large scale classification application and it allows modeling non-stationarity through online learning of single scalar parameter $\\gamma$.  We propose a procedure to learn non-stationarity parameters online as well as to finetune / learn the Neural Network backbone. Finally, we provide extensive empirical evidence of the effectiveness of our approach in practice.  In contrast to our scalable KF method, other more standard KF methods having more complex systems matrices will not be scalable in our application since typically will have quadratic cost wrt the number of outputs/classes; see also responses to reviewers  BEuY and VM8e.\n\n\n> Have you considered extending the work of Kozdoba et al (https://doi.org/10.1609/aaai.v33i01.33014098) showing that Kalman filters can be approximated arbitrarily closely using ARMA models? This could reduce the runtime of O(m^2) to O(d) for recursion depth of d.\n\nWe would like to thank you for suggesting this paper. It is not yet clear for us how exactly to extend this work to our case. The setting of the suggested papers studies the special class of linear models - ARMA models. In our paper, we work with Neural Networks and use the Kalman Filter in the last layer. On top of that we learn the dynamics parameters (gamma) online and finetune Neural Network representation. This is not entirely clear for us how to connect our paper to this suggested paper. The scope of our paper is not about Kalman Filter research, but rather of using Kalman Filter tools in the large scale non-stationary classification problem.\n\n> How exactly do you envision to use \"reparametrize the integral to be an expectation under the standard normal and then apply Monte Carlo\"? Plausibly, the scaleability of Monte Carlo could be a constraint? Could you present the statistical performance and runtime of the precomputing and the actual application of the method parametrized by S, as in the O(KS) or O(Km + m^2)?\n\nAs we have explained in the paper, see equation 11, the Monte-Carlo approximation is done over 1 dimensional scalars (which are phi(x)^t w) per class. Therefore the MC operations are very cheap. In practice, we found that S = 50 worked very well in all the considered settings. In practice, we found little difference in performance using smaller values for S, and because these operations are very cheap, we are not compute bound and could use even such a high S as 50."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5320/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677527143,
                "cdate": 1700677527143,
                "tmdate": 1700687184443,
                "mdate": 1700687184443,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "OliXpEZMUo",
            "forum": "ZzmKEpze8e",
            "replyto": "ZzmKEpze8e",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5320/Reviewer_reom"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5320/Reviewer_reom"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes an approach to the online learning of regression and classification models using Kalman filtering. The key idea of the paper is to divide the learning into two parts: that of the feature extractor \"backbone\" and the \"last-layer\" representation. The backbone is assumed to be either pretrained and fixed, pretrained and fine-tuned, or trained from scratch using standard optimization approaches. When the backbone is fine-tuned or trained from scratch, the negative log-predictive probability is used as the objective and parameter updates are done using standard SGD steps. The last-layer representation is updated using efficient Kalman filter updates with standard Gaussian assumptions on the emission and transition densities. By allowing the weights on the last-layer representation to evolve as a Gaussian process, with a tuning parameter governing model plasticity (this parameter is shown to be learnable online), the model is able to adapt to \"natural\" (i.e., realistic) distribution shift. It is shown that the proposed hybrid scheme of an SGD-trained feature extractor and Kalman filter updates on the last layer allows the creation of an accurate online classifier in some large sequentially presented data sets."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Online training of neural networks with Kalman filters is not a new idea, but one that has gotten some attention recently with developments aimed at making the procedures more scalable. The present paper makes a good potential addition to this literature by proposing a method to achieve this on practically-sized neural networks for image classification problems. The paper is well-written with comprehensive appendices motivating the various design decisions made. A major strength of the paper are the convincing experimental results and carefully done comparisons. The combination of pre-training (or SGD training) with some plasticity on the parameters on the last-layer representation is, to the best of my knowledge, a novel idea. The use of a Kalman filter on the last layer only allows for efficient updates, as the number of parameters on the last layer is small relative to the rest of the network."
                },
                "weaknesses": {
                    "value": "The paper is primarily focused on learning in the case of having a pre-trained extractor of a representation. Training a representation extractor can require considerable offline computational expenses. Also, having a fixed representation extractor does not ensure plasticity of the weights of the extractor itself, which may be important in cases where there is sufficient heterogeneity in the data over time that this becomes an issue. It is unclear how much of an issue this can be in practice, and therefore a comment, or some justification, should be made on the implicit assumption that plasticity in the last layer is sufficient. \n\nAs presented, the methodology does not allow for a simple incorporation of additional weight blocks into the Kalman filter update, as this breaks the linear Gaussian assumption that the method relies on. This can perhaps be circumvented with a linearization of the observation model. Have the authors considered extensions of their method in this direction? This would lead to the additional question of what weights to learn with SGD and which weights to learn with the Kalman filter (intuitively, the ones requiring the most plasticity), and how to select them."
                },
                "questions": {
                    "value": "Figure 3 is perhaps the most interesting of the paper, and raises the main question that I have: why is the gap between using a pretrained extractor and Kalman filtering and SGD so much greater than between an extractor trained from scratch with Kalman filtering and SGD? Memory costs aside, why does the Kalman filter add no additional performance gain in this situation? I don't expect it to beat or match the pretrained case, of course, but perhaps be in between the SGD and the pretrained case. What is it about the interaction between learning these two parameter sets that causes this? Have the authors tried experimenting to try and improve the from scratch performance of their proposed method to be even a bit better than online SGD? An understanding of this is basically the missing part of the paper for me."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "None."
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5320/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699218698671,
            "cdate": 1699218698671,
            "tmdate": 1699636533736,
            "mdate": 1699636533736,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xKMWfWDGcQ",
                "forum": "ZzmKEpze8e",
                "replyto": "OliXpEZMUo",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5320/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5320/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to reom: part 1"
                    },
                    "comment": {
                        "value": "We would like to thank the reviewer reom for the feedback. Please find our detailed answer below.\n\n> The paper is primarily focused on learning in the case of having a pre-trained extractor of a representation. Training a representation extractor can require considerable offline computational expenses. Also, having a fixed representation extractor does not ensure plasticity of the weights of the extractor itself, which may be important in cases where there is sufficient heterogeneity in the data over time that this becomes an issue. It is unclear how much of an issue this can be in practice, and therefore a comment, or some justification, should be made on the implicit assumption that plasticity in the last layer is sufficient.\n\nYou are absolutely right, having non-stationarity modeled only in the last layer is more restrictive compared to modeling it in all the layers. If we use pre-trained frozen representation and the distribution shift is significant, our approach might fail to capture non-stationarity. The fact that we can update both, representation and the parameter gamma, allows us to be more flexible and capture larger classes of non-stationarity. However, we believe that this only partially mitigates the issue with plasticity. If we were to use this method in settings with extreme plasticity loss, then it is likely that our approach may not work well and more would need to be done with neural network representation. However, we are not framing our method as the solution to the plasticity problem but rather as a practical algorithm to learn in non-stationary classification domains, and in the scenarios we studied the method, it seems to work reasonably well. It would be interesting to study the performance of the method in settings with significant plasticity loss. That's said, we will add a comment about that in the text and we thank you for this useful suggestion.\n\n> As presented, the methodology does not allow for a simple incorporation of additional weight blocks into the Kalman filter update, as this breaks the linear Gaussian assumption that the method relies on. This can perhaps be circumvented with a linearization of the observation model. Have the authors considered extensions of their method in this direction? This would lead to the additional question of what weights to learn with SGD and which weights to learn with the Kalman filter (intuitively, the ones requiring the most plasticity), and how to select them.\n\nThis is a very interesting suggestion. We did think about linearisation of the observation model. In fact, the paper Chang et al. does exactly that. The drawback of their approach is that they lose scalability (their complexity scales as O(P C^2), where P is the number of NN parameters and C is the number of classes) as well as that it introduces additional approximation error due to linearisation. However, what you suggest goes beyond a simple Extended Kalman Filter (EKF) approach. In fact, what you suggest is to have some set of parameters to be used for EKF (if it's not only the last layer) and the other to be used with a normal SGD. It is unclear for us how to best select such subsets and whether it could be done in a tractable  and automatic way, but this gives us a great food for thought. We will think about how to do that in the future work. Nevertheless, we will add a comment about it in the paper."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5320/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677439216,
                "cdate": 1700677439216,
                "tmdate": 1700677439216,
                "mdate": 1700677439216,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "DzwojnaUtK",
            "forum": "ZzmKEpze8e",
            "replyto": "ZzmKEpze8e",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5320/Reviewer_BEuY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5320/Reviewer_BEuY"
            ],
            "content": {
                "summary": {
                    "value": "The article presents a method for adapting a neural network model to a possibly non-stationary stream of data. The method consists of applying a Kalman filter to infer the weights of the last layer of the neural network over a data stream defined by an Online Continual Learning task. In addition, the neural network providing the representation and a parameter setting the level of forgetting in the last layer are adapted online, further adapting to a new problem setting. The method was applied to a simple regression task and two large-scale classification problems in continual learning which used the CIFAR-100 and CLOC datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Impressive experimental results in terms of accuracy metrics. The classification experiments demonstrate that the method can be applied to large-scale high-dimensional tasks and provides clear improvements over previous work.\n- A novel method which clearly differentiates itself from similar recent work through improvements in efficiency and allowing for online adaptation of forgetting parameters\n- The modeling choices were well-motivated, and the discussion in the Related work section on a meaningful setting for continual learning tasks was convincing\n- Section 2.1. was well-written, as it thoroughly explains the details of online learning in the last layer, while also motivating the modeling choices"
                },
                "weaknesses": {
                    "value": "- It was not clear while reading the Related work section how using Kalman Filters in the representation space impacted the results compared to EKF on the full network as in Chang et al. I understood that the method presented here is more efficient, but how is the performance of the model in CL tasks impacted by only considering the last layer in the filter, especially when the representation network is trained from scratch? \n- A further explanation of the backbone finetuning in the main text would be beneficial to understanding the Experiments section, since the impact of finetuning the representation neural network $\\phi$ was often even greater than the impact of tuning the forgetting parameter $\\gamma$, but the former did not appear in the list of contributions and was explained very quickly in the main text.\n- A code implementation was not provided within the supplementary material\n- Claims of efficiency in terms of computation were not supported by empirical results. The comparison to Chang et al. is supported by a theoretical analysis, but a comparison to the baselines Online SGD and ER++ would further support the claims of the method achieving improvements in accuracy metrics without sacrificing computational efficiency, especially when the backbone neural network is finetuned. \n- Some visualizations could be improved for better readability. For instance, Figure 2 has very thin lines in the legend and plot which makes it difficult to see what is happening. \n\nChang, P. G., Dur\u00e1n-Mart\u00edn, G., Shestopaloff, A. Y., Jones, M., and Murphy, K. (2023). Low-rank extended kalman filtering for online learning of neural networks from streaming data."
                },
                "questions": {
                    "value": "How does the training of the representation neural network (backbone finetuning) impact the behaviour of the Kalman filter, since the latent space is changed mid-stream? Can this impact or explain the results in Figure 3, where the Kalman filter method performs better compared to previous work when used on a pre-trained network rather than from scratch?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5320/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699353750166,
            "cdate": 1699353750166,
            "tmdate": 1699636533644,
            "mdate": 1699636533644,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WlIWlYG8Gn",
                "forum": "ZzmKEpze8e",
                "replyto": "DzwojnaUtK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5320/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5320/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to BEuY: part 1"
                    },
                    "comment": {
                        "value": "We would like to thank reviewer BEuY for the feedback. Please find our detailed response below.\n\n> [...] not clear while reading the Related work section how using Kalman Filters in the representation space impacted the results compared to EKF[...], especially when the representation network is trained from scratch?\n\nOur method (last-layer KF) and EKF are quite different. EKF linearises the model likelihood $p(y|x,\\theta)$ wrt $\\theta$ around the previous estimate $\\theta_t$, and then uses the KF to update mean and variance for the distribution of $\\theta$. These parameters represent all the parameters of the Neural Network (NN). In principle, the EKF approach could represent a richer non-stationarity structure because it models it in all the NN parameters. On the other hand, as a drawback, the approximation induced by the linearisation might be problematic. The second drawback is that the naive version of EKF scales $O(P^3)$ where P represents the total number of NN parameters. Chang et al. present a low-rank plus diagonal approximation for the precision matrix which renders the complexity of the algorithm to be $O(PC^2)$, where C is the number of classes. Therefore, even using this approximation becomes problematic when applying this technique to very large-scale classification problems. Moreover, EKF replaces gradient descent on the NN parameters by KF updates applied to the linearised model. This makes it hard to combine EKF with many optimization algorithms developed for NNs. Our method uses KF only in the last layer and then updates the representation using gradient descent on parameters through backpropagation. Since it is only the last layer, this method in principle could model a limited set of non-stationarities. The advantage of the method is that it is very simple, it does not require linearizing the system and it could be used together with many powerful algorithms for NN optimization. Moreover, as we show in the paper, our method allows us to learn the parameter controlling the drift (parameter gamma) in the last layer online, allowing the system to adapt to the non-stationarity in the data distribution. This makes our approach quite flexible. In many practical problems, it might be enough to model non-stationarity in the last layer only, but in order to conclude, an additional empirical study is required.\nFinally, there exists a number of works which have shown that it is not necessary to reap the benefits of Bayesian Inference in NNs to be Bayesian wrt all parameters, and often just being Bayesian with the last layer works surprisingly well. Some papers are https://proceedings.mlr.press/v206/sharma23a.html,  https://proceedings.mlr.press/v139/daxberger21a.html, https://arxiv.org/abs/1502.05700\n\n> further explanation of the backbone finetuning in the main text would be beneficial...\n\nThank you for the suggestion, we will add more information in the experiments section about how we finetune the backbone. In fact, we already have some information about it in the text. In the main contributions in the introduction, we point out that finetuning backbone and showing that it works empirically, is one of the contributions. Moreover, we have Appendix C where we describe in more details how we finetune the backbone. We already have references to Appendix C in text \u2013  in Section 2.1,(ii); in Algorithm 1. We also have Appendix F which contains additional information about finetuning backbone. The reason why we use Appendix is the space constraints. Following your suggestion, we will add a reference to Appendix C in the experiments section as well as a sentence about the impact of fine-tuning backbone.\n\n> A code implementation was not provided...\n\nUnfortunately, we do not have an open-source code for this project. As an intermediate solution, we will write a more detailed pseudo-code in python format in Appendix, such that it is easier to reproduce.\n\n> Some visualizations could be improved for better readability...\n\nThank you for this suggestion. We will improve the visibility of Figure 2."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5320/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700677247040,
                "cdate": 1700677247040,
                "tmdate": 1700677247040,
                "mdate": 1700677247040,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "EXL3Xbhvit",
                "forum": "ZzmKEpze8e",
                "replyto": "sZne9jz6XH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5320/Reviewer_BEuY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5320/Reviewer_BEuY"
                ],
                "content": {
                    "title": {
                        "value": "Brief response"
                    },
                    "comment": {
                        "value": "Thank you for your answer. As the author-reviewer period is now coming to an end, I will go through your response in more detail during the reviewer discussion period."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5320/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700738924411,
                "cdate": 1700738924411,
                "tmdate": 1700738924411,
                "mdate": 1700738924411,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]