[
    {
        "title": "Chain-of-Verification Reduces Hallucination in Large Language Models"
    },
    {
        "review": {
            "id": "E85rsbg9Jy",
            "forum": "VP20ZB6DHL",
            "replyto": "VP20ZB6DHL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2690/Reviewer_gDTz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2690/Reviewer_gDTz"
            ],
            "content": {
                "summary": {
                    "value": "This study introduces an approach aimed at mitigating hallucinations by harnessing the self-verification capabilities of Large Language Models (LLMs). The proposed method involves generating verification questions by an LLM to cross-check the accuracy of its initial responses, autonomously providing answers to these queries, and ultimately generating a refined response. The authors conducted a series of experiments to empirically establish the efficacy of this approach in addressing hallucination problems exhibited by the LLM across a range of tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "(1) The Chain-of-Verification concept is straightforward and can be practically implemented without necessitating adjustments to LLM's parameters.\n\n(2) The empirical evaluations conducted on a varity of tasks, including list-based questions (Wikidata), closed-book MultiSpanQA, and longform text generation, demonstrate the effectiveness of the proposed method in mitigating LLMs' hallucinations.\n\n(3) This paper is well-written and presents its ideas in a clear and comprehensible manner."
                },
                "weaknesses": {
                    "value": "(1) The introduction of the proposed method does incur additional inference overhead. It would enhance the paper's rigor to compare these added computational costs with those associated with alternative methods that also target the reduction of LLM hallucinations.\n\n(2) The utilization of few-shot learning for enabling LLMs to perform planning, verification, and generation (3-shot as detailed in the Appendix) raises a potential concern that the results might be influenced by variations in the few-shot examples used."
                },
                "questions": {
                    "value": "(1) What criteria were employed for the selection of few-shot examples, and what is the potential influence of employing different examples on the results?\n\n(2) Could you provide an estimate of the additional computational overhead that the proposed method would introduce?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2690/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698477034200,
            "cdate": 1698477034200,
            "tmdate": 1699636210550,
            "mdate": 1699636210550,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "52v0lFyCWb",
                "forum": "VP20ZB6DHL",
                "replyto": "E85rsbg9Jy",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2690/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2690/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the review and the positive comments on our approach\u2019s ease of implementation, on the breadth of our experimentation, and the clarity of our draft! \n\nWe have responded to your concerns below. \n\n**Inference overhead**\n\nPlease check the common response. We also provide further comparative analysis in section 8 in our appendix.\n\n\n**Few-shot examples**\n\nWe have now added the few-shot prompts used in our supplementary material for reproducibility.  We agree that changing the few-shot prompts can lead to a variance in the final output. In our comparative analysis, we use the same prompts to generate the baseline response for all our baseline approaches.\n\n\n\n**Questions:**\n\n1. Few-shot prompts:\n      1. Our few-shot prompts for the list-based QA benchmarks were randomly chosen from the train set. We however keep these \n                  prompts constant across our variants.\n      2. For the biographies dataset, we choose some of the most viewed people's pages on Wikipedia and craft our prompts \n                  based on their biographies.  We find few-shot examples that allow us to show the model our desired verification responses. \n                  We have added the few-shot prompts used to the supplementary material.\n2. We perform an analysis of the inference overhead. Please see the common response and Section 8 in our appendix."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2690/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726235177,
                "cdate": 1700726235177,
                "tmdate": 1700726235177,
                "mdate": 1700726235177,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "A8y72s9y8O",
            "forum": "VP20ZB6DHL",
            "replyto": "VP20ZB6DHL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2690/Reviewer_kMJo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2690/Reviewer_kMJo"
            ],
            "content": {
                "summary": {
                    "value": "Large Language Models (LLMs) can sometimes exhibit \"hallucination,\" which refers to the generation of factually incorrect or misleading information. This work proposes the Chain-of-Verification (CoVe) method, a strategy for self-correcting LLM responses by asking and answering verification questions. The experimental results demonstrate that the CoVe approach reduces hallucinations in a variety of tasks, including Wikidata-based list problems, closed-book MultiSpanQA, and long-form generation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The writing is well. \n2) The idea of correcting LLM responses by answering and answering verification questions from the model itself is valuable.\n3) The proposed method is effective in alleviating hallucination problems."
                },
                "weaknesses": {
                    "value": "1) There is an absence of comparative analysis with other methods aimed at mitigating the hallucination issue. It remains to be clarified whether CoVe offers an enhancement in performance relative to other methods.\n2) More instances of prompts are required. For example, in Section 3.3, some examples of prompts need to be provided to distinguish between the several variants of verification variants.\n3) Needs to provide more examples of the use of CoVe in different tasks."
                },
                "questions": {
                    "value": "1) How do we verify that the model can do plan verification, execution verification, and verified response well after a few shots? What happens if you use zero-shot CoVe?\n\n2) Is it possible to skip the step of generating a baseline response by directly generating questions similar to plan verification based on the query and generating the final response based on the response? Does CoVe have any advantages over this method?\n\n3) More details of prompts are needed in the supplemental materials, or it would be difficult for readers to follow the ideas.\n\n4) What is the time complexity of the proposed method and other competitors?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2690/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2690/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2690/Reviewer_kMJo"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2690/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698761276830,
            "cdate": 1698761276830,
            "tmdate": 1699636210444,
            "mdate": 1699636210444,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QgRau4j7WY",
                "forum": "VP20ZB6DHL",
                "replyto": "A8y72s9y8O",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2690/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2690/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the review and for finding our draft well-written. We are grateful for your positive comments on the value and efficacy of our approach.\nWe have addressed your concerns and answered your questions below\n\n**Concerns:**\n\n1. We have now added three new state-of-the-art baselines to the paper. Our work deals with hallucinations specifically for longform generations. We have thus updated Table 3 with baselines using SelfCheckGPT (NLI and LLM) and ChatProtect - three state-of-the-art papers that deal with hallucinations in longform generations. We find our approach, CoVe, outperforms both SelfCheckGPT (with NLI and LLM)  and ChatProtect. Please see our common response for more details.\n\n2. We add our few-shot prompts in the supplementary material.\n\n3. Our approach is intended for mitigating hallucinations in longform generations, a highly relevant problem. As LLMs are used more and more as replacements to search engines, it is crucial that we show CoVe reduces hallucinations across 4 generation benchmarks.  We are optimistic that CoVe style verifications can be extended to generative models on real applications, which is beyond the scope of this paper.\n\n**Questions**\n\n1. Like several reasoning tasks, it is hard to measure the performance of the intermediate parts of the chain owing to the absence of labels, although clearly you can measure the end-to-end performance, or hand annotate some data. For the Wikidata task we manually measure the accuracy of the verification questions and we find that the Llama 65B is around 70% accurate at answering factoid questions related to the professions and birth-places of people. We describe this in the paper under Shortform verification questions are more accurately answered than longform queries.\n2. That\u2019s an interesting approach to try. For some tasks, it is not likely to work as you will not know which questions to ask, e.g. consider list-based questions (e.g. \u201cwho are some politicians born in NYC\u201c), the only question you can really ask at the start is the list-based question itself.\n3. We had added our few-shot prompts in the supplementary material\n4. We perform a comparative analysis of the inference overhead encountered. Please check the common response and Section 8 in our appendix."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2690/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726578664,
                "cdate": 1700726578664,
                "tmdate": 1700726578664,
                "mdate": 1700726578664,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "FfG5tlK3fj",
            "forum": "VP20ZB6DHL",
            "replyto": "VP20ZB6DHL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2690/Reviewer_r3aM"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2690/Reviewer_r3aM"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a new prompting method to mitigate LLMs\u2019 hallucination issues, dubbed chain of verification (CoVE). The model first generates an initial output, and then, conditioning on it together with the input, it generates a series of questions targeting the facts stated in the output. The model then answers the verification questions and identifies the factual errors in the initial response, which is then revised to produce the final outputs. Various variants are explored, each answering the verification questions in different manners. CoVE is tested on several question-answering datasets on Wikipedia, including Wikidata and Wiki category list (both are created from templates), MultiSpanQA, as well biography generation. Results show that CoVE reduces hallucination, at a cost of slightly worse helpfulness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Mitigating hallucinations in LLMs is a timely and important topic\n- CoVE is simple and widely applicable\n- Various variants are tested out"
                },
                "weaknesses": {
                    "value": "- The technical contribution is thin. I had a hard time justifying the paper\u2019s technical contribution since CoVE looks very similar to [1]\nThe two Wikipedia QA datasets (Wikidata and Wiki-category list) are created from simple templates and are rather toyish. I am not sure how much they add to the paper.\n- Hallucinations are especially tricky to address in generation; the only generation task considered is biography generation, which is way less challenging than most real-world applications\n- The paper aims to address hallucination problems. However, I do not find this reflected in the designs of the experiments: 3 out of 4 experiments are question answering, and the remaining one is a rather confined biography generation task. Therefore I find the experiments of this paper weak. Including a more diverse and challenging set of tasks can strengthen the results.\n- Wikipedia is a domain that most models have a lot of exposure to; it would be more interesting to see how CoVE performs in other domains such as scientific, legal, and medical domains.\n- It seems that CoVE can potentially negatively impact the model\u2019s helpfulness: in Table 1, the number of factual outputs reduces; similarly, in Table 3, CoVE produces fewer facts. Some discussion on this would be interesting.\n- I am concerned about making strong conclusions solely based on the FactScore metric: will a model receive higher FactScore by producing shorter outputs with less information? Complementing it with models\u2019 helpfulness, generation quality, and controlling for the number of facts might be necessary\n\n[1] https://arxiv.org/abs/2210.03350"
                },
                "questions": {
                    "value": "- A key assumption behind CoVE is that even when the model is not able to generate factual outputs, it might still be able to identify nonfactual statements. A similar observation is mentioned by [2], but they argue that retrieval is necessary. Can the authors elaborate their views on whether or not the model needs external information to identify factual errors in its own outputs?\n\n[2] https://www.semanticscholar.org/paper/Language-Models-Hallucinate%2C-but-May-Excel-at-Fact-Guan-Dodge/45653ad43124f02dc2cf2db3357be1d1d78ddb18?utm_content=title&utm_medium=unfurl&utm_source=slackbot"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2690/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698819024002,
            "cdate": 1698819024002,
            "tmdate": 1699636210367,
            "mdate": 1699636210367,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vziX7ZqB5i",
                "forum": "VP20ZB6DHL",
                "replyto": "FfG5tlK3fj",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2690/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2690/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank you for your review and are appreciative of your positive comments on the applicability of our approach and our experimentation!\n\nWe have addressed your concerns below\n\n**Technical contribution**\n\nThe paper \u201cMeasuring and Narrowing the Compositionality Gap in Language Models\u201d (which is cited) as the title suggests concentrates on compositional questions with multiple hops such as \u201cWho won the Master\u2019s Tournament the year Justin Bieber was born?\u201d \u2013 and breaking into subquestions. It does not focus on hallucinations (does not mention that word at all). It uses a method called self-ask to build this subquestion decomposition. In our work questions are used for a different purpose: to verify hallucinations, rather than decompose a multi-hop question, so this is different. Further, we develop joint, 2-step, factored and factor+revise approaches and compare these and show how they are important \u2013 none of which is related to this paper.  \n\n**Wikidata + Wiki-Category datasets**\n\nWe would like to clarify that we do not create the Wiki-Category benchmark ourselves but use a subset of an existing benchmark: Quest (https://aclanthology.org/2023.acl-long.784.pdf).\nWikidata uses real data from Wikipedia, but the queries are synthetically constructed. \nWe also report results on the MultiSpanQA task which does use real queries from humans (from Google). Overall, list-form answers are very natural in assistant-based applications. List-generation questions can be excellent ways to understand LLM hallucinations and rapidly test approaches. List generations mimic longform generations with the advantage that they can be automatically, efficiently and perfectly evaluated, while in other tasks evaluation is much more tricky.\nWe agree that questions of the form \u201cName some politicians born in Boston\u201d appear deceptively trivial **but large-scale commercially deployed LLM systems fail in producing correct answers**. We have provided several qualitative examples of this in the supplementary material.\n\n\n\n**Set of tasks used**\n\nHallucinations mitigation in longform generations has very recently started receiving significant attention and a lot of benchmarks don\u2019t yet exist for this problem. We chose these four tasks because (i) they are known to produce hallucinations for state-of-the-art models (as shown in experiments), both open source and commercial products; and (ii) ways of evaluating these tasks exist. In particular, the FactScore metric was recently built for the biography task, and hence several other papers, other than ours, are biographies as well (see new baselines). \nWe agree that it would be interesting to see how CoVe would perform on further benchmark tasks as they are developed. Scientific, legal and medical domains are great applications and will warrant deep analysis beyond a methods paper such as this one. Hence, this is exciting future work beyond this paper.\n\n**Number of facts vs. number of hallucinations**\n\nAny method that reduces hallucinations will reduce the number of facts. As we note in our research questions, we are interested in whether CoVe can selectively remove fictitious facts with a minimal impact on the number of truthful facts. \nThe FactScore metric is normalized by number of facts. So a shorter generation does not automatically mean fewer hallucinations. Note that we also built a baseline whereby the number of facts is reduced by chopping off the ends of passages to match the same number of facts as our best model and we see CoVe significantly outperforms this baseline. We describe this in our passage: CoVe outperforms existing hallucination mitigation baselines.\n\n**Questions**\n\nOur work along with several prior works, eg SelfCheckGPT, ChatProtect have shown that it is possible to mitigate hallucinations without retrieval. We agree that it is impossible to guarantee veracity without access to a verified source of information. We would argue that even retrieval as shown by several  Retrieval Augmented Generation (RAG) style approaches still cannot guarantee this without assuming that everything on the internet is true. We also would like to note that errors can still creep into the RAG pipeline for example the PerplexityAI result from our paper which uses a retrieval system."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2690/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727033621,
                "cdate": 1700727033621,
                "tmdate": 1700727033621,
                "mdate": 1700727033621,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "wPzBAy9SSr",
            "forum": "VP20ZB6DHL",
            "replyto": "VP20ZB6DHL",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2690/Reviewer_Lhor"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2690/Reviewer_Lhor"
            ],
            "content": {
                "summary": {
                    "value": "This paper tackles a hallucination problem of LLMs via a carefully designed framework of iterative prompting. Specifically, the authors propose chain-of-verification (CoVe), which revised the original response of LLMs by generating verification questions and then answering to them. All these processes are conducted via few-shot prompting. Through the experiments on various applications, the effectiveness of CoVe has been demonstrated as it successfully reduces a hallucination of applied LLM (LLaMA-65B) and outperforms the performance of carefully fine-tuned LLM to follow the instruction (LLaMA2-70B Chat)."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. **Clarity**. Overall, the writing is clear and easy to follow. In addition, the organization of the main draft is well-established.\n2. **Well motivated problem**. Reducing the hallucination and improving the factuality of LLMs is an interesting and important problem. To this end, considering the improved prompting framework is a reasonable and well-motivated direction. \n3. **Simple and efficient method.** The proposed method is simple and can be applicable regardless of the types of LLMs. Also, it shows consistent improvement across the various applications."
                },
                "weaknesses": {
                    "value": "1. **Absence of necessary baselines**. As the authors pointed out in the Related work sections, there are many relevant works based on prompting, to reduce the hallucination of LLMs [1,2,3] or improve LLMs\u2019 reasoning [4,5]. However, these baselines are never compared through the experiments now. Therefore, it\u2019s hard to verify the effectiveness of the proposed CoVe compared to them.  \n2. **Difficulty of direct comparison**. Currently, only zero-shot (or CoT) results are presented for LLaMA2 and few-shot results for LLaMA65B, respectively. It makes be hard to compare both models as there is no overlap. To ease their comparison, including the results of LLaMA2 few-shot and LLaMA zero-shot is strongly encouraged.  \n3. **Inconsistency across Tables**. While Factor+revise is presented in Table 3 and it achieves the best score, there are no such results in Tables 1 and 2. Does this method not perform well on the setups in Tables 1 and 2? To facilitate the understanding of the working mechanism of the proposed framework.  \n4. **More qualitative examples**. While the authors present some examples in Figures 1 and 3, it would be better to present more examples to help the understand of readers.\n\n[1] Manakul et al., Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models., arXiv:2303  \n[2] Cohen et al., Lm vs lm: Detecting factual errors via cross examination., arXiv:2305  \n[3] Varshney et al., A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation., arXiv:2307  \n[4] Miao et al., Selfcheck: Using llms to zero-shot check their own step-by-step reasoning., arXIv:2308  \n[5] Madaan et al., Self-Refine: Iterative Refinement with Self-Feedback., NeurIPS 23"
                },
                "questions": {
                    "value": "Please address the concerns in above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2690/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699603755198,
            "cdate": 1699603755198,
            "tmdate": 1699636210271,
            "mdate": 1699636210271,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "sv2zGkl1kK",
                "forum": "VP20ZB6DHL",
                "replyto": "wPzBAy9SSr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2690/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2690/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your feedback and we are happy you found our draft clear and easy to follow.\nWe agree that hallucinations are an important and interesting problem and we are glad you found our approach well-motivated.\n\nWe have responded to your comments below\n\n**Absence of necessary baselines:**\n\nWe have added three new state-of-the-art baselines to the paper. Our work deals with hallucinations specifically for longform generations. We have thus updated Table 3 with baselines using SelfCheckGPT (NLI and LLM) and ChatProtect - three state-of-the-art papers that deal with hallucinations in longform generations. We find our approach, CoVe, outperforms both SelfCheckGPT (with NLI and LLM)  and ChatProtect.\n\n**Difficulty of direct comparison**\n\nLlama65B is not an instruction-tuned model and hence produces gibberish (without stopping) when prompted zero-shot. This is expected behavior for a base language model, hence we do not report it (we will clarify this in the paper). When prompted zero-shot with \u201cLet\u2019s think step-by-step\u201d, the model generated erroneous steps, for example, to check information on the web instead of generating the required answer, and did not produce answers.\nWe have updated our appendix on the unsuccessful experiments of getting Llama 65B to generate the desired response when prompted zero-shot. In any case, these serve as baselines to CoVe rather than being interesting in themselves to compare. The CoVe experiments are also few-shot using Llama65B, hence the baseline few-shot Llama65B comparison. Llama 2 (which should be superior) is there as a baseline just to show that zero-shot instruction-tuned model baselines do not actually fare better.\n\n**Inconsistency across Tables**\n \nThe Factor+revise method adds an additional step where the LLM can check for inconsistencies between the generated content and the answered verification question. \n\nConsider the example\n\n          \u201cTrump received a BS in Business from the University of Pennsylvania\u201d \n           And the verification question and output:\n\n          What degree did Donald Trump graduate with from the University of Pennsylvania in 1968? \n          Answer: A BS in Economics. \n\nThe Factor+revise method adds a separate step that has the LLM check if these two outputs are consistent, inconsistent or partially consistent and then recomposes the output based on the consistent parts. \n\nFor the set QA tasks checking for inconsistencies and removing an answer from the list is more trivial, as it is just an entity match.\n\nExample:\n\n         Name some politicians born in New York, NY\n        A: Hillary Clinton\n\n        Where was Hillary Clinton born?\n        A: Chicago (Chicago != New York)\n\nCoVe - 2 step is a variant of CoVe-Factored where the Verification questions are evaluated in one step (thus reducing the need to prompt the LLM several times for each verification question). This is feasible to use on tasks such as set QA but as the number of verification questions grows answering them all can be difficult in the fixed context size. \nWe hence don\u2019t experiment with this approach on the harder benchmarks. \n\n\n**More qualitative examples**\n\nFigure 1 and 3 show full examples of CoVe. \nThere is also analysis in section 9 of how and why ChatGPT fails on a number of tasks, and why verification questions can work.\nFinally, we have now added the prompts used as part of our supplementary material"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2690/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700727356778,
                "cdate": 1700727356778,
                "tmdate": 1700727356778,
                "mdate": 1700727356778,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]