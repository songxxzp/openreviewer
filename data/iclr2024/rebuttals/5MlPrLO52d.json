[
    {
        "title": "Neural Tangent Kernels for Axis-Aligned Tree Ensembles"
    },
    {
        "review": {
            "id": "KJtUQnCNPH",
            "forum": "5MlPrLO52d",
            "replyto": "5MlPrLO52d",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission969/Reviewer_Nxzp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission969/Reviewer_Nxzp"
            ],
            "content": {
                "summary": {
                    "value": "This paper builds upon the theoretical analysis framework introduced by Kanoh & Sugiyama (2022; 2023) to investigate the learning behavior of soft tree ensembles. It focuses on deriving closed-form solutions for the Neural Tangent Kernel (NTK) induced by infinitely many axis-aligned tree ensembles, considering two scenarios: always axis-aligned (AAA) and axis-aligned at initialization (AAI). Additionally, the paper extends the NTK framework to accommodate multiple tree architectures and demonstrates that any non-oblivious axis-aligned tree ensemble can be transformed into an axis-aligned oblivious tree ensemble while preserving the same NTK. The paper also explores the potential applications of multiple kernel learning (MKL) in identifying suitable tree architectures and features under the axis-aligned constraint. Empirical results are provided to validate the theoretical findings and illustrate the practical significance of the axis-aligned constraint in tree ensemble learning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is original in extending the NTK framework to the axis-aligned soft tree ensembles, which have not been theoretically analyzed before.\n    \n2. The paper is of high quality in deriving the closed form solution of the NTK for both AAA and AAI cases, and proving the equivalence between axis-aligned non-oblivious and oblivious tree ensembles.\n    \n3. The paper is clear in presenting its main results and providing intuitive explanations and illustrations for its theoretical findings."
                },
                "weaknesses": {
                    "value": "1. In Section 3.1, if I interpret it correctly, AAA refers to axis-aligned initialization, with the assumption that the selected split feature at each node remains constant throughout training. This assumption appears rather restrictive, considering that typical axis-aligned trees do not impose such constraints. It would be valuable if the author provided further insights or comments regarding this assumption.\n    \n2. The paper lacks an exploration of the computational complexity and scalability issues related to employing Multiple Kernel Learning (MKL) for tree architecture search. It is crucial to assess the feasibility and efficiency of utilizing MKL, particularly for large-scale datasets and high-dimensional feature spaces."
                },
                "questions": {
                    "value": "Please find my main concerns in the weakness part. Additionally, I have another question regarding the analysis framework: does this analysis framework allow non-zero initialized parameters (selected split features) become zero (no split at one node) during the training? If the framework allows for such adaptability, it could potentially accommodate changes in the shapes or depths of trees during training, thus enhancing its overall generality."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission969/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission969/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission969/Reviewer_Nxzp"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission969/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698243096147,
            "cdate": 1698243096147,
            "tmdate": 1699636022712,
            "mdate": 1699636022712,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "faRPbg4KrZ",
                "forum": "5MlPrLO52d",
                "replyto": "KJtUQnCNPH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission969/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission969/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors\u2019 Response to Reviewer Nxzp"
                    },
                    "comment": {
                        "value": "Thank you for your review.\n\n> In Section 3.1, if I interpret it correctly, AAA refers to axis-aligned initialization, with the assumption that the selected split feature at each node remains constant throughout training. This assumption appears rather restrictive, considering that typical axis-aligned trees do not impose such constraints. It would be valuable if the author provided further insights or comments regarding this assumption.\n\nOur idea is to divide the learning process of decision trees into two components:\n\n1. selecting tree topological structure and split features,\n2. tuning parameters for splitting thresholds at internal nodes and leaves,\n\nand providing theoretical analysis for AAA and AAI about the second component with Theorem 2. Since it can be combined with any method for the first component, the entire model is no longer restrictive in practice. Similar to the typical decision trees, it is possible to grow a tree by trying out several splitting patterns and adopting the ones with better performance. In our experiments in Section 4.2, we have used MKL to learn tree architectures for the first component and showed its effectiveness.\n\n\n> The paper lacks an exploration of the computational complexity and scalability issues related to employing Multiple Kernel Learning (MKL) for tree architecture search. It is crucial to assess the feasibility and efficiency of utilizing MKL, particularly for large-scale datasets and high-dimensional feature spaces.\n\nThe overall computational cost of MKL can be divided in two parts: computation of the kernel matrix and optimization of weights.\n\nTo obtain a single kernel matrix, a calculation defined in Equation (9) is performed $N^2$ times, where $N$ is the size of an input dataset. When we denote the number of leaves as $\\mathcal{L}$ and the depth of the tree as $D$, the worst case overall computational complexity is $\\mathcal{O}(N^2 \\mathcal{L}D^2)$. Additionally, this calculation is needed to be repeated for all the kernel matrices, while parallelization is possible in this process. Note that if there are duplicates in the output of $h(a_\\ell)$ for all $\\ell \\in [\\mathcal{L}]$, the computational cost can be reduced by aggregating and computing these duplicates together, so the actual computational cost is often less than this. For example, when considering oblivious trees, the computational complexity reduces to $O(N^2 D^2)$.\n\nThe cost to calculate the weights of the kernels by EasyMKL depends on the number of kernels. Specifically, EasyMKL uses Kernelized Optimization of the Margin Distribution (KOMD), and its computational cost is known to be linear with respect to the number of kernels [1]. \n\nWe have added the above discussion about the computational complexity in the Appendix (Section E) in our revision.\n\nPlease note that MKL is mainly chosen for a better understanding of the behavior of AAI and AAI, and we have successfully shown that suitable features can differ between AAA and AAI in Figure 7. If one would like to construct more practical methods, other approaches should be considered, while it is beyond the scope of our paper.\n\n[1] Fabio Aiolli and Michele Donini (2015), EasyMKL: a scalable multiple kernel learning algorithm\n\n> Additionally, I have another question regarding the analysis framework: does this analysis framework allow non-zero initialized parameters (selected split features) become zero (no split at one node) during the training? If the framework allows for such adaptability, it could potentially accommodate changes in the shapes or depths of trees during training, thus enhancing its overall generality.\n\nThank you for your comment. It is indeed an interesting idea. For example, incorporating L1 regularization on weights in the training process using gradient descent might be possible to obtain space weights. \nIt could also be interesting to apply a temperature-scaled sparsemax [2] to the weights. We believe these ideas are interesting future directions.\n\n[2] Martins & Astudillo (2016), From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission969/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700227694129,
                "cdate": 1700227694129,
                "tmdate": 1700227694129,
                "mdate": 1700227694129,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "nZXYs7bKhn",
            "forum": "5MlPrLO52d",
            "replyto": "5MlPrLO52d",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission969/Reviewer_QWBa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission969/Reviewer_QWBa"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a theoretical analysis of the Neural Tangent Kernel for axis aligned decision trees. Experiments are limited to a single dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This is some work towards the goal of better understanding how to optimally train axis aligned decision trees."
                },
                "weaknesses": {
                    "value": "- The paper is theoretical in nature, but its impact for practical applications seems very limited to me.\n- Experimental validation is limited to one dataset. This raises concerns about the practical use of this work.\n- The comparison with Random Forest in unfair because RF is limited to depth 3. It is not clear how many attributes were used for splittiong at each node for RF. If the number of attributes is small, RF needs deep trees to find the relevant features.\n- There is no comparison with existing modern ensembling techniques such as Gradient Boost."
                },
                "questions": {
                    "value": "- What is the difference between this paper and Kanoh and Sugiyama. \"Investigating Axis-Aligned Differentiable Trees through Neural Tangent Kernels\". In ICML 2023 Workshop on Differentiable Almost Everything: Differentiable Relaxations, Algorithms, Operators, and Simulators?\n- What are the practical implications of knowing the NTK induced by axis-aligned tree ensembles? Does this imply that we can obtain better/faster training algorithms?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Research integrity issues (e.g., plagiarism, dual submission)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "The main results of the paper (Theorem 2 and Prop 1) are also present in: R Kanoh, M Sugiyama. \"Investigating Axis-Aligned Differentiable Trees through Neural Tangent Kernels. ICML 2023 Workshop on Differentiable Almost Everything: Differentiable Relaxations, Algorithms, Operators, and Simulators."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission969/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698765444822,
            "cdate": 1698765444822,
            "tmdate": 1699636022628,
            "mdate": 1699636022628,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gMmiBHgLLA",
                "forum": "5MlPrLO52d",
                "replyto": "nZXYs7bKhn",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission969/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission969/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors\u2019 Response to Reviewer QWBa"
                    },
                    "comment": {
                        "value": "Thank you for your review.\n\n> The paper is theoretical in nature, but its impact for practical applications seems very limited to me. \n\nWe disagree with this point. For instance, Proposition 2 guarantees that the consideration of oblivious trees is sufficient for searching tree architectures, resulting in a substantial reduction in the architecture search space in practical situations. Moreover, the insight that feature selection is not important for AAI can lead to more efficient processes by reducing unnecessary searches. We believe that acquiring the NTK that enables us to comprehend training behavior, and using it to theoretically support these characteristics in a rigorous manner, is of significant importance.\n\n> Experimental validation is limited to one dataset. This raises concerns about the practical use of this work. The comparison with Random Forest in unfair because RF is limited to depth 3. \n\n> There is no comparison with existing modern ensembling techniques such as Gradient Boost.\n\nWhile we show the results of only one dataset in the main text, we have already conducted numerical experiments on 14 datasets about MKL weight distributions, as written in the last sentence of the second paragraph in Section 4.2. These experiments have been already described in our initial submission before revision. In addition, while it is not directly related to the main claim of the paper, empirical analyses of the generalization performance across numerous datasets have also been added to the Appendix (Section D.5). The tic-tac-toe dataset was chosen for the main content because we think it made understanding the properties of models easier. We would like to note that, in our paper, we do not focus on the practical utility of these forest models; rather, we aim to understand their theoretical properties. It is not fundamental to make a general claim that AAA and AAI are better or worse than other models in terms of generalization performance as it depends on datasets. This is a natural consequence and orthogonal to our claim in this paper. The tic-tac-toe dataset intuitively demonstrates that methods like AAA and AAI perform better than greedy methods such as Random Forest. This discussion has been added to the Appendix (Section D.5).\n\nTo further address your concern, we have added comparative results of typical forest models on the tic-tac-toe dataset in the Appendix (Section D.6), including experiments where tree depth was varied or Gradient Boosting was used. Typical forest models did not surpass AAA across all the tested settings of the tree depth or learning algorithms. \n\n> It is not clear how many attributes were used for splitting at each node for RF. If the number of attributes is small, RF needs deep trees to find the relevant features.\n\nIn Random Forest, each splitting node uses a single feature to make a split, which is a typical approach, and this is the same as AAA. \n\n> What are the practical implications of knowing the NTK induced by axis-aligned tree ensembles? Does this imply that we can obtain better/faster training algorithms?\n\nYes, it does. That is an important argument of our paper. As described in Proposition 2, it is sufficient to consider only oblivious trees as tree topological structures, which significantly reduces the search space and can lead to faster training. Furthermore, our empirical results show that feature selection holds minimal importance for AAI, which also contributes to efficient training.\n\n> What is the difference between this paper and Kanoh and Sugiyama. \"Investigating Axis-Aligned Differentiable Trees through Neural Tangent Kernels\". In ICML 2023 Workshop on Differentiable Almost Everything: Differentiable Relaxations, Algorithms, Operators, and Simulators?\n\nThere is no concern regarding this matter. We have already communicated individually with the program chairs and senior area chairs. Due to the author's instructions, we are unable to provide further details in this forum."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission969/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700227555530,
                "cdate": 1700227555530,
                "tmdate": 1700227555530,
                "mdate": 1700227555530,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "OtxQTNhHii",
                "forum": "5MlPrLO52d",
                "replyto": "gMmiBHgLLA",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission969/Reviewer_QWBa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission969/Reviewer_QWBa"
                ],
                "content": {
                    "title": {
                        "value": "I maintain my rating"
                    },
                    "comment": {
                        "value": "Many of my main concerns still stand:\n- The theoretical justification is about an infinite number of trees, from which we cannot derive the conclusion that the authors derive: \"Proposition 2 guarantees that the consideration of oblivious trees is sufficient for searching tree architectures\" because we cannot train infinitely many trees in practice. So the authors are overclaiming.\n- The weight experiments on 14 datasets are not experiments on accuracy. So I still think that experiments are limited to one dataset.\n- Also, it is not clear from the experiment which ones are the oblivious trees. Looking at table A1 (which is missing two sample t-test for comparison with the best method) I see that AAA (optimal) is the best method. Are those the oblivious trees? If none of them are, what is the point of the experiment?\n- Seems that the authors did not understand my observation about RF even though it is well known that a single attribute is used in RF for splitting at each node, but it is selected from a larger pool. So I rephrase my observation: It is not clear how many attributes were used in the pool for selecting the splitting attribute at each node for RF. If the pool is small, RF needs deep trees to find the relevant features. So I still think the RF was misrepresented in the experiment."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission969/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700503551638,
                "cdate": 1700503551638,
                "tmdate": 1700503551638,
                "mdate": 1700503551638,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BlHTTKwe4o",
                "forum": "5MlPrLO52d",
                "replyto": "EjnFTRHTuF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission969/Reviewer_QWBa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission969/Reviewer_QWBa"
                ],
                "content": {
                    "title": {
                        "value": "Better, but some issues still remain."
                    },
                    "comment": {
                        "value": "Still some issues remain:\n- Figure 4 and Figure A7 show that some of the trees (not clear, are they oblivious or not?) fit the infinite limit pretty well. What about the other kind? Because Proposition 2 talks about two types of infinite trees: axis aligned oblivious and non-oblivious. If you can approximate one kind and not the other with finite trees, Proposition 2 is not useful.\n- Ok, Section D5 was not present in the original submission, I just found it in the revised submission. Yes, the results look pretty good but they should be put in a table instead of a number of figures so that other people can cite your work and your results precisely, without having to approximate them from the figures. The same goes for Figure A10. Tables should also have standard deviations from which p-values could be computed to check significance by anyone interested.\n- Random forest is still misrepresented by displaying results with depth 3. The original random forest is supposed to have fully grown trees. I see infinite depth RF results are much better, even though not as good as the AAA and AAI."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission969/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700610365290,
                "cdate": 1700610365290,
                "tmdate": 1700610365290,
                "mdate": 1700610365290,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UY0BdDY1tZ",
            "forum": "5MlPrLO52d",
            "replyto": "5MlPrLO52d",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission969/Reviewer_w3rN"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission969/Reviewer_w3rN"
            ],
            "content": {
                "summary": {
                    "value": "[I made a mistake in the form.] I found out that I have accidentally checked the \"First Time Reviewer\" question, but in fact, I'm not. It seems that I cannot undo it now, so I'm instead writing it here.\n\nThis paper proposes a way to analyze the training dynamics of axis-aligned tree ensembles using neural tangent kernels (NTK). The idea is two-fold: (1) using soft trees and assigning proper weights to derive NTK, and (2) using multiple kernel learning (MKL) for finding suitable tree structures. The paper also shows that, from any ensemble of axis-aligned trees, one can find that of axis-aligned oblivious trees with the same limiting NTK, which justifies the use of oblivious trees."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper cleverly derives a way to analyze the training dynamics of axis-aligned trees using soft trees and NTK. The idea here is to deliberately assign weights in NTK so that it represents the behavior of axis-aligned trees, which is interesting.\n\n- The online feature selection in actual tree construction is modeled using MKL, which is also interesting.\n\n- The paper also provides a justification regarding the use of oblivious trees based on the above framework."
                },
                "weaknesses": {
                    "value": "- I did not find any weaknesses."
                },
                "questions": {
                    "value": "- The paper argues that one of its contributions is including finite tree ensemble scenarios. However, in Section 4, the proposition is only on infinite trees. Why is this?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission969/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission969/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission969/Reviewer_w3rN"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission969/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698833146979,
            "cdate": 1698833146979,
            "tmdate": 1699636022572,
            "mdate": 1699636022572,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0amDB72qHs",
                "forum": "5MlPrLO52d",
                "replyto": "UY0BdDY1tZ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission969/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission969/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors\u2019 Response to Reviewer w3rN"
                    },
                    "comment": {
                        "value": "Thank you for your review.\n\n> The paper argues that one of its contributions is including finite tree ensemble scenarios. However, in Section 4, the proposition is only on infinite trees. Why is this?\n\nProposition 2 focuses on the property of a deterministic closed-form formula (=limiting NTK). A deterministic closed-form kernel is derived when assuming an infinitely large number of trees, and a deterministic NTK is not induced due to initialization randomness when the number of trees is finite. As the number of trees increases, the NTK asymptotically approaches a deterministic closed-form formula."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission969/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700227427084,
                "cdate": 1700227427084,
                "tmdate": 1700227427084,
                "mdate": 1700227427084,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CSgb43QnGP",
            "forum": "5MlPrLO52d",
            "replyto": "5MlPrLO52d",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission969/Reviewer_PQpi"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission969/Reviewer_PQpi"
            ],
            "content": {
                "summary": {
                    "value": "The authors derive a Neural Tangent Kernel for axis-aligned trees, and show several extensions such as non-oblivious trees and multiple different architectures. They also run a numerical experiment on the tic-tac-toe dataset and show the empirical utility of such methods."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors build on a few prior papers, Kanoh and Sugiyama 2022, 2023, which establish NTK on tree ensembles.  Though I did not check the proofs carefully, the theory seems sound. Prior work on NTK for tree ensembles seemed limited to all trees having the same architecture, and each tree being oblivious, and the authors extended the theory into those regimes."
                },
                "weaknesses": {
                    "value": "1. I do not really understand why we would *want* NTK for tree ensembles, or more specifically axis-aligned trees.  Typically, I look to theory because it can provide insight or intuition about why some method is working well.  I did not get any of that here.  What does this theory explain about axis-aligned trees that we did not already know? \n\n2. There are several papers relating trees/forests to kernels.  The mondrian kernel is probably the most famous, https://dl.acm.org/doi/10.5555/2969033.2969177, though Breiman wrote about it over 20 years ago prior to his random forest paper, https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.24.7078&rep=rep1&type=pdf. And we https://arxiv.org/abs/1812.00029, and others, have some work on it as well, https://ieeexplore.ieee.org/document/7373647.  Those kernels are exact, whereas this kernel is an approximation.  Given that trees/forests directly induce a kernel, I do not understand why we would want to derive an approximate kernel?\n\n3. The primary results seem to depend on all the trees having the same architecture, and also all the features being somehow already selected?  I do not understand where the architecture or selected features come from?\n\n4.  The results also all seem to depend on soft, rather than hard, trees. Is that because the math is easier for soft trees? A few sentences about that in the discussion would be helpful context. I do not know of any tree packages that leverage soft trees, so if people actually use them in practice (I know the papers on Neural Forests, but I do not know whether they are actually used anywhere), that would be helpful context as well.  If nobody uses soft trees, that's ok, it just a limitation, and future work might be about getting similar results on hard trees, unless it is obviously (to you) not tractable. \n\n5. The empirical results are on a single dataset: tic-tac-toe.  It is a nice illustration.  I wonder, however, why this dataset was chosen specifically? Was it cherry-picked to have good results? Or was it because all the features are binary, and that helps for some reason? Or because depth 3 trees work well (at least the AAA/AAI ones)?  \n\n6.  For me, the fact that AAA works better than RF for certain alpha's is by far the most interesting result.  What features of the distribution is the axis-aligned NTK capturing that the RF fails to acquire? I am guessing the fact that they are depth 3 trees has something to do with it, because the RF can only handle 3 feature splits per path, and more are required to achieve Bayes optimal.  How does the NTK get around this issue, what is happening?  For me, this is by far the most interesting result, and I did not understand or see any text attempting to explain it."
                },
                "questions": {
                    "value": "My main question is why/when can axis-aligned NTKs outperform axis-aligned forests?  What information can they leverage that is missed by the forests? What is the inductive bias of the NTK relative to the axis-aligned forests.  While the theory, on its own, is fine, I do not find it compelling on its own.  The arbitrarily slow convergence theorem (https://link.springer.com/article/10.1007/BF00534199) implies that any given approach will outperform another with finite data.  So, from that perspective, the point of any paper describing a new approach is to provide insight into when/why it outperforms other approaches.  Curves plotting performance vs sample size, dimensionality, or various simulation parameters can all provide insight into this issue.  If the authors can provide clean compelling explanations about when/why their NTK would/does outperform RF or other kernel forest approaches, I think it would be very interesting.  Without that, however, I am just not that interested."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission969/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699285960198,
            "cdate": 1699285960198,
            "tmdate": 1699636022483,
            "mdate": 1699636022483,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "JdQhOcJGq3",
                "forum": "5MlPrLO52d",
                "replyto": "CSgb43QnGP",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission969/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission969/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors\u2019 Response to Reviewer PQpi [1/2]"
                    },
                    "comment": {
                        "value": "Thank you for your review.\n\n> I do not really understand why we would want NTK for tree ensembles, or more specifically axis-aligned trees. Typically, I look to theory because it can provide insight or intuition about why some method is working well. I did not get any of that here. What does this theory explain about axis-aligned trees that we did not already know?\n\nAs stated in Section 2.2, the NTK precisely describes training behavior when an axis-aligned tree ensemble is trained through gradient methods. This ability of the NTK leads to our findings such as the sufficiency of considering oblivious trees (Proposition 2) and the realization that feature selection is not important in the case of AAI (Figure 7).\n\n> Those kernels are exact, whereas this kernel is an approximation. Given that trees/forests directly induce a kernel, I do not understand why we would want to derive an approximate kernel?\n\nThe typical kernel obtained from a trained forest simply measures the similarity between data points. However, the kernel we have derived is the NTK, which allows us to analytically describe the training behavior of a model without actually conducting the training. An example of this is shown in Figure 4, and the theoretical background is presented in Section 2.2. While both fall under the category of kernels, their roles are fundamentally different and our NTK is not an approximation of the kernels you have listed.\n\nNTKs have successfully provided theoretical insights for not only the tree ensemble models but also various other models such as typical MLPs, CNNs, RNNs, and Transformers, and NTKs help validate the models. Examples include the equivalence between global average pooling in CNNs and certain data augmentations [1] and the explanation of why performance does not significantly deteriorate in ResNets with Skip Connections even for deeper models [2].\n\nWe would also like to clarify the issue of 'exact/approximation.' The closed-form formula for the NTK is available when considering an infinite number of trees. Moreover, even in the finite case, the kernel can be precisely derived without any approximation. Therefore our NTK is also exact in that sense. There is a randomness due to initialization and the kernel value could vary slightly with each initialization. Nevertheless, as shown in Figure A.2, this variation converges to zero as the number of trees increases, matching the deterministic kernel given in Theorem 2. We believe that such randomness also exists in the existing kernels for trained typical forest models, for example due to bagging, and this is unrelated to the question of whether it is an approximation.\n\n[1] Li et al., (2019), Enhanced Convolutional Neural Tangent Kernels\n\n[2] Huang et al., (2020), Why Do Deep Residual Networks Generalize Better than Deep Feedforward Networks? \u2014 A Neural Tangent Kernel Perspective\n\n> The primary results seem to depend on all the trees having the same architecture, and also all the features being somehow already selected? I do not understand where the architecture or selected features come from?\n\nWe believe you are referring to Theorem 2. Then you are right that our primary result (Theorem 2) assumes that all the trees are equivalent in terms of the tree topological structure and selected features. However, it does not specify how to choose the features or tree topological structure, which can be independently implemented. One can select any method for choosing the features or tree structure, and our theorem applies to any selection. Even in AAA, it is possible to develop a tree by empirically examining various splitting patterns and adopting those with better performance on training data. As demonstrated in Section 4.2, it is also feasible to learn tree architectures using MKL. There are various approaches."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission969/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700227918413,
                "cdate": 1700227918413,
                "tmdate": 1700227918413,
                "mdate": 1700227918413,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4FANADTclw",
                "forum": "5MlPrLO52d",
                "replyto": "JdQhOcJGq3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission969/Reviewer_PQpi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission969/Reviewer_PQpi"
                ],
                "content": {
                    "title": {
                        "value": "Exact for finite trees?"
                    },
                    "comment": {
                        "value": "> Moreover, even in the finite case, the kernel can be precisely derived without any approximation.\n\nThm 1 & 2 both have $M \\to \\infty$.  I do not see an exact NTK for finite M.  If that exists, and is in this paper, I've missed it. Perhaps I misunderstood the above comment though?"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission969/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700683825945,
                "cdate": 1700683825945,
                "tmdate": 1700683825945,
                "mdate": 1700683825945,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LQWpUTVt8L",
                "forum": "5MlPrLO52d",
                "replyto": "gdR2Ng7IE4",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission969/Reviewer_PQpi"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission969/Reviewer_PQpi"
                ],
                "content": {
                    "title": {
                        "value": "Additional numerical experiments"
                    },
                    "comment": {
                        "value": "Looking at Figures A.11 - A.13, it does not seem like the NTK methods outperform classical RF on typical datasets.  New methods do not need to outperform old ones on benchmark datasets to be interesting.  But for papers to be fully in integrity, imho, they need to be super clear about it.  I understand this is not the standard policy in ML/AI papers (or papers in other disciplines), but I stand by this conviction.  For me, the sentence:\n\n> Such a trend appears to hold true on not only the tic-tac-toe dataset but across a wide range of datasets. Details can be found in the Appendix D.4.\n\nis inadequate.  Highlighting a particular dataset to make a point is great, being honest about when this point holds, is greater. \n\nWhat I can infer currently, based on the additional numerical experiments, is that 'greedy' methods perform poorly relative to gradient-based approaches, specifically on datasets in which individual features contain relatively little information, and much more information is in the joint.  That totally makes sense.  And, that is a point that does not require (or even obviously benefit from) NTK, rather, that point can be made simply by comparing RF to PyTorch Tabular. \n\nThe key question for me to motivate this entire line of work (developing NKT for axis-aligned RF) is what insight do we gain specifically from the NKT derivation.  I still do not see it.  I see why gradient-based methods work better than RF in certain cases.  And I see that you can do it, and do neural architecture search, and MKL, etc.  And this is all great.  I just don't know why.\n\nThe answer I am looking for would be something of the following form.  The terms in Eq. (9) illustrate that if <some property of the distribution is satisfied> then the kernel has desirable property X, rendering it more effective than Y (for some Y).\n\nIn other words, if the explanation provided is in terms of the derived quantities, then the explanation demands those quantities.  And if not, it doesn't, and therefore also doesn't demand the theory. \n\nThe paper has been improved.  But I cannot in good conscious recommend a paper, despite having impressive results, without adequate motivation, so I will keep my score.  I believe that the authors can use the theory to provide insight, and look forward to reading it when they do."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission969/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700684927546,
                "cdate": 1700684927546,
                "tmdate": 1700684927546,
                "mdate": 1700684927546,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]