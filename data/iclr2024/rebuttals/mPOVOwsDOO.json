[
    {
        "title": "Talking Models: Distill Pre-trained Knowledge to Downstream Models via Interactive Communication"
    },
    {
        "review": {
            "id": "q5tyrgglSb",
            "forum": "mPOVOwsDOO",
            "replyto": "mPOVOwsDOO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8647/Reviewer_2VSL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8647/Reviewer_2VSL"
            ],
            "content": {
                "summary": {
                    "value": "In their paper, the authors present a novel technique for knowledge distillation that leverages an interactive communication process. This approach draws inspiration from Osgood-Schramm's two-way communication model and employs communication encoders and decoders. Additionally, the authors introduce three supplementary loss functions to guarantee the desired behavior of the distillation process. To assess the efficacy of their method, they conduct experiments on four different datasets, covering two distinct tasks: movie recommendation and image classification. The results of these experiments demonstrate that this interactive distillation process can lead to performance enhancements."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The incorporation of Osgood-Schramm's model into the knowledge distillation process is novel and interesting.\n2. The introduction of three new loss functions helps to realize the desired interactive distillation process.\n3. The paper has a well-crafted structure and easy to follow."
                },
                "weaknesses": {
                    "value": "The paper has several limitations that need to be addressed:\n\n1. **Limited Comparison Baselines:** The study only compares the proposed method with four baseline approaches. To provide a more comprehensive evaluation, it is advisable to consider more advanced knowledge distillation techniques and include a comparison with state-of-the-art models in the field. For instance, [a], \n\n2. **Limited Tasks:** The paper only explores two specific tasks, which may not represent the full spectrum of potential applications for the proposed approach. Expanding the scope of evaluation to cover a broader range of tasks would provide a more robust assessment.\n\n3. **Insufficient Comparison with IAKD:** While the paper introduces a novel approach, it does not adequately differentiate it from Interactive Knowledge Distillation (IAKD). A clear comparison highlighting the advantages and distinctions between the proposed method and IAKD is needed to help readers understand the contribution.\n\n4. **Underwhelming Performance:** The reported performance metrics, such as RMSE and accuracy in Table 3 and 4, do not appear to be competitive when compared to state-of-the-art results. The ablation study also suggests that the new losses (L_MC and L_SC) do not significantly improve performance. For more up-to-date results on the datasets, it is recommended to refer to sources like [RMSE on ML100k](https://paperswithcode.com/sota/collaborative-filtering-on-movielens-100k) and [Cifar-10](https://paperswithcode.com/sota/image-classification-on-cifar-10) to provide a clearer context for your results.\n\nAddressing these issues will help strengthen the paper and provide a more comprehensive and competitive assessment of the proposed approach.\n\n[a] Radhakrishnan, Adityanarayanan, et al. \"Transfer learning with kernel methods.\" Nature Communications 14.1 (2023): 5570."
                },
                "questions": {
                    "value": "1. How dose this approach perform on natural language processing tasks such as text classification, token classification, question answering, etc. Further investigation on these NLP tasks is essential to assess the adaptability and effectiveness of the proposed method in a broader range of applications.\n2. How is the approach compared with the most recent knowledge distillation methods? To establish the novelty and competitiveness of the proposed method, it is crucial to benchmark it against recent state-of-the-art knowledge distillation techniques, considering various datasets and evaluation metrics.\n3. How do you determine w_1, w_2, w_3? The determination of the weights, namely w_1, w_2, and w_3, is not clearly elucidated in the paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8647/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698709731155,
            "cdate": 1698709731155,
            "tmdate": 1699637082976,
            "mdate": 1699637082976,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "NvglRqPgDa",
                "forum": "mPOVOwsDOO",
                "replyto": "q5tyrgglSb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8647/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8647/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the on-point comments and insightful questions. We address the reviewer\u2019s questions below:\n\nWeaknesses:\n1. Limited Comparison Baselines\n\nWe thank the reviewer for providing an additional baseline method. One thing we want to point out here is that our goal is not to improve knowledge distillation in general, instead, we want to extend knowledge distillation to the case where it can be used to help transfer knowledge efficiently from pre-trained large models to smaller models specific for downstream tasks. Therefore, we don\u2019t compare with many KD baselines, we carefully select the canonical ones from which our method builds upon and can be extended to our application. Since we use intermediate representations, many state-of-the-art feature distillation methods (such as [4]) can be used in our framework by modifying the distance metric d in section 3.3. We carefully examine the paper the reviewer provided. It proposed a very scalable and general method for transfer learning with the kernel method. It seems non-trial to apply this method to our problem setup, which is knowledge distillation between pre-trained teacher model and downstream student. \n\n2. Limited tasks.\n\nWe totally agree that expanding the scope of evaluation with more tasks would provide a more robust assessment. This is also the reason why we picked two datasets from totally different domains, i.e., image classification and recommendation systems. We believe that these two different domains can cover many popular use cases. We also studied natural language tasks, however, the task being formulated as generative language modeling tasks totally changed the paradigm of knowledge distillation. Therefore, we focus on verifying our method on the CV and recsys domains. \n\n3. Insufficient Comparison with IAKD. \n\nWe thank the reviewer for pointing out IAKD. IAKD indeed shares similar motivation as our work. But the key idea of IAKD is very different and cannot be directly applied in our case. IAKD directly swapped blocks of models, which require models to be similar in model architecture. In the IAKD paper, the authors used resnet without a pre-training dataset. In our case, we use ViT pre-trained on ImageNet21k, which is much larger and cannot be directly swapped to students due to layer and shape mismatch. \n \n4. Underwhelming Performance\n\nWe thank the reviewer for pointing out the SOTA results. And we want to emphasize that by using smaller models, it is still not realistic to achieve results better than or close to SOTA. This is actually one of the key discussions we want to raise in our paper (Section 5 and Appendix 6.5). In our humble opinion, we don\u2019t think closing the gap between student and teacher model\u2019s performance (fine-tuned teacher in this case, which is SOTA) is realistic for any distillation techniques. The gap caused by capacity and training resources (training extensively on pre-trained data) cannot be overcomed by simply letting one teach the other, as pointed out by many related works [2], [3]. But we still think it\u2019s hopeful that using large pre-trained models can improve downstream models (compared to without distillation), especially when fine-tuning is not realistic in training (limited resource and access to pre-trained model) and serving (serving cost limitation). \n\nEspecially the training from scratch ViT model is not as good as some of the SOTA results, as discussed in [1], ViT models can be relatively hard to train from scratch. However, we want to highlight the relevant improvement that different distillation methods can bring with the help of a large pre-trained ViT model. We believe this is an important use case, because not only is fine-tuning a large pretrained model costly, there will be much more cost when actually serving the model. And in many use cases, the limitation on serving makes it impossible to use a large model hence smaller downstream model is our only choice. In this case, the proper baseline is the training from scratch smaller model, not the fine-tuned teacher model."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8647/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732051180,
                "cdate": 1700732051180,
                "tmdate": 1700732051180,
                "mdate": 1700732051180,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Y9kKdDfU6P",
                "forum": "mPOVOwsDOO",
                "replyto": "q5tyrgglSb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8647/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8647/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "To answer reviewer's questions:\n\n1. How dose this approach perform on natural language processing tasks?\n\nWe actually conducted additional experiments on language modeling tasks using T5 to show by asking questions to T5 Large, we can significantly improve the T5 small student on downstream benchmarks such as SuperGLUE. However, we decided not to include this result due to the design of encoding/decoding (through text prompt) and the communication process is entirely different to our current method, even though the main idea is the same. We want to focus on the general concept of interactive communication in this paper. \n\n2. How is the approach compared with the most recent knowledge distillation methods? \n\nAs discussed above, we compared with  the canonical ones from which our method builds upon and can be extended to our application. However, many most recent knowledge distillation for specific domains can be extended and integrated into our interactive communication framework. \n\n\n3. How do you determine w_1, w_2, w_3? \n\nWe conducted hyper-parameter search on our method and baseline methods, the details of hyper-paramter search and results are discussed in Appendix 6.3 and 6.5.\n\n[1] https://arxiv.org/abs/2106.01548\n\n[2] https://openreview.net/forum?id=q6bZruC3dWJ\n\n[3] https://openreview.net/pdf?id=0ltDq6SjrfW\n\n[4]  https://openreview.net/pdf?id=157Usp_kbi"
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8647/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700732076287,
                "cdate": 1700732076287,
                "tmdate": 1700732089603,
                "mdate": 1700732089603,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KFqO1KRH5m",
            "forum": "mPOVOwsDOO",
            "replyto": "mPOVOwsDOO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8647/Reviewer_93jL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8647/Reviewer_93jL"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a knowledge distillation approach for knowledge transfer from large scale pre-trained foundation models to specific downstream tasks. The approach leverages the design of encoder and decoder for better communication and to shorten the gap between teacher and student models\u3002"
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The topic of distilling pre-trained knowledge to benefit the downstream tasks is important and practical.\n2. The solution is building up interactive communication between teacher and student models by encoder and decoder is novel and quite interesting.\n3. The results look reasonable.\n4. The paper is clearly written and well presented."
                },
                "weaknesses": {
                    "value": "1. The experiments on movie prediction only cover a narrow scope, and the teacher/student tasks are quite similar with student task is to predict movie from one genre. The results could be more convincing if more varied tasks are involved, and if the \"gap\" between teacher and student is larger.\n2. The approach makes sense but quite straightforward by adding teacher receiving messages. It's worth more discussion on insights of this effect to the teacher model (if not frozen)."
                },
                "questions": {
                    "value": "Same as above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8647/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8647/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8647/Reviewer_93jL"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8647/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698792297647,
            "cdate": 1698792297647,
            "tmdate": 1699637082856,
            "mdate": 1699637082856,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7v3arZ5pkO",
                "forum": "mPOVOwsDOO",
                "replyto": "KFqO1KRH5m",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8647/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8647/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the careful examination and insightful comments for our paper. We address the reviewer\u2019s comments below:\nWeaknesses\n\n1. The experiments on movie prediction only cover a narrow scope, and the teacher/student tasks are quite similar with student task is to predict movie from one genre. The results could be more convincing if more varied tasks are involved, and if the \"gap\" between teacher and student is larger.\n\nWe agree with the reviewer that our method can be further verified with more distribution and task differences. In fact, this is also one of the reasons we choose movielens to be one of our evaluation datasets. The user behaviors on movies from different genres can be entirely different, especially on genres with very few user interactions, such as documentary, it can have totally different user behaviors patterns compared to popular genres such as comedy. We intentionally split the genres to two categories, i.e., dense and sparse during our evaluation. We reported their results separately, where we can see improvement of our methods on both categories. Then on image classification, the distribution between CIFAR10, CIFAR100 and ImageNet can also be very different. We believe that our experiments covered different scenarios. We think it can definitely be improved via experiments in more extreme case, e.g., distilling language modeling teacher to movie lens students, we included this as one of the scenarios we want to verify in our future work. \n\n2. The approach makes sense but quite straightforward by adding teacher receiving messages. It's worth more discussion on insights of this effect to the teacher model (if not frozen).\n\nWe thank the reviewer for the great insights. It is indeed very interesting to discuss the effect on the teacher model if not frozen. In our paper, we conduct analysis by comparing the representation between teacher and student on input images with different classes, where we have shown by introducing the encoder/decoder, we can better align their representations. It can be different when we enable fine-tuning. This is indeed one of our future work, however, some early results suggest fine-tuning doesn\u2019t always align the representations between teacher and student. We added some discussion in our future work section."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8647/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731637491,
                "cdate": 1700731637491,
                "tmdate": 1700731637491,
                "mdate": 1700731637491,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kVdvHYIbF7",
            "forum": "mPOVOwsDOO",
            "replyto": "mPOVOwsDOO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8647/Reviewer_yycb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8647/Reviewer_yycb"
            ],
            "content": {
                "summary": {
                    "value": "The paper interprets the standard knowledge distillation as one-way communication and proposes an interactive communication method to distill knowledge from large models to small models."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea of interactive communication between the teacher model and student model is interesting and novel. The introduction and Related work sections are very clear."
                },
                "weaknesses": {
                    "value": "1. The idea seems novel and interesting, but direct evidence is lacked to support its advantages. The analogy of personal communication, though also interesting, is not enough to explain why the proposed method works. We know that the two models are interacting with each other, but with the concrete communication method, it is hard to say that they are actually \"talking\" to each other like two persons as hypothesized in Introduction. We actually don't know why the proposed method works. In fact, it is hard to understand the rational of the three proposed loss L_{interact}, L_{MC} and L_{SC}. For example, why should the messages of the teacher and the student be consistent (L_{MC}), considering that they are produced by the two models sequentially? \n\nIn addition, the two additional encoders and two additional decoders can account for most unaligned factors governed by the last three terms in the last equation in page 7 because these four modules are learnable. Then how much internal knowledge of the teacher model could be transferred to the student model by modifying its parameters?  \n\nI doubt that the performance improvement largely comes from the four additional modules as they bring more parameters. A desirable baseline approach for comparison is a knowledge distillation method (such as the one illustrated in Fig 1 left) with some additional modules (e.g., adding some modules between the student and teacher). \n\n2. The experiments are not enough to support the advantage of the proposed method. The compared methods are quite old. It is stated that: Note that most recent KD approaches (such as Beyer et al. (2022), Yang et al. (2022a)) focus on one single application such\nas image classification or recommendation, and assume teacher and student share similar tasks. This does not make much sense because the authors could compare with those recent approaches on (same) single applications individually.\n\n3. The presentation is poor. The paper introduces too many notations without a clear rule, in other words, the notations seem to be introduced in an arbitrary manner. For example, the subscripts g and h are used to indicate the student and the teacher, respectively. But in other places, h is used to indicate higher hidden layers of a neural network. This leads to weird notations such as H_{h_{h}}^h, a total of four h's! It is hard to get the meaning of a notation by looking at it. I spent a difficult time in reading the paper. In my opinion, many notations and equations are actually unnecessary. The proposed method is simple, and there is really no need to use such a complicated and tedious manner to describe it. \n\n4. Some technical details are missing. For example, each iteration between the teacher and the student will result in three additional losses (the last three terms in the last equation in page 7). Then with N iterations, does it mean that we need add 3N additional losses? If yes, how should we set the weighting factors? For another example, the method part introduces an encoder-decoder pair for both student and teacher, but in Appendix, only two modules are described. Is the encoder-decoder pair shared by the teacher and the student?"
                },
                "questions": {
                    "value": "The first two points listed above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8647/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699112165232,
            "cdate": 1699112165232,
            "tmdate": 1699637082745,
            "mdate": 1699637082745,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "f527hrD4uT",
                "forum": "mPOVOwsDOO",
                "replyto": "kVdvHYIbF7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8647/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8647/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the comments, careful examination of our paper, and understanding of our key insights. And we address the reviewer\u2019s comments below:\n\nWeaknesses:\n1. direct evidence is lacking to support its advantages.  For example, why should the messages of the teacher and the student be consistent (L_{MC}), considering that they are produced by the two models sequentially? I doubt that the performance improvement largely comes from the four additional modules as they bring more parameters.\n\nWe thank the reviewer for the question. We provided some evidence that our interactive communication method works better than other one-way communication based methods by some case study included in Section 4.4. We showed that the ``undistillable\u2019\u2019 class between teacher and student can be distillable after the training of encoder and decoder. We believe the training of encoder and decoder with the consistency loss can provide two benefits: (1) Prevent inefficient knowledge transfer from domain shift between pre-trained data and downstream tasks. (2) Enable the teacher to understand what information students can provide, because the student cannot get complex representation from the input as the teacher due to capacity gap, this will allow the teacher to teach with limited representation suitable for the student\u2019s capacity. \nWe provided a detailed discussion on the current design regarding the reviewer\u2019s questions in Appendix 6.1 (for design simplicity and future work). To verify our explanation and insights, we also show that (figure 3) using interactive communication, we can bridge the capacity gap between teacher and student for \u201cundistillable classes\u201d[1].\n\nFor the question on performance improved due to additional parameters: our method didn\u2019t add significant computation costs compared to existing KD methods. Costs added by encoder and decoder are comparable to feature distillation methods. But our costs are (k-1 times) higher when we are doing interactive communication in k iterations. Besides discussion in section 4.3, we will add a quantitative analysis in our revision. \n\n2. The experiments are not enough to support the advantage of the proposed method.\n\nOne thing we want to highlight is that our paper is not targeted at improving knowledge distillation in general. Our focus is to extend knowledge distillation to the application of efficient knowledge transfer from pre-trained large models to smaller models for downstream tasks. Therefore, different from general KD methods, our proposed method considers two additional challenges: (1) the teacher model is trained differently using pre-trained tasks. (2) fine-tuning the teacher to arbitrary downstream tasks is not practical. We believe this is a very important application as pre-trained foundation models have shown great advantages in different domains such as NLP and CV, and directly using such a large model is not realistic as fine-tuning is not practical, nor serving the fine-tuned large model.\n\nTherefore, we don\u2019t compare our methods to the KD algorithms which cannot be used in our application without non-trivial modification. Another reason is that since our application is on efficient knowledge transfer from large pre-trained models to smaller downstream models, we don\u2019t want our method to be tied to a specific domain (e.g., CV), instead, we evaluate our method on various domains including recommendation system and CV. Therefore, many KD methods targeting a single domain, e.g., CV, or NLP, cannot be directly used to compare with our method.\n\nHowever, with our proposed interactive communication framework, we can integrate many feature distillation methods to further improve KD in different domains. As an example, the two papers the reviewer pointed out can be very useful to improve KD for ViT and they can be integrated by modifying the distance function d in section 3.3 (for example, mimincing and generating in ViTKD can be used to improve state and message consistency losses).\n\n3. The presentation is poor.\n\nWe will make significant improvements to make our presentation clear in our revision. \n\n4. Some technical details are missing. \n\nWe thank the reviewer for pointing this out! We include our experimental designs in Appendix 6.2 and 6.3. For the losses introduced in interaction, only the interactive loss \u201cL_{interact}\u201d are added multiple times with equal loss. We made this clear in our revision. \n\n[1] https://openreview.net/forum?id=q6bZruC3dWJ"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8647/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700731346601,
                "cdate": 1700731346601,
                "tmdate": 1700731346601,
                "mdate": 1700731346601,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "G8dN577i7v",
            "forum": "mPOVOwsDOO",
            "replyto": "mPOVOwsDOO",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8647/Reviewer_njLH"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8647/Reviewer_njLH"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents a new distillation framework that aims to distill knowledge from a pretrained foundation model to a smaller model for downstream tasks. The method is inspired by an interactive communication model, and instantiated by an encoder-decoder architecture. Such a design allows transferring knowledge adaptively based on student model capacities and handling different task distributions. Experiments on vision and recommendation are conducted to verify its effectiveness."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. I haven't kept up with recent developments in KD, both problem setting and the proposed algorithm appear to be novel given the context provided in the paper.\n2. Distillation across different tasks or distributions is challenging problem, yet the proposed model performs well in both vision and recommendation applications.\n3. The paper is generally well written, the idea is easy to follow. The analogy between KD and communication models is interesting. It provides a unified view of existing KD approaches and is a clever choice for motivational purpose."
                },
                "weaknesses": {
                    "value": "1. While the method intuitively makes sense and I understand the paper is centered on applications, it would be nice to make the paper more formal, e.g. by defining different task distributions and the problem you are to tackle. \n2. The link between the method and different task distributions does not seem very clear (partially also due to a lack of formality). Particularly, I still do not fully understand why extending KD to a two-way interactive communication process is relevant solving distribution shift.\n3. In terms of writing, I do not find the first half of the paper (section 1 and 2) very informative. I think empirical studies in 4.2-4.4 are especially useful for justifying such type of approach, but regrettably they are not highlighted in the main paper. \n\nMinors: better use vector graphics such pdf rather than bitmap for figures."
                },
                "questions": {
                    "value": "1. How are $l_g$ and $h_g$ chosen? There are also many other hyperparameters, how are they tuned?\n2. Can you provide more insights on the question in weakness 2?\n3. How is the approach related with foundation models, as teacher models are just some pretrained models, the same as standard KD setting?\n4. How distribution shifts are reflected in experiments?\n5. Can you discuss the connection with existing cross-task KD approaches?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8647/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699428455327,
            "cdate": 1699428455327,
            "tmdate": 1699637082620,
            "mdate": 1699637082620,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GxNYBBYHlu",
                "forum": "mPOVOwsDOO",
                "replyto": "G8dN577i7v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8647/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8647/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the thorough comments and insightful questions, especially pointing out the weak connection between our method and distribution shifts. We address the reviewer\u2019s comments below:\n\nWeaknesses:\n1. it would be nice to make the paper more formal, e.g. by defining different task distributions and the problem you are to tackle.\n\nThanks for the suggestion, we added a formal definition of our problem in our Appendix, along with the formal description of the algorithm. However, the definition of task/data distribution seems hard to be added in the formal definition since we only empirically verify our hypothesis on the distribution shift between pre-training and downstream datasets. The theoretical quantification of the differences is still challenging. \n\n2. The link between the method and different task distributions does not seem very clear. \n\nThe connection between our method and task distribution lies in the pre-training fine-tuning paradigm of large foundation models. In this paper, we are not improving the knowledge distillation algorithm in general, but extending knowledge distillation to a very specific use-case: leveraging the knowledge of pre-trained large models to teach smaller student models for downstream tasks, without extensive or at-all fine-tuning. We believe this is a very important application as pre-trained foundation models have shown great advantages in different domains such as NLP and CV. In this case, the teacher cannot adapt to the downstream tasks\u2019 distribution, which is often different from the pre-training distribution. Therefore, we propose to use the encoder and decoder to help the teacher learn the downstream tasks\u2019 distributions without modifying the teacher model. We strengthened our introduction to make this clear. \n\n3. I do not find the first half of the paper (section 1 and 2) very informative. I think empirical studies in 4.2-4.4 are especially useful for justifying such type of approach, but regrettably they are not highlighted in the main paper.\n\nThanks for pointing this out! Following your suggestion, we modified our paper to mention our empirical study results in our introduction to help justify our approach. \n\nMinors: better use vector graphics such as pdf rather than bitmap for figures.\nThanks for the suggestion, we replaced our figures to pdfs."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8647/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730824337,
                "cdate": 1700730824337,
                "tmdate": 1700730824337,
                "mdate": 1700730824337,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "61Pf8VEiDH",
                "forum": "mPOVOwsDOO",
                "replyto": "G8dN577i7v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8647/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8647/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "To answer reviewer's questions:\n1. How is hg and lg chosen? There are also many other hyperparameters, how are they tuned?\n\nWe don\u2019t tune h_g and l_g, instead, l_g is just after the first input projection layer and h_g is before the last classification layer. Therefore, we enable teacher and student to align their input and label space via the encoder and decoder. We totally agree with the review that deciding which layers are lower and higher is tricky. The point we want to highlight is that we don\u2019t tune this as hyper-parameters. We believe (and use the setup) that the lower should be as low as possible and higher can be as high as possible. Because this is to solve the challenge of domain and task differences between teacher and student. The question of what proper layer and techniques to distill can be partially solved using existing `feature distillation` techniques, as they can be extended into our framework by modifying the distance function d in the equations in section 3.3. There are discussions in some recent feature distillation works [1] and [2] on which layers and losses to use for feature distillation. We will add more discussion and clarification in our revision. \n\nAs for other hyper-parameters, we performed grid search of some of hyper-parameters such as loss weights for our method and baseline methods. We included the detailed hyper-parameter tuning in Appendix 6.3. \n\n2. Can you provide more insights on the question in weakness 2?\n\nDiscussed above.\n\n3. How is the approach related with foundation models, as teacher models are just some pretrained models, the same as standard KD setting?\n\nThe key difference here is that we assume fine-tuning teachers for downstream tasks is costly. And also the pretraining dataset can be very general and broad covering various distributions but the downstream tasks are relatively specialized and different from the pre-training dataset and tasks. We believe this differentiates our problem setup with the standard KD setting.\n\n4. How distribution shifts are reflected in experiments?\n\nWe designed our experiments as the pre-trained teacher model is not fine-tuned for downstream tasks. For example, on Movielens, the pretrained model sees data from user ratings on all movie genres while the student is trained on a specific genre. We can see that students for different genres benefit differently from the teacher model using different baseline methods.\n\n5. Can you discuss the connection with existing cross-task KD approaches?\n\nThanks the reviewer for pointing out this related literature. By comparing our method with some recent cross-task KDD approaches [3,4,5], our proposed method and problem setup have the following differences:\n(1) The data for training the teacher and student is largely different. Existing cross-task KD don\u2019t assume the distribution differences on the input side between pre-training data and fine-tuning data.\n(2) Limited ability in modifying the teacher. \n(3) Method: These work adopt typical or modified feature distillation paradigm, where our method introduced the inter-active communication paradigm. \n\n\n[1] https://arxiv.org/pdf/2209.02432.pdf\n\n[2] https://arxiv.org/pdf/2104.09044.pdf\n\n[3] Ye et al., Distilling Cross-Task Knowledge via Relationship Matching\n\n[4] Li et al., Prototype-guided Cross-task Knowledge Distillation for Large-scale Models\n\n[5] Yang et al., Cross-Task Knowledge Distillation in Multi-Task Recommendation"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8647/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700730845675,
                "cdate": 1700730845675,
                "tmdate": 1700732112283,
                "mdate": 1700732112283,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]