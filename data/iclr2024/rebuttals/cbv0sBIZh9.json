[
    {
        "title": "Diffusion Models for Multi-Task Generative Modeling"
    },
    {
        "review": {
            "id": "MpwSqoeliN",
            "forum": "cbv0sBIZh9",
            "replyto": "cbv0sBIZh9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4645/Reviewer_4Fko"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4645/Reviewer_4Fko"
            ],
            "content": {
                "summary": {
                    "value": "In this study, four different scenarios for multi-task learning are presented to enhance the performance of image generation and classification. The authors emphasize two specific settings, namely masked-image training and joint image-label generation. Through experiments conducted on ImageNet, the paper successfully demonstrates the viability of the suggested multi-task frameworks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Multi-task learning in generation, the subject of investigation in this study, is a captivating and relatively unexplored area of research.\n\n- The methods proposed in this study exhibit simplicity and effectiveness when applied to the ImageNet-64 baseline."
                },
                "weaknesses": {
                    "value": "- The explanation of the implementation details (2.2.4) regarding the encoders and decoders can be perplexing, particularly in terms of how the classification label is encoded and how it is \"aggregated\" with other tensors.\n\n- The theoretical portion of the paper does not provide a clear and comprehensive explanation of the proposed multi-task models, as the focus of the paper is primarily empirical. It would be beneficial to have a more detailed architectural explanation of the models designed for various multi-task settings. If space constraints are a concern, the theoretical portion can be entirely moved to the appendix.\n\n- While the proposed method demonstrates significant improvements on the ImageNet-64 benchmark, it lacks experiments on more widely used and challenging benchmarks, as well as comparisons with newer generation models. Additionally, some auxiliary experiments in the Appendix utilize stable diffusion, which is now commonly employed as a baseline, while the primary experiments do not present any relevant results. The absence of these experiments makes it challenging to provide sufficient justification for the superiority of the proposed complex multi-task training pipeline."
                },
                "questions": {
                    "value": "- Could you please elaborate on the specific encoders employed for each task and the type of data they operate on? It would be helpful to understand the label space for each task, particularly regarding how a one-hot classification vector is encoded in the latent space.\n\n- It is unclear from my understanding what the single-task learning model for ImageNet classification, depicted in Figure 6, entails. Further clarification would be appreciated.\n\n- To ensure the readiness of the paper for publication, it is crucial to address the concerns mentioned in the Weakness section. These issues should be at least partially resolved. Thank you for your attention to these matters."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4645/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4645/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4645/Reviewer_4Fko"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4645/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698076983930,
            "cdate": 1698076983930,
            "tmdate": 1700633261381,
            "mdate": 1700633261381,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yGeVqBhdkW",
                "forum": "cbv0sBIZh9",
                "replyto": "MpwSqoeliN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4645/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4645/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review and valuable feedback. We appreciate the opportunity to address your concerns:\n\nFor Weakness 1: Implementation Details (2.2.4):\nResponse: We acknowledge the complexity of the implementation details in Section 2.2.4. More detailed descriptions for different task settings, including how label data is encoded, are provided in Section 4 (Experimental Part) and Appendix E. Please refer to those sections for a comprehensive understanding, which we will put effort in to make it more clear in the revision.\n\nSpecifically, for label data, we utilize the original U-Net structure from the ADM model, accepting input with a noisy image and label (from an embedding layer as a conditional input). In our case, the noisy image is set as the original image, and label embeddings are jointly encoded through the U-Net.\n\nFor Weakness 2: Theoretical Portion:\nResponse: Thanks for the suggestions. We respectfully disagree that our work is primarily empirical. Indeed, there is no theory on how to design a diffusion model that can handel multi-modal multi-task data by incorporating different task information in the forward process like our model. We consider this as one key contribution of our work. We believe it is necessary to present the underlying theoretical perspective of our model, which will provide guidance to design specific model architectures for multi-task generative modeling. However, we agree that the presentation can be simplified and emphasizes more on the model design part. We will be happy to do a thorough revision of our paper to reflect the suggestions from the reviewer.\n\nFor Weakness 3: Experiments on Widely Used Benchmarks:\nResponse: We understand the importance of benchmarking on widely used datasets. In our main experiments, we train on both ImageNet-64 and ImageNet-128 based on the ADM codebase, providing a well-documented baseline with accepted evaluation scores for comparison. Training larger models on datasets like LAION from scratch is computationally challenging. More importantly, there are many factors such as the data quality in the training that can significantly impact the final model performance, thus making directly comparing with these models in a fair and faithful way difficult. We believe a quantitative comparison with the ADM baseline on ImageNet sufficiently demonstrates the effectiveness of our method.\n\nFor question on what the single-task learning model for ImageNet classification, depicted in Figure 6, entails.\nResponse: The single-task learning method in Figure 6 is the simple U-Net trained based on pure supervised learning, which is the most direct way to do classification."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4645/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700455553024,
                "cdate": 1700455553024,
                "tmdate": 1700455553024,
                "mdate": 1700455553024,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tTLE1Dwbcq",
                "forum": "cbv0sBIZh9",
                "replyto": "MpwSqoeliN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4645/Reviewer_4Fko"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4645/Reviewer_4Fko"
                ],
                "content": {
                    "title": {
                        "value": "Some concerns are addressed, while some are remained. I will raise the scores to borderline accept."
                    },
                    "comment": {
                        "value": "I acknowledge the authors' efforts in providing clarification regarding weakness 1 and weakness 2.\n\nHowever, I still find the writing in the current version of the paper to be unsatisfactory because of the limited space. The description of the implementation details of the method in section 2.2.4 and the experiment section remain unclear. Additionally, there are several typos and format issues in the reference section.\n\nFurthermore, for weakness 3, the absence of a comparison with the prevalent latent diffusion raises doubts about the effectiveness of the proposed method."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4645/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633229701,
                "cdate": 1700633229701,
                "tmdate": 1700633286339,
                "mdate": 1700633286339,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ukaw9d3pDL",
            "forum": "cbv0sBIZh9",
            "replyto": "cbv0sBIZh9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4645/Reviewer_oMx9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4645/Reviewer_oMx9"
            ],
            "content": {
                "summary": {
                    "value": "This paper extends the diffusion process in a multi-task learning, with a shared diffusion space for all task data. The paper verifies its formulation in multiple variations of a two-task setting (though it should be relatively straightforward to generalise into many tasks), showing improved performance over standard single task learning baselines. The paper also lists several accompanied architecture designs for the proposed multi-task diffusion formulation, based on different choices of data domains."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Disclaimer: I am probably not the right person to review this paper. I have background in multi-task learning but have limited experience in diffusion models. The paper seems to more focus on diffusion models and have limited context in multi-task learning.\n\n-\tThe problem formulation is clean and straightforward. I have no problem understanding its derivation of ELBO and loss functions.\n-\tThe presented architectures consider multiple choices of data types."
                },
                "weaknesses": {
                    "value": "I will present my concerns and weaknesses here fully based on my experience in multi-task learning.\n\n1.\tThe related work and experiments are all around diffusion models with very trivial baselines and simple experiments. I understand the author shape this paper as the one of the first to explore diffusion in a multi-task learning setting. But at the same time, I saw some other papers also using the multi-task learning technique in diffusion to enforce geometric constrains, particularly in the 3d shape synthesis.  For example, Wonder3D (https://www.xxlong.site/Wonder3D/) and DreamCraft3D (https://mrtornado24.github.io/DreamCraft3D/) are two examples applying diffusion on both RGB and normal maps to improve multi-view/geometric consistency. I am aware both papers were released very recently and seem to be submitted to ICLR as well, I am just wondering how the proposed paper differentiates itself from the straightforward implementation of using multi-task learning in diffusion like bering used in these two papers, and from which both were based on the same assumption of using shared diffusion space as well?\n2.\tAs such, I am not exactly sure how to comment and understand the performance of the proposed formulation, since the experiment setting is simple and only compare with simple single task baseline. For example, it might be more intriguing to see i) the performance / formulation without having a shared diffusion space, ii) how to compare with multi-task predictive models without any diffusion (e.g. MTAN, Adasahre, Cross-Stitch, PadNet, as multi-task learning in computer vision is an active research area.), iii) is task conflict issue in multi-task learning alleviated? What are the other benefits other than improved performance?"
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4645/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4645/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4645/Reviewer_oMx9"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4645/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698701425762,
            "cdate": 1698701425762,
            "tmdate": 1700937093404,
            "mdate": 1700937093404,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Qs4TXC4N9M",
                "forum": "cbv0sBIZh9",
                "replyto": "ukaw9d3pDL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4645/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4645/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "We appreciate your thoughtful feedback on our work and would like to address the concerns raised in your review. We are committed to addressing these points in our revised manuscript, providing additional clarity and context where necessary.\n\nFor Weakness 1: We respectfully assert that our choice of baselines and experiments is non-trivial. ADM by OpenAI, a baseline model, stands as one of the state-of-the-art diffusion generative models. Additionally, conducting experiments on the ImageNet dataset, a large and computationally demanding dataset, is far from trivial. For instance, a single experiment on 8 A100 GPU server takes several days. We believe our experiments are well-justified given the computational demands of the tasks and the significance of the chosen baselines.\n\nThank you for pointing out these interesting references. We want to point out that an important difference between these methods and ours is that they are essentially based on the single-task diffusion model, i.e., the underlying principle is the conditional diffusion model, with additional losses to introduce some regularizations for the 3D generation. We consider at least two differences from our work: 1) the conditional generation only models a conditional distribution, whereas ours models the joint distribution of different modality data. This allows us to perform not only joint generation, but also conditional generation as the purpose of the references, i.e., ours is a more general generation framework. 2) The addition of extra losses in the reference works is mostly ad hoc, whereas in our framework, all the losses are attributed from the new multi-task ELBO, providing a more general and principled approach for multi-task diffusion generation modeling.\n\nFor Weakness 2: We acknowledge the complexity of our experimental settings, focusing on incorporating multi-task learning into diffusion models\u2014a non-trivial task given the unique characteristics of diffusion models. We appreciate your specific questions and provide the following responses:\n1. Performance/formulation without a shared diffusion space corresponds to the baseline of modeling tasks using independent single-task diffusion models, which we have compared with.\n\n2. How to compare with multi-task predictive models without any diffusion: Since we focus on generative modeling, it is not exactly clear to us how to compare our model with these predictive models. However, we believe some ideas can be incorporated into our framework for further performance improvement, which we would like to leave as interesting future work.\n\n3. Is task conflict issue in multi-task learning alleviated: As a first work, the task conflict issue alleviation was not investigated in this study, which we believe would need significant effort. THus, we consider it as an interesting avenue for future exploration.\n\n4. What are the other benefits other than improved performance: We believe the most important benefit of our framework is that it enables multi-task generative modeling, i.e., one can model multiple generation tasks in a single model without hurting single-task performance. This can not only save training cost, but also alleviate the burden of maintaining different models for different tasks."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4645/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700455018614,
                "cdate": 1700455018614,
                "tmdate": 1700455018614,
                "mdate": 1700455018614,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "4eCu2XTW2a",
                "forum": "cbv0sBIZh9",
                "replyto": "Qs4TXC4N9M",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4645/Reviewer_oMx9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4645/Reviewer_oMx9"
                ],
                "content": {
                    "title": {
                        "value": "Response"
                    },
                    "comment": {
                        "value": "I'd like to thank for the authors' thorough explanation and clarification. I have now a better understanding of this paper as it is set apart from other straightforward multi-task learning with conditional diffusion by learning a joint distribution with a multi-task ELBO loss.\n\nAs such, I am keen to improve my rating conditioned on the following adjustments to the paper:\n1. Adding this conditional distribution for multi-task diffusion formulation as an important baseline on small-scale datasets.\n2. Adding additional discussions / related works on multi-task learning designed for predictive models, as it's easier to be appreciated by such community."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4645/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700588908276,
                "cdate": 1700588908276,
                "tmdate": 1700588908276,
                "mdate": 1700588908276,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kiYHqeoBK0",
                "forum": "cbv0sBIZh9",
                "replyto": "ukaw9d3pDL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4645/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4645/Authors"
                ],
                "content": {
                    "title": {
                        "value": "experiment update"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nAs the deadline for rebuttal is approaching, we just want to update with you the CIFAR-10 experiment we are running. We have run the experiment on CIFAR-10 with MT-diffusion for masked-image training for around 8 hours on a single A100 GPU, as well as the conditional diffusion model that generate images from random masked images. The experiments are still not converging. In order to catch up with the rebuttal deadline, we have used the intermediate checkpoints (at the same running time) to generate 10K images, and calculate the FID and IS scores. The results are listed as follows:\n\n|Model | IS | FID|\n|:----|:----:|:----:|\n|MT-Diffusion\t|11.25\t\t|7.33|\n|Cond-DIffusion\t|10.11\t\t|8.43|\n\nPlease note since the algorithm has not converged, these results are not comparable to current state of the art. However, we believe our final results can catch up quickly. More importantly, the current results indicate our model by modeling the joint distribution performs better than the simple conditional diffusion model. We will continue the experiment and add the final results to the final revision.\n\nWe appreciate your willingness to adjust your scores. Given our current and existing results, as well as our above rebuttals, can you please consider doing so? Thank you very much."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4645/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700715109691,
                "cdate": 1700715109691,
                "tmdate": 1700715618683,
                "mdate": 1700715618683,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zDyctFvJyM",
            "forum": "cbv0sBIZh9",
            "replyto": "cbv0sBIZh9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4645/Reviewer_KnqL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4645/Reviewer_KnqL"
            ],
            "content": {
                "summary": {
                    "value": "This work discusses the potential of diffusion-based models in generative modeling. While current diffusion models excel in single-generation modeling, the paper explores the possibility of extending them for multi-task generative training. The authors introduce a unified multi-task diffusion model, MT-Diffusion, that operates in a shared diffusion space. This model aggregates information from multiple types of task-data and employs a shared backbone denoising network with task-specific decoder heads. The paper presents several multi-task generation settings, such as image transition, masked-image training, joint image-label, and joint image-representation generative modeling. Experimental results on ImageNet demonstrate the model's effectiveness in multi-task generative modeling."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper introduces MT-Diffusion, a novel approach to multi-task generative modeling using diffusion models.\n- The proposed model effectively aggregates information from different task-data types, enhancing its versatility.\n- Extensive experiments on ImageNet validate the model's effectiveness and potential in various multi-task generative modeling scenarios."
                },
                "weaknesses": {
                    "value": "- Experiments are done on low resolution and small datasets, undermining its effectiveness. \n- The paper is lack of model details for each task"
                },
                "questions": {
                    "value": "- Can you explain the model structure for each task and result? Especially results in Fig 4 and Fig 5."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4645/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4645/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4645/Reviewer_KnqL"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4645/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809849188,
            "cdate": 1698809849188,
            "tmdate": 1700582398844,
            "mdate": 1700582398844,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ap0GMQwaWC",
                "forum": "cbv0sBIZh9",
                "replyto": "zDyctFvJyM",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4645/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4645/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful evaluation and constructive feedback. We appreciate the opportunity to address your concerns:\n\nFor Weakness 1: Experiments on Low Resolution and Small Datasets:\nResponse: We respectfully disagree with the characterization of our experiments. While the primary experiments were conducted on the ImageNet1K dataset, which is widely acknowledged as substantial in the generative modeling community, we acknowledge the existence of larger datasets such as LAION. However, due to the prohibitive cost of pretraining from scratch on these datasets, they are not considered standard for comparisons.\n\nWe conducted experiments on two resolution levels, 64x64 and 128x128, leveraging our computational resources. These experiments, although time-consuming, are deemed sufficient to demonstrate the effectiveness of our method. Additionally, qualitative experiments on the larger LAION dataset, based on the pretrained stable diffusion model, are detailed in the appendix for further insights.\n\nFor Weakness 2: Lack of Model Details for Each Task:\nResponse: We appreciate your observation, and we want to clarify that all model details are provided in the experiment section and Section E of the Appendix, adhering to page limits. To enhance clarity, we are willing to paraphrase the paper and incorporate more detailed experimental settings into the main text.\n\nFor the Question on Model Structure:\nResponse: Detailed information on experimental settings and model structures is provided in Appendix E. The model structures closely resemble the ADM model, incorporating a shared U-Net backbone shared by all tasks, as illustrated in Figure 3. Task-specific encoder structures are introduced in our model for added specificity. For a comprehensive understanding, please refer to Appendix E, and feel free to reach out with any additional questions."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4645/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700454885411,
                "cdate": 1700454885411,
                "tmdate": 1700454885411,
                "mdate": 1700454885411,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "gdPte4Ahh3",
                "forum": "cbv0sBIZh9",
                "replyto": "Ap0GMQwaWC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4645/Reviewer_KnqL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4645/Reviewer_KnqL"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for response"
                    },
                    "comment": {
                        "value": "Thank you for your response. \n\nAfter reviewing the rebuttal and the supplementary materials, I decide to increase my score."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4645/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700582505334,
                "cdate": 1700582505334,
                "tmdate": 1700582505334,
                "mdate": 1700582505334,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9KncLktgWx",
            "forum": "cbv0sBIZh9",
            "replyto": "cbv0sBIZh9",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4645/Reviewer_bBAj"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4645/Reviewer_bBAj"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel approach to generative modeling by extending diffusion-based models to a multi-task learning framework. The proposed Multi-Task Diffusion Model (MT-Diffusion) is capable of generating multi-type data (e.g., images and their corresponding labels) within a single unified model. It integrates multi-task learning losses into the diffusion process, supported by a theoretical foundation. The authors propose and experiment with several multi-task generative settings, including image transition, masked-image training, joint image-label, and joint image-representation generation, demonstrating the framework's versatility and effectiveness on the ImageNet dataset. MT-Diffusion handles multiple data types through a shared diffusion space, with a forward process aggregating multi-task data and a reverse process using task-specific decoder heads to reconstruct data for different tasks. This approach results in a novel multi-task variational lower bound that generalizes the standard diffusion model, achieving simultaneous multi-task generation without compromising individual task performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper provides a sound theoretical explanation for the utility of a multi-task loss using the Evidence Lower Bound (ELBO).\n- The idea of enabling multi-task learning for inputs of various modalities through a shared latent space is innovative.\n- Considering the connection to guided diffusion models is a thoughtful approach that takes into account the expansiveness of the research."
                },
                "weaknesses": {
                    "value": "- The paper does not specify the extent of increased training costs resulting from the proposed methodology.\n- While significant performance improvements are shown across various metrics, including FID, the analysis lacks control of variables to confirm that these improvements truly stem from a multi-task setting. Following the previous point, it is my view that the proposed methodology likely entails considerably higher training costs and an increased number of data samples seen by the model compared to baseline learning. Therefore, it is necessary to deeply analyze whether the performance improvement is due to positive transfer resulting from multi-task learning, or merely an effect akin to data augmentation from masked samples. The absence of such analysis has influenced my evaluation towards rejection.\n- (Minor point) There is prior (possibly concurrent) work proposing a multimodal, multi-task diffusion process through a Versatile Diffusion[1] multi-flow diffusion process.\n- (Minor point) The caption of Table 2 does not provide sufficient information, making it difficult for the reader to interpret the experimental results.\n\n[1]: Xu, Xingqian, et al. \"Versatile diffusion: Text, images and variations all in one diffusion model.\" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023."
                },
                "questions": {
                    "value": "- How do you think the proposed off-the-shelf guidance method would integrate with previous research focused on efficient training of diffusion models, such as P2-Weighting[2], Min-SNR[3], ANT[4], and Task Routing[5]? Particularly, Min-SNR[3], ANT[4], and Task Routing[5] view the methodology of diffusion models as naturally creating a multi-task situation through various time steps, each requiring different levels of denoising. Considering this study aims to extend multi-task learning by increasing the input modality for denoising, there seems to be an overlap. I would be interested to hear your insights on this matter.\n\n\n[2]: Choi, Jooyoung, et al. \"Perception prioritized training of diffusion models.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.\n\n[3]: Hang, Tiankai, et al. \"Efficient diffusion training via min-snr weighting strategy.\" arXiv preprint arXiv:2303.09556 (2023).\n\n[4]: Go, Hyojun, et al. \"Addressing Negative Transfer in Diffusion Models.\" arXiv preprint arXiv:2306.00354 (2023).\n\n[5]: Park, Byeongjun, et al. \"Denoising Task Routing for Diffusion Models.\" arXiv preprint arXiv:2310.07138 (2023)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4645/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4645/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4645/Reviewer_bBAj"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4645/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699430833834,
            "cdate": 1699430833834,
            "tmdate": 1699636444550,
            "mdate": 1699636444550,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "bB1vv8Df4Z",
                "forum": "cbv0sBIZh9",
                "replyto": "9KncLktgWx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4645/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4645/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review and valuable feedback. We appreciate the careful consideration given to our work. Below, we address each of the raised points and provide additional clarification.\n\nFor Weakness 1: Increased Training Costs:\nWe appreciate your concern regarding potential increased training costs. In our two-task setting, the additional computational overhead is minimal, amounting to approximately 10% more time per iteration than a pure single-task diffusion model. Importantly, our model remains significantly more efficient than training two individual diffusion models for the two tasks separately in terms of both time and storage efficiency. We'd like to highlight that our method not only handles multi-task learning effectively but also converges faster, as indicated in Table 2. We will incorporate this discussion into the final revision for enhanced clarity.\n\nFor Weakness 2: Where performance improvement comes multi-task learning:\nWe wish to argue that the additional data augmentation in the masked image training is also a part of the multi-task setting, offering additional benefits without incurring extra overhead. We believe separating data augmentation from the multi-task setting is challenging, as that is part of the multi-task framework. \n\nWe do not fully understand your point about positive transfer and data augmentation, because the auxiliary task only contains data augmentation. Thus, we believe the data augmentation indeed attributes to the performance gain, which can also be understood as positive transfer. We will explicitly discuss the insight in the revised manuscript.\n\nFor Weakness 3: Comparison with Prior Work:\nThank you for bringing up the reference to Versatile Diffusion. After a careful review, we recognize that it is indeed a concurrent work that we did not initially notice. The Versatile diffusion mainly focuses on developing new neural architectures that make different tasks interact with each other within the single-task diffusion framework. We believe that our work is different, as it not only introduces a novel neural architecture but also generalizes the single-task diffusion in the loss function. We will enrich the discussion on the relationship between our model and Versatile Diffusion in the revision.\n\nFor Weakness 4: Table 2 Caption:\nWe appreciate your feedback on the clarity of the Table 2 caption. We acknowledge that additional details about different variants of our model are described in Sections 4.1 and 4.2. To enhance clarity, we will provide more comprehensive information in the title for improved interpretability in the revised manuscript.\n\nFor Questions: Relation with Previous Research:\nWe are grateful for the references to inspiring papers interpreting standard diffusion models from a multi-task learning perspective. While we acknowledge the relevance of these papers, we maintain that our ideas are orthogonal, with no technical overlap. We are open to exploring the incorporation of these insights into our framework for potential further improvement\u2014a promising avenue for future work.\nThank you once again for your thoughtful review. We are committed to addressing each of these points thoroughly in the final revision, ensuring a more robust and comprehensive presentation of our work. We hope the reviewer can reconsider his/her decision."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4645/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700454803122,
                "cdate": 1700454803122,
                "tmdate": 1700454803122,
                "mdate": 1700454803122,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "u2iAGIgjSF",
                "forum": "cbv0sBIZh9",
                "replyto": "9KncLktgWx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4645/Reviewer_bBAj"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4645/Reviewer_bBAj"
                ],
                "content": {
                    "comment": {
                        "value": "The concerns \bI have raised have been largely addressed, and I appreciate the efforts made by the authors in this regard. However, my concern regarding Weakness 2 remains unaddressed. I acknowledge that it may be challenging to completely disentangle the effects of data augmentation from multi-task learning. Nonetheless, it is imperative to support, through experimental or theoretical analysis, whether the proposed multi-task scenarios actually synergized to enhance performance. For instance, assessing task affinity to verify positive transfer between tasks would significantly bolster the validation of this research's utility."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4645/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545708601,
                "cdate": 1700545708601,
                "tmdate": 1700545749944,
                "mdate": 1700545749944,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "O62TUuYzKJ",
                "forum": "cbv0sBIZh9",
                "replyto": "9KncLktgWx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4645/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4645/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your feedback"
                    },
                    "comment": {
                        "value": "Dear Reviewer,\n\nWe appreciate your feedback on our rebuttal. We are happy to learn that you are mostly satisfied with our rebuttal, and only have some concerns on verifying whether the performance comes from positive transfer. We are happy to explore and performance more ablation studies on this. Specifically, we have set up to run the following experiments: train our multi-task diffusion model on 5 tasks to simultaneously learning to generate original images,, masked images, corresponding captions, random captions, and class labels. Intuitively, generating original images and masked images are two closest tasks, which is expected to have more positive transfers. While the other tasks such as generating random captions are completely independent of generating images, thus we expect the performance would drop if the task of generating random captions are added. We are working on the experiments now, but please note this is a more challenging setting, thus we are not expecting to be able to produce some results before the rebuttal deadline. However, we are committed report the results in our revision.\n\nEven though we are not able to provide the aforementioned results in the rebuttal period ending in one day, we wish to point out our current results actually have the implications that more similar tasks tend to have more positive transfers. For example, comparing the following two settings indicated in Table 2: 1) simultaneously generating images and the corresponding masked images; 2) simultaneously generating images and the corresponding labels. The former two tasks are considered closer as they are in the same data space. And from Table 2, we can clearly see that the former setting (Generation with Masked-Image Training (Section 4.1)) outperforms the latter one (Generation with Joint Image-Label Modeling (Section 4.2)), indicating there are more positive transfers in the former setting.\n\nWe hope this can address your concerns, and our explanation can provide more information on your final decision. Thank you very much for your time."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4645/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700590341246,
                "cdate": 1700590341246,
                "tmdate": 1700593988278,
                "mdate": 1700593988278,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]