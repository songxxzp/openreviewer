[
    {
        "title": "Efficiency Pentathlon: A Standardized Benchmark for Efficiency Evaluation"
    },
    {
        "review": {
            "id": "Lc7a98rOFG",
            "forum": "Qyp3Rni2g1",
            "replyto": "Qyp3Rni2g1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6018/Reviewer_pV3M"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6018/Reviewer_pV3M"
            ],
            "content": {
                "summary": {
                    "value": "Evaluating the model efficiency is an important evaluation aspect for practical applications. The paper claimed that existing metics, such as FLOPs often did not reflect the advantages of the models in real-world applications. So, it proposes efficiency Pentathlon, a benchmark of holistic and realistic evaluation of model efficiency, including five ways: standard hardware enviroment, four distinct evaluation scenarios, diverse metrics for comprehensive efficiency evaluation, a evaluation software library, and flexiable evaluations. The established benchmark contain three NLP tasks and the corresponding datasets (WMT14 DE-EN, GSM8K, RAFT). The evaluation results show that the proposed Pentathlon could drastically reduce the workoad to make fair and reproducible effciency comparisons."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1) Efficiency evaluation is very important for model evlaution and seldomly addressed before.\n2) The proposed five evaluation aspects are interesting and novel."
                },
                "weaknesses": {
                    "value": "1) The whole paper is not very clear. In the section 2,  the reason that considering the proposed five aspects for effeciency evaluation is not described clearly.\n2) The experimental parts are not very suffcient. Only two tasks are selected which makes the results not very convincing."
                },
                "questions": {
                    "value": "1) Table 1 is not very clear. What is the schema meaning in table 1, such as Acc., TP., Latency, Mem., etc. The authors should describe them in the tabel title.\n2) What did only three kinds of NLP tasks are selected? I concern what are the results in other NLP tasks.\n3) Why this benchmarks could be trusted and applied when evaluting the real-applications. The authors should prove the advantages of the proposed benchmarks and plantforms."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6018/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698660320784,
            "cdate": 1698660320784,
            "tmdate": 1699636646232,
            "mdate": 1699636646232,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XrEDbWuxWD",
                "forum": "Qyp3Rni2g1",
                "replyto": "Lc7a98rOFG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6018/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6018/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for their feedback. We are glad that the reviewer finds efficiency evaluation important and some of our design choices interesting and novel.\n\n> The whole paper is not very clear. In the section 2, the reason that considering the proposed five aspects for efficiency evaluation is not described clearly.\n\nWe believe that using multiple metrics to evaluate a model's efficiency is crucial for a comprehensive understanding of its performance and suitability for various applications. It can also help understand the tradeoff among these aspects and help make better decisions while deploying the models. Each metric provides a different perspective \n- Latency is crucial for real-time applications, such interactive systems like chatbots,  and real-time translation systems, where quick response is critical for the user\u2019s experience.\n- Throughput is important in scenarios where the model needs to process large volumes of data, e.g., batched processing of many scientific papers or news articles.\n- Memory overhead can reveal whether a model might not be suitable for deployment on devices with limited memory, such as mobile phones or IoT devices.\n- Energy consumption is important for justifying the model\u2019s suitability for battery-powered devices or in situations where energy efficiency is a priority.\n- Model size  affects its portability and the speed of its deployment. Smaller models are easier to distribute and can be deployed on devices with limited storage capacity.\n\nWe will highlight these motivations in the revision.\n\n> The experimental parts are not very suffcient. Only two tasks are selected which makes the results not very convincing.\n\n> What did only three kinds of NLP tasks are selected? I concern what are the results in other NLP tasks.\n\nOur decision of experimenting with three tasks (instead of two) aims to demonstrate the Pentathlon\u2019s versatility supporting a variety of architectures and settings.  Please refer to the general response for further details.\n\n> Table 1 is not very clear. What is the schema meaning in table 1, such as Acc., TP., Latency, Mem., etc. The authors should describe them in the tabel title.\n\nWe chose to use abbreviations in the interest of space, which are described in the caption of Table 1. Acc.: accuracy, TP.: throughput; Mem.: memory overhead."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6018/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728823434,
                "cdate": 1700728823434,
                "tmdate": 1700728823434,
                "mdate": 1700728823434,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PnNMg13lac",
            "forum": "Qyp3Rni2g1",
            "replyto": "Qyp3Rni2g1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6018/Reviewer_uM5U"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6018/Reviewer_uM5U"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents Pentathlon, a benchmark for holistic and realistic evaluation of model efficiency. The benchmark offers a strictly controlled hardware platform and incorporates metrics to measure efficiency, including latency, throughput, memory overhead, number of parameters, and energy consumption. The authors also provide a software library that can seamlessly integrate into any codebase and enable evaluation."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper introduces a standardized and centralized evaluation platform, which can reduce the workload to make fair and reproducible efficiency comparisons and stimulate algorithmic innovations in building efficient models."
                },
                "weaknesses": {
                    "value": "- The paper is more like a technical report, which may suit a benchmark or industry track. It would be great if the authors could provide additional scientific findings and conclusions through the evaluations. This work has provided a relatively comprehensive and mature evaluation benchmark. With more inspiring and interesting findings and conclusions based on the evaluations, the paper would be more valuable to the community.\n\n- There are many datasets designed for large language model evaluation. Using classical tasks (e.g., machine translation, mathematical reasoning, and classification) makes the experiments less convincing."
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6018/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698811697833,
            "cdate": 1698811697833,
            "tmdate": 1699636646134,
            "mdate": 1699636646134,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hlDjY4dgLu",
                "forum": "Qyp3Rni2g1",
                "replyto": "PnNMg13lac",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6018/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6018/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> The paper is more like a technical report, which may suit a benchmark or industry track. It would be great if the authors could provide additional scientific findings and conclusions through the evaluations. This work has provided a relatively comprehensive and mature evaluation benchmark. With more inspiring and interesting findings and conclusions based on the evaluations, the paper would be more valuable to the community.\n\nWe agree with the reviewer that this submission fits the datasets and benchmarks track of ICLR 24, the exact track that this paper is submitted to: https://iclr.cc/Conferences/2024/CallForPapers\n\nWe believe that the value of this work lies in providing the community with a platform that can help discover interesting findings and conclusions about the efficiency of machine learning models.\n\n\n> There are many datasets designed for large language model evaluation. Using classical tasks (e.g., machine translation, mathematical reasoning, and classification) makes the experiments less convincing.\n\nOur decision to experiment with machine translation, classification, and math word problems aims to demonstrate the Pentathlon\u2019s versatility supporting a variety of architectures and settings.  Please refer to the general response for further details."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6018/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728601798,
                "cdate": 1700728601798,
                "tmdate": 1700728601798,
                "mdate": 1700728601798,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Y6unh6bkPh",
            "forum": "Qyp3Rni2g1",
            "replyto": "Qyp3Rni2g1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6018/Reviewer_5UHy"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6018/Reviewer_5UHy"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces `Pentathlon`, a benchmark created for the comprehensive and realistic evaluation of model inference efficiency. Pentathlon provides a strictly controlled hardware platform, including GPUs and CPUs, and incorporates a suite of metrics targeting different aspects of efficiency, including latency, throughput, memory overhead, parameter count, and energy consumption. As a standardized and centralized evaluation platform, Pentathlon aims to significantly reduce the workload required for fair and reproducible efficiency comparisons. While its initial focus is on natural language processing (NLP) models, Pentathlon is designed to be flexibly extended to other fields."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "-   Clarity: The paper is exceptionally well-written, offering a comprehensive presentation of the Pentathlon benchmark suite. The authors provide a detailed explanation of its design, which emphasizes equitable model comparisons and incorporates testing settings for both CPUs and GPUs.\n  \n-   Thoughtful Metric Selection: Pentathlon's use of five carefully chosen evaluation metrics addresses critical properties of models, ensuring that the benchmark accurately assesses key aspects of efficiency.\n    \n-   Visual Aid: The inclusion of radar charts is a notable advantage, as they effectively illustrate the strengths and weaknesses of models, making it easier for readers to comprehend the benchmark's findings.\n    \n-   Centralized Benchmarking: While the concept of centralized benchmarking isn't entirely novel, it remains highly valuable for gaining a deeper understanding of the diverse impacts algorithms have on model efficiency. Pentathlon offers a structured and standardized approach to this essential process.\n    \n-   Realistic Workloads: The authors' meticulous design of workloads to mirror batching scenarios and real-world service loads enhances the reliability of the benchmark's results, ensuring they are more reflective of practical use cases."
                },
                "weaknesses": {
                    "value": "1.  Hardware Flexibility: While the authors have outlined CPU and GPU settings, it remains unclear whether the benchmark suite can easily accommodate other hardware platforms. Given the growing popularity of new platforms like Metal, ROCm, Mali, and Vulkan, it's essential to address the adaptability of Pentathlon to ensure it remains relevant and applicable to diverse hardware configurations. Moreover, certain models, such as Llama-70b, may require multiple high-end GPUs like A100 or H100 for distributed inference, highlighting the need for flexibility in hardware options.\n    \n2.  Software Environment Assumptions: The paper primarily focuses on a specific software environment, potentially overlooking the fact that various software stacks, such as TVM and Cutlass, may require an additional step called tuning. This tuning phase optimizes the compilation stack for the given hardware, which can significantly improve model performance. However, the tuning process itself may not always be efficient and can be time-consuming. It's crucial to consider these software-related aspects for a more comprehensive evaluation.\n    \n3.  Controlled Hardware vs. Cloud-Based Platforms: While the controlled hardware setting provides fairness and accuracy, it may not fully cater to researchers who heavily rely on cloud-based platforms like AWS, Azure, or Google Cloud. Many recent large language models (LLMs) are built and deployed on cloud platforms, and their efficiency and latency results may significantly differ from those obtained in a controlled environment. To make Pentathlon more applicable to a wider range of real-world scenarios, consideration could be given to extending the benchmarking to include cloud-based machines and their specific challenges."
                },
                "questions": {
                    "value": "1.  What level of effort is required to expand Pentathlon to accommodate a new hardware platform or incorporate a new model into the benchmark?\n2.  Beyond the BLEU score, are there additional metrics available within Pentathlon to assess model quality, such as perplexity or other relevant NLP-specific metrics?\n3.  How can you distinguish the impact of \"algorithmic innovations\" from other efficiency-related factors in the Pentathlon benchmark?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6018/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698813189458,
            "cdate": 1698813189458,
            "tmdate": 1699636645984,
            "mdate": 1699636645984,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3prrH5ngN4",
                "forum": "Qyp3Rni2g1",
                "replyto": "Y6unh6bkPh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6018/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6018/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank reviewer 5UHy for their constructive feedback. It is encouraging to hear that they found our work well-written and appreciated many of our design choices.\n\n> Hardware Flexibility\n\nWe appreciate the reviewer for raising the valid concern that controlling the hardware leads to less flexibility. In preliminary experiments, we found it possible to simulate mobile phone platforms by connecting to our Linux-host an Arm chip device. Some on-device applications can be simulated by running our platform on the NVIDIA Jeston module: https://developer.nvidia.com/embedded/jetson-tx2\n\nWe acknowledge the challenge of continuously supporting new hardware platforms, which is beyond the capabilities of the authors. Trying to address this, Efficiency Pentathlon is designed to be flexible, and can be used as a standalone software without our machine. It is compatible with any hardware platform that supports Linux, offering a versatile solution for users on their preferred hardware platform when centralized evaluation and controlling for the hardware are not primary concerns.\n\n\n> Software Environment Assumptions: \n\nTrying to maximize software flexibility is something we considered while designing the Pentathlon. Our benchmark is designed with minimal assumptions regarding the evaluated system's software environment, with the primary requirement of being able to run in a Docker container. As a quick verification, Cutlass, which the reviewer brought up, is compatible with our benchmark. Due to the limited time of the response period, the authors were not able to verify TVM at the time of this response. We plan to include this discussion in the revision.\n\nEven when the evaluated system requires compilation, its efficiency will not be negatively affected. Specifically, the Efficiency Pentathlon begins tracking efficiency metrics only once the system has completed its compilation phase and is operational.This is implemented through a 'handshake protocol': The benchmark sends a dummy input batch to the system. It will start to measure the efficiency only after it has received the system\u2019s outputs for this dummy batch, indicating that the model has finished its preparation stage and is ready to go. \n\n> Controlled hardware vs. cloud-based platforms\n\nWe fully agree with the reviewer\u2019s great point that controlling for the hardware may not always be is not always practical or desirable in practice. This consideration is integrated into the design of our benchmark. The benchmark is developed as a standalone, open-source software, allowing users to employ it on their preferred hardware platforms, including many cloud machines that support Linux. We will emphasize this point more explicitly in the revision."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6018/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728418819,
                "cdate": 1700728418819,
                "tmdate": 1700728418819,
                "mdate": 1700728418819,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "W32zJ3NSUC",
            "forum": "Qyp3Rni2g1",
            "replyto": "Qyp3Rni2g1",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6018/Reviewer_HHde"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6018/Reviewer_HHde"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a benchmark for evaluating model efficiency (compared with most benchmarks on performance), in particular, for LLM inference. It compares different use case scenarios for calling an LLM as well as using several metrics to evaluate the inference efficiency and environmental impacts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* Propose several use case scenarios for model inference, like batching, streaming, offline, etc.\n* Propose several metrics to measure the model inference efficiency as well as environmental impact"
                },
                "weaknesses": {
                    "value": "* Benchmark selection is a bit limited, only a few tasks are chosen. For instance, there is no typical (monolingual) language generation task"
                },
                "questions": {
                    "value": "* Figure 3 (a) and (b) are exactly the same? Is it a coincidence or mistake?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6018/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6018/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6018/Reviewer_HHde"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6018/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698842392565,
            "cdate": 1698842392565,
            "tmdate": 1699636645840,
            "mdate": 1699636645840,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "YovAupLAFy",
                "forum": "Qyp3Rni2g1",
                "replyto": "W32zJ3NSUC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6018/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6018/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We appreciate reviewer HHde\u2019s recognition of our efforts to include a diverse range of efficiency scenarios and metrics.\n\n> Propose several use case scenarios for model inference, like batching, streaming, offline, etc.\n\nOur benchmark supports all tasks from Hugging Face, including many language generation tasks. Please see the general response.\n\n> Figure 3 (a) and (b) are exactly the same? Is it a coincidence or mistake?\n\nThanks for catching this. This was due to a LaTex typo and has been fixed."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6018/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700728222787,
                "cdate": 1700728222787,
                "tmdate": 1700728222787,
                "mdate": 1700728222787,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]