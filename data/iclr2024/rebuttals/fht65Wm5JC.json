[
    {
        "title": "Borda Regret Minimization for Generalized Linear Dueling Bandits"
    },
    {
        "review": {
            "id": "CCMYcLmcrq",
            "forum": "fht65Wm5JC",
            "replyto": "fht65Wm5JC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4042/Reviewer_Ln8Y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4042/Reviewer_Ln8Y"
            ],
            "content": {
                "summary": {
                    "value": "In this work, the authors consider the problem of linear dueling bandits, both in the stochastic and the adversarial setting. Dueling bandits is a variant of the standard multi-armed bandits problem, particularly well suited for practical applications, where at each round the algorithm selects two arms (also called items) to play and receives receives feedback that expresses which of the two items was preferred, As this preferences don't necessarily imply the existence of a unique ranking between the arms, the authors of the papers consider the Borda score (which is the average win rate against all arms) as the measure used to define the regret.\nThe goal of this work is to take advantage of the side information given by the linear bandit structure to learn faster. \n\nThe first contribution of the paper is a well detailed lower bound proof, which shows that the difficulty of the problem scales as $d^{2/3}T^{2/3}$, where $T$  is the time horizon and $d$ is the dimension of the contextual vectors.\nThey then present a simple Explore then commit strategy that achieves near optimal performance for the stochastic version of the problem, and a variation of the EXP3 algorithm that also achieves near optimal (and optimal rate when the number of arms is of order $2^d$). \nFinally, they present experiments on both generated and real world datasets showing that the presented algorithms can successfully use the linear structure of the problem to outperform the all the baselines."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper considers a new variation of the linear dueling bandits problem using Borda regret and provide very complete results. Specifically, the most important result presented in this paper is the lower bound, which is well detailed.\n\nThis first result is crucial to justify that the proposed algorithms, that shine by their simplicity, are sufficient for the problem at hand. I find the dual explore structure of the ETC algorithm fairly interesting: in the first part, the exploration is uniform to initialize correctly the algorithm, but then the exploration is refined to be tuned according to the precision required for each pairs of arms. The authors derive near optimal high probability bounds for this algorithm, which are stronger guarantees compared to results holding in expectation.\n\nThe second algorithm builds upon the very standard EXP3 algorithm, and its DEXP3 variant for dueling bandits. This algorithm is more robust, as it holds for the adversarial version of the problem, which is more challenging. Interestingly, the proposed bound is actually tight when the number of arms is exponential in the dimension of the contextual vector, and in the experiments, BEXP3 outperforms its ETC counterpart.\n\nIt is also appreciated that the authors discuss how to modify the algorithms to adapt for small number of arms as well as infinite number of arm, and it is worth noting that the paper is particularly well written, with details like ensuring that all notations are clearly defined and visual representations of the lower bound."
                },
                "weaknesses": {
                    "value": "This work seems very solid overall.\n\nOne limitation that seems perhaps unnecessary is the fact that the time horizon has to be known. It would be nice for the authors to discuss which approaches (such as a doubling trick) could be used to remove this limitation, which would make these algorithms even easier to use in practice. (For the EXP3 algorithm, it is now standard to use a time varying learning rate for more applications).\n\nIn the experiments section, it would be nice to see more experiments that compare the two presented algorithms with different number of arms and time horizons: Will the ETC algorithm always have larger regret than the BEXP3 one due to the cost of the exploration phase or are there problem settings in which the ETC algorithm is better? as the BEXP3 algorithm is more robust and performs better in the experiments, I wonder if there is any case where one would prefer the ETC algorithm?"
                },
                "questions": {
                    "value": "Besides for the original version of the EXP3 algorithm (Auer et al. 2002), it is more common to find this algorithm stated with the use of losses rather than rewards, as it is not necessary to include extra exploration. Have you considered converting these rewards into losses and relying on EXP3 with losses (and possibly with time dependent leaning rate?)\n\n(as stated in the weaknesses:) Will the ETC algorithm always have larger regret than the BEXP3 one due to the cost of the exploration phase or are there problem settings in which the ETC algorithm is better? as the BEXP3 algorithm is more robust and performs better in the experiments, I wonder if there is any case where one would prefer the ETC algorithm?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4042/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4042/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4042/Reviewer_Ln8Y"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4042/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697980506116,
            "cdate": 1697980506116,
            "tmdate": 1700643782987,
            "mdate": 1700643782987,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "wdY1h0iT97",
                "forum": "fht65Wm5JC",
                "replyto": "CCMYcLmcrq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4042/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4042/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer Ln8Y"
                    },
                    "comment": {
                        "value": "Thank you for your positive feedback and strong support. We address your comments and questions as follows:\n\n---\n\n**Q1**: Besides for the original version of the EXP3 algorithm (Auer et al. 2002), it is more common to find this algorithm stated with the use of losses rather than rewards, as it is not necessary to include extra exploration. Have you considered converting these rewards into losses and relying on EXP3 with losses (and possibly with time-dependent leaning rate?) \\\n**A1**: We believe the reward formulation and loss formulation are essentially equivalent to a negation sign. For an unknown time horizon, indeed, we can incorporate a time-varying learning rate. For simplicity, we did not add this variant. We will add comments on this. \n\n---\n\n**Q2**: Will the ETC algorithm always have larger regret than the BEXP3 one due to the cost of the exploration phase or are there problem settings in which the ETC algorithm is better? as the BEXP3 algorithm is more robust and performs better in the experiments, I wonder if there is any case where one would prefer the ETC algorithm? \\\n**A2**: *ETC versus EXP3*: Two algorithms have essentially the same order of regret in our setting because $K = 2^d$. During our experiment, we did not carefully tune the constant in front of each parameter. As shown in Appendix I of the revised version, tuning the parameters of BETC can lead to an improvement of up to 50%. Our belief is that two algorithms should perform equally well in a stochastic setting, and only differ by a constant order. \n\n*When can BETC beat BEXP3?* BETC can work for general link functions, while BEXP3 is guaranteed to work only for the linear model. Even though in our experiments BEXP3 worked well with the logistic link function, it is not guaranteed to work well under any link function. For a less favorable link function, it might fail.\\\nBesides, our current implementation of BEXP3 requires $K$ to be not too large $K = O(2^d)$, which is often true in real-world applications. When $K$ is extremely large, then BETC is preferred as the sample complexity does not depend on $K$ but only on the dimension $d$. (see Q2 in our response to Reviewer Kuh2 for more details)"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4042/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700191609329,
                "cdate": 1700191609329,
                "tmdate": 1700191609329,
                "mdate": 1700191609329,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CRPPDXN2TS",
                "forum": "fht65Wm5JC",
                "replyto": "wdY1h0iT97",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4042/Reviewer_Ln8Y"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4042/Reviewer_Ln8Y"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the added details"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4042/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643224446,
                "cdate": 1700643224446,
                "tmdate": 1700643224446,
                "mdate": 1700643224446,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "qTelkVkdFI",
                "forum": "fht65Wm5JC",
                "replyto": "CRPPDXN2TS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4042/Reviewer_Ln8Y"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4042/Reviewer_Ln8Y"
                ],
                "content": {
                    "comment": {
                        "value": "After reading your rebuttal and the considering the other reviewers concerns, I have adjusted my score. I believe that the results are still interesting, but the novelty is indeed not as high as I had previously assumed."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4042/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700643770982,
                "cdate": 1700643770982,
                "tmdate": 1700643770982,
                "mdate": 1700643770982,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "c918Dt1IeI",
            "forum": "fht65Wm5JC",
            "replyto": "fht65Wm5JC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4042/Reviewer_PcGb"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4042/Reviewer_PcGb"
            ],
            "content": {
                "summary": {
                    "value": "This paper delves into the realm of generalized linear dueling bandits within a stochastic setting and linear bandits within an adversarial setting. The scenario involves K arms with fixed features, and at each time step, the agent selects a pair of items (i_t, j_t), receiving stochastic feedback indicating whether i_t is preferred over item j_t. The probability model adopted encompasses a generalized linear model with an unknown parameter $\\theta^*\\in\\mathbb{R}^d$ for the stochastic setting and linear model with $\\theta_t$ for the adversarial setting. Regret is assessed through the Borda score, defined as the average winning probability of an arm over the other arms.\n\nThe authors establish a lower bound for both the stochastic and adversarial settings. For the stochastic setting, they introduce an algorithm based on ETC, tightly matching the lower bound concerning T and d. In the adversarial setting, they propose the BEXP3 algorithm based on EXP3, achieving regret of (dlog K)^{1/3}T^{2/3}. The paper concludes with a demonstration of the proposed algorithms using synthetic and real-world datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors explore generalized linear dueling bandits with Borda scores, conducting an analysis of regret lower bounds and presenting algorithms for both stochastic and adversarial settings with upper bounds on regret."
                },
                "weaknesses": {
                    "value": "-The authors assert that previous work by Saha (2021) on linear contextual duel bandits can be considered a special case of their model. However, the cited work involves a contextual set of arms that may change over time and a more generalized Multi-nomial logistic model, in contrast to the fixed feature vectors and dueling bandits considered in this study. Notably, the previously proposed algorithm achieves a regret bound of $\\sqrt{T}$, while the algorithm presented in this work achieves a $T^{2/3}$ regret bound.\n\n-As acknowledged by the authors, Saha (2021a) addressed adversarial duel bandits. While this present study introduces a linear model for the adversarial case, the extension to a linear model appears to follow the adversarial linear bandit algorithm outlined in \"Bandit Algorithms\" by Lattimore and Szepesvari and D-EXP3 [Saha 2021a]. There is a concern regarding whether there are discernible factors indicating that this extension is not a trivial one."
                },
                "questions": {
                    "value": "-As indicated in the Weakness section, what accounts for the fact that the previously proposed algorithm in [Saha 2021] attains a regret bound of $\\sqrt{T}$\u2014which appears to be superior to the results presented in this work, specifically $T^{2/3}$ in both lower and upper bounds?\n\n-Regarding Theorem 4.1, what constitutes the primary technical challenge in analyzing the lower bound for the linear bandit model compared to the Multi-Armed Bandit (MAB) scenario discussed in [Saha 2021a]?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4042/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4042/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4042/Reviewer_PcGb"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4042/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698555155834,
            "cdate": 1698555155834,
            "tmdate": 1699636367340,
            "mdate": 1699636367340,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zNRw6yGJoS",
                "forum": "fht65Wm5JC",
                "replyto": "c918Dt1IeI",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4042/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4042/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer PcGb"
                    },
                    "comment": {
                        "value": "Thank you for your feedback! We address your comments and questions as follows:\n\n---\n\n**Q1**: The authors assert that previous work by Saha (2021) on linear contextual duel bandits can be considered a special case of their model. However, the cited work involves a contextual set of arms that may change over time and a more generalized Multi-nomial logistic model, in contrast to the fixed feature vectors and dueling bandits considered in this study. \\\n**A1**: As explained in Section A.1, in Saha (2021), the assumption is that each item has a `score\u2019 $v_i = x_i^{\\top} \\theta^*$. Then, a changing set of arms is possible as long as the comparison follows the logistic model $P(i \\succ j) = \\mu(v_i - v_j) = \\mu((x_i - x_j)^{\\top}\\theta^*)$. Similarly, the generalized multi-nomial logistic model can be satisfied as $P(i \\text{ chosen among all items}) \\propto e^{v_i}$.\n\nWe claimed that our setting covers theirs because we consider $P(i \\succ j) = \\mu(\\phi_{i,j}^{\\top} \\theta^*)$, and if $\\phi_{i,j} = x_i - x_j$, then our setting becomes theirs, albeit with fixed arms and pairwise comparison only. On the other hand, in our general setting, $x_i$ is not available. Then it will not allow either a set of arms that may change over time or a multi-nomial logistic model that depends on the existence of $x_i$. \n\n---\n\n**Q2**:   Why the previously proposed algorithm in [Saha 2021] attain a regret bound of  $\\sqrt{T}$ which appears to be superior to the results presented in this work, specifically $T^{2/3}$ in both lower and upper bounds? \\\n**A2**: We believe there is some misunderstanding here. As explained in the last question if we do only have $\\phi_{i,j}$ instead of $x_i$ and $x_j$, then the algorithms in [Saha 2021] cannot work at all in our setting, because they have no access to the non-existent $x_i$, not to mention a $\\sqrt{T}$ regret.\n\nWe can only achieve $T^{2/3}$ regret because our studied problem is a more general one, as explained above. And a more general class of problems will incur no less regret than a subset of problems does. So it is normal to have their $\\sqrt{T}$ regret for a subset of problems against our $T^{2/3}$ for the whole problem set.\n\n---\n\n**Q3**: Regarding Theorem 4.1, what constitutes the primary technical challenge in analyzing the lower bound for the linear bandit model compared to the Multi-Armed Bandit (MAB) scenario discussed in [Saha 2021a]? \\\n**A3**:  In the previous work [Saha et al 21], there are $K-1$ good arms and only the best one of them differs from the others. This design will not work in our setting as it leads to lower bounds sub-optimal in dimension $d$. The new construction of our lower bound is based on the hardness of identifying the best arm in the $d$-dimensional linear bandit setting, which is quite different from the multi-armed setting. \n\n[Saha et al 21]\u2019s proof is directly based on hypothesis testing: either identifying the best arm with gap $\\epsilon$ within $T$ rounds (if $T > \\frac{K}{1440 \\epsilon^3}$) or incurring $\\epsilon T$ regret (if $T \\le \\frac{K}{1440 \\epsilon^3}$). In contrast, our proof technique bounds from below the regret by the expected number of sub-optimal arm pulls and does not divide the problem instances into two cases (i.e. whether $T\u2264\\frac{K}{1440 \\epsilon^3}$). To prove the lower bound, we first apply a new reduction step to restrict the choice of $i_t$. Then we bound from below the regret by the expected number of sub-optimal arm pulls."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4042/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700191341977,
                "cdate": 1700191341977,
                "tmdate": 1700191341977,
                "mdate": 1700191341977,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JKGvHQr2OE",
                "forum": "fht65Wm5JC",
                "replyto": "zNRw6yGJoS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4042/Reviewer_PcGb"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4042/Reviewer_PcGb"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate your clarification regarding the distinction between the current work and the prior study by Saha in 2021.\n\nAligning with the comments by other reviewers, I agree that adopting Borda regret simplifies the problem, and the suggested algorithms seem to be a straightforward extension of earlier work (Saha et al. 2021a, \"Bandit Algorithms\" by Lattimore and Szepesvari). Consequently, I will keep my original score."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4042/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700478416259,
                "cdate": 1700478416259,
                "tmdate": 1700478416259,
                "mdate": 1700478416259,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TpblPdIcJv",
            "forum": "fht65Wm5JC",
            "replyto": "fht65Wm5JC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4042/Reviewer_Kuh2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4042/Reviewer_Kuh2"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies generalized linear dueling bandits with the goal of tracking the Borda winner and minimizing Borda regret. They have matching upper and lower gap-free regret bounds in the stochastic setting and a regret upper bound in the adversarial setting.\n\nWhile I believe the proofs are technically correct, I think the results are not surprising and so the novelty is limited. Overall, this paper seems to mostly combine two well-known theories in a straightforward application: (1) that of Borda regret minimization in dueling bandits (especially calling on the results of Saha et al., 2021) and (2) known techniques for generalized linear bandits (e.g., Li et al. 2017). Plese see \"Weaknesses\" below for specific discussions."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* I appreciate the honest and consistent references to prior works to help understand where the proof strategies were borrowed.\n* The proofs and constructions are generally easy to follow, and the paper is overall well-written.\n* There are also experiments to support the theoretical findings.\n* The paper is the first to study generalized linear dueling bandits with Borda objective, to my knowledge. So, the results are not subsumed by any prior works. \n* In particular, the first lower bounds are shown for this settin and a matching minimax upper bound is shown."
                },
                "weaknesses": {
                    "value": "* I am curious why the authors pursued the Borda setting over the more established Condorcet setting where one targets a Condorcet winner and minimizes Condorcet regret. It is somewhat debatable in the literature which setting is preferable. In my opinion, since the Borda setting relies on pure exploration tactics (e.g., explore-then-commit in stochastic setting or EXP3 with T^{-1/3} exploration in adversarial setting), it is very amenable to the generalizing to this GLM model without hassle. Thus, I think the regret upper bounds in this paper are not surprising. The stochastic BETC-GLM regret bound seems to follow almost immediately from well-known sample complexity bounds for optimal design, where estimation of $\\theta^*$ is completely decoupled from regret. Meanwhile, the adversarial regret bound for BEXP3 seems to be identical to that of DEXP3 in Saha et al., 2021 except for plugging in slightly different variance bounds at the end. \nTo contrast, in the Condorcet setting, where pure exploration is innapropriate to target $\\sqrt{T}$ reget, one would have had to carefully decouple estimation of $\\theta^*$ and regret minimization. So, I think the Condorcet setting would have been more technically interesting to study.\n* Alternatively, it would have been more interesting to study instance-dependent regret rates (e.g., those appearing in Jamieson et al., 2015) as it's more unclear to me how those would behave for GLM dueling bandits.\n* The adversarial regret upper bound seems to only be able to get the $(d\\log(K))^{1/3}$ dependence and not the $d^{2/3}$ dependence if $K \\gg 2^d$ because there is an unavoidable $\\log(K)$ appearing in the EXP3 analysis. Therefore, it is not necessarily optimal in all regimes. \n* As is the case for generalized linear MAB, there is a mysterious dependence on $\\kappa^{-1}$ in the regret upper bounds. It is unclear to me if this dependence is optimal and calls into question how realistic this regret bound can be."
                },
                "questions": {
                    "value": "# Questions\n* As mentioned above, can the adverserial dueling bandit analysis be improved to $d^{2/3} T^{2/3}$ for very large $K$?\n* Can the authors comment on the dependence in the regret of $\\kappa^{-1}$ in the regret and whether it is optimal or realistic for common link functions?\n* It seems like BEXP3 seems to perform the best in your experiments. This seems a little confusing to me because the constructed environments seem to be stochastic and not adversarial. Can the authors comment on this?\n\n# Writing Notes\n* The term \"contextual vector\" or \"contextual dueling bandit\" is used many times to refer to the feature ${\\bf x}_i$ of arm $i$. This can be easily confused with contextual bandits where one observed a context $X_t$ independent of the arms, and so some clarification in the language might be helpful."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4042/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4042/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4042/Reviewer_Kuh2"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4042/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698787568724,
            "cdate": 1698787568724,
            "tmdate": 1699636367263,
            "mdate": 1699636367263,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AkwbxN93MW",
                "forum": "fht65Wm5JC",
                "replyto": "TpblPdIcJv",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4042/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4042/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer Kuh2"
                    },
                    "comment": {
                        "value": "Thank you for your feedback and your suggestions on writing! We address your comments and questions as follows:\n\n---\n**Q1**: Can the adversarial dueling bandit analysis be improved to $d^{2/3} T^{2/3}$  for very large K? \\\n**A1**: Yes. The main idea is to use an $\\epsilon$-covering argument. In the $d$-dimensional space, it suffices to choose $ O((1/\\epsilon)^{d})$ representative vectors to cover all $K$ average contextual vectors $\\frac{1}{K} \\sum_{j=1}^{K}\\phi_{i,j}$ with error up to $\\epsilon$. Now BEXP3 will be performed as the original case except that at Line 7 in Alg 2, we replace the average contextual vectors $\\frac{1}{K} \\sum_{j=1}^{K}\\phi_{i,j}$ with those nearest representative vectors. Since there are only $O((1/\\epsilon)^{d})$ uniquely different rewards, the EXP3 argument (equation D.3) can be performed on $O((1/\\epsilon)^{d})$ unique arms and eventually the algorithm suffers a regret of $\\tilde{O}(d^{2/3} T^{2/3} \\log^{1/3}(1/\\epsilon))$. The $\\epsilon$-net incurs an additional approximation error of order $O(\\epsilon T)$. Setting $\\epsilon = T^{-1}$ will improve the regret to $d^{2/3} T^{2/3}$ up to log factors. We add a section explaining this in detail (Appendix H) in the revised version.\n\n---\n\n\n**Q2**: Can the authors comment on the dependence in the regret of  $\\kappa^{-1}$ in the regret and whether it is optimal or realistic for common link functions? \\\n**A2**: For general link functions, all previous works on generalized linear models suffer the same order of dependence on $\\kappa^{-1}$ in their regret upper bounds. For some specific link functions, such as the logistic link function, [1] showed better dependence on $\\kappa^{-1}$. Still, it is so far unknown to the community if $\\kappa^{-1}$ for general link functions can be improved.  We will add comments on this in the revised version.\n\n---\n**Q3**: BEXP3 seems to perform the best in your experiments. This seems a little confusing to me because the constructed environments seem to be stochastic and not adversarial. Can the authors comment on this?\\\n**A3**: In principle, we don\u2019t think it is surprising, because the adversarial setting strictly covers the stationary setting when the link function is linear. So BEXP3 should work at least as well as BETC, which is indicated by the regret upper bound. We believe this gap can be eliminated by carefully tuning the constant of each input parameter. Please see Appendix I for the additional experiment, where we show BETC performs better than BEXP3.\n\n---\n\\\n\\\n[1] Improved Optimistic Algorithms for Logistic Bandits, Faury et al., ICML 2020"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4042/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700190953026,
                "cdate": 1700190953026,
                "tmdate": 1700191698934,
                "mdate": 1700191698934,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Z0XYbYM1mr",
                "forum": "fht65Wm5JC",
                "replyto": "AkwbxN93MW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4042/Reviewer_Kuh2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4042/Reviewer_Kuh2"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the response and for addressing the question about optimal dependence on $d$. Overall, I would still say the novelty is limited due to the results being close adaptations of the corresponding results in dueling bandits with finite number of arms, especially in this Borda version of the problem. This point also seems to be echoed by some of the other reviewers. I will keep my score the same."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4042/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700411199681,
                "cdate": 1700411199681,
                "tmdate": 1700411199681,
                "mdate": 1700411199681,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MvMKCw5nJ6",
            "forum": "fht65Wm5JC",
            "replyto": "fht65Wm5JC",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4042/Reviewer_vgjv"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4042/Reviewer_vgjv"
            ],
            "content": {
                "summary": {
                    "value": "The authors prove a regret lower and upper bounds for the Borda regret minimization problem for generalized linear models with K arms, which largely depend polynomial on d is the dimension of the contextual vectors and T is the time horizon. Specifically, they achieve matching upper and lower bounds of d^{2/3}T^{2/3} for the stochastic setting, as well as a d^{1/3}T^{2/3} upper bound in the adversarial setting."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The authors extend the previous works by allowing the regret bound to not inherently depend the number of arms K (generally log(K)); rather it depends on the inherent dimensionality of contextual vectors, which are given apriori. They study both the adversarial and stochastic settings, with generally similar conclusions. Furthermore, their lower bounds demonstrate that their upper bounds are in fact tight due to the dual-regret nature of Borda regret. Their lower bounds seem to imply that the preference information + regret structure makes Borda regret minimization inherently harder than typical bandit regret settings as note that the action pair with the highest reward does not lead to optimal minimization."
                },
                "weaknesses": {
                    "value": "The main weakness is the novelty of the paper and it's derived bounds. It appears that the lower bounds use the standard hypercube + info theoretical argument from [Dani et al 08 or survey on bandits by Lattimore] and fails to clarify the novelty in their lower bounds from previous works. Furthermore, the upper bound uses a simple ETC algorithm and analysis and it is unclear how the novelty from the typical ETC analysis [see survey on bandits by Lattimore]. \n\nFurthermore, the paper mentions Borda regret in the adversarial setting but it becomes less obvious why Borda regret in this setting is even possible without assumptions 3.2 and 3.3 (it appears that Algorithm 2 does not use the structure of mu at ALL!). If that is indeed possible, why is there no reduction from adversarial to stochastic?"
                },
                "questions": {
                    "value": "In algorithm 2 (adversarial setting), where does mu show up? How can it work without inferring mu at all and without assumptions 3.2/3.3?\n\nCan you explain the novelty in your lower/upper bounds in the stochastic setting?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4042/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699131086760,
            "cdate": 1699131086760,
            "tmdate": 1699636367202,
            "mdate": 1699636367202,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XetT7jUyeG",
                "forum": "fht65Wm5JC",
                "replyto": "MvMKCw5nJ6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4042/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4042/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer vgjv"
                    },
                    "comment": {
                        "value": "Thank you for your feedback! We address your comments and questions as follows:\n\n---\n**Q1**: In algorithm 2 (adversarial setting), where does $\\mu$ show up? How can it work without inferring $\\mu$ at all and without assumptions 3.2/3.3? \\\n**A1**: We would like to clarify that in the adversarial setting, we consider linear models rather than generalized linear models. In detail, we consider the linear link function is $\\mu(x) = \\frac{1}{2} + x$ in the adversarial setting, and it satisfies Assumption 3.2 with $\\mu\u2019(\\cdot) = 1$ and Assumption 3.3 with $L_{\\mu}=M_{\\mu}=1$.  \n\nWe will add comments on this in the revised version to avoid any misunderstanding.\n\n---\n**Q2**: Can you explain the novelty in your lower/upper bounds in the stochastic setting? \\\n**A2**: One key observation in solving the stochastic setting is that, to estimate the Borda score, the most sample-efficient way is to query each pair uniformly. In the linear setting, this means we need to explore each direction in $\\mathbb{R}^d$ uniformly well. \n\nConverting this idea into the regret-minimization setting requires new techniques. For the lower bound, this leads to our design of good/bad arms. In the previous work [Saha et al 21], there are $K-1$ identical arms against one best arm. This design will not work in our setting as it leads to lower bounds sub-optimal in dimension $d$. The new construction of our lower bound is based on the hardness of identifying the best arm in the $d$-dimensional linear bandit model.  To prove the lower bound, we first apply a new reduction step to restrict the choice of $i_t$. Then we bound from below the regret by the expected number of sub-optimal arm pulls.\n\nFor the upper bound, our hard instance construction already sheds light on the algorithm design (see comments below Theorem 4.1):  to differentiate the best item from its close competitors, the algorithm must query the bad items to gain information. This means BETC algorithms should naturally be optimal and do not need further complication. To explore each direction in $\\mathbb{R}^d$ uniformly, we adopt the G-optimal design to explore all directions efficiently."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4042/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700190471984,
                "cdate": 1700190471984,
                "tmdate": 1700190471984,
                "mdate": 1700190471984,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "cBF5bNIG0q",
                "forum": "fht65Wm5JC",
                "replyto": "XetT7jUyeG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4042/Reviewer_vgjv"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4042/Reviewer_vgjv"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the clarifications! Generally, it would still seem that your work seems to be a combination of [Saha et al 21] and standard (generalized) linear bandit upper/lower bounds. Therefore, I would be hesitant to say that the novelty is sufficient for a clear accept.\n\nMy main concern with a marginal accept is that in this model, the reward received as feedback does not clearly correspond to the regret incurred in the round. This allows for generally easier lower bounds and I am unsure why the Borda regret model provides interesting theoretical or empirical insights."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4042/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700244115501,
                "cdate": 1700244115501,
                "tmdate": 1700244115501,
                "mdate": 1700244115501,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]