[
    {
        "title": "Concept Alignment as a Prerequisite for Value Alignment"
    },
    {
        "review": {
            "id": "OvwTIpzrF1",
            "forum": "MQ4JJIYKkh",
            "replyto": "MQ4JJIYKkh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8403/Reviewer_ouXJ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8403/Reviewer_ouXJ"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors analyze the value alignment problem from the conceptual alignment perspective. Inspired by recent theories on the cognitive understanding of users in problem-solving, a framework is proposed to solve the Inverse Reinforcement Learning (IRL) problem. The method builds on a bayesian formulation of the learning objective, which considers the conceptual understanding of different users in planning. The experiments are conducted over a simple planning maze, taken from previous works in cognitive science."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The authors address the failure of value alignment in IRL as an issue of concept misalignment. This is a new idea that could lead to potential benefits in learning personalized policies that are aligned with end users' concepts. The paper is well-written and clear and draws this important connection with previous work in cognitive sciences. \n\nOverall, the idea, while simple, leads to sensible improvements compared to other strategies that are agnostic to users' concepts. The authors verified their method also via user studies which confirms the superiority of the proposed method. Appreciably, the IRL formulation taking into account the user construal attains a higher positive correlation with the human inference reward.\n\nIn conclusion, the authors motivate and prove that the integration of the users' construals has its own merits and deserves to be taken into account for future improvements in the setting of IRL."
                },
                "weaknesses": {
                    "value": "The idea of connecting IRL with a construal model of the user is interesting but by far a milestone proposed in a series of previous papers on the subject. In the related work section, it is mentioned that Ho and Griffiths (2022) treated the problem in IRL. I understand that the fundamental contribution of the paper is essentially introducing (Eq. 3). The user concepts are already known beforehand and treating the hard case, where they are discovered or taken from a potentially big vocabulary, is only mentioned as future work for benchmarking this method. It is not clear what could be potentially cases solved by this approach, which requires more scrutiny. \n\nMoreover, no details are provided on how Eq. 3 is implemented and the experiments are entirely synthetic, which leaves open how the framework could be adapted and what the impact would be in real-world cases where users' construal may change more. How do you estimate $P(R, \\hat T)$? Is it already known beforehand? It is also not entirely clear what concepts $\\hat T$ encode, are they semantical concepts related to some properties of the world (specific to one user) or are they just non-interpretable values reflecting the state of the user? Are the user concepts causal in nature, or belonging to some ontology? How is this related to the standard use of concepts in explainable AI, e.g. [1,2,3]?\n\nMy feeling is that this work goes in the right direction but lacks a challenging case study for the proposed method which renders the contribution limited for a paper in this venue. It is not mentioned whether the code will be also publicly available.\n\n[1] P. W. Koh et al., Concept Bottleneck Models, ICML (2020) \\\n[2]  B. Kim et al. \"Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav).\" ICML  (2018) \\\n[3] S. Kambhampati et al., Symbols as a Lingua Franca for Bridging Human-AI Chasm for Explainable and Advisable AI Systems, AAAI (2022) (here referred to as symbols)"
                },
                "questions": {
                    "value": "I asked questions in the weaknesses section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8403/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8403/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8403/Reviewer_ouXJ"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8403/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698702304926,
            "cdate": 1698702304926,
            "tmdate": 1699637046844,
            "mdate": 1699637046844,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Zadzhr1suo",
                "forum": "MQ4JJIYKkh",
                "replyto": "OvwTIpzrF1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8403/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8403/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to Reviewer ouXJ"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback. It is quite true that our experiment is run on a simplified setting and not open-world settings. We use a simplified setting intentionally for two main reasons: First, we intended to demonstrate in an unambiguous and interpretable manner the limitations of value alignment as it is currently framed in the literature (often in simplified settings\u2014see, for example, see Hadfield-Menell et al. 2016; Sumers et al., 2022), and to show how incorporating concept alignment could address those limitations. This is a necessary prerequisite for scaling up to more complex settings and a standard research strategy in AI safety and alignment research (see previous citations). Second, a major component of our contribution is the comparison of our model with human judgments that were experimentally elicited. By focusing on a simplified, controlled set of scenarios, we can make more direct comparisons between the formalism and behavioral data.\n\nWithin this scope, we demonstrate the potential negative impact of concept-level misalignment if it is not taken into account, correct for it, and show human reasoning does take into account concept misalignment when inferring values. We know that it is necessary to both demonstrate the negative impact of concept misalignment and correct for it in real-world settings as well; in fact, this is the motivation for our work. Demonstrating this misalignment thoroughly in a simplified setting highlights the exact nature of the problem, and lays the groundwork for demonstrating it at scale and in the real world. \n\nCorrecting for the problem in a measurable way, which is afforded to us by the closed nature of the simplified setting, allows us to start the critically important discussion of how it can be corrected in larger-scale, open-world settings. It will take time and experimentation to demonstrate the impact of concept misalignment on value misalignment for these large-scale, real-world settings, and even more time and discussion to correct for the misalignment in real-world settings. Each real-world setting comes with significant contextual background that must be taken into account when defining what \u201calignment\u201d means in that setting, and how much alignment is needed. For example, scaling up concept alignment to a healthcare diagnostic algorithm would not require agent-based RL at all, and would not be implemented in an IRL setting. Rather it would be about the medical imaging artifacts that lead to diagnostic decisions, the words human physicians use to describe those artifacts, and how to align those concepts to the diagnostic algorithm which analyzes the same medical image and suggests a course of action, like surgery. This type of concept alignment would be a necessary prerequisite to discussing surgery based on what the patient values most (mobility, lack of pain, effect on other organs, long-term psychological impact of the procedure). We have significantly revised our \u201cSocietal Impacts\u201d and \u201cLimitations and Future Work\u201d sections (6.2-6.3) to discuss this as suggested in the reviewer\u2019s feedback.\n\nThe code and data are indeed publicly available, linked here: https://osf.io/hd9vc/?view_only=967b0c2f981d4a87bf4d21ff818f1322\n\nTo answer the reviewer\u2019s questions regarding the translation of Eqn 3 to code: \n1. $\\tilde{T}$, the demonstrator\u2019s construed task dynamics, encodes blocks and notches as shown in Figure 4. If the demonstrator\u2019s construal is that they are paying attention to notches, their MDP reflects this based on which blue tiles seem traversable to them (notches) and which don\u2019t (blocks). If they are not paying attention to notches, then their MDP reflects this with all blue squares as blocks. \n2. The prior $P(R, \\tilde{T})$ is uniform. \nWe have added text to the updated paper clarifying both of these points, thank you for the questions.\n\nReferences\n\nHadfield-Menell, D., Russell, S. J., Abbeel, P., & Dragan, A. (2016). Cooperative inverse reinforcement learning. Advances in neural information processing systems, 29.\n\nSumers, T., Hawkins, R., Ho, M. K., Griffiths, T., & Hadfield-Menell, D. (2022). How to talk so AI will learn: Instructions, descriptions, and autonomy. Advances in Neural Information Processing Systems, 35, 34762-34775."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8403/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700612946332,
                "cdate": 1700612946332,
                "tmdate": 1700612992547,
                "mdate": 1700612992547,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "4aunwrFvtH",
            "forum": "MQ4JJIYKkh",
            "replyto": "MQ4JJIYKkh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8403/Reviewer_LqqC"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8403/Reviewer_LqqC"
            ],
            "content": {
                "summary": {
                    "value": "The work motivates the importance of modeling human mental model of the task (or the dynamics function they would use) to generate demonstrations for an agent while the agent attempts to perform IRL. They present this as \"Inverse Construal\" problem and provide a grid world based example for the same. They argue, citing prior works, that a way human dynamics function may be different from the agent dynamics function is because humans may simplify the dynamics so that they are able to plan / generate demonstrations easily. They conduct a subject study to highlight their arguments for this domain."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "The example is helpful in understanding the arguments made in the paper.\n\nThe paper presents a useful subject-study that establishes the need for modelling human dynamics along with learning reward models in the context of IRL."
                },
                "weaknesses": {
                    "value": "It seems that the work is pushing for \u201cIRL agents attempting to learn from human trajectories should take into account human mental model of the task\u201d. There is a considerable body of work that highlights the importance of modelling human mental models for behavior synthesis (as an example [1]) in automated planning. That is, taking into account human mental models (such as the transition function being used by them) for agent tasks like behavior synthesis, goal recognition, intention prediction etc. is already well motivated (and the current discussion seems to rediscover this for IRL).\n\nEven in the context of IRL, the notion of the correspondence problem (see Related work in [4]) is very related here (not discussed in the paper). The key idea is the dynamics of the demonstrations are different than the dynamics of the agent and the mis-match causes issues with leveraging the demonstrations. In this work the mis-match is motivated through a specific situation where the human demonstrator plans on \u201csimplified dynamics\u201c, however the formulation does not make this distinction. With respect to the formulation the authors assume that the demonstrator has a different dynamics $\\tilde{T}$ where there is no formal restriction on  $\\tilde{T}$ and a loose specification that  $\\tilde{T}$ is \u201dsimpler or easier to solve\u201c. Typically correspondence problem has been viewed through the lens that the demonstrations were collected in a different domain and the agent is acting in a different domain (for example demonstrations is a real human providing robot arm movement and the agent is working in a simulated environment), which still satisfies the problem formulation in this paper, that the dynamics are different.\n\nThere are missing arguments on what  $\\tilde{T}$ can be. For example, it seems that it has to be defined over the same states and actions as the agent dynamics function  ${T}$ (through result in section 3.4 and equation 4). This implies that human mental model (or the dynamics they are using to come up with a plan) cannot have arbitrarily different state representations (which I believe comes from the authors description on \u201csimpler and easier to solve\u201d, but the formalism is unclear from text). \n\nThe example considered by the authors, as I understand, assumes that the demonstrator has a different dynamics function as a consequence of state-aliasing [2, 3] or perceptual aliasing (i.e. they mix up the light blue and blue cells etc.). These concepts are well studied in sequential decision making but the current manuscript fails to make essential connections highlighting limited literature review.\n\nAuthors have not reported IRB and subject study details like demographics. I am flagging the work for ethics review.\n\n[1] Chakraborti, T., Kulkarni, A., Sreedharan, S., Smith, D. E., & Kambhampati, S. (2019). Explicability? legibility? predictability? transparency? privacy? security? the emerging landscape of interpretable agent behavior. In Proceedings of the international conference on automated planning and scheduling (Vol. 29, pp. 86-96).\n\n[2] Gopalakrishnan, S., Verma, M., & Kambhampati, S. (2021, June). Synthesizing policies that account for human execution errors caused by state aliasing in markov decision processes. In ICAPS 2021 Workshop on Explainable AI Planning.\n\n[3] McCallum, A. K. (1996). Reinforcement learning with selective perception and hidden state. University of Rochester.\n\n[4] Cao, Z., Hao, Y., Li, M., & Sadigh, D. (2021). Learning feasibility to imitate demonstrators with different dynamics. arXiv preprint arXiv:2110.15142."
                },
                "questions": {
                    "value": "Please refer to \"Weaknesses\" section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "Yes, Responsible research practice (e.g., human subjects, data release)"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "I would request the authors to provide details on whether the study was approved by an IRB and details on subject study such as participant demographics, criteria for hiring subjects etc."
                },
                "rating": {
                    "value": "1: strong reject"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8403/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698831656180,
            "cdate": 1698831656180,
            "tmdate": 1699637046682,
            "mdate": 1699637046682,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "IFZenRcOak",
                "forum": "MQ4JJIYKkh",
                "replyto": "4aunwrFvtH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8403/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8403/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to Reviewer LqqC"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback. It is quite true that this work is not intended to propose an improved IRL algorithm or framework. Rather the purpose of the work, as an AI safety contribution, is to use an existing IRL framework to experimentally demonstrate the consequences of concept-level misalignment to value-level misalignment between humans and machines, and also demonstrate how this misalignment can be remedied if concepts are taken into account. \n\nTo this end, our goal is to provide an empirical starting point for a discussion on how we can incorporate concept-level alignment across forms of AI. Many instances where concept alignment is required will not involve RL at all. For example, scaling up concept alignment to a healthcare diagnostic algorithm would not require agent-based RL at all, and would not be implemented in an IRL setting. Rather it would be about the medical imaging artifacts that lead to diagnostic decisions, the words human physicians use to describe those artifacts, and how to align those concepts to the diagnostic algorithm which analyzes the same medical image and suggests a course of action, like surgery. This type of concept alignment would be a necessary prerequisite to discussing surgery based on what the patient values most (mobility, lack of pain, effect on other organs, long-term psychological impact of the procedure). Due to the breadth of AI safety concerns and applications, it is not possible to exhaustively address concept alignment in a single paper; rather, it will require a larger discussion around how to achieve concept-level alignment between humans and AI. \n\nWe do of course have IRB approval, and a walkthrough of our study is included as an appendix to the original submission. The study was conducted through the commonly-used research participant recruitment platform Prolific, and participants were from the US and UK. Participants were not filtered by other demographics or through any other questions. We are happy to share deanonymized IRB documentation with ethics reviewers if appropriate.\n\nFinally, we thank the reviewer for drawing our attention to this important body of work in automated planning on modeling human mental models. In the updated version of the paper, we now reference this literature. Our work builds on these important contributions in two ways that have not been previously approached in the literature (as far as we are aware). First, we benchmark discrepancies in mental models between standard IRL and real humans using controlled behavioral studies that builds on an established experimental paradigm from the psychology literature. Second, we propose a general inferential framework for approaching this problem grounded in recent work on computational cognitive science. As far as we are aware, this has not been previously explored in the literature. For example, although the workshop paper that the reviewer references by Gopalakrishnan et al (2021) does report comparisons with humans, it makes only coarse-grained predictions about policy imitation (e.g., in complex versus simple policies), whereas our account explicitly considers the role of mental models via inference about the process of construal. Our work takes a computational cognitive science perspective to ensure that discussion of value alignment in AI safety remains concretely grounded in human behavior."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8403/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700612805539,
                "cdate": 1700612805539,
                "tmdate": 1700612805539,
                "mdate": 1700612805539,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "uK16VMfuHe",
                "forum": "MQ4JJIYKkh",
                "replyto": "4aunwrFvtH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8403/Reviewer_LqqC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8403/Reviewer_LqqC"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "I appreciate author's response to my concerns. [I am not able to see the revised pdf, as open review still shows that there are no revisions.]\n\nAfter the author's response, I feel my understanding of the work was correct and I remain on my point on limited novelty as the need for mental modeling has been established in several prior works. I would appreciate if the authors could address my concerns on their assumptions on the transition dynamics function [see para 2 and 3 of my review]. \n\nI am happy to read that the authors followed an approved IRB protocol. They should follow HRI works to report human subject study information [for double-blind submissions] and provide details such as demographics, participant hiring information etc. for reproducibility. This becomes all the more important as the authors feel their primary contribution is to \"benchmark discrepancies in metal models .... using controlled behavioral studies\". \n\nI will maintain my score based on our existing discussion and after reading comments by other reviewers. I feel the work can substantially benefit from making it explicit what their claims are and clearly contrast with prior work. The current study is definitely a first step towards establishing the need for mental modeling and the authors can explore its utility further in their setting of IRL or AI safety."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8403/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700681192136,
                "cdate": 1700681192136,
                "tmdate": 1700681222752,
                "mdate": 1700681222752,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "IkzqSxSOFj",
                "forum": "MQ4JJIYKkh",
                "replyto": "xb9tgDgUl3",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8403/Reviewer_LqqC"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8403/Reviewer_LqqC"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "It seems I am still unable to see the updated pdf. I am checking with \"Revisions\" on the submission which says that there are no revisions. In any case, I assure the authors that I will check again later / with other reviewers as needed to make sure I do not miss any details critical to my review. \n\nMeanwhile my question on transition dynamics is as given in the original description. It is possible that authors have improved the formalism to answer that, but it is not present in their rebuttal text. From the author's rebuttal it seems like a their description of the transition dynamics as what the demonstrator is paying attention to, is again, similar to the arguments made in the several works arguing for mental modeling of other agents. \n\nWhile the rebuttal period is about to end, I would suggest the authors to view relevant works in Kulkarni et al, and Dragan et al on Plan Legibility which by definition takes into account the human mental models. Further, they should also view several works on the correspondence problem. The current work motivates the need to model the transition dynamics of the human (which the authors rediscover as construals for their IRL setting). I feel there are fundamental similarities between their work and those in AI Planning etc. that should be highlighted and can make up for a stronger contribution that clearly highlights their contribution."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8403/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700720111322,
                "cdate": 1700720111322,
                "tmdate": 1700720111322,
                "mdate": 1700720111322,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "33lZS4oYg1",
            "forum": "MQ4JJIYKkh",
            "replyto": "MQ4JJIYKkh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8403/Reviewer_thM9"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8403/Reviewer_thM9"
            ],
            "content": {
                "summary": {
                    "value": "This paper highlights the importance of considering a human's mental approximations (described as \"construals\") when making inferences about human preferences from their observed behavior.  As humans often rely on approximate models of the real world when planning their actions, evaluating potential preference models under the assumption that they plan under an exact world model may lead to incorrect inferences about their true preferences.  This in turn may lead to misalignment between an AI's future actions and what the human would have expected.\n\nThey formalize this problem in terms of Bayesian inverse reinforcement learning, where the \"true\" dynamics of the environment (which the AI knows exactly) are replaced with multiple possible approximations, which are jointly inferred with the reward function encoding the human's preferences.  Their main contribution is a set of experiments demonstrating that their \"construal\" IRL model matches the inferences of human subjects far better than standard Bayesian IRL in a navigation task where an approximate model leads to behavior that is very different from optimal behavior under the true model."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The primary strength of the paper is in highlighting the importance of concept alignment in efforts to achieve human-AI alignment.  This issue takes on far greater significance today than it did a few years ago, as AI agents trained with human feedback and demonstrations are now widely deployed in the real world.  The work demonstrates that even in simple settings, failure to account for mental approximations can lead to catastrophic misalignment.  The key takeaway is that we must be careful when learning from humans that we account for limitations in the human's knowledge of the world, and how this might affect their behavior relative to their preferences.\n\nThe paper also draws important links between existing psychological research on human conceptualization and planning, and the problem of human-AI value alignment."
                },
                "weaknesses": {
                    "value": "My main concern is that there are conceptual barriers to applying the insights of this work to algorithms that scale to real-world problems.  Bayesian IRL can be viewed as a \"regularized\" form of behavioral cloning, where the preference for policies that are optimal under high-probability reward functions improves generalization from limited amounts of data.  The inference model proposed here retains this advantage because the space of reward functions and concepts is tightly constrained.  Scaled-up, however, both the reward model and the approximate planning model would need to be far more complex, to the point where we would not expect (an approximation of) Bayesian IRL to be any more sample efficient that behavioral cloning with a similarly complex policy model.  Put another way, given enough data and a sufficiently flexible reward model, we would expect \"exact\" Bayesian IRL to be able to predict human behavior as well as \"construal\" BIRL, with the difference in sample complexity becoming less significant as we scale up to more complex tasks.\n\nWhile I wouldn't expect the paper to solve these issues itself, a deeper discussion of these potential limitations would be useful to the reader.  It would also have been nice to see connections drawn between this work and more scalable approaches to learning from demonstration (e.g., Ho and Ermon, 2016)\n\nThe other weakness with the work is that the theoretical model is not itself particularly novel.  Essentially they do Bayesian IRL where the parameters of both the reward function *and* the dynamics model are inferred from human behavior.  A number of previous works have used essentially the same model (e.g., Herman et al. 2016)\n\nThe authors should reference previous work in this space, and highlight how the motivations of this work differ from those of previous works with similar mathematical models.\n\nReferences:\n1. Ho, Jonathan, and Stefano Ermon. \"Generative adversarial imitation learning.\" Advances in neural information processing systems 29 (2016).\n2. Herman, Michael, et al. \"Inverse reinforcement learning with simultaneous estimation of rewards and dynamics.\" Artificial intelligence and statistics. PMLR, 2016."
                },
                "questions": {
                    "value": "1. Did any of the human subjects experiments evaluate human inference with a subset of the trajectories?  It seems possible that humans might make the same inferences about preferences and concepts without sufficient examples to rule out alternative hypotheses.  For example, differences between experiments might que them to provide different answers than they did in previous experiments.\n2. A minor point, but were any participants rejected because they failed to correctly understand the \"notch\" concept themselves?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8403/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8403/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8403/Reviewer_thM9"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8403/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698955463938,
            "cdate": 1698955463938,
            "tmdate": 1700686034507,
            "mdate": 1700686034507,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qWxVKnTYGJ",
                "forum": "MQ4JJIYKkh",
                "replyto": "33lZS4oYg1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8403/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8403/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to Reviewer thM9"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback. It is true that this is not intended to scale in its current form, but rather demonstrate concretely the limitations of value alignment as it is currently discussed in the literature, and to show how incorporating concept alignment could address those limitations. To understand the limitations of value alignment at scale, the AI safety community will need to study it across many different contexts and disciplines, of which this paper addresses only one.\n\nWhat these limitations look like vary greatly depending on the context of the AI being used, and the degree of human-AI alignment needed in each of those contexts. For example, scaling up concept alignment to a healthcare diagnostic algorithm would not require agent-based RL at all, and would not be implemented in an IRL setting. Rather it would be about the medical imaging artifacts that lead to diagnostic decisions, the words human physicians use to describe those artifacts, and how to align those concepts to the diagnostic algorithm which analyzes the same medical image and suggests a course of action, like surgery. This type of concept alignment would be a necessary prerequisite to discussing surgery based on what the patient values most (mobility, lack of pain, effect on other organs, long-term psychological impact of the procedure). If the AI is not able to honestly and concretely align concepts with the patient or the doctor, and meaningfully explain why a diagnostic judgment was made, the resulting medical decision is likely to be value misaligned with the patient in critical ways.\n\nIn contrast, scaling up concept alignment to something like deep RL could be much more similar to our Bayesian IRL example, with the values explicitly tied to the reward functions. We anticipate and encourage this breadth of work studying concept alignment and the resulting effect on value alignment, or misalignment, as a critical step forward in AI safety discussions. \n\nWe have significantly revised our \u201cSocietal Implications of Concept Misalignment\u201d section and \u201cLimitations and Future Work\u201d section (6.2-6.3) to discuss these implications, per the reviewer\u2019s suggestion. We have also highlighted how our motivations differ from the previous works with similar mathematical models mentioned in the review, and the updated paper now references these works, thank you for bringing them to our notice.\n\nAnswers to questions:\n1. All human subjects were shown the full set of trajectories, which were designed to have sufficient examples to rule out alternative hypotheses. The human subjects and the IRL models were of course shown the same set of trajectories.\n\n2. Participants were not rejected/excluded for failing to correctly understand the \"notch\" concept."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8403/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700612705323,
                "cdate": 1700612705323,
                "tmdate": 1700612705323,
                "mdate": 1700612705323,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VV026eudvE",
                "forum": "MQ4JJIYKkh",
                "replyto": "qWxVKnTYGJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8403/Reviewer_thM9"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8403/Reviewer_thM9"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "Thank you for taking the time to respond to my comments and questions.\n\nI tend to agree with the other reviewers that most of the ideas presented in this paper have appeared elsewhere.  A number of previous works have considered the challenge of imitation learning in settings where the human's understanding of the environment (either the dynamics or the current state) may not match that of the AI.  The conceptual contribution of the paper is therefore very limited.\n\nThe main contribution of the work is really the experimental results showing that humans tend to make \"construal\" inferences from observations of other's behaviour, and are able to recognise mental approximations used by others.  These results are also somewhat limited, as they only consider a single experimental task, and it remains possible that there are other explanations for the observed results.  Clearly demonstrating that humans reliably infer other's mental approximations would require a larger set of experiments, with an emphasis on identifying situations in which humans *fail* to make such inferences (for example, if the space of possible simplifications is not made explicit in the instructions for the experiment).\n\nFurthermore, I feel that there is something of a disconnect between the focus of discussion in the paper on the importance of accurate construal inference for human-AI alignment, and the experimental results which essentially go in the opposite direction (showing that humans themselves make construal inferences).  The real take-away from the paper is that AI agents need to consider how construal inferences might affect human's predictions of the AI's behaviour.  This could be an issue in human-robot interaction settings, for example."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8403/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700685999498,
                "cdate": 1700685999498,
                "tmdate": 1700685999498,
                "mdate": 1700685999498,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "kaKRCpR8YG",
            "forum": "MQ4JJIYKkh",
            "replyto": "MQ4JJIYKkh",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8403/Reviewer_cFhp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8403/Reviewer_cFhp"
            ],
            "content": {
                "summary": {
                    "value": "The authors look at the benefits of implementing concept simplification strategies into inverse reinforcement learning (this yields \"inverse construal\").  The key message is that if an IRL algorithms that fail to model what the demonstrator knows risk failing to understand their reward function.  A bound is provided for the gap in performance between a construal-aware estimate and an entropy-regularized one.  The authors demonstrate this issue in a novel synthetic setting and with user-collected data."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "**Originality**: To the best of my knowledge, the idea of modeling construals (what the demonstrator knows or is constrained by) is new in IRL.\n\n**Clarity**: The text is crystal clear and very easy to follow.  The examples are well constructed.  All critical steps are properly formalized, and the notation is consistent.\n\n**Quality**: The related work section is very well done.  The experimental setup also seems reasonable -- but I am not an expert, so I wouldn't be able to tell whether there are implicit biases in the data collection.\n\n**Significance**: I think the high-level message is very much important and I agree with it.  I think the message is well worth discussing at the conference."
                },
                "weaknesses": {
                    "value": "**Clarity**: I am confused about the usage of the notion of \"concept\" in this context.  In explainable AI, concepts refer to high-level representations (presumably interpretable) of a given input to be explained or otherwise processed.  In cognitive science and logic it has a similar meaning.  Here it is used as a synonym for knowlede or construal.  I would appreciate if the authors could clarify this in the introduction.\n\n**Quality**: [Q1] The single biggest issue with the paper is that it considers only a rather toy synthetic setting.  I am not sure if this is common in the IRL literature."
                },
                "questions": {
                    "value": "I would appreciate some clarification regarding Q1 above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission8403/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8403/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission8403/Reviewer_cFhp"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8403/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699039086789,
            "cdate": 1699039086789,
            "tmdate": 1699637046445,
            "mdate": 1699637046445,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dzrNcd3uay",
                "forum": "MQ4JJIYKkh",
                "replyto": "kaKRCpR8YG",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8403/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8403/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to Reviewer cFhp"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their feedback. The notion of a \u201cconcept\u201d is indeed broad, and construals here are meant to be one potential instantiation of what a concept could look like in a concrete, contained example. Per the reviewer\u2019s suggestion, we have added a clarifying sentence to the introduction explaining that different construals are used to encode different conceptual understandings of the world. \n\nThe reviewer is correct that we use a synthetic, simplified setting to study the problem of concept alignment. This was done intentionally for two main reasons. First, we intended to demonstrate in an unambiguous and interpretable manner the limitations of value alignment as it is currently framed in the literature (often in simplified settings\u2014see, for example, see Hadfield-Menell et al. 2016; Sumers et al., 2022), and to show how incorporating concept alignment could address those limitations. This is a necessary prerequisite for scaling up to more complex settings and a standard research strategy in alignment research (see previous citations). Second, a major component of our contribution is the comparison of our model with human judgments that were experimentally elicited. By focusing on a simplified, controlled set of scenarios, we can make more direct comparisons between the formalism and behavioral data.\n\nTo understand the limits of value alignment at scale, the AI safety community must study it across many different contexts and disciplines, of which this paper addresses only one. \u201cValues\u201d of AI systems are tricky to define, and an IRL setting gives us one very concrete way to define them (in terms of reward functions). We agree that it is very important to study AI \u201cvalues\u201d outside of such a controlled setting, and even more important that we also study concepts in tandem with values (and we hope this paper demonstrates why studying both together is important). We hope that our empirical framework provides a starting point for designing such studies.\n\nReferences\n\nHadfield-Menell, D., Russell, S. J., Abbeel, P., & Dragan, A. (2016). Cooperative inverse reinforcement learning. Advances in neural information processing systems, 29.\n\nSumers, T., Hawkins, R., Ho, M. K., Griffiths, T., & Hadfield-Menell, D. (2022). How to talk so AI will learn: Instructions, descriptions, and autonomy. Advances in Neural Information Processing Systems, 35, 34762-34775."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8403/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700612596540,
                "cdate": 1700612596540,
                "tmdate": 1700613030983,
                "mdate": 1700613030983,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0VE8KxfmeW",
                "forum": "MQ4JJIYKkh",
                "replyto": "dzrNcd3uay",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8403/Reviewer_cFhp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8403/Reviewer_cFhp"
                ],
                "content": {
                    "title": {
                        "value": "Reply to Authors"
                    },
                    "comment": {
                        "value": "Thank you for the detailed response.  I agree that there is value in working with simplified settings, but it is also clear that - by constructions - these settings do not match the full complexity of real-world applications.  In this sense, I still think that working with *only* synthetic settings is a weakness.\n\nRegardless, considering how negative the other reviewers were concerning novelty, I will have to discuss with them before deciding whether and how to revise my score."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8403/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700724408499,
                "cdate": 1700724408499,
                "tmdate": 1700724408499,
                "mdate": 1700724408499,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]