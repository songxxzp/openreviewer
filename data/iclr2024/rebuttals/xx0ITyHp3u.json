[
    {
        "title": "Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging"
    },
    {
        "review": {
            "id": "FPU0Rv9uH9",
            "forum": "xx0ITyHp3u",
            "replyto": "xx0ITyHp3u",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5239/Reviewer_hicK"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5239/Reviewer_hicK"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a new pruning technique named Sparse Model Soups, which combines the weight averaging methods from [1] with the pruning method described in [2]. They provide empirical evidence to support the idea that this straightforward aggregation enhances the performance of pruned models in image classification tasks.\n\nReferences\n\n[1] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. In International Conference on Machine Learning, 2022.\n\n[2] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems, 2015."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "Clarity\n- The paper is well written, ensuring it is understandable for readers.\n- The suggested method is straightforward and effectively explained for easy understanding.\n\nOriginality and Significance\n- This paper introduces a novel pruning method that incorporates model averaging techniques based on the Model Soup methods.\n- They provide empirical evidence demonstrating that the proposed method offers improved performance when compared to baseline approaches."
                },
                "weaknesses": {
                    "value": "Method\n- Although they can parallelize the training process, performing $m\\times k$ training epochs still imposes a substantial computational burden. And if the training cost becomes small, the overall performance gain significantly drops for the extreme sparsity cases.\n\nExperiments\n- It would be valuable to conduct empirical analyses to investigate why performance degradation occurs in regions of extreme sparsity.\n- Similarly, it would be beneficial to empirically analyze why SMS performs well in situations of early sparsity, where batch randomness can lead to divergence between averaged weights [3].\n- It would be advantageous to include an ablation study exploring different combinations of averaging coefficients where the $\\lambda_i$ values differ from each other.\n\nRecommend\n- I suggest that the authors consider adding an Ethics Statement and a Reproducibility Statement immediately following the main paper.\n\nReferences\n\n[3] Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang. What is being transferred in transfer learning? Advances in neural information processing systems, 2020."
                },
                "questions": {
                    "value": "See the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5239/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698487469155,
            "cdate": 1698487469155,
            "tmdate": 1699636522858,
            "mdate": 1699636522858,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GitVUdzbId",
                "forum": "xx0ITyHp3u",
                "replyto": "FPU0Rv9uH9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5239/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5239/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer hicK"
                    },
                    "comment": {
                        "value": "We thank you for your positive feedback and constructive remarks.\n\n### Weaknesses\n> Although they can parallelize the training process, performing $m \\times k$ training epochs still imposes a \n> substantial \n> computational burden. And if the training cost becomes small, the overall performance gain significantly drops for \n> the extreme sparsity cases.\n\nWe acknowledge that all model-averaging methods that rely on training multiple models in parallel (as e.g. the \noriginal model soups) induce some computational overhead in order to increase the final performance. Approaches that \naverage models along the training trajectory (such as e.g. SWA) do not have that drawback, but fall behind in \nperformance, at least in our particular setting (please see the response to Reviewer fESa for a full account on \nmethods like SWA).\n\nHowever, please note that this is only true for extreme levels of sparsity, that would require iterative pruning in \nany case to reach the full performance of classical IMP. In fact, as visible in Figure 4b, for reasonable levels of \nsparsity SMS is able to outperform its prolonged IMP counterpart after a minimum number of epochs and with \nsignificant improvements. In Appendix C, we further study that SMS is able to improve among its individual averaging \ncandidates independent of the retraining length and sparsity - there just exists a certain tradeoff at which \nretraining is too short such that averaging $m$ models gives better results than retraining $m$ times as long.\n\n> It would be valuable to conduct empirical analyses to investigate why performance degradation occurs in regions of \n> extreme sparsity.\n\nThank you for that remark - we have addressed this point from a qualitative perspective, but we agree that further \nempirical substantiation improves our work. In the paper, we argue that extremely high sparsity levels (the problem \noccurs at 99\\% pruned in One Shot and above) lead to a model that is not stable to randomness anymore, i.e., two \nretrained models do not lie in the same basin and thus cannot be averaged. To substantiate this claim, we have run \nfurther experiments and tracked several metrics to see how the candidates for averaging behave. One important \nempirical observation lies in tracking the $L_2$-norm distance between the candidates: we observe that for \nsparsities in the range 90\\%-98\\%, the mean and maximal $L_2$-distance between the 5 candidate models as in the \nexperiment for Figure 4a are relatively stable among sparsities. Increasing the sparsity to 99\\% and 99.5% however \nleads to a much increased distance between the retrained model. At this sparsity, the models are driven further \napart, supporting our hypothesis of instability to randomness - they do not converge to the same basin.\n\nThe upcoming revision will include an ablation study including these experiments in a more detailed form. We will \ninclude them in the manuscript as soon as possible and will let you know.\n\n> Similarly, it would be beneficial to empirically analyze why SMS performs well in situations of early sparsity, \n> where batch randomness can lead to divergence between averaged weights [3].\n\nCould you elaborate what you mean by 'early sparsity'? We are not entirely sure if we understand you correctly here.\n\n> It would be advantageous to include an ablation study exploring different combinations of averaging coefficients \n> where the $\\lambda_i$ values differ from each other.\n\nWe have experimented a lot with what Wortsman et al. call *LearnedSoup*, where the $\\lambda_i$ coefficients are \nlearned through optimization on a held-out validation set. In our experiments, LearnedSoup was never able to reach \nthe performance of UniformSoup or GreedySoup. We are happy to include these results in the appendix in the next \nrevision - do you have any particular suggestion on how to perform an ablation study on the effect of differing \n$\\lambda_i$? How should the values be chosen in such a study, if not automatically as in LearnedSoup?\n\n### Recommendations\n> I suggest that the authors consider adding an Ethics Statement and a Reproducibility Statement immediately \n> following the main paper.\n\nThanks for that remark, we will add both to the revision.\n\nWe thank you again for your insightful review and hope to have addressed your concerns. Please let us know in case \nthere are further doubts or questions."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5239/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700034394842,
                "cdate": 1700034394842,
                "tmdate": 1700034394842,
                "mdate": 1700034394842,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "STH9eg2PHQ",
                "forum": "xx0ITyHp3u",
                "replyto": "GitVUdzbId",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5239/Reviewer_hicK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5239/Reviewer_hicK"
                ],
                "content": {
                    "title": {
                        "value": "Additional question"
                    },
                    "comment": {
                        "value": "Thank the authors for the detailed explanation.\n\nSimilarly, it would be beneficial to empirically analyze why SMS performs well in situations of early sparsity, where batch randomness can lead to divergence between averaged weights [3].\n\nHere, what I mean by 'early sparsity' is when a lower sparsity level in IMP iterations occurs (which are early iteration stages in IMP). In such cases, batch randomness can cause divergence among solutions, creating a high loss barrier between particles. This divergence could impede the particles from being effectively averaged.\n\nAnd, I am curious how the SMS can perform well in that situation."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5239/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700109996909,
                "cdate": 1700109996909,
                "tmdate": 1700109996909,
                "mdate": 1700109996909,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "s3zwZP1nDW",
                "forum": "xx0ITyHp3u",
                "replyto": "Dz2jPdpF4X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5239/Reviewer_hicK"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5239/Reviewer_hicK"
                ],
                "content": {
                    "title": {
                        "value": "Response to Authors"
                    },
                    "comment": {
                        "value": "Your extra explanations are appreciated. I intend to maintain my score comfortably above the acceptance threshold."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5239/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700267526122,
                "cdate": 1700267526122,
                "tmdate": 1700267526122,
                "mdate": 1700267526122,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "SPBYAgdxf5",
            "forum": "xx0ITyHp3u",
            "replyto": "xx0ITyHp3u",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5239/Reviewer_fESa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5239/Reviewer_fESa"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents the Sparse Model Soups (SMS) framework, which applies the model soup algorithm to the neural network pruning procedure. Experimental results show that the model soup algorithm, a one of the most well-known weight-averaging methodologies that had demonstrated notable success in training dense neural networks, is also effective in iterative pruning procedures for training sparse neural networks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Originality and significance: The proposed SMS framework does not bring a substantial level of novelty, as one can consider it as an application of the existing model soup algorithm to an arbitrary iterative pruning process. However, its originality comes from the actual implementation in the domain of neural network pruning, even though the individual elements may already exist separately. Considering the recent success of weight averaging methods for dense neural networks, it is valuable to explore the extension of these techniques to sparse neural networks.\n\nQuality and clarity: Based on observations in the field of transfer learning, where fine-tuned models from the same pre-trained model tend to stay in the nearby region, the hypothesis that a similar phenomenon will occur when re-training the same pruned model is well-founded. The effectiveness of the proposed SMS framework is confirmed through a range of experiments conducted in the domains of image classification, semantic segmentation, and neural machine translation."
                },
                "weaknesses": {
                    "value": "While the proposed SMS framework incorporates the model soup algorithm in the context of neural network pruning, it does not provide specific insights into the unique factors that are especially pertinent to sparse network training. The questions remain: What attributes of the model soup algorithm contribute to its effectiveness in the neural network pruning regime? Is it the same reason why weight-averaging methods have succeeded in conventional dense network training?"
                },
                "questions": {
                    "value": "1. Alongside model soups, another prominent weight-averaging strategy is Stochastic Weight Averaging (SWA). The fact that SMS requires m training runs at each cycle makes SWA, which performs weight averaging within a single SGD trajectory, somewhat appealing. Are there any baseline results using SWA instead of model soups?\n\n2. I understand that the authors have opted to show exclusively the UniformSoup results for CityScapes and WMT16 since the GreedySoup algorithm utilizes validation data, which are the test split here. However, despite the potential fair comparison issue, presenting additional GreedySoup results might offer valuable insights into the benefits of selectively using soup ingredients at high sparsity levels."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5239/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5239/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5239/Reviewer_fESa"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5239/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698841788227,
            "cdate": 1698841788227,
            "tmdate": 1700690453915,
            "mdate": 1700690453915,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "hklqrkCKwa",
                "forum": "xx0ITyHp3u",
                "replyto": "SPBYAgdxf5",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5239/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5239/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer fESa"
                    },
                    "comment": {
                        "value": "Thank you for your review! We are happy to address the individual points raised, as they help us to improve our work.\n\n### Weaknesses\n> What attributes of the model soup algorithm contribute to its effectiveness in the \n> neural network pruning regime? Is it the same reason why weight-averaging methods have succeeded in conventional \n> dense network training?\n\nTo clarify, let us re-emphasize the key insight of our work. First of all, addressing your second question, \nweight-averaging methods that \"have succeeded in conventional dense network training\" (such as e.g. SWA) are of a \ndifferent nature: they average models along the optimization path to explore different optima but always train a \nsingle model with fixed hyperparameters.\n\nIn contrast, the less conventional model soup approach is specific to transfer learning and involves training \nand averaging multiple models with varied hyperparameters. Its effectiveness in transfer learning has been \nthoroughly validated by Wortsman et al., with their empirical evidence (Figure 2 in Wortsman et al., 2022a) showing \nthe best generalizing solution often resides 'between' two finetuned networks.\n\nReturning to our work, we maintain the same downstream task without domain shifts. However, we think and have argued \nthat pruning and retraining a model strongly resembles the finetuning paradigm in transfer learning: we interpret \npruning as feature perturbation and retraining as adapting these features to the original task, as if the original \npretraining would have been done on a different task (i.e. the one leading to the perturbed features). While we \nthink that this provides a good intuition, it is far from obvious that the model soup mechanism also holds when a \nlarge portion of weights are pruned, reducing the model's performance to that of a random classifier. The model \nundergoes significant \nperturbation/damage, raising questions about its 'stability to randomness' (as per Frankle et al. 2020). It's \nuncertain if two versions of such a heavily pruned model would be averageable, particularly as aggressive learning \nrate schedules during retraining could drive the models apart.\n\nOur work is the first to empirically demonstrate that this is indeed possible and significant performance gains, \nboth in-distribution and out-of-distribution, are achievable across diverse architectures and datasets. Returning to \nyour first question, we think that the insights from transfer learning can be transferred to \nthe network pruning regime, motivated by the resemblance between the two paradigms as outlined above, and substantiated \nby extensive results.\n\n### Questions\n> Are there any baseline results using SWA instead of model soups?\n\nThank you for suggesting the comparison between SWA and SMS, it's an interesting improvement to our work. We \nconducted experiments, but before reporting and discussing the results, let us make some general remarks.\n\n1. SWA is only beneficial if models of the same sparsity level and pattern are averaged, as differing sparsities \n   will densify the model (see Figure 1). We apply SWA separately in each phase, starting each phase with the averaged model from \n   the previous phase and reinitializing SWA accordingly.\n2. SWA and SMS are not excluding each other, they can be used in conjunction, potentially further improving the \n   effect of SMS.\n3. SWA requires either a cyclic or high constant learning rate to explore multiple optima for beneficial averaging. \n   However, retraining after \n   pruning uses specific translated learning rate schedules (such as FT, LRW, CLR, LLR or ALLR) to maximize \n   performance.\n\nIn our experiments using ResNet-50 on ImageNet, following the same setup as in Table 1 for a sparsity of 90\\% in \nthree cycles,we observed slightly inferior results with SWA:\n\n- IMP: Test Accuracy 73.72\\% ($\\pm$ 0.00\\%)\n- IMP+SWA: Test Accuracy 73.05\\% ($\\pm$ 0.02\\%)\n\nSWA is not able to improve the results of classical IMP (and hence also falls behind SMS by a large margin, cf. \nTable 1). We think that this is mostly due to the specific retraining schedules used for IMP, which stand in \nconflict with the requirements for SWA. We will add an ablation study dissecting these factors as soon as possible.\n\n\n> [...] despite the potential fair comparison issue, presenting additional GreedySoup results might offer valuable \n> insights into the benefits of selectively using soup ingredients at high sparsity levels.\n\nWe are happy to create a separate validation split for these two specific tasks and will add the experiments as \nsoon as possible, however we do not think that GreedySoup will be able to offer much of an improvement - apart from \nextreme levels of sparsity, the uniform approach lead to the most consistent improvements.\n\nThanks for your review, we hope to have addressed your concerns and answered your questions. Please let us know if \nfurther clarification is needed."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5239/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700034210116,
                "cdate": 1700034210116,
                "tmdate": 1700034210116,
                "mdate": 1700034210116,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "AdKUUomMay",
                "forum": "xx0ITyHp3u",
                "replyto": "hklqrkCKwa",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5239/Reviewer_fESa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5239/Reviewer_fESa"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the authors' efforts in addressing my concerns.\n\n1. Regarding the SWA baseline: The observed performance decline of IMP + SWA compared to IMP is likely due to insufficient hyperparameter tuning, as per my experimental experience. I acknowledge the time limitations during the rebuttal, making comprehensive experiments challenging. I expect that future revisions will provide more thorough experiments and insightful discussions. I am inclined to place the trust in the authors' commitment, as stated in their response: \"We will add an ablation study dissecting these factors as soon as possible.\"\n\n2. Regarding the GreedySoup on CityScapes and WMT16: I thought the experiment would be simple, applying the GreedySoup algorithm to the ingredients already employed for the UniformSoup results provided by the authors. Although the authors said, \"we will add the experiments as soon as possible,\" the end of the discussion period is approaching. I am curious if there are any issues or complications with the experiment."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5239/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700637386618,
                "cdate": 1700637386618,
                "tmdate": 1700637386618,
                "mdate": 1700637386618,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fOcb3nZT9K",
                "forum": "xx0ITyHp3u",
                "replyto": "Q1Rga14Q7P",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5239/Reviewer_fESa"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5239/Reviewer_fESa"
                ],
                "content": {
                    "comment": {
                        "value": "I am thankful for the prototype results that were presented to address my concerns, and I expect dealing with them more extensively in subsequent revisions. Consequently, I am increasing the score to six."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5239/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700690434487,
                "cdate": 1700690434487,
                "tmdate": 1700690434487,
                "mdate": 1700690434487,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "77qU4jCT2j",
            "forum": "xx0ITyHp3u",
            "replyto": "xx0ITyHp3u",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5239/Reviewer_ja8N"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5239/Reviewer_ja8N"
            ],
            "content": {
                "summary": {
                    "value": "The idea is to combine model soups from transfer learning with pruning. The proposal is : in prune-retrain paradigm (be it from scratch or pretrained), the training portion is replaced by model-soup. It improves the overall quality of the final pruned model as is validated extensively in experiments.\n\n[Update]\nIn light of the responses by authors, I am increasing my score to 6."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This being an empirical paper with simple idea proposal, the authors do an excellent job of evaluating their idea in terms of \n(1) extensive experiments on different domains \n(2) covering baselines that are natural competitors to the proposal."
                },
                "weaknesses": {
                    "value": "1) I am not sure if this paper contributes new ideas or analysis. The proposal is to replace the training portion of prune-retrain with model soups. I do not have background on transfer learning, but as a general machine learning person, it is not surprising that it improves the accuracy of the model given the backdrop of model soups paper . Since in both the cases it holds that m copies of model start from the same initialization."
                },
                "questions": {
                    "value": "1) How is IMP $m \\times$ implemented? Is the pruning rate for each prune step reduced? or is the training portions increased m$\\times$. The latter, I suspect, will not be very useful. \n2) Are there any challenges that are specific to using model soups for training portion of prune-retrain algorithm which differentiate it from applying model soups to finetuning of pretrained models? I felt that there were no new challenges here."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5239/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5239/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5239/Reviewer_ja8N"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5239/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698961774798,
            "cdate": 1698961774798,
            "tmdate": 1700694434391,
            "mdate": 1700694434391,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "VIOTHogBRh",
                "forum": "xx0ITyHp3u",
                "replyto": "77qU4jCT2j",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5239/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5239/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ja8N"
                    },
                    "comment": {
                        "value": "We thank you for reviewing our work and acknowledging its strengths! Let us respond to your concerns and questions \nin detail.\n\n### Weaknesses\n> I do not have background on transfer learning, but as a general machine learning \n> person, it is not surprising that it improves the accuracy of the model given the backdrop of model soups paper . \n> Since in both the cases it holds that m copies of model start from the same initialization.\n\nWe regret to hear your doubts about the novelty of our work and wish to clarify. To begin with, the averageability of \nmodels trained from identical initializations is not obvious. Neyshabur et al. (2020) observed that models with the \nsame random \ninitialization but varying batch selections diverge and are not averageable, whereas Frankle et al. (2020) \nestablished averageability after starting from a sufficiently pretrained model. Wortsman et al. (2022a) found that \nfinetuning models from a single pretrained model are averageable, \ndespite task changes (i.e. transfer learning) and the pretrained model potentially being bad at the new task.\n\nIn our research, we avoid domain shifts, focusing on the same downstream task. In non-pruning scenarios, it's \nsomewhat clear that extended training with a reasonable learning rate might yield models whose average is not worse than individual models. However, this is far from obvious in heavily pruned models, where performance drops \nsignificantly. The substantial damage these models undergo raises questions about their 'stability to randomness' \n(as per Frankle et al. 2020). It's not evident that two heavily pruned models are averageable, especially \nunder aggressive retraining schedules.\n\n\nOur work is the first to empirically show that averaging such models leads to notable performance improvements in- \nand out-of-distribution across various architectures and datasets. We link pruning and retraining to finetuning in \ntransfer learning, treating pruning as feature perturbation and retraining as features adaption to the original task \n(as if pretraining would have lead to the perturbed features). A key challenge was overcoming the decreased sparsity \nfrom naive weight averaging due to different sparsity patterns (cf. Figure 1), requiring retraining at the expense of \nperformance (cf. IMP-RePrune, Table 1). Our \nsolution, SMS, effectively addresses this issue. Therefore, we respectfully disagree that our papers does \nnot contribute new ideas, as outlined.\n\n\n### Questions\n> How is $IMP_{m\\times}$ implemented? Is the pruning rate for each prune step reduced? or is the training portions \n> increased m times. The latter, I suspect, will not be very useful.\n\n$IMP_{m\\times}$ increases training epochs per prune-retrain-cycle by a factor of $m$. We believe this, arguably most \nnatural baseline is useful; could you elaborate on why you think it isn't?. Nevertheless, we \nagree that reducing the pruning rate accordingly (i.e. for $m=3$, triple the \namount of prune-retrain cycles to reach a final sparsity) is an interesting \nbaseline, thank you for that remark! In our experience, lower \npruning rates do not necessarily enhance IMP's final product; in fact, empirical evidence from Bartoldson et al. \nindicates that the instability from significant pruning can actually aid generalization.\n\nIn response to your question, we conducted experiments with ResNet-50 on ImageNet, aiming for a 90% sparsity goal, \nreplicating the setting in Table 1. We compared the final test accuracies of $IMP_{m\\times}$, which extends retraining \nlength per cycle, with a variant increasing the number of cycles by $m$ but maintaining a 10-epoch retraining length \nper cycle. The results are as follows:\n\n| Method | $m=3$ | $m=5$ | $m=10$                 |\n|---|---|---|------------------------|\n| $IMP_{m\\times}$ | 74.34\\% ($\\pm$ 0.09\\%) | 74.56\\% ($\\pm$ 0.24\\%) | 74.50\\% ($\\pm$ 0.09\\%) |\n| IMP with $m$ phases | 73.69\\% ($\\pm$ 0.10\\%) | 74.08\\% ($\\pm$ 0.04\\%) | 74.70\\% ($\\pm$ 0.02\\%) |\n\nThis approach mostly falls behind $IMP_{m\\times}$ and in consequence also behind SMS. We however thank you for \nsuggesting this additional baseline, we will include it in the manuscript.\n\n> Are there any challenges that are specific to using model soups for training portion of prune-retrain algorithm \n> which differentiate it from applying model soups to finetuning of pretrained models?\n\nWe hope that this concern is sufficiently addressed by our explanations regarding the novelty and contribution of \nour work - the setting is different and it is a priori not clear whether aggressive pruning yields \nmodels that are averageable, especially given the existing (aggressive) learning rate schemes for retraining. \nFurther, it is a priori not clear how to mitigate the problem of reduced sparsity when averaging arbitrary sparse \nmodels; a problem that does not exist in the setting of transfer learning.\n\nThanks again for your review - please let us know if further clarification is needed!"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5239/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700033989262,
                "cdate": 1700033989262,
                "tmdate": 1700033989262,
                "mdate": 1700033989262,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "m7R3ZgDUvX",
            "forum": "xx0ITyHp3u",
            "replyto": "xx0ITyHp3u",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5239/Reviewer_TvoY"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5239/Reviewer_TvoY"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes merging sparse models by initiating each prune-retrain cycle with the averaged model from the previous phase. They show that averaging these models significantly enhances generalization and OOD performance over their individual counterparts. Overall, in summary, it is an extension of model soups for sparse models."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The experimental section of the paper + supplementary is rich illustrating noticeable gain.\n2. OOD experiments are new for sparse model soup showing SMS consistently improves over the baselines."
                },
                "weaknesses": {
                    "value": "I have significant novelty concerns with the draft. \n\n1. The idea of sparse model averaging has been widely explored including the model soups (eg. https://arxiv.org/abs/2205.15322 https://arxiv.org/abs/2208.10842 https://arxiv.org/abs/2306.10460  etc). \n2. The authors have failed to detail how their method contrasts with existing sparse model soup papers in their related work section. I feel it is just an incremental work over the existing literature. The benefits of averaging the sparse masks is already known.\n3. Although I appreciate the extensive experiments by authors, I still feel the experiments are limited to small-scale datasets and models (maybe ViT scale or OPT models-based experiments will add value).\n4. I feel auxiliary benefits of model soups like OOD robustness, fairness, etc are good directions to explore."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5239/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5239/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5239/Reviewer_TvoY"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5239/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699143092321,
            "cdate": 1699143092321,
            "tmdate": 1700694294549,
            "mdate": 1700694294549,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aBVg5eZVuN",
                "forum": "xx0ITyHp3u",
                "replyto": "m7R3ZgDUvX",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5239/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5239/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer TvoY"
                    },
                    "comment": {
                        "value": "Thank you for reviewing our manuscript. In the following, let us address your concerns in detail.\n\n### Weaknesses\n\n> 1. The idea of sparse model averaging has been widely explored including the model soups (eg. https://arxiv.\n> org/abs/2205.15322 https://arxiv.org/abs/2208.10842 https://arxiv.org/abs/2306.10460 etc). \n\nWe are surprised to see the novelty of our work being questioned, especially since we cite all the papers you \nmention. We regret if this misunderstanding stems from poor communication from our side, see also the \nresponse to your second concern below. Let us clarify how our work differs from the cited papers:\n\n**2205.15322: Yin et al.** focus on Dynamic Sparse Training (DST), which is different from our domain (prune-after- \nand -during-training). They average models within a single training run using fixed hyperparameters, not across \nmultiple runs. Their averaging destroys the sparsity (cf. Figure 1), requiring re-pruning: this is inherent to their \nprune-and-grow DST approach, requiring to explore different sparsity patterns. To mitigate the negative impacts of \nre-pruning, they propose averaging methods like *CIA* and *CAA*. In contrast, our work deliberately \navoids re-pruning by keeping consistent sparsity patterns. Note that **we explicitly compare to the re-pruning \napproach** (cf. Table 1, IMP-RePrune), using both their methods CIA and CAA, outperforming their re-pruning schemes \nsignificantly.\n\n**2208.10842: Yin et al.** use IMP with weight rewinding (IMP-WR) and concentrate on generating lottery tickets, a \ndistinct goal from ours of creating high-performing sparse models. Weight rewinding rewinds the networks to the \npoint of stability (as per Frankle et al.), the issue whether adjacent models are \naverageable is thus entirely \ndifferent to whether they are averageable without rewinding. While the authors average models with varying \nsparsity (those adjacent in the lottery ticket generation process) necessitating re-pruning, our method averages \nparallely trained models, avoiding this detrimental step in our setting.\n\n**2306.10460: Jaiswal et al.** do not average model parameters at all, but rather average early pruning \nmasks to quickly generate subsequent masks. This approach doesn't overlap with our contributions, focusing on \nfast mask generation rather than averaging sparse models.\n\nIf there are other publications you believe align closely with our work, especially those implied by \"etc.\" in your \nlist, could you please specify them?\n\n> 2. I feel it is just an incremental work over the existing literature. The benefits of \n> averaging the sparse masks is already known.\n\nWe were unaware that our manuscript could be misunderstood in that regard, but we take your concern seriously and \nwill clarify the differences with the mentioned papers in our revised manuscript, which will be available soon. We \nhope that this resolves the issue. \n\nWe respectfully disagree that our work is incremental. To our knowledge, no other publication \nhas explored sparse model averaging in the context of prune-after-training or \nduring-training; in fact, \"averaging the sparse masks\" is entirely different from what we do (cf. our \nexplanations regarding Jaiswal et al.). Secondly, previous works on averaging sparse models involve a single \ntraining run and fixed hyperparameters, unlike our model soup approach that averages models from multiple runs with \nvarying hyperparameters. It has been far from \nobvious that this was possible given that pruning drastically alters the model and reduces stability to randomness.\nFinally, our approach uniquely preserves the sparsity pattern, avoiding the need for repruning, a step we \ndemonstrate as harmful and which we significantly improve upon. \n\n> 3. Although I appreciate the extensive experiments by authors, I still feel the experiments are limited to \n> small-scale datasets and models (maybe ViT scale or OPT models-based experiments will add value).\n\nWe respectfully note that our experiments extend beyond \"small-scale datasets and models.\" In fact, they encompass \nmodels of \"ViT scale\", including extensive experimentation with the vision-transformer MaxViT on ImageNet and the \nT5-transformer on WMT16, detailed in Appendix B.\n\n> 4. I feel auxiliary benefits of model soups like OOD robustness, fairness, etc are good directions to explore.\n\nWe have conducted thorough experiments regarding the benefits of SMS in the OOD (Appendix B.2.3) and fairness \n(Appendix B.2.4) setting, demonstrating a consistent and significant improvement over the baseline, particularly in the \nOOD-case. Could you elaborate what additional directions you think are worth exploring?\n\nWe hope our responses have clarified the contributions and novelty of our work. If you \nhave any more questions or concerns, or suggestions for improvement, please let us know. Otherwise, we kindly ask \nyou to reconsider your initial evaluation."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5239/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700033598843,
                "cdate": 1700033598843,
                "tmdate": 1700033598843,
                "mdate": 1700033598843,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "evPOV3dEBI",
                "forum": "xx0ITyHp3u",
                "replyto": "aBVg5eZVuN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5239/Reviewer_TvoY"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5239/Reviewer_TvoY"
                ],
                "content": {
                    "title": {
                        "value": "Post rebuttal response"
                    },
                    "comment": {
                        "value": "After reviewing the authors' responses, I can find some distinguishing factors between the past literature and this paper's method. I again went through the new experiments added in the rebuttal.  I raise my score to 5 based on the author's rebuttal despite having some reservations about novelty."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5239/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700694269243,
                "cdate": 1700694269243,
                "tmdate": 1700694269243,
                "mdate": 1700694269243,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]