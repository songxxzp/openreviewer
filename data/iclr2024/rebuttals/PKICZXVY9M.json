[
    {
        "title": "Overcoming the Pitfalls of Vision-Language Model Finetuning for OOD Generalization"
    },
    {
        "review": {
            "id": "C5Rpz9OUSJ",
            "forum": "PKICZXVY9M",
            "replyto": "PKICZXVY9M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2931/Reviewer_v4h7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2931/Reviewer_v4h7"
            ],
            "content": {
                "summary": {
                    "value": "The paper works on improving the OOD generalization of finetuned vision-language models. Specifically, a class-conditional feature generator and adaptive self-distillation mechanism are proposed to serve the goal."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "[$\\textbf{Interesting Idea}$] The idea of generating unknown-class features is interesting.\n\n[$\\textbf{Presentation Quality}$] The presentation is clear and easy to follow."
                },
                "weaknesses": {
                    "value": "[$\\textbf{Unconvincing Statement}$] The work mentions that it is the first to unveil the pitfalls of finetuning VLMs by prompt learning can cause overfitting on base classes, resulting in poor performance on novel classes. However, CoCoOp \u201cConditional Prompt Learning for Vision-Language Models, CVPR 2022\u201d has already observed this and proposed conditional prompt learning to address it.\n\n[$\\textbf{Unclear Model Design}$] This work uses known image and text features as K and V, while unknown text features as Q to generate unknown image features. The rationale behind this is not clear. It would be nice to explain this in more details.\n\n[$\\textbf{Missing Related Works}$] For finetuning methods, there are many works that need to be discussed, e.g., \u201cCLIP-Adapter: Better Vision-Language Models with Feature Adapters\u201d, \u201cTask Residual for Tuning Vision-Language Models\u201d, \u201cImproving Zero-Shot Generalization for CLIP with Synthesized Prompts\u201d, \u201cMaPLe: Multi-modal Prompt Learning\u201d and \u201cSelf-regulating Prompts: Foundational Model Adaptation without Forgetting\u201d.\n\n[$\\textbf{Small Performance Gains}$] The results in Table 1 show the improvements from adding the proposed method are rather limited. Moreover, the performance is much worse than some SOTA methods, e.g., \u201cMaPLe: Multi-modal Prompt Learning, CVPR 2023\u201d, \u201cImproving Zero-Shot Generalization for CLIP with Synthesized Prompts, ICCV 2023\u201d and \u201cSelf-regulating Prompts: Foundational Model Adaptation without Forgetting, ICCV 2023\u201d. It would be nice to see the performance of these SOTA methods by adding the proposed components."
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2931/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2931/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2931/Reviewer_v4h7"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2931/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697982469781,
            "cdate": 1697982469781,
            "tmdate": 1699636236638,
            "mdate": 1699636236638,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "H5kBxUZegK",
                "forum": "PKICZXVY9M",
                "replyto": "C5Rpz9OUSJ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2931/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2931/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer v4h7"
                    },
                    "comment": {
                        "value": "Thanks for the detailed feedback. Please find below our response to each comment.\n\n**Unconvincing statement: CoCoOp already observes overfitting with a method proposed to address it.**\n\nWe humbly argue that we are the first to provide a ``comprehensive\u2019\u2019 study of overfitting across different finetuning baselines, datasets and OOD settings. Hence in the revised paper, we will tone down the point about being the first and instead focus on our contribution of being comprehensive. Algorithm-wise, instead of using input-conditional prompts in CoCoOp that implicitly tackles overfitting, we propose OGEN to directly reduce overfitting via unknown-aware optimization with OOD feature synthesis and adaptive model distillation. Our benefits are validated by the gains when applying OGEN on top of CoCoOp (Table 1 & 2).\n\n**Unclear model design: explain the rationale behind the attention method used to generate unknown image features.**\n\nPlease refer to Section 3.2 (3rd paragraph) for the reasoning. At high level, to generate unknown image features from just the class name (using its text features), we found it's hard to directly learn the text-to-image feature mapping. This is backed by our ablations in Table 4 - see the poorly performing ``no extrapolation'' baseline. Then we choose to condition the feature generation process on some knowledge about known classes (i.e. the image/text features of kNN known classes), arriving at an extrapolation solution which is easier. The remainder of Section 3.2 introduces two extrapolation schemes, both naturally implemented by cross-attention. The attention mechanism is similar to measuring similarities between the unknown text (query) and kNN known text (key) features first, and then using such text similarities to extrapolate unknown image features from kNN known image features (value). We will clarify more on such rationale in the revised paper.\n\n**Missing related works.**\n\nWe will add discussions and empirical comparisons with the suggested works in the revised paper. Comparing results for many of the new works can also be found in our \u201cResponse to common concern: comparing with the SOTA\u201d.\n\n**Small performance gains in Table 1, and results of OGEN+SOTA methods.**\n\nOur observations from Table 1 are that 1) OGEN achieves notable improvements on new classes, 2) while its base class accuracy remains competitive. This is the desired behavior to us since we focus more on the new class generalization, although this may make the gains in the Harmonic mean of base and new accuracies seem smaller. Table 2 is another example where OGEN achieves convincing gains on new datasets without hurting the source ImageNet performance. For SOTA results, please refer to our \u201cResponse to common concern: comparing with the SOTA\u201d."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2931/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700467596270,
                "cdate": 1700467596270,
                "tmdate": 1700467596270,
                "mdate": 1700467596270,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ogTCr4fcvC",
            "forum": "PKICZXVY9M",
            "replyto": "PKICZXVY9M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2931/Reviewer_Rp4n"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2931/Reviewer_Rp4n"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method to regularize ERM for OOD generalization with CLIP models. The method uses a feature prediction network to hallucinate image features corresponding to unknown texts at training time. The resulting training procedure is more robust, since it takes into account synthesized features from unseen classes. The authors also propose a self-distillation mechanism to complement the method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- I found the method interesting and novel.\n- I found the paper easy to follow. Section 3.2 and Figure 2 are especially informative and organized very intuitively.\n- The method seems like it could be useful."
                },
                "weaknesses": {
                    "value": "- the self-distillation mechanism is easy-to-think-of, but this is okay, since it is not the main innovation.\n- My main concern with this paper would be the results. In particular, they are not state-of-the-art (see [Maple] and [Clipood]). Furthermore, the reported CoOp performance seems low. With some tuning, CoOp can be much better, e.g. [KgCoOp] reported a harmonic mean of 74.6 for CoOp on average, compared to 71.7 reported by the authors.  From personal experiments, I know that simply finetuning both encoders along with the prompt with cross-entropy can achieve much better results on these benchmarks, (80.3 % HM on the base-to-novel benchmark, average of 11 datasets). However, the authors seem to focus on just prompt tuning, so it might be ok.\n\nI'm incline to think that that the method is interesting enough for acceptance, even though, in my opinion, the results are not state-of-the-art.\n\n[Maple] Muhammad Uzair Khattak et al. \"MaPLe: Multi-modal Prompt Learning\"\n\n[Clipood] Yang Shu, Xingzhuo Guo et al. \"CLIPood: Generalizing CLIP to Out-of-Distributions\"\n\n[KgCoOp] Hantao Yao et al. \"Visual-Language Prompt Tuning with Knowledge-guided Context Optimization\""
                },
                "questions": {
                    "value": "None."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2931/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2931/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2931/Reviewer_Rp4n"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2931/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698270672476,
            "cdate": 1698270672476,
            "tmdate": 1700519776726,
            "mdate": 1700519776726,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7mVW8qmiwT",
                "forum": "PKICZXVY9M",
                "replyto": "ogTCr4fcvC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2931/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2931/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Rp4n"
                    },
                    "comment": {
                        "value": "Thanks for the positive feedback on our work and recognition of its novelty. To address concerns about SOTA results, we include more comparisons in our \u201cResponse to common concern: comparing with the SOTA\u201d, where our method is found to consistently improve more recent methods, including the suggested Maple and KgCoOp."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2931/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700467235732,
                "cdate": 1700467235732,
                "tmdate": 1700467235732,
                "mdate": 1700467235732,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BOIKRalKlz",
                "forum": "PKICZXVY9M",
                "replyto": "ogTCr4fcvC",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2931/Reviewer_Rp4n"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2931/Reviewer_Rp4n"
                ],
                "content": {
                    "title": {
                        "value": "response to response"
                    },
                    "comment": {
                        "value": "The new results are quite impressive. I raise my score to 8 and advocate for publication. \n\nIn my original review I had commented that CoOp is much better than what the authors report if the authors make a good faith attempt at tuning that baseline instead of copying the number from previous publications. This point remains unaddressed, but I believe it should not factor into my review since I know so many papers in this area do not tune baselines."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2931/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700519756194,
                "cdate": 1700519756194,
                "tmdate": 1700573444674,
                "mdate": 1700573444674,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "UvMt5WbYwu",
            "forum": "PKICZXVY9M",
            "replyto": "PKICZXVY9M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2931/Reviewer_Y9rR"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2931/Reviewer_Y9rR"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses the limited generalization capabilities of existing vision-language models, which struggle to handle open-domain visual concepts. The authors propose a novel approach called OGEN to improve the out-of-distribution (OOD) generalization of finetuned models. OGEN introduces a class-conditional feature generator that synthesizes OOD features using only the class name of any unknown class, helping to regularize the decision boundary between in-distribution (ID) and OOD data. Additionally, an adaptive self-distillation mechanism is employed to prevent overfitting. Experimental results demonstrate that OGEN achieves considerable improvements in OOD generalization performance across different settings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The forgetting problem is important in foundation models during fine-tuning.\n* The over-fitting observation can support the paper's main claim.\n* The proposed method is reasonable."
                },
                "weaknesses": {
                    "value": "I believe it would be beneficial for this paper to include a comparison with relay-based methods, such as sampling a subset from Lioan-5B and using it for replay. The \"class-conditional feature generator\" seems to serve as a proxy for the replay data, so it would be valuable to directly explore the use of replay methods. As a result, I find the novelty of the proposed approach to be somewhat limited considering this concern."
                },
                "questions": {
                    "value": "See above."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2931/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2931/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2931/Reviewer_Y9rR"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2931/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698569158968,
            "cdate": 1698569158968,
            "tmdate": 1700549575989,
            "mdate": 1700549575989,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2xseB1MFcg",
                "forum": "PKICZXVY9M",
                "replyto": "UvMt5WbYwu",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2931/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2931/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer Y9rR"
                    },
                    "comment": {
                        "value": "Thanks for the feedback! Below is our answer to your main (and great!) question to hopefully address the novelty concern.\n\n**Compare with relay-based methods, e.g. sampling a subset from LAION-5B and using it for replay.**\n\nWe have experimented with replay methods on the older but sufficiently large LAION-400M dataset. The goal is to compare our synthesized OOD features against the real replay data (sampled from LAION-400M) in terms of their impact on training regularization. Please refer to **Appendix-Section D** in the revised paper for experimental details, especially how we sample the replay data, including class filtering, sampling strategy and data size. Our high-level observations from **Fig. 7** are that replay methods perform well with large data size, but suffer from low data efficiency as well as large memory cost (see detailed reasoning in Section D). By contrast, our feature synthesis approach avoids these issues by synthesizing hard OOD features on the fly. One future direction is to combine feature synthesis with replay methods to take advantage of their respective benefits of data efficiency and diversity. We will consider moving some of these discussions/results to the main paper to highlight the novelty and benefits of our feature synthesis approach."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2931/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700467134224,
                "cdate": 1700467134224,
                "tmdate": 1700467134224,
                "mdate": 1700467134224,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ljUWT97Zbc",
                "forum": "PKICZXVY9M",
                "replyto": "2xseB1MFcg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2931/Reviewer_Y9rR"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2931/Reviewer_Y9rR"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for the effort on new experiments. I raised my score to 6 to acknowledge this."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2931/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700549551962,
                "cdate": 1700549551962,
                "tmdate": 1700549551962,
                "mdate": 1700549551962,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "MwCINaRT0K",
            "forum": "PKICZXVY9M",
            "replyto": "PKICZXVY9M",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2931/Reviewer_sauS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2931/Reviewer_sauS"
            ],
            "content": {
                "summary": {
                    "value": "The paper improves the out-of-distribution (OOD) generalization of vision-language models, especially CLIP, when they are finetuned on downstream tasks. The paper makes the following contributions:\n\n- It reveals the overfitting problem of existing finetuning methods, such as prompt learning, that degrade the OOD performance of CLIP models.\n\n- It proposes a novel method called OGEN, which consists of two components: a class-conditional feature generator and an adaptive self-distillation mechanism.\n\n- The paper evaluates OGEN on various downstream tasks and datasets and shows that it consistently improves the OOD generalization of different finetuning methods for CLIP models."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The proposed method addresses a novel and important problem of improving the OOD generalization of vision-language models, especially CLIP when they are finetuned on downstream tasks.\n\n- The paper proposes a novel method called OGEN, which consists of two components: a class-conditional feature generator and an adaptive self-distillation mechanism. The feature generator synthesizes OOD image features given the name of an unknown class, by extrapolating from the most similar known classes. The self-distillation mechanism uses an adaptive teacher model that is an exponential moving average of past model checkpoints within a local time window. The teacher model guides the student model to avoid overfitting and maintain a good trade-off between in-distribution and OOD performance.\n\n- The paper evaluates OGEN on various downstream tasks and datasets and shows that it consistently improves the OOD generalization of different finetuning methods for CLIP models. It also provides comprehensive ablation studies and analysis to validate the effectiveness of each component of OGEN."
                },
                "weaknesses": {
                    "value": "(1) The proposed method mainly compared with CoOp (IJCV'22), Co-CoOp (CVPR'22), and VPT (ECCV'22).\nHowever, before the deadline of ICLR, the state-of-the-art methods are released here: https://github.com/muzairkhattak/PromptSRC\nMaPle (CVPR'23) and  PromptSRC (ICCV'23) need to be discussed in this paper.\n\n(2) In Tab 3 and Tab 4, the proposed class-conditional feature generator slightly decreases the performance of the base classes.\nIn the appendix Fig 5, there are some explanations regarding the performance increase in the new classes and performance variation in the base classes. These discussions need to move to the main script.\n\n(3)\tThe first step of OGEN, novel class extrapolation, is problematic. It is unreasonable to utilize base features to extrapolate novel features, since there are usually large conceptual gaps between base and novel classes. The authors provide a special case, that is \u201ccat, bear->raccoon\u201d. But in CIFAR-10, for example, I think the \u201cship\u201d class is not conceptual close to any other classes.\n\n(4)\tAnother contribution of this paper, claimed by authors, is Adaptive Local Mean Teacher (ALMT), which I think is just a trivial trick of hyperparameter tuning. The difference between ALMT and conventional MT is just modifying the sliding window size. The novelty is quite low, and the performance improvement brought by ALMT over \u201cNo distillation\u201d is insignificant (less than 1%), as shown in Table 6.\n\n(5)\tThe performance of OGEN is too low and outdated. In existing prompt learning papers in CVPR\u201923 (such as [1,2]) and ICCV\u201923 (such as [3,4]), the \u201cNew\u201d accuracy in base-to-novel generalization setting is already about 75%, but OGEN can only achieve around 70%. \n\n(6)\tThe paper writing is quite poor and hard to follow. The objective function is missing in Sec 3.3.\n\n[1] CVPR 2023. MaPLe: Multi-modal Prompt Learning\n[2] CVPR 2023. LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision & Language Models\n[3] ICCV 2023. Self-regulating Prompts: Foundational Model Adaptation without Forgetting\n[4] ICCV 2023. Read-only Prompt Optimization for Vision-Language Few-shot Learning"
                },
                "questions": {
                    "value": "My main concern is the baselines selected to compare in this paper are too old (methods published in 2022). MaPle (CVPR'23) and  PromptSRC (ICCV'23) need to be discussed in this paper."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2931/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698789938676,
            "cdate": 1698789938676,
            "tmdate": 1699636236380,
            "mdate": 1699636236380,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "0Lb9Kq9AeT",
                "forum": "PKICZXVY9M",
                "replyto": "MwCINaRT0K",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2931/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2931/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer sauS"
                    },
                    "comment": {
                        "value": "Thank you for the constructive feedback and helpful suggestions for our manuscript. Regarding the comparison with state-of-the-art methods, please refer to our \u201cResponse to common concern: comparing with the SOTA\u201d.  For your other comments, below is our point-by-point response.\n\n**Move Appendix Fig. 5 to the main paper, poor paper writing, and missing objective function in Sec 3.3.**\n\nWe appreciate the reviewer for highlighting these presentation issues. We will fix them as suggested, and try to identify and rewrite sections that are dense or difficult to follow as well. If you have other suggestions on areas that could be improved to help us understand where to focus, it would be really helpful.\n\n**Class extrapolation is unreasonable when there are large conceptual gaps between base and novel classes.**\n\nWe agree the quality of class extrapolation highly depends on the semantic similarity between base and novel classes. We do observe faithfully extrapolated class examples when the semantic gap is small (Fig. 3 for example), but less faithful ones otherwise.\n\nAmong the many downstream datasets we have experimented on, most of them demonstrate shared regularities between classes (unlike CIFAR10), which gives rise to good extrapolation quality. Examples include datasets with a large number of conceptually close classes like FGVCAircraft and StanfordCars, as well as datasets with only a few but pattern-sharing classes like EuroSAT (satellite images) and DTD (texture images). The well-extrapolated OOD data on these datasets yield consistent, and sometimes large, gains on novel class performance, see Fig. 6(a).\n\nOn the other hand, there are datasets UCF101 and Caltech101 that have relatively large semantic gap between classes. Their data size is also small (13k and 9k respectively). Both of the two factors will negatively impact the class extrapolation quality. However, we still observe from Fig. 6(a) convincing gains on novel classes for UCF101 and Caltech101. The reason is twofold: 1) the abundance of extrapolated class examples alleviates overfitting on small datasets by scaling up the negative data, 2) extrapolated data from known classes, despite being noisy, can still serve as ``semi-hard'' negatives to regularize decision boundaries. One natural direction for improvement is to enrich the data used for extrapolation, e.g. by combining a living memory of diverse web data with the few training classes on downstream datasets.\n\n**Limited novelty and performance gains of ALMT.**\n\nIn response, ALMT is not the main innovation of this paper, and we view ALMT as a surprisingly simple add-on to reduce overfitting. Note behind the adaptive window size in ALMT lies a well-grounded rationale, i.e. to reduce both overfitting and underfitting during model distillation (see texts before Eq. 7). Such MT adaptation does not require extensive hyperparameter tuning to gain improvements, which is non-trivial.\n\nPerformance-wise, Tables 3&6 verify that, ALMT in its simple form, can indeed improve both base (ID) and new (OOD) class performance. Notably, the improvement on base class is larger ($ \\approx$ 1\\%), which is particularly useful in our case since our OOD feature generator may slightly degrade the base class performance when improving OOD generalization. Overall, ALMT serves as an optional booster that helps to strike a good balance between the base and new class performance. We will add such discussions in the revised paper, with ALMT's role to be toned down accordingly."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2931/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700466860190,
                "cdate": 1700466860190,
                "tmdate": 1700466860190,
                "mdate": 1700466860190,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]