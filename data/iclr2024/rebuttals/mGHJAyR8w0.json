[
    {
        "title": "Rethinking the Benefits of Steerable Features in 3D Equivariant Graph Neural Networks"
    },
    {
        "review": {
            "id": "WK7z6o6OMH",
            "forum": "mGHJAyR8w0",
            "replyto": "mGHJAyR8w0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6451/Reviewer_3HqL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6451/Reviewer_3HqL"
            ],
            "content": {
                "summary": {
                    "value": "This paper compares the expressive power of invariant and equivariant GNNs, from a theoretical perspective, backed by a couple of experiments (1 toy experiment and 2 real datasets).\nIt is an extension of what was recently done in [16] to the case of k-hop GNNs.\nThe main theoretical contribution is Lemma 1, which states that there is a unique invariant feature corresponding to any equiavariant one. It is made more concrete by theorem 2 and corrolary 1.\nThe experiments investigate the interplay between type order L (order of the Spherical Harmonics expansion, e.g. L=1 is vectors, L=0 scalars, L>1 are high order tensors), depth, and channel number, in the context of fixed feature dimension. This allows the authors to formulate the cautious claim \"preserving the feature dimension, the expressiveness of equivariant GNNs employing steerable features up to type-L may not increase as L grows.\"\n\nI update my rating to 6, marginally above acceptance threshold."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea of studying equivariant GNNs in the context of k-hop aggregation is new.\nThe formalism the authors use to formulate proofs is general and does not rely on Spherical Harmonics explictly (this is also a weakness though).\nThe idea of working at fixed budget to compare models, albeit probably not new, is to be saluted."
                },
                "weaknesses": {
                    "value": "See my questions below for details. The main Weaknesses are:\n\nLack of novelty compared to what was done in [16]. See my points 2d., 4., 7a., 8.\n\nOverall clarity: although wording is clear, some definitions are not given (as k-hop, see questions below) and the fact that theorems prove existence but do not provide an example of such function (like lambda, in some concrete example) make it hard to follow. See questions.\n\nAlso, it is not clear whether all claims are correct (or, some of them may be correct but trivial if properly re-phrased -- but I may be wrong !).\n\nI may be misunderstood in my assessment, hence my numerous questions."
                },
                "questions": {
                    "value": "1. In the definition of k-hop geometric GNNs, k is not defined (its role is not defined).\nActually, the definition of k-hop GNN is a bit overlooked (although it's rather intuitive what the meaning of k-hop is), and more importantly, it is not clear to me how these are not essentially equivalent to regular GNNs with more layers (since a single layer of a 2-hop GNN aggregates information with a similar receptive field as a 1-hop GNN with 2 layers).\nProbably authors should elaborate on that or cite appropriate references.\n\n2a. About Lemma 1, page 4. When you define the function c, why can't it simply be always the identity? One could always choose the neutral element from the group G.\nI do understand why the set {g \u2208 G | g \u00b7 c(G \u00b7 X) = X} is not empty.\nI do not understand what is non trivial about c(G \u00b7 X)\n\n2b. Also, V_0^{\u2295d} is not explicitly defined early enough. What is d ? Does it relate to the maximal rotation order L ?\n\n2c. You proove that the decomposition of Lemma 1 is unique. But, concretely, what is lambda ? Isn't it almost always simply the norm of the irrep ? Like, for a vector, its norm, or for a higher-order tensor, also its norm ? And then \\rho(g_X) is just the usual (matrix) representation of g_X.\nCan it be otherwise ? Or is it more complicated ? Am I missing the point ?\nIf in practice things are simple, in most of the usual settings, saying so is helpful to the reader.\nOR, maybe g_X simply encodes the orientation of X (say if it's a vector)?\n\n\n2d. In ref [16], page 3, it says:\n|At initial-\n|isation, we assign to each node i \u2208 V a scalar node colour\n|c i \u2208 C \u2032 and an auxiliary object g i containing the geometric\n|information associated to it\n|i.e. they already separate (factorize) the scalar (feature only) contributions from the geometric ones (vectorial one v + previous color).\nIn this respect it is not obvious how contribution (1) (your lemma 1) is new.\nFurthermore, I lack an intuitive view of what may concretely go into your scalar function lambda(X).\n\n\n3. gothic{g} appears in the r.h.s. of Eq (4) or in the last line of the previous est of equations (let me call this last line Eq. 3bis) (actually the gothic-style has been forgotten in Eq. (4), this is a typo I believe).\nHow can we identify the right term in the r.h.s. of Eq. 3bis as a lambda (invariant), when g^-1 appears in it ? I don't see why g^-1\\cdot x_ij should be invariant, on the contrary, it seems to me like an equivariant feature.\n\n\nIn Eq. 7, first line, why isn't it 2l+2 instead of 2l+1 (in the r.h.s.) ? I understood that representing O(3) required one additional component ? Could you provide an intuitive explanation?\n\n4. You write, in page 6:\n|Does any (2l + 1)-dimensional invariant feature \u03bb(X) correspond to a type-l steerable feature f (X)?\nWhich sounds like an interesting question (although ideally I'd like to know the mapping, not just know about its existence)\n|Note that this question is essentially asking whether the space of all type-l steerable features f (X) has a dimension of 2l + 1 since D l (g X ) is invertible.\nBut that sounds like a known fact: using Spherical Harmonics, it is known that  type-l steerable features  need components $h_m$ with  $m\\in[-l,+l]$ to be represented. That is, they need 2l+1 numbers (dimensions)\n\n5. The remark below corrolary 3 is a very strong, original conclusion.\nI think I understand corrolary 3 (altohough I feel like in practice lambda is the norm of the representation, and in that case it's kind of trivial..).\nIn any case I do not see how the remark  \"the expressiveness of learning steerable features is primarily characterized by the feature dimension \u2013 independent of the highest type utilized\"  follows from corrolary 3.\n\n\n6. In table I, I can do the maths for line 1 and 2, but not line 3.\n(5+3+1)*256 = 2304\n(5+3+1)*824 = 7416\nbut then,\n(1+3+5+7+9+11+13)*256 =12544 , not 7424\n\n\n7. experiments are a bit disappointing.\n7a. First, Table 2 shows a number of red (intersting) cases, but they turn out to be related to eSCN being SO(3)-eqivariant, when the task is O(3). This is not making the point of the paper, and instead is rather confusing, at first sight.\nMost importantly, it's not clear to me in which sense table 2 is different from the table 1 of ref [16] (which by the way seemed more readable and dense).\nPlease clarify this.\n\n7b. IS2RE. Table 3.\nHere I enjoyed a lot the idea of comparing models with comparable feature dimension.\nHowever, several points:\n- why not report also the total number of parameters ? They do not grow as fast as feature dimension since weights are shared for a given L (type order)\n- although I understand it's somewhat hardware dependent, please also report the training time for these models. Maybe even the memory footprint (on GPU).\n- Figure 3 gives an idea of how things perform over a reasonable number of epochs (and I salute the attempt to not go to an exceedingly large number of epochs), but it seems that a more thorough study of variations with L and c, reporting only the epoch=12 validation performances, would be useful to the reader (I did not have time to look at the content of ref [5])\n\n7c. S2EF. Table 4 is commented, but no conclusion stands out. What is the point of that table ?\n\n\n8. Conclusion. Two key findings are claimed:\n| (1) Propagating steerable features of type \u2265 1\n| equips geometric GNNs with the inherent ability to automatically capture the geometry between local\n| structures. (2) When the feature dimension remains constant, increasing the use of steerable features\n| up to type-L cannot essentially improve the performance of equivariant GNNs\nBut I think that  (1) is vague and already known. For instance from (16), which clearly distinguishes invariant (L=0) from equivariant L>=1\nnetworks.\nAnd that (2) is known as oversquashing and was known before.\nI note that the term oversquashing is absent from the manuscript, although it is mentionned several times in [16].\nI think the term should appear. If it sounds too buzzwordy to the authors, they should mention it at least once, and explain why they don't like it."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6451/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6451/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6451/Reviewer_3HqL"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6451/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698224306477,
            "cdate": 1698224306477,
            "tmdate": 1700686307680,
            "mdate": 1700686307680,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Dz2uDjSoru",
                "forum": "mGHJAyR8w0",
                "replyto": "WK7z6o6OMH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3HqL (1/6)"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review and valuable feedback. We fear the reviewer has some misunderstandings of our work which we have worked to clarify in this rebuttal and in our revision.\n\nRegarding your concern about the novelty, we would like to clarify there are some misunderstandings. In particular, our results cover $k$-hop geometric GNN, such as ComENet, which is mentioned in the limitation section in Joshi et al.\u2019s work. Moreover, we investigate whether, when the feature dimension remains constant, the performance of equivariant GNNs employing steerable features up to type-$L$ increases as $L$ grows.\n\nIn what follows, we provide point-by-point responses to your comments.\n\n---\n\n**Q1. In the definition of $k$-hop geometric GNNs, $k$ is not defined (its role is not defined). Actually, the definition of $k$-hop GNN is a bit overlooked (although it's rather intuitive what the meaning of $k$-hop is), and more importantly, it is not clear to me how these are not essentially equivalent to regular GNNs with more layers (since a single layer of a 2-hop GNN aggregates information with a similar receptive field as a 1-hop GNN with 2 layers). Probably authors should elaborate on that or cite appropriate references.**\n\n**Reply:** We present the architecture of $k$-hop geometric GNNs in Eq (1), where $k$ denotes the size of neighborhoods from which a node aggregates information -- or equivalently, the maximum distance over which it aggregates information in a single message-passing step. Additionally, we reference ComENet as an instance of 2-hop invariant GNNs in Section 1.1.\n\nMoreover, it is crucial to note that $k$-hop geometric GNNs are not necessarily equivalent to regular 1-hop geometric GNNs with $k$ layers. As proved in Appendix D of the revised version, under some assumptions on UPD and AGG, k-hop invariant GNNs can distinguish any two $k$-hop distinct geometric graphs but fail to distinguish any two $k$-hop identical geometric graphs. ***This indicates that 1-hop invariant GNNs might be less powerful than their 2-hop counterparts, and this trend continues.***\n\nThe reason lies in the fact that the value of $k$ influences the size of local neighborhoods, consequently affecting the input of the aggregation process. Utilizing a smaller $k$ may cause $k$-hop invariant GNNs to overlook changes in geometry between small local neighborhoods, whereas using a larger $k$ can capture these variations.\n\nHowever, it's noteworthy that, for any finite $k$, we can always construct two $k$-hop identical but $(k+1)$-hop distinct geometric graphs. In this scenario, $k$-hop invariant GNNs cannot differentiate them even with thousands of layers, while 1-hop equivariant GNNs, due to the faithfulness of features, can achieve this with $k+1$ layers. This finding sheds light on the advantages of learning steerable features up to type-$L\\geq1$. We have substantiated this point through empirical results for ComENet. In the revision, we have bolstered these observations by considering the LEFTNet model in Appendix F.1 Table 5 of the revision.\n\nTo clarify this implication, we have refined the concluding remarks. In particular, we have added the following remark: relying solely on propagating invariant features confined to specific k-hop neighborhoods is insufficient for enabling geometric GNNs to capture the global information of geometric graphs precisely.\n\n\n---\n\n**Q2. About Lemma 1, page 4. When you define the function $c$, why can't it simply be always the identity? One could always choose the neutral element from the group $G$. I do understand why the set {$g\\in G | g\\cdot c(G\\cdot {\\bf X})={\\bf X}$} is not empty. I do not understand what is non trivial about $c(G\\cdot X)$**\n\n**Reply:** The function $c$ is defined on the quotient space $\\mathbb{R}^{3\\times m}/G$ and maps to $\\mathbb{R}^{3\\times m}$. Therefore, it cannot be a trivial identity, which is a function mapping from a space to itself. The composition that sends ${\\bf X}$ to $c(G \\cdot {\\bf X})$ is also not an identity because, for any two ${\\bf X}$ and ${\\bf X}'$ lying in the same orbit, we have $c(G\\cdot{\\bf X}) = c(G\\cdot{\\bf X}')$.\n\n---\n\n**Q3. Also, $V_0^{\\oplus d}$ is not explicitly defined early enough. What is $d$? Does it relate to the maximal rotation order $L$?**\n\n**Reply:** We appreciate your feedback. However, we have introduced $d$ in the first sentence of the statement of Lemma 1, where we mention, \"Let $V$ be a $d$-dimensional $G$-steerable vector space\u2026\". The parameter $d$ represents the dimension of the $G$-steerable vector space $V$ and is not directly related to the maximal rotation order $L$.\n\nRegarding $V_0$, we acknowledge that we overlooked mentioning in Lemma 1 that we use $V_0$ to denote the trivial 1-dimensional representation of an arbitrary group $G$. We have made a revision to address this."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700177226262,
                "cdate": 1700177226262,
                "tmdate": 1700684310064,
                "mdate": 1700684310064,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "VCKtT0OamK",
                "forum": "mGHJAyR8w0",
                "replyto": "WK7z6o6OMH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3HqL (2/6)"
                    },
                    "comment": {
                        "value": "---\n\n**Q4. You prove that the decomposition of Lemma 1 is unique. But, concretely, what is lambda? Isn't it almost always simply the norm of the irrep? Like, for a vector, its norm, or for a higher-order tensor, also its norm? And then $\\rho(g_X)$ is just the usual (matrix) representation of $g_X$. Can it be otherwise? Or is it more complicated? Am I missing the point? If in practice things are simple, in most of the usual settings, saying so is helpful to the reader. OR, maybe $g_X$ simply encodes the orientation of $X$ (say if it's a vector)?**\n\n**Reply:** There seem to be a few points of misunderstanding. \n\nFirstly, the output of the G-invariant function $\\lambda$ is a vector of invariant scalars, while the norm is just a scalar. They cannot be the same, especially when the dimension $d$ is greater than $1$. Concretely, $\\lambda({\\bf X}) = f(c(G \\cdot{\\bf X}))$ is equal to evaluating the equivariant function $f$ at the representative $c(G \\cdot{\\bf X})$. Further details can be found in the proof of Lemma 1 (refer to Appendix C in the revised version).\n\nRegarding $\\rho$, it represents the group representation on the steerable vector space $V$. On the other hand, $g_{{\\bf X}}$ is a group element that satisfies $g_{{\\bf X}}\\cdot c(G \\cdot {\\bf X})={\\bf X}$. It is true that $g_{{\\bf X}}$ encodes the orientation of ${\\bf X}$ in the sense that applying rotations or reflections to ${\\bf X}$ may lead to a change in $g_{{\\bf X}}$ while $c(G\\cdot{\\bf X})$ remains unchanged. We have provided additional clarification on this point in Remark 1.\n\nIt is worth mentioning that $g_{\\bf X}$ and $\\lambda({\\bf X})$ share similarities with local frames and the scalarization proposed in [1], respectively. To provide a more comprehensive understanding, we have included an additional section on related works to discuss the similarities and distinctions between them. Please refer to Appendix A in the revised paper for details.\n\n[1] Du et al. SE(3) Equivariant Graph Neural Networks with Complete Local Frames, ICML 2022.\n\n\n---\n\n**Q5. In ref [16], page 3, it says: At initialisation, we assign to each node $i\\in V$ a scalar node color $c_i\\in C\u2019$ and an auxiliary object $g_i$ containing the geometric information associated with it i.e. they already separate (factorize) the scalar (feature only) contributions from the geometric ones (vectorial one v + previous color). In this respect, it is not obvious how contribution (1) (your lemma 1) is new. Furthermore, I lack an intuitive view of what may concretely go into your scalar function lambda(X).**\n\n**Reply:** We believe this is a misunderstanding. In the initialization of the GWL test in [16], the scalar node color only considers the node features. As iterations progress, the color $c_i^{(t)}$ and the auxiliary object $g_i^{(t)}$ incorporate not only the node features but also the geometric information and the information about the graph structure. In contrast, Lemma 1 exclusively focuses on the coordinates (geometric information) and remains independent of node features and the graph structure. In other words, both  $\\rho(g_{{\\bf X}})$ and $c(G \\cdot {\\bf X})$ are unaffected by node features and the graph structure. \n\nThe separating technique used in Lemma 1 separates ${\\bf X}$ into the representative $c(G \\cdot{\\bf X})$ and the associated group element $g_{{\\bf X}}$. Using this technique, we can effectively separate any steerable feature $f({\\bf X})$ into its equivariant part $\\rho(g_{{\\bf X}})$ and its invariant part $\\lambda({\\bf X})$. It provides us with a framework to investigate steerable features by examining their corresponding invariant features. While we acknowledge that this separating technique might share similarities with the one you mentioned, it is important to emphasize that our framework is more flexible -- it is not limited to GNNs but can be applied to other neural networks or algorithms.\n\nConcretely, $\\lambda({\\bf X}) = f(c(G \\cdot{\\bf X}))$ is equal to evaluating the equivariant function $f$ at the representative $c(G \\cdot{\\bf X})$. Further details can be found in the proof of Lemma 1 (refer to Appendix C in the revised version)."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700177317813,
                "cdate": 1700177317813,
                "tmdate": 1700180803988,
                "mdate": 1700180803988,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ch8NjbM0vI",
                "forum": "mGHJAyR8w0",
                "replyto": "WK7z6o6OMH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3HqL (3/6)"
                    },
                    "comment": {
                        "value": "---\n\n**Q6. gothic{g} appears in the r.h.s. of Eq (4) or in the last line of the previous est of equations (let me call this last line Eq. 3b is) (actually the gothic-style has been forgotten in Eq. (4), this is a typo I believe). How can we identify the right term in the r.h.s. of Eq. 3b is as a lambda (invariant), when $g^-1$ appears in it? I don't see why $g^{-1}\\cdot x_{ij}$ should be invariant, on the contrary, it seems to me like an equivariant feature.**\n\n**Reply:** Thank you for pointing out the typo. Notice that $g$ is a group element satisfying $g\\cdot c(G \\cdot{\\bf X})={\\bf X}$. In other words, we have $ g^{-1}\\cdot{\\bf X}=c(G\\cdot {\\bf X})$. It is crucial to note that $c(G \\cdot{\\bf X})$ is invariant as it only depends on the orbit. The term $g^{-1}\\cdot x_{ij}$ is a linear combination of the columns in $g^{-1}\\cdot{\\bf X}= c(G\\cdot{\\bf X})$. From this, we deduce its invariance.\n\n---\n\n**Q7. In Eq. 7, first line, why isn't it $2l+2$ instead of $2l+1$ (in the r.h.s.)? I understood that representing O(3) required one additional component? Could you provide an intuitive explanation?**\n\n**Reply:** The construction of O(3) representations has been thoroughly documented in existing literature, and we have provided a concise review in Section 2. Notably, the irreducible representations of O(3) arise from the product of the irreducible representations of $SO(3)$ and those of ${\\pm {\\bf I}}$ (rather than a direct sum). Hence, the dimension remains $2l+1$.\n\n---\n\n**Q8. You write, in page 6: Does any $(2l+1)$-dimensional invariant feature $\\lambda({\\bf X})$ correspond to a type-l steerable feature $f({\\bf X})$? Which sounds like an interesting question (although ideally I'd like to know the mapping, not just know about its existence). Note that this question is essentially asking whether the space of all type-l steerable features $f({\\bf X})$ has a dimension of $2l+1$ since $D^l (gX)$ is invertible. But that sounds like a known fact: using Spherical Harmonics, it is known that type-l steerable features need components to be represented. That is, they need $2l+1$ numbers (dimensions)**\n\n**Reply:** We believe the reviewer has a misunderstanding regarding the definition of steerable features in our context. It is true that SO(3)-steerable vector spaces are spanned by Spherical Harmonics, and thus, a type-$l$ steerable vector space $V_l$ has a known dimension of $2l+1$. However, the space of steerable features is a subspace of $V_l$, consisting of type-$l$ steerable vectors that are generated by some equivariant function from the input ${\\bf X}$. More formally, it can be expressed as {$ f({\\bf X}) \\mid \\text{ G-equivariant }f:\\mathbb{R}^{3\\times m}\\to V_l $}. In Fig 4 of the revised version, we provide an illustrative example to emphasize that the space of type-$1$ features can be 1-dimensional rather than 3-dimensional, showing that they might not be equal.\n\nThe map sending $f({\\bf X})$ to $\\lambda({\\bf X})$ is defined by the composition $\\lambda({\\bf X}):=f(c(G \\cdot{\\bf X}))$. The function $c$ is highly related to the notion of alignment in reference [1] listed below. We have included relevant discussions in Appendix A of the revised version.\n\n[1] Winter, R. et al. Unsupervised learning of group invariant and equivariant representations. NeurIPS 2022."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700177457224,
                "cdate": 1700177457224,
                "tmdate": 1700685134310,
                "mdate": 1700685134310,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6MSpub9CV3",
                "forum": "mGHJAyR8w0",
                "replyto": "WK7z6o6OMH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3HqL (4/6)"
                    },
                    "comment": {
                        "value": "**Q9. The remark below corollary 3 is a very strong, original conclusion. I think I understand Corollary 3 (although I feel like in practice lambda is the norm of the representation, and in that case it's kind of trivial..). In any case I do not see how the remark \"the expressiveness of learning steerable features is primarily characterized by the feature dimension -- independent of the highest type utilized\" follows from corollary 3.**\n\n**Reply:** We have clarified the non-triviality of $\\lambda$ in the above responses. Here we would like to address your concern about the implication of Corollary 3. First, we would like to acknowledge that there are two main aspects related to the concept of expressiveness: the capacity of features to carry information and the model's ability to extract that information -- the latter is commonly referred to as universality.\n\nOur discussion on the utilization of different types of steerable features focuses on the former, given that the latter is subject to the model\u2019s architecture. Therefore, asserting that \u201cthe expressiveness of learning steerable features is primarily characterized by the feature dimension -- independent of the highest type utilized\u201d might lack precision without considering the model's universality. We have refined the assertion to state that: \u201cthe invariant features carried by steerable features are primarily characterized by the feature dimension -- independent of the highest type utilized.\u201d\n\nAdditionally, we have rephrased the relevant sections and included a new section in Appendix B of the revised version to delve into universality and potential methods. We believe these revisions address your concerns and provide more precise implications of our results.\n\n---\n\n**Q10. In table I, I can do the maths for line 1 and 2, but not line 3. (5+3+1)\\*256 = 2304 (5+3+1)\\*824 = 7416 but then, (1+3+5+7+9+11+13)\\*256 =12544, not 7424**\n\n**Reply:** Two hyperparameters are used to control the degree $l$ and order $m$ of the spherical harmonics when generating steerable features. The degree $l$ is directly determined by the steerable feature type $L$. However, the maximum absolute order $m$ is set to $|m|\\leq 2$ by default [1] in both eSCN and EquiformerV2. The order $m$ of the spherical harmonics can range from $-l$ to $l$, and when $|m|\\leq l$ it serves to restrict the feature space. When $|m|\\leq 2$ the equation becomes (1+3+5+5+5+5+5)*256 =7424.\n\n[1] https://github.com/Open-Catalyst-Project/ocp/tree/main\n\n---\n\n**Q11. experiments are a bit disappointing. 7a. First, Table 2 shows a number of red (interesting) cases, but they turn out to be related to eSCN being SO(3)-equivariant, when the task is O(3). This is not making the point of the paper, and instead is rather confusing, at first sight. Most importantly, it's not clear to me in which sense table 2 is different from table 1 of ref [16] (which by the way seemed more readable and dense). Please clarify this.**\n\n**Reply:** The inclusion of eSCN in Table 2 is used to provide a robust experiment section for all models considered. However, to avoid confusion with the limitations of eSCN, namely the O(3)-equivariance and the quasi-equivarianc, we have included additional discussion in Section 5 and Appendix F.1.\n\nTable 1 of ref [16] considers only chain lengths of $k=4$. This makes it difficult to discern the impact of incorporating $k$-hop information. This is not a concern for the authors of [16] because they do not provide a theoretical understanding of models like ComENet and LEFTNet. Table 2 and Table 5 incorporate a more robust analysis of different models. To improve the clarity of the paper, we have included a more detailed description of the figure in the caption."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700177578080,
                "cdate": 1700177578080,
                "tmdate": 1700685821838,
                "mdate": 1700685821838,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lgCl5iijca",
                "forum": "mGHJAyR8w0",
                "replyto": "WK7z6o6OMH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3HqL (5/6)"
                    },
                    "comment": {
                        "value": "**Q12. IS2RE. Table 3. Here I enjoyed a lot the idea of comparing models with comparable feature dimension. However, several points: 1) why not report also the total number of parameters? They do not grow as fast as feature dimension since weights are shared for a given $L$ (type order). 2) although I understand it's somewhat hardware dependent, please also report the training time for these models. Maybe even the memory footprint (on GPU). 3) Figure 3 gives an idea of how things perform over a reasonable number of epochs (and I salute the attempt to not go to an exceedingly large number of epochs), but it seems that a more thorough study of variations with $L$ and $c$, reporting only the epoch=12 validation performances, would be useful to the reader (I did not have time to look at the content of ref [5])**\n\n**Reply:** \n\nThank you for the suggestion. Details for # Param, Run Time, Memory, and validation results for $L=1,3,5$ have been incorporated into Appendix F.2 in Table 6 and Table 7. Below is a reproduction of these tables for the reviewer's convenience.\n\n| Model     \t| L |   c | Feat. Dim. | # Param. | Loss                 \t| Energy MAE [meV]      \t| EwT [%]            \t|\n|---------------|---|-----|------------|----------|--------------------------|---------------------------|------------------------|\n| eSCN      \t| 1 | 464 | 1856   \t| 11M  \t| 0.380 \u00b1 0.006        \t| 865 \u00b1 14              \t| 1.91 \u00b1 0.09        \t|\n| eSCN      \t| 2 | 206 | 1854   \t| 10M  \t| 0.369 \u00b1 0.006        \t| 842 \u00b1 13              \t| 1.94 \u00b1 0.12        \t|\n| eSCN      \t| 3 | 133 | 1862   \t| 9M   \t| 0.397 \u00b1 0.001        \t| 904 \u00b1 3               \t| 1.85 \u00b1 .12         \t|\n| eSCN      \t| 4 | 98  | 1862   \t| 9M   \t| 0.408 \u00b1 0.006        \t| 929 \u00b1 15              \t| 1.74 \u00b1 0.12        \t|\n| eSCN      \t| 5 | 77  | 1848   \t| 8M   \t| 0.409 \u00b1 0.003        \t| 933 \u00b1 7               \t| 1.61 \u00b1 .12         \t|\n| eSCN      \t| 6 | 64  | 1856   \t| 8M   \t| 0.3836 \u00b1 0.003       \t| 872 \u00b1 6               \t| 1.91 \u00b1 0.19        \t|\n| EquiformerV2  | 1 | 77  | 304    \t| 7M  \t| OOM                  \t| OOM                   \t| OOM                \t|\n| EquiformerV2  | 2 | 34  | 306    \t| 9M   \t| 0.369 \u00b1 0.009        \t| 841 \u00b1 21              \t| 2.02 \u00b1 0.14        \t|\n| EquiformerV2  | 3 | 22  | 306    \t| 12M  \t| 0.363 \u00b1 0.009        \t| 828 \u00b1 21              \t| 1.94 \u00b1 0.08        \t|\n| EquiformerV2  | 4 | 16  | 304    \t| 15M  \t| 0.364 \u00b1 0.005        \t| 832 \u00b1 11              \t| 2.03 \u00b1 0.14        \t|\n\n\n| Model     \t| L |   c | Run Time (min)   \t| Memory  |\n|---------------|---|-----|----------------------|---------|\n| eSCN      \t| 1 | 464 | 151 \u00b1 1          \t| 12.3GB  |\n| eSCN      \t| 2 | 206 | 297 \u00b1 5          \t| 9.1GB   |\n| eSCN      \t| 3 | 133 | 207 \u00b1 3          \t| 9.0GB   |\n| eSCN      \t| 4 |  98 | 347 \u00b1 4          \t| 9.2GB   |\n| eSCN      \t| 5 |  77 | 246 \u00b1 3          \t| 10.9GB  |\n| eSCN      \t| 6 |  64 | 429 \u00b1 8          \t| 11.5GB  |\n| EquiformerV2  | 1 |  77 | OOM              \t| 18.6GB  |\n| EquiformerV2  | 2 |  34 | 284 \u00b1 6          \t| 12.6GB  |\n| EquiformerV2  | 3 |  22 | 256 \u00b1 1          \t| 11.1GB  |\n| EquiformerV2  | 4 |  16 | 298 \u00b1 2          \t| 11.5GB  |\n\n\nNote that for EquiformerV2 L=1, the model will fit onto the GPU.  However, during training the data and the model will exceed the GPU memory capabilities. We denote the out of memory phenomenon OOM in the tables above and report the model parameters and size.\n\n---\n\n**Q13. S2EF. Table 4 is commented, but no conclusion stands out. What is the point of that table ?**\n\n**Reply:** Table 4 is used to support our theory that no model stands out under a suitable ablation study on $L$ and $c$. This is directly motivated by the analysis in Table 1. Both eSCN[L=6,c=256] and eSCN[L=2,c=824] achieve strong results in comparison to eSCN[L=2,c=256]. However, the fact that eSCN[L=6,c=256] and eSCN[L=2,c=824] themselves are indistinguishable provides empirical support for our theory. In particular, there is no significant gain by increasing $L$ when $c$ is proportionately increased.\n\n---"
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700177693095,
                "cdate": 1700177693095,
                "tmdate": 1700683981672,
                "mdate": 1700683981672,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8mVBNcr0kR",
                "forum": "mGHJAyR8w0",
                "replyto": "WK7z6o6OMH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 3HqL (6/6)"
                    },
                    "comment": {
                        "value": "**Q14. Conclusion. Two key findings are claimed: (1) Propagating steerable features of type $\\geq 1$ equips geometric GNNs with the inherent ability to automatically capture the geometry between local structures. (2) When the feature dimension remains constant, increasing the use of steerable features up to type-L cannot essentially improve the performance of equivariant GNNs But I think that (1) is vague and already known. For instance from (16), which clearly distinguishes invariant ($L=0$) from equivariant $L\\geq 1$ networks. And that (2) is known as oversquashing and was known before. I note that the term oversquashing is absent from the manuscript, although it is mentioned several times in [16]. I think the term should appear.**\n\n**Reply:** We respectfully disagree with these comments and fear that there are substantial misunderstandings of our contribution. Please allow us to clarify our contribution below.\n\nConcern about (1), we would like to clarify our results not only ***cover cases like ComENet -- as outlined in the limitation section of [16]*** but also emphasize ***the significance of faithfulness in effectively capturing geometry between local neighborhoods during message passing***. It's noteworthy that [16] suggests that incorporating pre-computing non-local features, as in ComENet, may offer a straightforward approach to overcoming the limitations of invariant GNNs. However, our results, both theoretical and empirical, indicate that this approach may not be as effective as suggested. Please refer to Theorem 1 and Table 2. Moreover, [16] asserts that their setup can be extended to higher-order tensors. However, we found that similar results for this extension might not hold if the geometric objects, which correspond to the steerable features, do not lie on faithful representations. In particular, our framework highlights that significant geometric information between local neighborhoods could be lost during message passing if the representations lack faithfulness. Please refer to Theorem 1 and Appendix B in the revised paper for details.\n\nConcern about (2), we did not include a discussion of over-squashing since it is not the problem we are studying and unrelated to our contributions. [16] does mention the over-squashing phenomenon for equivariant features in geometric GNNs, using the example of EGNN, which utilizes a single vector feature to aggregate and propagate geometric information. Specifically, these models are constrained to learn growing information within a fixed-length vector, potentially failing to propagate long-range information and learning only short-range signals from the training data [1].\n\nLet us clarify the empirical evidence supporting the two contributing components of our work. Table 2 presents empirical evidence for **first** part of our theory involving $k$-hop models like ComENet \u2013 this was previously unsupported in [16]. In Table 2, we observe the over-squashing in some models as in [16] and **we have included a discussion on over-squashing in Appendix F.2.**\n\nThe ablation studies in Figure 2, Table 3, and Table 4 support the **second** part of our contribution which is aimed at answering: **When the feature dimension remains constant, does the performance of equivariant GNNs employing steerable features up to type-$L$ increase as $L$ grows?**\n\nWe study the information-carrying ability of different types of steerable features by investigating their corresponding invariant features. In Table 3 and Table 4 we fix the steerable feature dimension while varying the steerable feature type $L$. We select the steerable feature dimension based on the best-performing model (see Table 1). We find that the performance may not necessarily increase when increasing $L$. We do not investigate the phenomenon of over-squashing in this ablation study and we do not observe any effects of over-squashing in our results. Moreover, our theory on the information-carrying ability of steerable features is not limited to GNNs.\n\nWe believe that Section 5.2 in [16] is more relevant to the problem we study. As discussed in Section 4, [16] has conducted a comparison of the utilization of different types of steerable features. However, that comparison is not explicitly designed for invariant classification.\n\nIn summary, our paper addresses the limitation mentioned in [16] and is the first research highlighting the significance of faithful representations in capturing geometry between local neighborhoods during message passing. Moreover, we provide theoretical reasons supporting the idea that when the feature dimension remains constant, the performance of equivariant GNNs employing steerable features up to type-$L$ may not increase as $L$ grows.\n\n[1] Alon, U., and Yahav, E. On the bottleneck of graph neural networks and its practical implications. ICLR 2021.\n\n[16] Joshi, C. K., Bodnar, C., Mathis, S. V., Cohen, T., and Lio, P. On the expressive power of geometric graph neural networks. ICML 2023."
                    }
                },
                "number": 15,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700177773070,
                "cdate": 1700177773070,
                "tmdate": 1700186216951,
                "mdate": 1700186216951,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "kVN0q11TAB",
                "forum": "mGHJAyR8w0",
                "replyto": "8mVBNcr0kR",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6451/Reviewer_3HqL"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6451/Reviewer_3HqL"
                ],
                "content": {
                    "title": {
                        "value": "Brief answer"
                    },
                    "comment": {
                        "value": "Thank you for your thorough answers. It seems indeed that some of the theoretical subtleties of your work have failed to reach me. I may have read too fast, but this also indicates a relative difficulty to read your messages, for someone who has been working with equivariant networks for over a year.\nProbably the writing could be further improved.\n\nI do not have the time to assess each of your answers individually. However, about over-squashing, I don't understand your answer. Your key empirical finding, that is that increasing L at fixed feature dimension (i.e. reducing the number of channels appropriately) does not lead to increased performance. But isn't that related to a form of oversquashing of the geometrical information ? Meaning, if there are too high order L to be carried, and not enough channels to carry them, then the benefit of these additional L is lost (or even deteriorates performance) ? \n\nI will further review your answers later today and most likely increase my rating."
                    }
                },
                "number": 28,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700660246033,
                "cdate": 1700660246033,
                "tmdate": 1700660246033,
                "mdate": 1700660246033,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "xpXjrzPI0G",
            "forum": "mGHJAyR8w0",
            "replyto": "mGHJAyR8w0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6451/Reviewer_BvXD"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6451/Reviewer_BvXD"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a theoretical analysis of the expressive power of steerable equivariant GNNs. In particular, the authors show that every type-$L$ steerable feature can be analyzed through its corresponding invariant features. The authors use this lemma to study $k$-hop invariant GNNs and show limited expressive power. Then, the authors argue that any type-$L$ steerable feature is as expressive as its dimension. Specifically, there is a one-to-one correspondence between steerable features and $d $ invariants. Hence, increasing $L$, when accounting for the increase of feature dimension, does not improve the GNN's performance. The authors test their findings on several numerical experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The writing is of high quality and clarity. Recent works have started analyzing the expressive power of steerable graph NNs, but little consensus has been reached. Hence I'd say this is a timely and significant work."
                },
                "weaknesses": {
                    "value": "See Questions for clarifications.\n\n- I think a highly related work is Villar et al. (2022) (\"Scalars are universal...\") with several related and perhaps even identical results: please include this in your related work section.\n- The notations sometimes could be clarified a bit further.\n  - For example, the logic above Lemma 1 could be laid out a bit clearer.\n  - It's not clear to me how $\\rho(g_X)$ acts on $\\lambda(X) \\in V_0^{\\oplus d}$. First off, what is the action of $\\rho(g_X)$? Further, $\\lambda(X)$ is a tuple of scalars, so are they all left-invariant? Or are you considering them as a vector in $V$? This is also how you define $f$ in Lemma 1.\n  - In general, how can an *equivariant* function be described by *invariants*? I guess I'm not following some notations here.\n- I find it slightly confusing what the exact message is that the paper is trying to convey, especially towards the end of the paper. \n  - It is claimed that one needs more than invariants to capture the global symmetries, yet Lemma 1 states that one can use invariants to represent equivariant functions (see also my previous comments).  \n  - I find it slightly confusing that according to the paper, one doesn't need more than $ L=1 $, but many experiments (Table 1, 2, 3) use $ L=2$ and higher. \n  - In the conclusion, it is claimed that $L \\geq 1$ captures local structures, but one doesn't need more than type $L$. Why don't the authors claim that $L=1$ is enough?"
                },
                "questions": {
                    "value": "- Can you contrast your results with Villar et al. (2022)? How do your results differ/improve upon theirs?\nLet's take a basis vector $e_1 \\in \\mathbb{R}^{3 \\times 1}$ and $f: \\mathbb{R}^{3 \\times 1} \\to \\mathbb{R}^3$ with $e_1 \\mapsto \\alpha_1 e_1, \\alpha \\in \\mathbb{R}$. This is clearly an equivariant function. What would the corresponding invariant function be such that Lemma 1 holds?\n- Let's take three basis vectors $e_1, e_2, e_3 \\in \\mathbb{R}^{3\\times3}$ and $f: \\mathbb{R}^{3 \\times 3} \\to \\mathbb{R}^3$ with $e_1, e_2, e_3 \\mapsto \\alpha_1 e_1 + \\alpha_2 e_2 + \\alpha_3 e_3, \\alpha \\in \\mathbb{R}$. \n  - Isn't the set stabilizer now only the trivial rotation?\n  - What would, in this case, be the corresponding unique $\\lambda$?\n- Why did you use $L=2$ and higher in your experiments in Tables 1, 2, and 3?\n- Could you elaborate on my last comments in the previous (weaknesses) section?\n- Do you know if your results were somehow already known in (geometric) invariant theory?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6451/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6451/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6451/Reviewer_BvXD"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6451/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698517743137,
            "cdate": 1698517743137,
            "tmdate": 1700642410449,
            "mdate": 1700642410449,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Ci12aPHxDL",
                "forum": "mGHJAyR8w0",
                "replyto": "xpXjrzPI0G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BvXD (1/3)"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review and valuable feedback. In what follows, we provide point-by-point responses to your comments.\n\n---\n\n**Q1. Can you contrast your results with Villar et al. (2022)? How do your results differ/improve upon theirs? Let's take a basis vector $e_1 \\in \\mathbb{R}^{3 \\times 1} $ and $f: \\mathbb{R}^{3 \\times 1} \\rightarrow \\mathbb{R}^3$ with $e_1 \\mapsto \\alpha e_1, \\alpha \\in \\mathbb{R}$. This is clearly an equivariant function. What would the corresponding invariant function be such that Lemma 1 holds?**\n\n**Reply:** We have integrated the work by Villar et al. (2022) into our related work, including a discussion in Appendix A of the revision on the distinctions between their research and ours. Here, we provide a brief discussion.\n\nOur results exhibit clear distinctions from theirs, mainly due to our consideration of steerable features of higher types $l>1$ and the different approaches we have taken. Instead of investigating the spanning space, we leverage the concept of \"representatives\" of orbits and apply the equivariance of the function to derive Lemma 1. \n\nOur form also differs from Villar et al. (2022) in terms of the number of basis vectors and the uniqueness of invariants. In our form, the number of basis vectors aligns with the dimension of (steerable) features, rather than the number of input coordinates. Taking your example into account, where the function $f$ sends $e_1$ to $\\alpha_1 e_1$. In the form provided in Villar et al. (2022), $f$ would be expressed as a linear combination of $e_1$, resulting in one basis vector. However, in our form, the number of basis vectors corresponds to the number of column vectors in $\\rho(g_{{\\bf X}})$ (which is three in this case). Moreover, $\\lambda({\\bf X})$ is directly determined by $\\rho(g_{{\\bf X}})^{-1}f({\\bf X})$, using the invertibility of $\\rho(g_{{\\bf X}})$. This not only ensures the uniqueness of $\\lambda$ but also highlights a key distinction. In Villar et al. (2022)\u2019s framework, the invariants may not be unique, especially if the number of input coordinates exceeds the rank of their spanning space.\n\nLet\u2019s consider the example you provided, where the function $f$ sends $e_1$ to $\\alpha_1 e_1$. In this case, the input embedding contains only one vector, namely $e_1$. There exists a rotation that transforms $e_1$ to the vector $(||e_1||, 0, 0)$, which lies on the $x$-axis. Here, we choose the representative $c(G\\cdot{\\bf X})$ to be the vector $(||e_1||, 0, 0)$ and the associated group element $g_{{\\bf X}}$ to be the transformation that rotates $(||e_1||, 0, 0)$ to $e_1$. Then, $\\lambda({\\bf X}) = f(c(G\\cdot{\\bf X})) = f((||e_1||, 0, 0)) =(\\alpha_1 ||e_1||, 0, 0)$ is equal to evaluating $f$ at the representative.\n\n\nAdditionally, we have established different correspondences between equivariant functions and invariant functions. Our results also involve a detailed investigation into the message-passing mechanisms of equivariant GNNs and invariant GNNs. We believe our results contribute to a more comprehensive understanding of the benefits offered by learning steerable features in geometric GNNs.\n\n---\n\n**Q2. Let's take three basis vectors and with $e_1,e_2,e_3\\in\\mathbb{R}^{3 \\times 1}$ and $f: \\mathbb{R}^{3 \\times 3} \\rightarrow\\mathbb{R}^3$ with $e_1,e_2,e_3\\mapsto \\alpha e_1 + \\alpha e_2 + \\alpha e_3, \\alpha \\in \\mathbb{R}.$ 1) Isn't the set stabilizer now only the trivial rotation? 2) What would, in this case, be the corresponding unique?**\n\n**Reply:** The stabilizer is defined based on a given input embedding ${\\bf X}$. Additionally, the size of stabilizers is contingent on the rank of ${\\bf X}$, as detailed in the proofs of Theorems 2 and 3. Notably, when the rank of ${\\bf X}$ is one, the stabilizer is formed by rotations around the line spanned by the column vectors in ${\\bf X}$.\n\nAs mentioned earlier, the corresponding $\\lambda({\\bf X})$ is equal to $\\rho(g_{{\\bf X}})^{-1}f({\\bf X})$, while we define $\\lambda({\\bf X})$ to be $f(c(G\\cdot{\\bf X}))$  in the proof of Lemma 1. Therefore, the explicit value of $\\lambda({\\bf X})$ may vary depending on the choice of the representative $c(G\\cdot{\\bf X})$ and the group element $g_{\\bf X}$. It is challenging to explicitly work through your example due to the difficulty in explicitly constructing $c(G\\cdot{\\bf X})$ or $g_{{\\bf X}}$ for any ${\\bf X}$. In particular, this construction is highly related to the notion of alignment in reference [1] listed below, where they apply an autoencoder to learn $c(G\\cdot{\\bf X})$. (We also added a few comments to discuss this in Appendix B of the revision)\n\nNevertheless, we have performed computations for the associated group element $c(G\\cdot{\\bf X})$ and the corresponding invariant function $\\lambda({\\bf X})$ using another example you provided in Q1. We hope this addresses your concerns.\n\n[1] Winter, R. et al. Unsupervised learning of group invariant and equivariant representations. NeurIPS 2022."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700176864013,
                "cdate": 1700176864013,
                "tmdate": 1700182442352,
                "mdate": 1700182442352,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "DpRr6qWwEW",
                "forum": "mGHJAyR8w0",
                "replyto": "xpXjrzPI0G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BvXD (2/3)"
                    },
                    "comment": {
                        "value": "---\n\n**Q3. Why did you use $L=2$ and higher in your experiments in Tables 1, 2, and 3?**\n\n**Reply:** Table 1 rows 1&3 report the baseline hyperparameters for the eSCN model. Table 1 row 2 illustrates what we propose as a suitable comparison. In particular, we fix $L=2$ to maintain the steerable feature type in the baseline, while adjusting the hidden channels $c$ in order to maintain the dimension of the steerable feature. Table 2 reports results for different models which use $L=0,1,2$. We report results for $L=2$ to demonstrate the effects of $k$-Chains when using higher-order steerable features. Table 3 reports results for $L=2,4,6$ for eSCN and $L=2,4$ for equiformerV2. This serves as an ablation study over the steerable feature dimension $L$. We have enhanced the robustness of this study in the revision which now includes results for $L=1,3,5$. \n\n---\n\n**Q4. Concern about exact message in the paper. In particular, elaborate on the last comments in the weaknesses section?**\n\n**Reply:** We would like to address each of your concerns individually.\n\nFor your first concern, we want to clarify that there is no inconsistency. While Lemma 1 allows us to study equivariant features by examining their corresponding invariant features, it is essential to consider the message-passing mechanisms, especially the local aggregation, when investigating geometric GNNs. We observe that, due to the faithfulness of representations, equivariant GNNs can effectively capture the geometry between local neighborhoods. However, hindered by trivial representations, invariant GNNs struggle in this regard significantly affecting their ability to precisely capture global geometry and obtain global invariant features from local invariant features. Basically, there are scenarios where equivariant GNNs can learn certain global invariant features that invariant GNNs cannot. \nIn Appendix B of the revision, we have added a detailed explanation of why faithfulness is important.\n\nThe response to your second concern is contained in our response to Q3.\n\nFor the last concern, we would like to acknowledge that there are two main aspects related to the concept of expressiveness: the capacity of features to carry information and the model's ability to extract that information -- the latter is commonly referred to as universality. Our discussion on the utilization of different types of steerable features focuses on the former, as the latter is subject to the given model architecture. It is not sufficient to conclude that $L=1$ is enough without considering universality. We have added a section in Appendix B of the revised version to discuss universality and potential methods. Nevertheless, our results still show that when the feature dimension remains constant, the performance of equivariant GNNs employing steerable features up to type-$L$ may not increase as $L$ grows.\n\nWe have revised the relevant sections in our paper to enhance clarity and eliminate potential misunderstandings. We believe these revisions address your concerns and provide more precise implications of our results.\n\n---\n\n**Q5. Do you know if your results were somehow already known in (geometric) invariant theory?**\n\n**Reply:** We have included an additional section in the revised version to discuss the similarities and distinctions between Lemma 1 and existing results. While we have conducted an extensive literature review, we have not identified an exact identical result in the existing literature.\n\nHowever, if there is the same result already presented in the existing literature, we would appreciate it if the reviewer could provide a specific reference. This information will be crucial for us to acknowledge and appropriately credit prior contributions."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700177042013,
                "cdate": 1700177042013,
                "tmdate": 1700177042013,
                "mdate": 1700177042013,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "K0vevDshvq",
                "forum": "mGHJAyR8w0",
                "replyto": "xpXjrzPI0G",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BvXD (3/3)"
                    },
                    "comment": {
                        "value": "**Q6. Concern about notations mentioned in weakness?**\n\n**Reply:** Thank you for your feedback. We have improved the presentation of the argument above Lemma 1. \n\nRegarding your concern about the group action in Lemma 1, we have provided further clarification in Remark 1, particularly emphasizing that the group action is absorbed by $\\rho(g_{\\bf X})$. An alternative perspective is to consider the product $\\rho(g_{\\bf X})\\lambda({\\bf X})$ as a linear combination of the column vectors of $\\rho(g_{\\bf X})$ with the coefficients given by the scalars in $\\lambda({\\bf X})$. Induced by the definition of group homomorphism $\\rho(h\\cdot g) = \\rho(h)\\cdot \\rho(g)$, the group action then acts on the column vectors of $\\rho(g_{\\bf X})$.\n\nFurthermore, the well-defined mapping in Lemma 1, which associates equivariant functions with invariant functions, allows us to use the corresponding invariant function $\\lambda({\\bf X})$ to understand an equivariant functions $f({\\bf X})$. In the proof of Lemma 1, we define $\\lambda({\\bf X})$ as $f(c(G\\cdot{\\bf X}))$, evaluating $f$ at the representative $c(G\\cdot{\\bf X})$. The key reason $\\lambda({\\bf X})$ sufficiently represents $f({\\bf X})$ is that we can use the associated group element $g_{\\bf X}$ to recover $f$ from $\\lambda({\\bf X})$ by computing $\\rho(g_{\\bf X})\\lambda({\\bf X})$. It is essential to note that $g_{\\bf X}$ is pre-selected and is independent of $f$. In fact, $g_{\\bf X}$ shares similarities with local frames in reference [1] listed below. To provide a more comprehensive understanding, we have included an additional section on related works to discuss the similarities and distinctions between $g_{\\bf X}$ and local frames. Please refer to Appendix A in the revised paper for details.\n\n\n[1] Du et al. SE(3) Equivariant Graph Neural Networks with Complete Local Frames, ICML 2022.\n\n---\n\nWe have updated our submission based on the reviewer's feedback, with the revision highlighted in blue. We are happy to address further questions on our paper. Thank you for considering our rebuttal."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700177095056,
                "cdate": 1700177095056,
                "tmdate": 1700182286441,
                "mdate": 1700182286441,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lNahHCWoNC",
                "forum": "mGHJAyR8w0",
                "replyto": "K0vevDshvq",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6451/Reviewer_BvXD"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6451/Reviewer_BvXD"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your detailed feedback. The authors have made considerable effort to improve the paper based on the extensive feedback. In light of these improvements, I have decided to promote my score slightly because I think it is worth publishing this paper rather than not. However, I still agree with many of the issues reviewers 3HqL and WxNP present, and encourage the authors to provide further clarifications."
                    }
                },
                "number": 23,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700642391856,
                "cdate": 1700642391856,
                "tmdate": 1700642391856,
                "mdate": 1700642391856,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ni9M9EmR4Y",
            "forum": "mGHJAyR8w0",
            "replyto": "mGHJAyR8w0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6451/Reviewer_WxNP"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6451/Reviewer_WxNP"
            ],
            "content": {
                "summary": {
                    "value": "In recent years there has been growing interest in various forms of `geometric graph neural networks', where the input are graphs whose node have attributes in R^3, and the tasks discussed are equivariant to both relabeling of the nodes and applying a global rigid motion to all node coordinates. \n\nThere is a zoo of different methods for these problems, which differ among others in the use of invariant vs equivariant features and in the type of GNN used as a backbone. This paper attempts to understand the importance  of these various different choices. It  discusses k-hop GNNs with invariant/equivariant features. Its main claims are:\n(a) Invariant features are less expressive than equivariant features.\n(b) equivariant features of order 1 are as expressive as higher order features- the main issue is the dimension of the features and not the equivariant order"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The paper has some observations I believe are novel and interesting:\n(a) an interesting correspondence between equivariant functions and invariant functions, and between equivariant functions spaces of different order. \n(b) the conjecture that high dimensional representations do not matter, once the number of features is evened out, is interesting and recieves some empirical and theoretical evidence in the paper."
                },
                "weaknesses": {
                    "value": "* Regarding (a): This correspondence has an important limitation which I believe the authors should mention:  Update and Aggregation functions which are actually useful for learning are reasonably `nice': for example, differentiable almost everywhere, continuous, etc. The correspondence suggested by lemma 1 uses a mapping of every orbit to a canonical element which indeed exists, but is not in general continuous or very well behaved. As a result an equivariant `nice' aggregation function will correspond to a `wild' invariant aggregation function.\n\n* The paper's stand on invariant vs equivariant features seems inconsistent. On the one hand the paper maintains that there is a one-to-one correspondence between invariant and equivariant features, and that \"propagation of steerable features can be effectively understood as propagation of invariant features, therefore analyzing the message passing using the corresponding invariant features is a reasonable approach (page 4)\" on the other hand once this analysis is carried out the paper maintains that it points to an advantage of equivariant over invariant.  \n\n*I have some issues with the writing, which could be addressed in a revision towards a camera ready version."
                },
                "questions": {
                    "value": "Dear authors: please relate to the first two weaknesses above- what your opinion is about them, and how you intend to address them if you agree they should be addressed. \n\nDetailed remarks about writing\n* Page 4: Steerable features: It would make more sense to me to define f to be the steerable features. What does it mean to say that a vector is a steerable feature? Say you have a vector x=[1,2,5] in R^3. Is it a steerable feature or not?\n* Page 5: \"without loss of generality we may assume the group reps are all the same\" why?\n* Remark 3 is vague and out of context. Corollary 3 seems related, is much more formal and accurate, but does not require the action to be faithful. Can you formally explain why faithfulness is important? This could be the criitical missing link explaining why in terms of the irreducible order $\\ell$ we claim to have 0<1=2=3=4=... which is the message you are trying to convey but I don't think the theory currently supports.\n* Page 6: \"This question is essentially asking whether the space of all type-ell features has a dimension of 2\\ell +1\" This is well defined once f is fixed I believe and X is free? Perhaps clarifying this will also help me understand how you define steerable features in general.\n* Page 6: Could you specify v=f(x_1,x_2) or whatever the relation between v and x_1,x_2 is?\n* Also I think a similar observation appears in [Villar, Lemma 3] I suggest you cite them\n* Table 1: what are the significance of the specific three values of (L,c) chosen in Table 1? Presumably you can run eSCN with any choice of the c-s?\n* In Table 2 the invariant equiformerV2 actually does reasonably well, seemingly contradicting your theorem?\n\nsmall comments that should not be addressed in rebuttal:\n* Page 2: the stabilizer is not only a set but also a group, perhaps you would prefer to say 'group' \n* Page 3: I didn't understand the definition of k-hop distinct. Possibly a typo somewhere there?  \n* in Lemma 1 and elsewhere: in the formula f(X)=... the output of lambda is in $V_0^{d}$ and then you apply $\\rho(g)$ and get something in $V$. I understand what you mean but technically the group action should go from V to V.\n* Page 4: \"for simplicity we represent g as g_X\" I believe here and a line or two above you accidently swapped between g and g_X\n* In theorem 2 has V_l,aug been previouisly defined? Couldn't easily find the definition.\n  [Villar et al. Scalars are universal: Equivariant machine learning, structured like classical physics. Lemma numbering from arxiv version]\n *Page 9: `our work has evoked' I would suggest different wording focusing on what the works does rather than what it caused the reader to do."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission6451/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission6451/Reviewer_WxNP",
                        "ICLR.cc/2024/Conference/Submission6451/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6451/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698663078645,
            "cdate": 1698663078645,
            "tmdate": 1700644207204,
            "mdate": 1700644207204,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aFY0yszjY7",
                "forum": "mGHJAyR8w0",
                "replyto": "ni9M9EmR4Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WxNP (1/3)"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review and valuable feedback. In what follows, we provide point-by-point responses to your comments.\n\n---\n\n**Q1. Regarding (a): This correspondence has an important limitation which I believe the authors should mention: Update and Aggregation functions which are actually useful for learning are reasonably \u2018nice\u2019: for example, differentiable almost everywhere, continuous, etc. The correspondence suggested by lemma 1 uses a mapping of every orbit to a canonical element which indeed exists, but is not in general continuous or very well behaved. As a result an equivariant \u2018nice\u2019 aggregation function will correspond to a \u2018wild\u2019 invariant aggregation function.**\n\n**Reply:** The point you raised is accurate. In the revised version, we have provided additional explanations and potential methods to address this concern. For detailed information, please refer to Appendix B.\n\n---\n\n**Q2. The paper's stand on invariant vs equivariant features seems inconsistent. On the one hand the paper maintains that there is a one-to-one correspondence between invariant and equivariant features, and that \u201cpropagation of steerable features can be effectively understood as propagation of invariant features, therefore analyzing the message passing using the corresponding invariant features is a reasonable approach (page 4)\u201d on the other hand once this analysis is carried out the paper maintains that it points to an advantage of equivariant over invariant.**\n\n**Reply:** We appreciate the reviewer's careful examination of our paper. We want to clarify that there is no inconsistency in our stance. Corollary 1 establishes one-to-one correspondences between invariant and equivariant features. However, when investigating geometric GNNs, it is essential to consider the message-passing mechanisms. We observe that, due to the faithfulness of representations, equivariant GNNs can effectively capture the geometry between local neighborhoods, while invariant GNNs -- hindered by trivial representations -- struggle in this regard. This distinction significantly affects their ability to precisely capture global geometry and derive global invariant features from local invariant features. **There are scenarios where equivariant GNNs can learn certain global invariant features that invariant GNNs cannot.** \n\nTo enhance clarity and mitigate potential misunderstandings, we have revised all relevant contexts in our paper. We believe these revisions address your concerns and provide more precise implications of our results.\n\n\n ---\n\n**Q3. Page 4: Steerable features: It would make more sense to me to define f to be the steerable features. What does it mean to say that a vector is a steerable feature? Say you have a vector $x=[1,2,5]$ in $\\mathbb{R}^3$. Is it a steerable feature or not?**\n\n\n**Reply:** Features, in the context of our discussion, refer to values obtained through a parameterized function applied to the input. That's why we refer to the steerable vector $f({\\bf X})$, generated by the equivariant function $f$ from the input ${\\bf X}$, as a steerable feature. It's important to note that not every arbitrary (steerable) vector, like $x=[1,2,5]$, qualifies as a steerable feature. There may not be an equivariant function such that $f({\\bf X}) = [1,2,5]$ for some ${\\bf X}$ with a rank less than 3. We have provided an example illustrating this in Figure 4 of the revision.\n\n---\n\n**Q4. Page 5: \"without loss of generality we may assume the group reps are all the same\" why?**\n\n\n**Reply:** To enhance clarity, we have replaced the phrase \u201cwithout loss of generality\u201d with \u201cin practice.\u201d In practice, models are typically designed to update hidden features in the same space before the last layer. Consequently, we can reasonably assume that the group representations are all the same."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700176436345,
                "cdate": 1700176436345,
                "tmdate": 1700176701605,
                "mdate": 1700176701605,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "me5G7uUJxb",
                "forum": "mGHJAyR8w0",
                "replyto": "ni9M9EmR4Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WxNP (2/3)"
                    },
                    "comment": {
                        "value": "**Q5. Remark 3 is vague and out of context. Corollary 3 seems related, is much more formal and accurate, but does not require the action to be faithful. Can you formally explain why faithfulness is important? This could be the critical missing link explaining why in terms of the irreducible order $\\ell$ we claim to have $0<1=2=3=4=\\ldots$ which is the message you are trying to convey but I don't think the theory currently supports.**\n\n\n**Reply:** We have added a detailed explanation of why faithfulness is important in Appendix B of the revised paper. Here, we elaborate on the significance of the faithfulness of representations to address your concern. The faithfulness of representations ensures that the data $\\rho\\big((\\mathfrak{g}^{(t+1)}_i)^{-1}\\mathfrak{g}^{(t)}_j\\big)$ must originate from $(\\mathfrak{g}^{(t+1)}_i)^{-1}\\mathfrak{g}^{(t)}_j$. Without faithfulness, $\\rho\\big((\\mathfrak{g}^{(t+1)}_i)^{-1}\\mathfrak{g}^{(t)}_j\\big)$ might stem from different group elements. For instance, consider the worst case where the representation is trivial. Then different $(\\mathfrak{g}^{(t+1)}_i)^{-1}\\mathfrak{g}^{(t)}_j$ are all mapped to the same matrix $\\rho\\big((\\mathfrak{g}^{(t+1)}_i)^{-1}\\mathfrak{g}^{(t)}_j\\big) = {\\bf I}$, the identity matrix. In other words, the faithfulness of representations ensures that our models do not lose this geometric information.\n\nIn practice, equivariant GNNs are designed to learn steerable features up to type $L>0$, including the learning of type-1 steerable features, which lie on faithful representations. Therefore, as we have explained in Remark 3, they exhibit the same ability to capture the geometry between local neighborhoods even as $L$ varies.\n\n---\n\n**Q6. Page 6: \u201cThis question is essentially asking whether the space of all type-$l$ features has a dimension of $2l+1$\u201d This is well defined once $f$ is fixed I believe and $X$ is free? Perhaps clarifying this will also help me understand how you define steerable features in general.**\n\n\n**Reply:** In this question, ${\\bf X}$ is a fixed input and $f$ represents any equivariant function. Therefore, we are examining the space {$ f({\\bf X}) \\mid \\text{ G-equivariant } f:\\mathbb{R}^{3\\times m}\\to V_l $}, consisting of all possible type-$l$ steerable features generated from ${\\bf X}$.\n\n---\n\n**Q7. Page 6: Could you specify v=f(x_1,x_2) or whatever the relation between v and x_1,x_2 is?**\n\n\n**Reply:** According to our definition of steerable features, $v = f(x_1, x_2)$ is a type-1 steerable vector that is generated by an equivariant function $f$ from the input coordinates $x_1,x_2$. The relation between $v$ and $x_1,x_2$ can be explained as follows: applying a transformation $g$ to $x_1$ and $x_2$ will result in a changed vector $g\\cdot v$ due to the equivariance of $f$. \n\nIn the revision, this example has been moved to Appendix G.\n\n---\n\n**Q8. I think a similar observation appears in [Villar, Lemma 3] I suggest you cite them**\n\n\n**Reply:** Thank you for bringing this to our attention. We have incorporated [Villar] into the additional related work section and provided a thorough discussion outlining the differences between their work and ours. For more details, please refer to Appendix A in the revised version.\n\n---\n\n**Q9. Table 1: what are the significance of the specific three values of (L,c) chosen in Table 1? Presumably you can run eSCN with any choice of the c-s?**\n\n\n**Reply:** Table 1 demonstrates that existing comparisons made between feature type L and channels c are unfair due to the significant decrease in the dimension of the hidden features and the number of parameters. The existing ablation study uses rows 1&3, only adjusting the feature type. This results in vastly different model sizes. By contrast, comparing rows 1&2 would be a more suitable comparison in which case the number of parameters and the feature dimension are similar. In particular, maintaining the feature dimension is crucial for providing empirical evidence supporting our results.\n\nTo clarify this we have added the following sentence to the revision at the end of the section \u201cNumerical comparisons in steerable feature types.\u201d: The ablation study in [23] compares rows 1&3, while rows 2&3 provide a more suitable comparison."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700176641878,
                "cdate": 1700176641878,
                "tmdate": 1700180973116,
                "mdate": 1700180973116,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "7qyaXlpsai",
                "forum": "mGHJAyR8w0",
                "replyto": "ni9M9EmR4Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer WxNP (3/3)"
                    },
                    "comment": {
                        "value": "---\n\n**Q10. In Table 2 the invariant equiformerV2 actually does reasonably well, seemingly contradicting your theorem?**\n\n\n**Reply:** While the equiformerV2 model obtains improved accuracy over other invariant architectures in Table 2, it is difficult to assume that this is a desirable performance feature. Due to the enhanced robustness of our dataset, which is obtained by augmenting the data with rotations, reflections, and translations, it is difficult to determine if the improvement in accuracy is due to sensitivity to the numerical precision in our augmentation. In particular, we observe that the best model is still three standard deviations away from perfectly classifying the $k$-chains. For this reason, we do not draw any strong conclusions about the invariant equiformerV2 architecture.\n\n\n---\nSmall Comment List:\n\nWe have clarified that stabilizers are subgroups.\n\nWe have corrected the typo in the definition of k-hop distinct.\n\nIn response to your concern about group action in Lemma 1, we have added further explanation in Remark 1, particularly emphasizing that the group action is absorbed by $\\rho(g_{\\bf X})$.\n\nThe definition of $V_{l, aug}$ is presented in Section 2, where we introduce $O(3)$-steerable vector spaces.\n\nWe have refined our phrasing to improve clarity and precision.\n\n\n---\n\nWe have updated our submission based on the reviewer's feedback, with the revision highlighted in blue. We are happy to address further questions on our paper. Thank you for considering our rebuttal."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700176690285,
                "cdate": 1700176690285,
                "tmdate": 1700176690285,
                "mdate": 1700176690285,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "E5bFK8xct1",
                "forum": "mGHJAyR8w0",
                "replyto": "7qyaXlpsai",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6451/Reviewer_WxNP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6451/Reviewer_WxNP"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for your answers. Here\u2019s an initial response hope to discuss in more detail in a later time.\n\nI feel like the theoretical message is still a little confusing. In particular one thing which could help imho is showing that:\n* equivariant networks can Separate geometric graphs from theorem 1 which invariant networks cannot\n* all equivariant networks with faithful reps can separate geometric graphs equally well\n\ncan you prove these two statements. How far is the second statement from remark3?"
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700467069180,
                "cdate": 1700467069180,
                "tmdate": 1700467069180,
                "mdate": 1700467069180,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "TeCJ67FWS9",
                "forum": "mGHJAyR8w0",
                "replyto": "ni9M9EmR4Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further Response to Reviewer WxNP"
                    },
                    "comment": {
                        "value": "Thank you for your continued engagement and feedback. We appreciate your interest in clarifying the theoretical aspects.\n\n- Regarding the 1st statement, demonstrating that equivariant networks can effectively distinguish geometric graphs require strong assumptions about the injectivity of the update function (UPD), the aggregation function (AGG), and the graph-level readout function, as highlighted in Prosition 2 in [20]. The work in [20] has already proven, under these assumptions, that equivariant networks have the same expressive power as the geometric Weisfeiler-Lehman (GWL) test, enabling them to distinguish any two $k$-distinct geometric graphs ($k$ is arbitrary). Please refer to Propositions 2 and 3 in Appendix D of our revised paper for a quick review. It's worth noting that using the GWL test to establish distinguishability is preferable due to its direct connection with the task at hand.  However, as the distinguishability of equivariant GNNs has been extensively explored in [20], we tend to focus on $k$-hop invariant GNNs, which cover the cases outlined in the limitation section of [20]. ***Our framework enables us to identify the underlying issues, specifically non-faithful representations, that limit the distinguishability of $k$-hop invariant GNNs.***\n\n\n- About the 2nd statement, it is easier to show that equivariant GNNs, assuming faithful representations and the injectivity of the update function, aggregation function, and graph-level readout function, have the same capability as the GWL test. Indeed, the proof of Proposition 2 in [20] can be directly applied to demonstrate this. We emphasize the significance of faithfulness in our work because the proof provided in [20] does not hold for equivariant GNNs learning features on non-faithful representations. Please refer to Appendix B in our revision for details.\n\nWe hope these responses address your concerns, and we are happy to address further questions regarding our paper.\n\n[20] Joshi, C. K., Bodnar, C., Mathis, S. V., Cohen, T., and Lio, P. On the expressive power of geometric graph neural networks. ICML 2023."
                    }
                },
                "number": 18,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700470511999,
                "cdate": 1700470511999,
                "tmdate": 1700490940936,
                "mdate": 1700490940936,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GCJci8KF98",
                "forum": "mGHJAyR8w0",
                "replyto": "TeCJ67FWS9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6451/Reviewer_WxNP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6451/Reviewer_WxNP"
                ],
                "content": {
                    "comment": {
                        "value": "* So what's the bottom line? The two claims I suggested above are correct? But you do not wish to state them in the revision?\nI would be inclined to raise my score if you would, or would provide some other precise theorems explaining why different faithful Equivariant networks are equivalent, but invariant networks are not. \n\nLet me fill in on some more details which I didn't get to earlier today. First of all thanks for the revision and taking my comment seriously, I appreciate it. Remarks:\n* As I see it, and this is a matter of taste of course, the main interesting claim in the paper is that faithful representations of any kind should be equivalent. This is demonstrated in experiments but on the theory there is only remark 3 which seems like hand-waving. If there is something precise to say (e.g. the second claim I suggested) I suggest you say it *in the main text*.\n\n* Corollary 3 applies to any representation, also non-faithful. It applies even for the trivial representation. So while interesting it is also a bit confusing as to the takeaway message. I'm not saying it isn't a good statement to have. But it is difficult to say that it justifies your main claims in the paper.\n\n* I find the k-hop contribution new but less interesting since it seems natural given the result in [20] re 1-hop. Also most geometric GNNs I believe, use 1-hop. In particular in the appendix where you discuss this all the methods you discuss are 1-hop methods.\n\n* Re invariant equiformer2: From reading the paper without going into the detail of this method, it seems to be two orders of magnitude better than the 50% I would expect from your theorems.  I think you should add this to the discussion and explain why you think this is (numerical errors, whatever)."
                    }
                },
                "number": 19,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700491284027,
                "cdate": 1700491284027,
                "tmdate": 1700491284027,
                "mdate": 1700491284027,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "GKvtSpGm7i",
                "forum": "mGHJAyR8w0",
                "replyto": "ni9M9EmR4Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further Response to Reviewer WxNP (2/2)"
                    },
                    "comment": {
                        "value": "**Q4. I find the $k$-hop contribution new but less interesting since it seems natural given the result in [20] re 1-hop. Also most geometric GNNs I believe, use 1-hop. In particular in the appendix where you discuss this all the methods you discuss are 1-hop methods.**\n\n**Reply:** While the findings in [20] extend easily from 1-hop to $k$-hop, it is crucial to note that [20] identified these works as limitations and suggested that integrating non-local features, like ComENet, might overcome the limitation of propagating invariant features.\n\nHowever, our work fills this gap and emphasizes the significance of incorporating global features. Notably, our results suggest that the integration of non-local features may not adequately address the limitations of invariant GNNs. Our analysis indicates that considering global features is necessary, as evidenced by the performance of ComENet and LEFTNet, both classified as 2-hop methods.\n\nIt is noteworthy that designing multi-hop methods is more challenging than 1-hop methods, and they have been considered a potential solution to mitigate the limitations of invariant GNNs. Nevertheless, our work emphasizes that relying solely on multi-hop methods might not be as effective a solution as previously anticipated.\n\n---\n\n**Q5. Re invariant equiformer2: From reading the paper without going into the detail of this method, it seems to be two orders of magnitude better than the 50% I would expect from your theorems. I think you should add this to the discussion and explain why you think this is (numerical errors, whatever).**\n\n**Reply:** The EquiformerV2 and eSCN architectures both rely on the spherical activation function introduced in eSCN which is not truly equivariant but quasi-equivariant. The details of the quasi-equivariance introduced by the architectures are discussed in Appendix D of [1] listed below. These quasi-equivariant architectures introduce error into the equivariance. This error is larger when the grid size of the spherical activation function is small as evidenced in Figure 10 of [1]. The grid size scales with $L$ and $m$ as $(2L+1)\\times(2m+1)$, therefore the error is largest and the grid is smallest when $L=m=0$. We have provided more discussion on the error introduced by quasi-equivariant architectures; see Section 5 and Appendix F.1 of the revised paper for details.\n\n[1] Saro Passaro and C. Lawrence Zitnick. Reducing SO(3) Convolutions to SO(2) for Efficient Equivariant GNNs. ICML 2023.\n\n---\n\nWe hope these responses address your concerns and we are happy to address further questions regarding our paper. Thank you for considering our rebuttal."
                    }
                },
                "number": 22,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700608460384,
                "cdate": 1700608460384,
                "tmdate": 1700608512105,
                "mdate": 1700608512105,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rpFtTaraqe",
                "forum": "mGHJAyR8w0",
                "replyto": "GKvtSpGm7i",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6451/Reviewer_WxNP"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6451/Reviewer_WxNP"
                ],
                "content": {
                    "title": {
                        "value": "Revised evaluation"
                    },
                    "comment": {
                        "value": "Thanks for addressing my remarks,  I\u2019ve raised my score to 6.\n\nTo summarize, I think the main nice result in the paper is the theoretical and empirical results suggesting that for geometric gnn there may not be a need to use high dimensional features. This is an important point.\n\nDisadvantages of the paper are that the proof of theorem 2 is really already known from [20] and more importantly that the results are not explained so well. The authors have made a considerable effort to fix this but I would strongly encourage them to thoroughly go over the paper so that it is clear for readers who have no previous acquaintance with [20]"
                    }
                },
                "number": 25,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644906217,
                "cdate": 1700644906217,
                "tmdate": 1700644906217,
                "mdate": 1700644906217,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "fKxdqZ3EfS",
                "forum": "mGHJAyR8w0",
                "replyto": "ni9M9EmR4Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further Response to Reviewer WxNP"
                    },
                    "comment": {
                        "value": "Dear Reviewer WxNP,\n\nThank you for your positive response to our contribution and the increased score. We sincerely appreciate your recognition of our efforts. Regarding the proof of Theorem 2, we acknowledge its origin in [20]. We would also like to take this opportunity to commend the work done by [20], which has undoubtedly laid a solid foundation in this domain.\n\nHowever, it is crucial to emphasize that the faithfulness of representations ensures the existence of injective functions, an aspect not explored in [20]. Additionally, as highlighted in our contribution, we would like to stress the novelty of Theorem 1, which conflicts with certain claims in [20], instead of that of Theorem 2.\n\nIn response to your valuable feedback, we have added a sentence at the beginning of Section 2 in the revised version to guide readers to the review of the Geometric Weisfeiler-Lehman test (GWL) and its relevant results in Appendix D. We believe this addition enhances the accessibility of the paper, especially for those unfamiliar with [20].\n\nWe are open to addressing any further questions regarding our contributions and remain committed to improving the comprehensibility of the background and the overall quality of our paper."
                    }
                },
                "number": 27,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700647780064,
                "cdate": 1700647780064,
                "tmdate": 1700648911629,
                "mdate": 1700648911629,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Z27eSIlXns",
            "forum": "mGHJAyR8w0",
            "replyto": "mGHJAyR8w0",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission6451/Reviewer_nbPE"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission6451/Reviewer_nbPE"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the expressivity of geometric graph networks which are \u201cmulti-hop\u201d, or aggregate information at each message passing step based on all nodes at most some number of steps from each node. They use the notion of equivariant moving frames to connect invariant features with equivariant features, illustrating that any equivariant activation space can be converted to an invariant activation space of equal dimension, so long as the frame is tracked (via an input-dependent group element). With this perspective, they show that k-hop invariant GNNs lose some geometric information relative to equivariant GNNs, but that the particular representations internal to equivariant GNNs matter less (in some sense) than the dimensions of the representations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "4 excellent"
                },
                "strengths": {
                    "value": "The problem studied in this paper is topical and practically meaningful, as geometric GNNs are quite widespread in application. Moreover, the insights presented in the paper are prescriptive, and give a rule of thumb for selecting hyper parameters such as the highest \u201cL\u201d (indexing the irrupts of O(3) or SO(3)) to include, and how many channels to use. The use of equivariant moving frames to convert to invariants is uncommon elsewhere in the literature, and its application to expressivity considerations is creative. The paper is generally clear and well-written. The authors\u2019 theoretical claims, and even some less rigorous intuitions, are backed up with large-scale experiments on cutting edge architectures for the OpenCatalyst dataset."
                },
                "weaknesses": {
                    "value": "1. My main qualm is with Corollary 3 and similar statements. In this Corollary, the authors note that given an equivariant function on a k-dimensional input representation space, it can be converted to an equivariant function on any other k-dimensional input representation space. As a result, they claim that the choice of representation to which the internal activations of a neural network transform is irrelevant (modulo its dimension). However, the conversion mapping from one equivariant vector space to a different (equal-dimensional) equivariant vector space may be arbitrarily complex or difficult to compute, and indeed, it may or may not be computable with a given architecture. In the case of rotations, one needs to recover g from a faithful representation (e.g. l=1), and then evaluate another representation at g \u2014 but computing a Wigner D matrix of high L may take many layers/parameters in the GNN. This is in the same spirit as universality results, which e.g. require increasing numbers of layers to approximate polynomials of increasing degree (see e.g. the appendix of Bogatskiy et al 2022, Lorentz Group Equivariant Neural Network for Particle Physics). In other words, there is a critical distinction between whether the **information** to compute a particular mapping is available, and whether a given architecture can actually **efficiently compute** that mapping. The authors of this work seem to focus more on the former perspective; namely, whether or not there is information loss from a certain architecture, which precludes one from ever computing a particular function (by any means). This has been a fruitful perspective for existing impossibility results on invariant GNNs \u2014 since these results roughly establish that, by only working with invariants, some neighborhoods are indistinguishable, and so **no** architecture can distinguish between them. This is a strong notion, but the converse does not hold: when the information is **not** lost, this does not imply that any architecture can actually compute the mapping. All of this is to say that, in my opinion, Corollary 3 does not sufficiently imply that the choice of equivariant representation is irrelevant up to dimension. I suspect even that the choice of representation may affect the universality of a fixed architecture family.\n2. On a related note, many of the paper\u2019s claims are informal \u2014 e.g. \u201cthere is no fundamental distinction in message-passing mechanisms arising from different values of L\u201d in Remark 3, or \u201canalyzing the message passing using the corresponding invariant features is a reasonable approach\u201d on page 4. It would be very helpful and important to make these precise. \n3. There are a few very related lines of work which aren\u2019t cited, but probably should be. For example, the paper\u2019s reasoning relies heavily on the concept of \u201cequivariant moving frames,\u201d which are a classical notion (dating back to Elie Cartan); yet this term does not appear in the paper, nor does a citation to e.g. Puny et al\u2019s frame averaging paper, which is a more recent machine learning paper that harnesses the concept of frames. A small section (the start of Section 3.2 on page 6) in this paper also notes that the output symmetry of an equivariant function must be at least the input symmetry; this is a well-established fact about equivariant functions, e.g. see Smidt et al 2021. Finally, and perhaps most significantly, related ideas were discussed in ClofNet (Du et al 2022) and its follow-up LeftNet (Du et al 2023), both of which use the precise idea of moving frames and invariantization to obtain equivariants from invariants. The latter work in particular includes an expressivity result for two-hop geometric GNNs. \n\n\nReferences:\n* Frame Averaging for Invariant and Equivariant Network Design by Omri Puny,\u00a0Matan Atzmon,\u00a0Heli Ben-Hamu,\u00a0Ishan Misra,\u00a0Aditya Grover,\u00a0Edward J. Smith, and\u00a0Yaron Lipman\nFinding symmetry breaking order parameters with Euclidean neural networks by Tess E. Smidt, Mario Geiger, and Benjamin Kurt Miller \n* SE(3) Equivariant Graph Neural Networks with Complete Local Frames by Weitao Du,\u00a0He Zhang,\u00a0Yuanqi Du,\u00a0Qi Meng,\u00a0Wei Chen,\u00a0Bin Shao, and\u00a0Tie-Yan Liu\u00a0\n* A new perspective on building efficient and expressive 3D equivariant graph neural networks by Weitao Du,\u00a0Yuanqi Du,\u00a0Limei Wang,\u00a0Dieqiao Feng,\u00a0Guifeng Wang,\u00a0Shuiwang Ji,\u00a0Carla Gomes,\u00a0and Zhi-Ming Ma\n\n\nHere are a few minor typos and writing notes:\n* On page 3, in the definition of geometric graphs, in the last two sentences: g inconsistently has a subscript $g_i$. Also, the last equality should presumably be an inequality (k-hop distinct if for all isomorphism, there is a node I, such that for any g, that equation does NOT hold).\n* Page 3, k-hop geometry GNNs: \u201cGiven a geometric graph G=(V,E,F,X).\u201d is not a full sentence; perhaps \u201cGiven\u201d should have been \u201cConsider\u201d.\n* In Lemma 1, was $V_0$ defined somewhere?\n* As a very minor nitpick, the calligraphic g the authors have chosen for the group element is used almost universally to refer to an element of a Lie algebra, not of a Lie group. I would recommend sticking to regular $g$."
                },
                "questions": {
                    "value": "1. Intuitively, where is the multi-hop aspect used in this work? The intuition about invariant features and moving frames seems true for 1-hop networks too. What is the takeaway regarding \u201cmulti-hop\u201d architectures from this work, in contrast to 1-hop networks?\n2. eSCN is an architecture evaluated in the experiments section. But is this a multi-hop architecture? \n3. Could the authors comment further on Corollary 3, regarding point (1) from the Weaknesses section? For example, doesn\u2019t the choice of internal representation affect the universality a given architecture (beyond just the dimension)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission6451/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698823137891,
            "cdate": 1698823137891,
            "tmdate": 1699636720411,
            "mdate": 1699636720411,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "i170vEYKcF",
                "forum": "mGHJAyR8w0",
                "replyto": "Z27eSIlXns",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nbPE (1/3)"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review, valuable feedback, and endorsement. In what follows, we provide point-by-point responses to your comments.\n\n---\n\n**Q1. Questions related to a few minor typos and writing note.**\n\n\n**Reply:** We appreciate your feedback, and we have made revisions to address the noted typos and have provided the requested clarifications. Specifically,  we have added the definition of $V_0$, representing the 1-dimensional trivial representation of a given group $G$. Regarding the use of the calligraphic g, we acknowledge the convention that it is commonly associated with the Lie algebra. In this context, we have chosen it to distinguish from the regular g, which is used as a function in the proof of Lemma 2.\n\n---\n\n**Q2. Answer the questions raised by weakness (1) and the question: Could the authors comment further on Corollary 3, regarding point (1) from the Weaknesses section? For example, doesn\u2019t the choice of internal representation affect the universality of a given architecture (beyond just the dimension)?**\n.**\n\n\n**Reply:** We acknowledge two main aspects related to the concept of expressiveness: the capacity of features to carry information and the model's ability to extract that information. We intended to focus on the first aspect for the exact reason you mentioned in the passage on weakness (1) -- the latter is subject to the architecture of the model given, while the former is not. We want to clarify that our intention is not to directly conclude that the choice of equivariant representation is irrelevant up to dimension. We appreciate your effort in pointing out the lack of precision of some claims in the paper, especially the misleading conclusion derived from Corollary 3. We have carefully revised our statements to convey the implications of our results more precisely. Additionally, we have added a section in Appendix B to discuss the universality and potential methods. Here, we present a part of that discussion.\n\nThe relationship between internal representation and universality is delicate. At first glance, the topological properties of the Lie group $G=SO(3)$ (or $G=O(3)$) prevent us from constructing a continuous function ${\\bf X}\\mapsto g_{\\bf X}$. That is, the poor regularity of $\\lambda({\\bf X})$ is inevitable, making any general analytical tool that relies on regularity assumption unsuitable for studying universality. However, a potential solution to this regularity issue is to extend the function ${\\bf X}\\mapsto g_{\\bf X}$ to a function from a suitable covering space of the spatial embedding to $G$. To put it simply, lifting the spatial embedding to the covering space makes it possible to select a continuous extension of ${\\bf X}\\mapsto g_{\\bf X}$. Once we have that, we naturally obtain an extended version of Corollary 3 with well-behaved correspondence between two representations on the covering space.\nAs a result, we suspect that under any architecture with universality, changing the internal representation while fixing the dimension does not affect the class of function the intrinsic invariant feature $\\lambda({\\bf X})$ can approximate. However, we cannot deny the possibility that certain choices of representation could benefit the model, allowing it to approximate the target function with fewer layers. To demonstrate this idea will require extensive work, and given the page limitation, we leave this to future works."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700176166808,
                "cdate": 1700176166808,
                "tmdate": 1700179978285,
                "mdate": 1700179978285,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "lLvXJAB8fC",
                "forum": "mGHJAyR8w0",
                "replyto": "Z27eSIlXns",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nbPE (2/3)"
                    },
                    "comment": {
                        "value": "**Q3. Questions raised by weaknesses (2) and (3).**\n\n\n**Reply:** We appreciate the reviewer's feedback and concerns. In response, we have made several revisions to enhance the precision and clarity of our statements. Additionally, we have incorporated a new section (refer to Appendix A in the revised paper) to discuss additional related works, including Puny et al.'s frame averaging paper and the works by Du et al. on ClofNet and LEFTNet, and the distinctions between these existing notions and Lemma 1.\n\nIndeed, within our framework, the function ${\\bf X}\\mapsto g_{\\bf X}$ is exactly a singleton-valued equivariant moving frame defined in [1], if we restrict the function\u2019s domain to where the action of $G$ is free. While Puny et al.'s frame averaging paper studies set-valued equivariant moving frames, the domain of these equivariant moving frames is still restricted to guarantee the equivariance. In contrast, our defining $g_{\\bf X}$ remains well-defined even in cases where the action of $G$ is not free, making it not necessarily equivariant. \n\nThe scalarization proposed in Du et al.'s papers, while similar to how we define $\\lambda({\\bf X})\\coloneqq \\rho(g_{\\bf X})^{-1} \\cdot f({\\bf X})$, differs in that the local frames used for scalarization are generated from two coordinates rather than all the coordinates in ${\\bf X}$. In contrast, our $g_{\\bf X}$ involves a consideration of all input coordinates in ${\\bf X}$ since it is related to the orbit $G\\cdot{\\bf X}$.\n\nTo augment the paper's comprehensiveness, we have included the empirical results of ClofNet and LEFTNet on $k$-chains. In particular, we believe the invariant design for LEFTNet belongs to the class of 2-hop invariant GNNs. So it's important to highlight that LEFTNet may still face the issues discussed in the last part of Section 3.1. Please refer to the table below for further details.\n\nLastly, we have cited Smidt et al. (2021) to acknowledge and reference the well-established fact about the output symmetry of an equivariant function being at least the input symmetry, as noted at the beginning of Section 3.2. We believe these revisions address your concerns and provide a more precise and well-referenced discussion of related works and concepts.\n\n---\n\n**K=2 & K=3**\n\n|        \t| 1 Layer       \t| 2 Layers     \t| 3 Layers     \t| 1 Layer       \t| 2 Layers     \t| 3 Layers     \t| 4 Layers     \t|\n|------------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|\n|        \t| k-hop chain   \t| k = 2         \t| k = 2         \t| k = 3         \t| k = 3         \t| k = 3         \t| k = 3         \t|\n| ClofNet\t| 50.0 \u00b1 0.0    \t| 50.0 \u00b1 0.0    \t| 100.0 \u00b1 0.0   \t| 50.0 \u00b1 0.0    \t| 50.0 \u00b1 0.0    \t| 100.0 \u00b1 0.0    \t| 100.0 \u00b1 0.0   \t| \n\n---\n\n**K=4**\n|                \t| 2 Layers      \t| 3 Layers      \t| 4 Layers      \t| 5 Layers      \t| 6 Layers      \t|\n|--------------------|-------------------|-------------------|-------------------|-------------------|-------------------\n| ClofNet        \t| 50.0 \u00b1 0.0    \t| 100.0 \u00b1 0.0   \t| 95.0 \u00b1 15.0   \t| 95.0 \u00b1 15.0   \t| 95.0 \u00b1 15.0   \t|\n| LEFTNet        \t| 50.0 \u00b1 0.0    \t| 50.0 \u00b1 0.0    \t| 50.0 \u00b1 0.0    \t| 50.0 \u00b1 0.0    \t| 50.0 \u00b1 0.0 |100.0 \u00b1 0.0   \t|               \t|\n\n\n[1] Olver, P. J. (2009). Lectures on moving frames. \n\n---"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700176311245,
                "cdate": 1700176311245,
                "tmdate": 1700177947286,
                "mdate": 1700177947286,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "QUgbYkMODN",
                "forum": "mGHJAyR8w0",
                "replyto": "Z27eSIlXns",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer nbPE (3/3)"
                    },
                    "comment": {
                        "value": "---\n\n**Q4. Intuitively, where is the multi-hop aspect used in this work? The intuition about invariant features and moving frames seems true for 1-hop networks too. What is the takeaway regarding \u201cmulti-hop\u201d architectures from this work, in contrast to 1-hop networks?**\n\n\n**Reply:** The concept of \"multi-hop\" in our work specifically refers to how far each message-passing step can propagate features. We present the architecture of $k$-hop geometric GNNs in Eq (1), where the variable $k$ denotes the size of neighborhoods from which a node aggregates information. In other words, it represents the maximum distance over which a node aggregates information. Additionally, we reference ComENet as an instance of 2-hop invariant GNNs in Section 1.1 since it utilizes dihedral angles.\n\nImportantly, $k$-hop geometric GNNs are not equivalent to regular 1-hop geometric GNNs. As demonstrated in Appendix B (refer to Appendix D in the revised paper), under certain assumptions on UPD and AGG, $k$-hop invariant GNNs can distinguish any two $k$-hop distinct geometric graphs but fail to distinguish any two k-hop identical geometric graphs. This indicates that 1-hop invariant GNNs might be less powerful than their 2-hop counterparts, and this trend continues.\n\nThe reason lies in the fact that the value of $k$ influences the size of local neighborhoods, consequently affecting the input of the aggregation process. Utilizing a smaller $k$ may cause $k$-hop invariant GNNs to overlook changes in geometry between small local neighborhoods, whereas using a larger k can capture these variations.\n\nHowever, it's noteworthy that, for any finite $k$, we can always construct two $k$-hop identical but $(k+1)$-hop distinct geometric graphs. In this scenario, $k$-hop invariant GNNs cannot differentiate them even with thousands of layers, while 1-hop equivariant GNNs, due to the faithfulness of features, can achieve this with $k+1$ layers. This finding sheds light on the advantages of learning steerable features up to type-$L\\geq1$. We have substantiated this point through empirical results for ComENet and the addition of the LEFTNet model in Table 5 of Appendix F.1 in the revision.\n\nTo clarify this implication, we have refined the concluding remarks. In particular, we have added the following remark: relying solely on propagating invariant features confined to specific $k$-hop neighborhoods is insufficient for enabling geometric GNNs to capture the global information of geometric graphs precisely.\n\n---\n\n**Q5. eSCN is an architecture evaluated in the experiments section. But is this a multi-hop architecture?**\n\n**Reply:** No, the eSCN architecture is a 1-hop architecture. However, to avoid confusion, we have limited the eSCN experiments to performing the ablation studies on the steerable feature type $L$ and dimension $c$.\n\n---\n\nWe have updated our submission based on the reviewer's feedback, with the revision highlighted in blue. We are happy to address further questions on our paper. Thank you for considering our rebuttal."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700177988551,
                "cdate": 1700177988551,
                "tmdate": 1700180518559,
                "mdate": 1700180518559,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "90GWuZv9XH",
                "forum": "mGHJAyR8w0",
                "replyto": "QUgbYkMODN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6451/Reviewer_nbPE"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6451/Reviewer_nbPE"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for the thoughtful response"
                    },
                    "comment": {
                        "value": "Many thanks to the authors for their thoughtful response. Two follow-up questions: \n\n* First, the authors said, \"Once we have that, we naturally obtain an extended version of Corollary 3 with well-behaved correspondence between two representations on the covering space.\". Where is this extended version of Corollary 3? \n* I am also not sure what \"lifting the spatial embedding to the covering space\" means, and Appendix B (unless I am looking in the wrong place) did not further clarify. What is the covering space -- could the authors give a concrete example?\n\nGenerally speaking, I stand by my endorsement of the paper, but I agree with reviewer WxNP that it is rather important to make these central claims formal theorems for a final version (including those added newly to Appendix B)."
                    }
                },
                "number": 20,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700607764531,
                "cdate": 1700607764531,
                "tmdate": 1700607764531,
                "mdate": 1700607764531,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Ky3VBXfNFl",
                "forum": "mGHJAyR8w0",
                "replyto": "Z27eSIlXns",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission6451/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Further Response to Reviewer nbPE"
                    },
                    "comment": {
                        "value": "Dear Reviewer nbPE,\n\nThank you for your further feedback and invaluable comments. We highly appreciate your endorsement and have taken your suggestions into careful consideration.\n\nRegarding the extended version of Corollary 3 and the concept of lifting the spatial embedding to the covering space, we have included the relevant details in Appendix B of the revision. We aim to provide a clear understanding of the key concepts involved, and we hope the additional information addresses your questions. \nIt's important to note that while we believe this extension can be proven, we have not delved into all the details and solved all technical challenges. As such, we have identified it as a potential direction for future work. We appreciate your understanding and are open to any further questions you may have.\n\n---\n\n**Q1. First, the authors said, \"Once we have that, we naturally obtain an extended version of Corollary 3 with well-behaved correspondence between two representations on the covering space.\". Where is this extended version of Corollary 3? I am also not sure what \"lifting the spatial embedding to the covering space\" means, and Appendix B (unless I am looking in the wrong place) did not further clarify. What is the covering space -- could the authors give a concrete example?**\n\n**Reply:** We apologize for the lack of precision in our previous statement. In response to your suggestion, we have incorporated Question 1 and Question 2 into Appendix B of the revised version. Notably, Question 2 complements Corollary 3 by considering the universality aspect. Let us briefly explain it as follows.\nConsider two steerable vector spaces, $V$ and $W$, of the same dimension. Suppose we have a continuous $\\mathfrak{G}$-equivariant function $f_V:\\mathbb{X}_3 \\to V$. According to Corollary 3, which implies the existence of a $\\mathfrak{G}$-equivariant function $f_W:\\mathbb{X}_3\\to W$ with the same corresponding invariant functions as $f_V$. However, the continuity of $f_W$ cannot be guaranteed, and hence it may not be approximated by a given architecture. Therefore, in Question 2, we ask if there exists a continuous $\\mathfrak{G}$-equivariant function $f'_W:\\mathbb{X}_3\\to W$ such that $f'_W$ and $f_W$ coincide on any compact set except an $\\epsilon$-small measurable subset. By applying the universality of the given architectures, we can approximate $f'_W$, which has a corresponding invariant function identical to that of $f_V$ on any compact set except an $\\epsilon$-small measurable subset.\nThen we delve into a detailed explanation of a potential method to address Question 2 in Appendix B. In particular, we describe how we adopt a similar approach used to obtain the equivariant moving frame in Puny et al.'s frame-averaging paper to construct the concrete covering space. Intuitively, the covering space $\\widetilde{\\mathbb{X}_3}$ of a space $\\mathbb{X}_3$ is a topological space that locally looks like multiple copies of $\\mathbb{X}_3$. We show that we have the desired continuity and equivariance of the function $g_X$ on these copies. Leveraging these properties, we should be able to construct the desired $f'_W$ for any given $f_W$. For a more in-depth understanding, please refer to the details provided in Appendix B.\n\n---\n\n**Q2. Generally speaking, I stand by my endorsement of the paper, but I agree with reviewer WxNP that it is rather important to make these central claims formal theorems for a final version (including those added newly to Appendix B)** \n\n**Reply:** We appreciate your valuable feedback. To enhance the formality of our claims, we have introduced formal results, namely, Theorem 2 and Proposition 8 (originated from Remark 3 in the old version), to elucidate the significance of faithful representations. Additionally, we have presented formal statements, namely, Questions 1 and 2, to delve into the universality aspect and have proposed potential methods to address Question 2. It's important to note that we've used the term \"Question\" instead of \"Theorem\" or \"Proposition\" as we have not yet verified all the details and have decided to leave this as future work. Nevertheless, we hope these improvements in the formality of our claims and statements address your concerns.\n\n---\n\nWe hope these responses address your concerns, and we are happy to address further questions regarding our paper. Thank you for considering our rebuttal."
                    }
                },
                "number": 26,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission6451/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700644958343,
                "cdate": 1700644958343,
                "tmdate": 1700645135003,
                "mdate": 1700645135003,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]