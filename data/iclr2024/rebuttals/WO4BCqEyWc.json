[
    {
        "title": "Augmentation-aware Self-Supervised Learning with Conditioned Projector"
    },
    {
        "review": {
            "id": "yLn1HYFw0a",
            "forum": "WO4BCqEyWc",
            "replyto": "WO4BCqEyWc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3997/Reviewer_Fj5t"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3997/Reviewer_Fj5t"
            ],
            "content": {
                "summary": {
                    "value": "This paper considers the problem of recent self-supervised learning methods that they learn to be invariant to data augmentations, which may be harmful for some downstream tasks. To tackle this problem, this paper proposes to modify the projector by feeding the information about data augmentations together with the encoder outputs. Experimental results show the effectiveness of the proposed method in transfer learning on image datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Learning augmentation-aware representations is a timely topic.\n\n- The proposed idea is simple and ablation studies show how the design choices are made well."
                },
                "weaknesses": {
                    "value": "- [Garrido et al.] would be one of the most recent work among prior works but missed in this paper.\n\n- The proposed method simply provides the additional information about augmentations together with the encoder outputs, and it is not clear how it helps to \"preserve more information about augmentations\" in representations. Figure 3 shows that injecting information of random augmentations results in reduced cosine similarities. This implies that the projector relies on the given information about augmentations, which is not directly related to the learned representations (the output of the encoder), i.e., learned representations do not have to be changed regardless of whether the projector relies on the additional information about augmentations or not. Any theoretical justification on the effect of the proposed method to the learned representations would be welcome.\n\n- The performance gain is overall minor and often it underperforms previous methods.\n\n- Why does MoCo-v2 in Table 1 contain only one performance of LooC? It looks quite not informative.\n\n- Why does MoCo-v3 in Table 1 miss the performance of \"AI by [Chavhan et al.],\" while the original paper presents its performance?\n\n- The reference section requires thorough proofreading, as there are many incomplete/inaccurate references. For example, the closest prior work by [Lee et al.] is published in NeurIPS'21, but its arXiv version is cited. Also, many references miss the name of the published venue.\n\n[Chavhan et al.] Amortised invariance learning for contrastive self-supervision. In ICLR, 2023.\n\n[Garrido et al.] Self-supervised learning of Split Invariant Equivariant representations. In ICML, 2023."
                },
                "questions": {
                    "value": "Please address concerns in Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3997/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698768575373,
            "cdate": 1698768575373,
            "tmdate": 1699636361897,
            "mdate": 1699636361897,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "KURZfILYST",
                "forum": "WO4BCqEyWc",
                "replyto": "yLn1HYFw0a",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3997/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3997/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer Fj5t,\n\nThank you vrey much for a thorough review of our work.  We would like to address your concerns below. If you have any additional questions, we are looking forward to answering them. \n\n**ad. W1: Comparison to the work of Garrido et. al.**\n\nThank you for this question. In their recent work, Garrido et. al propose to extend VicReg with a hypernetwork-based predictor to learn representations that are equivariant to transformations. Similarly to one of the variants of CASSLE, their hypernetwork predicts the parameters of the predictor based on the transformation descriptors. In our work, we show that there exist methods of conditioning superior to hypernetworks. We also evaluate our approach on real-life image data, as well as a wider range of SSL methods.\n\n**ad. W2: request for additional theoretical justification:**\n\nWe have expanded the discussion of Figure 3 in Section 3.2, showing that the conditional probability of feature extractor representations on the condition of correct augmentation information is larger than on the condition of any random augmentation information. This implies that the learned representations of images are correlated, to an extent, with the parameters of augmentations applied to them.\n\n**ad. W3: The performance gain is overall minor and often it underperforms previous methods**\n\nWhile it is true that CASSLE does not always achieve the best downstream performance, it does so in the majority of cases. We have compiled the linear evaluation results of different approaches on different downstream tasks (Tables 1 and 7) and ranked each approach from best to worst-performing in each downstream task (See Figure 5 in the revised manuscript). \n\nWe find that:\n\n* CASSLE performs best in 54 out of 91 downstream tasks, i.e. 59.34% of cases where it was evaluated.\n* AugSelf performs best in 31 out of 91 downstream tasks, i.e. 34.07% of cases where it was evaluated.\n* AI performs best in 5 out of 16 downstream tasks, i.e. 31.25% of cases where it was evaluated.\n* IFM performs best in 1 out of 13 downstream tasks, i.e. 7.69% of cases where it was evaluated.\n* Vanilla performs best in 0 out of 91 downstream tasks, i.e. 0.00% of cases where it was evaluated.\n\n(we excluded LooC from the analysis, see the below answer to W4).\n\n**ad. W4: Why does MoCo-v2 in Table 1 contain only one performance of LooC? It looks quite not informative.**\n\nOut of the datasets we are transferring to in Table 1, the authors of LooC have conducted experiments only with transferring to the CUB dataset, which we re-reported in our work. To the best of our knowledge, neither the codebase nor network checkpoints trained by LooC are publicly available. As such, we were not able to benchmark this method on more data.\nIt is also worth noting that both we and the authors of AugSelf observed lower performance of baseline MoCo-v2 transferred to CUB, compared to the authors of LooC.\nWe also note that other works on augmentation-aware SSL [1,2] do not reimplement LooC and also re-reported their results.\n\n**ad. W5: Why does MoCo-v3 in Table 1 miss the performance of \"AI by [Chavhan et al.],\" while the original paper presents its performance?**\n\nThis is because we trained a different backbone (ViT-Small, which is 4x smaller than ViT-base), compared to the one used by Chavhan et. al (ViT-Base).  \n\n**ad. W6: updating the bibliography**\n\nThank you for pointing this out. We have updated the bibliography in the revised version of the paper in order to refer to journal /conference versions of the cited papers.\n\n\nWe hope that you find our answers satisfactory. We are looking forward to answering any of your additional questions.\n\nKind regards, \n\nAuthors\n\n[1] Amortised invariance learning for contrastive self-supervision. In ICLR, 2023.\n\n[2] Improving Transferability of Representations via Augmentation-Aware Self-Supervision Hankook Lee, Kibok Lee, Kimin Lee, Honglak Lee, Jinwoo Shin, 2021"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700090958082,
                "cdate": 1700090958082,
                "tmdate": 1700090958082,
                "mdate": 1700090958082,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "yG3ifuoFzK",
                "forum": "WO4BCqEyWc",
                "replyto": "KURZfILYST",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3997/Reviewer_Fj5t"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3997/Reviewer_Fj5t"
                ],
                "content": {
                    "comment": {
                        "value": "Thanks for addressing my questions. Below I provide more comments, that would hopefully be helpful regardless of the acceptance of this paper.\n\n> **ad. W2: request for additional theoretical justification:** We have expanded the discussion of Figure 3 in Section 3.2, showing that the conditional probability of feature extractor representations on the condition of correct augmentation information is larger than on the condition of any random augmentation information. This implies that the learned representations of images are correlated, to an extent, with the parameters of augmentations applied to them.\n\nI went through the revision, but still I don't think it fully explains if the representations $e$ retains the augmentation information. If there is no theoretical analysis to prove this, then an additional experiment to check if $e$ better retains the augmentation information when trained the proposed method compared to the baseline would be helpful.\n\n> **ad. W3: The performance gain is overall minor and often it underperforms previous methods** While it is true that CASSLE does not always achieve the best downstream performance, it does so in the majority of cases. We have compiled the linear evaluation results of different approaches on different downstream tasks (Tables 1 and 7) and ranked each approach from best to worst-performing in each downstream task (See Figure 5 in the revised manuscript).\n\nIn my opinion, the proposed method does not have to outperform the others in all cases. Rather than counting how frequently the proposed method outperforms the others, analyzing why the proposed method fails to outperform the others would be more useful. By looking at Table 1 and 7, CASSLE is not better than AugSelf when combined with siamese representation learning (BYOL and SimSiam) and/or when the architecture is ViT.\n\n> **ad. W5: Why does MoCo-v3 in Table 1 miss the performance of \"AI by [Chavhan et al.],\" while the original paper presents its performance?** This is because we trained a different backbone (ViT-Small, which is 4x smaller than ViT-base), compared to the one used by Chavhan et. al (ViT-Base).\n\nThen, can you say the proposed CASSLE is scalable? Experimental results with a larger backbone would be preferable, as some progress on DL in small-scale settings is often not scalable.\n\n> **ad. W6: updating the bibliography** Thank you for pointing this out. We have updated the bibliography in the revised version of the paper in order to refer to journal /conference versions of the cited papers.\n\nI don't think they are properly fixed yet. I can see \\citet is often misused in the place of \\citep, and some references still missed their venue, e.g., CPC (Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding, 2019.) is an arXiv paper, and the context prediction paper (Carl Doersch, Abhinav Gupta, and Alexei A. Efros. Unsupervised visual representation learning by context prediction. December 2015.) is published in ICCV'15. Also, references are generally too verbose."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700668281338,
                "cdate": 1700668281338,
                "tmdate": 1700668281338,
                "mdate": 1700668281338,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fyasPgwZjl",
            "forum": "WO4BCqEyWc",
            "replyto": "WO4BCqEyWc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3997/Reviewer_ad4G"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3997/Reviewer_ad4G"
            ],
            "content": {
                "summary": {
                    "value": "Self-supervised methods are known to learn representations invariant to augmentations applied during training. This can be problematic when features of such augmentations are important for downstream tasks. This work considers the important task of performing self-supervised learning without losing important semantic features in the data. To achieve this, CASSLE is proposed, a method which conditions the learned projection head on the augmentations of each view. The work demonstrates that this results in features which are still augmentation-aware."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The manuscript is well written and experiments are well picked to test the purported claims regarding sensitivity of learned features to augmentations applied during training.\n* CASSLE is simple and has demonstrated efficacy when training augmentation-based contrastive models. When compared to other methods that condition on augmentations applied during training, table 1 shows that CASSLE has superior performance across many datasets."
                },
                "weaknesses": {
                    "value": "* Based on Table 7, the proposed method seems to less effective for SimSiam and BYOL compared to InfoNCE based methods. The manuscript currently claims that CASSLE is applicable to all joint-embedding architectures, but the current experimental results do not demonstrate this.\n* The experiments in 4.2 use the InfoNCE to evaluate augmentation-awareness, which is sensitive to the negative examples that are used. Instead of this, why not perform linear probing to predict the specific augmentation applied to an image? This would be a more direct measure of the augmentation-awareness.\n* The work does not address the large body of work surrounding \u201cfeature suppression\u201d, an important issue of contrastive models becoming invariant to features important for downstream tasks. I believe the work can be strengthened by including comparisons to methods proposed to address feature suppression [2], as well as evaluation on some feature suppression benchmarks [1].\n* Current experiments do not demonstrate the effectiveness of CASSLE with augmentation-free approaches to self-supervised learning. This limits the modalities in which it can be applied to those where augmentations can be selected a priori. \n\nMinor:\n* For Table 1, and other similar tables, could the authors add a column denoting mean improvement, taken over datasets, over the vanilla baseline to more easily compare each of the methods to CASSLE? It does not have to specifically be an additional column, but it would be nice to have an aggregate metric of performance in comparison to the baseline. \n* Some of the citations should be updated to include the full Author name (E.g., MoCo and SimSiam citations)\n\n\n[1] \"Intriguing Properties of Contrastive Losses,\u201d Chen et al., 2021\n\n[2] \u201cCan contrastive learning avoid shortcut solutions?,\u201d Robinson et al., 2021."
                },
                "questions": {
                    "value": "* How does CASSLE relate to feature suppression [1] and shortcut solutions [2] in contrastive learning?\n* Table 4 indicates that many of the methods were trained with a batch size of 256. Can the authors clarify why this was set so low? In the SimCLR paper it is shown that contrastive methods perform much worse when trained with a smaller batch size. Does CASSLE scale to larger batch sizes? Does CASSLE still perform well with a large batch size?\n* Can CASSLE be applied to masked self-supervision? There seems to be a connection between CASSLE and the MAE, where the latter conditions on mask tokens to reconstruct masked patches.\n* Have the authors tried performing feature inversion like in [3]? It would be interesting to see if CASSLE results in inverted features that are more reconstructive of attributes like color compared to vanilla contrastive features.\n\n\n[1] \"Intriguing Properties of Contrastive Losses,\u201d Chen et al., 2021\n\n[2] \u201cCan contrastive learning avoid shortcut solutions?,\u201d Robinson et al., 2021.\n\n[3] \u201cWhat makes instance discrimination good for transfer learning?,\u201d Zhao et al., 2021."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3997/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3997/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3997/Reviewer_ad4G"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3997/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698797973614,
            "cdate": 1698797973614,
            "tmdate": 1700434992855,
            "mdate": 1700434992855,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6cP9NDmKxd",
                "forum": "WO4BCqEyWc",
                "replyto": "fyasPgwZjl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3997/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3997/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Revier ad4G,\n\nThank you for a detailed review of our work.  We would like to address your concerns below. If you have any additional questions, we are looking forward to answering them. \n\n**ad. W1: applicability to SimSiam and BYOL.**\n\nWhen talking about applicability, the meaning is that the design of CASSLE is compatible with a wide range of joint-embedding approaches and thus, can be applied to them. We do not claim that it is guaranteed to outperform approaches such as AugSelf. Nevertheless, while AugSelf outperforms CASSLE on SimSiam and BYOL, CASSLE still offers a performance boost compared to the vanilla versions of those methods, confirming its applicability in the sense of performance.\n\n\n**ad. W2: predicting augmentation parameters instead of InfoNCE in 4.2**\n\nThank you for your suggestion. We do not directly predict the augmentation parameters as we find it quite inconsequential. During training, we present the perturbed images with the induced parameters so that we build a stronger feature extractor, informed by the nuances caused by the augmentations. Note that for Augself it makes sense to predict the (difference of) augmentations as it is the key component for their loss function. However, in this experiment, we aim to measure the sensitivity in various stages rather than a quantitative description of applied augmentations.\n\n\n**ad. W4: comparison with augmentation-free approaches**\n\nOur research problem focuses on how augmentation-based SSL methods become invariant to augmentations. We are not aware of any evidence in the literature that MIM-based methods also suffer from this issue. As such, a comparison to them is not relevant in the context of CASSLE.\n\n**ad. W5: aggregate metric of improvement over baseline**\n\nThank you for this valuable suggestion. We have compiled the linear evaluation results of different approaches on different downstream tasks (Tables 1 and 7) and summarized them in Figure 5 (appendix C) in the revised manuscript. CASSLE is usually ranked the best in terms of performance on downstream tasks compared to AugSe and Vanilla approaches. AugSelf and CASSLE improve over Vanilla approaches by a comparable margin. Finally, CASSLE achieves the best performance in the largest number of downstream tasks.\n\n**ad. W7: updating the bibliography**\n\nThank you for raising this concern. We have updated the bibliography in the revised version of the paper in order to refer to journal and conference versions of the cited papers.\n\n**ad. Q1/ W3: How does CASSLE relate to feature suppression [1] and shortcut solutions [2] in contrastive learning?**\n\nThank you for pointing this out. We believe that invariance to augmentation can be regarded as a consequence of feature suppression, and have added an according reference to the above works in the Related Work section of the revised version of our manuscript. It is also interesting to verify how transferable are methods designed for mitigating feature suppression. We also added a comparison to MoCo-v2 + implicit feature modification (IFM) [2] to find how this established method of mitigating feature suppression transfers to downstream tasks.\n\n**ad. Q2: Question about batch size of 256** \n\nWe have set this batch size for all methods in compliance with the hyperparameters of AugSelf [4]. We note that the batch size is mainly important for SimCLR, as approaches such as MoCo decouple the batch size from the number of negatives, and Barlow Twins, SimSiam and BYOL work well with batch sizes of 256. CASSLE is unrelated to the batch sizes and we expect it to perform equally well with larger batch sizes.\n\n**ad. Q3: Can CASSLE be applied to masked self-supervision?**\n\nIndeed, there seems to be such a connection. While CASSLE has been designed with joint-embedding approaches in mind, applying it to MIM-based approaches could be an interesting idea for future work. However, we are not aware of any works that indicate that MIM-based methods also suffer from the issue of augmentation invariance.\n\n**ad. Q4: Have the authors tried performing feature inversion like in [3]?**\n\nWhile this could be interesting, we did not perform such an evaluation. We are interested in whether features learned by CASSLE are more transferable to downstream tasks and harder to match together, whereas the quality of reconstruction of particular attributes such as color could be subject to large variance.\n\nWe hope that you find our answers satisfactory. We are looking forward to answering any of your additional questions.\n\nKind regards, \n\nAuthors\n\n[1] \"Intriguing Properties of Contrastive Losses,\u201d Chen et al., 2021 \n\n[2] \u201cCan contrastive learning avoid shortcut solutions?,\u201d Robinson et al., 2021. \n\n[3] \u201cWhat makes instance discrimination good for transfer learning?,\u201d Zhao et al., 2021.\n\n[4] Improving Transferability of Representations via Augmentation-Aware Self-Supervision Hankook Lee, Kibok Lee, Kimin Lee, Honglak Lee, Jinwoo Shin, 2021"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700091346008,
                "cdate": 1700091346008,
                "tmdate": 1700091346008,
                "mdate": 1700091346008,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "9NkVo6xF2f",
                "forum": "WO4BCqEyWc",
                "replyto": "6cP9NDmKxd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3997/Reviewer_ad4G"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3997/Reviewer_ad4G"
                ],
                "content": {
                    "title": {
                        "value": "Response to Rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for engaging with the review of their work. I believe the additional baseline comparisons and discussion of related works strengthen the work, and I have updated my score as such.\n\nI disagree that predicting the augmentations applied to an image would be inconsequential to support the claim that CASSLE reduces feature-space invariance to the augmentations used during training. I view the use of the InfoNCE objective to measure this as problematic, since this loss is sensitive to the choice of negative examples and the hyper-parameters selected (making it difficult to compare between different methods)."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700434970282,
                "cdate": 1700434970282,
                "tmdate": 1700434970282,
                "mdate": 1700434970282,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "2x26PcprSF",
            "forum": "WO4BCqEyWc",
            "replyto": "WO4BCqEyWc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3997/Reviewer_N37y"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3997/Reviewer_N37y"
            ],
            "content": {
                "summary": {
                    "value": "State-of-the-art approaches to self-supervised representation learning (SSL) optimize invariance-inducing objectives of representations to augmented views of an input observation, while preventing their collapse to a trivial solution. Effective optimization of these objectives reasonably results to information loss about the features excited by the augmentations in the representation space. These features, however, could be useful to maintain for some downstream prediction (potentially transfer learning) tasks. While the projector network is a common feature of these methods which mitigates this effect, the invariance still persists. The authors propose a simple intervention to typical SSL pipelines in order to further mitigate this effect: ***they suggest to condition the projector network with information about the particular augmentation used to derive a view of an observation***. Experiments on downstream transfer learning tasks with pretrained networks demonstrate an improvement in performance compared to baseline methods, targeted at the same issue. Analysis of representations and the projector demonstrate that augmentation information is indeed used and the method leads to more sensitivity to the variations induced by augmentations in earlier activations of the pretrained network. They also provide with ablation analyses of various implementation design choices."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The identified problem is known and significant for representation learning. The authors discuss fairly well the related literature and approaches to its solution.\n2. The idea is fairly novel, there have been some similar approaches that essentially \u201ccondition the projector network\u201d. Please, refer to Question 1.\n3. Nonetheless, their results generally convince that the detail is in the implementation level, rather than the conceptual.\n4. The paper is well-written and well-argumented.\n\nOverall, the paper convinces that conditioning the projector with augmentation information is a good direction towards creating more potent and transferable representations."
                },
                "weaknesses": {
                    "value": "1. Experiments remain relatively small-scale in dataset and model size. Especially, it would have been interesting to examine the effect of conditioning as pretraining data becomes abundant.\n2. CASSLE performs better (compared to AugSelf) for contrastive methods and BarlowTwins than others, i.e. BYOL and SimSiam. A discussion on why this happens can be interesting.\n3. Semi-supervised (few-shot classification) results are competitive, but weaker.\n4. Experiments on object detection task demonstrate a marginal improvement.\n\n5. The paper does not report confidence intervals of their results.\n6. Citation and bibliography style needs serious editing. Sometimes \u201cet al.\u201d is retained in bibliography, journals/conferences/proceedings are frequently missing and style is generally inconsistent."
                },
                "questions": {
                    "value": "1. Missing relevant approach to CASSLE is [1]. They provide with a method which can be perceived as a kind of conditioning to augmentation information.\n2. In *Related Work*, contrastive learning objectives usually refer to methods which prevent representational collapse by contrasting against negative pairs. Please clarify this distinction.\n3. In *Section 4.2*, an analysis of activation invariance is presented based on the InfoNCE loss. Which similarity function was used to compare earlier representations?\n4. In *Table 3*, how is the rank computed exactly?\n\n[1] Bhardwaj, Sangnie, et al. \"Steerable equivariant representation learning.\" arXiv preprint arXiv:2302.11349 (2023)."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3997/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3997/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3997/Reviewer_N37y"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3997/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698815905710,
            "cdate": 1698815905710,
            "tmdate": 1699636361671,
            "mdate": 1699636361671,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "gTcaEnf8nK",
                "forum": "WO4BCqEyWc",
                "replyto": "2x26PcprSF",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3997/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3997/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer N37y,\n\nThank you for your positive review of our work and all the comments. We would like to address your concerns below. If you have any additional questions, we are looking forward to answering them. \n\n\n**ad. W2: CASSLE performs better (compared to AugSelf) for contrastive methods and BarlowTwins than others, i.e. BYOL and SimSiam. A discussion on why this happens can be interesting.**\n\nThank you for this question. We suspect that this may be caused by the asymmetric representation used by BYOL and SimSiam, which distill the representations between the projector and additional predictor network. Observe that CASSLE and AugSelf also perform comparably on MoCo-v3, which also utilizes an additional predictor. It is worth noticing that CASSLE improves the performance of the vanilla versions of those methods as well.\nWe have added this remark to the revised version of the paper.\n\n**ad. W4: Experiments on object detection task demonstrate a marginal improvement.**\n\nWhile this improvement is small, CASSLE consistently improves over the performance of Vanilla and AugSelf models for both MoCo-v2 and SimCLR.\n\n**ad. W5: The paper does not report confidence intervals of their results.**\n\nWe did not report the confidence intervals of individual downstream task performance, as this would limit the readability of (already massive) Tables 1 and 7. We have compiled the linear evaluation results of different approaches on different downstream tasks in Figure 5 (Appendix C) of the revised manuscript and report the mean rank of each model (when ranked from worst to best performance on downstream tasks) and mean improvements of CASSLE and AugSelf over Vanilla approaches, both with 95% confidence intervals. \n\n**ad. W6: Citation and bibliography style needs serious editing. **\n\nThank you for pointing this out. We have updated the bibliography in the revised version of the paper in order to refer to journal /conference versions of the cited papers.\n\n**ad. Q1: Comparison to Bhardwaj et. al. [1]**\n\nThe authors of [1] use a mapping function that, similarly to our conditioned projector, acts on the combined information of image embeddings and augmentation parameters. This mapping allows for supplementing the training objective with equivariance regularization and leads to learning representations that are equivariant to augmentations applied to the images. There are two key differences between our work and [1]:\nCASSLE does not modify the objective function of the trained model. While we do not optimize for equivariance, we observe that increased sensitivity to augmentations emerges in CASSLE due to injecting augmentation information into the projector.\n[1] is evaluated in the supervised learning setting, whereas CASSLE is designed for self-supervised learning. We have included a reference to [1] in our related work section. \n\n**ad. Q2: Contrastive learning objectives nomenclature**\n\nWhile there obviously exists a distinction between contrastive, distillation-based, and CCA-based approaches, their objectives are often collectively referred to as contrastive objectives [2,3]. We have adopted this term in our work since CASSLE is applicable to different joint-embedding models regardless of their objective. We are open to your suggestions for a more accurate term describing those three kinds of objectives.\n\n**ad. Q3: Which similarity function was used to compare earlier representations in Section 4.2?**\n\nFor all stages of the network, we measure the cosine similarity of representations and calculate the InfoNCE loss subsequently.\n\n\n**ad. Q4: In Table 3, how is the rank computed exactly?**\n\nWe compare different variants of MoCo-v2+CASSLE on the same classification and regression tasks as in Section 4.1. We rank the models from best to worst performance on each task and report the average ranks in Table 3. The full results are in Table 12.\n \nWe hope that you find our answers satisfactory. We are looking forward to answering any of your additional questions.\n\nKind regards, \nAuthors"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700091537678,
                "cdate": 1700091537678,
                "tmdate": 1700091537678,
                "mdate": 1700091537678,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "byOpbQwy9B",
                "forum": "WO4BCqEyWc",
                "replyto": "gTcaEnf8nK",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3997/Reviewer_N37y"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3997/Reviewer_N37y"
                ],
                "content": {
                    "title": {
                        "value": "Answer to rebuttal"
                    },
                    "comment": {
                        "value": "Thank you for your responses! I choose to maintain my original positive assessment of the paper!"
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700691720304,
                "cdate": 1700691720304,
                "tmdate": 1700691720304,
                "mdate": 1700691720304,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Kxbr4bl8HO",
            "forum": "WO4BCqEyWc",
            "replyto": "WO4BCqEyWc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3997/Reviewer_hzhB"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3997/Reviewer_hzhB"
            ],
            "content": {
                "summary": {
                    "value": "Many self-supervised learning methods aim to learn augmentation-invariant representations. Such an approach could be harmful when a downstream task is sensitive to augmentation-aware information. To overcome this limitation of existing SSL methods, this paper proposes a simple yet effective approach that injects augmentation information (i.e., augmentation parameters) into the projection MLP used in the SSL framework. The approach shows superior performance over existing augmentation-aware information learning methods on ImageNet-100 experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This paper is generally well-written. It is easy to understand.\n- The idea is simple, intuitive, and seems to be widely applicable.\n- The proposed method, CASSLE, outperforms baselines (LooC, AugSelf, and AI) that also learn augmentation-aware information."
                },
                "weaknesses": {
                    "value": "**(1) Lack of comparison with recent augmentation-free SSL methods.** \\\nRecently, there have been proposed many augmentation-free self-supervised learning methods, including data2vec [1-2], I-JEPA [3], and Masked Image Modeling (MIM) [4-5]. The augmentation-free SSL methods do not use augmentation, in other words, they aim to learn full information about original images, rather than learning augmentation-invariant representations. Also, since they are often better than MoCo-v2 and SimCLR in various benchmarks (e.g., linear evaluation, fine-tuning, scalability), the authors should compare the proposed method with the methods.\n\n[1] Baevski et al., data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language, ICML 2022 \\\n[2] Baevski et al., Efficient Self-supervised Learning with Contextualized Target Representations for Vision, Speech and Language, 2022 \\\n[3] Assran et al., Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture, ICCV 2023 \\\n[4] He et al., Masked Autoencoders Are Scalable Vision Learners, CVPR 2022 \\\n[5] Xie et al., SimMIM: a Simple Framework for Masked Image Modeling, CVPR 2022\n\n**(2) Experimental results are not convincing.** \\\nThe performance improvement of CASSLE over AugSelf is marginal.\n\n**(3) Lack of novelty.** \\\nI feel that the proposed method is neither novel nor interesting. First, the goal of this paper has been widely studied via augmentation-aware objectives (e.g., AugSelf) and augmentation-free SSL methods (e.g., I-JEPA). Also, it is hard to find a strong advantage of the proposed idea compared to AugSelf. In my opinion, the choice between injection and prediction cannot make meaningful novelty."
                },
                "questions": {
                    "value": "Can the proposed method be applied to generative modeling like GAN training? It is worth noting that the main baseline, AugSelf, can be utilized for efficient GAN training [1].\n\n[1] Hou et al., Augmentation-Aware Self-Supervision for Data-Efficient GAN Training, NeurIPS 2023"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3997/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698839957503,
            "cdate": 1698839957503,
            "tmdate": 1699636361574,
            "mdate": 1699636361574,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "mzVXys6flt",
                "forum": "WO4BCqEyWc",
                "replyto": "Kxbr4bl8HO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3997/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3997/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply (1/2)"
                    },
                    "comment": {
                        "value": "Dear Reviewer hzhB,\n\nThank you for a detailed review of our work. We would like to address your concerns below. If you have any additional questions, we are looking forward to answering them. Please note that we have split our answer into two comments.\n\n**ad. W1: Lack of comparison with recent augmentation-free SSL methods.**\n\nOur research problem focuses on how augmentation-based SSL methods become invariant to augmentations. We are not aware of any evidence in the literature that MIM-based methods also suffer from this issue. As such, a comparison to them is not relevant in the context of CASSLE. \n\n**ad. W2: The performance improvement of CASSLE over AugSelf is marginal.**\n\nWhile it is true that CASSLE does not always achieve the best downstream performance, it does so in the majority of cases. We have compiled the linear evaluation results of different approaches on different downstream tasks (Tables 1 and 7) and ranked each approach from best to worst-performing in each downstream task (See Figure 5 in the revised manuscript). We find that:\n\n* CASSLE performs best in 54 out of 91 downstream tasks, i.e. 59.34% of cases where it was evaluated.\n* AugSelf performs best in 31 out of 91 downstream tasks, i.e. 34.07% of cases where it was evaluated.\n* AI performs best in 5 out of 16 downstream tasks, i.e. 31.25% of cases where it was evaluated.\n* IFM performs best in 1 out of 13 downstream tasks, i.e. 7.69% of cases where it was evaluated.\n* Vanilla performs best in 0 out of 91 downstream tasks, i.e. 0.00% of cases where it was evaluated.\n\n\n**ad. W3a: Lack of novelty. First, the goal of this paper has been widely studied via augmentation-aware objectives (e.g., AugSelf) and augmentation-free SSL methods (e.g., I-JEPA).**\n\nWe disagree that the existence of augmentation-free methods such as MIM and I-JEPA exhausts the need for augmentation-based methods and therefore, for research on augmentation awareness. We see several reasons for research into augmentation-aware approaches:\n* recent augmentation-based SSL approaches such as DINO-v2 [9] achieve state-of-the-art against augmentation-free methods\n* augmentation-based and augmentation-free methods learn different kinds of features - while augmentation-based methods lack spatial sensitivity which requires modeling the local structure within each image, MIM does not have good semantic alignment [8].\n* it should also be highlighted that augmentation-free models focus primarily on pretraining Vision Transformers (with some rare exceptions [7]). On the other hand, contrastive SSL continues to be the mainstream family of approaches for pretraining convolutional architectures such as ResNets, which continue to be widely used.\n\nThus, better understanding and improving these methods remains an important research problem.\n\n\n[1] Baevski et al., data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language, ICML 2022 \n\n[2] Baevski et al., Efficient Self-supervised Learning with Contextualized Target Representations for Vision, Speech and Language, 2022 \n\n[3] Assran et al., Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture, ICCV 2023 \n\n[4] He et al., Masked Autoencoders Are Scalable Vision Learners, CVPR 2022 \n\n[5] Xie et al., SimMIM: a Simple Framework for Masked Image Modeling, CVPR 2022\n\n[6] What Should Not Be Contrastive in Contrastive Learning Tete Xiao, Xiaolong Wang, Alexei A. Efros, Trevor Darrell\n\n[7] Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling Keyu Tian, Yi Jiang, Qishuai Diao, Chen Lin, Liwei Wang, Zehuan Yuan\n\n[8] Siamese Image Modeling for Self-Supervised Vision Representation Learning Chenxin Tao, Xizhou Zhu, Weijie Su, Gao Huang, Bin Li, Jie Zhou, Yu Qiao, Xiaogang Wang, Jifeng Dai; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 2132-2141\n\n[9] DINOv2: Learning Robust Visual Features without Supervision https://arxiv.org/abs/2304.07193\n\n[10] Hou et al., Augmentation-Aware Self-Supervision for Data-Efficient GAN Training, NeurIPS 2023"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700091838731,
                "cdate": 1700091838731,
                "tmdate": 1700091838731,
                "mdate": 1700091838731,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PGYTCR7uWr",
                "forum": "WO4BCqEyWc",
                "replyto": "Kxbr4bl8HO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3997/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3997/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Reply (2/2)"
                    },
                    "comment": {
                        "value": "**Note**: This is the second part of our reply.\n\n**ad. W3b: Also, it is hard to find a strong advantage of the proposed idea compared to AugSelf.**\n\nWe believe our approach has several advantages over AugSelf:\n* a stronger theoretical justification, which we added in the revised Section 3.2 of the paper\n* AugSelf is a multi-task learning method. It requires the user to choose the proportions of the SSL, and augmentation prediction losses. In fact, the authors of AugSelf show that different proportions are needed for different types of backbones and that the choice of augmentations whose parameters to predict during pretraining is not trivial. \n* On the other hand, CASSLE does not introduce new losses to the training. We also demonstrate that utilizing all possible augmentation information during pretraining yields the best downstream performance. \n* Finally, during linear evaluation, there are 12 cases of AugSelf performing worse than the vanilla approach (in MoCo-v1, MoCo-v3, SimCLR, SimSiam, Barlow Twins), whereas CASSLE does so in only 4 cases which are isolated to the BYOL model.\n\nThus, CASSLE requires simpler modifications to the extended SSL models, while offering a larger performance boost than AugSelf in the majority of cases. \n\n\n\nad. Q1: Can the proposed method be applied to generative modeling like GAN training?\n\nCertainly, CASSLE could be used for efficient GAN training in a similar manner to AugSelf as shown in [10]. In this case, we should combine CASSLE conditioning with a discriminator network. Verification of this approach is however out of the scope of this paper but seems to be an interesting future direction.\n\nWe hope that you find our answers satisfactory. We are looking forward to answering any of your additional questions.\n\nKind regards, \n\nAuthors\n\n[1] Baevski et al., data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language, ICML 2022 \n\n[2] Baevski et al., Efficient Self-supervised Learning with Contextualized Target Representations for Vision, Speech and Language, 2022 \n\n[3] Assran et al., Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture, ICCV 2023 \n\n[4] He et al., Masked Autoencoders Are Scalable Vision Learners, CVPR 2022 \n\n[5] Xie et al., SimMIM: a Simple Framework for Masked Image Modeling, CVPR 2022\n\n[6] What Should Not Be Contrastive in Contrastive Learning Tete Xiao, Xiaolong Wang, Alexei A. Efros, Trevor Darrell\n\n[7] Designing BERT for Convolutional Networks: Sparse and Hierarchical Masked Modeling Keyu Tian, Yi Jiang, Qishuai Diao, Chen Lin, Liwei Wang, Zehuan Yuan\n\n[8] Siamese Image Modeling for Self-Supervised Vision Representation Learning Chenxin Tao, Xizhou Zhu, Weijie Su, Gao Huang, Bin Li, Jie Zhou, Yu Qiao, Xiaogang Wang, Jifeng Dai; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023, pp. 2132-2141\n\n[9] DINOv2: Learning Robust Visual Features without Supervision https://arxiv.org/abs/2304.07193\n\n[10] Hou et al., Augmentation-Aware Self-Supervision for Data-Efficient GAN Training, NeurIPS 2023"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700091916717,
                "cdate": 1700091916717,
                "tmdate": 1700091916717,
                "mdate": 1700091916717,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "rFXg2aoHsH",
                "forum": "WO4BCqEyWc",
                "replyto": "Kxbr4bl8HO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3997/Reviewer_hzhB"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3997/Reviewer_hzhB"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response."
                    },
                    "comment": {
                        "value": "Thank you for your efforts and time in this response.\n\nI have thoroughly read the response, but I remain unconvinced of the effectiveness of the proposed method for representation learning; hence, I will maintain my score.\n\n**(W1) Necessity of comparison with augmentation-free methods is required.** I believe all self-supervised learning (SSL) methods share the common goal of \"learning better representations from unlabeled images\". In this context, CASSLE employs augmentation-aware information while the augmentation-free methods (e.g., MIM) take alternative approaches. From my perspective, \"learning augmentation-aware information\" appears to be an approach rather than a distinct problem. Since CASSLE and other SSL methods are addressing the same problem, a comparison with augmentation-free methods should be provided.\n\n**(W2) Marginal performance improvements.** I think the winning rate of ~50% with a marginal gap is not convincing.\n\n**(W3) Lack of novelty.** While I acknowledge the need for research on augmentation-aware approaches, this cannot be the reason of the unnecessary of the comparison with recent (augmentation-free) methods as previously mentioned. Also, there have been proposed many SSL methods to extract more (e.g., augmentation-aware) information (e.g., patch-level objective in iBOT and multi-crop in DINO) as well as augmentation-free methods (MIM, I-JEPA, ...). Compared to the recent progress in the SSL literature, I think that \"the proposed method is superior to AugSelf\" is insufficient. In addition, although I acknowledge several advantages of CASSLE over AugSelf, but the advantages seem not strong due to the unsatisfactory performance improvements."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3997/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700718163217,
                "cdate": 1700718163217,
                "tmdate": 1700718252970,
                "mdate": 1700718252970,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]