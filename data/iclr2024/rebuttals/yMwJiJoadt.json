[
    {
        "title": "TransNeXt: Aggregating Diverse Attentions in One Vision Model"
    },
    {
        "review": {
            "id": "qqcXWHOv1v",
            "forum": "yMwJiJoadt",
            "replyto": "yMwJiJoadt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4422/Reviewer_2Wbt"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4422/Reviewer_2Wbt"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents TransNeXt, a novel biomimetic design-based token mixer that combines fine-grained attention to neighboring tokens and coarse-grained attention to global features, taking into account spatial information aggregation. The proposed method incorporates learnable tokens that interact with conventional queries and keys, enabling the generation of affinity matrices that go beyond relying solely on the similarity between queries and keys. Additionally, the paper introduces Convolutional GLU, a channel mixer that bridges the gap between GLU and SE mechanisms, facilitating channel attention based on neighboring features.\n\nTo evaluate the effectiveness of the proposed method, experiments were conducted on various benchmark datasets. These include ImageNet for image classification tasks, COCO for object detection tasks, and ADE20K for semantic segmentation tasks. The results of these experiments demonstrate the efficacy of the proposed method in achieving state-of-the-art performance across these different tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The paper is commendably well-written, effectively presenting its ideas in a clear and coherent manner. The proposed framework is explained in a manner that facilitates easy comprehension for readers.\n\n2. The incorporation of pixel-focused attention, which encompasses both fine-grained local attention and coarse-grained global attention while engaging in competition, is a notable aspect of the research. Furthermore, the integration of query embedding and positional attention mechanisms within the pixel-focused attention framework enhances the generation of affinity matrices. This diversification of the affinity matrix generation process moves beyond a sole reliance on query-key similarity, enabling the aggregation of multiple attention mechanisms within a single attention layer.\n\n3. An additional contribution of the paper is the introduction of Convolutional GLU as a novel channel mixer. This component bridges the gap between GLU and SE mechanisms, facilitating channel attention based on neighboring features. The incorporation of Convolutional GLU adds a valuable element to the proposed method.\n\n4. The effectiveness of the proposed method is demonstrated through comprehensive experiments conducted on various benchmarks, including image classification, object detection, and semantic segmentation tasks. The obtained results showcase the state-of-the-art performance achieved across these diverse tasks, further validating the efficacy of the proposed approach."
                },
                "weaknesses": {
                    "value": "1. It is important to note that the combination of local attention and global attention mechanisms, as well as the incorporation of competition between different grained attention, have been explored in previous works on architecture design. These mechanisms are not novel and have been utilized in the context of related research. It is crucial to acknowledge the existing literature and the contributions made by previous studies in these areas.\n\n[1] Jiang et al.Dual Path Transformer with Partition Attention, in Arxiv 2023.\n\n2. While providing the theoretical complexity of the proposed Aggregated Attention is informative, it is indeed crucial to consider latency comparisons as well. Latency does not always correlate directly with model parameters and FLOPs (floating-point operations per second). Therefore, it is essential to conduct comparisons with different methods on the same device to assess the real-world performance in terms of latency. This empirical evaluation would provide valuable insights into the practical efficiency of the proposed method and enable a more comprehensive assessment of its performance.\n\n3. While this paper offers an in-depth examination of each element within the Aggregated Attention model, it appears to primarily combine existing technologies to enhance performance. Could you kindly summarize the three principal components of the proposed Aggregated Attention model?"
                },
                "questions": {
                    "value": "The running speed of the proposed method in terms of efficiency is an important aspect to consider. It would be valuable to compare the efficiency of the proposed method with related works to assess its performance in this regard. By conducting a comparative analysis, we can gain insights into how the proposed method fares in terms of running speed and efficiency when compared to existing approaches in the field."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4422/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698472358143,
            "cdate": 1698472358143,
            "tmdate": 1699636416549,
            "mdate": 1699636416549,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WyLm0T97EZ",
                "forum": "yMwJiJoadt",
                "replyto": "qqcXWHOv1v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4422/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4422/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer 4 #part.1"
                    },
                    "comment": {
                        "value": "We express our sincere gratitude to the reviewers for their recognition of our paper as \u201ccommendably well-written\u201d, acknowledging our PFA module as \u201ca notable aspect\u201d, and endorsing the Convolutional GLU as \u201ca valuable element\u201d. These affirmations serve as a significant motivation for our work, which has taken over a year to complete.\n\n**R1: Novelty and Distinction from Previous Work**\n\nWe highly appreciate the reviewers\u2019 insightful comment that the combination of global and local attention, as well as competition at different granularities, has been explored in previous work. This indicates that our previous description of our contributions did not adequately express the innovative aspects of our work compared to previous studies. We have revised these descriptions in the new version of our paper.\n\nFurthermore, we are grateful for the papers provided by the reviewers. Citing previous work is crucial, and we have referenced it in the revised version of our paper.\n\nWe believe that the contribution and novelty of our work lie in proposing an attention mechanism that aligns more closely with biological foveal vision than previous work. Its pixel-wise operation can effectively simulate the continuous movement of the eyeball, while each pixel possesses a comprehensive global perception. (In contrast to our approach, Focal attention operates on a window-wise manner, and its method based on window partitioning also deviates from foveal vision of human eyes.)\n\nThe motivation for proposing PFA is that previous work and experiments have found that deep networks with residual blocks behave like ensembles of shallower networks. This implies that cross-layer information exchange is not as effective as expected. In convolutional networks, this is manifested as the receptive field of deep networks based on stacked convolutional kernels not being as large as expected (hence the proposal of the super-large convolutional kernel scheme). So, what about ViT models? We found that in various efficient ViT models, the information exchange between cross-window or global and local features formed by stacking blocks is not as sufficient as expected. This can be clearly seen in our Appendix E based on Effective Receptive Field (ERF) visualization. Models based on local windows (Swin, CSwin) present unique blocky patterns in visual perception, and these pattern styles are closely related to their window design methods. This indicates that even after many layers of stacking, the exchange of information across windows is still insufficient, resulting in unnatural information mixing. Therefore, we believe that it is very important to implement a pixel-wise global-local perception mode that is closer to biological vision and can simulate eyeball movement in a single token mixer, so we proposed PFA and subsequent Aggregated Attention. The visualization based on ERF shows that our method forms a very natural and smooth information perception mode.\n\nIn Appendix B.2, we conducted a comparison with the super-large convolutional kernel scheme. It can be seen that our model has more advantages in large-scale image inference than RepLKNet(CVPR 2022) and SLaK(ICLR 2023), while the latter two show some limitations in large-size image inference. We believe that this comparative experiment is enlightening for the subsequent research of the computer vision community in related fields."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699844397833,
                "cdate": 1699844397833,
                "tmdate": 1700171859360,
                "mdate": 1700171859360,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "2bZ9R8Cafw",
                "forum": "yMwJiJoadt",
                "replyto": "qqcXWHOv1v",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4422/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4422/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer 4 #part.3"
                    },
                    "comment": {
                        "value": "**R3: Summary of Principal Components in the Model**\n\nOur manuscript and model encapsulate a series of innovative points accomplished over the span of more than a year. Our novel contributions primarily manifest in the following aspects:\n\n1. A biomimetic attention mechanism, Pixel-focused attention, which is highly consistent with the biological foveal visual system and capable of simulating continuous eye movements.\n2. A novel channel mixer, Convolutional GLU, which possesses channel attention based on nearest neighbor features.\n3. Length-scaled cosine attention, which enhances the extrapolation of existing attention mechanisms for multi-scale inputs.\n\nIn the review process, we found that each reviewer acknowledged the novelty and contribution of different components. Reviewer 1 recognized the novelty of the PFA module, Reviewer 3 acknowledged the contribution of Length-scaled cosine attention in multi-scale inference, and Reviewer 4 recognized the PFA module as \"a notable aspect\" and Convolutional GLU as \"a valuable element\". We express our sincere gratitude to the reviewers for their recognition of these contributions.\n\nThe main concern raised in the review comments was that the introduction of QLV and LKV attention operations in PFA made our method appear to be a combination of existing technologies. We believe that such concerns are reasonable and insightful. This indicates that our paper did not effectively explain the advantages and competitiveness of our method of integrating these three mechanisms. We have revised the relevant statements in the latest revision. Our integration method requires very little additional overhead, is highly efficient, and has novelty in the way it is integrated.\n\nSince the introduction of the Non-QKV mechanism in the Synthesizer(ICML 2021) paper, works such as QnA (CVPR 2022) and VOLO(TPAMI 2023)/Involution(CVPR 2021) have validated the feasibility of LKV and QLV mechanisms in visual models. We note that no previous work has attempted to unify QKV, LKV, and QLV, these three attention mechanisms, in a single attention layer (we believe that merely implementing through block stacking or ensemble methods would be trivial and lack novelty, as these methods would also be affected by the potential depth degradation of residual connections). We consider this work to be the first attempt to unify these three attention mechanisms in a single attention layer.\n\nOur design of PFA serves as a promising foundation for unifying these three attention mechanisms. It inherently has a sliding window attention branch, and reusing this branch reduces the overhead required by the QLV mechanism to $HWk^2C$. Our method of introducing the LKV mechanism is more efficient, requiring only the addition of a query embedding to all queries to achieve the sum of the LK affinity matrix and the QK affinity matrix, with its additional overhead being negligible. In terms of experimental results, enhancing PFA to Aggregated Attention requires only about **0.2%(of base model) to 0.3%(of  micro model)** of the additional computational consumption in the entire model, but the improvement is significant, achieving a very cost-effective trade-off. From this perspective, our method of unifying QKV, QLV, and LKV attentions does not employ a trivial approach, and is both efficient and has its innovative aspects. We believe this is a successful attempt. \n\n| Models | Params | FLOPs\uff08G\uff09 | IN-1K | IN-A |\n| --- | --- | --- | --- | --- |\n|TransNeXt-Micro(FPA)|12.78|2.65|81.8|26.9|\n|TransNeXt-Micro(AA)|12.81 (+0.2%)|2.66(+0.3%)|82.5(+0.7%)|29.9(+3.0%)|\n\nWe once again express our gratitude to the reviewers for their meticulous review and insightful suggestions regarding our paper."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699844545858,
                "cdate": 1699844545858,
                "tmdate": 1700172442189,
                "mdate": 1700172442189,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "61SwtnfdwY",
            "forum": "yMwJiJoadt",
            "replyto": "yMwJiJoadt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4422/Reviewer_vXBQ"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4422/Reviewer_vXBQ"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the authors introduce modifications to the Vision Transformers (ViTs) design, chiefly, the Aggregated Attention mechanism and the Convolutional Gated Linear Unit (GLU). The Aggregated Attention is biomimetic and facilitates each token to attend to both nearest neighbor features and global features finely and coarsely, respectively. It harmoniously integrates multiple attention mechanisms within a single layer, eradicating the necessity for alternating stacking of various token mixers. This novel attention mechanism also imbues the model with the richness of pixel-focused attention and relative positional bias, improving the models' ability to aggregate essential spatial information and enhance their translational equivariance. The paper also proposes a Convolutional GLU, a novel channel mixer adept for image-related tasks, which bridges the gap between the conventional GLU and Squeeze-and-Excitation (SE) mechanisms. It leverages local feature-based channel attention, bolstering the model's robustness and local modeling capabilities. This architecture exhibits commendable performance, standing at the pinnacle across multiple model sizes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed Length-scaled cosine attention improves existing attention mechanisms by enhancing extrapolation capabilities, enabling models to adeptly manage and interpret multi-scale image inputs, fostering better adaptability and effectiveness in processing diverse image sizes and scales."
                },
                "weaknesses": {
                    "value": "1. Novelty is limited as the method proposed, where queries attend to both fine-grained and coarse-grained information simultaneously, has been extensively studied previously [1,2].\n2. More ablation studies are needed. In Table 4, step 5, the paper only reports performance gains of PFA over SRA. Since PFA is central to this paper, it would be insightful to see if replacing SRA with Cross-Shaped Window Self-Attention from CSWin[3], Focal Self-attention[1], or Shunted Attention[4] would yield higher performance.\n\n[1] Yang, J., Li, C., Zhang, P., Dai, X., Xiao, B., Yuan, L., & Gao, J. (2021). Focal attention for long-range interactions in vision transformers. Advances in Neural Information Processing Systems, 34, 30008-30022.\n[2] Chen, M., Lin, M., Li, K., Shen, Y., Wu, Y., Chao, F., & Ji, R. (2023, June). Cf-vit: A general coarse-to-fine method for vision transformer. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 37, No. 6, pp. 7042-7052). \n[3] Dong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., ... & Guo, B. (2022). Cswin transformer: A general vision transformer backbone with cross-shaped windows. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 12124-12134).\n[4] Ren, S., Zhou, D., He, S., Feng, J., & Wang, X. (2022). Shunted self-attention via multi-scale token aggregation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 10853-10862)."
                },
                "questions": {
                    "value": "Please refer weakness"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4422/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4422/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4422/Reviewer_vXBQ"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4422/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698702128432,
            "cdate": 1698702128432,
            "tmdate": 1699636416474,
            "mdate": 1699636416474,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "L2tsuiKIye",
                "forum": "yMwJiJoadt",
                "replyto": "61SwtnfdwY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4422/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4422/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer 3 #part.1"
                    },
                    "comment": {
                        "value": "We express our heartfelt gratitude to the reviewers for their excellent evaluation of our paper\u2019s Presentation and for their recognition of the contribution and potential impact of Length-scaled cosine attention. This serves as a significant encouragement for our work that has spanned over a year.\n\n**R1: Novelty and Distinction from Previous Work**\n\nWe highly appreciate the insightful comments from the reviewers that the concept of queries attending to both fine-grained and coarse-grained information simultaneously has been extensively studied previously. This indicates that our previous description of our contributions did not adequately express the innovative aspects of our work compared to previous studies. We have revised these descriptions in the new version of our paper.\n\nWe believe that the contribution and novelty of our work lie in proposing an attention mechanism that aligns more closely with biological foveal vision than previous work. Its pixel-wise operation can effectively simulate the continuous movement of the eyeball, while each pixel possesses a comprehensive global perception. \n\nIn contrast to our approach, Focal Transformer still operates on a window-wise manner. The visual perception of queries located at the window\u2019s edge is unnatural compared to biological foveal vision. They cannot maintain fine-grained attention to all their nearest neighbor tokens, and fine-grained features are not centered on the query at the window\u2019s edge. Imagining the effect of changing the black area of Local Attention in Fig.1 of our paper to a blurred area might be helpful. We believe that this trace of window partitioning could introduce some artifacts into the model\u2019s perception. \n\nFurthermore, the CF-VIR paper recommended by the reviewer is commendable. They further subdivide the image patches of the main part of the picture into fine-grained divisions. Although this combination of coarse-grained and fine-grained does not involve the concept of biological foveal vision, it is still an enlightening method. We would like to express our sincere gratitude to the reviewer for recommending relevant papers.\n\nThe motivation for proposing PFA is that previous work and experiments have found that deep networks with residual blocks behave like ensembles of shallower networks. This implies that cross-layer information exchange is not as effective as expected. In convolutional networks, this is manifested as the receptive field of deep networks based on stacked convolutional kernels not being as large as expected (hence the proposal of the super-large convolutional kernel scheme). So, what about ViT models? We found that in various efficient ViT models, the information exchange between cross-window or global and local features formed by stacking blocks is not as sufficient as expected. This can be clearly observed in our Appendix E based on Effective Receptive Field (ERF) visualization. Models based on local windows (Swin, CSWin) present unique blocky patterns in visual perception, and these pattern styles are closely related to their window design methods. This indicates that even after many layers of stacking, the exchange of information across windows is still insufficient, resulting in unnatural information mixing. Therefore, we believe that it is very important to implement a pixel-wise global-local perception mode that is closer to biological vision and can simulate eyeball movement in a single token mixer, so we proposed PFA and subsequent Aggregated Attention. The visualization based on ERF shows that our method forms a very natural and smooth information perception mode.\n\nRegrettably, the official model weights for the Focal-Transformer are no longer accessible, and there is a lack of third-party backups. Consequently, we were unable to include it in the ERF visualization comparison. As a compensation, we have visualized the ERF of CSWin, recommended by the reviewer, in our latest revision. It can be observed that in Stage 3, even after 21 layers of stacking, CSWin still exhibits its unique cross-shaped pattern.\n\nMoreover, in Appendix B.2, we conducted a comparison with the super-large convolutional kernel scheme. It can be seen that our model has more advantages in large-scale image inference than RepLKNet(CVPR 2022) and SLaK(ICLR 2023), while the latter two show some limitations in large-size image inference. We believe that this comparative experiment is enlightening for the subsequent research of the computer vision community in related fields."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699844794719,
                "cdate": 1699844794719,
                "tmdate": 1700171981529,
                "mdate": 1700171981529,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "YmGHpCV8ux",
                "forum": "yMwJiJoadt",
                "replyto": "61SwtnfdwY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4422/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4422/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer 3 #part.2"
                    },
                    "comment": {
                        "value": "**R2 Ablation Experiment of Replacing the Baseline Model**\n\nWe fully understand the reviewer\u2019s concern about using PVTv2 as a potentially weak baseline. We used CSwin-T to change the token mixer of stages 1-3 to PFA and changed the token mixer of the fourth stage to MHSA (using the P-P-P-M structure to keep consistent with our TransNeXt\u2019s A-A-A-M). Under the condition that the dimensions, structures, and other aspects of the model remain consistent with CSWin-T, we conducted a direct comparison with CSWin on ImageNet-1K. After the replacement, it can be observed that our PFA method can improve the accuracy by 0.5%.\n\n| Models | Channels|Blocks|Params(M) | FLOPs\uff08G\uff09 |Top-1 (%)|\n| --- | --- | --- | --- |--- |--- |\n|CSWin|[64,128.256,512]|[1,2,21,1]|23|4.3|82.8|\n|CSWin-PFA|[64,128.256,512]|[1,2,21,1]|24|4.8|83.3|\n\nWe once again express our gratitude to the reviewers for their meticulous review and insightful suggestions regarding our paper."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699844811698,
                "cdate": 1699844811698,
                "tmdate": 1699943092523,
                "mdate": 1699943092523,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tm4dMBwQvl",
            "forum": "yMwJiJoadt",
            "replyto": "yMwJiJoadt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4422/Reviewer_4sZd"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4422/Reviewer_4sZd"
            ],
            "content": {
                "summary": {
                    "value": "The author introduces a novel attention mechanism, pixel-focused attention (PFA), inspired by biomimetic design principles. This mechanism effectively captures both fine-grained local and coarse-grained global features, eliminating the need for alternately stacking token mixers or incorporating convolution in attention operations, as commonly done in existing methods. Building upon PFA, the author introduces enhanced modules, such as Conv-GLU, Learnable LKV, QLV, and others, to establish the new ViT backbone, TransNeXt, tailored for visual tasks. The effectiveness of TransNeXt is substantiated through comprehensive experiments."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is technically sound;\n- The representation of the paper is good;\n- The experiments conducted are comprehensive and fully validate the effectiveness of the proposed method."
                },
                "weaknesses": {
                    "value": "My primary reservation regarding the acceptance of this paper pertains to its limited novelty. As depicted in Table 11, many of the modules or concepts presented in the paper have previously been explored in existing research. For instance, the approach to fine-grained local features and coarse-grained global features rooted in biological visual design has been introduced by Focal-Transformer. The non-QKV strategy has already been employed in works like Involution and VOLO. Additionally, the ConvGLU module seems to be a marginal enhancement to the existing blocks illustrated in Figure 3. While I recognize that there might be subtle, optimized implementation details unique to this paper, the overarching narrative gives the impression that TransNeXt is primarily an assembly of modules sourced from other research papers."
                },
                "questions": {
                    "value": "See weakness part."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N.A."
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4422/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698731187699,
            "cdate": 1698731187699,
            "tmdate": 1699636416405,
            "mdate": 1699636416405,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vr3qzinbh8",
                "forum": "yMwJiJoadt",
                "replyto": "tm4dMBwQvl",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4422/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4422/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer 2 #part.1"
                    },
                    "comment": {
                        "value": "We are deeply appreciative of the reviewers for their meticulous reading of our paper. We are gratified that they acknowledged the representation of our paper and deemed our experiments comprehensive, fully validating the effectiveness of our proposed method. We understand the reviewers\u2019 concerns about the novelty of our method, which is a valuable insight. This indicates that our writing did not effectively reflect the novelty of our method compared to previous work, causing confusion for the reviewers. We have revised these descriptions in the updated version of our paper.\n\nOur manuscript and model encapsulate a series of innovative points accomplished over the span of more than a year. In essence, our paper is a comprehensive redesign of the original baseline, integrating these innovative points. In the review process, we found that each reviewer acknowledged the novelty and contribution of different components. Reviewer 1 recognized the novelty of the PFA module, Reviewer 3 acknowledged the contribution of Length-scaled cosine attention in multi-scale inference, and Reviewer 4 recognized the PFA module as \"a notable aspect\" and Convolutional GLU as \"a valuable element\". We express our sincere gratitude to the reviewers for their recognition of these contributions.\n\nTherefore, we kindly request the opportunity to elaborate on the novelty of our work and its distinction from previous works in the following sections.\n\n**R1: Novelty and Motivation of FPA and its Distinction from Previous Work**\n\nWe believe that the contribution and novelty of our work lie in proposing an attention mechanism that aligns more closely with biological foveal vision than previous work. Its pixel-wise operation can effectively simulate the continuous movement of the eyeball, while each pixel possesses a comprehensive global perception. \n\nIn contrast to our approach, Focal Transformer still operates on a window-wise manner. The visual perception of queries located at the window\u2019s edge is unnatural compared to biological foveal vision. They cannot maintain fine-grained attention to all their nearest neighbor tokens, and fine-grained features are not centered on the query at the window\u2019s edge. Imagining the effect of changing the black area of Local Attention in Fig.1 of our paper to a blurred area might be helpful. We believe that this trace of window partitioning could introduce some artifacts into the model\u2019s perception. \n\nThe motivation for proposing PFA is that previous work and experiments have found that deep networks with residual blocks behave like ensembles of shallower networks. This implies that cross-layer information exchange is not as effective as expected. In convolutional networks, this is manifested as the receptive field of deep networks based on stacked convolutional kernels not being as large as expected (hence the proposal of the super-large convolutional kernel scheme). So, what about ViT models? We found that in various efficient ViT models, the information exchange between cross-window or global and local features formed by stacking blocks is not as sufficient as expected. This can be clearly observed in our Appendix E based on Effective Receptive Field (ERF) visualization. Models based on local windows (Swin, CSWin) present unique blocky patterns in visual perception, and these pattern styles are closely related to their window design methods. This indicates that even after many layers of stacking, the exchange of information across windows is still insufficient, resulting in unnatural information mixing. Therefore, we believe that it is very important to implement a pixel-wise global-local perception mode that is closer to biological vision and can simulate eyeball movement in a single token mixer, so we proposed PFA and subsequent Aggregated Attention. The visualization based on ERF shows that our method forms a very natural and smooth information perception mode.\n\nRegrettably, the official model weights for the Focal-Transformer are no longer accessible, and there is a lack of third-party backups. Consequently, we were unable to include it in the ERF visualization comparison. As a compensation, we have visualized the ERF of CSWin in our latest revision. It can be observed that in Stage 3, even after 21 layers of stacking, CSWin still exhibits its unique cross-shaped pattern.\n\nMoreover, in Appendix B.2, we conducted a comparison with the super-large convolutional kernel scheme. It can be seen that our model has more advantages in large-scale image inference than RepLKNet(CVPR 2022) and SLaK(ICLR 2023), while the latter two show some limitations in large-size image inference. We believe that this comparative experiment is enlightening for the subsequent research of the computer vision community in related fields."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699844592844,
                "cdate": 1699844592844,
                "tmdate": 1700171909071,
                "mdate": 1700171909071,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ysH3TdRDpN",
            "forum": "yMwJiJoadt",
            "replyto": "yMwJiJoadt",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4422/Reviewer_hJkq"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4422/Reviewer_hJkq"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a novel attention mechanism, several new components, and a transformer architecture. The motivation for the attention mechanism is combining global perception with local recognition. Reasonable results are reported."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. Some of the proposed components are novel, including PFA and Activate and Pool.\n\n2. Though some other proposed structures are nothing interesting, they work well (e.g., ConvGLU)."
                },
                "weaknesses": {
                    "value": "1. Some claims are inappropriate or wrong\n\n1.1 [it attains a box mAP of 55.1 using the DINO detection head, outperforming ConvNeXt-L ...] They are not comparable. ConvNeXt used UPerNet.\n\n1.2 [This is the first token mixer that simultaneously satisfies fine-grained perception near the focus, coarse-grained global perception at a distance, and pixel-wise translational equivariance] Very large (larger than 51x51) and sparse convolution is exactly a token mixer with such properties so it is inappropriate to claim the proposed token mixer as the first. The magnitudes of outer parameters of a very large convolution kernel are small and sparse while the central parameters are dense. Please refer to the paper of [SLaK].\n\n1.3 [More elegant design] Compared to what? Is adding a depthwise 3x3 elegant?\n\n2. I seriously doubt the efficiency of the proposed structure. It is too complicated and the implementation of PFA may require naive indexing operations, which are extremely inefficient. The actual throughput and latency test results and the comparisons with other competitors are missing, which is unacceptable.\n\n3. I admit PFA is novel but do not take Aggregated Attention as a significant contribution since Aggregated Attention = PFA + query embedding + positional attention, and the latter two are common practices.\n\nIn summary, I recommend rejecting this paper because it reads like yet another customized attention (which is neither simple nor efficient) plus some common practices borrowed from other works. And though the results on ImageNet-1K look promising, no results with larger models nor bigger data are reported."
                },
                "questions": {
                    "value": "It is claimed that the proposed pixel-focused attention \"possesses visual priors comparable to convolution.\" So why not just use convolution? Discussions and comparisons are missing."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4422/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698825986687,
            "cdate": 1698825986687,
            "tmdate": 1699636416330,
            "mdate": 1699636416330,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "aJzQO9O4N5",
                "forum": "yMwJiJoadt",
                "replyto": "ysH3TdRDpN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4422/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4422/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer 1"
                    },
                    "comment": {
                        "value": "We are deeply appreciative of the reviewers\u2019 meticulous reading of our paper, and we find many of their suggestions to be insightful and helpful. We are grateful for the reviewers\u2019 recognition of the novelty of our proposed Pixel-Focused Attention (PFA) and Activate and Pool modules.\n\n**Q1:** Why not use convolution? Why propose pixel-focused attention \u201cpossesses visual priors comparable to convolution\u201d?\n\nWe appreciate the reviewer's insightful question, which touches upon the underlying motivation for our Pixel-Focused Attention (PFA) design.\n\nThe primary motivations for proposing pixel-focused attention are twofold: \n1. To circumvent the depth degradation resulting from the stacking of residual modules, which hampers cross-layer information exchange to a degree less effective than anticipated. \n2. On the basis of satisfying (1), to propose an token mixer that aligns more closely with the biological vision system, capable of simulating the continuous movement of the eyeball, ensuring that the visual perception of the query at each position on the feature map remains highly consistent with biological foveal vision. \n\nFrom this perspective, convolution is closer to the working mode of the eyeball than various local window-based attentions (Swin, Focal Transformer), which we believe is the advantage of convolution in visual priors. However, the depth degradation of residual connections affects convolution by making its effective receptive field smaller than expected, hence various large convolution kernel models (RepLKNet(CVPR 2022), SLaK(ICLR 2023)) have been proposed to address this issue. The results under ERF visualization can prove the effectiveness of the large convolution kernel strategy.\n\nHowever, we observed a significant performance decay in large-scale image inference for these models based on the large convolution kernel strategy pre-trained at a resolution of $224^2$ (in Appendix B.2). The top-1 accuracy of RepLKNet-31B at a resolution of $640^2$ is only 0.9%, which to some extent shows the limitations of the large convolution kernel strategy in broader application scenarios. Therefore, we proposed pixel-focused attention, attempting to approximate the priors of convolution in biomimetic vision from the perspective of the attention mechanism. We proposed length-scaled cosine attention and used a position encoding scheme with extrapolation capability, achieving better large-scale image extrapolation performance than pure convolution networks. PFA and AA also have a linear complexity inference mode comparable to convolution. We compared TransNeXt-T with ConvNeXt-B and RepLKNet-B in Appendix B.2 and discussed this point. In the latest revision, we added SLaK-S as a comparison (thanks again to the reviewer for recommending reading the SLaK paper) Our observations indicate that TransNeXt-T significantly outperforms pure convolution models in large-scale image inference, demonstrating that this is another effective approach to addressing the two motivations mentioned above. Furthermore, the performance degradation of large convolution kernel models such as RepLKNet and SLaK, as reported in our experiments during large-scale image inference, warrants attention from the research community. These findings may contribute to future research in this field.\n\n**R1.2:** The statement \u201cThis is the first token mixer that\u2026\u201d is inappropriate.\n\nWe appreciate the reviewer\u2019s careful reading. Indeed, we overlooked the blurring effect caused by the sparsity of the large convolution kernel periphery during our writing. Therefore, such a statement is not rigorous within the scope of the token mixer. We have used a more precise description in the new revision.\n\n**R1.1:** \"It attains a box mAP of 55.1 using the DINO detection head, outperforming ConvNeXt-L \u2026\" They are not comparable. ConvNeXt used UPerNet.\n\nWe value the reviewer\u2019s attention to data details, which is commendable. Indeed, the ConvNeXt paper did not provide results for the DINO detection head. Our data references the results of the open-source weight report from the detrex project: https://github.com/IDEA-Research/detrex/tree/main/projects/dino#pretrained-dino-with-convnext-backbone . The box map of DINO-ConvNeXt-Large-384-4scale using IN-1K is 53.4, which is correct. For Swin using the DINO detection head, we referenced the open-source weight results from mmdet as much as possible, as we also used mmdet for training, to maintain consistency."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699843871578,
                "cdate": 1699843871578,
                "tmdate": 1700172328972,
                "mdate": 1700172328972,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Hpd4igaQ4V",
                "forum": "yMwJiJoadt",
                "replyto": "ysH3TdRDpN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4422/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4422/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer 1 #part.2"
                    },
                    "comment": {
                        "value": "**R1.3:** [More elegant design] Compared to what? Is adding a depthwise 3x3 elegant?\n\nWe appreciate the reviewer\u2019s inquiry into the details of our paper. The term \u201cmore elegant design\u201d refers to our ConvGLU in comparison to the Feedforward using the SE mechanism. We have noticed that some studies, such as FAN-ViT-SE(ICML 2022) and MaxViT(ECCV 2022) (used in their MBConv block), still employ the SE mechanism to enhance channel attention in models, thereby improving robustness. The SE mechanism uses Global Average Pooling for downsampling, an extremely coarse-grained solution that subjects all pixel tokens on the feature map to the same gating signal, regardless of their individual information. In contrast, we incorporate a 3x3 convolution into GLU without downsampling, allowing each token on the feature map to receive a distinct gating signal. Moreover, these gating signals are generated based on the nearest neighbor features around the token. Compared to the self-gating mode of GLU itself, the gating signals of ConvGLU have a larger perception, hence more effective decision-making can be expected. Additionally, it has fewer FLOPs than ConvFFN, which incorporates convolution into FFN, and it exhibits stronger performance and robustness. It also possesses the ability to serve as position encoding. In Appendix B.4, we explore three alternative methods of integrating 3x3 convolution in GLU, underscoring the significance of the placement of 3x3 convolution.  It can be observed that our ConvGLU design is currently optimal.\n\nIn summary, we believe this is a design that can effectively meet the diverse requirements of contemporary ViT and is sufficiently simple.\n\n**R2:** Throughput and Latency Test\n\nCurrently, we have only implemented native CUDA code without extensive optimization. Consequently, it is not as efficient as highly optimized dense GPU operators. We tested the model\u2019s throughput on a V100 16G with FP32 precision at a batch size of 64, and the model\u2019s latency at a batch size of 10. Our biomimetic vision implementation based on the sliding window is more efficient than the Focal Transformer method. Moreover, compared to previous models such as MaxViT (ECCV 2022), QuadTree (ICLR 2022), and BiFormer (CVPR 2023), our model achieved competitive TOP-1 accuracy results under similar throughput conditions.\n\n| Models | Params | FLOPs\uff08G\uff09 | TOP-1 | throughputs(img/s) | latency(ms) |\n| --- | --- | --- | --- | --- | --- |\n| BiFormer-T | 13.1 | 2.2 | 81.4 | 828 | 1.73 |\n| Swin-T | 28.3 | 4.5 | 81.2 | 790 | 1.44 |\n| ConvNeXt-T | 28.6 | 4.5 | 82.3 | 779 | 1.42 |\n| QuadTree-B-b1 | 13.6 | 2.3 | 80.0 | 663 | 2.85 |\n| **TransNeXt-Micro** | 12.8 | 2.7 | **82.5** | 641 | 3.95 |\n| Swin-S | 49.6 | 8.7 | 83.1 | 460 | 2.51 |\n| MaxViT-Tiny | 30.9 | 5.6 | 83.4 | 459 | 2.77 |\n| ConvNeXt-S | 50.2 | 8.7 | 83.1 | 441 | 2.54 |\n| **TransNeXt-Tiny** | 28.2 | 5.7 | **84.0** | 413 | 3.98 |\n| BiFormer-S | 25.5 | 4.5 | 83.8 | 396 | 3.62 |\n| QuadTree-B-b2 | 24.2 | 4.5 | 82.7 | 361 | 5.51 |\n| Focal-Transformer-T | 29.1 | 4.9 | 82.2 | 337 | 3.3 |\n| Swin-B | 87.8 | 15.4 | 83.5 | 292 | 3.73 |\n| ConvNeXt-B | 88.6 | 15.4 | 83.8 | 290 | 3.8 |\n| MaxViT-Small | 68.9 | 11.7 | 84.4 | 273 | 4.2 |\n| BiFormer-B | 56.8 | 9.8 | 84.3 | 241 | 4.7 |\n| QuadTree-B-b3 | 46.3 | 7.8 | 83.7 | 238 | 8.9 |\n| **TransNeXt-Small** | 49.7 | 10.3 | **84.7** | 214 | 6.85 |\n| Focal-Transformer-S | 51.1 | 9.1 | 83.5 | 203 | 6.46 |\n| QuadTree-B-b4 | 64.2 | 11.5 | 84.0 | 166 | 13.17 |\n| **TransNeXt-Base** | 89.7 | 18.4 | **84.8** | 151 | 7.38 |\n| Focal-Transformer-B | 89.8 | 16 | 83.8 | 145 | 7.63 |\n| MaxViT-Base | 119.5 | 24 | 84.9 | 144 | 8.13 |\n\nFurthermore, the speed of TransNeXt is expected to improve with more engineering efforts. We will continue to provide more efficient operator optimizations in the future to enhance the competitiveness of TransNeXt."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699843948086,
                "cdate": 1699843948086,
                "tmdate": 1699843948086,
                "mdate": 1699843948086,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "c66EtiYrlg",
                "forum": "yMwJiJoadt",
                "replyto": "ysH3TdRDpN",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4422/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4422/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer 1 #part.3"
                    },
                    "comment": {
                        "value": "**R3:** PFA is novel, but Aggregated Attention = PFA + query embedding + positional attention.\n\nWe would like to express our gratitude once again to the reviewers for recognizing the novelty of our Pixel-Focused Attention (PFA) module. We believe that such concerns are reasonable and insightful. This indicates that our paper did not effectively explain the advantages and competitiveness of our method of integrating these three mechanisms. We have revised the relevant statements in the latest revision. Our integration method requires very little additional overhead, is highly efficient, and has novelty in the way it is integrated.\n\nSince the introduction of the Non-QKV mechanism in the Synthesizer(ICML 2021) paper, works such as QnA (CVPR 2022) and VOLO(TPAMI 2023)/Involution(CVPR 2021) have validated the feasibility of LKV and QLV mechanisms in visual models. We note that no previous work has attempted to unify QKV, LKV, and QLV, these three attention mechanisms, in a single attention layer (we believe that merely implementing through block stacking or ensemble methods would be trivial and lack novelty, as these methods would also be affected by the potential depth degradation of residual connections). We consider this work to be the first attempt to unify these three attention mechanisms in a single attention layer.\n\nOur design of PFA serves as a promising foundation for unifying these three attention mechanisms. It inherently has a sliding window attention branch, and reusing this branch reduces the overhead required by the QLV mechanism to $HWk^2C$. Our method of introducing the LKV mechanism is more efficient, requiring only the addition of a query embedding to all queries to achieve the sum of the LK affinity matrix and the QK affinity matrix, with its additional overhead being negligible. In terms of experimental results, enhancing PFA to Aggregated Attention requires only about **0.2%(of base model) to 0.3%(of  micro model)** of the additional computational consumption in the entire model, but the improvement is significant, achieving a very cost-effective trade-off. From this perspective, our method of unifying QKV, QLV, and LKV attentions does not employ a trivial approach, and is both efficient and has its innovative aspects. We believe this is a successful attempt. \n\n| Models | Params | FLOPs\uff08G\uff09 | IN-1K | IN-A |\n| --- | --- | --- | --- | --- |\n|TransNeXt-Micro(FPA)|12.78|2.65|81.8|26.9|\n|TransNeXt-Micro(AA)|12.81 (+0.2%)|2.66(+0.3%)|82.5(+0.7%)|29.9(+3.0%)|\n\nWe once again express our gratitude to the reviewers for their meticulous review and insightful suggestions regarding our paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4422/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699843988258,
                "cdate": 1699843988258,
                "tmdate": 1699844989369,
                "mdate": 1699844989369,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]