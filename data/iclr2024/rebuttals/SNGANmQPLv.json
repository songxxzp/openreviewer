[
    {
        "title": "Understanding Multimodal Instruction Format for In-context Learning"
    },
    {
        "review": {
            "id": "DdECDcxoL9",
            "forum": "SNGANmQPLv",
            "replyto": "SNGANmQPLv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8964/Reviewer_y2C2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8964/Reviewer_y2C2"
            ],
            "content": {
                "summary": {
                    "value": "This work explores the impact of different instruction formats on the in-context learning ability of MLLM. The authors propose a UMIT method, which introduces task definition and transfers the multimodal instructions of diverse tasks in a unified style. A retrieval-based approach is also proposed to enhance exemplars sampling."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. This paper is the first to investigate the impact of different instruction formats on the in-context learning ability of MLLM.\n2. The proposed UMIT outperforms OpenFlamingo and Otter in terms of performance on the MME Benchmark and SEED-Bench."
                },
                "weaknesses": {
                    "value": "1. This work mainly focuses on instruction formats, lacks innovation, and fixed instruction formats are difficult to generalize to open-domain tasks.\n2. When testing, does UMIT require using ChatGPT to obtain the task definition for each new sample? This can result in significant inference costs for the unseen task.\n3. The description of the details of UMIT, especially the use of symbols, is somewhat confusing. For example, what does the text encoder encode in exemplar sampling? And where does X_{instruct}^{i} come from in section 2.5?\n4. The experimental results on SEED-Bench show a significant improvement when training OpenFlamingo and Otter on the data collected by the authors. In contrast, the gains from changing the format seem less pronounced. This raises a question of whether the role of data diversity is much greater than the task definition proposed by the author.\n5. Some experimental results on VizWiz contradict the author's conclusions and should be analyzed in more detail. For example, in Table 4, \"DEIQA (F3) mixed\" performs worse than \"DEIQA (F3) random\". And in Table 5, \"vqa+diff\" performs worse than \"vqa+same\"."
                },
                "questions": {
                    "value": "1. In Otter / MMICL, what is the difference between \"instance-level instruction\" and \"question\"?\n2. Do other ICL methods harm zero-shot performance?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8964/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698645179936,
            "cdate": 1698645179936,
            "tmdate": 1699637128595,
            "mdate": 1699637128595,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "vt2sz62vSd",
                "forum": "SNGANmQPLv",
                "replyto": "DdECDcxoL9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8964/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8964/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response for Reviewer y2C2 (Part 1)"
                    },
                    "comment": {
                        "value": "Dear Reviewer y2C2:\n\nThank you for your comments. We provide discussions and explanations about your concerns as follows.\n\n**Q1: This work mainly focuses on instruction formats, lacks innovation, and fixed instruction formats are difficult to generalize to open-domain tasks.**\n\n**A1:** We acknowledge your concerns regarding the generalization of our method to open-domain tasks. Here, we categorize the open-domain tasks you mentioned into two types. The first type belongs to traditional vision-language tasks, such as open-domain Visual Question Answering (VQA) tasks like GQA [16], VQAv2 [17], etc. Our proposed UMIT can generalize to these tasks, which has been demonstrated by our results shown in Table 6.\nThe second type is real-world tasks. We must acknowledge that, since real-world instructions are diverse and challenging to describe in terms of task definition, our framework currently has limitations when extending to such tasks. However, we contend that, at present, existing MLLMs other than GPT-4V lack in-context learning abilities for real-world tasks. Simultaneously, there is a lack of a reasonable benchmark to effectively evaluate the in-context learning ability of MLLMs in real-world tasks and provide quantifiable results. Therefore, this should be considered a common limitation in the current field of multimodal large language models, rather than being exclusive to our work. A detailed discussion can be found in our comments to reviewer 4eXe in **A2**. \n\n\n**Q2: When testing, does UMIT require using ChatGPT to obtain the task definition for each new sample? This can result in significant inference costs for the unseen task.**\n\n**A2:** No, during the testing stage, UMIT only requires using ChatGPT to obtain the task definition for each task rather than each sample. As demonstrated in Table 12 and Table 13, we showcase the task definitions we obtained for tasks from the MME benchmark and SEED Bench using ChatGPT before the inference stage. This process is handled before inference, so it does not add to the inference costs.\n\n**Q3: The description of the details of UMIT, especially the use of symbols, is somewhat confusing. For example, what does the text encoder encode in exemplar sampling? And where does X_{instruct}^{i} come from in section 2.5?**\n\n**A3:** Sorry for this confusion. We will revise the paper to clarify them. \n+ we use text embeddings of both question X_q and answer X_a to retrieve in-context exemplars in training stage. This ensures that we can still retrieve similar in-context examples in tasks with specific questions other than VQA tasks. And for all evaluations, we randomly select in-context exemplars in testing stage.\n+ All X_{instruct}^{i} are derived from annotations in the original datasets we collected. In our response **A1** to **reviewer 4eXe**, we provided a detailed list of three instances from three distinct multimodal instruction datasets. We hope that reviewing these instances will help clarify this confusion.\n\n\n**Q4: The experimental results on SEED-Bench show a significant improvement when training OpenFlamingo and Otter on the data collected by the authors. In contrast, the gains from changing the format seem less pronounced. This raises a question of whether the role of data diversity is much greater than the task definition proposed by the author.**\n\n**A4:** We highly agree with your perspective. At least for the present, the improvement brought about by increasing task diversity on traditional vision-language metrics is indeed irreplaceable. This is also confirmed by LLaVA 1.5 [18], and in the NLP community, it has also been proven, as seen in works like Flan-2022 [19], opt-iml [20], etc. The purpose of introducing \"task definition\" is also to effectively merge existing multimodal instruction datasets using a unified format. This is done to enhance task diversity while, as much as possible, preserving the ability for in-context learning without compromising zero-shot performance."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8964/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631148641,
                "cdate": 1700631148641,
                "tmdate": 1700631148641,
                "mdate": 1700631148641,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "F8hptlKGIQ",
            "forum": "SNGANmQPLv",
            "replyto": "SNGANmQPLv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8964/Reviewer_srth"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8964/Reviewer_srth"
            ],
            "content": {
                "summary": {
                    "value": "This paper aims to find a better vision-language instruction format for in-context learning. Based on the existing components of [examples, image, instruction, question, answer] in instruction format, the authors further introduce the task definition as the prefix of the instruction. During the test, the authors also explored different types of exemplar sampling methods. The models are compared with the previous OpenFlamingo and Otter models."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The problem of enhancing the in-context learning capabilities of the vision-language model is important.\n- The proposed method of adding a task definition is very simple."
                },
                "weaknesses": {
                    "value": "- The contribution and novelty of this work is insufficient. Adding a carefully designed task definition with minor improvements (0.3 in Tab3) is not that significant in terms of technical contributions or scientific findings.\n- Although the paper uses a mix of many existing datasets for training, the evaluation is limited to a few benchmarks.\n- The writing is not that good. For example, the name Octopus is not introduced in the paper, which is confusing. And some typos need to be revised.\n- The literature review in Sec2.1 is not correct, e.g., \"Moreover, Otter (Li et al., 2023b) and MMICL (Zhao et al., 2023b) further introduced\". Otter was before Qwen-VL."
                },
                "questions": {
                    "value": "Please refer to the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8964/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698676744506,
            "cdate": 1698676744506,
            "tmdate": 1699637128459,
            "mdate": 1699637128459,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "v9Oh2O5oFY",
                "forum": "SNGANmQPLv",
                "replyto": "F8hptlKGIQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8964/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8964/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response for Reviewer srth"
                    },
                    "comment": {
                        "value": "Dear Reviewer srth:\n\nThank you for your comments. We provide discussions and explanations about your concerns as follows.\n\n**Q1: The contribution and novelty of this work is insufficient. Adding a carefully designed task definition with minor improvements (0.3 in Tab3) is not that significant in terms of technical contributions or scientific findings.**\n\n**A1:** Thank you for your careful examination of our table. In fact, the UMIT framework is designed to efficiently merge different multimodal instruction datasets with various tasks using a unified format, preserving the model's in-context learning ability on traditional vision-language tasks to the greatest extent possible without damaging the zero-shot performance of MLLMs. \n\nWe contend that multimodal instruction datasets are currently in a nascent stage of development, with significant limitations in terms of task diversity. Currently, the multimodal instruction dataset with the highest number of tasks is vision-flan [13], boasting over 200 tasks. However, this is still considerably fewer than instruction datasets in the NLP community, such as Flan-2022 [14] (1836 tasks) and Ni-v2 [15] (1616 tasks). Therefore, to rapidly increase task diversity, we can merge existing multimodal instruction datasets. Designing a unified format is what we consider the foremost requirement for merging these datasets, and this is the significance behind the birth of the UMIT framework.\n\nRegarding Table 3, our primary focus has been to investigate the impact of the Unified Multimodal Instruction Format (UMIT) on multimodal in-context learning. In Table 3, our format demonstrates a performance improvement of 1.2 through in-context learning, while the otter-style format yields a more modest improvement of 0.5. Although the OpenFlamingo-style format exhibits a stronger in-context learning capability, with an improvement of 2.5, this is due to its lack of instructions. Consequently, it minimizes the gap between fine-tuning and pre-training data format, preserving in-context learning to the greatest extent. However, the consequence is a significant impact on its zero-shot performance. Therefore, we consider our format to be the optimal compromise.\n\nMoreover, given that SEED Bench comprises 15,425 examples, we argue that both the 0.7 improvement in few-shot performance compared to the Otter-style format and the remarkable 3.2 improvement in zero-shot performance compared to the OpenFlamingo-style format are highly significant and noteworthy.\n\n\n**Q2: Although the paper uses a mix of many existing datasets for training, the evaluation is limited to a few benchmarks.**\n\n\n**A2:** Thank you very much for acknowledging the diversity of our training dataset. However, for evaluation, we conducted tests on the MME Benchmark with 14 tasks and the SEED Bench with 12 tasks, as shown in Tables 2 and 3. Additionally, we performed zero- and few-shot tests on seven traditional visual-linguistic tasks, including VizWiz, hatefulmeme, ISEKAI, Flickr30K, ScienceQA-Image, OK-VQA, and Text-VQA, as shown in Tables 4 and 6. If you feel that our experiments are not comprehensive, we sincerely hope you could recommend additional benchmarks or datasets, and we will make every effort to conduct thorough testing on these datasets.\n\n\n**Q3: The writing is not that good. For example, the name Octopus is not introduced in the paper, which is confusing. And some typos need to be revised.**\n\n**A3:** Sorry for the confusion. We will revise the article to change \"Octopus\" to \"UMIT\".\n\n**Q4: The literature review in Sec2.1 is not correct, e.g., \"Moreover, Otter (Li et al., 2023b) and MMICL (Zhao et al., 2023b) further introduced\". Otter was before Qwen-VL.**\n\n**A4:** We apologize for this error. It occurred because we wrote in the order of the increasing complexity of their instruction format components. In the Qwen-VL format, there is a lack of instructions, while Otter includes instructions, so we placed Qwen-VL first. We will revise the article to correct this error in Section 2.1. Part of the revised text is as follows:\n> Unlike the format utilized by Qwen-VL, Otter and MMICL incorporated the instance-level instruction component into the prompt format design for in-context instruction tuning.\n\n[13] [Xu, Trevor Ashby et al. \u201cVision-Flan: Scaling Visual Instruction Tuning.\u201d](https://vision-flan.github.io/)\n[14] Longpre, S. et al. \u201cThe Flan Collection: Designing Data and Methods for Effective Instruction Tuning.\u201d International Conference on Machine Learning (2023).\n[15] Wang, Yizhong et al. \u201cSuper-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks.\u201d Conference on Empirical Methods in Natural Language Processing (2022)."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8964/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700631083448,
                "cdate": 1700631083448,
                "tmdate": 1700631083448,
                "mdate": 1700631083448,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "WTCcwjbk1I",
            "forum": "SNGANmQPLv",
            "replyto": "SNGANmQPLv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8964/Reviewer_Sdyx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8964/Reviewer_Sdyx"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Unified Multimodal Instruction Tuning (UMIT), a framework to suggest how to construct a text-image interleaved instruction dataset by merging diverse visual instruction datasets in a unified multimodal instruction format. The experiments are based on OpenFlamingo. This paper also studies the impact of different components in multimodal instruction formats."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "The proposed approach seems reasonable for certain tasks; studying how the format of the multimodal instruction will affect the in-context learning performance is also an interesting topic. The experiments show positive results on the benchmarked datasets and tasks."
                },
                "weaknesses": {
                    "value": "One concern the reviewer has is that: the strategy of using uniform instruction styles and defining tasks clearly helps with tasks we already know well. However, this method assumes we can list and describe every possible task type, which might not be practical. The real world is full of unexpected and varied tasks, and a model that's too focused on a set of specific tasks might struggle to adapt to new or different ones. This over-specialization could limit the model's usefulness in a wider range of real-world situations.\nAlso, if the reviewer understands it correctly, the task definition for each instruction-exemplar pair is manually crafted, which might introduce empirical errors/bias, and might not be scalable enough."
                },
                "questions": {
                    "value": "How will the quality of the manually crafted task definition affect the overall performance? Have the authors tried efforts to automate this process to make it maybe more scalable?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8964/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698903051564,
            "cdate": 1698903051564,
            "tmdate": 1699637128356,
            "mdate": 1699637128356,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "t20gQrHIlb",
                "forum": "SNGANmQPLv",
                "replyto": "WTCcwjbk1I",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8964/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8964/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response for Reviewer Sdyx (Part 1)"
                    },
                    "comment": {
                        "value": "Dear Reviewer Sdyx:\n\nThank you for your comments. We provide discussions and explanations about your concerns as follows.\n\n\n**Q1: If the reviewer understands it correctly, the task definition for each instruction-exemplar pair is manually crafted, which might introduce empirical errors/bias, and might not be scalable enough.**\n\n**A1:** Sorry for the confusion. We acknowledge that the description in Section 2.3 may have been a bit unclear. In reality, we did not manually create a \"task definition\" for each instruction-exemplar pair. As illustrated in Table 7, our dataset comprises a total of 55 tasks from various multimodal instruction datasets, and for each of these tasks, there exists a specific \"task definition.\" These task definitions may originate from annotations in the original dataset, such as MultiInstruct dataset [10]. If the original data lacks annotations, we manually write the \"task definition\", such as OCR-VQA [11]. In essence, we manually craft \"task definition\" for just around 30 tasks. Additionally, to reduce bias caused by style inconsistency in various task definitions, we used GPT-3.5 to transform these 55 \"task definitions\" into a consistent style: \"In this task, you need to...\". Therefore, the \"task definition\" for each instruction-exemplar pair is derived from the task to which they belong.\n\n\n**Q2: This method assumes we can list and describe every possible task type, which might not be practical. The real world is full of unexpected and varied tasks, and a model that's too focused on a set of specific tasks might struggle to adapt to new or different ones. This over-specialization could limit the model's usefulness in a wider range of real-world situations.**\n\n**A2:** We highly agree with your perspective on the important role of in-context learning in the real world tasks. Since real-world instructions are quite diverse and hard to describe their task definition, we instead focus on instructions that describe traditional vision-language tasks to rigorously study the effects of different components of multimodal instruction format on in-context learning ability. Furthermore, unlike LLMs, current MLLMs, with the exception of GPT-4V, have not demonstrated a robust in-context learning ability in complex real-world tasks. A detailed discussion can be found in our comments to reviewer 4eXe in **A2**. \nFurthermore, our evaluations are all conducted on tasks that the model has not seen during training stage, such as the MME benchmark in Table 2 and the SEED Bench in Table 3. The results indicate that our model's generalization capabilities on few-shot performance remains significant. Therefore, once MLLMs with demonstrated in-context learning ability in the real world are open-sourced and there is a benchmark for MLLMs' in-context learning ability in the real world, we believe that the insights derived from our study could potentially be applied to real-world tasks and yield quantifiable results."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8964/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630973794,
                "cdate": 1700630973794,
                "tmdate": 1700630973794,
                "mdate": 1700630973794,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "1CgPXuEg8P",
                "forum": "SNGANmQPLv",
                "replyto": "jkoZRd5KKO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8964/Reviewer_Sdyx"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8964/Reviewer_Sdyx"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the clarifications. I've read other reviews and rebuttals."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8964/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700695355561,
                "cdate": 1700695355561,
                "tmdate": 1700695355561,
                "mdate": 1700695355561,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "PBUex302yD",
            "forum": "SNGANmQPLv",
            "replyto": "SNGANmQPLv",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission8964/Reviewer_4eXe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission8964/Reviewer_4eXe"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a new format for multimodal instruction tuning that includes a task definition prompt into the input context for multimodal models that can do in-context learning."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The contribution of retrieval to select the best exemplars for in-context learning is interesting, and the results in Table 4 show the benefit of retrieving relevant in-context examples.\n\n- The main contribution of \"task definition\" does not seem to be particularly novel, but the results in Tables 3 and 4 do seem to indicate the benefit of using task descriptions."
                },
                "weaknesses": {
                    "value": "- The main contribution here seems to be the new \"task definition\" component of the prompt that precedes the in-context examples, which does not seem particularly novel, and looking at the examples in Figure 1, I don't really see what information they provide that is not provided in the instance instruction itself -- the task definition just seems like a more verbose form of the instance-level instruction.\n\n- The retrieval augmentation in Section 2.4 requires us to have a reasonably large database of training examples to choose from, which may enhance task performance on benchmarks but defeats the purpose of in-context learning in the real world.\n\nOne ablation experiment I would suggest is to use the task-definition prompt without the instance-level instruction (DEQA)."
                },
                "questions": {
                    "value": "- In equation 5, is the text embedding E_text computed using the question X_q, answer X_a or both? this is not made clear"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission8964/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699048487283,
            "cdate": 1699048487283,
            "tmdate": 1699637128249,
            "mdate": 1699637128249,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "TAltctmcqx",
                "forum": "SNGANmQPLv",
                "replyto": "PBUex302yD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8964/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8964/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 4eXe:\n\nThank you for your comments. We provide discussions and explanations about your concerns as follows.\n\n**Q1: The difference between the extra information provided about \"task definition\" and \"instance-level instruction\".**\n\n**A1**: We understand your concerns about whether adding the \"task definition\" component can enhance information beyond that provided by \"instance-level instruction.\". Firstly, we would like to illustrate the differences between the three components: \"task definition,\" \"instance-level instruction,\" and \"question,\" by presenting an instance from one of the three different multimodal instruction datasets:\n1. **Otter-LA dataset**. This dataset lacks the \"task definition\" component. And the majority of this dataset is derived from real-world tasks, so we consider the \"instruction\" and the \"question\" as the same component. \n```\n### Otter-LA dataset ###\n\n[component 1] Task Definition: None\n\n[component 2] In-context Exemplars: \nInstruction 1: None\nQuestion1: What skill set might someone need to perform such a frisbee trick?\nAnswer1: To perform the frisbee trick shown in the image, where the man is passing a frisbee between or underneath his legs, a person would need a combination of skills...\nInstruction 2: None\nQuestion2: ...\nAnswer2: ...\n...\n\n[component 3] Instruction: None (you can consider the \"question\" as an \"instance-level instruction\".)\n[component 4] Question: What skills or techniques might the man need to successfully play with the frisbee?\n[component 5] Answer: To successfully play with the frisbee, the man needs various skills and techniques such as hand-eye coordination, spatial awareness, and understanding the aerodynamics...\n```\n\n2. **MMICL dataset**: This dataset also lacks the \"task definition\" component. However, the majority of its examples come from traditional vision-language tasks, such as OK-VQA, COCO-caption, etc. Therefore, we can distinguish its \"instruction\" component from the \"question\" component.\n```\n### MMICL dataset (OK-VQA) ###\n\n[component 1] Task Definition: None\n\n[component 2] In-context Exemplars: \nInstruction 1: Make sure your answers are based on the information presented in the image 0: <image0>\u56fe.\nQuestion 1: What is the purpose of the red decorations in this photo? \nAnswer 1: lighting\nInstruction 2: ...\nQuestion 2: ...\nAnswer 2: ...\n\n[component 3] Instruction: Carefully examine image 2 labeled <image2>\u56fe before answering the question.\n[component 4] Question: Name the type of plant this is?\n[component 5] Answer: vine\n```\n\n3. **MultiInstruct dataset**: This dataset encompasses 62 traditional vision-language tasks, with 5 task-level instructions manually crafted for each task, referred to as \"task definition.\" Therefore, it does not require additional \"instance-level instruction.\" However, it lacks the \"in-context exemplars\" component.\n```\n### MultiInstruct dataset (OK-VQA) ###\n\n[component 1] Task Definition: In this task, you will be asked a question about image. However, in order to answer this question, you need knoweldge outside of this image. \n\n[component 2] In-context Exemplars: None\n\n[component 3] Instruction: None (you can consider the \"task definition\" as an \"task-level instruction\".)\n[component 4] Question: Name the type of plant this is?\n[component 5] Answer: vine\n```\n\nTherefore, in our definition, the \"task definition\" component contains background information about the task, while the \"instruction\" component only guides the model to use the image content to answer relevant questions, without containing specific task information. Thus, with the addition of \"task definition,\" we can even consider \"instance-level instruction\" and \"question\" as the same component.\n\nIn fact, the UMIT framework is designed to efficiently merge different multimodal instruction datasets with various tasks using a unified format, preserving the model's in-context learning ability on traditional vision-language tasks to the greatest extent possible without damaging the zero-shot performance of MLLMs. \n\nWe contend that multimodal instruction datasets are currently in a nascent stage of development, with significant limitations in terms of task diversity. Currently, the multimodal instruction dataset with the highest number of tasks is vision-flan [1], boasting over 200 tasks. However, this is still considerably fewer than instruction datasets in the NLP community, such as Flan-2022 [2] (1836 tasks) and Ni-v2 [3] (1616 tasks). Therefore, to rapidly increase task diversity, we can consolidate existing multimodal instruction datasets. Designing a unified format is what we consider the foremost requirement for merging these datasets, and this is the significance behind the birth of the UMIT framework."
                    },
                    "title": {
                        "value": "Author Response for Reviewer 4eXe (Part 1)"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8964/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630780099,
                "cdate": 1700630780099,
                "tmdate": 1700631609425,
                "mdate": 1700631609425,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "WHuSVQn1RL",
                "forum": "SNGANmQPLv",
                "replyto": "PBUex302yD",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission8964/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission8964/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response for Reviewer 4eXe (Part 2)"
                    },
                    "comment": {
                        "value": "**Q2: The retrieval augmentation in Section 2.4 requires us to have a reasonably large database of training examples to choose from, which may enhance task performance on benchmarks but defeats the purpose of in-context learning in the real world.**\n\n**A1:** Firstly, we think that our work is highly meaningful. Currently, in the multimodal field, there are numerous instruction datasets such as LLaVA, Otter, MultiInstruct, Vision-Flan, etc. However, the tasks included in these datasets are very limited. For example, even Vision-Flan only has 200 tasks, which still represents a significant gap compared to instruction datasets in the NLP community, such as Flan-2022 with over 1800 tasks. Additionally, after visual instruction tuning, the in-context learning capability of MLLMs tends to decline, as evident in Table 2 and Table 3. Therefore, based on these two problems, we propose a unified multimodal instruction format to merge existing datasets, increase task diversity, and, at the same time, maximize the retention of in-context learning ability on traditional visual language tasks. \n\nWe strongly agree with your perspective on the significant value of in-context learning in the real world, surpassing its importance in traditional vision-language tasks. However, the focus of our paper still lies in the latter, aiming to enhance the in-context learning ability of MLLMs on traditional vision-language tasks through retrieval methods. There are two reasons for this:\n+ Firstly, we argue that, except for GPT-4V, all current MLLMs lack the capability to achieve in-context learning in the real world. For instance, MLLMs like Flamingo [4], OpenFlamingo [5], Kosmos-2 [6], Emu [7], Otter [8] among others, merely showcase their few-shot performance on traditional vision-language tasks. However, there are no other MLLMs demonstrating real-world in-context learning examples as illustrated in Figures 8 \u2014 13 in GPT-4V technical report [9]. \n+ Secondly, We think that there is currently a lack of a reasonable evaluation method and dataset to effectively measure MLLMs' in-context learning ability in the real world. For example, in Figures 8 to 10 of the GPT-4V technical report [9], GPT-4V accurately judges the speed range of a dashboard through in-context learning, demonstrating the utility of in-context learning. However, we face challenges in quantifying it effectively.\n\nIn summary, we believe that exploring the in-context learning ability of MLLMs in the real world still requires further investigation. In our paper, we can only focus on the performance improvement on traditional VL metrics brought about by constructing a unified multimodal instruction format using retrieval method.\n\n**Q3: One ablation experiment I would suggest is to use the task-definition prompt without the instance-level instruction (DEQA).**\n\n**A3:** As we mentioned in our response in **A1**, after adding the \"task definition\" component, \"instance-level\" and \"question\" can be considered as the same component. Therefore, the results of our DEIQA are consistent with what you perceive as DEQA.\n\n**Q4: In equation 5, is the text embedding E_text computed using the question X_q, answer X_a or both?**\n\n**A4:** Thank you for your comments! We are sorry that we didn't make that clear in Section 2.3. We\u2019ve revised the content of Section 2.3. \nIn fact, we use text embeddings of both question X_q and answer X_a to retrieve in-context exemplars in training stage. This ensures that we can still retrieve similar in-context examples in tasks with specific questions other than VQA tasks. However, for all evaluations, we randomly select in-context exemplars in testing stage and our method still can achieve promising performances. \n\n[1]  [Xu, Trevor Ashby et al. \u201cVision-Flan: Scaling Visual Instruction Tuning.\u201d](https://vision-flan.github.io/)\n\n[2]  Longpre, S. et al. \u201cThe Flan Collection: Designing Data and Methods for Effective Instruction Tuning.\u201d International Conference on Machine Learning (2023).\n\n[3] Wang, Yizhong et al. \u201cSuper-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks.\u201d Conference on Empirical Methods in Natural Language Processing (2022).\n\n[4] Alayrac, Jean-Baptiste et al. \u201cFlamingo: a Visual Language Model for Few-Shot Learning.\u201d ArXiv abs/2204.14198 (2022): n. pag.\n\n[5] Awadalla, Anas et al. \u201cOpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models.\u201d ArXiv abs/2308.01390 (2023): n. pag.\n\n[6] Peng, Zhiliang et al. \u201cKosmos-2: Grounding Multimodal Large Language Models to the World.\u201d ArXiv abs/2306.14824 (2023): n. pag.\n\n[7] Sun, Quan et al. \u201cGenerative Pretraining in Multimodality.\u201d ArXiv abs/2307.05222 (2023): n. pag.\n\n[8] Li, Bo et al. \u201cMIMIC-IT: Multi-Modal In-Context Instruction Tuning.\u201d ArXiv abs/2306.05425 (2023): n. pag.\n\n[9] Yang, Zhengyuan et al. \u201cThe Dawn of LMMs: Preliminary Explorations with GPT-4V(ision).\u201d ArXiv abs/2309.17421 (2023): n. pag."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission8964/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700630872295,
                "cdate": 1700630872295,
                "tmdate": 1700718281186,
                "mdate": 1700718281186,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]