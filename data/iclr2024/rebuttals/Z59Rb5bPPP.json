[
    {
        "title": "Trajeglish: Learning the Language of Driving Scenarios"
    },
    {
        "review": {
            "id": "1odIqVH99H",
            "forum": "Z59Rb5bPPP",
            "replyto": "Z59Rb5bPPP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1477/Reviewer_mABk"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1477/Reviewer_mABk"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method called Trajeglish to generate future trajectories for traffic participants in a scenario. In particular, they propose a method to tokenize trajectory data using a small vocabulary. Besides, they propose a transformer-based architecture for modeling the action tokens on map information as well as initial states of traffic participants. To evaluate Trajeglish, the authors compare it with a behavior cloning method and a baseline that only models single agent trajectories. The result shows that their method achieves superior performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "### 1.The idea of tokenization using a small vocabulary is moderately novel.\n\n### 2.The visualization and illustration are well made and help the readers to understand the paper."
                },
                "weaknesses": {
                    "value": "## Major:\n\n### 1.motivation of using tokenization (compared with using the actual values as in most of existing work in Appendix B) is not very clear.\n\n### 2.the experimental results are not very impressive\n\n(1) Improvements in Table 1 seem quite small. Can you show standard deviations for the results? \n\n(2) only evaluate on open-loop simulation but not on close-loop simulation\n\n(3) the baseline details are not given (e.g., \u201cThe \u201cmarginal\u201d baseline is an equally important baseline designed to mimic the behavior of models such as Wayformer (Nayakanti et al., 2022) and MultiPath++ (Varadarajan et al., 2021) that are trained to model the distribution over single agent trajectories instead of multi-agent scene-consistent trajectories.\u201d However, it is unclear if this baseline really can achieve similar performance as Wayformer / MultiPath++ as the authors did not give further details) and it is hard for one to assess if they are really strong baselines.\n\n### 3.motivation of having a model that take order into account is not very convincing\n\nIn particular, the idea of having this order seems very unnatural. For example, in the real world the likelihood of equally capable drivers (whether human or AI) to have collisions should be equal?\n\n## Minor:\n\n### 1.missing some relevant work on multi-agent trajectory prediction:\n\nHivt: Hierarchical vector transformer for multiagent motion prediction, Z. Zhou, L. Ye, J. Wang, K. Wu, and K. Lu.\n\nLanguage-Guided Traffic Simulation via Scene-Level Diffusion, Z. Zhong, D. Rempe, Y. Chen, B. Ivanovic, Y. Cao, D. Xu, M. Pavone, B. Ray\n\n### 2.did not discuss the limitations of the current work"
                },
                "questions": {
                    "value": "-What\u2019s the motivation of using a small vocabulary compared with using the actual values as in most of existing work (as in Appendix B)?\n\n-The provided video is a bit confusing. How do you control other vehicles that are neither replay nor trajeglish? In some videos the legends only show these two types but there are vehicles of other colors.\n\n-Can you also show the variance for Figure 8?\n\n-Figure 9 why the collision rate decreases when the rollout becomes longer?\n\n-Trajgelish behavior under longer horizon rollout (e.g., 200 timesteps)?\n\n-For the experiments, 16 scenarios are sampled for every clip in WOMD? Or only 16 clips in total?\n\n-Can you also provide some qualitative visualization for nuscenes in Appendix?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1477/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1477/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1477/Reviewer_mABk"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1477/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697569667489,
            "cdate": 1697569667489,
            "tmdate": 1700755713620,
            "mdate": 1700755713620,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "QUPMlJXW0b",
                "forum": "Z59Rb5bPPP",
                "replyto": "1odIqVH99H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1477/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1477/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the review (Part 1)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their questions and feedback. We are glad the reviewer appreciates the novelty of our method for tokenizing trajectories as well as the quality of the visualizations we\u2019ve created to communicate how our method works. We hope to answer the reviewer\u2019s questions and clear up the reviewer\u2019s misunderstandings about our method with our response below:\n\n2 misunderstandings to clarify:\n- \u201conly evaluate on open-loop simulation but not on close-loop simulation\u201d - we want to emphasize that we exclusively evaluate in closed-loop simulation in this paper. We consider a key component of our contribution to be that we identify formal requirements for a tokenization strategy to enable closed-loop policies (Section 2) and then introduce a tokenization method that satisfies these requirements (Section 3). For the full autonomy evaluation in Figure 9 and Table 1, the partial autonomy evaluation in Figure 10, and our results on the WOMD sim agents benchmark [(see our note to all reviewers)](https://openreview.net/forum?id=Z59Rb5bPPP&noteId=VvSB6yXfHY), the rollouts are fully closed-loop; the encoder-decoder transformer functions as a policy that outputs a single action representing the change in state over the next 0.1 seconds for each agent at each step, upon observing all previously selected actions for all agents.\n- \u201cThe provided video is a bit confusing. How do you control other vehicles that are neither replay nor trajeglish?\u201d - All agents in our visualization and evaluation are either controlled by replay or trajeglish. For the left 3 panes of the video, we control all vehicles in the scene with trajeglish and title these panes \u201cFull Autonomy '' to indicate the setting. These are the rollouts we use to evaluate our method in Table 1, Figure 9, and the WOMD sim agents benchmark (see our note to all reviewers). For the rightmost pane, only the blue agent is controlled by trajeglish and the rest are on replay, as we denote in the legend. These are the rollouts we use to evaluate our method in Figure 10.\n\n    To be clear, our ultimate goal is to control agents in simulation such that they interact in realistic ways to any number of agents controlled by a black-box AV system. In the absence of a black-box AV system at hand to test, we use replay as a surrogate black-box controller. The purpose of our experiments in the \"partial control setting\" is to demonstrate the property of trajeglish agents that they interact with other agents in a closed-loop setting independent of the underlying method that controls the other agents."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1477/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700333323124,
                "cdate": 1700333323124,
                "tmdate": 1700510070697,
                "mdate": 1700510070697,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "ivmX5TNx9L",
                "forum": "Z59Rb5bPPP",
                "replyto": "1odIqVH99H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1477/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1477/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the review (Part 2)"
                    },
                    "comment": {
                        "value": "Responses to the reviewer\u2019s other questions and requests:\n- \u201cmotivation of using tokenization (compared with using the actual values as in most of existing work in Appendix B) is not very clear\u201d + \u201cWhat\u2019s the motivation of using a small vocabulary compared with using the actual values as in most of existing work (as in Appendix B)?\u201d - Our motivation stems from the impressive results we\u2019ve seen from applying discrete sequence models in other continuous domains, for instance Paella [1], Stable Diffusion [2] Parti [3], and GAIA-1 [4]. We share the intuition behind these works that sacrificing a small amount of resolution (e.g. a few centimeters in our case as reported in Figure 12 and Figure 13) can be well worth the expressiveness and stability of the categorical distribution compared to a mixture-of-x parameterization, for instance. Empirically, this tradeoff appears to be well worth exploring, as our method outperforms all prior work based on diffusion, VAEs, and mixture-of-x modeling on the WOMD sim agents benchmark [(see our note above)](https://openreview.net/forum?id=Z59Rb5bPPP&noteId=VvSB6yXfHY).\n- \u201cImprovements in Table 1 seem quite small. Can you show standard deviations for the results?\u201d We would like to highlight that for the mADE and ADE reported in Table 1, the increase in performance due to intra-timestep conditioning and tokenization regularization is quite significant, improving by 6% and 2.3% respectively. We additionally quantify the performance gap due to these modeling choices in Figure 9 and Figure 10 which show significant improvements when intra-timestep coupling is taken into account. Finally, trajeglish improves on interaction metrics by 4.4% over all prior work on the official WOMD sim agents benchmark, a large jump in performance over all prior work [(see our note above)](https://openreview.net/forum?id=Z59Rb5bPPP&noteId=VvSB6yXfHY).\n- \u201cit is unclear if this baseline really can achieve similar performance as Wayformer / MultiPath++ as the authors did not give further details) and it is hard for one to assess if they are really strong baselines\u201d - In [our note above](https://openreview.net/forum?id=Z59Rb5bPPP&noteId=VvSB6yXfHY), we report results on the WOMD sim agents benchmark which features an official submission for Wayformer as well as a submission based on MultiPath++. Trajeglish improves over Wayformer by 9.2%, and MultiPath++ by 5.4%. We hope these direct comparisons are convincing for the reviewer. We are also updating our draft to include more details about our implementation of the \u201cno-intra\u201d and \u201cmarginal\u201d baselines referenced in Table 1.\n- \u201cmotivation of having a model that take order into account is not very convincing\u201d - We note that the focus of our work is on how we apply discrete sequence modeling to multi-agent trajectories, and less about the design of our encoder for scene context. We are aware of prior work that studies how to adjust the attention mechanism to maintain permutation equivariance while still leveraging tracking information, such as AgentFormer [5]. However, we find we are able to achieve state of the art performance for traffic modeling by simply randomizing the order during training, so we reserve the improvement of our architecture for encoding scene context for later work.\n\n[1] \u201cA Novel Sampling Scheme for Text- and Image-Conditional Image Synthesis in Quantized Latent Spaces\u201d, Rampas et al, 2023.\\\n[2] \u201cHigh-Resolution Image Synthesis with Latent Diffusion Models\u201d, Rombach et al, 2021.\\\n[3] \u201cScaling Autoregressive Models for Content-Rich Text-to-Image Generation\u201d, Yu et al, 2022.\\\n[4] \u201cGAIA-1: A Generative World Model for Autonomous Driving\u201d, Hu et al, 2023.\\\n[5] \u201cAgentFormer: Agent-Aware Transformers for Socio-Temporal Multi-Agent Forecasting\u201d, Yuan et al, 2021."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1477/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700333355516,
                "cdate": 1700333355516,
                "tmdate": 1700334021531,
                "mdate": 1700334021531,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "C0bKyG4jp6",
                "forum": "Z59Rb5bPPP",
                "replyto": "1odIqVH99H",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1477/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1477/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the review (Part 3)"
                    },
                    "comment": {
                        "value": "- Additional papers to cite - We note that HiVT studies how mixture-of-x distributions can be used to model single-agent trajectories and \u201cLanguage-Guided Traffic Simulation via Scene-Level Diffusion\u201d focuses on how text can be used to control generation of a static traffic scenario, so we don\u2019t consider them directly relevant to our work which studies how discrete sequence models can be applied to closed-loop traffic simulation. However, if the reviewer feels strongly that these are relevant, we can add these citations.\n- \u201cdid not discuss the limitations of the current work\u201d - we discuss the fact that our encoder-decoder transformer is not affine equivariant and not permutation equivariant in section 3.2 and the fact that map representation may be a limiting factor for successful transfer to other datasets in section 4.3. We will elaborate on these limitations and update our paper.\n- \u201cCan you also show the variance for Figure 8?\u201d - Yes, we will update our paper before the end of the discussion period.\n- \u201cFigure 9 why the collision rate decreases when the rollout becomes longer?\u201d - the reason is that for the rollouts evaluated in Figure 9, we adjust the padding of the WOMD scenarios to prevent agents from reappearing if they ever disappear. As a result, the number of agents in the scene decreases over time, so the number of collisions also decreases over time. We use this setting for evaluation because our tokenizer relies on the existence of the previous state in order to tokenize, so we are unable to tokenize states that do not have a recorded state in the timestep before. On the other hand, for the WOMD sim agents benchmark [(see our note above)](https://openreview.net/forum?id=Z59Rb5bPPP&noteId=VvSB6yXfHY), we evaluate in the setting where all agents visible at the tenth timestep need to be simulated for all 80 timesteps into the future, as required by the benchmark. We will add these details on how padding is taken into account in each of our evaluation settings to our paper.\n- \u201cTrajgelish behavior under longer horizon rollout (e.g., 200 timesteps)?\u201d We will add visualizations of longer rollouts to our video.\n- \u201cFor the experiments, 16 scenarios are sampled for every clip in WOMD? Or only 16 clips in total?\u201d 16 scenarios are sampled per clip for each of the 44k WOMD validation clips.\n- \u201cCan you also provide some qualitative visualization for nuscenes in Appendix?\u201d Yes, we will update our paper before the end of the discussion period."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1477/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700333387556,
                "cdate": 1700333387556,
                "tmdate": 1700334135565,
                "mdate": 1700334135565,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "6bXXM4kMjy",
                "forum": "Z59Rb5bPPP",
                "replyto": "C0bKyG4jp6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1477/Reviewer_mABk"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1477/Reviewer_mABk"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response!"
                    },
                    "comment": {
                        "value": "Thank you for the response! The responses have addressed my major concerns. I will update my score once the updated version is available."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1477/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700586950138,
                "cdate": 1700586950138,
                "tmdate": 1700586950138,
                "mdate": 1700586950138,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "8G2LC3gKzk",
            "forum": "Z59Rb5bPPP",
            "replyto": "Z59Rb5bPPP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1477/Reviewer_qzEz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1477/Reviewer_qzEz"
            ],
            "content": {
                "summary": {
                    "value": "\u201cTrajeglish: Learning the Language of Driving Scenarios\u201d proposes a model that can create scene-consistent rollouts for a subset of agents in a scene. In particular, the proposal consists of a tokenization algorithm, \u201ck-disks\u201d, for tokenization an agent\u2019s motion, and a transformer-based model architecture that autoregressively and causally rolls out agents\u2019 future trajectories. The authors provide competitive results on WOMD and transfer to nuScenes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* Strong tokenizer k-disks outperforming kMeans baselines with low discretization errors and convincing ablation study\n* Autoregressive and casual rollouts\n* Experiments demonstrating the benefits of intra-timestep dependence of agents\n* Experiments demonstrating the transfer to nuScenes"
                },
                "weaknesses": {
                    "value": "* Missing WOMD baseline results from other models\n* Similar contributions as the recently published \u201cMotionLM: Multi-Agent Motion Forecasting as Language Modeling\u201d (https://arxiv.org/pdf/2309.16534.pdf)"
                },
                "questions": {
                    "value": "* How does your approach compare to the recently published \u201cMotionLM: Multi-Agent Motion Forecasting as Language Modeling\u201d (https://arxiv.org/pdf/2309.16534.pdf)?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1477/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698446791989,
            "cdate": 1698446791989,
            "tmdate": 1699636076715,
            "mdate": 1699636076715,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "6do4qgZrJW",
                "forum": "Z59Rb5bPPP",
                "replyto": "8G2LC3gKzk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1477/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1477/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the review"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback. We are glad that the reviewer appreciates several aspects of our work, including the \u201cstrong tokenizer k-disks\u201d performance with \u201cconvincing ablation study\u201d of different tokenization approaches as well as experiments investigating \u201cthe benefits of [modeling] intra-timestep dependence of agents\u201d, and our \u201cexperiments demonstrating the transfer to nuScenes\u201d. We respond to the reviewer\u2019s comments below:\n\n- \u201cMissing WOMD baseline results from other models\u201d - In order to offer direct comparison to baseline results from other models, we have made a submission to the WOMD sim agents benchmark, and documented the results in [our note above to all reviewers](https://openreview.net/forum?id=Z59Rb5bPPP&noteId=VvSB6yXfHY). To summarize, trajeglish outperforms all other single-model submissions, including models based on the Wayformer, Multipath++, and MTR architectures, and is in 2nd place, falling just behind a submission that uses ensembling by only 0.25%. Our method additionally sets a new state of the art for the interaction metrics measured by the benchmark, surpassing all prior work by 4.4%. We hope these results provide the comparison to WOMD baseline results that the reviewer was hoping to see for our method.\n- \"Similar contributions as the recently published MotionLM\" + \u201cHow does your approach compare to MotionLM?\u201d - The main difference between our work and MotionLM is the application they target; MotionLM targets online deployment, and therefore models 1- and 2-agent (x,y) trajectories at 2hz. Since we target offline simulation of agents, we model up to 24-agent (x,y,h) trajectories at 10hz. The differences between the distributions that these methods seek to model give rise to differences in tokenization schemes; MotionLM discretizes the second-order derivatives of motion in a global coordinate frame, whereas we leverage the fact that we decode heading to discretize first-order derivatives of motion in the per-timestep Frenet frame of the agent. As a practical consideration, the difference in target distribution also results in significantly different sequence lengths modeled between the two approaches; sequences modeled by MotionLM are of length 2 agents * 16 timesteps = 32, while sequences modeled by trajeglish are of length 24 agents * 63 timesteps = 1512. Finally, while temporal causality is shown to be a useful inductive bias for MotionLM and the attention mask used by the autoregressive decoder does not take into account intra-timestep conditioning, our goal in Section 2 is to show that temporal causality is actually required for our application of controlling reactive agents in simulation, and we explicitly measure in Figures 9, 10, and 11 to what extent intra-timestep conditioning affects performance, especially when only a single timestep of trajectory context is provided at initialization.\n\n    While we plan to add a citation for MotionLM to our paper, we would also like to make it clear that MotionLM was released on arxiv on September 28, the same day that we submitted our work to ICLR. As a result, we hope that the reviewer recognizes our work and MotionLM as fully concurrent successful applications of discrete sequence modeling to two different variants of motion prediction."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1477/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700292274333,
                "cdate": 1700292274333,
                "tmdate": 1700293615368,
                "mdate": 1700293615368,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Gid8XS4tzZ",
                "forum": "Z59Rb5bPPP",
                "replyto": "6do4qgZrJW",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1477/Reviewer_qzEz"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1477/Reviewer_qzEz"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the response"
                    },
                    "comment": {
                        "value": "Thank you for the detailed response.\n\n**WOMD results**: Thank you for adding the new results on the WOMD sim agents benchmark. I'm happy to see this comparison to the baselines, which makes your work stronger.\n\n**MotionLM**: Thanks a lot for the detailed comparison to MotionLM. It's great that you will add a reference to this publication. I also agree that both publications should be regarded as concurrent for the review.\n\nI have no additional concerns and am looking forward to the updated version."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1477/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700584803614,
                "cdate": 1700584803614,
                "tmdate": 1700584803614,
                "mdate": 1700584803614,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "NqBIX5lL1W",
            "forum": "Z59Rb5bPPP",
            "replyto": "Z59Rb5bPPP",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1477/Reviewer_KYot"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1477/Reviewer_KYot"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a language modeling inspired approach to data-driven traffic simulation. The key step involved is tokenizing future driving scenario data into a sequential, language-style format, for which this paper compares several tokenization schemes. Once tokenized, a simple transformer encoder-decoder architecture is proposed to encode the initial scene state and autoregressively decode the tokenized scene future. Training the model follows the standard next token prediction objective as in language modeling, with an optional noise term on the ground truth tokens to deal with distribution shifts caused by teacher forcing. A diverse set of experiments on the Waymo Open Motion Dataset (WOMD) provide several insights on how design choices for this new formulation impact simulation quality."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The key strengths of this work lie in its conceptual and architectural simplicity in comparison to existing methods. The idea is well-motivated and the presentation is clear. Besides this, the paper provides a detailed experimental analysis on different aspects of the proposed design space."
                },
                "weaknesses": {
                    "value": "1. The benchmarking in Table 1 follows a much simpler setting with fewer max agents (24 vs. 128) and a shorter time horizon (6 seconds vs. 8 seconds) than prior work on WOMD [1,2,3]. \n2. As a result of this simpler benchmark and missing comparisons to any prior architecture, this paper does not address the key question of whether the proposed method is competitive to the current state-of-the-art despite its simplicity. At a glance, it seems to be much worse, with a minADE >3m in comparison to the SoTA methods with minADE < 1m on the more challenging standard WOMD setting. \n3. The paper is not self-contained, with important details (e.g., related work and several figures referenced during discussions in the main paper) only available in the appendix\n\n[1] https://arxiv.org/abs/2209.13508\n\n[2] https://arxiv.org/abs/2306.17770\n\n[3] https://arxiv.org/abs/2309.16534"
                },
                "questions": {
                    "value": "1. Please see \u201cWeaknesses\u201d - these are the key points with the most influence on my rating. If addressed via a fair and direct comparison to existing work, I am inclined to improve my rating.\n2. Given the simple and scalable architecture, it would be interesting to analyze the importance of scale (in terms of #parameters in the encoder/decoder) towards the performance of the proposed model.\n3. The clarity of Figure/Table captions and their placement within the document could be improved, currently, they are often very far from the text referencing them.\n4. How are actors ordered in the decoder? Is this randomized for each scene during both training and inference?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1477/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1477/Reviewer_KYot",
                        "ICLR.cc/2024/Conference/Submission1477/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1477/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698944558818,
            "cdate": 1698944558818,
            "tmdate": 1700811255661,
            "mdate": 1700811255661,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qx3N8rXjQf",
                "forum": "Z59Rb5bPPP",
                "replyto": "NqBIX5lL1W",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1477/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1477/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the review"
                    },
                    "comment": {
                        "value": "We thank the reviewer for their valuable feedback. We are glad that the reviewer found our method \u201cwell-motivated\u201d and our presentation \u201cclear\u201d, with \u201cdetailed experimental analysis on different aspects of the proposed design space\u201d. We respond to the reviewer\u2019s comments below:\n\n- \u201cThis paper does not address the key question of whether the proposed method is competitive to the current state-of-the-art despite its simplicity\u201d + \u201cIf addressed via a fair and direct comparison to existing work, I am inclined to improve my rating.\u201d  - As outlined in our [note to all reviewers](https://openreview.net/forum?id=Z59Rb5bPPP&noteId=VvSB6yXfHY) above, to compare more directly to prior work, we have prepared a submission to the Waymo \u201csim agents\u201d leaderboard. To summarize the results, trajeglish is the top performing method among single-model submissions, surpassing methods built on top performing architectures for motion prediction such as Wayformer, Multipath++, and MTR. We are adding the details to our paper of how we use our model to rollout for all 128 agents for the full 80 timesteps as required for submissions to the benchmark. We hope these results provide the direct comparison the reviewer is hoping to see for our method.\n- \u201cAt a glance, it seems to be much worse, with a minADE >3m in comparison to the SoTA methods with minADE < 1m\u201d - The minADE in Table 1 is calculated by sampling scenarios given only 1 timestep of ground-truth history. We expect the optimal minADE for this setting to be significantly higher than the optimal minADE when 11 timesteps are given as input, as is the case for all 3 WOMD benchmarks related to motion prediction. For an apples-to-apples comparison of minADE to prior work, we report a minADE of 1.68 meters on the WOMD sim agents benchmark (see our [note to all reviewers](https://openreview.net/forum?id=Z59Rb5bPPP&noteId=VvSB6yXfHY)), which is competitive with that of other methods, although we did not optimize hyperparameters for this particular metric since it is not included in the benchmark\u2019s ranking metric. In the paper, we will be more precise about the evaluation setting we use for Table 1.\n- \u201cThe paper is not self-contained, with important details (e.g., related work and several figures referenced during discussions in the main paper) only available in the appendix\u201d + \u201cThe clarity of Figure/Table captions and their placement within the document could be improved\u201d - We appreciate this feedback. We are reorganizing the paper to make sure the related work and main figures are included in the body of the paper. We will also edit the captions for clarity, and post an updated version before the end of the discussion period.\n- \u201cit would be interesting to analyze the importance of scale (in terms of #parameters in the encoder/decoder) towards the performance of the proposed model\u201d - we certainly agree with the sentiment of this proposal. The fact that discrete sequence modeling has proven to be so reliably scalable in other domains is one of our main motivations for studying how these models can be applied to traffic modeling. We will add a limited study of scaling laws to the paper before the end of the discussion period.\n- \u201cHow are actors ordered in the decoder? Is this randomized for each scene during both training and inference?\u201d - During training, we randomize the order. As a result, we are free to choose any order at inference. We use this capability to evaluate how predictive ability changes as we adjust the agent's position in the ordering, as shown in Figure 11. Across all other experiments, we keep the order stable by sorting the agents by distance to the centroid of the agent centers. We will add these details on our evaluation setting to the paper."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1477/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700285906816,
                "cdate": 1700285906816,
                "tmdate": 1700286213230,
                "mdate": 1700286213230,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w116t3w3fg",
                "forum": "Z59Rb5bPPP",
                "replyto": "qx3N8rXjQf",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1477/Reviewer_KYot"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1477/Reviewer_KYot"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the clarifications"
                    },
                    "comment": {
                        "value": "Thank you for the clear and detailed response to all points raised in the review. I think the new results and clarifications address all of my initial concerns, and I do not have any follow-up questions for now.\n\nI look forward to the promised updated version and will decide on my final score once this is available."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1477/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700503825928,
                "cdate": 1700503825928,
                "tmdate": 1700503825928,
                "mdate": 1700503825928,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]