[
    {
        "title": "Communication-Efficient Heterogeneous Federated Learning with Generalized Heavy-Ball Momentum"
    },
    {
        "review": {
            "id": "ABntaQdtTc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5505/Reviewer_8a6j"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5505/Reviewer_8a6j"
            ],
            "forum": "oJ1tx3fXDA",
            "replyto": "oJ1tx3fXDA",
            "content": {
                "summary": {
                    "value": "This paper proposes to adopt the generalization of heavy-ball momentum in FL which uses an uncertain interval according to the storage state of each local client to construct the momentum term (the current state minus the last local state). No theoretical analysis of convergence or generalization is provided. Experiments on CIFAR-10/100, Shakespeare are conducted to validate its efficiency without learning rate decay."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The proposed shift window can help to reduce the communication requirements.\n2. This paper proposes a summary of the classical momentum-based method in FL, i.e., FedCM, FedADC, and MimeMom. \n3. The experiments are widely conducted on several setups to validate the efficiency."
                },
                "weaknesses": {
                    "value": "1. In Eq.(5), what is the MVR term? Could the author explain this in detail? Its first part is a sum of $J_i$-step updates, while its second part is a sum of $j$-step updates. I know these two parts come from the additional term from $\\theta_{i,j}^t-\\theta_i^{t-\\tau_i}$ to $\\theta^{t-1}-\\theta^{t-\\tau_i-1}$, but how does it perform as a variance reduction? The $u$ term is the update performed in each local iteration, so the MVR term is the difference between $j$ local updates and $J_i$ local updates where $j\\neq J_i$. Please provide the equivalent form in [1] and demonstrate its variance reduction efficiency in the main text.\n\n   [1]: Momentum-based variance reduction in non-convex sgd\n\n2. The authors claim that they propose the GHB form in Table 1. However, the same updates have been studied in [2] which adopts a multi-step momentum. By setting the specific coefficients as some fixed constants, [2] performs the same updates as the global GHB. Actually, the claimed global GHB in this paper is only a special case of [2]. It also provides some convergence analysis to understand more complicated cases. Therefore, the contribution of this paper seems to only extend the global GHB to the local GHB without any analysis of optimization or generalization, which greatly reduces the novelty of this paper.\n\n    [2]: Enhance local consistency in federated learning: A multi-step inertial momentum approach\n\n3. In the experiments, it indicates that the non-iid dataset split adopts a $\\alpha\\rightarrow 0$. While in the appendix, it shows $\\alpha=0$. Please unify this statement.\n\n4. The results are incomplete. For instance, in Table.2, 3, 4, there are no results of FedCM and FedADC. However, in Figure.3, the curves of FedCM are stated as an ablation study. As the very important baselines of this paper, the comparison with FedCM and FedADC is very necessary, which are also two SOTA methods among the momentum-based methods. Authors should add their performance tests to this paper.\n\n5. The experiments do not adopt the learning rate decay. On page 15 in the appendix, the authors claim that all experiments do not use a learning rate scheduler for simplicity. However, this will significantly reduce the performance of some algorithms and make comparisons unfair. For instance, the ADMM-based method, i.e. FedDyn, requires to be optimized well enough on the local client. Otherwise, dual variables will be updated with very large biases. A similar phenomenon also happens in SCAFFOLD and even FedCM. I think this is the main weakness of the experiment in this paper. All results only reflect the phenomenon under a fixed learning rate. While in the current machine learning, the learning rate decay in non-convex optimization is very important and one of the major concerns. Although changing the learning rate complicates comparisons, this kind of comprehensive study can broadly reflect the performance of the proposed method."
                },
                "questions": {
                    "value": "Thanks for this submission with the FedGBH method. My main concerns are stated in the weaknesses, mainly including the novelty with repetitive parts from the previous studies, and the lack of baselines and hyperparameter settings."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5505/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697447002277,
            "cdate": 1697447002277,
            "tmdate": 1699636563269,
            "mdate": 1699636563269,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "DWlsVwmXpY",
                "forum": "oJ1tx3fXDA",
                "replyto": "ABntaQdtTc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5505/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5505/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to W1, W3, W4, W5 (1/2)"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the feedback. In the following, we address the reviewer\u2019s remarks on the weaknesses (W).\n* **W1:** In our initial analysis of the second term in Eq. (5), we drew parallels with the concept presented in [1], perceiving it as a variance reduction mechanism due to observed similarities. We appreciate the Reviewer's keen observation regarding the distinction between these two terms. This insight has led us to a more accurate interpretation and, consequently, a revision in our manuscript. We now clarify that this term in our formulation is intended to penalize the direction of the last updates at round $t - \\tau_i$ \u200b, in contrast to the progressive update of local steps at the current round $t$.\nAlthough at the moment we cannot formally prove that this term acts as variance reduction, we investigated its importance in the ablation study section, specifically in Figure 2, where it is evident that it provides additional speedup w.r.t Local-GHB and reaches better accuracy (see fig. 3 and table 10).\n* **W3:** Thanks for spotting the typo, we fixed it.\n* **W4:** In the first part of section 4.3 we discuss how both FedCM and FedADC are equivalent formulations of GHB with fixed $\\tau=1$ (and hence, extra communication): this is shown analytically in table 1, but also experimentally in figure 3, where the performance of GHB $\\tau=1$ and FedCM are shown to be equivalent. This is the reason why we didn\u2019t include them in the main results, as we specified in the first paragraph of section 4.1. However, since this may not be immediately obvious to the reader, we added the values of FedCM and FedADC in the main tables, as you suggested, to dispel any doubts about the completeness of our experimental validation. \n* **W5:** The reason why we did not adopt a learning rate decay is because we considered that this would have caused unfair comparison since algorithms have very different convergence speeds. Please notice that this way of conducting the experiments is in line with most existing well-established works in FL [3,4,5,6,7].  However, to demonstrate that our findings hold even when adopting learning rate decay, we added in the supplementary an additional section showing the accuracy curves for the iid and non-iid training of CIFAR-100 with ResNet-20, with full hyperparameter search the learning rate decay and algorithm-specific hyperparameters. For ease of comparison, we also added in Table 10 the results for iid training of CIFAR-10/100 without learning rate decay."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5505/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700153601405,
                "cdate": 1700153601405,
                "tmdate": 1700153601405,
                "mdate": 1700153601405,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "HOWhrxfp8R",
                "forum": "oJ1tx3fXDA",
                "replyto": "n03gv530ok",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5505/Reviewer_8a6j"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5505/Reviewer_8a6j"
                ],
                "content": {
                    "title": {
                        "value": "Response to the rebuttal"
                    },
                    "comment": {
                        "value": "I have read all rebuttals from the authors and thanks for the explanation. I have to strengthen some unsolved concerns:\n\n1. Variance reduction is a rigorous algorithm in the optimization. The reason \"perceiving it as a variance reduction mechanism due to observed similarities\" is insufficient. I'm not sure if this calculation is equal to the VR algorithm mentioned in the article, this needs further confirmation in detail. It is best for the authors to provide some proof to show that this term has variance-reducing properties or asymptotic properties.\n\n2. The learning rate is constant. The authors claim that classical works adopt the constant learning rate. However, the works mentioned were mostly published 2021 years ago. Actually, as the baseline in this paper, FedCM and FedADC all adopt the learning rate decaying in their experiments. As I said in the comments, the experimental phenomena of constant learning rate and learning rate with decay term are very different. Therefore, I believe that the comparison in this article is biased to the real results. On the basis of the constant learning rate experiment, I think that the experiment of learning rate decay should at least be tested like the baseline papers.\n\nThanks for the good submission and I will keep the current score for the above main concerns."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5505/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700633108380,
                "cdate": 1700633108380,
                "tmdate": 1700633108380,
                "mdate": 1700633108380,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "BawgqxjwMI",
                "forum": "oJ1tx3fXDA",
                "replyto": "ABntaQdtTc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5505/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5505/Authors"
                ],
                "content": {
                    "title": {
                        "value": "About the remaining concerns"
                    },
                    "comment": {
                        "value": "Thanks for the feedback. About the remaining concerns:\n1. We appreciated your observation, and the rebuttal version provides a different intuition about that term. The current version does not claim that the term is equal to the algorithm previously mentioned or a variance reduction term.\n2. **About constant learning rate:** as the reviewer suggested in its initial review, we added these extra experiments in our revision (in both iid and non-iid scenarios, in appendix B.4, figure 8). As suggested and following the practice mentioned in FedDyn and FedCM, we used an exponential learning rate decay by decreasing the local learning rate by a $0.9999$ factor at each round. This value has been searched to be the optimal one among $\\\\{0.999, 0.9992, 0.9995, 0.9999\\\\}$. \\\nOur results are clear in proving that adopting a learning rate decay does not change the comparisons between algorithms in the setting tested ($\\alpha=0$ and $\\alpha=10000$) and showing that our method outperforms all the baselines.\\\nWe remark that all the works considered in our evaluation represent the state-of-the-art methods generally used as baselines in all the Federated Learning papers. \n\nGiven the requested experiments, it is unclear to us what is missing to fully convince the reviewer that our results are valid and our method has been fairly and extensively compared against the baselines."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5505/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700740317111,
                "cdate": 1700740317111,
                "tmdate": 1700741043446,
                "mdate": 1700741043446,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "U9poXipXwg",
            "forum": "oJ1tx3fXDA",
            "replyto": "oJ1tx3fXDA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5505/Reviewer_Yr1w"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5505/Reviewer_Yr1w"
            ],
            "content": {
                "summary": {
                    "value": "The paper adjusts and combines two existing momentum-based methods applied to federated learning. The first method (acceleration) is the heavy ball method (i.e., Polyak's momentum). The second method is a variance reduction technique [1]. The adjustment for these two methods is to parameterize the time step for the previous iterate of the momentum term for each client. For example, with the heavy ball, the momentum term becomes: $\\frac{\\beta}{\\tau_i}(\\theta^{t-1} - \\theta^{t-{\\tau_i}-1})$ where $\\tau_i$ is the time step parameter for client $i$. Experiments are conducted on both iid and non-idd federated learning datasets.\n\n[1] Cutkosky, A. and Orabona, F. Momentum-based variance reduction in non-convex SGD. 2019.\n\n\n\nUpdate: I thank the authors for replying to my review/questions. I have also read through the other reviews and responses. I will maintain my score."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. Explores the use of momentum for federated learning.\n2. The paper is easy to read. However, the writing needs to be improved."
                },
                "weaknesses": {
                    "value": "1. The novelty of this paper is limited. As mentioned in the summary, two existing momentum-based techniques are additively combined with the modification to adjust previous iterate time step parameter. In my opinion, to call the proposed formulation generalized heavy-ball momentum is somewhat a far-stretch.\n2. Some theoretical analysis not provided. Although averaging has been applied, it is not clear why having this gap or \"window\" is desirable in general and particularly in the federated learning setting. Motivation/intuition needs to be given.\n3. Not clear if the results are reproducible since code is not given.\n4. Missing some related work:\n   * Xin, R. and Khan, U. Distributed heavy-ball: a generalization and acceleration of first-order methods with grading tracking. 2018.\n   * Das, R. et al. Faster non-convex federated learning via global and local momentum. 2022.\n   * Kim, G. et al. Communication-Efficient Federated Learning with Acceleration of Global Momentum. 2022.\n\nAdditional: labels missing on graphs."
                },
                "questions": {
                    "value": "Very little information is provided on setting $\\tau_i$. Section A of supplemental states: $\\tau_i \\rightarrow \\tau = 1/C$ where $C$ is the number of clients. If this is the case, then the term appears negligible under large scale setting."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5505/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5505/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5505/Reviewer_Yr1w"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5505/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699130317669,
            "cdate": 1699130317669,
            "tmdate": 1700693990402,
            "mdate": 1700693990402,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "WyZGb0og4w",
                "forum": "oJ1tx3fXDA",
                "replyto": "U9poXipXwg",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5505/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5505/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the feedback. In the following, we address the reviewer\u2019s remarks on the weaknesses (W) and questions (Q).\n* **W1:** First, let us restate the novelty of our approach. The generalization we claim is based on the fact that, while standard momentum and its declinations in FL use the updates that occurred in the last round, we consider a larger window (GHB) across multiple rounds. While this modification results in a simple implementation, we showed that enlarging this window to a length $\\tau>1$ is crucial to developing the robustness against extreme heterogeneity (see fig. 3). On top of this idea, we developed communication-efficient variants of GHB, namely Local-GHB and our proposed FedHBM, which self-control the window length according to client participation and this is optimal both in terms of window length and communication costs (see again Fig 3). \nAs an additional point, we frame existing momentum FL algorithms inside this generalization, and show how they are special cases (see Table 1): in this sense, our work brings some order to the literature on momentum in FL, which is part of our contribution. \n* **W2:** We acknowledge that a formal proof could strengthen our paper. However, we also argue that theoretical convergence rates may only render an incomplete picture, whereas extensive experiments may reveal more than an elaborate yet very specific proof. As a matter of fact, in our experiments, empirical evidence demonstrated that in extreme cases state-of-art algorithms, despite being supported by formal guarantees, happen to actually fail (e.g. see Table 4). \\\n**Motivation behind a window $\\tau > 1$.** \\\nThe main intuition behind our method is that the trajectory of the server updates over a window $\\tau>1$ provides a better estimate for the momentum term in a federated setting. Intuitively, as $\\tau$ increases,  the momentum term increasingly incorporates information from a broader range of clients. A key observation is that when $\\tau$ equals the average period length (e.g. $\\tau = \\frac{1}{C}$), under uniform client sampling, the momentum term contains the information on the global distribution and hence is optimal.  We experimentally verified this hypothesis, demonstrating its validity in practice as we showed by purposely varying $\\tau$ in Figure 3. We revised the manuscript to better describe these details. \\\n**Is it always beneficial?** \\\nYes. According to our experiments, **in FL this is always beneficial** since it improves even in the iid scenario, where the simple FedAvgM is often a strong baseline. However, its effect is most noticeable in extreme non-iid scenarios where computing the momentum over a larger window mitigates the effects of heterogeneity and partial participation (see Fig. 2 and 3)..\n\n\n* **W3:** We take reproducibility into great consideration since we provided a precise algorithmic description in Algorithm 1 and accurately reported the datasets, the split procedures, the hyperparameters, and the framework used (see section B and in particular table 7). We did not attach the code to the original submission, but following your suggestion we are attaching it now, we hope this may dispel your concerns. \n* **W4:** Thanks for reporting to us these works we were not aware of, we will take the time to go into details and we will include them in our revision of the related works.\n* **Q1:** The formula reported is correct, but the term C is not the total number of clients. Rather, it is the participation ratio, i.e., the portion of clients that participate in each round (i.e., $0<C\\leq1$). In practice, $\\frac{1}{\\tau_i}$ term has the meaning of an averaging factor, and so it fits the number of contributions it averages. This is perhaps clearer by looking at the GHB row in Table 1 (right), which contains the expansion for a fixed $\\tau$.\nLet us remark that in Local-GHB and FedHBM, $\\tau_i$ is not hand-tuned, but it is instead determined stochastically by client participation: in practice, under uniform sampling, on average each client automatically considers a window of length $\\tau_i \\approx \\frac{1}{C}$. \nWe showed the optimality of  $\\tau_i$ under uniform client sampling in figure 2, which also crucially allows us to be communication-efficient, whereas hand-tuning $\\tau$ requires an overhead of $1.5\\times$ w.r.t. FedAvg.\n\nWe hope that our answer addresses your concerns and that you consider revising your rating of our work for an \u201caccept\u201d decision."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5505/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700153439234,
                "cdate": 1700153439234,
                "tmdate": 1700153439234,
                "mdate": 1700153439234,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9bkyRvAKn7",
            "forum": "oJ1tx3fXDA",
            "replyto": "oJ1tx3fXDA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5505/Reviewer_ZLT2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5505/Reviewer_ZLT2"
            ],
            "content": {
                "summary": {
                    "value": "This paper studies the problem of system and statistical (data) heterogeneity in the context of Federated Learning. Specifically, the authors propose a novel federated algorithm that utilizes a generalization of the the heavy-ball momentum method on the client side to achieve improved final accuracy and convergence speed in non-iid regimes both in cross-silo and cross-device settings. Extensive numerical results are presented both on academic data set as well as on real-world applications. These experiments showcase the superiority of the proposed FedHBM (and practical variation of this algorithm) in terms of communication cost, accuracy, and convergence speed compared to state of the art federated methods."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "-The paper studies an important problem in the area of Federated Learning namely the combination of system and statistical heterogeneity.\n\n-The paper is well structured and easy to follow.\n\n-A simple and intuitive algorithm is proposed that relies on a generalization of the heavy-ball momentum on the client side.\n\n-Both cross-silo and cross-device settings have been explored in the experiments. Both academic and real-world datasets have been studied.\n\n-Extensive ablation study exhibits the effects of $\\tau$ (captures which model is used for calculating the momentum i.e. from how many rounds in the past this model is chosen) on the performance of the algorithm.  Additionally, a practical variation of the algorithm has been presented without additional communication requirements with strong performance."
                },
                "weaknesses": {
                    "value": "-The main weakness of the paper lies on the absence of theoretical results. \n\n-The proposed algorithm is a rather simple generalization of the heavy-ball momentum and as a result the novelty is limited.\n\n-In the cross-device setting the algorithm requires a 'proper' initial model to achieve the required improvement.\n\n-In table 2 it seems that FEDDYN achieves higher target accuracy (90%) in the seemingly more challenging cross-device setting. I would appreciate it if the authors could elaborate on that.\n\n-In the plots of Figure 3 and Figure 4 (a) it is hard to distinguish between some of the methods. I would recommend to either utilize different colors or increase the scale. \n\n\nMinor issues\n\n- Page 3, paragraph 3, in\"..existing algorithms can be express as special..\" replace express by expressed.\n- Page 6, paragraph 4, in \"$\\alpha = 10k$\" define $k$.\n- In table 5  for $C\\approx 0.5$ FEDAVG's performance is in bold which appears to be a typo."
                },
                "questions": {
                    "value": "See weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5505/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699140691943,
            "cdate": 1699140691943,
            "tmdate": 1699636563092,
            "mdate": 1699636563092,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "T2W6W0U92b",
                "forum": "oJ1tx3fXDA",
                "replyto": "9bkyRvAKn7",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5505/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5505/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the feedback, we are pleased to notice that the reviewer appreciated our work. In the following, we address the reviewer\u2019s remarks on the weaknesses (W).\n* **W1:** We acknowledge that a formal proof could strengthen our paper. Certainly, a theoretical proof of convergence is one way to support the claims of a method, but it is not the only way. Here we chose to have an empirical approach, where we test extensively FedHBM and existing methods in a wide range of tasks, extreme scenarios, and real-world cases, as we are pleased you appreciated. By doing so we provide evidence that our method consistently outperforms existing methods which are supported by articulated formal proofs [1,2]. We also argue that theoretical convergence rates may only render an incomplete picture, whereas extensive experiments may reveal more than an elaborate yet very specific proof. As a matter of fact, in our experiments, empirical evidence demonstrated that in extreme cases state-of-art algorithms, despite being supported by formal guarantees, happen to actually fail (e.g. the case of FedDyn diverging already known in the literature, and SCAFFOLD and MimeMom in our table 4).\n* **W2:**  We agree that our algorithm is actually simple to implement, and we consider it as a strength. In fact, more often than not, very complex and articulated methods require careful tuning, which may be case-specific, and are prone to introducing coding errors. On the contrary, we think that _simplicity is the ultimate sophistication_ [Leonardo Da Vinci]. \nWe also believe that simplicity should not be considered as an indication of a lack of novelty.\nTo the best of our knowledge, we are the first to propose a generalization of the heavy-ball momentum computed over a period $\\tau>1$, and we showed that it has a significant impact on performance in FL (figure 3). Within this framework (GHB), FedHBM allows to capture **locally** (i.e., on the client side) global information that encompasses more than one round, without communication overhead. \nWe analyzed its practical advances under a wide range of scenarios, to support our claims of FedHBM being superior to the state-of-the-art FL methods. These results, as well as the practical demonstration of the failure of well-established FL algorithms in extreme non-iid cases, are themselves a novelty that adds to the knowledge base of the community, and points to new problems to be considered.\n* **W3:** Results in Table 4 use the formulation proposed in algorithm 1, and nonetheless our method is in most cases the best, so the practical variation is not strictly necessary to achieve good performance. Our point is that, although our algorithm is primarily designed for cross-silo scenarios, it can still be used for cross-device scenarios, as the variation we proposed can help obtain a speedup earlier (see Figure 4).\n\n\n* **W4:** Table 2 reports the number of rounds to reach a target accuracy w.r.t the accuracy of centralized training (reported in table 6), hence the lower the number the faster the algorithm. In a cross-device scenario, FedDyn reaches 90% of the accuracy reached by centralized training in 7600 rounds, which is worse than our FedHBM, which needs only 6510 rounds to reach the same result.\nNamely, table 2 reports the speed while Tables 3 and 4 report the final performance of the algorithms. Together, these results show that our algorithm is the most communication efficient and that it yields the best model quality.\n* **W5:** Thank you for your suggestion, we updated the manuscript accordingly.\n* **Minor issues:** Thanks for spotting the typos, we fixed them. On page 6, par. 4, $k$ simply stands for $\\times1000$. As we understand it can generate confusion, we replaced it with $\\alpha=10.000$\nWe hope that our answer addresses your concerns and that our revision will contribute positively to the evaluation of our work.\n\n\n[1] Scaffold: Stochastic controlled averaging for federated learning, PMLR 2020 \\\n[2] Breaking the centralized barrier for cross-device federated learning, NeurIPS 2021"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5505/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700153271806,
                "cdate": 1700153271806,
                "tmdate": 1700153271806,
                "mdate": 1700153271806,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "JmOWgzisBE",
                "forum": "oJ1tx3fXDA",
                "replyto": "T2W6W0U92b",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5505/Reviewer_ZLT2"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5505/Reviewer_ZLT2"
                ],
                "content": {
                    "title": {
                        "value": "Post Rebuttal"
                    },
                    "comment": {
                        "value": "I thank the authors for addressing my concerns and for implementing minor revisions to their draft. After carefully reading the authors rebuttal and the comments from the other reviewers I consider this paper to lie on the acceptance threshold and I slightly lean towards acceptance. Although the experimental results appear promising, the main drawbacks of the paper (absence of theoretical results and limited novelty especially in light of \"Enhance Local Consistency in Federated Learning: A Multi-Step Inertial Momentum Approach\") remain."
                    }
                },
                "number": 13,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5505/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700702263765,
                "cdate": 1700702263765,
                "tmdate": 1700702263765,
                "mdate": 1700702263765,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "z2oHPQ40VU",
            "forum": "oJ1tx3fXDA",
            "replyto": "oJ1tx3fXDA",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission5505/Reviewer_wwM7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission5505/Reviewer_wwM7"
            ],
            "content": {
                "summary": {
                    "value": "Th paper proposes a new Federated Learning (FL) algorithm based on heavy-ball momentum, designed to be more robust to statistical data heterogeneity without significant communication cost, compared to state-of-the-art FL techniques. The method proceeds by computing local momentum at the client level, and results in a novel algorithm called FedHBM (heavy ball momentum) which generalizes existing momentum based FL algorithms. The empirical properties of the method are illustrated on vision and NLP tasks, and compared favorably to competitors.\n\nI have read the authors comments which clarified some important aspects of the experiments performed. Accordingly, I upgraded the \"soundness\" rate to 3-good, as well as my overall rating to 8."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper is very well written, clear and dynamic. Both mathematical statements and intuitive explanation are very clear and easy to follow.\n- The proposed methodology, which efficiently exploits participation of clients at multiple optimization rounds to estimate local momentum without increasing communication cost is novel and clever.\n- The relation with prior work is very clear and thoroughly discussed, in terms of general properties, precise mathematical formulation and empirical performance\n- The limitations of the FedHBM are discussed, particularly in the case of high cross-device settings, and alternatives are considered to alleviate them."
                },
                "weaknesses": {
                    "value": "- The paper does not provide any insight on how to set the values of the step sizes \\eta and \\beta. In addition, the authors do not mention how they set it for their experiments. This is a major limitation to practical use of the algorithm. \n- In relation with aforementioned issue, the authors do not discuss how other hyperparameters were set for competitors, thus comparisons are not easy to interpret (was their hyperparameter optimization for all algoritms ?)\n- Overall, the rationale for the choice of the results presented (nb of rounds to reach accuracy level, final model accuracy) is not completely clear. For instance, the computational cost per round is not discussed, so it is difficult to understand the implication of these results. In addition, the non-iid setting is not clear. The authors mention using a Dirichlet distribution, which suggests drawing vectors of probabilities to assign samples to clients, but I don't see how this would lead to heterogeneity (only size imbalance between clients)."
                },
                "questions": {
                    "value": "- How were step sizes \\eta and \\beta set in the experiments ? How about similar hyperparameters of competitors ?\n- Is the computational cost per round similar between algorithms ?\n- What does final mean ? Are all algorithms stopped after 10000 rounds ?\n- Could you provide more insight on how you designed the non iid setting ? How do you split the data ? How do you use the Dirichlet distribution ? Is it used to draw probabilities of assignment to each client ? Then, how do you assign samples to clients ? It should depend on the sample value, and I don't see it in your explanations."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission5505/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission5505/Reviewer_wwM7",
                        "ICLR.cc/2024/Conference/Submission5505/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission5505/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699269049393,
            "cdate": 1699269049393,
            "tmdate": 1700729939507,
            "mdate": 1700729939507,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "dA4zwC6zDp",
                "forum": "oJ1tx3fXDA",
                "replyto": "z2oHPQ40VU",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5505/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5505/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "We thank the reviewer for the feedback, we are pleased to notice that the reviewer appreciated our work. In the following, we address the reviewer\u2019s remarks on the weaknesses (W) and questions (Q).\n* **W1-W2-Q1:** For clarity, let us restate the hyperparameters involved in our method. $\\eta$ is the server learning rate, $\\eta_l$ is the client learning rate, $\\hat{\\beta_i}$ is our momentum factor, defined as $\\frac{\\beta C}{J_i}$, where $J_i$ is the number of local steps and $C$ is the participation ratio ($0<C\\leq1$). The hyperparameters to be tuned are $\\eta$, $\\eta_l$, and $\\beta$.\nFor each method, the hyperparameters have been searched via a grid search. As we take into great consideration reproducibility, in Table 7 we reported our extensive hyperparameter search for all the algorithms, indicating in bold the best values.\n* **W3-Q2:** We assumed the computational cost per round to be equal for all algorithms, as usually, communication cost is the bottleneck. By doing so, we acted conservatively, as Mime, MimeMom, and MimeLiteMom actually require multiple forward and backward passes on the local dataset, and so require much more computations (we refer to [1] for additional details on their algorithm). All the other algorithms (including ours) have similar computational costs since they just require addition/subtraction to model parameters, which are not expensive. As a matter of fact, this is reflected in the wall-clock training time of our simulations, as indicated in section B.2, under the paragraph \u201cImplementation details and practicality of experiments\u201d. \n* **W3-Q4:** Regarding the non-iid setting, we followed the practice in [2], which is common in FL works. In practice, it consists of drawing class probabilities for each client using a Dirichlet distribution with parameter $\\alpha$. The lower the value of $\\alpha$, the higher the class imbalance we introduce into local datasets. We acknowledge your suggestion, and we added some more words in the supplementary B.1 to better explain the procedure in [2].\n\n\n* **Q3:** _final model quality_ means the top-1 accuracy over the last 100 rounds of training, averaged over 5 independent runs. The number of rounds depends on the dataset and the model used, and it is equal to all the algorithms. For example, for the CIFAR-10 dataset and LeNet, the round budget is fixed at 10.000 rounds (for more details, see Table 8)\nWe hope that our answer addresses your concerns and that our revision will contribute positively to the evaluation of our work. Please let us know if there are any remaining concerns and if our answers clarify your doubts.\n\n[1] Breaking the centralized barrier for cross-device federated learning, NeurIPS 2021 \\\n[2] Federated Visual Classification with Real-World Data Distribution, ECCV 2020"
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5505/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700153143219,
                "cdate": 1700153143219,
                "tmdate": 1700153143219,
                "mdate": 1700153143219,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Zi5Smy5AVu",
                "forum": "oJ1tx3fXDA",
                "replyto": "dA4zwC6zDp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission5505/Reviewer_wwM7"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission5505/Reviewer_wwM7"
                ],
                "content": {
                    "comment": {
                        "value": "Many thanks for your answers which clarify the experiments, I upgraded my score accordingly."
                    }
                },
                "number": 14,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission5505/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729970357,
                "cdate": 1700729970357,
                "tmdate": 1700729970357,
                "mdate": 1700729970357,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]