[
    {
        "title": "How Does Wild Data Provably Help OOD Detection?"
    },
    {
        "review": {
            "id": "2KOOKp43sz",
            "forum": "jlEjB8MVGa",
            "replyto": "jlEjB8MVGa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1419/Reviewer_ZN1X"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1419/Reviewer_ZN1X"
            ],
            "content": {
                "summary": {
                    "value": "This study addresses wild Out-Of Distribution (OOD) detection, benefiting from the availability of more unlabeled data to enhance OOD data identification. The paper proposes the utilization of the top singular value as a criterion to differentiate between In-Distribution (ID) and OOD samples. The authors, grounded in novel theoretical insights, posit that ID samples should exhibit larger top singular values compared to OOD samples. Both experimental results and theoretical analyses corroborate the effectiveness of the proposed method, Separate And Learn (SAL). Overall, this work stands out for its solid foundation."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- Using gradients to make distinctions is a common and intuitive approach that makes sense to me. How ever, the method of distinguishing based on the top singular value is both intriguing and non-obvious. This innovative discovery holds significant importance for OOD detection.\n\n- The availability of extra unlabeled data undoubtedly enables wild OOD detection to outperform traditional OOD detection, which does not utilize unlabeled data. However, what I find astonishing is that the performance of wild OOD detection surpasses even that of outlier exposure, showcasing its remarkable effectiveness.\n\n- The method is supported by a theoretical examination of its generalization bounds, ensuring a solid foundation for reliable ML."
                },
                "weaknesses": {
                    "value": "- One of my major concerns with the paper lies in the discrepancy assumption stated in Theorem 1. To validate this, additional experiments with diverse datasets are imperative to ascertain whether the assumption is consistently met in practical scenarios. While I acknowledge that assumptions are essential for theoretical foundations, and the experiments in Appendix F provide a good start, they are insufficient for a comprehensive evaluation. I recommend extending the empirical analysis to additional datasets (e.g., CIFAR-10 as ID and another data as OOD), with the goal of thoroughly investigating the generality of the assumption and its ability to underpin SAL robustly.\n\n- In the majority of the experiments conducted, the value of pi is set to 0.1. To ensure a comprehensive evaluation, it is crucial to conduct additional experiments with varied values of pi."
                },
                "questions": {
                    "value": "Please address the issues outlined in the weaknesses. This work is solid and good. So addressing all of my concerns comprehensively will lead me to reconsider and potentially raise my score."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1419/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1419/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1419/Reviewer_ZN1X"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1419/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698499784974,
            "cdate": 1698499784974,
            "tmdate": 1699636070329,
            "mdate": 1699636070329,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "XqyuGuxONs",
                "forum": "jlEjB8MVGa",
                "replyto": "2KOOKp43sz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1419/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1419/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZN1X--part I"
                    },
                    "comment": {
                        "value": "We are deeply encouraged that you recognize our method to be novel, significant, and solid in both the algorithm and theory and with remarkable empirical results.\n\nYour summary and comments are insightful and spot-on :)\n\n**A1. Verification of the assumptions on additional datasets**\n\nThank you for the suggestion! As suggested, we verified the assumption of distribution discrepancy using CIFAR-10 as ID and five other OOD datasets, i.e.,  SVHN, PLACES365, LSUN-C, TEXTURES, and LSUN-R. The result is shown as follows and in **Appendix Section W**, and we can conclude that $\\zeta$ can indeed satisfy the regulatory condition in Theorem 2, i.e., $\\zeta > 1.011\\sqrt{\\pi}$.\n\n#### SVHN\n|  $\\pi$      | 0.05|    0.1     | 0.2     |   0.5 |        0.7 |      0.9   | 1.0 |\n| ------ | ----- | ----- |----- | ----- |----- | ----- |----- | \n| $\\zeta$ | 0.26 | 0.37 |0.49 |  0.71 |0.97 |  1.24| 1.36|  \n| 1.011 $\\sqrt{\\pi}$ |  0.23 |0.32| 0.45 |0.71| 0.84| 0.96| 1.0 |\n\n#### PLACES365\n|  $\\pi$      | 0.05|    0.1     | 0.2     |   0.5 |        0.7 |      0.9   | 1.0 |\n| ------ | ----- | ----- |----- | ----- |----- | ----- |----- | \n| $\\zeta$ | 0.28 | 0.33 |0.53 |   0.77| 0.85|0.98  | 1.04|   \n| 1.011 $\\sqrt{\\pi}$ |  0.23 |0.32| 0.45 |0.71| 0.84| 0.96| 1.0 |\n\n#### LSUN-C\n|  $\\pi$      | 0.05|    0.1     | 0.2     |   0.5 |        0.7 |      0.9   | 1.0 |\n| ------ | ----- | ----- |----- | ----- |----- | ----- |----- | \n| $\\zeta$ | 0.29 | 0.34 |0.47 |  0.72 | 0.87| 1.09 | 1.20  | \n| 1.011 $\\sqrt{\\pi}$ |  0.23 |0.32| 0.45 |0.71| 0.84| 0.96| 1.0 |\n\n#### TEXTURES\n|  $\\pi$      | 0.05|    0.1     | 0.2     |   0.5 |        0.7 |      0.9   | 1.0 |\n| ------ | ----- | ----- |----- | ----- |----- | ----- |----- | \n| $\\zeta$ |  0.28| 0.33 |0.46 | 0.74  |0.85 |  0.96| 1.05| \n| 1.011 $\\sqrt{\\pi}$ |  0.23 |0.32| 0.45 |0.71| 0.84| 0.96| 1.0 |\n\n#### LSUN-R\n|  $\\pi$      | 0.05|    0.1     | 0.2     |   0.5 |        0.7 |      0.9   | 1.0 |\n| ------ | ----- | ----- |----- | ----- |----- | ----- |----- | \n| $\\zeta$ | 0.28 | 0.35 |0.47 | 0.73  | 0.87|  1.10 | 1.22 | \n| 1.011 $\\sqrt{\\pi}$ |  0.23 |0.32| 0.45 |0.71| 0.84| 0.96| 1.0 |"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1419/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699996042919,
                "cdate": 1699996042919,
                "tmdate": 1699996042919,
                "mdate": 1699996042919,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "YPpZnPrDRp",
            "forum": "jlEjB8MVGa",
            "replyto": "jlEjB8MVGa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1419/Reviewer_dSEG"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1419/Reviewer_dSEG"
            ],
            "content": {
                "summary": {
                    "value": "Leveraging unlabeled data has shown potential in enhancing the safety and reliability of machine learning models for out-of-distribution (OOD) detection, despite the challenges posed by the heterogeneity of both in-distribution (ID) and OOD data. This paper introduces SAL (Separate And Learn), a novel learning framework that addresses the existing gap in understanding how unlabeled data aids OOD detection by providing strong theoretical guarantees and empirical effectiveness. SAL operates by isolating potential outliers from the unlabeled data, training an OOD classifier with these outliers and labeled ID data, and achieving state-of-the-art performance on standard benchmarks, thereby validating the theoretical framework and its components."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The manuscript presents some theoretical analyses as well as a number of intriguing illustrations.\n2. The experimental results look promising, with comparisons made against numerous baseline methods."
                },
                "weaknesses": {
                    "value": "1. The manuscript lacks crucial baselines, such as [1] and [2], which are essential for a comprehensive evaluation, and fails to provide an analysis or comparison with them.\n2. The essential topic of the article is weakly supervised out-of-distribution detection, although it is described from different perspectives.\n\n[1] Zhou, Zhi, et al. \"Step: Out-of-distribution detection in the presence of limited in-distribution labeled data.\" Advances in Neural Information Processing Systems 34 (2021): 29168-29180.\n\n[2] He, Rundong, et al. \"Topological structure learning for weakly-supervised out-of-distribution detection.\" arXiv preprint arXiv:2209.07837 (2022)."
                },
                "questions": {
                    "value": "1. Near OOD Scenario: It is unclear how effective the method presented in this article would be in a near OOD scenario, such as treating the first 50 classes of CIFAR-100 as ID and the last 50 classes as OOD. This specific situation could pose challenges since near OOD samples may share similarities with the ID data, potentially affecting the method's performance.\n2. Generalization Performance Across Different Backbones: The article does not provide information on how well the method generalizes when different backbone architectures are used. The performance stability of the method when transitioning between various model architectures is a critical aspect to consider for its widespread applicability.\n3. Generalization Performance on unseen OOD data: There is no discussion on the method's effectiveness when the test OOD data and the OOD data in the unlabeled set are not identically distributed. Understanding how the method handles such disparities is crucial for evaluating its robustness in real-world scenarios."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1419/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1419/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1419/Reviewer_dSEG"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1419/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698590237525,
            "cdate": 1698590237525,
            "tmdate": 1700043254560,
            "mdate": 1700043254560,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AMULUlzkX8",
                "forum": "jlEjB8MVGa",
                "replyto": "YPpZnPrDRp",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1419/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1419/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer dSEG--part I"
                    },
                    "comment": {
                        "value": "We thank you for recognizing our method to be novel with promising results. We thank the reviewer for the thorough comments and suggestions, which we address below:\n\n**A1. Baselines**\n\nAs suggested, we have additionally compared with the two related works (TSL [1] and STEP [2]). To ensure a fair comparison, we strictly follow the experimental setting in TSL [1], and rerun SAL under the identical setup. The comparison on CIFAR-100 is shown as follows. Accordingly, we have also added discussion and proper citations of the mentioned papers in the revised paper (See **related work section and Appendix Section U**). We thank you for pointing them out!\n\n|           |LSUN-C |        |LSUN-R |     | \n| ------ | ----- | ----- |----- | ----- |\n| Method | FPR95 | AUROC |FPR95 |  AUROC |\n| STEP | **0.00**  |99.99 |9.81 |97.87|\n|   TSL |**0.00** | **100.00**| 1.76|  99.57 |\n|SAL (Ours)|**0.00**| 99.99 |**0.58**| **99.95**| \n\n**A2. Discussion on weakly supervised OOD detection**\n\nWe agree that weakly supervised OOD detection is indeed similar to the problem setting of SAL. We have already updated the **related work section** of our paper and included more discussions/citations on weakly supervised OOD detection. Thank you for pointing this out!\n\n**A3. Near-OOD Scenario**\n\nWe are glad you bring that up. We have already evaluated the near-OOD detection in **Appendix J**. Specifically, we use the CIFAR-10 as the in-distribution data and the CIFAR-100 as the OOD data in the wild. During test time, we use the test set of CIFAR-100 as the OOD for evaluation. With a mixing ratio $\\pi$ of 0.1, SAL achieves an FPR95 of 24.51% and AUROC of 95.55% compared to 38.92% (FPR95) and 93.27% (AUROC) of WOODS.\n\n\nIn addition, we follow the suggested data setting by the reviewer, i.e., the first 50 classes of CIFAR-100 as ID and the last 50 classes as OOD. The comparison with the most competitive baseline is reported as follows. We have also added the new results to **Appendix Section J** in the revised manuscript.\n\n|        |  CIFAR-50|              | |\n| ------ | ----- | ----- |----- | \n| Method | FPR95 | AUROC |ID ACC|\n| WOODS | 41.28 | 89.74| 74.17|\n|SAL (Ours)| **29.71**|**93.13** |73.86|"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1419/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699995929848,
                "cdate": 1699995929848,
                "tmdate": 1699995929848,
                "mdate": 1699995929848,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "IQbxpbruWt",
            "forum": "jlEjB8MVGa",
            "replyto": "jlEjB8MVGa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1419/Reviewer_XJRS"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1419/Reviewer_XJRS"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a novel setting for OOD detection, termed as \u201dwild OOD detection,\u201d building upon the foundation established by the preceding work \u201dTraining OOD Detectors in their Natural Habitats.\u201d A novel methodology, denoted as SAL, is presented, encapsulating at wo-stage process comprising filtering and classification components. Empirical evaluations have demonstrated that SAL achieves SOTA performance, boasting substantial improvements over existing methods. Additionally, theoretical underpinnings are provided to bolster the credibility and effectiveness of SAL."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. A theory has been established to investigate aspects of separability and learnability. This contribution is both novel and significant. \n\n2. Experimental evaluations conducted on standard benchmarks demonstrate that SAL achieves SOTA performance. \n\n3. A novel method grounded in theory has been developed to advance safe machine learning practices. Theory serves as a crucial driver in this endeavour. I am very happy to see the novel work on provable OOD detection."
                },
                "weaknesses": {
                    "value": "1. Could you provide explanations or conduct experiments to elucidate the factors contributing to the decreased ID accuracy depicted in Table 1? \n\n2. Why is pi set to 0.1 in most experiments? Could you conduct additional experiments to investigate whether pi remains robust across a range of values? Furthermore, does the performance of pi align with theoretical predictions? \n\n3. It appears that the top singular vector is crucial for SAL. Have you conducted any experiments to demonstrate the performance when considering the top 2, top 3, ..., top k singular vectors?\n\n4. What would occur if you were to use the gradient norm in place of the top singular vector? Could you elucidate the rationale behind opting for the top singular vector instead of the norm?"
                },
                "questions": {
                    "value": "Please refer to the Weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1419/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1419/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1419/Reviewer_XJRS"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1419/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698701001277,
            "cdate": 1698701001277,
            "tmdate": 1700688502186,
            "mdate": 1700688502186,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5bTSVt4tUi",
                "forum": "jlEjB8MVGa",
                "replyto": "IQbxpbruWt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1419/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1419/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer XJRS"
                    },
                    "comment": {
                        "value": "We are glad to see that the reviewer finds our work significant and novel from various perspectives. We thank the reviewer for the thorough comments and suggestions. We are happy to clarify as follows:\n\n**A1. ID accuracy**\n\nGreat observation! As explained in **paragraph 2 of Section 5.2** in the original submission, the slight discrepancy is due to that our method only observes 25,000 labeled ID samples, whereas baseline methods (without using wild data) utilize the entire CIFAR training data with 50,000 samples. We have used bold fonts to highlight it in the revision.\n\n**A2. Additional experiment results with varying $\\pi$**\n\nThank you for your suggestion! In our main experiment, we default $\\pi$ to be 0.1, which strictly follows the original setting in WOODS [1]. This reflects the practical scenario that the majority of test data may remain ID. Compared to larger $\\pi$, our setting with $\\pi=0.1$ is also more challenging due to limited information of OOD data.\n\nWe would like to point the reviewer to the **Appendix Table 6** of the original submission, where we report the OOD detection result and the filtering error on SVHN with different mixing ratios $\\pi$. The result aligns well with our observation of the bounds presented in Section 4.1 of the main paper.\n\nDuring rebuttal, we provide additional results on more OOD datasets with varying $\\pi$, i.e., 0.05, 0.2, 0.5, 0.9, and contrast with the baselines, which are added to **Appendix Section T**, and also shown below (CIFAR-100 as the in-distribution dataset). We found that the advantage of SAL still holds. \n\n|        | SVHN|         | Places365  |       |LSUN-C |        |LSUN-R |     | Textures|        | ID ACC |\n| ------ | ----- | ----- |----- | ----- |----- | ----- |----- | ----- |----- | ----- |----- |\n| $\\pi=0.05$|\n| Method | FPR95 | AUROC |FPR95 |  AUROC |FPR95 | AUROC |FPR95 | AUROC |FPR95 | AUROC | |  |  |\n|   OE | 2.78 | 98.84|63.63 |80.22 | 6.73| 98.37| 2.06| 99.19 | 32.86| 90.88| 71.98|\n| Energy w/ OE|2.02 | 99.17| 56.18|  83.33| 4.32| 98.42| 3.96| 99.29|  40.41 | 89.80 | 73.45|\n | WOODS |0.26 |99.89 |32.71   | 90.01| **0.64**| 99.77| **0.79**| 99.10 | 12.26| 94.48| 74.15|\n |SAL (Ours)| **0.17** | **99.90** | **6.21** | **96.87** |0.94 | **99.79**|0.84 |**99.37** |**5.77** | **97.12**|73.99 | |\n| $\\pi=0.2$|\n| Method | FPR95 | AUROC |FPR95 |  AUROC |FPR95 | AUROC |FPR95 | AUROC |FPR95 | AUROC | |  |  |\n|   OE | 2.59|  98.90| 55.68| 84.36| 4.91|99.02 | 1.97| 99.37|25.62 |93.65 | 73.72|\n| Energy w/ OE|1.79 | 99.25| 47.28|86.78 |  4.18| 99.00| 3.15| 99.35|36.80 | 91.48| 73.91|\n| WOODS |  0.22| 99.82 | 29.78 | 91.28 | 0.52|99.79 | 0.89|99.56 |10.06 |95.23 |73.49|\n|SAL (Ours)| **0.08** | **99.92**| **2.80**| **99.31**|**0.05**| **99.94** | **0.02**| **99.97**| **5.71**| **98.71**| 73.86|\n| $\\pi=0.5$|\n| Method | FPR95 | AUROC |FPR95 |  AUROC |FPR95 | AUROC |FPR95 | AUROC |FPR95 | AUROC | |  |  |\n|   OE |  2.86| 99.05| 40.21| 88.75| 4.13|99.05  | 1.25| 99.38|22.86 | 94.63| 73.38| \n| Energy w/ OE| 2.71| 99.34| 34.82|90.05 | 3.27|99.18 |  2.54|99.23| 30.16| 94.76|72.76|\n| WOODS | 0.17| 99.80| 21.87| 93.73| 0.48| 99.61| 1.24| 99.54  | 9.95|95.97 | 73.91|\n|SAL (Ours)| **0.02** | **99.98** |  **1.27**| **99.62**| **0.04**| **99.96** | **0.01**| **99.99**| **5.64**|**99.16** |73.77|\n| $\\pi=0.9$|\n| Method | FPR95 | AUROC |FPR95 |  AUROC |FPR95 | AUROC |FPR95 | AUROC |FPR95 | AUROC | |  |  |\n|   OE | 0.84 | 99.36| 19.78| 96.29|  1.64| 99.57  | 0.51| 99.75  |12.74  |94.95 | 72.02 | \n| Energy w/ OE| 0.97| 99.64|17.52 |96.53 | 1.36|99.73 |0.94 | 99.59  | 14.01 | 95.73| 73.62|\n| WOODS | 0.05 | 99.98| 11.34| 95.83| 0.07 | **99.99** | 0.03| **99.99**|6.72 |98.73 | 73.86|\n|SAL (Ours)|  **0.03** | **99.99** |  **2.79**|**99.89** |**0.05**| **99.99**| **0.01**| **99.99**| **5.88**| **99.53**| 74.01|\n\n[1] Julian Katz-Samuels et al. Training ood detectors in their natural habitats. In International Conference on Machine Learning, 2022.\n\n**A3. Additional experiments on multiple principal components**\n\nAnother great point! In our original submission, we reported results using multiple principal components in **Appendix Section K**. We observed that using the top 1 singular vector for projection achieves the best performance. \n\n**A4. Discussion on using gradient norm**\n\nWe have already evaluated the GradNorm score as suggested in **Table 2**, where we replace the filtering score in SAL with the GradNorm score and then train the OOD classifier. The result underperforms SAL, showcasing the effectiveness of our filtering score. \n\nWe have also extensively discussed the design rationale of the filtering scores of SAL in **Section 3.1**, saying that the scores in SAL for ID and OOD data are shown to be provably well-separated (**Remark 1**) and thus ensure a low filtering error, while the norm of the gradient is not. Both the theoretical result and empirical verification can demonstrate the advantage of SAL compared with GradNorm."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1419/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699995659753,
                "cdate": 1699995659753,
                "tmdate": 1699995659753,
                "mdate": 1699995659753,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "n5Uz7is9ST",
                "forum": "jlEjB8MVGa",
                "replyto": "5bTSVt4tUi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1419/Reviewer_XJRS"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1419/Reviewer_XJRS"
                ],
                "content": {
                    "title": {
                        "value": "Official Comment by Reviewer XJRS"
                    },
                    "comment": {
                        "value": "Thank you for your reply. The clarifications and additional experimental results solved all of my concerns. Therefore, I have raised my rating of this work."
                    }
                },
                "number": 11,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1419/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700688575281,
                "cdate": 1700688575281,
                "tmdate": 1700688575281,
                "mdate": 1700688575281,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "hnjDxVo5iO",
            "forum": "jlEjB8MVGa",
            "replyto": "jlEjB8MVGa",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1419/Reviewer_BtG8"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1419/Reviewer_BtG8"
            ],
            "content": {
                "summary": {
                    "value": "This paper presents a novel framework for Out-Of-Distribution (OOD) detection, named SAL, which aims to improve machine learning models through regularization using unlabeled data. SAL comprises two main components: (1) Filtering\u2013distinguishing potential outliers from the general dataset, and (2) Classification utilizing the identified candidate outliers to train an OOD classifier. The paper includes pertinent theoretical proofs to substantiate the proposed method, and experimental results are provided to demonstrate its effectiveness."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1 SAL\u2019s methodology is structured around two distinct phases\u2014screening and classification\u2014which can be independently optimized, offering enhanced flexibility.\n2 Utilizing a Large Volume of Unlabeled Data: SAL effectively leverages substantial amounts of unlabeled data to extract valuable information, thereby bolstering its detection capabilities.\n3 Theoretical Support: Beyond its impressive empirical performance, SAL is underpinned by robust theoretical foundations."
                },
                "weaknesses": {
                    "value": "1 In scenarios where the actual OOD data markedly diverges from the outliers present in the unlabeled dataset, there arises a question regarding the preservation of SAL\u2019s performance.\n 2 The efficacy of SAL is significantly influenced by the quality of the unlabeled data employed, indicating a substantial dependence on data integrity."
                },
                "questions": {
                    "value": "1 Could you design experiments to further confirm and answer issues in weaknesses?\n2 In table 1, how can this discrepancy be accounted for in datasets or scenarios that perform well in other methods but have degraded performance (ID ACC) in SAL?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1419/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698835114810,
            "cdate": 1698835114810,
            "tmdate": 1699636070067,
            "mdate": 1699636070067,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "GKkMJSdE73",
                "forum": "jlEjB8MVGa",
                "replyto": "hnjDxVo5iO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1419/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1419/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer BtG8"
                    },
                    "comment": {
                        "value": "We thank the reviewer for the thorough comments and suggestions. We are encouraged that you recognize our method to be novel and effective, and with robust theoretical analysis. We address your questions below:\n\n\n**A1. Different outlier dataset from the actual test OOD**\n\nWe are glad you bring that up! In the original submission, we have included the result where the OOD data in the wild is different from the test OOD data (please see **Appendix Section I and Table 8**). Specifically, we use 300K RANDOM IMAGES from outlier exposure [1] to create the wild OOD training dataset. We evaluate on SVHN, PLACES365, LSUN-C, LSUN-RESIZE, and TEXTURES as the test unseen OOD data. We observe that SAL can perform competitively on unseen OOD datasets as well, compared to the most relevant baseline WOODS.\n\n[1] Hendrycks et al. Deep anomaly detection with outlier exposure. In Proceedings of the International Conference on Learning Representations, 2019.\n\n**A2. Quality of the unlabeled data**\n\nTo address your concern, we have designed the following experiment where the quality of the unlabeled data deteriorates. The results of SAL and competitive baselines are shown in the table below and have also been added to the **Appendix Section S**.\n\nSpecifically, we corrupt the outlier data in the wild with additive Gaussian noise [2]. As such, the filtered candidate outliers will have a much lower quality compared to the outliers in SAL. We use the CIFAR-10 as the in-distribution dataset and keep other configurations the same. \n\n|        | SVHN|         | Places365  |       |LSUN-C |        |LSUN-R |     | Textures|   | Average |    | ID ACC |\n| ------ | ----- | ----- |----- | ----- |----- | ----- |----- | ----- |----- | ----- |----- |----- |----- |\n| Method | FPR95 | AUROC |FPR95 |  AUROC |FPR95 | AUROC |FPR95 | AUROC |FPR95 | AUROC |FPR95 | AUROC |  |\n|   OE | 23.11 |  86.61| 32.01| 86.27| 22.98| 82.75 | 19.53| 87.43  | 25.68 |84.46 |  24.66 |85.50 |93.81 |\n | Energy w/ OE| 26.76 | 85.91|26.09 | 87.48|22.32 | 82.26|  22.69 | 85.77|27.49 |82.18 | 25.07 |84.72 |92.38 |   \n| WOODS | 18.33 | 89.83| 23.45| 90.04| 19.70|  84.27| 17.79| 90.82| 22.37| 84.83| 20.33|   87.95| 94.00|   \n|SAL (Ours)|**15.23** | **91.22**| **18.23**| **93.51**| **14.62**| **89.04**| **13.93**| **91.82**| **18.58**| **92.42**|**16.12** |**91.60** | 93.91| \n\n\n[2] Hendrycks et.al., Benchmarking neural network robustness to common corruptions and surface variations. In Proceedings of the International Conference on Learning Representations, 2019.\n\n**A3. ID accuracy**\n\nGreat observation! As explained in **paragraph 2 of Section 5.2** in the original submission, the slight discrepancy is due to that our method only observes 25,000 labeled ID samples, whereas baseline methods (without using wild data) utilize the entire CIFAR training data with 50,000 samples. We have used bold fonts to highlight it in the revision."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1419/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1699995585994,
                "cdate": 1699995585994,
                "tmdate": 1699995585994,
                "mdate": 1699995585994,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]