[
    {
        "title": "Fairness without Sensitive attributes via Noise and Uncertain Predictions"
    },
    {
        "review": {
            "id": "sPiXGCbeWY",
            "forum": "7OwML7fwl8",
            "replyto": "7OwML7fwl8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4806/Reviewer_zU7R"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4806/Reviewer_zU7R"
            ],
            "content": {
                "summary": {
                    "value": "This paper addresses \"fairness without demographics\" by first splitting the training data into low-confident and high-confident subsets because they observe that low-confident encodes the knowledge of fairness while high-confident is for discriminative information. The refinement is based on dual VAEs in which the high-conf one is to extract most label-related features for classification while simultaneously injecting fairness knowledge from the low-conf one (in an EMA style). Low-conf one is a set unsupervised whose latent representation encouraged to be close to high-conf samples's expectation. They demonstrated the idea on new Adult and COMPAS datasets."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. The overall framework looks good and two stage presentation is clear and logical.\n2. The idea of training dual VAEs for two subsets and passing fairness knowledge from low-conf to high-conf is interesting.\n3. The improvement on New Adult dataset is significant in terms of EO metric."
                },
                "weaknesses": {
                    "value": "1. The inconsistence between motivation, methodology, and experiments. It will be better if authors can position this paper in a more consistent way.\n2. I felt that although authors tried to provide some hints before presenting a component of the method, some of the refinement techniques are still heuristic, which underscores the overall method quality.\n\nI will give the detailed comments in Questions."
                },
                "questions": {
                    "value": "1. In the first paragraph of Introduction, the evaluation to works that maximize the utility of the worst-case group is not precise. The Rawsian fairness (no sensitive attribute) based research leveraged side information (e.g., group ratio) to identify protected group, instead of relying on the correlation with observed features. Following this concern, DRO and ARL used as baselines in this paper are not very suitable, because they both highlighted that accuracy-related utility should be equal across different groups. Note that EO and DP are not criteria designed for these works, although you can conduct so.\n\n2. Given a threshold of 0.6 for splitting the training dataset and the resultant subsets have the distinct performance on EO and DP, which might be questionable. Recalling the definition of EO and DP, we see that they are both computed over predicted y. Since low-conf data tend to appear near to decision boundary, they certainly will yield small EO and DP values. Thus, letting low-conf data represent fairness is not very convincing for me. OOD data is NOT unbiased data.\n\n3. In the refinement stage, notice that only high-conf data will be trained for classification while low-conf examples only contributes some \"fair features\" in a regularization style. However, from Fig.2, the label in the purple box tried to suggest all data were well mapped to their ground truth labels. So, have you checked if the model has correctly classified low-conf data after training? If yes, why not incorporate a supervised loss therein? If not, how to guarantee a better generalization on test set?\n\n4. As low-conf generator is thought having the desired fairness knowledge, then how about using its mu and sigma as a pseudo supervision (regression) for high-conf data? Any theoretical or experimental evidence of your method? Basically, you are minimizing the distance between any two of N(mu1, sigma1), N(mu2, sigma2), and N(0,I).\n\n5. Generator in the method should refer to the entire VAE. As the final model only takes encoder, can you clarify the role of decoder during training? \n\n6. Two compared methods are from Chai's recent work, leading me to check the connection with this baseline. In Chai's work, they have pointed out the samples near to the decision influence the fairness, while this paper starts from confidence, similar to my insight mentioned above. Authors should clarify their connections.\n\n7. The applied baselines are not very supportive to the targeted challenges. I felt confused why using proxy attributes and fairness-accuracy trade-off works are not included. Also, why only two datasets are used in the paper? Are the proposed method restricted to some specific datasets? \n\n8. Regarding the learnable noise, it is like a patch to this framework, as you have to learn additional model parameters. To make latent features only related to label, one can encourage H(z,y) and reduce H(x,z), from the information theory perspective."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4806/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4806/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4806/Reviewer_zU7R"
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "NA"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4806/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698406195263,
            "cdate": 1698406195263,
            "tmdate": 1699636463427,
            "mdate": 1699636463427,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "8Hblq0g92G",
                "forum": "7OwML7fwl8",
                "replyto": "sPiXGCbeWY",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4806/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4806/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "First of all, we would like to thank you for all these inspiring reviews. All the ideas and hints are very much helpful for improvement of our work in this paper. Especially on the concerns of our motivations and the suggestions on the role of learnable noise. We appreciate the recognition of parts of this work from an expert. Thank you for your contribution to this community. Here are some of our thoughts according to your reviews: \n\n1. *In the first paragraph of Introduction, the evaluation to works that maximize the utility of the worst-case group is not precise. The Rawsian fairness (no sensitive attribute) based research leveraged side information (e.g., group ratio) to identify protected group, instead of relying on the correlation with observed features. Following this concern, DRO and ARL used as baselines in this paper are not very suitable, because they both highlighted that accuracy-related utility should be equal across different groups. Note that EO and DP are not criteria designed for these works, although you can conduct so.* \n> **1.** Thank you for this correction. Indeed, DRO\u2019s lower bound and upper bound are impacted by group proportion. In the new version, we change to \u201cmethods that focus on ensuring that accuracy-related utility is equal across various demographic groups.\u201d \nDRO and ARL used as baselines in this paper are not very suitable  \n> **2.** The reason we want to include these two baselines is because they are important methods to deal with fairness without sensitive attributes, though they are in a different branch with us. We self-identify our method as implicit method since we deal with fairness via representation (e.g. mitigating bias from input information). In the new version of paper, we add different marks to better indicate the different types of applied baselines in the table for clearance to our readers.\n\n2. *Given a threshold of 0.6 for splitting the training dataset and the resultant subsets have the distinct performance on EO and DP, which might be questionable. Recalling the definition of EO and DP, we see that they are both computed over predicted y. Since low-conf data tend to appear near to decision boundary, they certainly will yield small EO and DP values. Thus, letting low-conf data represent fairness is not very convincing for me. OOD data is NOT unbiased data.* \n> **Why showcase the values of EO and DP**\uff1fThe results presented in the paper regarding EO and DP values aim to provide readers with a more intuitive understanding of how the model tends to be misled by data with high differentiability. Our emphasis is not on highlighting fairness but rather on underscoring the severity of unfairness in the assessments of these data, considered easily differentiable. This becomes evident through a comparison of EO and DP values between these specific data points and the entire dataset. Building on this, our approach develops a framework to correct the model\u2019s biases and alleviate unfairness. In the revised version of the paper, we have modified explanations for certain analytical results.\n > **Representing fairness with low-confidence data is inaccurate:**  Indeed, the values of EO and DP tend to decrease near the decision boundary. In the new version of paper, we will retain the EO and DP values from high-conf dataset and train_set, and present the number of samples comparison according to different small subsets e.g. # of white vs. # of black with positive labels to better demonstrate our motivations. Since the definition of EO and DP, presenting the number of samples also can help to connect fairness and confidence level."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4806/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729105804,
                "cdate": 1700729105804,
                "tmdate": 1700729105804,
                "mdate": 1700729105804,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lZSbx9p74Y",
            "forum": "7OwML7fwl8",
            "replyto": "7OwML7fwl8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4806/Reviewer_4bz2"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4806/Reviewer_4bz2"
            ],
            "content": {
                "summary": {
                    "value": "The paper studies how to mitigate bias when having no access to the full sensitive attributes. The idea is that they find empirically the low-confidence samples (predicted by a classifier) are more biased than the high-confident samples. Therefore, they train a classifier to split the data into low- and high-confidence subsets, and then train VAE on each data together, and the final prediction comes from both VAEs."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The problem is a practical and important problem given it is harder and harder to access full sensitive attributes"
                },
                "weaknesses": {
                    "value": "1. I have trouble understanding the high-level insights of the paper. The authors did not do a good job of presenting their method. e.g. why do they need VAEs? If the idea is to leverage low- and high-confidence data, why not just train two separate classifiers and then use ensemble? What is the purpose of adding learnable noise? I do not understand the author's explanation in 4.3.1. What is $\\eta$ in Eq. (1)? What is the corresponding mathematical definition of \"Pseudo-distribution\" in Figure 1? Is it $\\mathcal{L}_L$ in Eq.(2)? In general, I think the technical part is poorly written, and would cause unnecessary confusion to readers.\n\n2. Can authors explain why the design can mitigate bias well when has no access to full sensitive attributes? The sensitive attribute $S$ is rarely mentioned after the problem formulation in Section 3. How does the method connect to missing $S$? I might miss it, but it shows the paper does not highlight how the method works.\n\n3. The experiment on COMPAS does not seem to outperform other methods in an obvious way. The two fairness measures improve but the accuracy also drops. It would be clearer if the results could be presented in an accuracy vs. fairness Pareto frontier style plot.\n\n4. The evaluation is done only on two tabular datasets. This is rare in fairness literature. Can authors justify why it is only tested on two tabular datasets?"
                },
                "questions": {
                    "value": "See weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4806/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698733610160,
            "cdate": 1698733610160,
            "tmdate": 1699636463332,
            "mdate": 1699636463332,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fFlGCd0vCp",
                "forum": "7OwML7fwl8",
                "replyto": "lZSbx9p74Y",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4806/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4806/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your time. \n1. *I have trouble understanding the high-level insights of the paper. The authors did not do a good job of presenting their method. e.g. why do they need VAEs? If the idea is to leverage low- and high-confidence data, why not just train two separate classifiers and then use ensemble? What is the purpose of adding learnable noise? I do not understand the author's explanation in 4.3.1. What is \ud835\udec8 in Eq. (1)? What is the corresponding mathematical definition of \"Pseudo-distribution\" in Figure 1? Is it \u2112L in Eq.(2)? In general, I think the technical part is poorly written, and would cause unnecessary confusion to readers.*\n> **Why not ensemble?:** We actually considered this ensemble way in designing the proposed method, but it is not working in the way we want. It makes the prediction more unstable. Preserving the High-Conf\u2019s weights, and adding Low-Conf\u2019s weights, it is like a regulation on High-Conf\u2019s weights, preventing it from getting biased on to those feature distribution patterns of the majority. \n> **Why vae?:** We use vae for two main reasons: First reason is its effectiveness presenting through the experiment results and second reason is that we use the decoder\u2019s output, which is the reconstructed input in the ablation study part. Observations through ablation study part help us to understand the reason of using learnable noise and pseudo-learning tricks together.  We admit that there are other model can improve the performance, such as two-layer MLP, which we will also include the experiment results of using MLP in the new version of this paper. Please, have a look.\n\n2. *Can authors explain why the design can mitigate bias well when has no access to full sensitive attributes? The sensitive attribute S is rarely mentioned after the problem formulation in Section 3. How does the method connect to missing S ? I might miss it, but it shows the paper does not highlight how the method works.*\n> **1.** To be frank, we are supervised by receiving these requirements of explaining the role of sensitive attributes. We clearly state that sensitive attributes are omitted during the training phase in Section 3 - \u2018Problem Definition\u2019, and we also provide motivation for this omission throughout the paper, even the title itself indicates that sensitive attributes are not used in the proposed framework. While it is disappointing to receive such comments, we have added a clear statement at the beginning of the last item in the summarised main points in the Introduction section. We hope that this new statement will help prevent any misunderstanding.\n> **2.** We explain the working flow in the overview part in Part 4 and also in the caption of Figure 2. Please, have a look.\n\n3. *The experiment on COMPAS does not seem to outperform other methods in an obvious way. The two fairness measures improve but the accuracy also drops. It would be clearer if the results could be presented in an accuracy vs. fairness Pareto frontier style plot.* \n> Thank you for the suggestion. Yes, it is more clear to present an accuracy vs fairness plot in the paper to help our readers, and we will include it in the new version of paper \n\n4. *The evaluation is done only on two tabular datasets. This is rare in fairness literature. Can authors justify why it is only tested on two tabular datasets?*\n>  We admit more datasets can help readers to understand the permanence of the proposed framework. Hence, we will include new experiments using one new dataset, CelebA for better present our proposed method\u2019s generalisation on different datasets."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4806/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729543387,
                "cdate": 1700729543387,
                "tmdate": 1700729543387,
                "mdate": 1700729543387,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Z4kFGDxOte",
            "forum": "7OwML7fwl8",
            "replyto": "7OwML7fwl8",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission4806/Reviewer_KsYL"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission4806/Reviewer_KsYL"
            ],
            "content": {
                "summary": {
                    "value": "This study tackles the problem of fairness without demographics by initially partitioning the training data into subsets characterized by low and high confidence. The rationale behind this division lies in the observation that low-confidence encodes fairness-related knowledge, while high-confidence pertains to discriminative information. The refinement process employs dual Variational Autoencoders (VAEs), where the high-confidence VAE extracts label-related features for classification. Simultaneously, it injects fairness knowledge from the low-confidence VAE, following an Exponential Moving Average (EMA) style. The low-confidence VAE operates in an unsupervised manner, with its latent representation encouraged to be proximate to the expectation of high-confidence samples. The efficacy of this approach is demonstrated on Adult and COMPAS datasets."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1) The problem of fairness without sensitive attributes is important and practical.\n\n2) The writing of this article is well-organized."
                },
                "weaknesses": {
                    "value": "1) (main concern) The proposed method is based on the observation that \u201cwhen data is close to the decision boundary, non-sensitive information associated with those data tends to be similarly distributed across demographic groups, leading to lower accuracy but increased fairness\u201d. However, it is unclear why and when it happens. For example, does this just happen to be due to the data distribution nature of the COMPAS dataset? The author should provide more explanations and discussions about this, theoretically or empirically, since this key property affects the scope of application of the proposed method.\n\n2) It is also unclear that why the learnable noises need to be added in this paper. Is it to block information of sensitive information? In addition to the final classification results, I suggest that the author give some relevant analytical experiments to prove the validity of the learned noises. For example, if we train a classifier for sensitive attributes on the data with learned noises, is it difficult to predict sensitive attributes accurately?\n\n3) (main concern) It seems that the compared baselines are not strong enough. Please note that some works have gone beyond the baselines used in this paper such as CvaR DRO, LfF, JTT. Moreover, only two tabular datasets are used. I encourage the author to perform experiments on more datasets to illustrate the effectiveness of the proposed method."
                },
                "questions": {
                    "value": "Please refer to Weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission4806/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4806/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission4806/Reviewer_KsYL"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission4806/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699677077891,
            "cdate": 1699677077891,
            "tmdate": 1699677077891,
            "mdate": 1699677077891,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "xkXGmPhnV2",
                "forum": "7OwML7fwl8",
                "replyto": "Z4kFGDxOte",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission4806/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission4806/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for the reviews helping to improve our work. We appreciate your suggestions on better data analysis for the paper\u2019s motivation and the interesting experiment on learnable noise for better illustration of its effectiveness.   \n\n1. *(main concern) The proposed method is based on the observation that \u201cwhen data is close to the decision boundary, non-sensitive information associated with those data tends to be similarly distributed across demographic groups, leading to lower accuracy but increased fairness\u201d. However, it is unclear why and when it happens. For example, does this just happen to be due to the data distribution nature of the COMPAS dataset? The author should provide more explanations and discussions about this, theoretically or empirically, since this key property affects the scope of application of the proposed method.*\n> Thank you for this suggestion. Yes the data analysis is conducted on one dataset, COMPAS. If we have a look on experiment results, the most improvement is in another dataset, New Adult. We argue that this is evidence of good generalisation of our proposed method. However, we appreciate this suggestion, hence we will add data analysis on New Adult in the new version of the paper. \n\n2. *It is also unclear that why the learnable noises need to be added in this paper. Is it to block information of sensitive information? In addition to the final classification results, I suggest that the author give some relevant analytical experiments to prove the validity of the learned noises. For example, if we train a classifier for sensitive attributes on the data with learned noises, is it difficult to predict sensitive attributes accurately?*\n> Thank you for the suggestion. It is a very interesting idea of training a classifier for sensitive attributes. May you help us to interpret this experiment better? We will be appreciated if we know why this can indicate learnable noises. In our assumptions, we argue the learnable noise is a sort of data augmentation technique, really focusing on how to retain the prediction-only information. Hence, it may also help to predict sensitive attributes and have good prediction results compared to vanilla ML models such as logistic regression. \nIn the beginning, we wanted to use learnable noise because of one simple idea. If we have multiple sensitive proxies in the dataset and want to find them all out, one naive way is mask each attribute and train a classifier to see the performance and the fairness evaluation. Currently we have an ablation study result that helps us to understand the effectiveness of learnable noise. We ask the decoder to generate reconstructed input and compare it with the original input. We find if we use learnable noise, although it is very different from the original input, it has the most accurate predictions in all the experiments. Hence we argue the learnable noise is necessary for this proposed framework for accurate predictions.\n\n3. *(main concern) It seems that the compared baselines are not strong enough. Please note that some works have gone beyond the baselines used in this paper such as CvaR DRO, LfF, JTT. Moreover, only two tabular datasets are used. I encourage the author to perform experiments on more datasets to illustrate the effectiveness of the proposed method.*\n> Thank you for the suggestion. We will include new experiments using one new dataset, CelebA for better present our proposed method\u2019s generalisation on different datasets. For the suggested baselines, we intend to just use DRO and ARL in the comparisons with our work since these two focusing on worst-group performance, ours is not. We include DRO and ARL because we don\u2019t want to miss the two mile stones in this area. Other baselines are chosen from recent research about fairness without missing sensitive attributes/demographic which are representing each different branch of methods. In the new version of paper, we will include proxy method and fairness-accuracy trade-off works."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission4806/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700729629857,
                "cdate": 1700729629857,
                "tmdate": 1700729629857,
                "mdate": 1700729629857,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]