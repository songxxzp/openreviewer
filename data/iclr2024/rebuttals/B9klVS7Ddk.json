[
    {
        "title": "Compressing LLMs: The Truth is Rarely Pure and Never Simple"
    },
    {
        "review": {
            "id": "ot5bsHf75X",
            "forum": "B9klVS7Ddk",
            "replyto": "B9klVS7Ddk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3743/Reviewer_X7p4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3743/Reviewer_X7p4"
            ],
            "content": {
                "summary": {
                    "value": "The paper benchmarks compressing techniques, pruning and quantization, on various datasets and metrics, highlighting that a common evaluation metric, perplexity, does not always translate into real-world values."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper is well-written, all the plots are clear and comprehensive. It's easy to understand information in most of the cases (see weaknesses for improvement). \nAuthors correctly pinpoint that most of the compression algorithms do not validate extensively on real-world scenarios and show some interesting insights. For example, there is evidence that pruning may not work as well as quantization methods. Or that 8-bit quantization works well on majority on datasets. These insights are easy to use and can have a big impact to how practitioners use these models."
                },
                "weaknesses": {
                    "value": "A few ways to improve this paper.\n1. More LLMs. Right now, it's only vicuna model (7B and 13B), having other architectures and perhaps bigger sizes (e.g. llama 70b) would add more evidence for the insights. \n2. Add prompt designs to the main body of the paper. This will make it easy to understand how each task is distinct. \n3. More quantization methods. AWQ/SmoothQuant are interesting to see."
                },
                "questions": {
                    "value": "Why do we see these performance degradations? Any way to explain why some methods work and some do not? \nAre there are any counter forces one can use to mitigate performance degradation while still preserving the quality?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3743/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1697633932627,
            "cdate": 1697633932627,
            "tmdate": 1699636330356,
            "mdate": 1699636330356,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pbMF0llHAW",
                "forum": "B9klVS7Ddk",
                "replyto": "ot5bsHf75X",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3743/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3743/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors Response to X7p4"
                    },
                    "comment": {
                        "value": "We first like to thank you for your time to review our work. We greatly appreciate that you have found our work to be well-written, comprehensive, and have interesting insights. We are glad that you think our work can have a big impact on how practitioners use these models. For the ease of use, we reiterate that we will be releasing codes, experimental details, and host the datasets on Dropbox, which will help LLM compression researchers use LLM-KICK with ease for evaluating their novel algorithms. \n\nNext, we would like to address all the weaknesses pointed by you point-by-point below:\n\n> **1. More LLMs. Right now, it's only vicuna model (7B and 13B), having other architectures and perhaps bigger sizes (e.g. llama 70b) would add more evidence for the insights.**\n\nThank you for your suggestion and we strongly agree that extending our experiments beyond Vicuna will bolster our work. In that direction, we have included some additional experiments with LLaMa-2 as you suggested in Appendix A6 which obey our previous findings. For larger model size, we additionally provide some results for Vicuna-33B as follows which again aligns with our conclusion. We also have plans to run our experiments on llama-2 70B as pointed out by you, and include in our final draft (rebuttal duration doesn\u2019t permit us to complete this experiment).\n\n|Factoid-QA|Dense|10%|20%|30%|40%|50%|60%|\n| ------------- |:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|\n|Magnitude|76.92|76.72|71.73|59.44|46.75|42.46|3.10|\n|SparseGPT|76.92|73.93|70.43|61.24|50.85|42.16|29.37|\n|Wanda|76.92|73.63|72.73|65.03|50.65|52.05|39.36|\n\n> **2. Add prompt designs to the main body of the paper. This will make it easy to understand how each task is distinct.**\n\nThank you for this great suggestion. We agree that the addition of prompt design/example will improve the distinguishability of our task settings. We will surely work towards incorporating this valuable suggestion into our final draft.\n\n> **3. More quantization methods. AWQ/SmoothQuant are interesting to see.**\n\nThank you for your suggestion. As also pointed out by b73d, we considered evaluating two more quantization methods AWQ and LLM.int8() across our different task settings and we summarize our results on Vicuna-7B as in the following table. We observe that LLM.int8() despite its simplicity and ease-of-use, achieves better results than AWQ (8-bit), and GPTQ (8-bit) across all listed tasks. Note that we couldn\u2019t complete our experiments on summarization and instruction following due to dependency and time constraints (on OpenAI) but we will update them in the final draft.\n\n\n| | GPTQ | AWQ | LLM-int() | \n| ------------- |:-------------:|:-------------:|:-------------:|\n|**Factoid-QA**|60.14%|60.31%|61.02%|\n|**MCR-QA (MMLU)**|47.10%|47.18%|47.82%|\n|**Retrieval Augmented QA**|75.55%|75.89%|75.91%|"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3743/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700333493181,
                "cdate": 1700333493181,
                "tmdate": 1700333493181,
                "mdate": 1700333493181,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "CH77B1FDYi",
            "forum": "B9klVS7Ddk",
            "replyto": "B9klVS7Ddk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3743/Reviewer_b73d"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3743/Reviewer_b73d"
            ],
            "content": {
                "summary": {
                    "value": "This paper revisits the efficiency of some compression (pruning and quantization) techniques for LLMs. It conveys that more than perplexity is needed for performance comparison among compressed and uncompressed models. It displays several tasks and benchmarks over which the compressed models exhibit performance degradation despite having similar perplexities. \n\nEssentially, the proposed benchmarks show that quantization if mostly more efficient than pruning where structured pruning appears to offer least ML performance. Interestingly, the paper also shows that even 8-bit quantization if not on par with the uncompressed baseline. \n\nImportantly, the paper states that their related codes are planed to be open-sourced"
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. Compression of LLMs is very timely and important.\n\n2. The paper reveals new and yet widely unknown gaps in compressed LLMs in comparison to their uncompressed counterparts. \n\n3. The paper shows that compressed models may offer better performance in some tasks (e.g., In-Context Text Summarization) than others (e.g., Factoid-based Question Answering)\n\n4. The authors plan to release their code which may be help in the development of future compression techniques."
                },
                "weaknesses": {
                    "value": "1. It would make the conclusions more robust and convincing if the evaluations use more than a single family of LLMs (i.e., Vicuna).  Why not repeat these experiments with, e.g., Llama 2 and Falcon?\n\n2. Regarding the observation that even 8-bit quantization has evident gaps with respect to uncompressed models, have the authors considered evaluating LLM.int8()?  (https://arxiv.org/pdf/2208.07339.pdf) \n\n3. It would help the reader to have a table summarizing all the tasks' performance over the different architectures, compression techniques and the their resulting perplexity."
                },
                "questions": {
                    "value": "See weakness 1 and 2. Also, does the authors have insights regarding why, in the paper's evaluation, quantization works better than pruning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3743/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3743/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3743/Reviewer_b73d"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3743/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698053952737,
            "cdate": 1698053952737,
            "tmdate": 1699636330286,
            "mdate": 1699636330286,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pHw4QbSPM9",
                "forum": "B9klVS7Ddk",
                "replyto": "CH77B1FDYi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3743/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3743/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors Response to b73d"
                    },
                    "comment": {
                        "value": "We would first like to thank you for the time to review our work. We greatly appreciate that you have found our work to be important, timely, and revealing novel widely unknown gaps in compressed LLMs in comparison to their uncompressed counterparts. We again assert that we will be releasing codes, experimental details, and host the datasets on Dropbox, which will help LLM compression researchers use LLM-KICK with ease for evaluating their novel algorithms.\n\nNext, we would like to address all the weaknesses pointed by you point-by-point below:\n\n\n> **1. Extending experiments beyond Vicuna Family?**\n\nThank you for your suggestion and we strongly agree that extending our experiments beyond a single family of LLMs will bolster our work. In that direction, we have included some additional experiments with LLaMa-2 as you suggested in Appendix A6 (Factoid-QA, MCR-QA, ICR-QA) which obey our previous findings. For larger model size, we additionally provide some results for Vicuna-33B as follows which again aligns with our conclusion. We are running more experiments on larger LLMs which will be included in our final draft.\n\n|Factoid-QA|Dense|10%|20%|30%|40%|50%|60%|\n| ------------- |:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|:-------------:|\n|Magnitude|76.92|76.72|71.73|59.44|46.75|42.46|3.10|\n|SparseGPT|76.92|73.93|70.43|61.24|50.85|42.16|29.37|\n|Wanda|76.92|73.63|72.73|65.03|50.65|52.05|39.36|\n\n\n\n> **2. Comparison with LLM.int8() ?**\n\nThank you for your suggestion. As also pointed out by X7p4, we considered evaluating AWQ and LLM.int8() across our different task settings and we summarize our results on Vicuna-7B as in the following table. We observe that LLM.int8() despite its simplicity and ease-of-use, achieves better results than AWQ (8-bit), and GPTQ (8-bit) across all listed tasks. Note that we couldn\u2019t complete our experiments on summarization and instruction following due to dependency and time constraints but we will update them in the final draft.\n\n| | GPTQ | AWQ | LLM-int() | \n| ------------- |:-------------:|:-------------:|:-------------:|\n|**Factoid-QA**|60.14%|60.31%|61.02%|\n|**MCR-QA (MMLU)**|47.10%|47.18%|47.82%|\n|**Retrieval Augmented QA**|75.55%|75.89%|75.91%|\n\n> **3. Table summarizing all the tasks' performance?**\n\nThank you again for your great suggestion and we agree that a table summarizing all task performance will be very helpful for the readers to quickly digest our experimental insights. We promise to include it in our final draft with an additional page. \n\n> **4. Quantization works better than pruning?**\n\nTo answer this question, we would like to bring attention to Junk DNA Hypothesis (JDH) recently proposed by https://arxiv.org/abs/2310.02277 . According to JDH, small-magnitude weights may appear \"useless\" for simple tasks and suitable for pruning (in most SoTA pruning methods), but actually encode crucial knowledge necessary for solving more difficult downstream tasks. Removing these seemingly insignificant weights can lead to irreversible knowledge forgetting and performance damage in difficult tasks. Their finding reveals fresh insights into how LLMs encode knowledge in a task-sensitive manner and pruning them significantly hurt the performance. On the other hand, quantization doesn\u2019t remove these small-magnitude weights completely, which can explain why quantization is more friendly to pruning for LLMs. Another recent work, https://arxiv.org/abs/2307.02973 provides a detailed thorough analysis but wrt small-scale models like ResNet, EfficientNet, etc.\n\n*We sincerely hope our responses answer your questions, and we again would like to take this opportunity to thank you for highly rating our work.*"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3743/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700333060117,
                "cdate": 1700333060117,
                "tmdate": 1700333060117,
                "mdate": 1700333060117,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9Ad2wXRhPV",
            "forum": "B9klVS7Ddk",
            "replyto": "B9klVS7Ddk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3743/Reviewer_PUep"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3743/Reviewer_PUep"
            ],
            "content": {
                "summary": {
                    "value": "This paper benchmarks a few LLM compression methods based on quantization and pruning, on different datasets."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "I think it is important to have a more fine-grained understanding of compression methods, specially to design new algorithms that can improve upon current weaknesses."
                },
                "weaknesses": {
                    "value": "- This paper is essentially benchmarking a few algorithms on a few datasets. Although the insights are interesting, the paper does not include any new model, data or algorithm, which I'd say makes this paper more suitable for a workshop, not a full conference paper.\n\n- Some arguments are rather subjective. Why choose the 5% threshold? If we change the threshold to 10% it seems 4-bit quantization is then in the range in most cases, and sparse models can still be \"competitive\" for around 50% sparsity.\n\n- The loss of accuracy also has to be contextualized with the inference time speedups. If a 5% loss of accuracy leads to a 10% reduction in inference time, I'd call that successful."
                },
                "questions": {
                    "value": "The authors say that SparseGPT is data-free. Is that true?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3743/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3743/Reviewer_PUep",
                        "ICLR.cc/2024/Conference/Submission3743/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3743/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698190592164,
            "cdate": 1698190592164,
            "tmdate": 1700545734661,
            "mdate": 1700545734661,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "AfzrHuUC0H",
                "forum": "B9klVS7Ddk",
                "replyto": "9Ad2wXRhPV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3743/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3743/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Authors Response to PUep"
                    },
                    "comment": {
                        "value": "We would first like to thank you for the time to review our work. We deeply appreciate that you have found that our work provides a more fine-grained understanding of compression methods. We are glad that you think our work provides interesting insights for SoTA LLM compression methods. \n\nNext, we would like to address all the weaknesses pointed by you point-by-point below:\n\n> **paper does not include any new model, data or algorithm, which I'd say makes this paper more suitable for a workshop, not a full conference paper**\n\nWe politely disagree with your comment that without proposing any new model, data, or algorithm, a submitted work is not suitable for a conference paper. To provide more context, [1,2,3,4,5,6,7,8] are some among numerous examples of top full conference papers (some cited >150) which doesn\u2019t propose any new model, data, or algorithm. We kindly request you to acknowledge that scientific research can encompass novelty in various forms. We believe that thoroughly exploring existing solutions, and providing novel insights holds equal significance to the pursuit of new methods or architectures. We also very politely request you to look at other reviewers who find our work very important, timely, and can have a big impact on the LLM compression community. \n\n[1] NeurIPS\u201919. A Meta-Analysis of Overfitting in Machine Learning, https://papers.nips.cc/paper_files/paper/2019/hash/ee39e503b6bedf0c98c388b7e8589aca-Abstract.html \n\n[2] NeurIPS\u201920. The Lottery Ticket Hypothesis for Pre-trained BERT Networks https://proceedings.neurips.cc/paper/2020/file/b6af2c9703f203a2794be03d443af2e3-Paper.pdf \n\n[3] CVPR\u201921. The Lottery Tickets Hypothesis for Supervised and Self-supervised Pre-training in Computer Vision Models https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_The_Lottery_Tickets_Hypothesis_for_Supervised_and_Self-Supervised_Pre-Training_in_CVPR_2021_paper.pdf \n\n[4] CVPR\u201923. Reproducible scaling laws for contrastive language-image learning, https://openaccess.thecvf.com/content/CVPR2023/papers/Cherti_Reproducible_Scaling_Laws_for_Contrastive_Language-Image_Learning_CVPR_2023_paper.pdf \n\n[5] ICLR\u201923 - Top25%: Sparsity May Cry: Let Us Fail (Current) Sparse Neural Networks Together! https://openreview.net/forum?id=J6F3lLg4Kdp \n\n[6] ICLR\u201922: The Unreasonable Effectiveness of Random Pruning: Return of the Most Naive Baseline for Sparse Training https://arxiv.org/pdf/2202.02643.pdf \n\n[7] NeurIPS\u201922: Subgroup Robustness Grows On Trees: An Empirical Baseline Investigation https://proceedings.neurips.cc/paper_files/paper/2022/file/408cf1a1d9ff35d5fea7075565dbf434-Paper-Conference.pdf \n\n[8] ACL\u201923: Understanding Factual Errors in Summarization: Errors, Summarizers, Datasets, Error Detectors https://aclanthology.org/2023.acl-long.650/  \n\n> **Some arguments are rather subjective. Why choose the 5% threshold? If we change the threshold to 10% it seems 4-bit quantization is then in the range in most cases, and sparse models can still be \"competitive\" for around 50% sparsity.**\n\nThank you for bringing this up and we are very happy to clarify your doubts. Firstly, **our threshold is simply an indicator to illustrate the tolerance level of performance drop** when we start compressing any LLM. Many prior works [1,2,3] consider matching thresholds to be the same as the dense subnetwork performance or within the margins of 1%. However, in our work, we carefully relaxed it to 5% performance drop as an acceptable tolerance (before calling the compressed model useless) keeping in mind that the performance of compressed LLM on any of our task categories/disciplines remains above the random guess. For example, to provide context, as you have mentioned, 10% threshold on MMLU can leave performance on some disciplines like Algebra, Econometrics drop to 19.7%, 21.1% which is below the random guess (25%) given MMLU has one correct out of four choices. Again, note that this threshold has nothing to do with any performance or insight reported in our draft. We will add additional clarification in our final draft.\n\n\n[1] CVPR\u201921: The Lottery Tickets Hypothesis for Supervised and Self-supervised Pre-training in Computer Vision Models \n\n[2] NeurIPS\u201923: The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter\n\n[3] NeurIPS\u201920: The Lottery Ticket Hypothesis for Pre-trained BERT Networks"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3743/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700274286472,
                "cdate": 1700274286472,
                "tmdate": 1700274286472,
                "mdate": 1700274286472,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "P3zoz7ZgXk",
                "forum": "B9klVS7Ddk",
                "replyto": "5xh2VlOQBO",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3743/Reviewer_PUep"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3743/Reviewer_PUep"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. \n\n- I think your point about certain tasks being worse than random makes sense. I'm not sure if this is clarified in the paper, but it would be good to discuss such objectively bad cases, rather than focusing on some threshold. \n\n- I still think inference time is important and should be contained in the comparisons. \n\nBased on this, I will increase my score to 5, though I think without inference time the paper still lacks."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3743/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700545777769,
                "cdate": 1700545777769,
                "tmdate": 1700545777769,
                "mdate": 1700545777769,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "88m33eXZuL",
            "forum": "B9klVS7Ddk",
            "replyto": "B9klVS7Ddk",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3743/Reviewer_QryF"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3743/Reviewer_QryF"
            ],
            "content": {
                "summary": {
                    "value": "The paper is very timely and identifies an important gap in evaluation of compression on LLMs. It points out how perplexity is not a correct metric to evaluate compression benchmarks (which is also previously observed in other contexts). They curate a set of datasets which can form a better representation of language model capabilities."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- timely ... with an array of papers on compressing LLMs with especially surprising results such as training free pruning coming out. It is important to enable researchers with better tools of evaluation\n- provides a decent array of dataset benchmarks that will be use ful in research.\n- clearly shows the gap between evaluation of perplexity and other proposed datasets."
                },
                "weaknesses": {
                    "value": "Not weaknesses. but suggestions. \n1. add a summarizing table to list dataset statistics."
                },
                "questions": {
                    "value": "None"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3743/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699120473439,
            "cdate": 1699120473439,
            "tmdate": 1699636330102,
            "mdate": 1699636330102,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "egzrm0Qjob",
                "forum": "B9klVS7Ddk",
                "replyto": "88m33eXZuL",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3743/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3743/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer QryF"
                    },
                    "comment": {
                        "value": "We would first like to thank you for the time to review our work. We greatly appreciate that you have found our work to be important, timely, and significantly necessary considering the growing interest in LLM compression, and the lack of any standard evaluation protocol to elucidate the merits and plights of novel algorithms.  \n\nWe also significantly value your suggestion to include the dataset statistics, and we will update our Appendix A5 to provide more details. We also have plans to release codes, experimental details, and host the datasets on Dropbox, which will help LLM compression researchers use LLM-KICK with ease for evaluating their novel algorithms. \n\nWe again take this opportunity to thank you for highly rating our work."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3743/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700273894134,
                "cdate": 1700273894134,
                "tmdate": 1700273894134,
                "mdate": 1700273894134,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]