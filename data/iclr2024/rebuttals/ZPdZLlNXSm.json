[
    {
        "title": "Mean Field Theory in Deep Metric Learning"
    },
    {
        "review": {
            "id": "Fgo9BKpaLr",
            "forum": "ZPdZLlNXSm",
            "replyto": "ZPdZLlNXSm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2363/Reviewer_q1qw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2363/Reviewer_q1qw"
            ],
            "content": {
                "summary": {
                    "value": "In this paper, the mean field theory is introduced into the domain of deep metric learning. By incorporating foundational components such as the Contrastive loss and Class Wise Multi-Similarity loss, the authors construct the Mean Field Contrastive loss and Mean Field Class Wise Multi-Similarity loss. The proposed method is evaluated through extensive experiments on benchmark datasets, covering two benchmark protocols. The results demonstrate the effectiveness of the proposed approach."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "1. The authors have integrated the mean field theory into the realm of deep metric learning. \n\n2. They have introduced two pair-based loss functions, namely the Mean Field Contrastive loss and the Mean Field Class Wise Multi-Similarity loss. \n\n3. The experiments and ablation studies conducted in the paper are comprehensive."
                },
                "weaknesses": {
                    "value": "Deep metric learning involves a range of loss functions, and it is unclear whether the mean field theory can be applied to other loss functions commonly used in this context. It would be valuable for the authors to specify under what conditions and contexts the mean field theory is applicable and offer practical guidance for its implementation in other scenarios."
                },
                "questions": {
                    "value": "How can the mean field theory be extended to encompass other commonly used loss functions in deep metric learning?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission2363/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2363/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission2363/Reviewer_q1qw"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2363/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698305304405,
            "cdate": 1698305304405,
            "tmdate": 1699636168926,
            "mdate": 1699636168926,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "5LWrWsczGp",
                "forum": "ZPdZLlNXSm",
                "replyto": "Fgo9BKpaLr",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2363/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2363/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review!"
                    },
                    "comment": {
                        "value": "Thank you for your insightful comments and the opportunity to discuss the extensions of our work. We are grateful for the chance to elaborate on how Mean-Field Theory (MFT) can further enhance deep metric learning.\n\nTo address your question about extending MFT to other common loss functions in deep metric learning, we have added theoretical insights into applying MFT to losses with anchors in Section A.4 in the appendix. \n\nUnlike the proxy-based method, which replaces positive and negative samples with proxies, the mean field approach results in two distinct terms. In one term, positive and negative samples are replaced by mean fields, similar to the proxy-based approach. However, in the other term, anchors are replaced by mean fields while the positive and negative samples remain. This term is analogous to the ProxyAnchor loss, which was introduced heuristically.\n\nWe hope this response adequately addresses your query and provides further clarity on our work. We are thankful for your engagement with our research and welcome any additional comments or questions you may have."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2363/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700152779528,
                "cdate": 1700152779528,
                "tmdate": 1700152779528,
                "mdate": 1700152779528,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "PkwcXt2Kxs",
                "forum": "ZPdZLlNXSm",
                "replyto": "kd6hgASvkz",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2363/Reviewer_q1qw"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2363/Reviewer_q1qw"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for the respense"
                    },
                    "comment": {
                        "value": "I lean more towards the comments of Reviewer zMTp and Reviewer aqYA. Currently, I will maintain my rating."
                    }
                },
                "number": 16,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2363/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700632776789,
                "cdate": 1700632776789,
                "tmdate": 1700632776789,
                "mdate": 1700632776789,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "z4OVPAtfIT",
            "forum": "ZPdZLlNXSm",
            "replyto": "ZPdZLlNXSm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2363/Reviewer_pubU"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2363/Reviewer_pubU"
            ],
            "content": {
                "summary": {
                    "value": "The paper considers mean field theory approximations of pair-based loss functions for metric learning. Inspired by techniques in statistical physics, pairwise calculations are approximated by comparing to a mean approximation, ie, turning a summation over $i$ and $j$ to only that of $i$. Using such an approach, two different mean field contrastive loss functions are proposed. Empirically, the proposed loss functions are evaluated and are shown to even out perform their non-mean field counterparts."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The approximation technique for reducing pairwise summations to their mean field approximation intuitively makes sense. This concept is particularly well illustrated in Figure 1.\n- The proposed approach seems promising. Surprisingly, the mean field approximations perform better than their non-mean field counterparts in many circumstances."
                },
                "weaknesses": {
                    "value": "- Part of the motivation for the approximation (not including its statistical physics analogy) was the reduction in runtime complexity pair-based loss function in metric learning. However, there is no runtime values reported in the paper.\n- Some terms in the paper are not explained. (See questions below)."
                },
                "questions": {
                    "value": "- What are the runtimes of the mean field variants compared to their regular counterparts? Does the additional complexity of requiring optimization of mean fields $\\mathbf{M}_c$ outweigh the reduction of computational complexity via the mean field approximation?\n- One of the major interesting components of the paper is that mean field approximation performs better than its non-mean field counterparts. Is there a good hypothesis for why this may be the case? Have you found a good characterization of when one out performs the other? Additional insight here would be great since the paper claims in the empirical section that the mean field losses provide better \"training complexity but also results in better embeddings\".\n- I am unsure exactly what the authors mean be \"resummation\" and \"unstable terms\". Clarification here would be great.\n- Furthermore, what do that authors mean by \"... the above discussion implies the mean field theory is independent of the concept of anchors ...\"?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2363/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698725908333,
            "cdate": 1698725908333,
            "tmdate": 1699636168852,
            "mdate": 1699636168852,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "SuqeDYmPRx",
                "forum": "ZPdZLlNXSm",
                "replyto": "z4OVPAtfIT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2363/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2363/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review!"
                    },
                    "comment": {
                        "value": "Thank you for your detailed and constructive feedback on our manuscript. We appreciate the time you have taken to review our work and your insightful comments, which have helped us improve the quality and clarity of our research. Below, we address each of your questions:\n\n**Runtime comparison**\n\nThank you for highlighting the importance of runtime comparison. We have now included a new figure in our revised manuscript (Fig. 4 in Appendix), which compares the runtimes of the mean field losses against their regular counterparts. As indicated in the figure, the mean field variants not only offer a reduction in computational complexity due to the mean field approximation but also demonstrate faster runtime performance. This improvement aligns with our theoretical expectations and supports the practical applicability of our approach.\n\n**Performance of mean field approximations**\n\nWe appreciate your interest in the superior performance of mean field approximations. We hypothesize that the mean field approximation reduces the noise introduced in pairwise comparisons by considering the class means, thus simplifying the optimization process. This could lead to more robust and efficient training, as reflected in our empirical results. As you suggested, we added a sentence\u00a0\n`This is perhaps because the mean field losses can reduce the noise introduced in pairwise comparisons by the mean fields.` in Sec. 4.3.\n\n**Clarification on 'resummation' and 'unstable terms'**\n\nWe realize that these terms were not sufficiently clarified in our original manuscript. Resummation refers to reconverting an infinite series back into a functional form using Taylor expansion inversely. Regarding unstable terms, these mean potential components that could violate the positivity of a loss function. We have addressed this by adding mean field constraints that effectively eliminate such terms. For clarity, additional explanations have been added to the manuscript (footnote 2 on page 5).\u00a0\n\n**Mean field theory and concept of anchors**\n\nYour question brings to light a crucial aspect of our work. Our approach differs fundamentally from proxy-based methods, which assume a loss depends on the triad of anchor, positive, and negative elements. In contrast, our mean field theory (MFT) approach can be applied to any loss function through Taylor expansion and does not assume a special form of the loss function. This distinct methodology allows for broader applicability and simplicity.\nTo clarify this point, we modified the sentence as follows:\n`Furthermore, in contrast to the proxy-based method, which can be applied only to a pair-based loss with an anchor, the mean field theory is applicable to wider types of pair-based loss functions as it is based on the Taylor expansions.`\n\n\nIn summary, we hope that our revisions and additional explanations adequately address your questions and enhance the clarity and impact of our work. We are grateful for the opportunity to improve our manuscript based on your feedback and look forward to any further suggestions you may have."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2363/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700151666308,
                "cdate": 1700151666308,
                "tmdate": 1700151666308,
                "mdate": 1700151666308,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "LnU4N3SY9m",
                "forum": "ZPdZLlNXSm",
                "replyto": "SuqeDYmPRx",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2363/Reviewer_pubU"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2363/Reviewer_pubU"
                ],
                "content": {
                    "comment": {
                        "value": "Thank you for your response. The clarification regarding runtime, resummation, and unstable terms was helpful.\n\nRegarding the addition explanation for the performance, it seems that an increase robustness is plausible. However, I am hoping that the authors might provide an extended discussion beyond a sentence. I think that even with this sentence, it is hard to justify the MFT approximation except for the computation / runtime improvements, similar to Reviewer zMTp's follow up response."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2363/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700536348849,
                "cdate": 1700536348849,
                "tmdate": 1700536348849,
                "mdate": 1700536348849,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "fnYKuslAGb",
            "forum": "ZPdZLlNXSm",
            "replyto": "ZPdZLlNXSm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2363/Reviewer_uk7M"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2363/Reviewer_uk7M"
            ],
            "content": {
                "summary": {
                    "value": "The manuscript proposes two new metric learning algorithms inspired by mean-field analysis from physics. In pair-based algorithms, the loss function pushes pairs of the same class to be closer and pairs of different classes to be apart, and need to be calculated over many training pairs; in contrast, for the proposed mean-field approach, the loss pushes each sample to be close to the class mean and away from other classes' mean, and further pushing class means away from each other. The underlying technical derivation is quite general, through taking a derivating of an energy function. Thus, the same method can be applied to other cases, such as proposing a loss function using class means in a minibatch setting."
                },
                "soundness": {
                    "value": "4 excellent"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "* Regardless of any inspiration taken from physics, replacing pair-based methods with mean-based methods seems a scalable approach, well grounded in statistics.\n * The proposed class of methods is possibly prudent in the sense that they can be used to derive loss functions, taking into account mean-class information for other problems.\n * The simulations performed seem great, and the authors explain the optimisation carried out on both their method and other methods used for comparison. The results suggest the proposed class of methods achieve very competitive results. \n * The writing is clear, almost tutorial-like, and easy to follow."
                },
                "weaknesses": {
                    "value": "* The authors hide the actual derivation in the appendix, so they do not detail enough their technical approach. On the face of it, the modified loss function could have been suggested just through statistical intuition, not derived from an energy approximation (Hubbard-Stratonovich, saddle-point approx, etc.), so it is a pity the authors don't sketch their methodology in the main text.\n * It is hinted that the method can be more efficient (e.g. due to the lack of pair sampling or anchor points choice), but it is not reported if the training time is superior to the other methods or if the optimisation over `M`-s causes a substantial overhead (which may be justifiable with the improved results)."
                },
                "questions": {
                    "value": "* Is it always favourable to optimize the means `M` and the parameters `theta` together? Maybe alternating between optimisation steps would be superior in numerical terms.\n * The method seems similar to classic soft-clustering approaches, which were optimised by alternating between two optimisation steps, such as the EM algorithm. Can you make the connection more explicit?\n * Are the new methods faster or slower than other methods?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2363/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698793053283,
            "cdate": 1698793053283,
            "tmdate": 1699636168780,
            "mdate": 1699636168780,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "pY7xR7Jvez",
                "forum": "ZPdZLlNXSm",
                "replyto": "fnYKuslAGb",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2363/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2363/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review!"
                    },
                    "comment": {
                        "value": "Thank you for your insightful feedback on our manuscript. We appreciate the detailed review and constructive comments. Here are our responses to your points:\n\n**Technical derivation in the main text**\n\nWe understand your concern regarding the detailed technical derivation. However, due to page limitations, it is challenging to include the full derivation in the main text without sacrificing other critical content. We have, therefore, made an effort to enhance the clarity and accessibility of the appendix, ensuring that readers can easily follow the derivation process.\u00a0\n\n**Optimization strategy**\n\nWe agree that the optimization of M and $\\theta$ is a crucial aspect of our approach.\nFor T=0, simultaneous optimization is feasible and effective since the standard stochastic gradient descent is applicable.\nHowever, for T>0, the optimization becomes more complex, and an EM-like algorithm becomes necessary.\nIn Section A.5, we have sketched the relationship between EM (Expectation-Maximization) and MFT (Mean-Field Theory) at T>0.\nbased on a similarity between the partition function in our method and the log-likelihood in the EM algorithm.\n\n\n**Efficiency of the method**\n\nWe are glad you brought up the issue of computational efficiency. The equation (Eq. 3) related to optimal M does not require explicit consideration during training at T=0, and thus, our losses have no potential overhead you concerned. To clarify this point, in our revised manuscript, we have presented a pseudo code and a learning curve in runtime in the SOP dataset in Figure 4 in Appendix. This demonstrates that our methods not only converge faster in terms of epochs but also show superior performance in runtime efficiency compared to existing methods.\n\nWe hope these responses and revisions adequately address your concerns and enhance the paper's value. We look forward to any further feedback you may have."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2363/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700150364507,
                "cdate": 1700150364507,
                "tmdate": 1700150364507,
                "mdate": 1700150364507,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "MsNzAFn1B9",
                "forum": "ZPdZLlNXSm",
                "replyto": "pY7xR7Jvez",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2363/Reviewer_uk7M"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2363/Reviewer_uk7M"
                ],
                "content": {
                    "title": {
                        "value": "Response to authors"
                    },
                    "comment": {
                        "value": "I'd like to thank the authors for their many clarifications, to me and to other reviewers.\n\nI am satisfied with the response and especially with the additional demonstration of run-time efficiency, a concern raised by reviewers @pubU, @aqYA, zMTp and myself. \n\nI am quite confident the mean-field theory is applicable to other loss functions in metric learning, and thus judge the work more positively than @q1qw, who raised a concern over that."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2363/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700550656922,
                "cdate": 1700550656922,
                "tmdate": 1700550656922,
                "mdate": 1700550656922,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "dFCz5yFHUd",
            "forum": "ZPdZLlNXSm",
            "replyto": "ZPdZLlNXSm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2363/Reviewer_aqYA"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2363/Reviewer_aqYA"
            ],
            "content": {
                "summary": {
                    "value": "This paper explores the mean field theory into metric learning  by designing two loss functions to train deep neural networks. The model's performance is evaluated on various benchmarks, including CUB, Cars, and SOP. While the paper is generally easy to follow, it lacks a sufficient level of novelty and performance improvement."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper explores the mean field theory into metric learning  by designing two loss functions to train deep neural networks. The model's performance is evaluated on various benchmarks, including CUB, Cars, and SOP, by comparing several other methods. The paper is generally easy to follow."
                },
                "weaknesses": {
                    "value": "The major concern is that the paper lacks a sufficient level of novelty and performance improvement.\n\nFirst, they mainly explore mean filed theory into metric learning. Such metric is close to central loss [R1]. \n[R1]. Wen, Yandong, et al. \"A discriminative feature learning approach for deep face recognition.\" Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11\u201314, 2016, Proceedings, Part VII 14. Springer International Publishing, 2016.\n\nSecond, the performance is not significant. In table I, compared with ArcFace and ProxyAnch, it is hard to justify the significant improvement. It is essential to do t-test."
                },
                "questions": {
                    "value": "The clarification of model novelty.\nThe performance improvement."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2363/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698807235568,
            "cdate": 1698807235568,
            "tmdate": 1699636168674,
            "mdate": 1699636168674,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CrUxQA8akB",
                "forum": "ZPdZLlNXSm",
                "replyto": "dFCz5yFHUd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2363/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2363/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review!"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful and constructive feedback on our manuscript. We appreciate the opportunity to clarify and expand upon the points you raised.\n\n**Comparison with central loss**\n\nWe acknowledge your observation regarding the similarity of our approach to central loss. However, there is a fundamental difference in methodology and computation. In central loss, there is a necessity to continually update class centers during training. In contrast, our approach rooted in mean field theory demonstrates that the optimal mean field equates to the class center. This is achieved without the extra computational cost associated with central loss. The theoretical underpinning of our method offers a more efficient alternative, emphasizing the novelty of our approach. This point is clarified in the revised manuscripts (see the last paragraph in Sec. 3.2).\n\n\n\n**Statistical significance through t-tests**\nUpon your suggestion, we conducted t-tests to validate the statistical significance of our results. The t-tests were performed comparing our methods (MFCont and MFCWMS) against ArcFace and ProxyAnchor, as shown in Tables 6 and 7 in the revised manuscript.\nThe tables highlight that our MFCWMS loss demonstrates statistically significant improvements over ArcFace in all situations except 512D in the Cars dataset and ProxyAnchor except 128D and 512D in the Cars dataset. Additionally, our simpler loss function, MFCont. loss, although slightly less competitive than MFCWMS loss, still holds up well against these benchmarks in CUB and SOP.\n\nWe believe these additional analyses and clarifications address your concerns regarding both the novelty and the performance improvement of our proposed methods. Our approach not only stands out in terms of theoretical efficiency over central loss but also demonstrates practical efficacy through statistically significant performance improvements.\n\nWe hope that these clarifications and additional analyses sufficiently address your concerns, and we are open to any further suggestions or queries you might have."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2363/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700148247553,
                "cdate": 1700148247553,
                "tmdate": 1700148247553,
                "mdate": 1700148247553,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "k8lTqUxy3j",
                "forum": "ZPdZLlNXSm",
                "replyto": "dFCz5yFHUd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2363/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2363/Authors"
                ],
                "content": {
                    "title": {
                        "value": "On clarification of our previous response and updates in our revised manuscript"
                    },
                    "comment": {
                        "value": "Thank you for your valuable feedback on our manuscript. We here clarify our previous response and notify you of the updates in our manuscript.\n\n**Distinction from central loss**\n\nWe appreciate your insights regarding central loss. As detailed in our previous response, our Mean Field Theory (MFT) approach differs significantly from central loss in two key aspects:\n\nDifference in Computational Method: Central loss requires dynamic updating of class centers for regularization. In contrast, MFT achieves optimal mean fields that automatically align with class centers, eliminating the need for continual updates. This fundamental computational difference sets MFT apart from central loss.\n\nDifference in Purpose of Method: While central loss aims to enhance the performance of classification-based losses, MFT's focus is on deriving classification-based losses from pair-based ones. Our application of MFT has led to the discovery of more efficient loss functions like MFCWMS loss.\n\n**Novelty of our method**\n\nOur method's applicability to a wider range of pair-based loss functions, including those without anchors, is a substantial novelty. This flexibility allows for the discovery of more efficient loss functions.\n\nMoreover, in the revised manuscript, we have provided a detailed comparison between the proxy-based method and MFT. Theoretically, we confirmed that a loss derived from MFT more accurately approximates the original loss function, especially near the optimal solution (see Section A.4). Empirically, the MLRC benchmark results for MFNCA loss on the CUB dataset demonstrate consistent improvements in accuracy metrics (see Table 15 in Section B.7). These findings underscore the enhanced performance achieved through a more accurate approximation of the original loss functions.\n\nWe believe these revisions and additions effectively address the concerns regarding novelty and comparison with both central loss and traditional proxy-based methods.\n\nWe are grateful for your feedback, which has been instrumental in enhancing our manuscript. We remain open to any further suggestions or comments."
                    }
                },
                "number": 17,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2363/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700646962988,
                "cdate": 1700646962988,
                "tmdate": 1700648177469,
                "mdate": 1700648177469,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "TiLZ1cgHMH",
            "forum": "ZPdZLlNXSm",
            "replyto": "ZPdZLlNXSm",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission2363/Reviewer_zMTp"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission2363/Reviewer_zMTp"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes new DML losses that are inspired from the Mean-Field Theory (MFT), which is a concept from statistical physics. Specifically, the authors follow the constructive loss and the multi-class loss to implement two MFT losses. Their extensive experiments demonstrate the efficiency of the new losses on popular DML benchmarks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The idea of introducing unique losses based on the theory\u00a0of statistical physics looks\u00a0interesting and novel. No prior research has taken on this particular task.\n\nThe proposed method is evaluated on several popular DML benchmarks. The authors evaluate their method on advanced MLRC metrics, making their results convincible."
                },
                "weaknesses": {
                    "value": "My major concern is that the proposed theory does not seems to be solid when it is applied on DML task. There is not enough theoretical clue that the mean-field theory (MFT) would directly benefit the DML task compared with the proxy-based losses. The authors should provide more analysis to explain the intrinsic connection between the interaction between the magnetic spin and the similarity (distance) between the data points in DML task.\n\nThe relation and comparison between the proposed loss and proxy-based loss is still not clear. The intuition behind the MFT looks similar to the proxy-based loss, where they both compare a sample with an anchor instead of all class members. Thus, a systematic compare between it and other close related losses should be provided. \n\n\nIt seems the computation complexity to get the mean field in Eq.5 is higher than compared with the proxies. Thus, a comparison of performance and running time may be essential to be discussed. To further clarify its arithmetic progress and complexity, it is also suggested to list the pseudo-code of the loss.\n\n\nExperimental results show that the improvement in some datasets is not significant (as illustrated in table 2, figure 2). But this is not a big issue for me."
                },
                "questions": {
                    "value": "Is there any assumption or internal connection between the point distances and the spin configuration?\n\nPlease respond to the above weakness."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 5,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission2363/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699171185857,
            "cdate": 1699171185857,
            "tmdate": 1699636168583,
            "mdate": 1699636168583,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "7PI6g7FFmt",
                "forum": "ZPdZLlNXSm",
                "replyto": "TiLZ1cgHMH",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2363/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2363/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Thank you for your review!"
                    },
                    "comment": {
                        "value": "Thank you for your detailed and constructive feedback on our manuscript. We appreciate the time you have taken to review our work and your insightful comments, which have helped us improve the quality and clarity of our research. Below, we address each of your concerns:\n\n**Internal connection between the point distances and the spin configuration**\n\nMagnetic spin is a special case of DML, where embedding dimensions equal to 3, and $F_\\theta(x)$ is L2 normalized\nThis is because we introduced the magnetic spin as \"a vector living on a sphere\" in Sec. 3.1, and the distance is given by $d(F_\\theta(x_i),F_\\theta(x_j)) = 1 - F_\\theta(x_i)^T F_\\theta(x_i)$ when $F_\\theta(x)$ is L2 normalized.\u00a0\nSince DML loss is written as a function of cosine similarities in this situation, Contrastive loss reduces to the Hamiltonian of magnetic spins when $|\\mathcal{C}|=1$, $m_p<0$, embedding dimension is 3, and $F_\\theta(x)$ is L2 normalized, as mentioned in Sec. 3.2.\nWe think these points are enough to motivate us to apply the MFT to DML loss functions.\n\n**Comparison with proxy-based approach**\nTo clarify the relationship between our proposed MFT existing proxy-based approaches, we have applied MFT to a loss with an anchor in Section A.4 of the appendix. The resulting loss function has not only a term obtained in proxy-based approach (i.e., positive and negative samples are replaced by proxies) but also a term where anchors are replaced by mean fields. This term is analogous to the ProxyAnchor loss, which was previously introduced heuristically. We believe this comparison clarifies the relation and distinction between the two methodologies.\n\n**Computational complexity and pseudo-code**\n\nWe understand your concern regarding the computational complexity of calculating the mean field in Eq.5. To address this, we clarify in the revised manuscript that Eq.5 represents an optimal condition of the mean field, which is not computed during training. This means the computational complexity of our approach is on par with classification-based losses. To further clarify this aspect, we have added Pseudocode in Section A.5 of the appendix, which delineates the computational process of our method.\nAdditionally, in response to your suggestion, we have included a new figure (Fig. 4 in Appendix) comparing the runtimes of the mean field variants with their regular counterparts. This empirical evidence demonstrates that the mean field losses also exhibit faster runtime performance, reinforcing the practical applicability of our method.\n\n\nIn conclusion, we hope that our revisions and additional explanations adequately address your concerns and enhance the clarity and impact of our work. We are grateful for the opportunity to improve our manuscript based on your feedback and look forward to any further suggestions you may have."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2363/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700147450999,
                "cdate": 1700147450999,
                "tmdate": 1700147450999,
                "mdate": 1700147450999,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "q1Fo5rXoC6",
                "forum": "ZPdZLlNXSm",
                "replyto": "7PI6g7FFmt",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission2363/Reviewer_zMTp"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission2363/Reviewer_zMTp"
                ],
                "content": {
                    "title": {
                        "value": "Thanks for your response"
                    },
                    "comment": {
                        "value": "Dear authors,\n\nThanks for your response. The response resolved my concern about computational complexity. However, I was still not convinced by the first two concerns. Since this may be the first work to bring concepts and theory in statistical physics into machine learning or deep metric learning, it is crucial to construct a clear explanation of the internal connection between them. If you assume magnetic spin as a special case of DML where embedding dimensions are equal to 3, what are their physical meaning when calculating the dot production (cosine similarity) between two magnetic pins? Why can we bring the Hamiltonian into the DML loss? \n\nAlso, as proposed by reviewer aqYA, the comparison between MFT and proxy-based loss or central-based loss in the machine learning field is not clear. It is still not clear what is the benefit of MFT except the low computational cost. Why it improve the overall performance?\n\n\nBest,\nReviewer"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission2363/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700441384759,
                "cdate": 1700441384759,
                "tmdate": 1700441384759,
                "mdate": 1700441384759,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]