[
    {
        "title": "Do Large Language Models Know about Facts?"
    },
    {
        "review": {
            "id": "W7LbP6dh6D",
            "forum": "9OevMUdods",
            "replyto": "9OevMUdods",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7061/Reviewer_SmAT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7061/Reviewer_SmAT"
            ],
            "content": {
                "summary": {
                    "value": "This work presented a benchmark containing 20K diverse factual questions that span different sources, timelines, domains, regions, and languages. This work also thoroughly investigated different sizes and types of LLMs on this benchmark and reported their findings."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This work presented a comprehensive benchmark for testing LLMs' factual and commonsense knowledge.\n\nThis work annotated 20K factual questions from seven diverse tasks, which provides a good testbed for the community.\n\nThe analyses presented are solid and insightful."
                },
                "weaknesses": {
                    "value": "Actually, I am satisfied the most part of this paper and willing to raise my score if the author can cover the following points:\n\nSome points in the main results could be verified through better experiment settings, such as the point that \"LLMs without instruction tuning underperform those with instruction tuning by 16.0%\". It's better to conduct a peer-to-peer comparison since the instruction tuning is not the only factor of the two groups. For example, we can compare LLaMA with Alpaca and Vicuna, OPT vs. OPT-IML, T5 vs. Flan-T5, BLOOM vs. BLOOMz. I am not saying that we need to compare all of them, but the comparison could be made between the instruction tuning model and its backbone. \n\nFor the \"Using the CoT method, we observed a relative boost in performance in LLMs subjected to\ninstruction tuning and RLHF\", the tested models may not have strong CoT ability. It's good to see LLaMA-2 or GPT-4 and other recent models that claim to have strong reasoning abilities. Since CoT is not a generic ability that every LLM has. \n\nBesides, you can include an additional metric to report the cases that LLMs failed to generate meaningful answers. Since LLM may refuse to answer according to security or other customized policies, treating all unanswered questions as wrong predictions may be unfair. \n\nSec-4.2 could be improved, especially the discussion around temporal analysis, \" GPT-3.5-Turbo exhibits superior performance when dealing with outdated data as compared to updated data.\", I think the numbers are close, and it may not be sufficient to support the claim. \n\nThere are some missing references for testing LLM's factuality. \n\nSelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models\nFActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation\nFacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios"
                },
                "questions": {
                    "value": "See weaknesses"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7061/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698304459845,
            "cdate": 1698304459845,
            "tmdate": 1699636830947,
            "mdate": 1699636830947,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "BxK2V6ktOC",
                "forum": "9OevMUdods",
                "replyto": "W7LbP6dh6D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7061/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7061/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer SmAT (Part 1/3)"
                    },
                    "comment": {
                        "value": "We deeply appreciate the recognition given by the reviewers for the innovation and thoroughness of our work. We are also grateful for the constructive comments provided. We have responded to each suggestion regarding the weaknesses of the manuscript and will incorporate corrections in the subsequent version of the manuscript.\n\n> (1) Some points in the main results could be verified through better experiment settings, such as the point that \"LLMs without instruction tuning underperform those with instruction tuning by 16.0%\". It's better to conduct a peer-to-peer comparison since the instruction tuning is not the only factor of the two groups. For example, we can compare LLaMA with Alpaca and Vicuna, OPT vs. OPT-IML, T5 vs. Flan-T5, BLOOM vs. BLOOMz. I am not saying that we need to compare all of them, but the comparison could be made between the instruction tuning model and its backbone\n\n\nThank you for your constructive feedback. As you recommended, we have conducted additional analyses to compare models with their instruction-tuned counterparts in Appendix A.9.\n\nThe comparisons between LLaMA and its instruction-tuned versions, Alpaca and Vicuna, can be found in Table 2. Furthermore, we have conducted extra tests under the few-shots with CoT setting for T5-11B vs. Flan-T5-11B and BLOOM-6.7B vs. BLOOMz-6.7B. For T5, the accuracy was 18.6%, and the Macro F1 was 25.2%. In contrast, as shown in Table 2, Flan-T5 achieved an accuracy of 38.4% and a Macro F1 of 38.4%. Similarly, BLOOM\u2019s performance was at an accuracy of 6.6% and Macro F1 of 12.2%, whereas BLOOMz showed a marked improvement with an accuracy of 27.5% and a Macro F1 of 27.7%.\n|             | Multifaceted |      | Structural |      | Adversarial |      | Temporal |      | Real-World |      | Domain-Specific |      | Multi-lingual |      | Overall |      |\n| :---------: | ------------ | ---- | ---------- | ---- | ----------- | ---- | -------- | ---- | ---------- | ---- | --------------- | ---- | ------------- | ---- | ------- | ---- |\n|             | ACC.         | F1   | ACC.       | F1   | ACC.        | F1   | ACC.     | F1   | ACC.       | F1   | ACC.            | F1   | ACC.          | F1   | ACC.    | F1   |\n|  LLaMA-7B   | 38.3         | 33.9 | 44.1       | 32.1 | 43.2        | 46.1 | 41.6     | 30.0 | 26.4       | 26.3 | 23.6            | 25.0 | 27.8          | 27.7 | 35.3    | 31.4 |\n|  Alpaca-7B  | 38.6         | 28.8 | 48.0       | 23.6 | 46.4        | 35.1 | 49.6     | 26.1 | 24.5       | 19.9 | 42.9            | 26.8 | 24.2          | 17.7 | 39.4    | 26.2 |\n|  Vicuna-7B  | 44.2         | 36.0 | 49.7       | 36.3 | 59.0        | 59.2 | 50.1     | 37.6 | 49.0       | 41.8 | 44.3            | 38.6 | 46.7          | 43.1 | 48.5    | 40.6 |\n|   T5-11B    | 26.3         | 31.3 | 29.2       | 27.6 | 16.5        | 25.1 | 24.9     | 28.5 | 2.2        | 2.9  | 3.9             | 7.3  | 27.7          | 18.1 | 18.6    | 25.2 |\n| Flan-T5-11B | 49.2         | 49.4 | 43.5       | 33.7 | 54.7        | 56.6 | 31.6     | 30.6 | 31.1       | 29.4 | 35.6            | 34.6 | 23.3          | 14.4 | 38.4    | 38.4 |\n| BLOOM-6.7B  | 10.7         | 13.5 | 0.8        | 3.5  | 2.0         | 3.7  | 3.7      | 7.7  | 5.4        | 8.5  | 11.8            | 15.6 | 9.8           | 15.9 | 6.6     | 12.2 |\n| BLOOMz-6.7B | 34.2         | 27.3 | 32.6       | 27.4 | 38.2        | 42.4 | 27.2     | 21.4 | 17.8       | 24.0 | 24.8            | 24.0 | 17.4          | 21.6 | 27.5    | 27.7 |\n\nThese peer-to-peer comparisons consistently demonstrate that models undergoing instruction tuning generally outperform their non-tuned backbones. With few exceptions (e.g., LLaMA vs. Alpaca in terms of Macro F1), we observed an average performance improvement of 11.3% across all compared models. This indicates that instruction tuning has a significant positive impact on the performance of LLMs."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7061/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700371031381,
                "cdate": 1700371031381,
                "tmdate": 1700392950138,
                "mdate": 1700392950138,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "CgCgIerZwI",
                "forum": "9OevMUdods",
                "replyto": "W7LbP6dh6D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7061/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7061/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> (2) For the \"Using the CoT method, we observed a relative boost in performance in LLMs subjected to instruction tuning and RLHF\", the tested models may not have strong CoT ability. It's good to see LLaMA-2 or GPT-4 and other recent models that claim to have strong reasoning abilities. Since CoT is not a generic ability that every LLM has.\n\nThank you for your valuable comments. To address your query, we specifically tested the performance of GPT-4-0613 on the Pinocchio dataset under the \"Few shots with CoT\" setting. **In this context, GPT-4 achieved an accuracy of 49.9% and a Macro F1 score of 47.2%. These scores represent the best performance across all tested models, showing an average improvement of 2.1% over GPT-3.5-Turbo.**\n\n|                    |Multifaceted |         | Structural |          | Adversarial |          | Temporal |          | Real-World |          | Domain-Specific |          | Multi-lingual |          |\n| :--------------: | ----------- | -------- | ---------- | -------- | ----------- | -------- | -------- | -------- | ---------- | -------- | --------------- | -------- | ------------- | -------- |\n|                  | ACC.        | F1       | ACC.       | F1       | ACC.        | F1       | ACC.     | F1       | ACC.       | F1       | ACC.            | F1       | ACC.          | F1       |\n| Text-Davinci-002 | 47.7        | 47.7     | **50.8**   | 38.4     | 64.2        | 64.3     | 33.9     | 31.1     | **51.7**   | 41.4     | 36.4            | 36.1     | 43.1          | 39.5     |\n| Text-Davinci-003 | 51.1        | 47.8     | 44.3       | 33.7     | 64.1        | 63.7     | 41.4     | 35.1     | 48.0       | 42.8     | 40.4            | 41.4     | **43.7**      | **43.6** |\n| GPT-3.5-Turbo    | 53.6        | **53.1** | 44.8       | 37.8     | 67.4        | 67.4     | 37.4     | 33.9     | 50.4       | 43.1     | 38.7            | 40.3     | 41.3          | 41.1     |\n| GPT-4            | **56.4**    | 51.3     | 43.2       | **38.6** | **73.7**    | **73.3** | **45.2** | **39.6** | 49.9       | **45.3** | **50.4**        | **49.1** | 41.8          | 41.3     |"
                    },
                    "title": {
                        "value": "Author Response to Reviewer SmAT (Part 2/3)"
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7061/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700371086545,
                "cdate": 1700371086545,
                "tmdate": 1700392964148,
                "mdate": 1700392964148,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "eNgjwWwtoZ",
                "forum": "9OevMUdods",
                "replyto": "W7LbP6dh6D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7061/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7061/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer SmAT (Part 3/3)"
                    },
                    "comment": {
                        "value": ">(3) Besides, you can include an additional metric to report the cases that LLMs failed to generate meaningful answers. Since LLM may refuse to answer according to security or other customized policies, treating all unanswered questions as wrong predictions may be unfair. \n\nDuring our experiments, we observed that only a negligible fraction of questions were left unanswered by the LLM due to security or other customized policies. Specifically, when using GPT-3.5-turbo as an example, out of 20,713 queries, fewer than 20 (< 1\u2030) responses were cases of refusal to answer. The main reason is that we have already excluded claims that could cause safety issues during the annotation progress.\n\n>(4) Sec-4.2 could be improved, especially the discussion around temporal analysis, \" GPT-3.5-Turbo exhibits superior performance when dealing with outdated data as compared to updated data.\", I think the numbers are close, and it may not be sufficient to support the claim.\n\nWe have revised this section to reflect a more cautious and accurate statement that aligns better with the observed data. We appreciate your guidance in improving the clarity and precision of our discussion.\n\n>(5) There are some missing references for testing LLM's factuality.\n\nThank you for your constructive suggestion. We have included all these references in the related work section (Page 9)."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7061/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700392715121,
                "cdate": 1700392715121,
                "tmdate": 1700392974814,
                "mdate": 1700392974814,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "zVC7YqzaTc",
            "forum": "9OevMUdods",
            "replyto": "9OevMUdods",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7061/Reviewer_9WHz"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7061/Reviewer_9WHz"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a benchmark named Pinocchio to research if LLMs learn factual knowledge correctly from seven aspects. The authors gather the text containing rich factual knowledge from existing datasets. Questions and answers are annotated manually based on the gathered text. With this proposed benchmark, the authors evaluate current popular LLMs."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "Identifying the drawbacks of LLMs in terms of the learned factual knowledge is important. The proposed benchmark contains questions from different aspects, which could encourage researchers to find the specific issues of LLMs and explore how to address them."
                },
                "weaknesses": {
                    "value": "1. Data leaky may happen. Note that the questions and answers are annotated based on the text of previous public datasets. The pretraining data of evaluated LLMs is likely to contain the text. This may cause data leaky, and LLMs might have memorized/learned shortcuts to answer the question. It makes the evaluation results on the benchmark less convincing.\n\n2. Overclaimed contribution (as well as title) to the research question. I think the main contribution of this paper is the newly proposed benchmark. In the experiment, the authors research the extent and scope of factual knowledge within LLMs. However, the authors only test LLMs on the benchmark. There are no baselines (LLMs, datasets) as a comparison. Only showing accuracy numbers is not sufficient to give the answer to this research question."
                },
                "questions": {
                    "value": "1. May I ask how you design the seven aspects for evaluation? Any high-level intuition?\n\n2. Can 92.4% annotation accuracy (as well as 85.6% inter-annotator agreement rate) promise the quality of the dataset? Does that mean when the test accuracy of LLMs is larger than ~= 85%, it's meaningless to use this benchmark for evaluation?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7061/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698580039978,
            "cdate": 1698580039978,
            "tmdate": 1699636830757,
            "mdate": 1699636830757,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "yS4JlRHWdF",
                "forum": "9OevMUdods",
                "replyto": "zVC7YqzaTc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7061/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7061/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer 9WHz (Part 1/2)"
                    },
                    "comment": {
                        "value": "We deeply appreciate the recognition given by the reviewers for the innovation and thoroughness of our work. We are also grateful for the constructive comments provided. We have responded to each suggestion regarding the weaknesses of the manuscript and will incorporate corrections in the subsequent version of the manuscript.\n\n> (1) Data leaky may happen. Note that the questions and answers are annotated based on the text of previous public datasets. The pretraining data of evaluated LLMs is likely to contain the text. This may cause data leaky, and LLMs might have memorized/learned shortcuts to answer the question. It makes the evaluation results on the benchmark less convincing\n\nThank you for raising the critical issue of data leakage. We have conducted several measures to alleviate data leakage.\n- We transformed the original dataset's statements into a question-and-answer (QA) format to mitigate this concern. This adaptation reduced the likelihood of data leakage. \n- It is also worth noting that the labels from the original fact-checking dataset, typically categorized as \"Refutes\" or \"Supports,\" do not directly apply to our new dataset. In the original context, these labels indicate whether the provided evidence supports or contradicts a given claim. The model determines whether a factual question is \"Factual\" or \"Non-Factual\" based on its inherent knowledge.\n- Furthermore, we manually revised their labels for questions that underwent factual changes, and around 7% of the statements experienced such factual alterations.\n\nLastly, we conducted experiments to evaluate the data contamination. Following the methodology proposed in [1], we evaluated the claim in the original dataset and found a data leakage rate of 21.3%. After reannotating the original claims into question-answer pairs manually (by annotators), this rate decreased to 7.4%. We compare this rate with other datasets below. When compared with other benchmarks, our benchmark has a significantly lower contamination rate. \n\n| Dataset | Contamination Rate |\n| :-----| ----: |\n| MMLU | 29.8% |\n| HellaSwag | 25.6% |\n| ARC | 16.3% |\n| Ours | **7.4%** |\n\n*Reference:*\n\n[1] An Open Source Data Contamination Report for Llama Series Models. ArXiv 2023. \n\n> (2) Overclaimed contribution (as well as title) to the research question. I think the main contribution of this paper is the newly proposed benchmark. In the experiment, the authors research the extent and scope of factual knowledge within LLMs. However, the authors only test LLMs on the benchmark. There are no baselines (LLMs, datasets) as a comparison. Only showing accuracy numbers is not sufficient to give the answer to this research question\n\n\nTo address your concern,  we have now provided performance results for GPT-3.5-Turbo in the few shots with CoT settings on various datasets, as shown in the table below. \nDue to time and resource constraints, we will gradually include more LLMs for comparison in future revisions of the paper.\n\n| Datasets    | Accuracy | F1   |\n| ----------- | -------- | ---- |\n| FEVER       | 56.3     | 55.7 |\n| Feverous    | 36.4     | 31.0 |\n| FoolMeTwice | 60.2     | 61.1 |\n| VitaminC    | 42.8     | 42.7 |\n| Politifact  | 36.2     | 35.5 |\n| PubHealth   | 56.3     | 49.5 |\n| X-Fact      | 35.8     | 33.1 |\n\n> (3) May I ask how you design the seven aspects for evaluation?  Any high-level intuition?\n\nThank you for your question regarding the design of the seven evaluation aspects. **To develop these aspects, we first undertook a comprehensive survey of numerous existing fact-checking datasets, the summary of which is presented in Appendix A.6, Table 9. ** Our analysis focused on identifying the various challenges these datasets aim to address regarding factuality.\n\nWe found that the challenges tackled by these datasets generally align with the seven aspects we have listed in our dataset. These aspects appear individually or in combination across the surveyed datasets, indicating their relevance and importance in fact-checking. This realization led us to intentionally design our evaluation framework around these seven specific challenges, ensuring that our benchmark is not only comprehensive but also directly addresses the core difficulties encountered in current fact-checking tasks."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7061/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700370753261,
                "cdate": 1700370753261,
                "tmdate": 1700382441793,
                "mdate": 1700382441793,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "0CWsOgpraA",
                "forum": "9OevMUdods",
                "replyto": "zVC7YqzaTc",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7061/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7061/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "> (4) Can 92.4% annotation accuracy (as well as 85.6% inter-annotator agreement rate) promise the quality of the dataset? Does that mean when the test accuracy of LLMs is larger than ~= 85%, it's meaningless to use this benchmark for evaluation?\n\nThe 92.4% annotation accuracy and 85.6% inter-annotator agreement rate indeed signify a high-quality dataset. Notably, the primary source of disagreement among annotators was the \"Not Enough Information\" (NEI) label. According to research by Guo et al. [1], defining the NEI category is inherently challenging, which can account for some of the disagreements.\nMoreover, compared to commonly accepted benchmarks in the field, our dataset's quality is notably superior. For instance, the FEVER [2] dataset has an inter-annotator agreement rate of 0.68, LIAR [3] is at 0.82, and FEVEROUS [4] stands at 0.65. Our dataset surpasses these widely used benchmarks, underscoring its reliability.\n\n*References:* \n\n[1] A Survey on Automated Fact-Checking. TACL 2021\n\n[2] FEVER: a Large-scale Dataset for Fact Extraction and VERification. NAACL 2018\n\n[3] \u201cLiar, Liar Pants on Fire\u201d: A New Benchmark Dataset for Fake News Detection. ACL 2017\n\n[4] FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information. NeurIPS 2021"
                    },
                    "title": {
                        "value": "Author Response to Reviewer 9WHz (Part 2/2)"
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7061/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700370778528,
                "cdate": 1700370778528,
                "tmdate": 1700382451956,
                "mdate": 1700382451956,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "7l76qLi5Yh",
            "forum": "9OevMUdods",
            "replyto": "9OevMUdods",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7061/Reviewer_1LZe"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7061/Reviewer_1LZe"
            ],
            "content": {
                "summary": {
                    "value": "1. Lack of datasets to evaluate LLMs ability to hold and reason over factual knowledge is the primary motivation of the paper to come up with a new benchmark called Pinocchio benchmark. This benchmarks can evaluate LLMs in tasks that needs to know factual knowledge from diverse set of domains, multiple facts to reason, and the facts may be present in different information sources. The work also performs varied experiments and analysis on factuality or non factuality for multiple large language models. The experiments and conclusions are presented well. \n    2. Datasets: \n        1. Most datasets have evidence but Pinnachio only cares yes or no answers without any evidence, specifically multiple choice question answers. More explanation on how this is a good strategy to conclude if LLMs are able to memorize and use facts is unclear from the paper.\n        2. There is no clear motivation in the paper on why these datasets were chosen \u2014 are these the only specific 7 categories of questions? Or was it the distribution of the known datasets and the data points?\n        3. Evaluating answers with evidences has led to better understanding on the performance of large language models. It will be good to compare performances of the large language models on the datasets vs how they are transformed for this setting\n    3. Conclusions from this work:\n        1. The work also performs varied experiments and analysis on factuality or non factuality and comes up with the following conclusions:\n            1. Instruction tuning performs better than non-instruct tuning\n            2. Few shot with COT > Few shot > Zero-shot with COT > Zero Shot\n            3. The number of hops in multi-hop reasoning dictates the performance decrease of GPT\n            4. LLMs have limited ability to use structured data\n            5. Numerical reasoning is harder for LLMs\n            6. LLMs are unable to use the latest data (temporal) but are able to answer with older data (trained on) \n        2. Comments: \n            1. These conclusions hold for Large Language Models in a more generic sense rather than just factuality. It\u2019s important for the authors to clarify the important of new conclusions in comparison on what already exists. Explanations of how these are different when made on the same datasets before deriving them to a multiple-choice QA dataset."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "1. An annotated dataset of 20k that can evaluate factuality of large language models.\n2. Work is presented well with experiments and conclusions."
                },
                "weaknesses": {
                    "value": "1. Data is derived from existing datasets but the motivation for transformation is not clearly specified\n2. Most conclusions of the experiments are already made in isolation prior to this work. The novelty in experiments and conclusion should be well specified."
                },
                "questions": {
                    "value": "Already in the summary"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7061/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699110054872,
            "cdate": 1699110054872,
            "tmdate": 1699636830646,
            "mdate": 1699636830646,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "zVq7NyL7E0",
                "forum": "9OevMUdods",
                "replyto": "7l76qLi5Yh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7061/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7061/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer 1LZe (Part 1/3)"
                    },
                    "comment": {
                        "value": "We deeply appreciate the recognition given by the reviewers for the innovation and thoroughness of our work. We are also grateful for the constructive comments provided. We have responded to each suggestion regarding the weaknesses of the manuscript and will incorporate corrections in the subsequent version of the manuscript.\n\n>(1) Most datasets have evidence but Pinnachio only cares yes or no answers without any evidence, specifically multiple choice question answers. More explanation on how this is a good strategy to conclude if LLMs are able to memorize and use facts is unclear from the paper.\n\nThanks for the insightful question. \n\nWe wish to clarify that there is a distinct separation in motivations between the existing fact-checking datasets and our Pinocchio benchmark. **The primary objective of traditional fact-checking datasets is to support the development of automated fact-checking systems**. To verify a claim, journalists typically must search through numerous sources to uncover pertinent evidence, assess the credibility of those sources, and synthesize a verdict based on the gathered evidence. To mimic this complex process, such datasets are designed to function within a framework comprising three essential components: claim detection, evidence retrieval, and claim verification, with evidence being a pivotal factor in these systems.\n\n**In contrast, our Pinocchio benchmark is designed primarily to assess the breadth and depth of factual knowledge contained within LLMs.** Unlike the datasets mentioned above, Pinocchio is not intended to assist in creating automated fact-checking systems that emulate the procedures carried out by professional fact-checkers. Additionally, we focus on evaluating the factual knowledge 'inside' LLMs. We are less concerned with an LLM's ability to process and infer from 'external evidence' to formulate conclusions.\n\nPinocchio aligns more closely with initiatives that utilize language models as a knowledge base [3] by employing multiple-choice formats or that aim to refine the information stored within a language model by implementing boolean answers [4]. Previous research [5] indicated that LLMs demonstrate robust calibration when presented with multiple-choice questions, drawing out the model's inherent knowledge. Our experiment supports this finding; as demonstrated in Table 5, posing questions as inputs yields a superior extraction of factual knowledge compared to the declarative claims in the original datasets, resulting in a 2.3% increase in performance.\n\n*References:*\n\n[1] Automated Fact-Checking for Assisting Human Fact-Checkers. IJCAI 2021\n\n[2] A Survey on Automated Fact-Checking. TACL 2021\n\n[3] Language Models as Fact Checkers? ACL 2020\n\n[4] Editing Factual Knowledge in Language Models. EMNLP 2021\n\n[5] Language Models (Mostly) Know What They Know. Anthropic Technical Report 2022\n\n>(2) There is no clear motivation in the paper on why these datasets were chosen \u2014 are these the only specific 7 categories of questions? Or was it the distribution of the known datasets and the data points? \n\n**As detailed in Appendix A.6, Table 9, our selection was based on an extensive survey of existing fact-checking datasets.** This survey aimed to identify and summarize the different challenges these datasets address associated with factuality.\n\nWe discovered that these datasets commonly target the seven question categories outlined in our study. These categories, appearing individually or in various combinations in the datasets we reviewed, represent the most significant and prevalent challenges in current fact-checking tasks. Based on this comprehensive analysis, we deliberately designed our benchmark to encompass these seven specific challenges, ensuring that our evaluation framework is both relevant and comprehensive."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7061/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700370151904,
                "cdate": 1700370151904,
                "tmdate": 1700382399094,
                "mdate": 1700382399094,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "KYrA3WWXVO",
                "forum": "9OevMUdods",
                "replyto": "7l76qLi5Yh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7061/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7061/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">(3) Evaluating answers with evidences has led to better understanding on the performance of large language models. It will be good to compare performances of the large language models on the datasets vs how they are transformed for this setting.\n\nThank you for your suggestion regarding the evaluation of LLMs using evidence-based answers. In the few shots with the CoT setting, LLMs provide outputs that include their reasoning processes. We manually sampled and analyzed 100 instances of these reasoning processes, comparing them with the gold evidence.\n\nOf these 100 instances, 38 showed correct reasoning processes, leading to accurate conclusions by the LLMs. However, 51 instances had incorrect reasoning processes, resulting in erroneous conclusions. Interestingly, in 4 instances, the reasoning process was flawed, but the LLMs still arrived at the correct answer. Conversely, there were 6 instances where the reasoning was accurate, but the LLMs reached incorrect conclusions. Additionally, we encountered a unique case where the LLM's reasoning process was internally contradictory.\n\n**A more detailed case study of these observations is included in Appendix A.7 (pages 30-31).**\n\n>(4) It\u2019s important for the authors to clarify the important of new conclusions in comparison on what already exists.\n\nThank you for your thoughtful feedback. Please allow us to clarify and highlight the important findings we had based on the proposed benchmark:\n\n- **Divergent Responses to Prompting Techniques:** We've observed that the efficacy of prompting techniques varies significantly between complex reasoning and factual knowledge tasks. Advanced prompting strategies like increased shot counts, complex Chains of Thought (CoT), and self-consistency show promise in reasoning tasks including mathematics reasoning, logical reasoning, symbolic reasoning, etc. However, these methods fall short in factual queries (refer to Table 5). Our conjecture is that this divergence might stem from fundamental differences in how LLMs handle factual-based tasks versus those requiring complex reasoning. In factual-based tasks, the LLMs should first remember the facts correctly and then perform reasoning on the relevant facts. If the facts are wrong in the first place, simply enhancing the reasoning is not helpful. Prior prompting techniques for reasoning can not help LLMs to better elicit factual knowledge.\n\n- **LLMs' Overconfidence in Facts:** LLMs display a higher level of confidence in their answers to factual questions, which can be misleading. Despite multiple iterations of the same question (as self-consistency prompting suggests), LLMs often replicate their errors (Table 5). After human inspections, we found that LLMs tend to make the same factual mistakes no matter whether we ask them how many times or adopt a decoding strategy to increase the diversity of answers.\n\n- **Self-Reflection in Correcting Factual Errors:** Self-refinement prompts LLMs to reassess their answers, showing significant improvements for factual questions (Table 5). After manual inspection, we found that when LLMs re-evaluate their earlier responses with CoTs, they can sometimes detect and rectify errors within their own reasoning pathways. This finding is illustrates a potential method for tempering overconfidence and enhancing fact-based accuracy in LLM responses.\n\n- **The Interplay of Structured and Unstructured Knowledge:** Contrary to initial assumptions that mixing structured (tables) and unstructured (text) knowledge sources complicates answering factual questions, our study reveals that LLMs are adept at integrating the two. This blend can even facilitate the recall of structured knowledge (Figure 3(b)). This equivalency between structured and combined knowledge sources is significant because it suggests that LLMs may use textual contexts to reinforce and access tabular facts, a valuable consideration for knowledge representation and retrieval strategies.\n\n- **Challenges in Factual Precision:** LLMs exhibit proficiency in distinguishing between entities but struggle with precise numerical information (Figure 3(c)). Our manual analyses suggest that although LLMs effectively memorize and differentiate entity details, they have difficulties with exact figures like numbers and dates. This distinction is crucial for understanding LLM limitations in tasks requiring precision and lays the groundwork for focused improvements in numeral cognition for LLMs."
                    },
                    "title": {
                        "value": "Author Response to Reviewer 1LZe (Part 2/3)"
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7061/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700370273086,
                "cdate": 1700370273086,
                "tmdate": 1700709193852,
                "mdate": 1700709193852,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "tmtTEf1mBY",
                "forum": "9OevMUdods",
                "replyto": "7l76qLi5Yh",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7061/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7061/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">(5) Explanations of how these are different when made on the same datasets before deriving them to a multiple-choice QA dataset.\n\nThank you for the insightful comment. Please let us elaborate on the nuances that distinguish the conversion to a QA format:\n- **Dataset Update and Factuality Assessment:** Facts and data can evolve over time. Therefore, as part of our preparation, we reassess and update the original dataset to ensure the accuracy and timeliness of the information. During this step, approximately 7% of statements were found to have changed in their degree of truthfulness or accuracy. \n- **Addressing Safety and Bias Concerns:** (LLMs) are sometimes preprogrammed to avoid engaging with content that violates certain safety or ethical guidelines. To circumvent any potential issues related to these models rejecting content during the analysis, we thoroughly inspect all items in the dataset. This review process includes discarding any items (4%) that may provoke controversy or be deemed unsafe, thereby streamlining the dataset to focus on informative and neutral content that LLMs can handle without contravening their operating principles.\n- **Eliciting Factual Knowledge:** As reflected in prior studies, presenting questions in a multiple-choice format can effectively harness the knowledge calibration of LLMs [1]. We've observed, and Table 5 on page 8 highlights, that this approach enhances the LLMs' capability to draw upon its internal factual knowledge\u2014delivering a measurable improvement of 2.3% in accuracy. Moreover, we manually sampled and analyzed 100 instances of reasoning processes under the QA format, revealing that presenting information in a QA format, more aligned with human reasoning patterns, significantly enhances the inference capabilities of LLMs. A more detailed case study of these observations is included in Appendix A.7 (pages 30-31).\n\nReferences:\n\n[1] Language Models (Mostly) Know What They Know. Anthropic Technical Report  2022"
                    },
                    "title": {
                        "value": "Author Response to Reviewer 1LZe (Part 3/3)"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7061/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700370322190,
                "cdate": 1700370322190,
                "tmdate": 1700382421672,
                "mdate": 1700382421672,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "29FKDF5JWE",
            "forum": "9OevMUdods",
            "replyto": "9OevMUdods",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7061/Reviewer_XTL4"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7061/Reviewer_XTL4"
            ],
            "content": {
                "summary": {
                    "value": "This paper introduces a benchmark that specifically evaluates the parametric knowledge acquired by LLMs during pretraining. The authors collected and recast existing knowledge-intensive datasets such as fact checking (e.g., FEVER) across seven domains:\n- Multiple-facts (e.g., multiple supporting facts)\n- Structural (e.g., with tables)\n- Adversarial (e.g., unrelated facts)\n- Temporal (e.g., Wikipedia revisions) \n- Real-world (e.g., politifact)\n- Domain-specific (e.g., medical and science)\n- Multi-lingual (e.g., French, Chinese, and more)\nAll examples are formulated as multiple-choice problems with three classes. 10 human annotators rewrote the original claims into questions while preserving the original facts and their labels. \n\nThe experiment setup primarily focuses on LLMs with prompting (zero-shot, few-shot, w/CoT). The major pretrained LLMs are evaluated on this benchmark, including public LLMs such as OPT, BLOOM, and LLaMA and commercial LLMs such as GPT3 and ChatGPT. Overall, ChatGPT and GPT3 outperform other LLMs by large margin, regardless of the prompting methods used. The fine-grained results show that the commercial LLMs are strong on many domains but underperform other LLMs on the temporal domain. Additionally, the authors performed analysis on different aspects such as question types and performance."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- This paper investigates factual knowledge in LLMs, which is a well-motivated problem.\n- A comprehensive benchmark covering different aspects of factual knowledge is proposed, particularly suitable for prompting."
                },
                "weaknesses": {
                    "value": "Dataset Design: _\u201c...multi-choice questions are a simple but good proxy to evaluate the potential of advanced abilities of LLMs\u2026\u201d_: I think this claim should be supported by prior work. I tend to disagree with this statement as Chen and Durrett (2019) concludes that, for some datasets, multiple-choice questions could be easily gamed (i.e., spurious correlations between questions and labels).  Link: https://arxiv.org/pdf/1904.12106.pdf\n\nA significant number of related papers have not been cited. There are numerous strands of prior work that investigate the factual knowledge and reasoning abilities of large language models (LLMs), including multi-hop reasoning, temporal and geographical questions, new entities, and complex inference. (I\u2019m listing a few of them here)\n- Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies (https://arxiv.org/abs/2101.02235) \n- SituatedQA: Incorporating Extra-Linguistic Contexts into QA (https://arxiv.org/abs/2109.06157)\n- A Dataset for Answering Time-Sensitive Questions (https://arxiv.org/abs/2108.06314)\n- Entity Cloze By Date: What LMs Know About Unseen Entities (https://arxiv.org/abs/2205.02832)\n- RealTime QA: What's the Answer Right Now? (https://arxiv.org/abs/2207.13332)\n- StreamingQA: A Benchmark for Adaptation to New Knowledge over Time in Question Answering Models (https://arxiv.org/abs/2205.11388)\n- Can LMs Learn New Entities from Descriptions? Challenges in Propagating Injected Knowledge (https://arxiv.org/abs/2305.01651)\n- FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation (https://arxiv.org/abs/2310.03214)"
                },
                "questions": {
                    "value": "- _\u201c...but in Pinocchio, we only need to judge the factuality of the question.\u201d_ Does this mean all supporting facts are not included in the questions?\n\nMinor:\n- The terms \u201cMultifaceted\u201d and \u201cmultifacted\u201d are used interchangeably. Is this a typo?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "8: accept, good paper"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7061/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7061/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7061/Reviewer_XTL4"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7061/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699231245185,
            "cdate": 1699231245185,
            "tmdate": 1700760814009,
            "mdate": 1700760814009,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "CVKSAUwjGI",
                "forum": "9OevMUdods",
                "replyto": "29FKDF5JWE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7061/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7061/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author Response to Reviewer XTL4 (Part 1/2)"
                    },
                    "comment": {
                        "value": "Thanks for the insightful questions.\n\n>(1)The claim \"...multi-choice questions are a simple but good proxy to evaluate the potential of advanced abilities of LLMs\u2026\" should be supported by prior work.\n\nMultiple-choice questions offer a practical approach to assessing current LLMs' complex capabilities, of which GPT-4 is a prime example [1]. Key benchmarks such as the MMLU [2], HellaSwag [3], ARC [4], BIG-Bench Hard [5], and TruthfulQA [6], all of which utilize multi-choice formats, serve distinct purposes in evaluating various aspects of GPT -4's proficiency. Precisely, the MMLU gauges an LLM's academic and professional knowledge breadth and depth. HellaSwag is designed to test commonsense reasoning, and ARC focuses on one's ability to tackle challenging questions. BIG-Bench Hard presents complex reasoning tasks that challenge even the most advanced LLMs, while TruthfulQA measures how LLMs mimic human falsehoods.\nFurthermore, the evaluation of language generation brings its own challenges, as a universal metric for measurement is currently lacking [7], which multiple-choice questions help mitigate by offering straightforward classification accuracy for assessment [1]. Also, prior studies [8] underscore that LLMs demonstrate reliable calibration when faced with multiple-choice scenarios, suggesting that this format is adept at extracting a model's latent knowledge. \nIn response to your insight, we have included this discussion in our paper, and rewritten section 2.2 **ANNOTATION AND QUALITY CONTROL** in the rebuttal version.\n\n*References:*\n\n[1] GPT4 Research Report. https://openai.com/research/gpt-4\n\n[2] Measuring Massive Multitask Language Understanding. ICLR 2021\n\n[3] HellaSwag: Can a Machine Really Finish Your Sentence? ACL 2019\n\n[4] Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. ArXiv 2018\n\n[5] Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them. ACL 2023\n\n[6] TruthfulQA: Measuring How Models Mimic Human Falsehoods. ACL 2022\n\n[7] A Survey of Evaluation Metrics Used for NLG Systems. ACM CSUR 2022\n\n[8] Language Models (Mostly) Know What They Know. Anthropic Technical Report 2022\n\n>(2) According to Chen and Durrett (2019), multiple-choice questions could be easily gamed (i.e., spurious correlations between questions and labels).\n\nChen and Durrett (2019) focused on multi-hop question-answering datasets, where a model is required to reason over multiple supporting documents to answer the question. In order to verify that it is possible to pick the correct answer without consulting the related documents, they construct a \"no context\" baseline. This \"no context\" baseline achieves higher performance on the multiple-choice WikiHop dataset, comparable to some state-of-the-art QA systems. This result shows that WikiHop can solve reasonably well without using multiple documents, indicating that the dataset is potentially gamed.\n\nFollowing Chen and Durrett (2019), we developed the same \"no context\" baseline to investigate the spurious correlations between questions and labels in our dataset. The results are shown below:\n\n| Method | Performance |\n| :-----| ----: |\n| No Context | **28.3** |\n| LLaMA-7B | 31.6 |\n| Alpaca-7B | 37.8 |\n| Vicuna-13B | 45.2 |\n| GPT-3.5 | 47.0 |\n\nOur experimental findings show that our dataset does not exhibit the same level of vulnerability to the exploitation of question-label correlations as observed by Chen and Durrett (2019) in the WikiHop dataset. With performance improvements of 16.9 points by Vicuna-13B and 18.7 by GPT-3.5 over the \"no context\" baseline, our results offer compelling evidence that our dataset is more resilient to such biases, contrary to the reported susceptibilities within WikiHop.\n\nWe extended our analysis to include a direct comparison with several established multiple-choice question-answering benchmarks, such as the WikiHop mentioned above, as well as with other prevalent benchmarks like TruthfulQA and ARC utilized in evaluating LLMs. The performance of the \"no context\" baseline across these benchmarks is displayed in the table below:\n\n| Dataset | Performance |\n| :-----| ----: |\n| WikiHop | 59.7 |\n| TruthfulQA | 34.5 |\n| ARC | 33.2 |\n| Ours | **28.3** |\n\nEvidently, our proposed dataset presented the most challenge to the \"no context\" baseline, marking the lowest performance compared to other datasets. The notable performance on WikiHop, with a \"no context\" baseline score of 59.7%, underscores the presence of spurious correlations that facilitate gaming that dataset. On the contrary, the lower baseline performances on TruthfulQA and ARC suggest that such issues are less prevalent. Our dataset, therefore, not only stands out as the least prone \"to be gamed\" but also underscores its robustness and the high level of rigor needed to tackle it effectively."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7061/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700369735172,
                "cdate": 1700369735172,
                "tmdate": 1700382320024,
                "mdate": 1700382320024,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "Cfy6X6esCe",
                "forum": "9OevMUdods",
                "replyto": "29FKDF5JWE",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7061/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7061/Authors"
                ],
                "content": {
                    "comment": {
                        "value": ">(3) A significant number of related papers have not been cited.\n\nThank you for providing the list of the related work missing in the paper. We have included the paper you listed in the rebuttal version. \n\nFurthermore, we extensively survey other related efforts in these topics. **As shown in Table 13 in the Appendix (page 36), we comprehensively compared 37 research efforts. We also included a new section called RELATED WORK: QUESTION ANSWERING DATASETS in Appendix A.10 (pages 32-35), where we discuss a significant number of related work in detail (more than 50 references are added)**.\n\n>(4) \u201c...but in Pinocchio, we only need to judge the factuality of the question.\u201d Does this mean all supporting facts are not included in the questions?\n\nApologies for any confusion earlier. Please allow us to clarify: in our proposed dataset, the focus is exclusively on assessing the factual accuracy of the questions themselves, without the inclusion of external supporting facts.\nIn typical fact-checking datasets, labels like \"Supports\" or \"Refutes\" are used to signal whether external evidence confirms or denies a particular claim. These datasets are aimed at fostering the development of systems able to emulate the comprehensive process journalists go through to fact-check a claim, which involves sifting through various sources for relevant evidence and consequently reaching a conclusion regarding the claim's accuracy. This process relies heavily on evidence gathering, a critical aspect of traditional fact-checking operations.\n\nHowever, our Pinocchio benchmark is designed with the objective of gauging the factual knowledge stored within LLMs. The benchmark evaluates LLMs' capacity to reason using the factual knowledge they have previously assimilated, thus, intentionally excluding the provision of external supporting facts. In simple terms, we are testing the LLMs' ability to verify facts based solely on the information they have internalized without referencing additional evidence. We are less concerned with an LLM's ability to process and infer from 'external evidence' to formulate conclusions.\nTo create this specific dataset, we have re-evaluated each statement from the original fact-checking datasets by employing annotators to research various sources and verify facts, which led us to re-annotate the claims. This method ensures that our dataset appropriately tests the LLMs' factuality without relying on the inclusion of supporting facts within the questions.\n\n> (5) Typo of \u201cMultifaceted\u201d.\n\nThank you very much for pointing out this inconsistency. Indeed, \"Multifacted\" was a typographical error in our manuscript. We have revised the document and replaced all instances of \"multifacted\" with \"multifaceted\" to ensure consistency and accuracy in our terminology."
                    },
                    "title": {
                        "value": "Author Response to Reviewer XTL4 (Part 2/2)"
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7061/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700369892623,
                "cdate": 1700369892623,
                "tmdate": 1700382356842,
                "mdate": 1700382356842,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "3bAvND7wxN",
                "forum": "9OevMUdods",
                "replyto": "Cfy6X6esCe",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7061/Reviewer_XTL4"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7061/Reviewer_XTL4"
                ],
                "content": {
                    "comment": {
                        "value": "I appreciate your clarification. While the authors have addressed several of my concerns, the paper's narrative remains somewhat misleading, as Reviewer 9WHz notes regarding overclaiming. This benchmark tests LLMs only on facts covered by existing datasets, which may include outdated information. I will adjust my score accordingly."
                    }
                },
                "number": 12,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7061/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700645128248,
                "cdate": 1700645128248,
                "tmdate": 1700645128248,
                "mdate": 1700645128248,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]