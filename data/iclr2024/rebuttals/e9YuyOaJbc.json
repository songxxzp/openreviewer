[
    {
        "title": "Optimal and Generalizable Multimodal Representation Learning Framework through Adaptive Graph Construction"
    },
    {
        "review": {
            "id": "bH2NlOKc5g",
            "forum": "e9YuyOaJbc",
            "replyto": "e9YuyOaJbc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission996/Reviewer_ZYST"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission996/Reviewer_ZYST"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces AutoBIND, a contrastive learning framework that can learn representations from an arbitrary number of modalities. \nThis approach leverages data from heterogeneous sources such as images and text and is able to deal with missing modalities as it can dynamically update the graph structure during training. \nAutoBIND uses a graph-based approach to automatically select the most correlated modalities from fully connected graph/MST and a contrastive loss to learn the representations. \nThe method is evaluated on Alzheimer's disease detection and house price prediction with 3D images, 2D images, and table modalities and outperforms existing baselines."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "The paper motivates well by tackling one of the most important problems in multimodality learning, which is dealing with a numerous number of modalities and missing modalities.\nThe author proposes a simple yet effective method for extending the ImageBind pairwise contrastive learning with graph learning to learn the relation between the modalities. Results demonstrate decent results in the two downstream datasets, showing the effectiveness and interpretability of the method."
                },
                "weaknesses": {
                    "value": "(1) Although the paper claims to bind multimodalities, in practice, it deals with only three modalities (image, text, tabular) and their corresponding features (various columns in the data). Such a number of modalities is relatively common in multimodal learning, unlike current works that learn from up to five modalities that contain image, video, text, audio, and IMU.\n(2) The modalities claimed in the current setting are more like different features in the same modalities (bedroom, bathroom, etc ...). Hence, the missing modality setting is more like missing features, which is a relatively explored problem. \n(3) It will be better to include the \nImageBIND method as the baseline in Table 1, to confirm if it is the proposed method or the use of MM features that achieve performance improvement.\n(4) Due to the loss design including all pairs of combination (or MST edges), it will be better to include a more rigorous run-time analysis (how the method scales in adding more modalities), computation resource used, and parameter used. It is very likely that the performance gain was from more parameters in the encoder.\n\nSome typos\nAlzhiemer's should be Alzheimer's in the title  \n4.2.1"
                },
                "questions": {
                    "value": "It is unclear to me what the message the author tries to convey of the epoch 123 vs. 62,63,64 in Figures 3 and 4.\nFor me, all of the graphs make sense in their own way, which is based on different interpretability of such a task.\nAlso, the change between 62,63,64 is also not clear in Figure 4. In 62 and 64, the label is dependent on various features. However, it is only correlated to two of them in 63. It will be better for the author to elaborate more on these findings."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission996/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission996/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission996/Reviewer_ZYST"
                    ]
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission996/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698716816545,
            "cdate": 1698716816545,
            "tmdate": 1699636025876,
            "mdate": 1699636025876,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "ztNIqSCUEr",
                "forum": "e9YuyOaJbc",
                "replyto": "bH2NlOKc5g",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission996/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission996/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer ZYST"
                    },
                    "comment": {
                        "value": "### In relation to the weaknesses mentioned:\n\nWhile the paper only deals with image, text and tabular data, the tabular data is categorised into different independent values and these are encoded independently, which leads to many different sources of data. \n\nCurrent works that use image, video, text, audio, and IMU data learns from these modalities separately (e.g. {image, video}, {image, text}, {text, audio}), whereas our approach considers the scenario where lots of modalities are available in a single dataset.\n\n### To answer the question:\n\nWe include the graphs for epoch 1, 2, 3 vs 62, 63, 64 to show the convergence of the MST graph. As shown in Figure 3: Graph Visualization of AutoBIND w/ 2D MRI, the (Image) - (Label) - (Biomarker) component of the graph is consistent for all 3 of the final epochs. We note that some other modality connections may fluctuate (e.g. demographics) since they are not correlated with the presence of AD."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission996/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700415318542,
                "cdate": 1700415318542,
                "tmdate": 1700415318542,
                "mdate": 1700415318542,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Sp0A21ORjd",
            "forum": "e9YuyOaJbc",
            "replyto": "e9YuyOaJbc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission996/Reviewer_pwEx"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission996/Reviewer_pwEx"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method name AutoBIND, which construct a graph on different modalities and dynamically update the graphs using a minimum spanning tree algorithm. This paper evaluates AutoBIND on a wide variety of datasets and show that AutoBIND outperforms previous methods."
                },
                "soundness": {
                    "value": "1 poor"
                },
                "presentation": {
                    "value": "1 poor"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. This paper Introduces AutoBIND, a novel framework that efficiently handles multiple modalities and is robust to missing modalities through a dynamic graph-based approach. Utilizing graphs to learn multimodal representations is reasonable, as this structured data can effectively model the relationships between different modalities.\n2. Extensive Experiments across various datasets demonstrate AutoBIND's effectiveness in multimodal contrastive learning tasks."
                },
                "weaknesses": {
                    "value": "1. In section 2.1, the definition of \\(d_{ij}\\) appears problematic. How is the value determined when the cosine similarity \\(Sim(Z_i, Z_j)\\) is zero? Moreover, when \\(Sim(Z_i, Z_j)\\) approaches 0, \\(d_{ij}\\) can become extremely large (if positive) or extremely small (if negative). This definition seems to be flawed at a fundamental level.\n2. What is the specific definitions of correlated and uncorrelated modalities?\n3. The symbol system is a mess. i,j are modality indices in equation 1 but are sets in equation 3, making this paper is hard to understand. \n4. Overall, the paper lacks a clear structure and appears to be written informally."
                },
                "questions": {
                    "value": "NA"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission996/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission996/Reviewer_pwEx",
                        "ICLR.cc/2024/Conference/Submission996/Senior_Area_Chairs"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission996/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698775629914,
            "cdate": 1698775629914,
            "tmdate": 1699855415958,
            "mdate": 1699855415958,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "2thVDeATIQ",
                "forum": "e9YuyOaJbc",
                "replyto": "Sp0A21ORjd",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission996/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission996/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "# 1\n\nNote that d_ij in experimentation is not exactly 1/Sim(zi, zj). We introduce a large constant d_ij = 1/(Sim(zi, zj) + 10^9). Therefore, the similarity value is never negative.\n\n# 2\n\nWe define correlated modalities as modalities that are capable of producing similar embeddings due to similarities in their data.\n\nWe define uncorrelated modalities as modalities that are not capable of this.\n\nE.g. patient demographics is uncorrelated with the presence of Alzheimer's disease, while the MRI is highly correlated."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission996/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700415238675,
                "cdate": 1700415238675,
                "tmdate": 1700415238675,
                "mdate": 1700415238675,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "gdH39SVKVS",
            "forum": "e9YuyOaJbc",
            "replyto": "e9YuyOaJbc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission996/Reviewer_Ehq7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission996/Reviewer_Ehq7"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method to learn multimodal representation using a graph representation. The target representation is per modality, i.e. each modality should yield a separate vector of a fixed size (=128). Each node in the graph represents a separate modality. A graph is constructed in an order to maximize the correlations between modalities iteratively. The method was evaluated on two multimodal datasets for prediction tasks."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "A graph construction method was used and the resulting graph may be used to understand the internal correlation of data.\nThe proposed method is very simple to implement."
                },
                "weaknesses": {
                    "value": "Technical descriptions are very unclear. Mathematical formalization is inconsistent and doesn't make sense. The algorithm description is incomplete. I think I can still guess what the paper tries to do, but not because the presentation is good, but because the method is extremely simple. \n\nAt high level, there is no proof or evaluation showing that this method is \"optimal\" or \"generalizable\". There is no proof that the proposed algorithm can optimize the global objective (eq 1). \n\nEq (1) is the global objective by choosing optimal {f_i}. z_i is instance Z_i should be the set. In the first equation in Sec 2.1 (doesn't have a numbering), the papers uses dot product/norms of set (Z). This must be z_i. But then w_ij should be defined in the whole set, so probably needs summation. \n\nThe next paragraph introduces d_ij, which is just 1/Sim(). There is no point of defining this term because this term never appears in the rest of the paper. \n\nEq (2) and (3) make no sense. First, i and j now represent sets, not instances. Eq (2) is only valid when the inputs are two different modalities. Eq (3) is passing same modality in each of the loss terms. \n\nAlgorithm 1 also uses in consistent and undefined terms like \"correlation factor\" \"get_embedding\". How do you train the contrastive model on modality pair? It's never explained, although I guess it's probably done using eq (2). \n\nThe evaluation is another (bigger) limitation of the current paper. This paper seeks to learn a \"unimodal\" representation that can be learned from multimodal sources. This representation takes one modality as an input and produces a representation of fixed size. This can be useful in many situations, like when there is only one modality present per instance at test time. This paper doesn't consider any situation like this in evaluation. Instead, it uses all the modalities and concatenates all the representations to get a *joint* representation. And of course, this leads to a better prediction performance than using a single modality, but this is pointless. Maybe Table 3 tried to evaluate the model when each modality is separately used. But Table 3 is never referred in the main body and there is not much explanation. More importantly, if Table 3 was indeed from the proposed method, this needs to be compared with some references (which learn multimodal representation like this paper). Currently this table delivers no information. \n\nThere is an arbitrary step of pruning nodes, but there's no ablation study showing its impact. So I assume that the authors needed to do this just to improve the performance. It was argued that the current method is robust against missing variables, but again there was no experiment designed to verify. \n\nOverall, I think the current paper needs a lot more improvement before it can be published."
                },
                "questions": {
                    "value": "Please see above and correct me if I was mistaken."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission996/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698780743040,
            "cdate": 1698780743040,
            "tmdate": 1699636025701,
            "mdate": 1699636025701,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "1fVh8uaPta",
                "forum": "e9YuyOaJbc",
                "replyto": "gdH39SVKVS",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission996/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission996/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### In relation to the optimality and generalisability:\n\nThe MST is optimal due to the following:\n\nGiven two correlated modalities A and B:\n\n- Consider that A and B are not directly connected due to particular biased batch or underfitting in the encoders.\n\n- If A and B are never connected, then the correspondance between A and B will never be found. We cannot determine that modalities A and B are similar without comparing them.\n\n- However, if A and B are indirectly connected, then their similarity will increase, as they are bound to one embedding space. Let A connect to C connect to B, where C is some component of the graph. In the embedding space of the vector of C, modalities A and B will be closer than if they were in disjoint graphs. Therefore, in the next iteration, the similarity between A and B increases.\n\n- Hence, all data modalities should be connected whilst similar modalities are being grouped together.\n\nNow further consider two uncorrelated modalities A and D:\n\n- A and D will always be connected. However, they will be distant in the embedding space, since they have little/no similarity\n- Hence, their edge will be pruned and they will be far apart on the graph\n- Since their modalities have low correlation AND they are far apart on the graph, there is very little interference to the overall contrastive training process\n\nThe MST maintains connectivity of all modalities, whilst increasing the correspondance between similar modalities. After multiple iterations, the graph converges and similar modalities are grouped together.\n\nAs shown in Figure 3: Graph Visualization of AutoBIND w/ 2D MRI, the (Image) - (Label) - (Biomarker) component of the graph is consistent for all 3 of the final epochs. \n\nThe converged MST highlights inherent similarities between two modalities across the entire dataset. \n\n### In relation to node pruning:\n\nAfter convergence, we can do node pruning. Node pruning eliminates the noise in the loss caused by the unrelated modalities. Furthermore, it reduces training time as less encoders require back-propagation. \n\n### In relation to eq (2)\n\nEq (2) defines the contrastive loss between two modalities i and j. To elaborate on the training process as a whole:\n\nOnce the MST is constructed, for each selected edge, we calculate the contrastive loss between the nodes it connects. Then, back-propagation is performed on the sum of the contrastive loss."
                    }
                },
                "number": 2,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission996/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700415170713,
                "cdate": 1700415170713,
                "tmdate": 1700415170713,
                "mdate": 1700415170713,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "QlPLt9jlBT",
            "forum": "e9YuyOaJbc",
            "replyto": "e9YuyOaJbc",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission996/Reviewer_5eim"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission996/Reviewer_5eim"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes AutoBIND, a new multimodal contrastive learning framework that can learn representations from any number of modalities without needing hand-crafted architectures. AutoBIND uses a graph-based approach to automatically select the most correlated modalities for contrastive learning. This allows it to be robust to missing modalities, as the graph structure is dynamically updated during training. Experiments across diverse datasets and modalities demonstrate AutoBIND's ability to generalize and superior performance compared with previous approaches."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "1. The paper proposes a reasonable solution to an important problem. \n\n2. The experimental results demonstrated satisfactory performance in a number of mixed-modality problem settings. \n\n3. Visualization and interpretation of the graph structure formed during the learning are interesting."
                },
                "weaknesses": {
                    "value": "1. The overall methodology is a simple application contrastive learning to multiple modalities. It is not clear why it outperforms previous approaches. Is it due to the additional modalities or due to one or several components in the proposed method? \n\n2. The MMST is constructed in every epoch (and also pruned)? If so, will this process converge to a stable tree and how does the final tree depend on the initial encoding quality (which determine the initial tree). \n\n3. The paper has the word \"optimal\" in the title, but there is no discussion or proof on the optimality. \n\n4. Eq (3) is not explained clearly. Is it a theorem?"
                },
                "questions": {
                    "value": "See weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission996/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission996/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission996/Reviewer_5eim"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission996/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699014088491,
            "cdate": 1699014088491,
            "tmdate": 1699636025616,
            "mdate": 1699636025616,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "kZUrBrqvT0",
                "forum": "e9YuyOaJbc",
                "replyto": "QlPLt9jlBT",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission996/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission996/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "### The overall methodology is a simple application contrastive learning to multiple modalities. It is not clear why it outperforms previous approaches. Is it due to the additional modalities or due to one or several components in the proposed method?\n\nThe benefit comes from the addition of relevant modalites, as they reduce the variance component of the prediction error. The relevance of these modalities is determined through the graph based methodology.\n\n### The paper has the word \"optimal\" in the title, but there is no discussion or proof on the optimality.\n\n\nThe fundamental idea of contrastive learning leverages the inherent conceptual commonalities between different correlated modalities of data. These commonalities are mirrored by learnable correlations in the data itself. Therefore, by the transitive property, modalities with low contrastive loss (or high similarity) can be mapped to each other, and these mappings can be propagated only if the modalities are connected. This motivates the use of MST.\n\nThe MST is optimal due to the following:\n\nGiven two correlated modalities A and B:\n\n- Consider that A and B are not directly connected due to particular biased batch or underfitting in the encoders.\n\n- If A and B are never connected, then the correspondance between A and B will never be found. We cannot determine that modalities A and B are similar without comparing them.\n\n- However, if A and B are indirectly connected, then their similarity will increase, as they are bound to one embedding space. Let A connect to C connect to B, where C is some component of the graph. In the embedding space of the vector of C, modalities A and B will be closer than if they were in disjoint graphs. Therefore, in the next iteration, the similarity between A and B increases.\n\n- Hence, all data modalities should be connected whilst similar modalities are being grouped together.\n\nNow further consider two uncorrelated modalities A and D:\n\n- A and D will always be connected. However, they will be distant in the embedding space, since they have little/no similarity\n- Hence, their edge will be pruned and they will be far apart on the graph\n- Since their modalities have low correlation AND they are far apart on the graph, there is very little interference to the overall contrastive training process\n\nThe MST maintains connectivity of all modalities, whilst increasing the correspondance between similar modalities. After multiple iterations, the graph converges and similar modalities are grouped together.\n\nAs shown in Figure 3: Graph Visualization of AutoBIND w/ 2D MRI, the (Image) - (Label) - (Biomarker) component of the graph is consistent for all 3 of the final epochs. \n\nThe converged MST highlights inherent similarities between two modalities across the entire dataset. \n\nAfter convergence, we can do node pruning. Node pruning eliminates the noise in the loss caused by the unrelated modalities. Furthermore, it reduces training time as less encoders require back-propagation. \n\n### Eq (3) is not explained clearly. Is it a theorem?\n\nThis is a claim that is part of the hypothesis. We say that the total loss, if we contrast uncorrelated modalities (Zn and Z'n) and correlated modalities (Zm, Z'm) separately (i.e. bring similar representations closer and move dissimilar representations apart), is less than if we contrast all modalities (Zp, Z'p) together, regardless of similarity."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission996/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700415086062,
                "cdate": 1700415086062,
                "tmdate": 1700415086062,
                "mdate": 1700415086062,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]