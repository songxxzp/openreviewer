[
    {
        "title": "InfoGround: Ground Manipulation Concepts with Maximal Information Boost"
    },
    {
        "review": {
            "id": "xpMA1rvUV8",
            "forum": "hCaoJh01io",
            "replyto": "hCaoJh01io",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3101/Reviewer_dSMV"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3101/Reviewer_dSMV"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes Mutual Information Boost (MIB) to generate key states for a task specified by natural language. Based on the observation that the uncertainty of a state reduces rapidly when approaching a key state, the proposed method identifies key states by maximizing the rate of mutual information changes between the key state and its preceding states. The paper showed that the selected subgoals can guide the policy to reach a higher success rate in manipulation tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "- The paper identifies the ambiguity issue of using VLMs to ground key states and proposes a method that uses changes in mutual information to improve it.\n- The information boost can apply to any key states that are defined by physical constraints and is a more principled way to identify key states.\n- The paper presents the method and idea clearly, and it is easy to follow."
                },
                "weaknesses": {
                    "value": "- The key states for some tasks are hard to describe using sentences, for example, in the task \u201chold the tube and align it with the table\u201d, it is hard to describe the key state for \u201calign\u201d if not know the dimension, size, or relative orientation/positions for the tube and table. The proposed method would be more practical if it could identify key states without the state-by-state instruction generated by LLMs.\n- Most tasks in the experiment do not really need to understand key states. For example, tasks such as pick and place cubes can be learned with pure behavior cloning (it is a bit surprising the performance reported in this paper was not that good), but it is harder to see the effect of key states grounding with short-horizon pick-and-place experiments.\n- There is no qualitative example of the identified key states. It is hard to understand if the selected key states are good and why predefined rules are worse than MIB."
                },
                "questions": {
                    "value": "- If the key states have the maximum mutual information boost, why there is a need to set \u201cn\u201d (the margin) to ensure any two key states are not too close to each other? It is possible that the two key states are quite close, for example, for the task \u201cinserting the key to the keyhole, aligning the key with the keyhole, and moving the key into the keyhole are both key states and can be quite close to each other temporally.\n- How much does paraphrase instruction help? If the embedding doesn\u2019t have knowledge about the physically relevant description, changing the sentence wouldn\u2019t have much help."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3101/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698723356046,
            "cdate": 1698723356046,
            "tmdate": 1699636256448,
            "mdate": 1699636256448,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "qOIq7sXmbO",
                "forum": "hCaoJh01io",
                "replyto": "xpMA1rvUV8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3101/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3101/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer dSMV,\n\nThanks for the constructive comments and suggestions. We also appreciate your acknowledgment of the principledness of our proposed metric, as well as the quality of the presentation. \n\nWe also thank you for many insightful questions that promote discussions to further reveal the scientific merits of the proposed metric. Please find our answers and clarifications below in detail.\n\nWe hope the answers can help finalize your assessment and the rating of our paper. Please also let us know if you have any further questions that we need to provide additional clarifications.\n***\n**W1: The key states for some tasks are hard to describe using sentences, for example, in the task \u201chold the tube and align it with the table\u201d, it is hard to describe the key state for \u201calign\u201d if not know the dimension, size, or relative orientation/positions for the tube and table. The proposed method would be more practical if it could identify key states without the state-by-state instruction generated by LLMs.**\n\n**A**: Thanks for the insightful comment. We agree that it would be nice if the agent could automatically discover key states for policy learning. And this is actually the direction we are heading to. In the sense that, manually defined key states (e.g., the one obtained by predefined rules in CoTPC) are too limited in terms of diversity. \n\nFor example, some key states are semantically meaningful but hard to be described by a set of predefined rules. We alleviate this constraint by employing LLMs to propose the key states (instead of constructing more rules). We admit that LLMs have their own limitations, which is out of the scope of our current paper. We focus on the grounding in this work but will study the limitation of LLMs in proposing key states in future research.\n\nHowever, we would also like to point out that there is a benefit of leveraging LLMs for proposing the states, as now the meaning of the key states is aligned with human understanding, which can help promote transparency of the grounded key states and the explainability of the trained policies.\n\n ***\n**W2: Most tasks in the experiment do not really need to understand key states. For example, tasks such as pick and place cubes can be learned with pure behavior cloning (it is a bit surprising the performance reported in this paper was not that good), but it is harder to see the effect of key states grounding with short-horizon pick-and-place experiments.**\n\n**A**: Thanks for the question. Even though Pick & Place Cube seems to be a short-horizon task, our experiments still show that key states help policy learning.\n\nThe comparison (on Pick & Place Cube) between the baselines not using key states and CoTPC already demonstrates the effectiveness of the key states. Also, the comparison between CoTPC and our results shows that there are even more key states that are useful even within such a short-horizon P&P Cube task. \n\nFor example, in CoPTC, only one key state is defined, which is \u201cgrasp.\u201d However, our method employs an LLM to propose many more (semantically meaningful) key states and ground them with the proposed information-theoretic criterion. For example, the key states proposed for Pick & Place Cube in our method are:\n\n{\n- `KS 1`: Robotic arm positioned above the cube.\n- `KS 2`: Robotic arm's gripper aligned with the cube.\n- `KS 3`: Robotic arm has securely grasped the cube.\n- `KS 4`: Robotic arm lifted, holding the cube.\n- `KS 5`: Robotic arm positioned above the goal position.\n\n}\n\nThe flexibility of employing LLMs to propose more semantically meaningful key states enables the better performance of our trained policies. It is also evidence that short-horizon tasks can also benefit from fine-grained (but semantically meaningful) key states.\n\nAdditionally, we also conducted experiments on the more complex long-horizon task, Franka Kitchen, to verify the effectiveness of key state grounding. The basic neural network architecture is Decision Transformer (DT). The results are shown below. We can find that in this task, DT with key states can achieve superior performance over the one without key states. In the unseen environment, the relative improvement was as high as 73.7%. \n| Franka Kitchen (Success rate) | Seen environment | Unseen environment |\n|-------------------------------|------------------|--------------------|\n| Without key states            | 78.4             | 13.7               |\n| With key states               | 81.8             | 23.8               |\n\n\n***"
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3101/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700498145253,
                "cdate": 1700498145253,
                "tmdate": 1700498145253,
                "mdate": 1700498145253,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "BXIf5cuy1M",
            "forum": "hCaoJh01io",
            "replyto": "hCaoJh01io",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3101/Reviewer_4ns7"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3101/Reviewer_4ns7"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a method to ground manipulation concepts generated by Large Language Models (LLMs) into physical states, facilitating the training of efficient manipulation policies. The authors observe that current methods, which rely on multimodal foundation models to localize key states, often lack accuracy and semantic consistency. To address this, they propose an information-theoretic criterion called Maximal Information Boost (MIB) to enhance grounding efficiency without requiring extensive fine-tuning. The approach is based on the observation that the uncertainty of a state diminishes rapidly as it approaches a key state."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "2 fair"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "This paper presents an interesting idea of using the maximal information boost (MIB) principle to align key state descriptions in language with image states in demonstration trajectories. The final goal of this approach is to enable better imitation learning from demonstration trajectories. The proposed approach is interesting in the sense that it introduces a novel metric for enforcing grounding."
                },
                "weaknesses": {
                    "value": "There are three major weaknesses of the paper.\n\nFirst, the writing of the paper is not very clear. For example, the majority of the paper describes how we can use MIB to segment trajectories, but there is no description of how this MIB-derived segmentation is actually used in training the policy network. I believe this part should be at least briefly described in the paper. (I found it in the supplementary material, but I would like to mention that reviewers are not subject to reading the full supplementary material).\n\nSecond, while I found the introduced metric novel and interesting, I did not fully get its intuition. In particular, is it just the case that the speed of the robot arm will be very low when it's close to a \"key state?\" Because the uncertainty is essentially defined as s_t | s_{t-dt}.\n\nThird, since the contribution of the paper lies in segmenting trajectories (the policy part is directly following CoTPC, based on the supplementary description), there is little comparison between the segmentation method and other related approaches, such as\n\n- Skill Induction and Planning with Latent Language https://arxiv.org/abs/2110.01517\n- Learning from Trajectories via Subgoal Discovery https://proceedings.neurips.cc/paper_files/paper/2019/file/6f518c31f6baa365f55c38d11cc349d1-Paper.pdf\n- CompILE: Compositional Imitation Learning and Execution https://arxiv.org/abs/1812.01483\n- Identifying useful subgoals in reinforcement learning by local graph partitioning https://dl.acm.org/doi/10.1145/1102351.1102454\n\namong many, many others. The authors should discuss, and compare with these algorithms.\n\nAlso, it seems that the proposed method has the limitation that each key state can only be visited once in a demonstration trajectory (due to the Chronological assumption). Therefore, it can't model task descriptions that require loops (e.g., scoop all sugar, possibly multiple rounds, to transfer them from one bowl into another bowl)."
                },
                "questions": {
                    "value": "If the paper is based on CoTPC and CoTPC uses predefined key states, why would the performance of this paper be better than CoTPC? Is it because CoTPC is using a smaller number of key states? or due to noises in the labels?\n\nIs the variance computed in the pixel space? (eq 7)"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3101/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698781693781,
            "cdate": 1698781693781,
            "tmdate": 1699636256364,
            "mdate": 1699636256364,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "saYWEfwYoR",
                "forum": "hCaoJh01io",
                "replyto": "BXIf5cuy1M",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3101/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3101/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer 4ns7,\n\nThanks for the constructive feedback and suggestions. We appreciate that you found our proposed metric novel and interesting.\n\nWe understand that more explanations and comparisons would help your re-evaluation of our paper. Especially, on the intuition of the proposed metric, as well as the comparison with more baselines. Please find our answers and clarifications below in detail.\n\nWe hope the answers can help finalize your assessment and the rating of our paper. Please also let us know if you have any further questions that we need to provide additional clarifications.\n\n***\n**W1: The writing of the paper is not very clear. For example, the majority of the paper describes how we can use MIB to segment trajectories, but there is no description of how this MIB-derived segmentation is actually used in training the policy network. I believe this part should be at least briefly described in the paper. (I found it in the supplementary material, but I would like to mention that reviewers are not subject to reading the full supplementary material).**\n\n**A**: We appreciate your careful review and constructive feedback. We placed the policy network training details in the supplementary material due to page restrictions in the main manuscript. We thought the experimental setup could serve a similar purpose, but apparently, it causes a gap in the information flow. In the revised version, we have added a **new subsection 2.5** to elaborate on the training of the policy networks.\n***\n**W2: While I found the introduced metric novel and interesting, I did not fully get its intuition. In particular, is it just the case that the speed of the robot arm will be very low when it's close to a \"key state?\" Because the uncertainty is essentially defined as s_t | s_{t-dt}**\n\n**A**: We appreciate your observations and would like to clarify that a key state does not necessarily correlate with a very low speed of the robot arm; it's more about the reduction of uncertainty in the latent state space.\n\nAn intuitive example is outlined in Appendix A.2. To clarify and avoid any confusion regarding speed, consider another example: We are in the source room aiming to get to the target room, which is connected solely by an open door. Regardless of our path within the source room, accessing the target room necessitates passing through this door. This action of passing through the door symbolizes a key state, which is independent of the speed we traverse it at\u2014we could do so either quickly or slowly. In the semantic space, however, the uncertainty s_t\u200b\u2223s_{t\u2212dt}\u200b is significantly reduced. \n\nMoreover, we would like to point out that since mutual information is invariant under scaling, the proposed metric is not affected by the speed, e.g., multiplying the state with a scalar can change the speed but not the mutual information. Thus, the proposed information-theoretic criterion is more than just determining key state by speed. \n\n***"
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3101/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700498042916,
                "cdate": 1700498042916,
                "tmdate": 1700498042916,
                "mdate": 1700498042916,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "KoEp30rIlV",
            "forum": "hCaoJh01io",
            "replyto": "hCaoJh01io",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3101/Reviewer_KD76"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3101/Reviewer_KD76"
            ],
            "content": {
                "summary": {
                    "value": "The paper proposes a method called Maximal Information Boost (MIB) to efficiently connect language models with physical states in robotic manipulation tasks. It also introduces Key State Localization Network (KSL-Net) for matching physical states to their descriptions and predicting the likelihood of correctness. The method improves semantic alignment, enhances performance, and generalizes manipulation policies across various tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Introduce the Maximal Information Boost (MIB) criterion to enhance grounding efficiency without requiring fine-tuning.\n- Develop the Key State Localization Network (KSL-Net) to determine the probability of each state being the grounded key state.\n- Enhance semantic compatibility and improve generalization in robotic tasks."
                },
                "weaknesses": {
                    "value": "- The LLM lacks visual information or constrains about the environment, which restricts its ability to make geometry-based decisions like collision avoidance.\n- It is necessary to manually create various paraphrases that are semantically consistent depending on the tasks.\n- The paper does not address how well the proposed approach can generalize to unseen tasks."
                },
                "questions": {
                    "value": "- Do you need to train a separate model for each task? Given the simulated environments are relatively simple, how can you justify that the model is not overfitting the environment?\n- The environments presented in the paper are clear scenarios, involving only a limited number of objects relevant to specific tasks. I'm curious about how the approach would handle complex scenes with multiple objects and some of them are unrelated to the task, as this might introduce ambiguity.\n- Do you still need to do additional planning in your setup to complete the task with key frames?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3101/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3101/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3101/Reviewer_KD76"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3101/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699440003834,
            "cdate": 1699440003834,
            "tmdate": 1699636256293,
            "mdate": 1699636256293,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "3P8GY5DAbr",
                "forum": "hCaoJh01io",
                "replyto": "KoEp30rIlV",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3101/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3101/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer KD76, \n\nThank you for the constructive comments and suggestions. We also appreciate your acknowledgment of the soundness, presentation, and effectiveness of our work.\n\nWe understand that clarifications on the usage of LLMs in our pipeline, the role of paraphrasing with LLMs, and discussion on generalization are needed for your re-evaluation of the proposed key state grounding method. We also provide an experiment on more complex scenes to demonstrate the effectiveness of our algorithm when facing distractions or ambiguities. Please find details below.\n\nWe hope the answers can help finalize your assessment and the rating of our paper. Please also let us know if you have any further questions that we need to provide additional clarifications.\n\n***\n**W1: The LLM lacks visual information or constrains about the environment, which restricts its ability to make geometry-based decisions like collision avoidance.**\n\n**A**: We agree that the LLM lacks visual information to make geometry-based decisions like collision avoidance. However, this is not a critical issue for our method, since we are not relying on LLM to make low-level control decisions.\n\nOur method only leverages an LLM to provide potential key states involved in achieving a task, which does not need accurate geometric information. Our method then focuses on grounding these key states in the demonstrations, which then provides the grounded physical states for better policy learning and generalization. The experiments have shown that by grounding the LLM-generated key states, the trained policies can surpass human-provided key states, evidencing the effectiveness of the proposed grounding method.\n\nAgain, thanks for the comment. In the future, we would like to see how the key states provided by an enhanced LLM (e.g., GPT-4V) can help with the learned policy.\n ***\n**W2: It is necessary to manually create various paraphrases that are semantically consistent depending on the tasks.**\n\n**A**: It appears there may be a slight misunderstanding. In our experiments, various paraphrases are generated automatically by LLM, eliminating the need for manual modifications. The first stage of the process, instruction generation for getting key state descriptions, relies on minimal task priors such as specified task description. The second stage, paraphrase generation, is solely dependent on the instructions generated in the first stage and does not require any task priors.\n\nThe necessity of paraphrases lies in the fact that pretrained visual-language encoders may be subject to domain gaps and insufficient granularity in their training data and thus introduce noise in the compatibility function (between the image and the key state description). Therefore, the primary purpose of paraphrasing the instruction is to create semantically equivalent versions of the key state description and use them to form an averaged compatibility function, so that the noise in the encoders can be canceled out.\n\nThis operation is observed to stabilize the training (optimization) process. In our experience, the incorporation of paraphrasing has been observed to speed up the training convergence by 20%.\n\n***\n**W3: The paper does not address how well the proposed approach can generalize to unseen tasks.**\n\n**A**: In terms of the generalization of the policy on unseen environments, Table 2 in our paper presents the required evaluation. To elaborate, Table 1 represents the performance of our proposed method on seen task configurations, while Table 2 demonstrates its performance on unseen task configurations, showcasing our strong generalization capabilities of the trained policies under key state guidance. \n\nIn terms of the generalization of the grounding network, we would like to point out that it is generalizable by nature. The full training procedure of the grounding network requires no human annotation of the ground-truth groundings. In other words, the training objective for the grounding network is unsupervised, which contains only the compatibility function, and the mutual information boost (MIB) for enhancing the physical meaningfulness of the grounding. Thus the pipeline can be trained in any situation without concerning the test-time generalization.\n\n***"
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3101/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700497883890,
                "cdate": 1700497883890,
                "tmdate": 1700497883890,
                "mdate": 1700497883890,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "tY7ajghbU9",
            "forum": "hCaoJh01io",
            "replyto": "hCaoJh01io",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission3101/Reviewer_S4jr"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission3101/Reviewer_S4jr"
            ],
            "content": {
                "summary": {
                    "value": "The authors note that existing techniques using multimodal foundation models for key state identification are often imprecise and semantically inconsistent. To improve this, the authors introduce a concept named Maximal Information Boost (MIB), an information-based metric to increase grounding effectiveness without extensive fine-tuning."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Innovative Concept: The approach leverages Multimodal Instance Binding (MIB) to focus on key state grounding, which is a novel method.\n- The method has proven to enhance semantic compatibility with instructions, as evidenced by the experiments."
                },
                "weaknesses": {
                    "value": "1. The robotic environment/task as described by the authors does not necessarily need key-state grounding. I believe 100 clean expert demonstrations with end-to-end BC-RNN could achieve similar performances. Key state grounding is useful for long-horizon tasks. I would recommend environments/tasks like [franka Kitchen](https://robotics.farama.org/envs/franka_kitchen/franka_kitchen/) used by BeT and by real-world tasks in [MimicPlay](https://www.notion.so/CoRL2023-reviews-92631182f55f47979d5f15dbdf7c0f7c?pvs=21) . I will change my view if I see more complex robotic applications.\n\n2. If I understand correctly, the LLM in your setup requires task-specific prompts. As described in Appendix A.9.\nIf the prompt already contains the information needed for the LLM to respond, I don\u2019t see why I should use an LLM. I could directly provide the information to the robot."
                },
                "questions": {
                    "value": "# \n\n1. Is the Exemplar part of the prompt? For example, on page 19:\n{Exemplar: For a task that involves picking up a cube and moving it to a goal position\u2026..}\n2. Doesn\u2019t the prompt already contain all the information needed for the task? Why do I need to use an LLM? What's the key difference between the exemplar and LLM response?\nHow many attempts did you make to ask ChatGPT to get the answer you wanted?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "3: reject, not good enough"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission3101/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3101/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission3101/Reviewer_S4jr"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission3101/-/Official_Review"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699656710728,
            "cdate": 1699656710728,
            "tmdate": 1699656913127,
            "mdate": 1699656913127,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "rnpbV4RIud",
                "forum": "hCaoJh01io",
                "replyto": "tY7ajghbU9",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission3101/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission3101/Authors"
                ],
                "content": {
                    "comment": {
                        "value": "Dear Reviewer S4jr,\n\nThanks for your questions and constructive suggestions. We also appreciate your acknowledgment of the novelty and effectiveness of the proposed key state grounding algorithm.\n\nWe understand that extra baseline comparisons are needed for your re-evaluation of our paper, and some clarifications are needed on the usage of the LLM to alleviate the misunderstanding. Accordingly, we have provided detailed answers to the questions and comments below.\n\nWe hope the answers can help finalize your assessment and the rating of our paper. Please also let us know if you have any further questions that we need to provide additional clarifications.\n***\n**W1.1: The robotic environment/task as described by the authors does not necessarily need key-state grounding. I believe 100 clean expert demonstrations with end-to-end BC-RNN could achieve similar performances.**\n\n**A**: Thanks for the comment. We have performed an additional experiment to provide more information on this point. Specifically, we have experimented with a BC-RNN structure [6] using 500 demonstrations per task for 4 tasks from our environment (listed below).\n\nBoth BC-RNN and Decision Transformer (DT) do not employ key states during the training of the policy, while CoTPC and Ours utilize key states to improve the policy training efficiency and generalization.\nWe can observe that the BC-RNN structure performs worse than the DT structure when trained using the same 500 demonstrations. Moreover, by employing key states during policy training, the latter two (CoTPC [1] and Ours) outperform BC-RNN and DT by a big margin. Together these comparisons demonstrate that BC-RNN without key state prediction encounters difficulty in efficiently learning the tasks used in our environment. Also, the comparison between CoPTC and Ours demonstrates that the way of using (grounding) key states proposed in our paper is more effective. \n\nParticularly, we would like to clarify that Peg insertion is a complex, multi-stage, long-horizon task, and we have observed that other methods do not achieve high success rates on such long-horizon tasks. Moreover, our method shows about a 30% improvement over the SOTA baseline (CoTPC), both in seen and unseen environments.\n| Algorithm         | P&P Cube | Stack Cube | Turn Faucet | Peg Insertion | Mean |\n|-------------------|----------|------------|-------------|---------------|------|\n| BC-RNN [6]        | 62.7     | 7.8        | 43.2        | 3.2           | 29.2 |\n| Decision Transformer | 65.4  | 13.0       | 39.4        | 5.6           | 30.9 |\n| CoTPC             | 75.2     | 58.8       | 56.4        | 52.8          | 60.8 |\n| Ours              | 89.4     | 83.2       | 67.6        | 75.4          | 78.9 |\n\n\n**W1.2: Key state grounding is useful for long-horizon tasks. I would recommend environments/tasks like Franka Kitchen used by BeT and by real-world tasks in MimicPlay. I will change my view if I see more complex robotic applications.**\n\n**A**: Thanks for your suggestion. We have further experimented with the mentioned environments/tasks to demonstrate that key states aid in policy learning. \n\nFor the Franka Kitchen task, where the success rate is measured in completing a specified sequence of multiple subtasks, the Decision Transformers (DT) with and without key states are compared. \n\nWe can observe that the DT with key states demonstrates competitive performance in seen environments, showing better learning efficiency of the policy when supplied with the key states. Moreover, we can observe the DT trained with key states performs much better than the DT w/o key states in unseen environments, achieving a relative 73.7% improvement. This indicates that key state guidance has a substantial positive impact on the generalization of the learned policy for complex long-horizon tasks. The results are listed below.\n\n| Franka Kitchen (Success rate) | Seen environment | Unseen environment |\n|-------------------------------|------------------|--------------------|\n| Without key states            | 78.4             | 13.7               |\n| With key states               | 81.8             | 23.8               |\n\n\nWe have also examined the MimicPlay dataset and attempted to perform similar experiments. However, we found that MimicPlay focuses on transferring human policy to robot policy with a large number of human demonstrations, which is currently out of our scope. But we would like to thank the reviewer for pointing this out, which can actually be an interesting future research direction for leveraging key state grounding for policy transfer.\n***"
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission3101/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700497624120,
                "cdate": 1700497624120,
                "tmdate": 1700497624120,
                "mdate": 1700497624120,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]