[
    {
        "title": "Divided Attention: Unsupervised Multiple-object Discovery and Segmentation with Interpretable Contextually Separated Slots"
    },
    {
        "review": {
            "id": "rbz3NvMqK8",
            "forum": "2MpOjashKU",
            "replyto": "2MpOjashKU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1012/Reviewer_5Uwa"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1012/Reviewer_5Uwa"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposed a motion segmentation method to segment multiple objects, based on optical flow in an unsupervised manner. In this setting, both images and optical flow are available. Based on the SAN method, this work proposed an adversarial conditional encoder-decoder architecture. The proposed method can handle different numbers of objects at training and test time. The experimental results demonstrate the effectiveness of the proposed method."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "(1) It can handle different numbers of objects at both training and test time.\n\n(2) It can run in real-time.\n\n(3) The performance is good."
                },
                "weaknesses": {
                    "value": "(1) The ability of handling multi-object comes from SAN. The main contribution may be the adversarial framework and the manner using the image information. The authors should highlight the contributions of this paper.\n\n(2) Handling multi-object is not new in video object segmentation.\n\n(3) Why g_theta is an \"adversarial\" decoder is not clear. It forces the decoder to reconstruct the entire flow with each slot, which seems the two decoder do not have an adversarial relationship. \n\n(4) This method \"frees the representation from having to encode complex nuisance variability in the image\", which should be demonstrated in the experiments. For example, simply combining (concat) the image and the optical flow as input can be considered as a baseline. Although the authors mentioned combined input is complex and the slots are low-dimentional. More reasonable explanation is needed.\n\n(5) This setting is interesting,  but I guess its performance is heavily related to performance of the optical flow network. In the inference stage, the optical flow is also need to calculate first. \n\n(6) P4, DivA has two key advantages, but the following text is mainly about the disadvantages of MoSeg."
                },
                "questions": {
                    "value": "see the weaknesses."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "N/A"
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1012/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698582110426,
            "cdate": 1698582110426,
            "tmdate": 1699636027208,
            "mdate": 1699636027208,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "nABbKbJlBY",
                "forum": "2MpOjashKU",
                "replyto": "rbz3NvMqK8",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1012/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1012/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to the initial review"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review. Here we address your concerns below:\n\n**W1:** *The ability of handling multi-object comes from SAN. The main contribution may be the adversarial framework and the manner using the image information. The authors should highlight the contributions of this paper.*\n\n**R1:** We appreciate your suggestion and will make revisions to highlight the contributions.\n\nRegarding SAN, it's important to note that, although SAN is designed to handle multi-object, naively applying SANs to motion leads to disappointing results, which is validated by the results in Table 2, Table 3, and Figure 5. To make SANs effective when applied to motion, our key contributions include 1) introducing a conditional decoder to align the segmentation mask with object boundaries and 2) incorporating an adversarial framework to render the slots in SAN mutually uninformative. We will make them clear to the readers.\n\n**W2:** *Handling multi-object is not new in video object segmentation.*\n\n**R2:** We acknowledge that we are not the first to address multiple objects, and do provide a detailed literature review of multi-object segmentation in Sect. 1.1. However, our method, DivA, stands out as the first to exhibit several distinctive features:\n- It can handle a different number of objects without the need for re-training (so that it is versatile for real applications where the number of object is unknown).\n- It trains without explicit regularization (so that it can be trained on any generic video data).\n- It only requires single image-flow pairs and operates in real-time (so that it is suitable for deployment in close-loop real-time perception systems with low algorithmic latency).\n- It does not necessitate pre-training (so that it discovers \"objects\" from scratch and is not susceptible to data bias in pre-training).\n\nThese unique characteristics demonstrate its advantages over existing multi-object methods.\n\n**W3:** *Why $g_\\theta$ is an \"adversarial\" decoder is not clear. It forces the decoder to reconstruct the entire flow with each slot, which seems the two decoder do not have an adversarial relationship.*\n\n**R3:** The adversarial relationship arises when $g_\\theta$ aims to reconstruct the entire flow, while the $f_w$ aims to hinder $g_\\theta$ from reconstructing the flow OUTSIDE the region $m_i$ from $x_i$, so that each slot $x_i$ is only \"informative\" about the flow inside $m_i$, and is \"uninformative\" about the flow outside $m_i$. This is modeled by the mini-max formulation of the loss in equation (2), and minimizes mutual information between different slots. Please refer to Sect. 2.3 for the details.\n\n**W4:** *This method \"frees the representation from having to encode complex nuisance variability in the image\", which should be demonstrated in the experiments. For example, simply combining (concat) the image and the optical flow as input can be considered as a baseline. Although the authors mentioned combined input is complex and the slots are low-dimentional. More reasonable explanation is needed.*\n\n**R4:** This is a good advice. In fact, we have tried this setup by concatenating the image to the flow and having SAN reconstruct the flow. On page 4, Sect 2.2, we wrote \"However, naive use of SAN to jointly auto-encode real-world images and flow leads to poor reconstruction.\" This refers to the exact setting you are suggesting, and we will revise the paper to make it clear.\n\nThe reason is, that although the input consists of both the image and flow, they are compressed through a narrow bottleneck. Due to nuisance variability, reconstructing from this highly compressed slot poses a challenge for the decoder, especially in real-world videos where the objects have complex shapes and textures. Our conditional decoder addresses this issue by allowing the slot to provide a general \"hint\" for reconstruction, with the conditional decoder handling fine details. Please refer to Figure 10 in the Appendix for a more detailed discussion.\n\n**W5:** *This setting is interesting, but I guess its performance is heavily related to the performance of the optical flow network. In the inference stage, the optical flow is also needed to calculate first.*\n\n**R5:** We agree that this is a legitimate concern, and applies to all object discovery schemes based on optical flow. And that's indeed why we keep the optical flow consistent with baseline methods (MoSeg, EM, etc.) when making comparisons.\n\n**W6:** *P4, DivA has two key advantages, but the following text is mainly about the disadvantages of MoSeg.*\n\n**R6:** We will rephrase.\n\nWe hope this response answers the reviewer's question. If there are further concerns, we are more than willing to provide additional explanations."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1012/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700175532946,
                "cdate": 1700175532946,
                "tmdate": 1700240343388,
                "mdate": 1700240343388,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "lmJ5yA1EMQ",
            "forum": "2MpOjashKU",
            "replyto": "2MpOjashKU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1012/Reviewer_RqAo"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1012/Reviewer_RqAo"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes a method to segment the visual field into independently moving regions. The proposed method uses a cross-modal conditional decoder that takes a second modality as input  to reconstruct the first modality.  This design frees the representation from having to encode complex nuisance variability in the image, such as illumination and reflectance properties of the scenes."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper is well-written and easy to follow.\n\nThe experimental results demonstrate better performance.\n\nThe idea is simple and effective."
                },
                "weaknesses": {
                    "value": "I cannot find the obvious weakness."
                },
                "questions": {
                    "value": "How about the GPU memory consumption?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "details_of_ethics_concerns": {
                    "value": "n/a"
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1012/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698809286046,
            "cdate": 1698809286046,
            "tmdate": 1699636027124,
            "mdate": 1699636027124,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "poIP0cwLYE",
                "forum": "2MpOjashKU",
                "replyto": "lmJ5yA1EMQ",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1012/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1012/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to the initial review"
                    },
                    "comment": {
                        "value": "Thank you for the positive review.\n\nRegarding GPU memory, as stated on page 6 \"We aim to keep the training pipeline simple and all models are trained on a single Nvidia 1080Ti GPU with PyTorch.\" The GPU has 11 Gb memory, and handles 4 slots and a batch size of 32 when trained with $128 \\times 128$ input resolution."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1012/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700176585322,
                "cdate": 1700176585322,
                "tmdate": 1700176585322,
                "mdate": 1700176585322,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "EVc2zxH6Ir",
            "forum": "2MpOjashKU",
            "replyto": "2MpOjashKU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1012/Reviewer_8fzT"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1012/Reviewer_8fzT"
            ],
            "content": {
                "summary": {
                    "value": "This work introduces Divided Attention, an extension of the Slot Attention Network (SAN) for unsupervised multi-object discovery in real-time. Divided Attention takes both the RGB image and optical flow as inputs, and learns a set of \u201cslots\u201d as encodings that can reconstruct the optical flow. By constructing the optical flow (instead of image reconstruction in typical SAN), the model can focus on separating the objects in the scene and understanding their motion, rather than overfitting to the relatively less related visual details such as illumination and texture. Another key component is an adversarially trained flow decoder, which attempts to reconstruct the entire flow from each individual slot (the main decoder reconstructs the flow only within each mask). By employing this adversarial training, the slots are encouraged to learn \u201ccontextually separated\u201d encoding of the scene, and consequently result in separated, interpretable object representations."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- This work proposes to leverage optical flow for unsupervised multi-object discovery. In a video setting, it is more intuitive to extract information from the motion of pixels, and discover coherent regions as independently moving objects.\n\n- Divided Attention does not require any pre-trained visual features, and thus is more flexible to be applied in various applications. Moreover, its training and inference can use a dynamic number of slots, depending on the context.\n\n- The model is very efficient, enabling real-time inference speed."
                },
                "weaknesses": {
                    "value": "- Flow input: In real-world practice, optical flow has to be obtained by running a flow estimation algorithm (e.g., RAFT). This would raise two concerns: 1) The flow estimation model is pre-trained with external knowledge and data in a supervised manner, which somehow contradicts with the claim that Divided Attention is unsupervised and requires no pre-trained features. 2) If we take the inference time of the flow estimation model into consideration, the FPS of the whole pipeline would be decreased, and achieving the real-time application would be more challenging.\n\n- Temporal consistency: In the base version of Divided Attention, temporal consistency across frames is not guaranteed. Additional tricks (e.g., inheriting slots from previous frames, or post-processing results of multiple frames) need to be incorporated for temporal consistency. This is not desirable considering the main application is object segmentation in videos.\n\n- Missing ablation study: It is suggested to quantitatively examine the design choices in Divided Attention via ablation study experiments. For example, $\\lambda$ in the adversarial training, the model architecture, and the number of slots during training and inference, should be tested with different choices for justification and better understanding of the proposed method, Divided Attention."
                },
                "questions": {
                    "value": "- In Table 1, why are the FPS of some baselines (including DyStab) not listed?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1012/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699336465690,
            "cdate": 1699336465690,
            "tmdate": 1699636027032,
            "mdate": 1699636027032,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "V8ifRqpzNk",
                "forum": "2MpOjashKU",
                "replyto": "EVc2zxH6Ir",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1012/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1012/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to the initial review"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review. Here we address your concerns below:\n\n**W1:** *Flow input: In real-world practice, optical flow has to be obtained by running a flow estimation algorithm (e.g., RAFT). This would raise two concerns: 1) The flow estimation model is pre-trained with external knowledge and data in a supervised manner, which somehow contradicts with the claim that Divided Attention is unsupervised and requires no pre-trained features. 2) If we take the inference time of the flow estimation model into consideration, the FPS of the whole pipeline would be decreased, and achieving the real-time application would be more challenging.*\n\n**R1:** This is a legitimate concern, and we have addressed it in the Page 3 footnote. We adhere to the convention in unsupervised moving object segmentation, treating optical flow as an off-the-shelf module. In practice, optical flow can be determined without the need for learning [1,2].  We use RAFT [3] only for a fair comparison with baseline methods. Notably, RAFT demonstrates excellent generalization to a diverse range of visual data, and we refrain from fine-tuning the it for our target data.\n\nFrom an engineering perspective, as more real-time optical flow methods become accessible (including RAFT, which can also operate in real-time), DivA has the potential to be applied across a wide range of applications.\n\n[1] Determining optical flow. Horn, Berthold KP, and Brian G. Schunck. \"Determining optical flow.\" Artificial intelligence 17.1-3 (1981): 185-203.\n\n[2] Secrets of optical flow estimation and their principles. Sun, Deqing, Stefan Roth, and Michael J. Black. \"Secrets of optical flow estimation and their principles.\" 2010 IEEE computer society conference on computer vision and pattern recognition. IEEE, 2010.\n\n[3] Raft: Recurrent all-pairs field transforms for optical flow. Teed, Zachary, and Jia Deng. \"Raft: Recurrent all-pairs field transforms for optical flow.\" Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part II 16. Springer International Publishing, 2020.\n\n**W2:** *Temporal consistency: In the base version of Divided Attention, temporal consistency across frames is not guaranteed. Additional tricks (e.g., inheriting slots from previous frames, or post-processing results of multiple frames) need to be incorporated for temporal consistency. This is not desirable considering the main application is object segmentation in videos.*\n\n**R2:** DivA is motivated by biological vision that operates in real-time with low latency and low computational budget (see Sect. 1), so we design it to handle instantaneous motion, operating on image-flow pairs. This makes DivA suitable for online closed loop visual perception systems that receive data sequentially and run inference on-the-fly (just like biological vision). The primary goal of DivA is object discovery. In real applications like video object segmentation, once an object is discovered, one is free to apply tracking, or engage image-level feature learning. \n\nHowever, in applications with more computational budget and no real-time constraints, incorporating temporal consistency could enhance the system, as suggested by the reviewer. It is worth noting that this is tangential to the primary purpose of DivA, but if needed, one may also integrate DivA into such systems where it can function as a sub-module.\n\n**W3:** *Missing ablation study: It is suggested to quantitatively examine the design choices in Divided Attention via ablation study experiments. For example,  in the adversarial training, the model architecture, and the number of slots during training and inference, should be tested with different choices for justification and better understanding of the proposed method, Divided Attention.*\n\n**R3:** We do provide ablations on adversarial training in Table 2 and Figure 5, and ablations on number of slots in Table 3. Ablations on network modules, including conditional decoder and adversarial decoder, are also in Table 2 and 3.\n\n**Q1:** *In Table 1, why are the FPS of some baselines (including DyStab) not listed?*\n\n**R4:** Our emphasis is on comparing the speed of DivA with single-frame methods, aligning with the purpose of DivA. When it comes to video batch methods, the FPS (some of them not reported by the original author) may not accurately represent the running speed because of the delay introduced by collecting the batch of frames. However, we acknowledge this concern, and in the next revision, we will include information about their speed.\n\nWe hope this response answers the reviewer's question. If there are further concerns, we are more than willing to provide additional explanations."
                    }
                },
                "number": 5,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1012/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700182089292,
                "cdate": 1700182089292,
                "tmdate": 1700185078436,
                "mdate": 1700185078436,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "8Ri9N0LSFH",
                "forum": "2MpOjashKU",
                "replyto": "V8ifRqpzNk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1012/Reviewer_8fzT"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1012/Reviewer_8fzT"
                ],
                "content": {
                    "title": {
                        "value": "Reviewer Response"
                    },
                    "comment": {
                        "value": "The reviewer appreciates the authors' helpful responses. Although the idea proposed in this work is indeed interesting for future reseach, the reviewer still has some concerns after reading the responses from the authors and comments from other reviewers:\n- The method is proposed for handling instantaneous motion. However, it requires running an optical flow estimator first (also mentioned by Reviewer 5Uwa), and the inference efficiency comparison is rather obscure currently.\n- The flow-based approach is only able to discover objects in motion (Reviewer i9DX). Objects, by the common understanding in vision, could be static but not considered in this work. It would be better if the problem scope is revised.\n\nGiven this concerns, the reviewer decides to remain the current rating. Again, the authors' effort in polishing this work and providng feedback is greatly appreciated."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1012/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700689736112,
                "cdate": 1700689736112,
                "tmdate": 1700689736112,
                "mdate": 1700689736112,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "ndSdCjTta6",
            "forum": "2MpOjashKU",
            "replyto": "2MpOjashKU",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission1012/Reviewer_i9DX"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission1012/Reviewer_i9DX"
            ],
            "content": {
                "summary": {
                    "value": "The paper presents Divided Attention (DivA), an unsupervised method for segmenting visual fields into independently moving regions without manual supervision. The model uses an adversarial conditional encoder-decoder architecture with interpretable latent variables, building on the Slot Attention architecture. It's designed to decode optical flow using the image as context without reconstructing the image itself, thus avoiding issues with complex image variability. DivA can handle varying numbers of objects and resolutions, is invariant to object label permutations, and doesn\u2019t require explicit regularization or pre-trained features. It achieves high run-time speed (up to 104FPS) and narrows the performance gap with supervised methods to 12% or less. The code will be made available upon publication."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "1 poor"
                },
                "strengths": {
                    "value": "+ This paper is well-written and easy to follow.  \n+ The model is capable of inference speeds up to 104FPS, significantly faster than current unsupervised methods.  \n+ The method does not rely on pre-trained image features from external datasets, enabling its use in a broader range of scenarios."
                },
                "weaknesses": {
                    "value": "Firstly, I suggest the authors pay attention to the terminologies: in the title and introduction, the authors claim to do multi-object discovery/segmentation. However, they are actually doing moving object segmentation. \"regions of an image whose corresponding motion is unpredictable from their surroundings\" This should be the definition of moving objects but not objects. In other words, for some datasets with both moving and non-moving objects, such as MOVI-D, the proposed method will fail to work. \n\nSecondly, I think the novelty/contribution of this work is limited. Reconstructing in the flow space with slot attention has been widely explored before, e.g. SAVI, the conditional decoder and the adversarial loss is also somewhat not quite novel. \n\nMoreover, there is no explanation why the author wants to take flow as the input and reconstruction space but RGB as the condition rather than take RGB as the input but flow as the condition. More ablations regarding this should be conducted. Also, quantitative results for the ablation with adversarial decoder should also be reported as that's one of the claimed contributions.\n\nFinally, more visualizations should be included, at least in the supplementary.\n\n---------------------\nThanks again for the quick reply.\n\nFor R1, I think I understand what the authors claimed -- the definition of the ``object'' is proper and in principle, the non-moving object can be captured in real practice, with the proposed method. However, I still doubt if that's the case -- it's hard to predict the results unless seeing the results.\n\nFor R2, the discussions for those references sound promising. Though one minor thing is, for [4], the authors have ablations to verify that their method can still work without pre-trained features -- they can first train the ViT from the target dataset and then do object discovery in that space. Not to mention both [3] and [4] require no additional data (flow) but just the RGB images.\n\nI respect the effort of the authors during the rebuttal stage and would like to slightly upgrade my rating, but will not champion this paper."
                },
                "questions": {
                    "value": "See previous section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission1012/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1012/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission1012/Reviewer_i9DX"
                    ]
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission1012/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1699396988138,
            "cdate": 1699396988138,
            "tmdate": 1700777583813,
            "mdate": 1700777583813,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "fOFBuDwubH",
                "forum": "2MpOjashKU",
                "replyto": "ndSdCjTta6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1012/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1012/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Author response to the initial review"
                    },
                    "comment": {
                        "value": "Thank you for your thoughtful review. Here we address your concerns below:\n\n**W1:** *Firstly, I suggest the authors pay attention to the terminologies: in the title and introduction, the authors claim to do multi-object discovery/segmentation. However, they are actually doing moving object segmentation. \"regions of an image whose corresponding motion is unpredictable from their surroundings\" This should be the definition of moving objects but not objects. In other words, for some datasets with both moving and non-moving objects, such as MOVI-D, the proposed method will fail to work.*\n\n**R:** There are many definitions of objects, we adopt Gibson\u2019s definition as \u201clayouts of surfaces surrounded by the medium\u201d [1]. Due to parallax, such objects elicit piecewise smooth optical flow *even if they are static* just due to the viewer\u2019s motion. Therefore, our definition is not restricted to independently moving objects, but to any scene that is non-convex and, consequently, results in piecewise smooth optical flow \u2014 with occluding boundaries \u2014 as a result of either viewer or object motion.\n\nNote that we define the object as \u201cregions of an image\u201d whose corresponding motion is unpredictable, we do not refer to regions of the scene. Static scenes (that do not move) result in regions of the images that move, as a result of the viewer\u2019s motion, and so long as they are not globally convex (which no real scene is) they fit our definition.\n\n[1] James J Gibson. The ecological approach to the visual perception of pictures.\n\n**W2:** *Secondly, I think the novelty/contribution of this work is limited. Reconstructing in the flow space with slot attention has been widely explored before, e.g. SAVI, the conditional decoder and the adversarial loss is also somewhat not quite novel.*\n\n**R:** Respectfully, we argue that the \u201cnot quite novel\u201d assessment may oversimplify the significance of our contributions. Given that slot attention was introduced three years ago, to our knowledge, it has not been widely applied to unsupervised multi-object discovery on data beyond toy examples and highly constrained data domains, whereas DivA has been rigorously tested on more generic video datasets such as DAVIS and FBMS, without restrictions on object classes or background motion. But we welcome any specific reference that the reviewer may have.\n\nFurther, it is important to note that SAVI is not strictly an unsupervised method, as it requires input in the first frame, as detailed in our discussion on page 9.\n\n**W3:** *there is no explanation why the author wants to take flow as the input and reconstruction space but RGB as the condition rather than take RGB as the input but flow as the condition.*\n\n**R:** We do provide an explanation in the first two paragraphs of Section 2.2. Here we provide further insights: We believe in embodied cognition, and anything that we may want to define as an object is either something that moves independently of the surrounding (e.g., an animal) or that is at least partially surrounded by the medium (e.g., trees, houses, rocks, etc.) so that, even if static, they induce apparent motion on the image plane that our method can bootstrap into objects. If an \u201cobject\u201d is neither moving, nor triggers occluding boundaries in response to the viewer\u2019s motion (e.g. an object painted on a wall, as opposed to the actual object), we are comfortable not discovering it as it has no affordances such as, in Gibson\u2019s words, graspability.\n\nAs suggested by the reviewer, we conducted the experiment of taking RGB images as the input and flow as the condition, and this practice yielded unmeaningful results.\n\n**W4:** *quantitative results for the ablation with adversarial decoder should also be reported as that's one of the claimed contributions.*\n\n**R:** We do provide ablation with adversarial decoder in Table 2 and 3.\n\n**W5:** *Finally, more visualizations should be included, at least in the supplementary.*\n\n**R:** We do have figure 1, 3, 4, 6, 8, 9, 10, 11 showing qualitative visualization."
                    }
                },
                "number": 7,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1012/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700709491991,
                "cdate": 1700709491991,
                "tmdate": 1700715744250,
                "mdate": 1700715744250,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "b4U2YxNGsT",
                "forum": "2MpOjashKU",
                "replyto": "ndSdCjTta6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1012/Reviewer_i9DX"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1012/Reviewer_i9DX"
                ],
                "content": {
                    "title": {
                        "value": "Response to author's rebuttal"
                    },
                    "comment": {
                        "value": "Thanks to the authors for providing the response. However, the response still cannot resolve my concerns. I also apologize for some of the unclear comments on my original reviews. \n\nR1: I agree that the definition of objects is different in different contexts and I respect the reference of [1]. However, I don't think using this kind of definition is proper. Firstly, the definition of [1] is in terms of images/graphics rather than videos/image sequences. In the context of images, there is no definition of moving objects and static objects. Secondly, even if this definition holds, \"Due to parallax, such objects elicit piecewise smooth optical flow even if they are static just due to the viewer\u2019s motion.\" this is a big challenge for optical flow estimation, which is not the main focus of this paper. In other words, if the optical flow cannot distinguish those static objects, the proposed method still fails to work. Again, taking MOVI-D as an example, MOVI-D has both non-moving and moving objects with camera motion but even for the GT flow, the static objects are invisible. The proposed method will still fail to work in this dataset. I am also glad to see the results from MOVI-D if I am wrong. \n\nI think I should also rephrase my point: the terminology may be fine. Based on the definition of objects from the paper, moving objects and static objects can still be distinguished, as ideally, they both should have different optical flows compared to the background. However, distinguishing static objects from the background with, or without ego-motion is super challenging. Usually, it is very hard for an optical flow estimation model to predict the desired flow for non-moving objects, making the proposed method hard to work for discovering non-moving objects.\n\nR2: There are indeed a few works running object discovery in the real world. Here are a few works in my mind:  \n[1] Simple Unsupervised Object-Centric Learning for Complex and Naturalistic Videos: https://arxiv.org/abs/2205.14065  \n[2] Discovering Objects that Can Move: https://arxiv.org/abs/2203.10159   \n[3] Bridging the Gap to Real-World Object-Centric Learning: https://openreview.net/forum?id=b9tUk-f_aG  \n[4] Object-Centric Learning for Real-World Videos by Predicting Temporal Feature Similarities: https://openreview.net/forum?id=t1jLRFvBqm&noteId=t1jLRFvBqm   \n\nR3: Thanks for the response for that. It helps a lot for better understanding.\n\nR4: I apologize for this concern. I originally meant the ablations of the $\\lambda$ but I also found the results were provided in Table 2. \n\nR5: For the original comment, I meant more qualitative results for each dataset that are *similar*  to Figure 6 (several images in the same benchmark in a figure) should be included in the supplementary."
                    }
                },
                "number": 8,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1012/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700714419383,
                "cdate": 1700714419383,
                "tmdate": 1700715016700,
                "mdate": 1700715016700,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "wFp6HdWBFR",
                "forum": "2MpOjashKU",
                "replyto": "ndSdCjTta6",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission1012/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission1012/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Segmenting static objects, additional related work"
                    },
                    "comment": {
                        "value": "**R1:** *I agree that the definition of objects is different in different contexts and I respect the reference of [1]. However, I don't think using this kind of definition is proper. Firstly, the definition of [1] is in terms of images/graphics rather than videos/image sequences. In the context of images, there is no definition of moving objects and static objects. Secondly, even if this definition holds, \"Due to parallax, such objects elicit piecewise smooth optical flow even if they are static just due to the viewer\u2019s motion.\" this is a big challenge for optical flow estimation, which is not the main focus of this paper. In other words, if the optical flow cannot distinguish those static objects, the proposed method still fails to work. Again, taking MOVI-D as an example, MOVI-D has both non-moving and moving objects with camera motion but even for the GT flow, the static objects are invisible. The proposed method will still fail to work in this dataset. I am also glad to see the results from MOVI-D if I am wrong.*\n\n**Response:** A \"detached object\" is part of the scene, not of the image, regardless of whether it moves or not, thus the definition of Gibson is aligned with our use of the term. Others have also used the same, for instance [5] characterizes detachable objects based on motion discontinuities, whether the scene is static or not. It is the occlusion phenomena, a result of either object or viewer motion, that defines detached objects as manifest in images or video.\n\nAs an example, see the example on a video of static scenes, taken from a moving camera (Figure 12 in the revised Appendix). We agree with the reviewer that optical flow methods sometimes fail to reliably detect occluding boundaries. This is evident in datasets like MOVI-D, where the camera angle is constrained. However, in a real embodied vision system where agents move continuously, the signal is eventually patent.\n\n**R2:** *There are indeed a few works running object discovery in the real world. Here are a few works in my mind: ...*\n\n**Response:** Thanks to the reviewer for providing the list of related work. Note that we have already referenced and cited [1] and [2] and described their limitations: \n\n[1] primarily works with synthetic data (MOVi-Solid, MOVi-E, MOVi-Tex) that has a finite set of object colors and shapes. The \"real-world\" data used by [1] (Youtube Traffic and Youtube Aquarium) is limited to mostly static backgrounds and specific object classes (cars and fish). On the other hand, DivA can handle more diverse video data, including unknown object appearances and classes, as well as dynamic backgrounds, as demonstrated by DAVIS and FBMS-59. This is crucial in the context of \"object discovery,\" where the algorithm explores the unknown.\n\n[2] depends on **supervised** features [6] trained on MS-COCO to generate motion masks and trains a slot attention model on top of that. In fact, DivA can be utilized alongside [2], providing motion masks without supervision.\n\nBoth [3] and [4] rely on self-supervised features (DINO) pre-trained on object-centric data. As mentioned in our manuscript,  we explore the minimal end of the supervision scale, where no purposeful viewpoint selection, 'primary object,' or human labels are used. DivA trains *ab-ovo* without any external pre-trained features, making it immune to data selection biases external to the model. This feature is also crucial for object discovery in embodied cognition. Nevertheless, we have revised the paper to include [3] and [4] in the citation as suggested.\n\nReference:\n\n[5] A. Ayvaci et al.,  Detachable object detection: Segmentation and depth ordering from short-baseline video.\n\n[6] D. Achal et al., Towards segmenting anything that moves."
                    }
                },
                "number": 9,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission1012/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700726192648,
                "cdate": 1700726192648,
                "tmdate": 1700726573551,
                "mdate": 1700726573551,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]