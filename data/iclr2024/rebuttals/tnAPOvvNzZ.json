[
    {
        "title": "JsonTuning: Towards Generalizable, Robust, and Controllable Instruction Tuning"
    },
    {
        "review": {
            "id": "O8F96mVa4D",
            "forum": "tnAPOvvNzZ",
            "replyto": "tnAPOvvNzZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7027/Reviewer_5Thw"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7027/Reviewer_5Thw"
            ],
            "content": {
                "summary": {
                    "value": "The authors propose JsonTuning which unifies different text-to-text tasks into a structured format, i.e. json format. Experiments and analyses are conducted on various popular benchmarks with models from the LLaMA family ranging from 7B to 13B."
                },
                "soundness": {
                    "value": "2 fair"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "* The motivation is simple and easy to understand.\n* The paper is well-written and easy to follow.\n* Comprehensive experiments and analyses are provided to support the authors' claims."
                },
                "weaknesses": {
                    "value": "There are several key limitations of the proposed approach:\n* Context window consumption: JsonTuning demands much more tokens than TextTuning and it poses a challenge both in the training and inference time. One key aspect of current LLMs is their context window, and how to use it efficiently. The authors should provide evidence relating to the extra overhead on training/inference of JsonTuning.\n*  It is not also obvious to transform the user's request into a structured format. Despite the authors' arguments on the generalization capability of JsonTuning, structured object like Json still lags far behind natural language in terms of expressivity. The tasks concerned in the work are relatively easy and can be transformed into json format with less effort. However, I am not convinced this generalizes to real industrial use cases where the user's request could be far more complex.\n* A simple question: assume a user requests the model to produce output following json format at the first place. Then how the output control be defined on the input side? I suppose it would be a nested json and could be fragile for the model to take as input."
                },
                "questions": {
                    "value": "Please see the weakness section."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 1,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7027/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698577586925,
            "cdate": 1698577586925,
            "tmdate": 1699636824568,
            "mdate": 1699636824568,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "onCkGeq5Kj",
                "forum": "tnAPOvvNzZ",
                "replyto": "O8F96mVa4D",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7027/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7027/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer 5Thw"
                    },
                    "comment": {
                        "value": "> Analysis of the tokens used for JsonTuning and TextTuning\n\nWe have calculated the input and output tokens used for each training and evaluation example in both TextTuning and JsonTuning. The average tokens of all training and evaluation examples for these methods are presented in the following table:\n\n|  | Training | Evaluation | \n| -------- | -------- | -------- | \n| Text w/ label space and control information | 317.53 | 236.24 |\n| Json w/ label space and control information | 394.61 | 296.55 |\n| Text w/o label space and control information | 278.17 | 183.91 |\n| Json w/o label space and control information | 349.23 | 235.16 |\n\nThe table shows that JsonTuning uses approximately 25% more tokens for training and evaluation compared to TextTuning. The additional tokens are primarily used to encode the JSON structured format.\n\nThere are potential ways to reduce the tokens used for JsonTuning:\n\n* JsonTuned models are robust to different prompts, so using lengthy prompts may not be necessary. We can replace them with concise prompts.\n* JsonTuning offers the flexibility to adjust the structure. Simplifying the input and output structures could be helpful, as JsonTuning requires more tokens for more complex structures. \n\n> Regarding the expressivity of natural language texts and JSON structures\n\nThe expressivity of the constructed JSON structures is at least equivalent to that of their corresponding natural language texts, as JSON structures can easily be converted into natural language texts. For instance, consider the following JSON input and text input. By replacing placeholders, such as {question} and {options}, in the JSON structures with their respective values, the text input can be obtained.\n\n**JSON Input**: {\"input\": {\"question\": \"Who is the CEO of Google?\", \"options\": \"(A) Sundar Pichai (B) Bill Gates (C) Tim Cook (D) Satya Nadella\", \"instruction\": \"Answering the following question: {question} {options}. Answer: {answer}\"}}\n\n**Text Input**: \"Answering the following question: Who is the CEO of Google? (A) Sundar Pichai (B) Bill Gates (C) Tim Cook (D) Satya Nadella. Answer:\" \n\nHowever, this transformation may lose some information, such as what are the key elements of the task. Our proposed JsonTuning method aims to address this issue. Consequently, JSON structures offer more valuable information than natural language texts, resulting in higher expressivity. \n\n> Regarding transforming the user's request into a structured format, especially for complex use cases\n\nTransforming user requests into a structured JSON format can be efficiently achieved by developing a user interface that enables users to input key task elements, task prompts, and control information. This approach ensures that users can effectively communicate their requirements without needing to understand JSON structures. As the input and output JSON structures are consistent across all tasks, we can automatically construct them based on the user inputs. For more complex use cases, the user interface can guide the model in identifying the essential information for the target task and the desired outputs.\n\nJsonTuning offers several advantages for applications:\n\n* Robustness: The method's resilience to varying prompts significantly reduces the need for users to test multiple prompts for optimal performance. If users do not wish to provide a task prompt, a default prompt can be generated based on the key task elements they supply, thereby minimizing user effort. Furthermore, as backend models are updated or changed, the optimal prompt for natural language texts may vary, necessitating adjustments for each model version. In contrast, Json-tuned models may not require prompt modifications even when the backbone model is updated, considerably reducing user effort.\n\n* Output Control: The output control component allows users to accurately specify and easily parse the output, which can be challenging when using natural language texts. Users only need to comprehend basic types such as string, object, and array. The descriptions and examples of these data types can be provided through the user interface. This approach necessitates minimal human effort, and the underlying JSON structures can be automatically constructed once users indicate the output elements they require.\n\nBy implementing JsonTuning in real-world applications, a more professional and efficient user experience can be achieved."
                    }
                },
                "number": 4,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7027/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700509491577,
                "cdate": 1700509491577,
                "tmdate": 1700509491577,
                "mdate": 1700509491577,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "5ljtALayww",
            "forum": "tnAPOvvNzZ",
            "replyto": "tnAPOvvNzZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7027/Reviewer_qRB3"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7027/Reviewer_qRB3"
            ],
            "content": {
                "summary": {
                    "value": "The paper introduces a new approach called JsonTuning, which leverages the structured JSON format to represent tasks. JsonTuning improves generalization by helping the model understand task elements and their relationships, enhances robustness by reducing ambiguity, and offers greater control over the model's output. The paper presents a thorough comparative study with different language models and evaluation benchmarks, demonstrating that JsonTuning surpasses TextTuning in various applications, achieving better performance, adaptability, robustness, and control."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "3 good"
                },
                "strengths": {
                    "value": "This paper provides a very simple method to convert the original instruction tuning into a unified Json format. The authors also conduct comprehensive comparison against baselines trained under text-to-text formulation, and show that JsonTuning can harvest better performance. The ablation studies help us to better understand the effect of the subparts such as label space and control information."
                },
                "weaknesses": {
                    "value": "Despite the commendable performance exhibited by JsonTuning, there are still several notable weaknesses:\n\n**Introduction of Additional Knowledge in Input**: The utilization of Json formatting can aid models in generating outputs that conform to the constraints specified in the Json input. However, it also introduces supplementary knowledge, such as information pertaining to input and output types, which is originally absent in the unstructured textual instructions. To facilitate a fair comparison, it becomes essential to incorporate the information present in the Json input but absent in the textual input into the original text instructions. This allows for a comprehensive assessment of whether the Json format indeed outperforms the text format.\n\n**Potential Incompleteness of Generalization Experiments**: While the training dataset encompasses a diverse range of tasks, including Flan and P3++, there appears to be some overlap between the types of tasks assessed during testing and those encountered in training. It is pertinent to explore whether the model can adeptly generalize to entirely unseen task types, which may not have been covered by the training data. Furthermore, the evaluation tasks, primarily centered on structure prediction, demand output format constraints. However, for the field of instruction tuning, there exists a research interest in user-oriented instruction following evaluation, exemplified by AlpacaEval. In cases involving more open-ended instruction data, the Json format may not bring additional benefits since there are no specific constraints imposed on the output format. It also remains uncertain how the model performs on the Super-NI test set, a widely employed dataset assessing a model's ability to follow instructions.\n\n**Challenges for Real-World Applications**: The practical applicability of Json-tuned models is constrained by the need to manually craft complex Json prompts that delineate output fields and their corresponding types. For users lacking expertise in computer science, utilizing such models proves challenging, as they may lack the proficiency required to construct a Json-formatted instruction. Unless a method for automatically generating Json instructions becomes available, the utility of this approach is notably limited in real-world scenarios."
                },
                "questions": {
                    "value": "1. Will there be a significant improvement after fine-tuning code LLMs (e.g., CodeLLAMA) on the Json instruction data? It seems that code LLMs might be a better counterpart that learns the Json data more easily.\n\n2. There might be several missing relevant references. The first one have studied Json-format instruction data. The second one is a resource that has the similar formulation with Super-NI and can be represented in well-structured Json format as well: \n- Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning. Yin et al., 2023.\n- Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation. Yin et al., 2023."
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "5: marginally below the acceptance threshold"
                },
                "confidence": {
                    "value": "4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7027/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7027/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7027/Reviewer_qRB3"
                    ]
                }
            },
            "number": 2,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7027/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698813123489,
            "cdate": 1698813123489,
            "tmdate": 1699636824454,
            "mdate": 1699636824454,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "i57CknlAv1",
                "forum": "tnAPOvvNzZ",
                "replyto": "5ljtALayww",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7027/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7027/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Responses to Reviewer qRB3"
                    },
                    "comment": {
                        "value": "> Introduction of Additional Knowledge in Input\n\nIn Section 4: Analysis of our paper, we conduct an ablation study of JsonTuning without label space and control information, where JsonTuning still outperforms TextTuning in terms of generalization. To address your question comprehensively, we perform additional experiments with TextTuning, incorporating label space and control information. The generalization and robustness results on LLaMA-7b are as follows:\n\nGeneralization Results:\n\n| Model | MMLU | BBH | NER | RE | EE | TQA | NL2SQL | Average |\n| -------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- |\n| Text w/ label space and control information | 44.56 | 36.77 | 35.70 | 15.77 | 1.45 / 0.00 | 16.40 | 14.60 | 23.50 |\n| Json w/ label space and control information | 44.69 | 37.08 | 41.80 | 15.56  | 3.09 / 8.24 | 16.40 | 16.40 | **25.37**\n| Text w/o label space and control information | 43.11 | 32.48 | 36.47 | 13.60 | 1.08 / 0.00 | 18.20 | 8.60 | 21.86\n| Json w/o label space and control information | 42.66 | 36.23 | 47.65 | 16.23 | 0.99 / 0.29 | 10.40 | 12.80 | **23.80**\n\nRobustness results on MMLU with 10 different prompts (we calculate the mean and standard deviation (Std) of the performance using these 10 prompts.):\n\n| Model | Mean | Std | \n| -------- | -------- | -------- | \n| Text w/ label space and control information | 42.02 | 3.31 | \n| Json w/ label space and control information | **44.63** | **0.11** |\n| Text w/o label space and control information | 38.82 | 6.28 |\n| Json w/o label space and control information | **42.19** | **0.21** |\n\nWe can make the following observations: (1) JsonTuning consistently surpasses TextTuning in both generalization and robustness, regardless of the inclusion of label space and control information. Importantly, JsonTuning significantly reduces the model's sensitivity to different prompts, as evidenced by the lower variance in performance across prompts. (2) TextTuning with control information still struggles to generalize well to more complex structured tasks, as illustrated by the poor performance on the EE task. JsonTuning demonstrates a marked improvement in this regard. These findings emphasize the importance of structured formats in instruction tuning. Furthermore, our proposed components \u2013 label space and control information \u2013 consistently enhance the model's generalization capabilities and robustness, highlighting their value in instruction tuning.\n\n> Potential Incompleteness of Generalization Experiments\n\nOur evaluation encompasses various unseen tasks and datasets. While some task types, such as multiple-choice QA, are encountered during training, the specific evaluation tasks and datasets remain unseen. Furthermore, tasks like EE and TQA represent entirely novel task types. Our JsonTuning method allows models to generalize to more complex EE structures, even if they were not encountered during training, demonstrating the robust generalization capabilities of our Json-tuned models.\n\nWe conduct an evaluation of LLaMA-7B-Json and LLaMA-7B-Text on AlpacaEval. Both models were trained with label space and control information. To assess LLaMA-7B-Json, we construct an input JSON structure for a natural language question Q, represented as {\"input\": {\"question\": Q, \"instruction\": \"{question}\\n{answer}\"}, \"output control\": {\"answer\": {\"type\": \"string\"}}}. The win rates for LLaMA-7B-Json and LLaMA-7B-Text are 4.78% and 4.64%, respectively. The slightly better performance of LLaMA-7B-Json suggests that JsonTuning may offer additional advantages in open-ended scenarios.\n\nSince the Super-NI test set is already incorporated into the Flan2022 collection, it would not be appropriate to evaluate models on this test set, as many instances in it are seen during training."
                    }
                },
                "number": 6,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7027/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700509558902,
                "cdate": 1700509558902,
                "tmdate": 1700509558902,
                "mdate": 1700509558902,
                "license": "CC BY 4.0",
                "version": 2
            },
            {
                "id": "w8o09krUwd",
                "forum": "tnAPOvvNzZ",
                "replyto": "b7rcFYCldi",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7027/Reviewer_qRB3"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7027/Reviewer_qRB3"
                ],
                "content": {
                    "comment": {
                        "value": "I greatly appreciate the author's comprehensive response to my review. However, I still have some concern about 1) the practical use, 2) in-depth analysis, and 3) novelty of the research.\n\nIn terms of the practical use, though you mention that it is possible to design a user interface and then convert the user input into instruction-tuning data format. However, different task types may involve distinct Json data fields. Once there comes a new task type that accepts completely different information, the user interface will have to be updated manually. Also, it is not realistic to require users to provide very detailed instruction and examples specifically for JsonTuning data, as people always prefer fully automatic and general approaches that guarantee the data quality. \n\nRegarding the new experimental results, it is shown that on some tasks such as TQA, \"Json w/o label space and control information\" underperforms the TextTuning counterparts by a large margin. Though the average performance is good, it is still unclear to me why JsonTuning has significantly worse performance than TextTuning on this task type. Also, if we remove the tasks such as NER and NL2SQL, the average performance may become lower than the text-only counterparts. More in-depth analysis (e.g., the error case study) is needed.\n\nFor the novelty, the Section 5 in Yin et al., 2023 (\"Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning\") shows the benefits of introducing Json formats in instruction tuning. The paper discusses something beyond identifying the key elements in task definitions. The authors might put more emphasis on how to highlight the distinct contributions not covered in the prior works."
                    }
                },
                "number": 10,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7027/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700605995834,
                "cdate": 1700605995834,
                "tmdate": 1700605995834,
                "mdate": 1700605995834,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "9YVBOYcKnk",
            "forum": "tnAPOvvNzZ",
            "replyto": "tnAPOvvNzZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7027/Reviewer_jpTc"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7027/Reviewer_jpTc"
            ],
            "content": {
                "summary": {
                    "value": "This paper uses JSON as a formatting scheme to tune language models, and shows that this can help models learn and generalize better than formatting examples as plain text. JSONTuning also reduces prompt sensitivity."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "3 good"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- The paper is clear and easy to follow. \n - The method is simple and works well."
                },
                "weaknesses": {
                    "value": "- In my opinion this paper needs some sort of analysis of the number of additional tokens introduced by the JSON format for training and inference. The additional training cost is probably negligible and unimportant, but the additional FLOPS and encoded/decoded tokens for the JSON format will add up for inference. Note: I am not saying that the fact that JSON-formatted examples have extra tokens is a weakness, but this extra cost should at least be quantified in my opinion. \n - I generally don't think papers should be penalized harshly for a lack of novelty or for overly simple methods, but it must be acknowledged that the idea of \"format exampels as JSON instead of plain text\" can only be taken so far. Note: I am not saying this is grounds for rejection, just that this paper's impact and contribution is limited."
                },
                "questions": {
                    "value": "- How do text-tuned models behave as a base for JSON-formatted examples? For example, I think some of the following questions should be answered or at least discussed: \n   * What if we text-tune the model and then JSONTune it on a small dataset? \n   * What if we text-tune the model and then use JSON-formatted examples in a few shot prompt? \n - What is the decoding mechanism used for evaluating models? I assume it is just greedy sampling, but it would be nice for more evaluation details to be in the appnendix. \n - How often was invalid JSON generated? What do the authors think about constrained decoding schemes like [Scholak et al](https://arxiv.org/abs/2109.05093), is there any point of using constrained decoding to improve JSONTuned models?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."
                },
                "code_of_conduct": {
                    "value": "Yes"
                },
                "first_time_reviewer": {
                    "value": "Yes",
                    "readers": [
                        "ICLR.cc/2024/Conference/Program_Chairs",
                        "ICLR.cc/2024/Conference/Submission7027/Senior_Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7027/Area_Chairs",
                        "ICLR.cc/2024/Conference/Submission7027/Reviewer_jpTc"
                    ]
                }
            },
            "number": 3,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7027/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698824094171,
            "cdate": 1698824094171,
            "tmdate": 1699636824340,
            "mdate": 1699636824340,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "v2XAVXuCFR",
                "forum": "tnAPOvvNzZ",
                "replyto": "9YVBOYcKnk",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7027/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7027/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer jpTc"
                    },
                    "comment": {
                        "value": "> Analysis of the tokens used for JsonTuning and TextTuning\n\nWe have calculated the input and output tokens used for each training and evaluation example in both TextTuning and JsonTuning. The average tokens of all training and evaluation examples for these methods are presented in the following table:\n\n|  | Training | Evaluation | \n| -------- | -------- | -------- | \n| Text w/ label space and control information | 317.53 | 236.24 |\n| Json w/ label space and control information | 394.61 | 296.55 |\n| Text w/o label space and control information | 278.17 | 183.91 |\n| Json w/o label space and control information | 349.23 | 235.16 |\n\nThe table shows that JsonTuning uses approximately 25% more tokens for training and evaluation compared to TextTuning. The additional tokens are primarily used to encode the JSON structured format.\n\nThere are potential ways to reduce the tokens used for JsonTuning:\n\n* JsonTuned models are robust to different prompts, so using lengthy prompts may not be necessary. We can replace them with concise prompts.\n* JsonTuning offers the flexibility to adjust the structure. Simplifying the input and output structures could be helpful, as JsonTuning requires more tokens for more complex structures. \n\n> What if we text-tune the model and then Json-tune it on a small dataset?\n\nTo explore this, we continue instruction-tuning the text-tuned model with JsonTuning using varying numbers of examples (120, 600, 1200, 6000, and 12000) while maintaining the relative data ratio of the Flan 2022 collection and structured tasks in our primary experiments. The LLaMA-7b results on MMLU and NL2SQL are as follows:\n\n|  | 120 | 600 | 1200 | 6000 | 12000 |\n| -------- | -------- | -------- |  -------- | -------- | -------- | \n| MMLU | 0.00 | 40.83 | 39.83 | 42.45 | 41.85 |\n| NL2SQL | 0.00 | 4.40 | 3.80 | 5.20 | 3.40 |\n\nThe results of the original text-tuned model on MMLU and NL2SQL are 43.11 and 8.60, respectively. \n\nWe observe that: (1) A very limited number of JSON-formatted examples (e.g., 120) is insufficient for the text-tuned model to handle JSON-formatted tasks. (2) Tuning the text-tuned model with a small set of JSON-formatted examples results in degraded performance. These observations suggest that using the text-tuned model as a base for JsonTuning is not appropriate.\n\n> What if we text-tune the model and then use JSON-formatted examples in a few shot prompt?\n\nThis approach is ineffective. We sampled 100 MMLU examples and used 3 JSON-formatted examples as few-shot prompts. The text-tuned LLaMA-7b model still generates natural language texts instead of JSON structures and these texts seem non-sense. While a significantly larger text-tuned model may generate valid JSON structures, its performance could be substantially lower than the JSON-tuned model.\n\n> What is the decoding mechanism used for evaluating models? I assume it is just greedy sampling, but it would be nice for more evaluation details to be in the appendix.\n\nYou are correct. We use greedy decoding. We appreciate your suggestion and will include more evaluation details in the updated paper.\n\n> How often was invalid JSON generated? What do the authors think about constrained decoding schemes like Scholak et al, is there any point of using constrained decoding to improve JSONTuned models?\n\nThe proportion of invalid JSON structures generated by LLaMA-7B on evaluation tasks is as follows:\n\n| MMLU | BBH | NER | RE | EE | TQA | NL2SQL | \n| -------- | -------- | -------- | -------- | -------- | -------- | -------- | \n0.00% | 1.57% | 5.74% | 0.27% | 4.19% | 0.00% | 3.60%\n\nAs anticipated, tasks with intricate output structures, such as NER and EE, exhibit a higher ratio of invalid JSON structures compared to tasks with simpler output structures, such as MMLU and BBH.\n\nPICARD, a constrained decoding method proposed by Scholak et al., is designed to produce valid SQL queries by rejecting inadmissible tokens at each decoding step. In the context of our JsonTuning paradigm, employing a constrained decoding scheme that ensures the generation of valid output structures adhering to the output control could be advantageous. However, implementing such a method may be challenging, and it could potentially increase decoding time. As an alternative, we may utilize post-processing techniques to obtain valid JSON structures, as they are typically efficient and cost-effective. Previously, we treated invalid output structures as NULL, which might have been overly strict for evaluation purposes. Moving forward, we plan to investigate suitable post-processing techniques that can be broadly applied to various output structures, aiming to enhance the performance and validity of Json-tuned models."
                    }
                },
                "number": 3,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7027/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700509447640,
                "cdate": 1700509447640,
                "tmdate": 1700509447640,
                "mdate": 1700509447640,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    },
    {
        "review": {
            "id": "Wv2rqahtm1",
            "forum": "tnAPOvvNzZ",
            "replyto": "tnAPOvvNzZ",
            "signatures": [
                "ICLR.cc/2024/Conference/Submission7027/Reviewer_pk2e"
            ],
            "nonreaders": [],
            "readers": [
                "everyone"
            ],
            "writers": [
                "ICLR.cc/2024/Conference",
                "ICLR.cc/2024/Conference/Submission7027/Reviewer_pk2e"
            ],
            "content": {
                "summary": {
                    "value": "This paper proposes JsonTuning,  a novel structure-to-structure approach for instruction tuning. It leverages the widely used JSON format and uses the structured format to define tasks. Empirical results show JsonTuning improves TextTuning on generalization, robustness, and controllability. Ablation study shows the label space and control information are both curial for JsonTuning. The paper also analyzes the effects of data size and structured tasks."
                },
                "soundness": {
                    "value": "3 good"
                },
                "presentation": {
                    "value": "4 excellent"
                },
                "contribution": {
                    "value": "2 fair"
                },
                "strengths": {
                    "value": "- Originality: The paper proposes JsonTuning, which is a new format of instruction tuning. It reformats standard instruction tuning data into JSON format to reduce ambiguity and improve controllability. The proposed method is novel in that it leverages the LLM's understanding of structured data format - JSON and makes use of it in downstream instruction tuning.\n- Clarity: The paper is well-written and easy to follow.\n- Quality: The claims in this paper are well supported by citations or empirical results. The authors clearly demonstrate JsonTuning's advantages in generalization, robustness, and controllability."
                },
                "weaknesses": {
                    "value": "- Significance: the format of instruction tuning data is only one of the many system choices of the overall instruction tuning. Others include tasks, base model, domains, and languages. Since the paper only focuses on the data format on a selection of tasks, the significance is limited.\n- Soundness: It is not clear if a TextTuning model with candidate answers and output control in plain text would also perform as good as JsonTuning. In other words, how much gain was from the structured format of JSON itself?"
                },
                "questions": {
                    "value": "- How did you construct training data for JsonTuning from plain text instruction tunning data? Did you use human annotator to do the conversion or use another language model?\n- Why did you choose JSON format instead of other popular structured formats, such as XML and YAML?"
                },
                "flag_for_ethics_review": {
                    "value": [
                        "No ethics review needed."
                    ]
                },
                "rating": {
                    "value": "6: marginally above the acceptance threshold"
                },
                "confidence": {
                    "value": "3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked."
                },
                "code_of_conduct": {
                    "value": "Yes"
                }
            },
            "number": 4,
            "invitations": [
                "ICLR.cc/2024/Conference/Submission7027/-/Official_Review",
                "ICLR.cc/2024/Conference/-/Edit"
            ],
            "domain": "ICLR.cc/2024/Conference",
            "tcdate": 1698829743425,
            "cdate": 1698829743425,
            "tmdate": 1699636824165,
            "mdate": 1699636824165,
            "license": "CC BY 4.0",
            "version": 2
        },
        "responses": [
            {
                "id": "Prnd6KTGiP",
                "forum": "tnAPOvvNzZ",
                "replyto": "Wv2rqahtm1",
                "signatures": [
                    "ICLR.cc/2024/Conference/Submission7027/Authors"
                ],
                "readers": [
                    "everyone"
                ],
                "writers": [
                    "ICLR.cc/2024/Conference",
                    "ICLR.cc/2024/Conference/Submission7027/Authors"
                ],
                "content": {
                    "title": {
                        "value": "Response to Reviewer pk2e"
                    },
                    "comment": {
                        "value": "> Regarding the significance of our work\n\nThe performance of instruction tuning is influenced by several key factors, including the base model, tuning data, and data format. While the base model and tuning data have received considerable attention from researchers, the significance of data format has often been overlooked. In this paper, we aim to emphasize the importance of data format in instruction tuning, focusing on its role in generalization, robustness, and output control. \n\nOur experiments and analysis demonstrate that a structured data format, such as JSON, is crucial for effective instruction tuning. First, our method, JsonTuning, consistently outperforms TextTuning across various base models and tuning data quantities, highlighting its generalizability and complementarity with other factors. Second, JsonTuning substantially enhances the robustness of instruction-tuned models, an improvement that is difficult to achieve solely through the base model and tuning data optimization. For instance, our robustness results indicate that upgrading the base model from LLaMA to LLaMA2 does not significantly enhance robustness; however, JsonTuning considerably improves it. Finally, JSON structures offer well-defined control information for output generation, a feature that is challenging to achieve using natural language text alone.\n\n> It is not clear if a TextTuning model with candidate answers and output control in plain text would also perform as well as JsonTuning. In other words, how much gain was from the structured format of JSON itself?\n\nIn Section 4: Analysis of our paper, we conduct an ablation study of JsonTuning without label space and control information, where JsonTuning still outperforms TextTuning in terms of generalization. To address your question comprehensively, we perform additional experiments with TextTuning, incorporating label space and control information. The generalization and robustness results on LLaMA-7b are as follows:\n\nGeneralization Results:\n\n| Model | MMLU | BBH | NER | RE | EE | TQA | NL2SQL | Average |\n| -------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- |\n| Text w/ label space and control information | 44.56 | 36.77 | 35.70 | 15.77 | 1.45 / 0.00 | 16.40 | 14.60 | 23.50 |\n| Json w/ label space and control information | 44.69 | 37.08 | 41.80 | 15.56  | 3.09 / 8.24 | 16.40 | 16.40 | **25.37**\n| Text w/o label space and control information | 43.11 | 32.48 | 36.47 | 13.60 | 1.08 / 0.00 | 18.20 | 8.60 | 21.86\n| Json w/o label space and control information | 42.66 | 36.23 | 47.65 | 16.23 | 0.99 / 0.29 | 10.40 | 12.80 | **23.80**\n\nRobustness results on MMLU with 10 different prompts (we calculate the mean and standard deviation (Std) of the performance using these 10 prompts.):\n\n| Model | Mean | Std | \n| -------- | -------- | -------- | \n| Text w/ label space and control information | 42.02 | 3.31 | \n| Json w/ label space and control information | **44.63** | **0.11** |\n| Text w/o label space and control information | 38.82 | 6.28 |\n| Json w/o label space and control information | **42.19** | **0.21** |\n\nWe can make the following observations: (1) JsonTuning consistently surpasses TextTuning in both generalization and robustness, regardless of the inclusion of label space and control information. Importantly, JsonTuning significantly reduces the model's sensitivity to different prompts, as evidenced by the lower variance in performance across prompts. (2) TextTuning with control information still struggles to generalize well to more complex structured tasks, as illustrated by the poor performance on the EE task. JsonTuning demonstrates a marked improvement in this regard. These findings emphasize the importance of structured formats in instruction tuning. Furthermore, our proposed components \u2013 label space and control information \u2013 consistently enhance the model's generalization capabilities and robustness, highlighting their value in instruction tuning."
                    }
                },
                "number": 1,
                "invitations": [
                    "ICLR.cc/2024/Conference/Submission7027/-/Official_Comment"
                ],
                "domain": "ICLR.cc/2024/Conference",
                "tcdate": 1700509368281,
                "cdate": 1700509368281,
                "tmdate": 1700509368281,
                "mdate": 1700509368281,
                "license": "CC BY 4.0",
                "version": 2
            }
        ]
    }
]